- en: ControlNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/controlnet](https://huggingface.co/docs/diffusers/api/pipelines/controlnet)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet was introduced in [Adding Conditional Control to Text-to-Image Diffusion
    Models](https://huggingface.co/papers/2302.05543) by Lvmin Zhang, Anyi Rao, and
    Maneesh Agrawala.
  prefs: []
  type: TYPE_NORMAL
- en: With a ControlNet model, you can provide an additional control image to condition
    and control Stable Diffusion generation. For example, if you provide a depth map,
    the ControlNet model generates an image that‚Äôll preserve the spatial information
    from the depth map. It is a more flexible and accurate way to control the image
    generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We present ControlNet, a neural network architecture to add spatial conditioning
    controls to large, pretrained text-to-image diffusion models. ControlNet locks
    the production-ready large diffusion models, and reuses their deep and robust
    encoding layers pretrained with billions of images as a strong backbone to learn
    a diverse set of conditional controls. The neural architecture is connected with
    ‚Äúzero convolutions‚Äù (zero-initialized convolution layers) that progressively grow
    the parameters from zero and ensure that no harmful noise could affect the finetuning.
    We test various conditioning controls, eg, edges, depth, segmentation, human pose,
    etc, with Stable Diffusion, using single or multiple conditions, with or without
    prompts. We show that the training of ControlNets is robust with small (<50k)
    and large (>1m) datasets. Extensive results show that ControlNet may facilitate
    wider applications to control image diffusion models.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [takuma104](https://huggingface.co/takuma104).
    ‚ù§Ô∏è
  prefs: []
  type: TYPE_NORMAL
- en: The original codebase can be found at [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet),
    and you can find official ControlNet checkpoints on [lllyasviel‚Äôs](https://huggingface.co/lllyasviel)
    Hub profile.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionControlNetPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.StableDiffusionControlNetPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L139)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî A `UNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    or `List[ControlNetModel]`) ‚Äî Provides additional conditioning to the `unet` during
    the denoising process. If you set multiple ControlNets as a list, the outputs
    from each ControlNet are added together to create one combined additional conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) ‚Äî Classification module that
    estimates whether generated images could be considered offensive or harmful. Please
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model‚Äôs potential harms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    ‚Äî A `CLIPImageProcessor` to extract features from generated images; used as inputs
    to the `safety_checker`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using Stable Diffusion with ControlNet
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L894)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, ‚Äî `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`): The ControlNet input
    condition to provide guidance to the `unet` for generation. If the type is specified
    as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can
    also be accepted as an image. The dimensions of the output image defaults to `image`‚Äôs
    dimensions. If height and/or width are passed, `image` is resized accordingly.
    If multiple ControlNets are specified in `init`, images must be passed as a list
    such that each element of the list can be correctly batched for input to a single
    ControlNet. When `prompt` is a list, and if a list of images is passed for a single
    ControlNet, each will be paired with each prompt in the `prompt` list. This also
    applies to multiple ControlNets, where a list of image lists can be passed to
    batch for each prompt and each ControlNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) ‚Äî The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timesteps` (`List[int]`, *optional*) ‚Äî Custom timesteps to use for the denoising
    process with schedulers which support a `timesteps` argument in their `set_timesteps`
    method. If not defined, the default behavior when `num_inference_steps` is passed
    will be used. Must be in descending order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) ‚Äî A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image ‚Äî (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) ‚Äî A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) ‚Äî The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_kwargs` (`dict`, *optional*) ‚Äî A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) ‚Äî The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`. If multiple ControlNets
    are specified in `init`, you can set the corresponding scale as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guess_mode` (`bool`, *optional*, defaults to `False`) ‚Äî The ControlNet encoder
    tries to recognize the content of the input image even if you remove all prompts.
    A `guidance_scale` value between 3.0 and 5.0 is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_start` (`float` or `List[float]`, *optional*, defaults to
    0.0) ‚Äî The percentage of total steps at which the ControlNet starts applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0)
    ‚Äî The percentage of total steps at which the ControlNet stops applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end` (`Callable`, *optional*) ‚Äî A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) ‚Äî The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw)
    content.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) ‚Äî When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è Don‚Äôt enable attention slicing if you‚Äôre already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won‚Äôt need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*) ‚Äî Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_textual_inversion`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike` or `List[str or os.PathLike]`
    or `Dict` or `List[Dict]`) ‚Äî Can be either one of the following or a list of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`)
    of a pretrained model hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_text_inversion_directory/`) containing
    the textual inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *file* (for example `./my_text_inversions.pt`) containing textual
    inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `List[str]`, *optional*) ‚Äî Override the token to use for
    the textual inversion weights. If `pretrained_model_name_or_path` is a list, then
    `token` must also be a list of equal length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel),
    *optional*) ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).
    If not specified, function will take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer),
    *optional*) ‚Äî A `CLIPTokenizer` to tokenize text. If not specified, function will
    take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_name` (`str`, *optional*) ‚Äî Name of a custom weight file. This should
    be used when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in ü§ó Diffusers format, but was saved under
    a specific weight name such as `text_inv.bin`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in the Automatic1111 format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) ‚Äî Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) ‚Äî A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) ‚Äî Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model won‚Äôt be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or *bool*, *optional*) ‚Äî The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) ‚Äî The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) ‚Äî The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mirror` (`str`, *optional*) ‚Äî Mirror source to resolve accessibility issues
    if you‚Äôre downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load Textual Inversion embeddings into the text encoder of [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (both ü§ó Diffusers and Automatic1111 formats are supported).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a Textual Inversion embedding vector in ü§ó Diffusers format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To load a Textual Inversion embedding vector in Automatic1111 format, make sure
    to download the vector first (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857))
    and then load the vector
  prefs: []
  type: TYPE_NORMAL
- en: 'locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L838)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L262)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L815)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`s1` (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s2` (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b1` (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b2` (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L253)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L303)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî prompt to be encoded device ‚Äî
    (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) ‚Äî number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) ‚Äî whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) ‚Äî A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_guidance_scale_embedding`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet.py#L843)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`timesteps` (`torch.Tensor`) ‚Äî generate embedding vectors at these timesteps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_dim` (`int`, *optional*, defaults to 512) ‚Äî dimension of the embeddings
    to generate dtype ‚Äî data type of the generated embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding vectors with shape `(len(timesteps), embedding_dim)`
  prefs: []
  type: TYPE_NORMAL
- en: See [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionControlNetImg2ImgPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.StableDiffusionControlNetImg2ImgPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L132)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî A `UNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    or `List[ControlNetModel]`) ‚Äî Provides additional conditioning to the `unet` during
    the denoising process. If you set multiple ControlNets as a list, the outputs
    from each ControlNet are added together to create one combined additional conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) ‚Äî Classification module that
    estimates whether generated images could be considered offensive or harmful. Please
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model‚Äôs potential harms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    ‚Äî A `CLIPImageProcessor` to extract features from generated images; used as inputs
    to the `safety_checker`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for image-to-image generation using Stable Diffusion with ControlNet
    guidance.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L905)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, ‚Äî `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`): The initial image
    to be used as the starting point for the image generation process. Can also accept
    image latents as `image`, and if passing latents directly they are not encoded
    again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, `List[np.ndarray]`, ‚Äî `List[List[torch.FloatTensor]]`,
    `List[List[np.ndarray]]` or `List[List[PIL.Image.Image]]`): The ControlNet input
    condition to provide guidance to the `unet` for generation. If the type is specified
    as `torch.FloatTensor`, it is passed to ControlNet as is. `PIL.Image.Image` can
    also be accepted as an image. The dimensions of the output image defaults to `image`‚Äôs
    dimensions. If height and/or width are passed, `image` is resized accordingly.
    If multiple ControlNets are specified in `init`, images must be passed as a list
    such that each element of the list can be correctly batched for input to a single
    ControlNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) ‚Äî The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) ‚Äî A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image ‚Äî (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_kwargs` (`dict`, *optional*) ‚Äî A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) ‚Äî The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`. If multiple ControlNets
    are specified in `init`, you can set the corresponding scale as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guess_mode` (`bool`, *optional*, defaults to `False`) ‚Äî The ControlNet encoder
    tries to recognize the content of the input image even if you remove all prompts.
    A `guidance_scale` value between 3.0 and 5.0 is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_start` (`float` or `List[float]`, *optional*, defaults to
    0.0) ‚Äî The percentage of total steps at which the ControlNet starts applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0)
    ‚Äî The percentage of total steps at which the ControlNet stops applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end` (`Callable`, *optional*) ‚Äî A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) ‚Äî The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw)
    content.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) ‚Äî When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è Don‚Äôt enable attention slicing if you‚Äôre already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won‚Äôt need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L230)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*) ‚Äî Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_textual_inversion`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike` or `List[str or os.PathLike]`
    or `Dict` or `List[Dict]`) ‚Äî Can be either one of the following or a list of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`)
    of a pretrained model hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_text_inversion_directory/`) containing
    the textual inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *file* (for example `./my_text_inversions.pt`) containing textual
    inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `List[str]`, *optional*) ‚Äî Override the token to use for
    the textual inversion weights. If `pretrained_model_name_or_path` is a list, then
    `token` must also be a list of equal length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel),
    *optional*) ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).
    If not specified, function will take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer),
    *optional*) ‚Äî A `CLIPTokenizer` to tokenize text. If not specified, function will
    take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_name` (`str`, *optional*) ‚Äî Name of a custom weight file. This should
    be used when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in ü§ó Diffusers format, but was saved under
    a specific weight name such as `text_inv.bin`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in the Automatic1111 format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) ‚Äî Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) ‚Äî A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) ‚Äî Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model won‚Äôt be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or *bool*, *optional*) ‚Äî The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) ‚Äî The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) ‚Äî The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mirror` (`str`, *optional*) ‚Äî Mirror source to resolve accessibility issues
    if you‚Äôre downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load Textual Inversion embeddings into the text encoder of [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (both ü§ó Diffusers and Automatic1111 formats are supported).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a Textual Inversion embedding vector in ü§ó Diffusers format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: To load a Textual Inversion embedding vector in Automatic1111 format, make sure
    to download the vector first (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857))
    and then load the vector
  prefs: []
  type: TYPE_NORMAL
- en: 'locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L878)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L255)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L855)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`s1` (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s2` (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b1` (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b2` (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L246)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py#L296)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî prompt to be encoded device ‚Äî
    (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) ‚Äî number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) ‚Äî whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) ‚Äî A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionControlNetInpaintPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.StableDiffusionControlNetInpaintPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L243)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    ‚Äî A `UNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet` ([ControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.ControlNetModel)
    or `List[ControlNetModel]`) ‚Äî Provides additional conditioning to the `unet` during
    the denoising process. If you set multiple ControlNets as a list, the outputs
    from each ControlNet are added together to create one combined additional conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) ‚Äî Classification module that
    estimates whether generated images could be considered offensive or harmful. Please
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model‚Äôs potential harms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    ‚Äî A `CLIPImageProcessor` to extract features from generated images; used as inputs
    to the `safety_checker`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for image inpainting using Stable Diffusion with ControlNet guidance.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights)
    for saving LoRA weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline can be used with checkpoints that have been specifically fine-tuned
    for inpainting ([runwayml/stable-diffusion-inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting))
    as well as default text-to-image Stable Diffusion checkpoints ([runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)).
    Default text-to-image Stable Diffusion checkpoints might be preferable for ControlNets
    that have been fine-tuned on those, such as [lllyasviel/control_v11p_sd15_inpaint](https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts to guide
    image generation. If not defined, you need to pass `prompt_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    ‚Äî `List[PIL.Image.Image]`, or `List[np.ndarray]`): `Image`, NumPy array or tensor
    representing an image batch to be used as the starting point. For both NumPy array
    and PyTorch tensor, the expected value range is between `[0, 1]`. If it‚Äôs a tensor
    or a list or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`.
    If it is a NumPy array or a list of arrays, the expected shape should be `(B,
    H, W, C)` or `(H, W, C)`. It can also accept image latents as `image`, but if
    passing latents directly it is not encoded again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    ‚Äî `List[PIL.Image.Image]`, or `List[np.ndarray]`): `Image`, NumPy array or tensor
    representing an image batch to mask `image`. White pixels in the mask are repainted
    while black pixels are preserved. If `mask_image` is a PIL image, it is converted
    to a single channel (luminance) before use. If it‚Äôs a NumPy array or PyTorch tensor,
    it should contain one color channel (L) instead of 3, so the expected shape for
    PyTorch tensor would be `(B, 1, H, W)`, `(B, H, W)`, `(1, H, W)`, `(H, W)`. And
    for NumPy array, it would be for `(B, H, W, 1)`, `(B, H, W)`, `(H, W, 1)`, or
    `(H, W)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_image` (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, ‚Äî `List[List[torch.FloatTensor]]`, or `List[List[PIL.Image.Image]]`):
    The ControlNet input condition to provide guidance to the `unet` for generation.
    If the type is specified as `torch.FloatTensor`, it is passed to ControlNet as
    is. `PIL.Image.Image` can also be accepted as an image. The dimensions of the
    output image defaults to `image`‚Äôs dimensions. If height and/or width are passed,
    `image` is resized accordingly. If multiple ControlNets are specified in `init`,
    images must be passed as a list such that each element of the list can be correctly
    batched for input to a single ControlNet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The height in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`)
    ‚Äî The width in pixels of the generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_mask_crop` (`int`, *optional*, defaults to `None`) ‚Äî The size of margin
    in the crop to be applied to the image and masking. If `None`, no crop is applied
    to image and mask_image. If `padding_mask_crop` is not `None`, it will first find
    a rectangular region with the same aspect ration of the image and contains all
    masked area, and then expand that area based on `padding_mask_crop`. The image
    and mask_image will then be cropped based on the expanded area before resizing
    to the original image size for inpainting. This is useful when the masked area
    is small while the image is large and contain information inreleant for inpainging,
    such as background.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strength` (`float`, *optional*, defaults to 1.0) ‚Äî Indicates extent to transform
    the reference `image`. Must be between 0 and 1\. `image` is used as a starting
    point and more noise is added the higher the `strength`. The number of denoising
    steps depends on the amount of noise initially added. When `strength` is 1, added
    noise is maximum and the denoising process runs for the full number of iterations
    specified in `num_inference_steps`. A value of 1 essentially ignores `image`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) ‚Äî The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    to guide what to not include in image generation. If not defined, you need to
    pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale
    < 1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) ‚Äî The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) ‚Äî Corresponds to parameter eta
    (Œ∑) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies to the
    [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    and is ignored in other schedulers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) ‚Äî A
    [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    tensor is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs (prompt weighting). If not provided, text
    embeddings are generated from the `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs (prompt weighting).
    If not provided, `negative_prompt_embeds` are generated from the `negative_prompt`
    input argument. ip_adapter_image ‚Äî (`PipelineImageInput`, *optional*): Optional
    image input to work with IP Adapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) ‚Äî The output format
    of the generated image. Choose between `PIL.Image` or `np.array`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_kwargs` (`dict`, *optional*) ‚Äî A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined in [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 0.5) ‚Äî The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`. If multiple ControlNets
    are specified in `init`, you can set the corresponding scale as a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guess_mode` (`bool`, *optional*, defaults to `False`) ‚Äî The ControlNet encoder
    tries to recognize the content of the input image even if you remove all prompts.
    A `guidance_scale` value between 3.0 and 5.0 is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_start` (`float` or `List[float]`, *optional*, defaults to
    0.0) ‚Äî The percentage of total steps at which the ControlNet starts applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`control_guidance_end` (`float` or `List[float]`, *optional*, defaults to 1.0)
    ‚Äî The percentage of total steps at which the ControlNet stops applying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end` (`Callable`, *optional*) ‚Äî A function that calls at
    the end of each denoising steps during the inference. The function is called with
    the following arguments: `callback_on_step_end(self: DiffusionPipeline, step:
    int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a
    list of all tensors as specified by `callback_on_step_end_tensor_inputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_on_step_end_tensor_inputs` (`List`, *optional*) ‚Äî The list of tensor
    inputs for the `callback_on_step_end` function. The tensors specified in the list
    will be passed as `callback_kwargs` argument. You will only be able to include
    variables listed in the `._callback_tensor_inputs` attribute of your pipeine class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw)
    content.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) ‚Äî When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è Don‚Äôt enable attention slicing if you‚Äôre already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won‚Äôt need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_attention_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L355)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L363)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*) ‚Äî Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ö†Ô∏è When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_xformers_memory_efficient_attention`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_textual_inversion`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike` or `List[str or os.PathLike]`
    or `Dict` or `List[Dict]`) ‚Äî Can be either one of the following or a list of them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`)
    of a pretrained model hosted on the Hub.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* (for example `./my_text_inversion_directory/`) containing
    the textual inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *file* (for example `./my_text_inversions.pt`) containing textual
    inversion weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A [torch state dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `List[str]`, *optional*) ‚Äî Override the token to use for
    the textual inversion weights. If `pretrained_model_name_or_path` is a list, then
    `token` must also be a list of equal length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel),
    *optional*) ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).
    If not specified, function will take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer),
    *optional*) ‚Äî A `CLIPTokenizer` to tokenize text. If not specified, function will
    take self.tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_name` (`str`, *optional*) ‚Äî Name of a custom weight file. This should
    be used when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in ü§ó Diffusers format, but was saved under
    a specific weight name such as `text_inv.bin`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The saved textual inversion file is in the Automatic1111 format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) ‚Äî Path to a directory where
    a downloaded pretrained model configuration is cached if the standard cache is
    not used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to resume downloading the model weights and configuration files. If set to `False`,
    any incompletely downloaded files are deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) ‚Äî A dictionary of proxy servers to
    use by protocol or endpoint, for example, `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only` (`bool`, *optional*, defaults to `False`) ‚Äî Whether to only
    load local model weights and configuration files or not. If set to `True`, the
    model won‚Äôt be downloaded from the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or *bool*, *optional*) ‚Äî The token to use as HTTP bearer authorization
    for remote files. If `True`, the token generated from `diffusers-cli login` (stored
    in `~/.huggingface`) is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) ‚Äî The specific model version
    to use. It can be a branch name, a tag name, a commit id, or any identifier allowed
    by Git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) ‚Äî The subfolder location
    of a model file within a larger model repository on the Hub or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mirror` (`str`, *optional*) ‚Äî Mirror source to resolve accessibility issues
    if you‚Äôre downloading a model in China. We do not guarantee the timeliness or
    safety of the source, and you should refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load Textual Inversion embeddings into the text encoder of [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    (both ü§ó Diffusers and Automatic1111 formats are supported).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load a Textual Inversion embedding vector in ü§ó Diffusers format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: To load a Textual Inversion embedding vector in Automatic1111 format, make sure
    to download the vector first (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857))
    and then load the vector
  prefs: []
  type: TYPE_NORMAL
- en: 'locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1088)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Disables the FreeU mechanism if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L380)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_freeu`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L1065)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`s1` (`float`) ‚Äî Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s2` (`float`) ‚Äî Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate ‚Äúoversmoothing effect‚Äù in the enhanced
    denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b1` (`float`) ‚Äî Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b2` (`float`) ‚Äî Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L371)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py#L421)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) ‚Äî prompt to be encoded device ‚Äî
    (`torch.device`): torch device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) ‚Äî number of images that should be generated
    per prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) ‚Äî whether to use classifier free guidance
    or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) ‚Äî The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) ‚Äî Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) ‚Äî A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) ‚Äî Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: StableDiffusionPipelineOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`List[PIL.Image.Image]` or `np.ndarray`) ‚Äî List of denoised PIL images
    of length `batch_size` or NumPy array of shape `(batch_size, height, width, num_channels)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nsfw_content_detected` (`List[bool]`) ‚Äî List indicating whether the corresponding
    generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw) content or `None` if safety
    checking could not be performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for Stable Diffusion pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: FlaxStableDiffusionControlNetPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.FlaxStableDiffusionControlNetPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL))
    ‚Äî Variational Auto-Encoder (VAE) model to encode and decode images to and from
    latent representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel))
    ‚Äî Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer))
    ‚Äî A `CLIPTokenizer` to tokenize text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel))
    ‚Äî A `FlaxUNet2DConditionModel` to denoise the encoded image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet` ([FlaxControlNetModel](/docs/diffusers/v0.26.3/en/api/models/controlnet#diffusers.FlaxControlNetModel)
    ‚Äî Provides additional conditioning to the `unet` during the denoising process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    ‚Äî A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of `FlaxDDIMScheduler`, `FlaxLMSDiscreteScheduler`, `FlaxPNDMScheduler`,
    or `FlaxDPMSolverMultistepScheduler`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety_checker` (`FlaxStableDiffusionSafetyChecker`) ‚Äî Classification module
    that estimates whether generated images could be considered offensive or harmful.
    Please refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for more details about a model‚Äôs potential harms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor))
    ‚Äî A `CLIPImageProcessor` to extract features from generated images; used as inputs
    to the `safety_checker`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax-based pipeline for text-to-image generation using Stable Diffusion with
    ControlNet Guidance.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline).
    Check the superclass documentation for the generic methods implemented for all
    pipelines (downloading, saving, running on a particular device, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py#L348)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_ids` (`jnp.ndarray`) ‚Äî The prompt or prompts to guide the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`jnp.ndarray`) ‚Äî Array representing the ControlNet input condition
    to provide guidance to the `unet` for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params` (`Dict` or `FrozenDict`) ‚Äî Dictionary containing the model parameters/weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prng_seed` (`jax.Array`) ‚Äî Array containing random number generator key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) ‚Äî The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) ‚Äî A higher guidance
    scale value encourages the model to generate images closely linked to the text
    `prompt` at the expense of lower image quality. Guidance scale is enabled when
    `guidance_scale > 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`jnp.ndarray`, *optional*) ‚Äî Pre-generated noisy latents sampled
    from a Gaussian distribution, to be used as inputs for image generation. Can be
    used to tweak the same generation with different prompts. If not provided, a latents
    array is generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`controlnet_conditioning_scale` (`float` or `jnp.ndarray`, *optional*, defaults
    to 1.0) ‚Äî The outputs of the ControlNet are multiplied by `controlnet_conditioning_scale`
    before they are added to the residual in the original `unet`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    return a [FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jit` (`bool`, defaults to `False`) ‚Äî Whether to run `pmap` versions of the
    generation and safety scoring functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This argument exists because `__call__` is not yet end-to-end pmap-able. It
    will be removed in a future release.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If `return_dict` is `True`, [FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)
    is returned, otherwise a `tuple` is returned where the first element is a list
    with the generated images and the second element is a list of `bool`s indicating
    whether the corresponding generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw)
    content.
  prefs: []
  type: TYPE_NORMAL
- en: The call function to the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: FlaxStableDiffusionControlNetPipelineOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`np.ndarray`) ‚Äî Denoised images of array shape of `(batch_size, height,
    width, num_channels)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nsfw_content_detected` (`List[bool]`) ‚Äî List indicating whether the corresponding
    generated image contains ‚Äúnot-safe-for-work‚Äù (nsfw) content or `None` if safety
    checking could not be performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output class for Flax-based Stable Diffusion pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: ‚ÄúReturns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
