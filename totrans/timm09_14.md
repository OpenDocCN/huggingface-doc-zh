# 学习率调度器

> 原文：[https://huggingface.co/docs/timm/reference/schedulers](https://huggingface.co/docs/timm/reference/schedulers)

此页面包含了`timm`中包含的学习率调度器的API参考文档。

## 调度器

### 工厂函数

#### `timm.scheduler.create_scheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/scheduler_factory.py#L48)

```py
( args optimizer: Optimizer updates_per_epoch: int = 0 )
```

`timm.scheduler.create_scheduler_v2`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/scheduler_factory.py#L60)

```py
( optimizer: Optimizer sched: str = 'cosine' num_epochs: int = 300 decay_epochs: int = 90 decay_milestones: typing.List[int] = (90, 180, 270) cooldown_epochs: int = 0 patience_epochs: int = 10 decay_rate: float = 0.1 min_lr: float = 0 warmup_lr: float = 1e-05 warmup_epochs: int = 0 warmup_prefix: bool = False noise: typing.Union[float, typing.List[float]] = None noise_pct: float = 0.67 noise_std: float = 1.0 noise_seed: int = 42 cycle_mul: float = 1.0 cycle_decay: float = 0.1 cycle_limit: int = 1 k_decay: float = 1.0 plateau_mode: str = 'max' step_on_epochs: bool = True updates_per_epoch: int = 0 )
```

### 调度器类

### `class timm.scheduler.CosineLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/cosine_lr.py#L18)

```py
( optimizer: Optimizer t_initial: int lr_min: float = 0.0 cycle_mul: float = 1.0 cycle_decay: float = 1.0 cycle_limit: int = 1 warmup_t = 0 warmup_lr_init = 0 warmup_prefix = False t_in_epochs = True noise_range_t = None noise_pct = 0.67 noise_std = 1.0 noise_seed = 42 k_decay = 1.0 initialize = True )
```

余弦衰减与重启。这在论文中有描述[https://arxiv.org/abs/1608.03983](https://arxiv.org/abs/1608.03983)。

灵感来自于[https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py](https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py)

基于`k-decay: 一种新的学习率调度方法`的k-decay选项 - [https://arxiv.org/abs/2004.05909](https://arxiv.org/abs/2004.05909)

### `class timm.scheduler.MultiStepLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/multistep_lr.py#L10)

```py
( optimizer: Optimizer decay_t: typing.List[int] decay_rate: float = 1.0 warmup_t = 0 warmup_lr_init = 0 warmup_prefix = True t_in_epochs = True noise_range_t = None noise_pct = 0.67 noise_std = 1.0 noise_seed = 42 initialize = True )
```

### `class timm.scheduler.PlateauLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/plateau_lr.py#L12)

```py
( optimizer decay_rate = 0.1 patience_t = 10 verbose = True threshold = 0.0001 cooldown_t = 0 warmup_t = 0 warmup_lr_init = 0 lr_min = 0 mode = 'max' noise_range_t = None noise_type = 'normal' noise_pct = 0.67 noise_std = 1.0 noise_seed = None initialize = True )
```

每当验证损失达到平稳状态时，将学习率按比例衰减。

### `class timm.scheduler.PolyLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/poly_lr.py#L18)

```py
( optimizer: Optimizer t_initial: int power: float = 0.5 lr_min: float = 0.0 cycle_mul: float = 1.0 cycle_decay: float = 1.0 cycle_limit: int = 1 warmup_t = 0 warmup_lr_init = 0 warmup_prefix = False t_in_epochs = True noise_range_t = None noise_pct = 0.67 noise_std = 1.0 noise_seed = 42 k_decay = 1.0 initialize = True )
```

带有预热、噪声和k-decay的多项式LR调度器

基于`k-decay: 一种新的学习率调度方法`的k-decay选项 - [https://arxiv.org/abs/2004.05909](https://arxiv.org/abs/2004.05909)

### `class timm.scheduler.StepLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/step_lr.py#L13)

```py
( optimizer: Optimizer decay_t: float decay_rate: float = 1.0 warmup_t = 0 warmup_lr_init = 0 warmup_prefix = True t_in_epochs = True noise_range_t = None noise_pct = 0.67 noise_std = 1.0 noise_seed = 42 initialize = True )
```

### `class timm.scheduler.TanhLRScheduler`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/scheduler/tanh_lr.py#L18)

```py
( optimizer: Optimizer t_initial: int lb: float = -7.0 ub: float = 3.0 lr_min: float = 0.0 cycle_mul: float = 1.0 cycle_decay: float = 1.0 cycle_limit: int = 1 warmup_t = 0 warmup_lr_init = 0 warmup_prefix = False t_in_epochs = True noise_range_t = None noise_pct = 0.67 noise_std = 1.0 noise_seed = 42 initialize = True )
```

带有重启的双曲正切衰减。这在论文中有描述[https://arxiv.org/abs/1806.01593](https://arxiv.org/abs/1806.01593)
