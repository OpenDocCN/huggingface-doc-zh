- en: Kandinsky 2.1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky 2.1 is created by [Arseniy Shakhmatov](https://github.com/cene555),
    [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega),
    [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om),
    [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).
  prefs: []
  type: TYPE_NORMAL
- en: 'The description from itâ€™s GitHub page is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion,
    while introducing some new ideas. As text and image encoder it uses CLIP model
    and diffusion image prior (mapping) between latent spaces of CLIP modalities.
    This approach increases the visual performance of the model and unveils new horizons
    in blending images and text-guided image manipulation.*'
  prefs: []
  type: TYPE_NORMAL
- en: The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community)
    organization on the Hub for the official model checkpoints for tasks like text-to-image,
    image-to-image, and inpainting.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: KandinskyPriorPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyPriorPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination with
    `prior` to generate image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for generating image prior for Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L397)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pt"`) â€” The output format of
    the generate image. Choose between: `"np"` (`np.array`) or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`KandinskyPriorPipelineOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `interpolate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L172)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`)
    â€” list of prompts and images to guide the image generation. weights â€” (`List[float]`):
    list of weights for each condition in `images_and_prompts`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prior_prompt` (`str`, *optional*) â€” The prompt not to guide the prior
    diffusion process. Ignored when not using guidance (i.e., ignored if `guidance_scale`
    is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt not to guide
    the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale`
    is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`KandinskyPriorPipelineOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when using the prior pipeline for interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: KandinskyPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L76)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L231)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: KandinskyCombinedPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyCombinedPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined Pipeline for text-to-image generation using Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L214)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_sequential_cpu_offload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L195)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Offloads all models (`unet`, `text_encoder`, `vae`, and `safety checker` state
    dicts) to CPU using ðŸ¤— Accelerate, significantly reducing memory usage. Models
    are moved to a `torch.device('meta')` and loaded on a GPU only when their specific
    submoduleâ€™s `forward` method is called. Offloading happens on a submodule basis.
    Memory savings are higher than using `enable_model_cpu_offload`, but performance
    is lower.
  prefs: []
  type: TYPE_NORMAL
- en: KandinskyImg2ImgPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyImg2ImgPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L98)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    â€” A scheduler to be used in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ image encoder and decoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for image-to-image generation using Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L293)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strength` (`float`, *optional*, defaults to 0.3) â€” Conceptually, indicates
    how much to transform the reference `image`. Must be between 0 and 1\. `image`
    will be used as a starting point, adding more noise to it the larger the `strength`.
    The number of denoising steps depends on the amount of noise initially added.
    When `strength` is 1, added noise will be maximum and the denoising process will
    run for the full number of iterations specified in `num_inference_steps`. A value
    of 1, therefore, essentially ignores `image`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: KandinskyImg2ImgCombinedPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyImg2ImgCombinedPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L330)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined Pipeline for image-to-image generation using Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L432)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process. Can also
    accept image latents as `image`, if passing latents directly, it will not be encoded
    again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strength` (`float`, *optional*, defaults to 0.3) â€” Conceptually, indicates
    how much to transform the reference `image`. Must be between 0 and 1\. `image`
    will be used as a starting point, adding more noise to it the larger the `strength`.
    The number of denoising steps depends on the amount of noise initially added.
    When `strength` is 1, added noise will be maximum and the denoising process will
    run for the full number of iterations specified in `num_inference_steps`. A value
    of 1, therefore, essentially ignores `image`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_sequential_cpu_offload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L412)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Offloads all models to CPU using accelerate, significantly reducing memory usage.
    When called, unet, text_encoder, vae and safety checker have their state dicts
    saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only
    when their specific submodule has its` forward`method called. Note that offloading
    happens on a submodule basis. Memory savings are higher than with`enable_model_cpu_offload`,
    but performance is lower.
  prefs: []
  type: TYPE_NORMAL
- en: KandinskyInpaintPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyInpaintPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L240)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    â€” A scheduler to be used in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ image encoder and decoder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-guided image inpainting using Kandinsky2.1
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L396)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image` or `np.ndarray`) â€” `Image`,
    or tensor representing an image batch, that will be used as the starting point
    for the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_image` (`PIL.Image.Image`,`torch.FloatTensor` or `np.ndarray`) â€” `Image`,
    or a tensor representing an image batch, to mask `image`. White pixels in the
    mask will be repainted, while black pixels will be preserved. You can pass a pytorch
    tensor as mask only if the image you passed is a pytorch tensor, and it should
    contain one color channel (L) instead of 3, so the expected shape would be either
    `(B, 1, H, W,)`, `(B, H, W)`, `(1, H, W)` or `(H, W)` If image is an PIL image
    or numpy array, mask should also be a either PIL image or numpy array. If it is
    a PIL image, it will be converted to a single channel (luminance) before use.
    If it is a nummpy array, the expected shape is `(H, W)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: KandinskyInpaintCombinedPipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.KandinskyInpaintCombinedPipeline`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L570)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combined Pipeline for generation using Kandinsky
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L672)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process. Can also
    accept image latents as `image`, if passing latents directly, it will not be encoded
    again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_image` (`np.array`) â€” Tensor representing an image batch, to mask `image`.
    White pixels in the mask will be repainted, while black pixels will be preserved.
    If `mask_image` is a PIL image, it will be converted to a single channel (luminance)
    before use. If itâ€™s a tensor, it should contain one color channel (L) instead
    of 3, so the expected shape would be `(B, H, W, 1)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#### `enable_sequential_cpu_offload`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L652)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Offloads all models to CPU using accelerate, significantly reducing memory usage.
    When called, unet, text_encoder, vae and safety checker have their state dicts
    saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only
    when their specific submodule has its` forward`method called. Note that offloading
    happens on a submodule basis. Memory savings are higher than with`enable_model_cpu_offload`,
    but performance is lower.
  prefs: []
  type: TYPE_NORMAL
