- en: Kandinsky 2.1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kandinsky 2.1
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/diffusers/api/pipelines/kandinsky](https://huggingface.co/docs/diffusers/api/pipelines/kandinsky)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Kandinsky 2.1 is created by [Arseniy Shakhmatov](https://github.com/cene555),
    [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega),
    [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om),
    [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinsky 2.1ç”±[Arseniy Shakhmatov](https://github.com/cene555)ã€[Anton Razzhigaev](https://github.com/razzant)ã€[Aleksandr
    Nikolich](https://github.com/AlexWortega)ã€[Vladimir Arkhipkin](https://github.com/oriBetelgeuse)ã€[Igor
    Pavlov](https://github.com/boomb0om)ã€[Andrey Kuznetsov](https://github.com/kuznetsoffandrey)å’Œ[Denis
    Dimitrov](https://github.com/denndimitrov)åˆ›å»ºã€‚
- en: 'The description from itâ€™s GitHub page is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨GitHubé¡µé¢ä¸Šçš„æè¿°æ˜¯ï¼š
- en: '*Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion,
    while introducing some new ideas. As text and image encoder it uses CLIP model
    and diffusion image prior (mapping) between latent spaces of CLIP modalities.
    This approach increases the visual performance of the model and unveils new horizons
    in blending images and text-guided image manipulation.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kandinsky 2.1ç»§æ‰¿äº†Dall-E 2å’Œæ½œåœ¨æ‰©æ•£çš„æœ€ä½³å®è·µï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€äº›æ–°çš„æƒ³æ³•ã€‚ä½œä¸ºæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œå®ƒä½¿ç”¨CLIPæ¨¡å‹å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´çš„æ‰©æ•£å›¾åƒå…ˆéªŒï¼ˆæ˜ å°„ï¼‰ã€‚è¿™ç§æ–¹æ³•æé«˜äº†æ¨¡å‹çš„è§†è§‰æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†åœ¨å›¾åƒå’Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œä¸­å¼€å¯æ–°çš„è§†é‡ã€‚*'
- en: The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹ä»£ç åº“å¯åœ¨[ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2)æ‰¾åˆ°ã€‚
- en: Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community)
    organization on the Hub for the official model checkpoints for tasks like text-to-image,
    image-to-image, and inpainting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹Hubä¸Šçš„[Kandinsky Community](https://huggingface.co/kandinsky-community)ç»„ç»‡ï¼Œè·å–å®˜æ–¹æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒå’Œä¿®å¤ç­‰ä»»åŠ¡ã€‚
- en: Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers)
    to learn how to explore the tradeoff between scheduler speed and quality, and
    see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines)
    section to learn how to efficiently load the same components into multiple pipelines.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·ç¡®ä¿æŸ¥çœ‹è°ƒåº¦å™¨çš„[æŒ‡å—](../../using-diffusers/schedulers)ï¼Œäº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æŸ¥çœ‹[è·¨ç®¡é“é‡ç”¨ç»„ä»¶](../../using-diffusers/loading#reuse-components-across-pipelines)éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•æœ‰æ•ˆåœ°å°†ç›¸åŒçš„ç»„ä»¶åŠ è½½åˆ°å¤šä¸ªç®¡é“ä¸­ã€‚
- en: KandinskyPriorPipeline
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyPriorPipeline
- en: '### `class diffusers.KandinskyPriorPipeline`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyPriorPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L128)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L128)'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥ä¸­è¿‘ä¼¼å›¾åƒåµŒå…¥çš„ç»å…¸unCLIPå…ˆéªŒã€‚'
- en: '`image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚'
- en: '`text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚'
- en: '`scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination with
    `prior` to generate image embedding.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (`UnCLIPScheduler`) â€” ä¸`prior`ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒåµŒå…¥ã€‚'
- en: Pipeline for generating image prior for Kandinsky
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”ŸæˆKandinskyå›¾åƒå…ˆéªŒçš„ç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L397)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L397)'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str`æˆ–`List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str`æˆ–`List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º25) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator`æˆ–`List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº
    `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” å¦‚ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
    ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)
    ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pt"`) â€” The output format of
    the generate image. Choose between: `"np"` (`np.array`) or `"pt"` (`torch.Tensor`).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pt"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©ä¹‹é—´: `"np"` (`np.array`)
    æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`KandinskyPriorPipelineOutput` or `tuple`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`KandinskyPriorPipelineOutput` æˆ– `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“ä»¥è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `interpolate`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `æ’å€¼`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L172)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py#L172)'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`)
    â€” list of prompts and images to guide the image generation. weights â€” (`List[float]`):
    list of weights for each condition in `images_and_prompts`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images_and_prompts` (`List[Union[str, PIL.Image.Image, torch.FloatTensor]]`)
    â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºå’Œå›¾åƒåˆ—è¡¨ã€‚æƒé‡ â€” (`List[float]`): `images_and_prompts` ä¸­æ¯ä¸ªæ¡ä»¶çš„æƒé‡åˆ—è¡¨'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 25) â€” The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 25) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch
    ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html) ä»¥ä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº
    `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`negative_prior_prompt` (`str`, *optional*) â€” The prompt not to guide the prior
    diffusion process. Ignored when not using guidance (i.e., ignored if `guidance_scale`
    is less than `1`).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prior_prompt` (`str`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å…ˆå‰æ‰©æ•£è¿‡ç¨‹çš„æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ `guidance_scale`
    å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt not to guide
    the image generation. Ignored when not using guidance (i.e., ignored if `guidance_scale`
    is less than `1`).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” å¦‚ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
    ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)
    ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: Returns
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`KandinskyPriorPipelineOutput` or `tuple`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`KandinskyPriorPipelineOutput` æˆ– `tuple`'
- en: Function invoked when using the prior pipeline for interpolation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…ˆå‰ç®¡é“è¿›è¡Œæ’å€¼æ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: KandinskyPipeline
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyPipeline
- en: '### `class diffusers.KandinskyPipeline`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L76)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L76)'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸ `unet` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶ U-Net æ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” ç”¨äºä»æ½œåœ¨ç©ºé—´ç”Ÿæˆå›¾åƒçš„ MoVQ è§£ç å™¨ã€‚'
- en: Pipeline for text-to-image generation using Kandinsky
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºä½¿ç”¨ Kandinsky è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L231)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py#L231)'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€”
    è´Ÿæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨ [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)
    ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`guidance_scale` å®šä¹‰ä¸º [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)
    ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch
    ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œåœ¨ç©ºé—´ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº
    `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œåœ¨ç©ºé—´å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"`
    (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸€æ­¥éƒ½ä¼šè°ƒç”¨å›è°ƒå‡½æ•°ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: KandinskyCombinedPipeline
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyCombinedPipeline
- en: '### `class diffusers.KandinskyCombinedPipeline`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyCombinedPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L113)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L113)'
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸ `unet` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” ç”¨äºé™å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶ U-Net æ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ è§£ç å™¨ï¼Œç”¨äºä»æ½œå˜é‡ç”Ÿæˆå›¾åƒã€‚'
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” ç”¨äºè¿‘ä¼¼ä»æ–‡æœ¬åµŒå…¥åˆ°å›¾åƒåµŒå…¥çš„ç»å…¸ unCLIP å…ˆéªŒã€‚'
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚'
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer` (`CLIPTokenizer`) â€” ç±»çš„åˆ†è¯å™¨ [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)ã€‚'
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºä¸ `prior` ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚'
- en: Combined Pipeline for text-to-image generation using Kandinsky
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Kandinsky è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L214)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L214)'
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚å¦‚æœä¸ä½¿ç”¨å¼•å¯¼ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚
    `guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2
    çš„ `w`ã€‚ é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥ç‰ºç‰²å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚
    `guidance_scale` å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2
    çš„ `w`ã€‚ é€šè¿‡è®¾ç½® `guidance_scale > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥ç‰ºç‰²å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`ï¼ˆ`torch.Generator`æˆ–`List[torch.Generator]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„ç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"pil"`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š"pil"ï¼ˆ`PIL.Image.Image`ï¼‰ã€"np"ï¼ˆ`np.array`ï¼‰æˆ–"pt"ï¼ˆ`torch.Tensor`ï¼‰ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯`callback_steps`æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸ªæ­¥éª¤æ—¶è°ƒç”¨å›è°ƒã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)æˆ–`tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `enable_sequential_cpu_offload`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_sequential_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L195)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L195)'
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Offloads all models (`unet`, `text_encoder`, `vae`, and `safety checker` state
    dicts) to CPU using ğŸ¤— Accelerate, significantly reducing memory usage. Models
    are moved to a `torch.device('meta')` and loaded on a GPU only when their specific
    submoduleâ€™s `forward` method is called. Offloading happens on a submodule basis.
    Memory savings are higher than using `enable_model_cpu_offload`, but performance
    is lower.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateå°†æ‰€æœ‰æ¨¡å‹ï¼ˆ`unet`ã€`text_encoder`ã€`vae`å’Œ`safety checker`çŠ¶æ€å­—å…¸ï¼‰è½¬ç§»åˆ°CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚æ¨¡å‹ç§»è‡³`torch.device('meta')`ï¼Œä»…åœ¨è°ƒç”¨å…¶ç‰¹å®šå­æ¨¡å—çš„`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ°GPUã€‚å¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äºä½¿ç”¨`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
- en: KandinskyImg2ImgPipeline
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyImg2ImgPipeline
- en: '### `class diffusers.KandinskyImg2ImgPipeline`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyImg2ImgPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L98)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L98)'
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`ï¼ˆ`MultilingualCLIP`ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`XLMRobertaTokenizer`ï¼‰â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    â€” A scheduler to be used in combination with `unet` to generate image latents.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`ï¼ˆ[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼‰â€”
    ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€”
    ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ image encoder and decoder'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq`ï¼ˆ[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)ï¼‰â€”
    MoVQå›¾åƒç¼–ç å™¨å’Œè§£ç å™¨'
- en: Pipeline for image-to-image generation using Kandinsky
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Kandinskyçš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L293)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æºä»£ç >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py#L293)'
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼‰â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œå°†ç”¨ä½œè¿‡ç¨‹çš„èµ·å§‹ç‚¹ã€‚'
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds`ï¼ˆ`torch.FloatTensor`æˆ–`List[torch.FloatTensor]`ï¼‰â€” ç”¨äºæ–‡æœ¬æç¤ºçš„å‰ªè¾‘å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€”
    ç”¨äºè´Ÿæ–‡æœ¬æç¤ºçš„clipå›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1`ï¼‰ï¼Œåˆ™å¿½ç•¥ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„å®½åº¦ï¼ˆåƒç´ ï¼‰ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, é»˜è®¤ä¸º100) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`strength` (`float`, *optional*, defaults to 0.3) â€” Conceptually, indicates
    how much to transform the reference `image`. Must be between 0 and 1\. `image`
    will be used as a starting point, adding more noise to it the larger the `strength`.
    The number of denoising steps depends on the amount of noise initially added.
    When `strength` is 1, added noise will be maximum and the denoising process will
    run for the full number of iterations specified in `num_inference_steps`. A value
    of 1, therefore, essentially ignores `image`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength` (`float`, *optional*, é»˜è®¤ä¸º0.3) â€” åœ¨æ¦‚å¿µä¸Šï¼ŒæŒ‡ç¤ºè¦è½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨0å’Œ1ä¹‹é—´ã€‚`image`å°†è¢«ç”¨ä½œèµ·ç‚¹ï¼Œæ·»åŠ çš„å™ªéŸ³è¶Šå¤šï¼Œ`strength`è¶Šå¤§ã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“
    `strength` ä¸º1 æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³å°†æœ€å¤§åŒ–ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„ `num_inference_steps` çš„å…¨éƒ¨è¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œå€¼ä¸º1 çš„æƒ…å†µä¸‹ï¼ŒåŸºæœ¬ä¸Šå¿½ç•¥äº†
    `image`ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, é»˜è®¤ä¸º4.0) â€” åœ¨[Classifier-Free Diffusion
    Guidance](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„æŒ‡å¯¼æ¯”ä¾‹ã€‚`guidance_scale` åœ¨[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)çš„æ–¹ç¨‹å¼2ä¸­å®šä¹‰ä¸º`w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale
    > 1` æ¥å¯ç”¨æŒ‡å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *optional*) â€” ä¸€ä¸ªæˆ–å¤šä¸ªç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š"pil" (`PIL.Image.Image`)ã€"np"
    (`np.array`) æˆ– "pt" (`torch.Tensor`)ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, é»˜è®¤ä¸º1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒå‡½æ•°ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: KandinskyImg2ImgCombinedPipeline
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyImg2ImgCombinedPipeline
- en: '### `class diffusers.KandinskyImg2ImgCombinedPipeline`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyImg2ImgCombinedPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L330)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L330)'
- en: '[PRE15]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ä¸ `unet` ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºç”Ÿæˆå›¾åƒæ½œåœ¨ç©ºé—´ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” ç”¨äºä»æ½œåœ¨ç©ºé—´ç”Ÿæˆå›¾åƒçš„MoVQè§£ç å™¨ã€‚'
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_prior`ï¼ˆ[PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)ï¼‰â€”ç”¨äºè¿‘ä¼¼ä»æ–‡æœ¬åµŒå…¥åˆ°å›¾åƒåµŒå…¥çš„è§„èŒƒåŒ–unCLIPå…ˆéªŒã€‚'
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_image_encoder`ï¼ˆ`CLIPVisionModelWithProjection`ï¼‰â€”å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚'
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder`ï¼ˆ`CLIPTextModelWithProjection`ï¼‰â€”å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer`ï¼ˆ`CLIPTokenizer`ï¼‰â€”ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚'
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler`ï¼ˆ`UnCLIPScheduler`ï¼‰â€”ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚'
- en: Combined Pipeline for image-to-image generation using Kandinsky
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Kandinskyè¿›è¡Œå›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„ç»„åˆç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè¿è¡Œåœ¨ç‰¹å®šè®¾å¤‡ä¸Šç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L432)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L432)'
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€”ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process. Can also
    accept image latents as `image`, if passing latents directly, it will not be encoded
    again.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`ï¼Œ`np.ndarray`ï¼Œ`List[torch.FloatTensor]`ï¼Œ`List[PIL.Image.Image]`æˆ–`List[np.ndarray]`ï¼‰â€”å°†ç”¨ä½œè¿‡ç¨‹èµ·ç‚¹çš„`Image`æˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ã€‚å¦‚æœç›´æ¥ä¼ é€’æ½œåœ¨å€¼ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆå³ï¼Œå¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€”æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º100ï¼‰â€”å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512ï¼‰â€”ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512ï¼‰â€”ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`strength` (`float`, *optional*, defaults to 0.3) â€” Conceptually, indicates
    how much to transform the reference `image`. Must be between 0 and 1\. `image`
    will be used as a starting point, adding more noise to it the larger the `strength`.
    The number of denoising steps depends on the amount of noise initially added.
    When `strength` is 1, added noise will be maximum and the denoising process will
    run for the full number of iterations specified in `num_inference_steps`. A value
    of 1, therefore, essentially ignores `image`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strength`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.3ï¼‰â€”åœ¨æ¦‚å¿µä¸Šï¼ŒæŒ‡ç¤ºè¦è½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»ä»‹äº0å’Œ1ä¹‹é—´ã€‚`image`å°†è¢«ç”¨ä½œèµ·ç‚¹ï¼Œæ·»åŠ æ›´å¤šçš„å™ªéŸ³ï¼Œ`strength`è¶Šå¤§ã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º1æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³å°†æ˜¯æœ€å¤§çš„ï¼Œå¹¶ä¸”å»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å®Œæ•´è¿­ä»£æ¬¡æ•°ã€‚å› æ­¤ï¼Œå€¼ä¸º1åŸºæœ¬ä¸Šå¿½ç•¥äº†`image`ã€‚'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4.0ï¼‰â€”åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`è¢«å®šä¹‰ä¸º[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º100ï¼‰â€”å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4.0ï¼‰â€”åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`è¢«å®šä¹‰ä¸º[Imagen
    Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼2çš„`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`ï¼ˆ`torch.Generator`æˆ–`List[torch.Generator]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torchç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ç”¨äºä½¿ç”Ÿæˆå…·æœ‰ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¢„ç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"pil"`ï¼‰â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`"pil"`ï¼ˆ`PIL.Image.Image`ï¼‰ï¼Œ`"np"`ï¼ˆ`np.array`ï¼‰æˆ–`"pt"`ï¼ˆ`torch.Tensor`ï¼‰ä¹‹é—´ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯`callback_steps`æ­¥åœ¨æ¨æ–­æœŸé—´è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1ï¼‰â€” è°ƒç”¨`callback`å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)æˆ–`tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE17]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#### `enable_sequential_cpu_offload`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_sequential_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L412)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L412)'
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Offloads all models to CPU using accelerate, significantly reducing memory usage.
    When called, unet, text_encoder, vae and safety checker have their state dicts
    saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only
    when their specific submodule has its` forward`method called. Note that offloading
    happens on a submodule basis. Memory savings are higher than with`enable_model_cpu_offload`,
    but performance is lower.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠ é€Ÿå°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ°CPUï¼Œæ˜¾ç€å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è°ƒç”¨æ—¶ï¼Œunetã€text_encoderã€vaeå’Œsafety checkerçš„çŠ¶æ€å­—å…¸å°†ä¿å­˜åˆ°CPUï¼Œç„¶åç§»åŠ¨åˆ°`torch.device('meta')ï¼Œä»…åœ¨å®ƒä»¬çš„ç‰¹å®šå­æ¨¡å—è°ƒç”¨å…¶`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ°GPUã€‚è¯·æ³¨æ„ï¼Œå¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚å†…å­˜èŠ‚çœé«˜äº`enable_model_cpu_offload`ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
- en: KandinskyInpaintPipeline
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyInpaintPipeline
- en: '### `class diffusers.KandinskyInpaintPipeline`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyInpaintPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L240)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L240)'
- en: '[PRE19]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`ï¼ˆ`MultilingualCLIP`ï¼‰â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`XLMRobertaTokenizer`ï¼‰â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler))
    â€” A scheduler to be used in combination with `unet` to generate image latents.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler`ï¼ˆ[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼‰â€”
    ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet`ï¼ˆ[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)ï¼‰â€”
    ç”¨äºå»å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ image encoder and decoder'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq`ï¼ˆ[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)ï¼‰â€”
    MoVQå›¾åƒç¼–ç å™¨å’Œè§£ç å™¨'
- en: Pipeline for text-guided image inpainting using Kandinsky2.1
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Kandinsky2.1è¿›è¡Œæ–‡æœ¬å¼•å¯¼å›¾åƒä¿®å¤çš„ç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè¿è¡Œåœ¨ç‰¹å®šè®¾å¤‡ä¸Šç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L396)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py#L396)'
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼‰â€” ç”¨äºå¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image` or `np.ndarray`) â€” `Image`,
    or tensor representing an image batch, that will be used as the starting point
    for the process.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`ï¼ˆ`torch.FloatTensor`ï¼Œ`PIL.Image.Image`æˆ–`np.ndarray`ï¼‰â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œå°†ç”¨ä½œè¿‡ç¨‹çš„èµ·å§‹ç‚¹ã€‚'
- en: '`mask_image` (`PIL.Image.Image`,`torch.FloatTensor` or `np.ndarray`) â€” `Image`,
    or a tensor representing an image batch, to mask `image`. White pixels in the
    mask will be repainted, while black pixels will be preserved. You can pass a pytorch
    tensor as mask only if the image you passed is a pytorch tensor, and it should
    contain one color channel (L) instead of 3, so the expected shape would be either
    `(B, 1, H, W,)`, `(B, H, W)`, `(1, H, W)` or `(H, W)` If image is an PIL image
    or numpy array, mask should also be a either PIL image or numpy array. If it is
    a PIL image, it will be converted to a single channel (luminance) before use.
    If it is a nummpy array, the expected shape is `(H, W)`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_image` (`PIL.Image.Image`,`torch.FloatTensor` æˆ– `np.ndarray`) â€” `Image`ï¼Œæˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œç”¨äºé®ç½©
    `image`ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ å°†è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ å°†è¢«ä¿ç•™ã€‚åªæœ‰å½“ä¼ é€’çš„å›¾åƒæ˜¯ pytorch å¼ é‡æ—¶ï¼Œæ‰èƒ½å°† pytorch å¼ é‡ä½œä¸ºé®ç½©ä¼ é€’ï¼Œå®ƒåº”è¯¥åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯
    3ï¼Œå› æ­¤é¢„æœŸçš„å½¢çŠ¶åº”ä¸º `(B, 1, H, W,)`ã€`(B, H, W)`ã€`(1, H, W)` æˆ– `(H, W)`ã€‚å¦‚æœå›¾åƒæ˜¯ PIL å›¾åƒæˆ– numpy
    æ•°ç»„ï¼Œåˆ™é®ç½©ä¹Ÿåº”è¯¥æ˜¯ PIL å›¾åƒæˆ– numpy æ•°ç»„ã€‚å¦‚æœæ˜¯ PIL å›¾åƒï¼Œå®ƒå°†åœ¨ä½¿ç”¨ä¹‹å‰è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œåˆ™é¢„æœŸå½¢çŠ¶ä¸º `(H,
    W)`ã€‚'
- en: '`image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`) â€” The clip
    image embeddings for text prompt, that will be used to condition the image generation.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€” ç”¨äºæ–‡æœ¬æç¤ºçš„
    clip å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_image_embeds` (`torch.FloatTensor` or `List[torch.FloatTensor]`)
    â€” The clip image embeddings for negative text prompt, will be used to condition
    the image generation.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_image_embeds` (`torch.FloatTensor` æˆ– `List[torch.FloatTensor]`) â€”
    ç”¨äºè´Ÿé¢æ–‡æœ¬æç¤ºçš„ clip å›¾åƒåµŒå…¥ï¼Œå°†ç”¨äºæ¡ä»¶å›¾åƒç”Ÿæˆã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨æ¥å¼•å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å½“ä¸ä½¿ç”¨å¼•å¯¼æ—¶è¢«å¿½ç•¥ï¼ˆå³å¦‚æœ
    `guidance_scale` å°äº `1` åˆ™è¢«å¿½ç•¥ï¼‰ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`
    å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale
    > 1` æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª[torch
    ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html)ï¼Œç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å˜ˆæ‚æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ä»¥ç”¨æ¥ä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨éšæœº
    `generator` é‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹© `"pil"` (`PIL.Image.Image`)ã€`"np"`
    (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯ `callback_steps` æ­¥è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒå‡½æ•°ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE21]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: KandinskyInpaintCombinedPipeline
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KandinskyInpaintCombinedPipeline
- en: '### `class diffusers.KandinskyInpaintCombinedPipeline`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.KandinskyInpaintCombinedPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L570)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L570)'
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_encoder` (`MultilingualCLIP`) â€” Frozen text-encoder.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder` (`MultilingualCLIP`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`tokenizer` (`XLMRobertaTokenizer`) â€” Tokenizer of class'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`XLMRobertaTokenizer`) â€” ç±»çš„åˆ†è¯å™¨'
- en: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” A scheduler to be used
    in combination with `unet` to generate image latents.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` (Union[`DDIMScheduler`,`DDPMScheduler`]) â€” ç”¨äºä¸`unet`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒæ½œåœ¨çš„è°ƒåº¦å™¨ã€‚'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” Conditional U-Net architecture to denoise the image embedding.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    â€” ç”¨äºé™å™ªå›¾åƒåµŒå…¥çš„æ¡ä»¶U-Netæ¶æ„ã€‚'
- en: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” MoVQ Decoder to generate the image from the latents.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movq` ([VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel))
    â€” ç”¨äºä»æ½œåœ¨å›¾åƒç”Ÿæˆå›¾åƒçš„MoVQè§£ç å™¨ã€‚'
- en: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” The canonincal unCLIP prior to approximate the image embedding from the text
    embedding.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_prior` ([PriorTransformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer))
    â€” ç”¨äºä»æ–‡æœ¬åµŒå…¥ä¸­è¿‘ä¼¼ç”Ÿæˆå›¾åƒåµŒå…¥çš„æ ‡å‡†unCLIPå…ˆéªŒã€‚'
- en: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” Frozen image-encoder.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_image_encoder` (`CLIPVisionModelWithProjection`) â€” å†»ç»“çš„å›¾åƒç¼–ç å™¨ã€‚'
- en: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” Frozen text-encoder.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_text_encoder` (`CLIPTextModelWithProjection`) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ã€‚'
- en: '`prior_tokenizer` (`CLIPTokenizer`) â€” Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_tokenizer` (`CLIPTokenizer`) â€” ç±»[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)çš„åˆ†è¯å™¨ã€‚'
- en: '`prior_scheduler` (`UnCLIPScheduler`) â€” A scheduler to be used in combination
    with `prior` to generate image embedding.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_scheduler` (`UnCLIPScheduler`) â€” ç”¨äºä¸`prior`ç»“åˆä½¿ç”¨ä»¥ç”Ÿæˆå›¾åƒåµŒå…¥çš„è°ƒåº¦å™¨ã€‚'
- en: Combined Pipeline for generation using Kandinsky
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Kandinskyè¿›è¡Œç”Ÿæˆçš„ç»„åˆç®¡é“
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚
- en: '#### `__call__`'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L672)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L672)'
- en: '[PRE23]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`prompt` (`str` or `List[str]`) â€” The prompt or prompts to guide the image
    generation.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` æˆ– `List[str]`) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, or `List[np.ndarray]`) â€” `Image`, or tensor representing
    an image batch, that will be used as the starting point for the process. Can also
    accept image latents as `image`, if passing latents directly, it will not be encoded
    again.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`,
    `List[PIL.Image.Image]`, æˆ– `List[np.ndarray]`) â€” ç”¨ä½œè¿‡ç¨‹èµ·ç‚¹çš„`Image`æˆ–è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ã€‚å¦‚æœç›´æ¥ä¼ é€’æ½œåœ¨å›¾åƒï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚'
- en: '`mask_image` (`np.array`) â€” Tensor representing an image batch, to mask `image`.
    White pixels in the mask will be repainted, while black pixels will be preserved.
    If `mask_image` is a PIL image, it will be converted to a single channel (luminance)
    before use. If itâ€™s a tensor, it should contain one color channel (L) instead
    of 3, so the expected shape would be `(B, H, W, 1)`.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_image` (`np.array`) â€” è¡¨ç¤ºå›¾åƒæ‰¹æ¬¡çš„å¼ é‡ï¼Œç”¨äºé®ç½©`image`ã€‚é®ç½©ä¸­çš„ç™½è‰²åƒç´ å°†è¢«é‡æ–°ç»˜åˆ¶ï¼Œè€Œé»‘è‰²åƒç´ å°†è¢«ä¿ç•™ã€‚å¦‚æœ`mask_image`æ˜¯PILå›¾åƒï¼Œåˆ™åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶è½¬æ¢ä¸ºå•é€šé“ï¼ˆäº®åº¦ï¼‰ã€‚å¦‚æœå®ƒæ˜¯å¼ é‡ï¼Œåˆ™åº”è¯¥åŒ…å«ä¸€ä¸ªé¢œè‰²é€šé“ï¼ˆLï¼‰è€Œä¸æ˜¯3ä¸ªï¼Œå› æ­¤é¢„æœŸå½¢çŠ¶å°†æ˜¯`(B,
    H, W, 1)`ã€‚'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) â€” The prompt or prompts
    not to guide the image generation. Ignored when not using guidance (i.e., ignored
    if `guidance_scale` is less than `1`).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–å¤šä¸ªæç¤ºã€‚å¦‚æœä¸ä½¿ç”¨å¼•å¯¼ï¼ˆå³å¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) â€” The number of
    images to generate per prompt.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number of
    denoising steps. More denoising steps usually lead to a higher quality image at
    the expense of slower inference.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`height` (`int`, *optional*, defaults to 512) â€” The height in pixels of the
    generated image.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚'
- en: '`width` (`int`, *optional*, defaults to 512) â€” The width in pixels of the generated
    image.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚'
- en: '`prior_guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale
    as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º4.0) â€” å¦‚[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`
    åœ¨[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)çš„æ–¹ç¨‹å¼2ä¸­å®šä¹‰ä¸º`w`ã€‚é€šè¿‡è®¾ç½®`guidance_scale
    > 1`æ¥å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬`prompt`å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`prior_num_inference_steps` (`int`, *optional*, defaults to 100) â€” The number
    of denoising steps. More denoising steps usually lead to a higher quality image
    at the expense of slower inference.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prior_num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º100) â€” é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´å›¾åƒè´¨é‡æ›´é«˜ï¼Œä½†æ¨ç†é€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: '`guidance_scale` (`float`, *optional*, defaults to 4.0) â€” Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 4.0) â€” åœ¨[æ— åˆ†ç±»å™¨æ‰©æ•£å¼•å¯¼](https://arxiv.org/abs/2207.12598)ä¸­å®šä¹‰çš„å¼•å¯¼æ¯”ä¾‹ã€‚`guidance_scale`
    å®šä¹‰ä¸º[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)ä¸­æ–¹ç¨‹å¼ 2 çš„ `w`ã€‚é€šè¿‡è®¾ç½® `guidance_scale
    > 1` å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹é¼“åŠ±ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œé€šå¸¸ä»¥é™ä½å›¾åƒè´¨é‡ä¸ºä»£ä»·ã€‚'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) â€” One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ä¸€ä¸ªæˆ–å¤šä¸ª [torch
    ç”Ÿæˆå™¨](https://pytorch.org/docs/stable/generated/torch.Generator.html) ä»¥ä½¿ç”Ÿæˆå…·æœ‰ç¡®å®šæ€§ã€‚'
- en: '`latents` (`torch.FloatTensor`, *optional*) â€” Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„å…ˆç”Ÿæˆçš„å™ªå£°æ½œå˜é‡ï¼Œä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ä»¥ç”¨æ¥ä½¿ç”¨ä¸åŒæç¤ºå¾®è°ƒç›¸åŒçš„ç”Ÿæˆã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™å°†ä½¿ç”¨æä¾›çš„éšæœº
    `generator` è¿›è¡Œé‡‡æ ·ç”Ÿæˆæ½œå˜é‡å¼ é‡ã€‚'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) â€” The output format
    of the generate image. Choose between: `"pil"` (`PIL.Image.Image`), `"np"` (`np.array`)
    or `"pt"` (`torch.Tensor`).'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚å¯é€‰æ‹©ï¼š`"pil"` (`PIL.Image.Image`)ã€`"np"`
    (`np.array`) æˆ– `"pt"` (`torch.Tensor`)ã€‚'
- en: '`callback` (`Callable`, *optional*) â€” A function that calls every `callback_steps`
    steps during inference. The function is called with the following arguments: `callback(step:
    int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¯éš” `callback_steps` æ­¥è°ƒç”¨ä¸€æ¬¡çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`ã€‚'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) â€” The frequency at which
    the `callback` function is called. If not specified, the callback is called at
    every step.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è°ƒç”¨ `callback` å‡½æ•°çš„é¢‘ç‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™åœ¨æ¯ä¸€æ­¥è°ƒç”¨å›è°ƒã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    instead of a plain tuple.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› [ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    or `tuple`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)
    æˆ– `tuple`'
- en: Function invoked when calling the pipeline for generation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ç®¡é“è¿›è¡Œç”Ÿæˆæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚
- en: 'Examples:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE24]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#### `enable_sequential_cpu_offload`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_sequential_cpu_offload`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L652)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/kandinsky/pipeline_kandinsky_combined.py#L652)'
- en: '[PRE25]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Offloads all models to CPU using accelerate, significantly reducing memory usage.
    When called, unet, text_encoder, vae and safety checker have their state dicts
    saved to CPU and then are moved to a `torch.device('meta') and loaded to GPU only
    when their specific submodule has its` forward`method called. Note that offloading
    happens on a submodule basis. Memory savings are higher than with`enable_model_cpu_offload`,
    but performance is lower.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŠ é€Ÿå™¨å°†æ‰€æœ‰æ¨¡å‹è½¬ç§»åˆ° CPUï¼Œæ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨ã€‚è°ƒç”¨æ—¶ï¼Œunetã€text_encoderã€vae å’Œå®‰å…¨æ£€æŸ¥å™¨çš„çŠ¶æ€å­—å…¸å°†ä¿å­˜åˆ° CPUï¼Œç„¶åç§»åŠ¨åˆ°
    `torch.device('meta')`ï¼Œä»…åœ¨å®ƒä»¬çš„ç‰¹å®šå­æ¨¡å—è°ƒç”¨`forward`æ–¹æ³•æ—¶æ‰åŠ è½½åˆ° GPUã€‚è¯·æ³¨æ„ï¼Œå¸è½½æ˜¯åŸºäºå­æ¨¡å—çš„ã€‚ä¸`enable_model_cpu_offload`ç›¸æ¯”ï¼Œå†…å­˜èŠ‚çœæ›´é«˜ï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚
