- en: Prefix tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/prefix_tuning](https://huggingface.co/docs/peft/package_reference/prefix_tuning)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) prefixes a series of task-specific
    vectors to the input sequence that can be learned while keeping the pretrained
    model frozen. The prefix parameters are inserted in all of the model layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fine-tuning is the de facto way to leverage large pretrained language models
    to perform downstream tasks. However, it modifies all the language model parameters
    and therefore necessitates storing a full copy for each task. In this paper, we
    propose prefix-tuning, a lightweight alternative to fine-tuning for natural language
    generation tasks, which keeps language model parameters frozen, but optimizes
    a small continuous task-specific vector (called the prefix). Prefix-tuning draws
    inspiration from prompting, allowing subsequent tokens to attend to this prefix
    as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text
    generation and to BART for summarization. We find that by learning only 0.1\%
    of the parameters, prefix-tuning obtains comparable performance in the full data
    setting, outperforms fine-tuning in low-data settings, and extrapolates better
    to examples with topics unseen during training*.'
  prefs: []
  type: TYPE_NORMAL
- en: PrefixTuningConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PrefixTuningConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/config.py#L21)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_hidden_size` (`int`) — The hidden size of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_projection` (`bool`) — Whether to project the prefix embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [PrefixEncoder](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixEncoder).
  prefs: []
  type: TYPE_NORMAL
- en: PrefixEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PrefixEncoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/model.py#L20)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PrefixTuningConfig](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixTuningConfig))
    — The configuration of the prefix encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `torch.nn` model to encode the prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform` (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix
    embeddings if `prefix_projection` is `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_projection` (`bool`) — Whether to project the prefix embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input shape: (`batch_size`, `num_virtual_tokens`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)'
  prefs: []
  type: TYPE_NORMAL
