- en: Prefix tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前缀调整
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/prefix_tuning](https://huggingface.co/docs/peft/package_reference/prefix_tuning)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/peft/package_reference/prefix_tuning](https://huggingface.co/docs/peft/package_reference/prefix_tuning)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) prefixes a series of task-specific
    vectors to the input sequence that can be learned while keeping the pretrained
    model frozen. The prefix parameters are inserted in all of the model layers.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[前缀调整](https://hf.co/papers/2101.00190)在输入序列中添加了一系列任务特定向量的前缀，这些向量可以在保持预训练模型冻结的同时学习。前缀参数被插入到所有模型层中。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Fine-tuning is the de facto way to leverage large pretrained language models
    to perform downstream tasks. However, it modifies all the language model parameters
    and therefore necessitates storing a full copy for each task. In this paper, we
    propose prefix-tuning, a lightweight alternative to fine-tuning for natural language
    generation tasks, which keeps language model parameters frozen, but optimizes
    a small continuous task-specific vector (called the prefix). Prefix-tuning draws
    inspiration from prompting, allowing subsequent tokens to attend to this prefix
    as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text
    generation and to BART for summarization. We find that by learning only 0.1\%
    of the parameters, prefix-tuning obtains comparable performance in the full data
    setting, outperforms fine-tuning in low-data settings, and extrapolates better
    to examples with topics unseen during training*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*微调是利用大型预训练语言模型执行下游任务的事实方式。然而，它修改了所有语言模型参数，因此需要为每个任务存储完整副本。在本文中，我们提出了前缀调整，这是一种用于自然语言生成任务的轻量级替代微调的方法，它保持语言模型参数冻结，但优化一个小的连续任务特定向量（称为前缀）。前缀调整受提示的启发，允许后续标记关注这个前缀，就像它是“虚拟标记”一样。我们将前缀调整应用于GPT-2进行表格到文本生成，以及应用于BART进行摘要。我们发现，通过仅学习0.1\%的参数，前缀调整在完整数据设置中获得可比较的性能，在低数据设置中优于微调，并且对于训练期间未见过的主题的示例具有更好的外推能力*。'
- en: PrefixTuningConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PrefixTuningConfig
- en: '### `class peft.PrefixTuningConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PrefixTuningConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/config.py#L21)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/config.py#L21)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`encoder_hidden_size` (`int`) — The hidden size of the prompt encoder.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_size` (`int`) — 提示编码器的隐藏大小。'
- en: '`prefix_projection` (`bool`) — Whether to project the prefix embeddings.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_projection` (`bool`) — 是否投影前缀嵌入。'
- en: This is the configuration class to store the configuration of a [PrefixEncoder](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixEncoder).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置类，用于存储[PrefixEncoder](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixEncoder)的配置。
- en: PrefixEncoder
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PrefixEncoder
- en: '### `class peft.PrefixEncoder`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.PrefixEncoder`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/model.py#L20)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/prefix_tuning/model.py#L20)'
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PrefixTuningConfig](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixTuningConfig))
    — The configuration of the prefix encoder.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PrefixTuningConfig](/docs/peft/v0.8.2/en/package_reference/prefix_tuning#peft.PrefixTuningConfig))
    — 前缀编码器的配置。'
- en: The `torch.nn` model to encode the prefix.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编码前缀的`torch.nn`模型。
- en: 'Example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Attributes**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prefix encoder.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding` (`torch.nn.Embedding`) — 前缀编码器的嵌入层。'
- en: '`transform` (`torch.nn.Sequential`) — The two-layer MLP to transform the prefix
    embeddings if `prefix_projection` is `True`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform` (`torch.nn.Sequential`) — 如果`prefix_projection`为`True`，则用于转换前缀嵌入的两层MLP。'
- en: '`prefix_projection` (`bool`) — Whether to project the prefix embeddings.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_projection` (`bool`) — 是否投影前缀嵌入。'
- en: 'Input shape: (`batch_size`, `num_virtual_tokens`)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输入形状：(`batch_size`, `num_virtual_tokens`)
- en: 'Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状：(`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)
