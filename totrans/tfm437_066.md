# 性能和可伸缩性

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/performance](https://huggingface.co/docs/transformers/v4.37.2/en/performance)

训练大型Transformer模型并将其部署到生产环境中会带来各种挑战。

在训练过程中，模型可能需要比可用GPU内存更多的GPU内存，或者表现出训练速度较慢。在部署阶段，模型可能难以处理生产环境中所需的吞吐量。

本文档旨在帮助您克服这些挑战，并找到适合您用例的最佳设置。指南分为训练和推理部分，因为每个部分都有不同的挑战和解决方案。在每个部分中，您将找到针对不同硬件配置的单独指南，例如单个GPU与多个GPU进行训练，或CPU与GPU进行推理。

将本文档作为您进一步导航到与您的情况匹配的方法的起点。

## 训练

训练大型Transformer模型高效地需要像GPU或TPU这样的加速器。最常见的情况是你只有一块GPU。您可以应用的方法来提高单个GPU上的训练效率也适用于其他设置，如多个GPU。然而，也有一些特定于多GPU或CPU训练的技术。我们在单独的部分中进行介绍。

+   [在单个GPU上进行高效训练的方法和工具](perf_train_gpu_one)：从这里开始学习可以帮助优化GPU内存利用率、加快训练速度或两者兼具的常见方法。

+   [多GPU训练部分](perf_train_gpu_many)：探索本节，了解适用于多GPU设置的进一步优化方法，如数据、张量和管道并行。

+   [CPU训练部分](perf_train_cpu)：了解在CPU上进行混合精度训练。

+   [在多个CPU上进行高效训练](perf_train_cpu_many)：了解分布式CPU训练。

+   [使用TensorFlow在TPU上训练](perf_train_tpu_tf)：如果您是TPU的新手，请参考本节，了解在TPU上训练和使用XLA的主观介绍。

+   [用于训练的自定义硬件](perf_hardware)：在构建自己的深度学习装置时找到技巧和窍门。

+   [使用Trainer API进行超参数搜索](hpo_train)

## 推理

在生产环境中使用大型模型进行高效推理可能与训练它们一样具有挑战性。在接下来的部分中，我们将介绍在CPU和单/多GPU设置上运行推理的步骤。

+   [在单个CPU上进行推理](perf_infer_cpu)

+   [在单个GPU上进行推理](perf_infer_gpu_one)

+   [多GPU推理](perf_infer_gpu_one)

+   [XLA集成用于TensorFlow模型](tf_xla)

## 训练和推理

在这里，您将找到适用于训练模型或使用模型进行推理的技术、提示和技巧。

+   [实例化一个大模型](big_models)

+   [性能问题的故障排除](debugging)

## 贡献

这份文档远未完整，还有很多需要添加的内容，所以如果您有补充或更正，请不要犹豫，打开一个PR，或者如果您不确定，请开始一个Issue，我们可以在那里讨论细节。

在提出A优于B的贡献时，请尽量包含可重现的基准测试和/或指向该信息来源的链接（除非信息直接来自您）。
