# 模型

> [`huggingface.co/docs/transformers/v4.37.2/en/main_classes/model`](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model)

基类 PreTrainedModel、TFPreTrainedModel 和 FlaxPreTrainedModel 实现了从本地文件或目录加载/保存模型的常用方法，或从库提供的预训练模型配置（从 HuggingFace 的 AWS S3 存储库下载）加载模型。

PreTrainedModel 和 TFPreTrainedModel 还实现了一些所有模型共有的方法：

+   当新的标记添加到词汇表中时，调整输入标记嵌入大小

+   修剪模型的注意力头。

每个模型共有的其他方法在 ModuleUtilsMixin（用于 PyTorch 模型）和 `~modeling_tf_utils.TFModuleUtilsMixin`（用于 TensorFlow 模型）中定义，或者用于文本生成的 GenerationMixin（用于 PyTorch 模型）、TFGenerationMixin（用于 TensorFlow 模型）和 FlaxGenerationMixin（用于 Flax/JAX 模型）。

## PreTrainedModel

### `class transformers.PreTrainedModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1157)

```py
( config: PretrainedConfig *inputs **kwargs )
```

所有模型的基类。

PreTrainedModel 负责存储模型的配置，并处理加载、下载和保存模型以及一些所有模型共有的方法：

+   调整输入嵌入大小，

+   修剪自注意力头中的头。

类属性（派生类覆盖）：

+   `config_class` (PretrainedConfig) — 用作此模型架构的配置类的 PretrainedConfig 的子类。

+   `load_tf_weights` (`Callable`) — 用于在 PyTorch 模型中加载 TensorFlow 检查点的 Python *方法*，参数为：

    +   `model` (PreTrainedModel) — 要加载 TensorFlow 检查点的模型实例。

    +   `config` (`PreTrainedConfig`) — 与模型关联的配置实例。

    +   `path` (`str`) — TensorFlow 检查点的路径。

+   `base_model_prefix` (`str`) — 一个字符串，指示派生类中基础模型关联的属性，该属性在基础模型的顶部添加模块。

+   `is_parallelizable` (`bool`) — 一个指示此模型是否支持模型并行化的标志。

+   `main_input_name` (`str`) — 模型的主要输入名称（通常为 NLP 模型的 `input_ids`，视觉模型的 `pixel_values` 和语音模型的 `input_values`）。

#### `push_to_hub`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id` (`str`) — 您要推送模型的存储库名称。在推送到特定组织时，应包含您的组织名称。

+   `use_temp_dir`（`bool`，*可选*）— 是否使用临时目录存储保存的文件，然后将它们推送到 Hub。如果没有名为`repo_id`的目录，则默认为`True`，否则为`False`。

+   `commit_message`（`str`，*可选*）— 推送时要提交的消息。默认为`"Upload model"`。

+   `private`（`bool`，*可选*）— 是否应该创建私有存储库。

+   `token`（`bool`或`str`，*可选*）— 用作远程文件的 HTTP bearer 授权的令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。如果未指定`repo_url`，则默认为`True`。

+   `max_shard_size`（`int`或`str`，*可选*，默认为`"5GB"`）— 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点将分片，每个分片的大小都小于此大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认设置为`"5GB"`，以便用户可以在免费的 Google Colab 实例上轻松加载模型，而不会出现任何 CPU OOM 问题。

+   `create_pr`（`bool`，*可选*，默认为`False`）— 是否创建带有上传文件的 PR 或直接提交。

+   `safe_serialization`（`bool`，*可选*，默认为`True`）— 是否将模型权重转换为 safetensors 格式以进行更安全的序列化。

+   `revision`（`str`，*可选*）— 要将上传的文件推送到的分支。

+   `commit_description`（`str`，*可选*）— 将创建的提交的描述

+   `tags`（`List[str]`，*可选*）— 要推送到 Hub 上的标签列表。

将模型文件上传到🤗模型中心。

示例：

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

# Push the model to your namespace with the name "my-finetuned-bert".
model.push_to_hub("my-finetuned-bert")

# Push the model to an organization with the name "my-finetuned-bert".
model.push_to_hub("huggingface/my-finetuned-bert")
```

#### `add_model_tags`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1270)

```py
( tags: Union )
```

参数

+   `tags`（`Union[List[str], str]`）— 要注入模型中的所需标签。

将自定义标签添加到推送到 Hugging Face Hub 的模型中。不会覆盖模型中的现有标签。

示例：

```py
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

model.add_model_tags(["custom", "custom-bert"])

# Push the model to your namespace with the name "my-custom-bert".
model.push_to_hub("my-custom-bert")
```

#### `can_generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1438)

```py
( ) → export const metadata = 'undefined';bool
```

返回

`bool`

此模型是否可以生成序列，使用`.generate()`。

返回此模型是否可以生成序列，使用`.generate()`。

#### `disable_input_require_grads`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1582)

```py
( )
```

删除`_require_grads_hook`。

#### `enable_input_require_grads`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1571)

```py
( )
```

启用输入嵌入的梯度。这对于微调适配器权重并保持模型权重固定很有用。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2617)

```py
( pretrained_model_name_or_path: Union *model_args config: Union = None cache_dir: Union = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' use_safetensors: bool = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`，*可选*）— 可以是：

    +   一个字符串，托管在 huggingface.co 模型存储库中的预训练模型的*模型 ID*。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   指向包含使用 save_pretrained()保存的模型权重的*目录*的路径，例如，`./my_model_directory/`。

    +   指向*TensorFlow 索引检查点文件*的路径或 URL（例如，`./tf_model/model.ckpt.index`）。在这种情况下，应将`from_tf`设置为`True`，并将配置对象作为`config`参数提供。使用此加载路径比使用提供的转换脚本将 TensorFlow 检查点转换为 PyTorch 模型并随后加载 PyTorch 模型要慢。

    +   一个包含*flax checkpoint file*的模型文件夹的路径或 url，格式为*.msgpack*（例如，`./flax_model/`包含`flax_model.msgpack`）。在这种情况下，`from_flax`应设置为`True`。

    +   如果您同时提供配置和状态字典（分别使用关键字参数`config`和`state_dict`），则为`None`。

+   `model_args`（位置参数序列，*可选*）— 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `config`（`Union[PretrainedConfig, str, os.PathLike]`，*可选*）— 可以是：

    +   从 PretrainedConfig 派生的类的实例，

    +   一个作为输入有效的字符串或路径，用于 from_pretrained()。

    代替自动加载的配置使用的模型配置。当：

    +   模型是库提供的模型（使用预训练模型的*模型 id*字符串加载）。

    +   模型是使用 save_pretrained()保存的，并通过提供保存目录重新加载。

    +   通过提供本地目录作为`pretrained_model_name_or_path`加载模型，并在目录中找到名为*config.json*的配置 JSON 文件。

+   `state_dict`（`Dict[str, torch.Tensor]`，*可选*）— 一个状态字典，用于替代从保存的权重文件加载的状态字典。

    如果您想从预训练配置创建模型但加载自己的权重，则可以使用此选项。不过，在这种情况下，您应该检查是否使用 save_pretrained()和 from_pretrained()不是更简单的选项。

+   `cache_dir`（`Union[str, os.PathLike]`，*可选*）— 下载预训练模型配置应缓存在其中的目录路径，如果不使用标准缓存。

+   `from_tf`（`bool`，*可选*，默认为`False`) — 从 TensorFlow 检查点保存文件加载模型权重（请参阅`pretrained_model_name_or_path`参数的文档字符串）。

+   `from_flax`（`bool`，*可选*，默认为`False`）— 从 Flax 检查点保存文件加载模型权重（请参阅`pretrained_model_name_or_path`参数的文档字符串）。

+   `ignore_mismatched_sizes`（`bool`，*可选*，默认为`False`) — 如果检查点中的某些权重与模型的权重大小不同，是否引发错误（例如，您从具有 3 个标签的检查点实例化具有 10 个标签的模型）。

+   `force_download`（`bool`，*可选*，默认为`False`）— 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download`（`bool`，*可选*，默认为`False`) — 是否删除接收不完整的文件。如果存在这样的文件，将尝试恢复下载。

+   `proxies`（`Dict[str, str]`，*可选*）— 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `output_loading_info(bool,` *可选*，默认为`False`) — 是否返回一个包含缺失键、意外键和错误消息的字典。

+   `local_files_only(bool,` *可选*，默认为`False`) — 是否仅查看本地文件（即，不尝试下载模型）。

+   `token`（`str`或`bool`，*可选*）— 用作远程文件的 HTTP bearer 授权的令牌。如果为`True`或未指定，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交 ID，因为我们在 huggingface.co 上使用基于 git 的系统来存储模型和其他工件，所以 `revision` 可以是 git 允许的任何标识符。

    要测试您在 Hub 上创建的拉取请求，可以传递 `revision=“refs/pr/<pr_number>“。</pr_number>

+   `mirror` (`str`, *可选*) — 镜像源以加速中国的下载。如果您来自中国并且有访问问题，您可以设置此选项来解决。请注意，我们不保证及时性或安全性。请参考镜像站点获取更多信息。

+   `_fast_init(bool,` *可选*, 默认为 `True`) — 是否禁用快速初始化。

    为了确保与 `transformers.__version__ < 4.6.0` 的种子模型初始化向后兼容，应该只禁用 *_fast_init*。此参数将在下一个主要版本中删除。有关更多信息，请参阅[拉取请求 11471](https://github.com/huggingface/transformers/pull/11471)。

大型模型推理的参数

+   `low_cpu_mem_usage(bool,` *可选*) — 尝试在加载模型时不使用超过 CPU 内存中的 1x 模型大小（包括峰值内存）。这是一个实验性功能，随时可能更改。

+   `torch_dtype` (`str` 或 `torch.dtype`, *可选*) — 覆盖默认的 `torch.dtype` 并在特定的 `dtype` 下加载模型。不同的选项有：

    1.  `torch.float16` 或 `torch.bfloat16` 或 `torch.float`: 以指定的 `dtype` 加载，忽略模型的 `config.torch_dtype`（如果存在）。如果未指定

        +   模型将以 `torch.float` (fp32) 加载。

    1.  `"auto"` - 将尝试使用模型的 `config.json` 文件中的 `torch_dtype` 条目。如果找不到此条目，则下一个检查是检查点中第一个浮点类型的权重的 `dtype` 并将其用作 `dtype`。这将使用模型在训练结束时保存的 `dtype` 加载模型。它不能用作模型训练方式的指示器。因为它可能是在半精度 `dtype` 中训练，但以 fp32 保存。

    <tip>对于一些模型，它们训练时使用的 `dtype` 是未知的 - 您可以尝试查看模型的论文或联系作者，并要求他们将此信息添加到模型的卡片中，并在 Hub 上的 `config.json` 中插入 `torch_dtype` 条目。</tip>

+   `device_map` (`str` 或 `Dict[str, Union[int, str, torch.device]]` 或 `int` 或 `torch.device`, *可选*) — 一个指定每个子模块应该放在哪里的映射。它不需要细化到每个参数/缓冲区名称，一旦给定模块名称在内，它的每个子模块都将被发送到同一设备。如果我们只传递模型将被分配的设备（例如，`"cpu"`、`"cuda:1"`、`"mps"`，或者像 `1` 这样的 GPU 序数等），设备映射将把整个模型映射到这个设备上。传递 `device_map = 0` 意味着将整个模型放在 GPU 0 上。

    让 Accelerate 自动计算最优化的 `device_map`，请设置 `device_map="auto"`。有关每个选项的更多信息，请参阅[设计设备映射](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map)。

+   `max_memory` (`Dict`, *可选*) — 设备标识符到最大内存的字典。如果未设置，将默认为每个 GPU 可用的最大内存和可用的 CPU RAM。

+   `offload_folder` (`str` 或 `os.PathLike`, *可选*) — 如果 `device_map` 包含任何值 `"disk"`，则我们将卸载权重的文件夹。

+   `offload_state_dict` (`bool`, *可选*) — 如果为 `True`，将临时将 CPU 状态字典转移到硬盘，以避免 CPU RAM 不足，如果 CPU 状态字典的重量 + 检查点的最大分片不适合。当存在一些磁盘卸载时，默认为 `True`。

+   `load_in_8bit` (`bool`, *optional*, 默认为 `False`) — 如果为`True`，将加载的模型转换为混合 8 位量化模型。要使用此功能，请安装`bitsandbytes`（`pip install -U bitsandbytes`）。

+   `load_in_4bit` (`bool`, *optional*, 默认为 `False`) — 如果为`True`，将加载的模型转换为 4 位精度量化模型。要使用此功能，请安装最新版本的`bitsandbytes`（`pip install -U bitsandbytes`）。

+   `quantization_config` (`Union[QuantizationConfigMixin,Dict]`, *可选*) — 量化的配置参数字典或 QuantizationConfigMixin 对象（例如 bitsandbytes, gptq）

+   `subfolder` (`str`, *optional*, 默认为 `""`) — 如果相关文件位于 huggingface.co 模型仓库的子文件夹中，您可以在这里指定文件夹名称。

+   `variant` (`str`, *optional*) — 如果指定，将从`variant`文件名加载权重，例如 pytorch_model.<variant>.bin。在使用`from_tf`或`from_flax`时忽略`variant`。</variant>

+   `use_safetensors` (`bool`, *optional*, 默认为 `None`) — 是否使用`safetensors`检查点。默认为`None`。如果未指定并且未安装`safetensors`，则将其设置为`False`。

+   `kwargs`（剩余的关键字参数字典，*可选*） — 可用于更新配置对象（加载后）并初始化模型（例如，`output_attentions=True`）。根据是否提供了`config`或自动加载，行为会有所不同：

    +   如果提供了配置`config`，`**kwargs`将直接传递给底层模型的`__init__`方法（我们假设配置的所有相关更新已经完成）

    +   如果未提供配置，`kwargs`将首先传递给配置类初始化函数（from_pretrained()）。与配置属性对应的`kwargs`的每个键将用提供的`kwargs`值覆盖该属性。不对应任何配置属性的剩余键将传递给底层模型的`__init__`函数。

从预训练模型配置实例化一个预训练的 pytorch 模型。

默认情况下，模型以评估模式设置，使用`model.eval()`（Dropout 模块被停用）。要训练模型，您应该首先使用`model.train()`将其设置回训练模式。

警告*Weights from XXX not initialized from pretrained model*表示 XXX 的权重不是与模型的其余部分一起预训练的。您需要使用下游微调任务来训练这些权重。

警告*Weights from XXX not used in YYY*表示层 XXX 未被 YYY 使用，因此这些权重将被丢弃。

激活特殊的[“离线模式”](https://huggingface.co/transformers/installation.html#offline-mode)以在受防火墙保护的环境中使用此方法。

示例：

```py
>>> from transformers import BertConfig, BertModel

>>> # Download model and configuration from huggingface.co and cache.
>>> model = BertModel.from_pretrained("bert-base-uncased")
>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
>>> model = BertModel.from_pretrained("./test/saved_model/")
>>> # Update configuration during loading.
>>> model = BertModel.from_pretrained("bert-base-uncased", output_attentions=True)
>>> assert model.config.output_attentions == True
>>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
>>> config = BertConfig.from_json_file("./tf_model/my_tf_model_config.json")
>>> model = BertModel.from_pretrained("./tf_model/my_tf_checkpoint.ckpt.index", from_tf=True, config=config)
>>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)
>>> model = BertModel.from_pretrained("bert-base-uncased", from_flax=True)
```

+   `low_cpu_mem_usage`算法：

这是一个使用约 1 倍模型大小 CPU 内存加载模型的实验性功能

以下是它的工作原理：

1.  保存我们拥有的 state_dict 键

1.  在创建模型之前删除 state_dict，因为后者需要 1 倍模型大小的 CPU 内存

1.  在实例化模型后，切换到元设备，所有将从加载的 state_dict 中替换的参数/缓冲区

1.  第二次加载 state_dict

1.  从 state_dict 中替换参数/缓冲区

目前，它无法处理 deepspeed ZeRO 阶段 3 并忽略加载错误

#### `get_input_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1588)

```py
( ) → export const metadata = 'undefined';nn.Module
```

返回

`nn.Module`

将词汇映射到隐藏状态的 torch 模块。

返回模型的输入嵌入。

#### `get_memory_footprint`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2540)

```py
( return_buffers = True )
```

参数

+   `return_buffers`（`bool`，*可选*，默认为`True`）— 是否在计算内存占用时返回缓冲张量的大小。缓冲区是不需要梯度且未注册为参数的张量。例如，批量归一化层中的均值和标准差。请参见：[`discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2`](https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2)

获取模型的内存占用。这将以字节为单位返回当前模型的内存占用。有助于基准测试当前模型的内存占用并设计一些测试。解决方案灵感来自 PyTorch 讨论：[`discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2`](https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2)

#### `get_output_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1614)

```py
( ) → export const metadata = 'undefined';nn.Module
```

返回

`nn.Module`

将隐藏状态映射到词汇表的 torch 模块。

返回模型的输出嵌入。

#### `gradient_checkpointing_disable`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2166)

```py
( )
```

为当前模型停用梯度检查点。

请注意，在其他框架中，此功能可能被称为“激活检查点”或“检查点激活”。

#### `gradient_checkpointing_enable`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2102)

```py
( gradient_checkpointing_kwargs = None )
```

参数

+   `gradient_checkpointing_kwargs`（字典，*可选*）— 传递给`torch.utils.checkpoint.checkpoint`函数的附加关键字参数。

为当前模型激活梯度检查点。

请注意，在其他框架中，此功能可能被称为“激活检查点”或“检查点激活”。

我们传递模块的`__call__`方法而不是`forward`，因为`__call__`会附加模块的所有钩子。[`discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2`](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2)

#### `init_weights`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2068)

```py
( )
```

如果需要修剪并可能初始化权重。如果使用自定义`PreTrainedModel`，则需要在`_init_weights`中实现任何初始化逻辑。

#### `post_init`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1256)

```py
( )
```

在每个 Transformer 模型初始化结束时执行的方法，用于执行需要模型模块正确初始化的代码（例如权重初始化）。

#### `prune_heads`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2085)

```py
( heads_to_prune: Dict )
```

参数

+   `heads_to_prune`（`Dict[int, List[int]]`）— 键为选定的层索引（`int`）的字典，相关值为该层中要修剪的头部列表（`int`的列表）。例如{1: [0, 2]，2: [2, 3]}将在第 1 层修剪头部 0 和 2，在第 2 层修剪头部 2 和 3。

修剪基本模型的头部。

#### `register_for_auto_class`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4427)

```py
( auto_class = 'AutoModel' )
```

参数

+   `auto_class`（`str`或`type`，*可选*，默认为`"AutoModel"`）— 要注册此新模型的自动类。

将此类与给定的自动类注册。这仅应用于自定义模型，因为库中的模型已经与自动类映射。

此 API 是实验性的，可能在下一个版本中有一些轻微的破坏性更改。

#### `resize_token_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1786)

```py
( new_num_tokens: Optional = None pad_to_multiple_of: Optional = None ) → export const metadata = 'undefined';torch.nn.Embedding
```

参数

+   `new_num_tokens` (`int`, *optional*) — 嵌入矩阵中的新标记数。增加大小将在末尾添加新初始化的向量。减小大小将从末尾删除向量。如果未提供或为`None`，则只返回指向模型的输入标记`torch.nn.Embedding`模块的指针，而不执行任何操作。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充嵌入矩阵到提供的值的倍数。如果`new_num_tokens`设置为`None`，则只会将嵌入填充到`pad_to_multiple_of`的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，计算能力`>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的 TPUs。有关此更多详细信息，或者有关选择调整大小的正确值的帮助，请参阅此指南：[`docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc`](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)

返回

`torch.nn.Embedding`

指向模型的输入标记嵌入模块。

如果`new_num_tokens != config.vocab_size`，则调整模型的输入标记嵌入矩阵的大小。

在模型类具有`tie_weights()`方法时负责绑定权重嵌入。

#### `reverse_bettertransformer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4481)

```py
( ) → export const metadata = 'undefined';PreTrainedModel
```

返回

PreTrainedModel

将模型转换回原始建模。

撤消从 to_bettertransformer()的转换，以便使用原始建模，例如为了保存模型。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2199)

```py
( save_directory: Union is_main_process: bool = True state_dict: Optional = None save_function: Callable = <function save at 0x7f3c5f9b0160> push_to_hub: bool = False max_shard_size: Union = '5GB' safe_serialization: bool = True variant: Optional = None token: Union = None save_peft_format: bool = True **kwargs )
```

参数

+   `save_directory`（`str`或`os.PathLike`） — 要保存到的目录。如果不存在，将创建该目录。

+   `is_main_process` (`bool`, *optional*, 默认为`True`) — 调用此函数的进程是否为主进程。在像 TPU 这样的分布式训练中很有用，需要在所有进程上调用此函数。在这种情况下，仅在主进程上设置`is_main_process=True`，以避免竞争条件。

+   `state_dict`（`torch.Tensor`的嵌套字典） — 要保存的模型的状态字典。将默认为`self.state_dict()`，但可以用于仅保存模型的部分或者在恢复模型的状态字典时需要采取特殊预防措施的情况（例如在使用模型并行时）。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在像 TPU 这样的分布式训练中很有用，当需要用另一种方法替换`torch.save`时。

+   `push_to_hub` (`bool`, *optional*, 默认为`False`) — 是否在保存后将模型推送到 Hugging Face 模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`的名称）。

+   `max_shard_size` (`int`或`str`，*optional*，默认为`"5GB"`) — 在分片之前的检查点的最大大小。然后，检查点分片的大小将小于此大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认为 5GB，以便模型能够在免费的 Google Colab 实例上轻松运行，而不会出现 CPU OOM 问题。

    如果模型的单个权重大于`max_shard_size`，则它将在自己的检查点分片中，该分片将大于`max_shard_size`。

+   `safe_serialization` (`bool`, *optional*, 默认为`True`) — 是否使用`safetensors`或传统的 PyTorch 方式（使用`pickle`）保存模型。

+   `variant` (`str`, *可选*) — 如果指定，权重将以 pytorch_model.<variant>.bin 的格式保存。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。

+   `save_peft_format` (`bool`, *可选*, 默认为 `True`) — 为了向后兼容 PEFT 库，如果适配器权重附加到模型上，适配器状态字典的所有键都需要以 `base_model.model` 为前缀。高级用户可以通过将 `save_peft_format` 设置为 `False` 来禁用此行为。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 push_to_hub() 方法的额外关键字参数。

将模型及其配置文件保存到目录中，以便可以使用 from_pretrained() 类方法重新加载。

#### `set_input_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1601)

```py
( value: Module )
```

参数

+   `value` (`nn.Module`) — 将词汇映射到隐藏状态的模块。

设置模型的输入嵌入。

#### `tie_weights`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1641)

```py
( )
```

将输入嵌入和输出嵌入之间的权重绑定在一起。

如果配置中设置了 `torchscript` 标志，则无法处理参数共享，因此我们会克隆权重。

#### `to_bettertransformer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4453)

```py
( ) → export const metadata = 'undefined';PreTrainedModel
```

返回

PreTrainedModel

转换为 BetterTransformer 的模型。

将模型转换为使用[PyTorch 的本机注意力实现](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)，通过[Optimum 库](https://huggingface.co/docs/optimum/bettertransformer/overview)集成到 Transformers 中。仅支持所有 Transformers 模型的子集。

PyTorch 的注意力快速路径允许通过内核融合和使用[嵌套张量](https://pytorch.org/docs/stable/nested.html)加速推理。详细的基准测试可以在[这篇博文](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)中找到。

#### `warn_if_padding_and_no_attention_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4503)

```py
( input_ids attention_mask )
```

如果输入的 input_ids 看起来包含填充并且没有给出注意力掩码，则显示一次警告。

### 大型模型加载

在 Transformers 4.20.0 中，from_pretrained() 方法已经重新设计，以适应使用[Accelerate](https://huggingface.co/docs/accelerate/big_modeling)的大型模型。这需要 Accelerate >= 0.9.0 和 PyTorch >= 1.9.0。与其在内存中创建完整模型，然后加载预训练权重（这需要模型大小的两倍的内存，一个用于随机初始化模型，一个用于权重），现在有一个选项可以创建模型作为空壳，然后只有在加载预训练权重时才实现其参数。

可以通过 `low_cpu_mem_usage=True` 激活此选项。模型首先在 Meta 设备上创建（带有空权重），然后状态字典被加载到其中（在分片检查点的情况下逐个分片）。这样，最大使用的 RAM 仅为模型的完整大小。

```py
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", low_cpu_mem_usage=True)
```

此外，如果模型无法完全适应 RAM（目前仅适用于推断），则可以直接将模型放置在不同的设备上。使用`device_map="auto"`，Accelerate 将确定将每个层放置在哪里以最大化您最快的设备（GPU）的使用，并将其余部分卸载到 CPU，甚至硬盘，如果您没有足够的 GPU RAM（或 CPU RAM）。即使模型分布在多个设备上，它也将按照您通常的预期运行。

在传递`device_map`时，`low_cpu_mem_usage`会自动设置为`True`，因此您无需指定它：

```py
from transformers import AutoModelForSeq2SeqLM

t0pp = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto")
```

您可以查看`hf_device_map`属性来查看模型如何分布在设备上：

```py
t0pp.hf_device_map
```

```py
{'shared': 0,
 'decoder.embed_tokens': 0,
 'encoder': 0,
 'decoder.block.0': 0,
 'decoder.block.1': 1,
 'decoder.block.2': 1,
 'decoder.block.3': 1,
 'decoder.block.4': 1,
 'decoder.block.5': 1,
 'decoder.block.6': 1,
 'decoder.block.7': 1,
 'decoder.block.8': 1,
 'decoder.block.9': 1,
 'decoder.block.10': 1,
 'decoder.block.11': 1,
 'decoder.block.12': 1,
 'decoder.block.13': 1,
 'decoder.block.14': 1,
 'decoder.block.15': 1,
 'decoder.block.16': 1,
 'decoder.block.17': 1,
 'decoder.block.18': 1,
 'decoder.block.19': 1,
 'decoder.block.20': 1,
 'decoder.block.21': 1,
 'decoder.block.22': 'cpu',
 'decoder.block.23': 'cpu',
 'decoder.final_layer_norm': 'cpu',
 'decoder.dropout': 'cpu',
 'lm_head': 'cpu'}
```

您还可以按照相同格式编写自己的设备映射（将层名称映射到设备的字典）。它应该将模型的所有参数映射到给定设备，但如果该层完全位于同一设备上，则不必详细说明一个层的所有子模块去哪里。例如，以下设备映射对于 T0pp 将正常工作（只要您有 GPU 内存）：

```py
device_map = {"shared": 0, "encoder": 0, "decoder": 1, "lm_head": 1}
```

减少模型内存影响的另一种方法是以较低精度 dtype（如`torch.float16`）实例化模型，或者使用下面描述的直接量化技术。

### 模型实例化 dtype

在 Pytorch 下，模型通常以`torch.float32`格式实例化。如果尝试加载权重为 fp16 的模型，则可能会出现问题，因为它将需要两倍的内存。为了克服这个限制，您可以使用`torch_dtype`参数显式传递所需的`dtype`：

```py
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype=torch.float16)
```

或者，如果希望模型始终以最佳内存模式加载，可以使用特殊值`"auto"`，然后`dtype`将自动从模型的权重中派生：

```py
model = T5ForConditionalGeneration.from_pretrained("t5", torch_dtype="auto")
```

从头开始实例化的模型也可以指定使用的`dtype`：

```py
config = T5Config.from_pretrained("t5")
model = AutoModel.from_config(config)
```

由于 Pytorch 设计，此功能仅适用于浮点 dtype。

## ModuleUtilsMixin

### `class transformers.modeling_utils.ModuleUtilsMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L853)

```py
( )
```

用作 mixin 的`torch.nn.Modules`的一些实用程序。

#### `add_memory_hooks`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L884)

```py
( )
```

在每个子模块正向传递之前和之后添加内存钩子以记录内存消耗的增加。

内存消耗的增加存储在每个模块的`mem_rss_diff`属性中，并可以使用`model.reset_memory_hooks_state()`将其重置为零。

#### `estimate_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1109)

```py
( input_dict: Dict ) → export const metadata = 'undefined';int
```

参数

+   `inputs`（`dict`）— 模型输入。

返回

`int`

令牌的总数。

用于估计模型输入中总令牌数的辅助函数。

#### `floating_point_ops`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1130)

```py
( input_dict: Dict exclude_embeddings: bool = True ) → export const metadata = 'undefined';int
```

参数

+   `batch_size`（`int`）— 正向传递的批量大小。

+   `sequence_length`（`int`）— 每个批次行中的令牌数。

+   `exclude_embeddings`（`bool`，*可选*，默认为`True`）— 是否计算嵌入和 softmax 操作。

返回

`int`

浮点运算的数量。

使用此转换器模型的批处理的正向和反向传递的浮点操作的数量（可选，非嵌入）。默认近似忽略对令牌数量的二次依赖（如果`12 * d_model << sequence_length`）如[本文](https://arxiv.org/pdf/2001.08361.pdf)第 2.1 节所述。对于具有参数重用的变压器（例如 Albert 或通用变压器）或者如果使用非常高的序列长度进行长距离建模，则应该进行覆盖。

#### `get_extended_attention_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L972)

```py
( attention_mask: Tensor input_shape: Tuple device: device = None dtype: torch.float32 = None )
```

参数

+   `attention_mask` (`torch.Tensor`) — 一个掩码，其中的 1 表示要关注的标记，0 表示要忽略的标记。

+   `input_shape` (`Tuple[int]`) — 模型的输入形状。

使可广播的注意力和因果掩码，以便将来和掩码的标记被忽略。

#### `get_head_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1024)

```py
( head_mask: Optional num_hidden_layers: int is_attention_chunked: bool = False )
```

参数

+   `head_mask` (`torch.Tensor`，形状为`[num_heads]`或`[num_hidden_layers x num_heads]`，*可选*) — 指示我们是否应保留头部的掩码（保留为 1.0，丢弃为 0.0）。

+   `num_hidden_layers` (`int`) — 模型中的隐藏层数量。

+   `is_attention_chunked` (`bool`, *可选*, 默认为 `False`) — 注意力分数是否按块计算。

如果需要，准备头掩码。

#### `invert_attention_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L920)

```py
( encoder_attention_mask: Tensor ) → export const metadata = 'undefined';torch.Tensor
```

参数

+   `encoder_attention_mask` (`torch.Tensor`) — 一个注意力掩码。

返回

`torch.Tensor`

反转的注意力掩码。

反转注意力掩码（例如，切换 0 和 1）。

#### `num_parameters`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1062)

```py
( only_trainable: bool = False exclude_embeddings: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `only_trainable` (`bool`, *可选*, 默认为 `False`) — 是否只返回可训练参数的数量

+   `exclude_embeddings` (`bool`, *可选*, 默认为 `False`) — 是否只返回非嵌入参数的数量

返回

`int`

参数的数量。

获取模块中（可选地，可训练或非嵌入）参数的数量。

#### `reset_memory_hooks_state`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L896)

```py
( )
```

重置每个模块的`mem_rss_diff`属性（参见 add_memory_hooks()）。

## TFPreTrainedModel

### `class transformers.TFPreTrainedModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1058)

```py
( config *inputs **kwargs )
```

所有 TF 模型的基类。

TFPreTrainedModel 类负责存储模型的配置，并处理加载、下载和保存模型的方法，以及一些所有模型通用的方法：

+   调整输入嵌入，

+   修剪自注意力头。

类属性（由派生类覆盖）：

+   `config_class` (PretrainedConfig) — 用作此模型架构的配置类的 PretrainedConfig 的子类。

+   `base_model_prefix` (`str`) — 一个字符串，指示派生类中基础模型关联的属性，在同一架构的派生类中添加模块到基础模型之上。

+   `main_input_name` (`str`) — 模型的主要输入的名称（通常为 NLP 模型的`input_ids`，视觉模型的`pixel_values`和语音模型的`input_values`）。

#### `push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3067)

```py
( repo_id: str use_temp_dir: Optional[bool] = None commit_message: Optional[str] = None private: Optional[bool] = None max_shard_size: Optional[Union[int, str]] = '10GB' token: Optional[Union[bool, str]] = None use_auth_token: Optional[Union[bool, str]] = None create_pr: bool = False **base_model_card_args )
```

参数

+   `repo_id` (`str`) — 您要将模型推送到的存储库的名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir` (`bool`, *可选*) — 是否使用临时目录存储保存的文件，直到它们被推送到 Hub。如果没有名为`repo_id`的目录，则默认为`True`，否则为`False`。

+   `commit_message` (`str`, *可选*) — 推送时要提交的消息。默认为`"Upload model"`。

+   `private`（`bool`，*可选*）— 是否应创建私有存储库。

+   `token`（`bool`或`str`，*可选*）— 用作远程文件的 HTTP bearer 授权的令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。如果未指定`repo_url`，则默认为`True`。

+   `max_shard_size`（`int`或`str`，*可选*，默认为`"10GB"`）— 仅适用于模型。在分片之前的检查点的最大大小。然后，检查点分片将每个大小小于此大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。

+   `create_pr`（`bool`，*可选*，默认为`False`）— 是否创建带有上传文件的 PR 或直接提交。

将模型文件上传到🤗模型 Hub，同时同步存储库的本地克隆到`repo_path_or_name`中。

示例：

```py
from transformers import TFAutoModel

model = TFAutoModel.from_pretrained("bert-base-cased")

# Push the model to your namespace with the name "my-finetuned-bert".
model.push_to_hub("my-finetuned-bert")

# Push the model to an organization with the name "my-finetuned-bert".
model.push_to_hub("huggingface/my-finetuned-bert")
```

#### `can_generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1301)

```py
( ) → export const metadata = 'undefined';bool
```

返回

`bool`

此模型是否可以使用`.generate()`生成序列。

返回此模型是否可以使用`.generate()`生成序列。

#### `compile`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1496)

```py
( optimizer = 'rmsprop' loss = 'auto_with_warning' metrics = None loss_weights = None weighted_metrics = None run_eagerly = None steps_per_execution = None **kwargs )
```

这是一个薄包装器，如果用户没有指定自己的损失函数，则将模型的损失输出头设置为损失。

#### `create_model_card`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1791)

```py
( output_dir model_name: str language: Optional[str] = None license: Optional[str] = None tags: Optional[str] = None finetuned_from: Optional[str] = None tasks: Optional[str] = None dataset_tags: Optional[Union[str, List[str]]] = None dataset: Optional[Union[str, List[str]]] = None dataset_args: Optional[Union[str, List[str]]] = None )
```

参数

+   `output_dir`（`str`或`os.PathLike`）— 创建模型卡片的文件夹。

+   `model_name`（`str`，*可选*）— 模型的名称。

+   `language`（`str`，*可选*）— 模型的语言（如果适用）

+   `license`（`str`，*可选*）— 模型的许可证。如果给定给`Trainer`的原始模型来自 Hub 上的 repo，则默认为使用的预训练模型的许可证。

+   `tags`（`str`或`List[str]`，*可选*）— 要包含在模型卡片的元数据中的一些标签。

+   `finetuned_from`（`str`，*可选*）— 用于微调此模型的模型的名称（如果适用）。如果来自 Hub 的原始模型的`Trainer`给出的 repo 的名称，则默认为原始模型的名称。

+   `tasks`（`str`或`List[str]`，*可选*）— 一个或多个任务标识符，要包含在模型卡片的元数据中。

+   `dataset_tags`（`str`或`List[str]`，*可选*）— 一个或多个数据集标签，要包含在模型卡片的元数据中。

+   `dataset`（`str`或`List[str]`，*可选*）— 一个或多个数据集标识符，要包含在模型卡片的元数据中。

+   `dataset_args`（`str`或`List[str]`，*可选*）— 一个或多个数据集参数，要包含在模型卡片的元数据中。

使用`Trainer`可用的信息创建模型卡片的草稿。

#### `eager_serving`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1213)

```py
( inputs )
```

参数

+   `inputs`（`Dict[str, tf.Tensor]`）— 保存模型的输入，作为张量字典。

用于提供模型的方法。此方法已弃用，将被移除。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2499)

```py
( pretrained_model_name_or_path: Optional[Union[str, os.PathLike]] *model_args config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None cache_dir: Optional[Union[str, os.PathLike]] = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Optional[Union[str, bool]] = None revision: str = 'main' use_safetensors: bool = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`，*可选*）— 可以是：

    +   一个字符串，预训练模型的*模型 id*，托管在 huggingface.co 上的模型存储库中。有效的模型 id 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用 save_pretrained()保存的模型权重，例如，`./my_model_directory/`。

    +   一个 *PyTorch state_dict 保存文件* 的路径或 url（例如，`./pt_model/pytorch_model.bin`）。在这种情况下，`from_pt` 应设置为 `True`，并且应将配置对象作为 `config` 参数提供。使用此加载路径比使用提供的转换脚本将 PyTorch 模型转换为 TensorFlow 模型并随后加载 TensorFlow 模型要慢。

    +   如果您同时提供配置和状态字典（分别使用关键字参数 `config` 和 `state_dict`），则为 `None`。

+   `model_args`（位置参数序列，*可选*） — 所有剩余的位置参数将传递给底层模型的 `__init__` 方法。

+   `config` (`Union[PretrainedConfig, str]`, *可选*) — 可以是：

    +   从 PretrainedConfig 派生的类的实例，

    +   作为 from_pretrained() 输入有效的字符串。

    用于替代自动加载配置的模型配置。当以下情况发生时，配置可以自动加载：

    +   模型是库提供的模型（使用预训练模型的 *模型 id* 字符串加载）。

    +   模型是使用 save_pretrained() 保存的，并通过提供保存目录重新加载。

    +   通过提供本地目录作为 `pretrained_model_name_or_path` 并在目录中找到名为 *config.json* 的配置 JSON 文件来加载模型。

+   `from_pt` (`bool`, *可选*, 默认为 `False`) — 从 PyTorch state_dict 保存文件加载模型权重（请参阅 `pretrained_model_name_or_path` 参数的文档字符串）。

+   `ignore_mismatched_sizes` (`bool`, *可选*, 默认为 `False`) — 是否在检查点的某些权重与模型的权重大小不同时引发错误（例如，如果您从具有 3 个标签的检查点实例化具有 10 个标签的模型）。

+   `cache_dir` (`str`, *可选*) — 下载的预训练模型配置应缓存在其中的目录路径，如果不应使用标准缓存。

+   `force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，将尝试恢复下载。代理 — (`Dict[str, str],` 可选`): 用于每个请求的协议或端点的代理服务器字典，例如` {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}`。代理将用于每个请求。output_loading_info(`bool`, *可选*, 默认为` False`): 是否返回包含缺失键、意外键和错误消息的字典。

+   `local_files_only(bool,` *可选*, 默认为 `False`) — 是否仅查看本地文件（例如，不尝试下载模型）。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或者未指定，则将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交 id，因为我们在 huggingface.co 上使用基于 git 的系统存储模型和其他工件，所以 `revision` 可以是 git 允许的任何标识符。

从预训练模型配置实例化预训练的 TF 2.0 模型。

警告 *Weights from XXX not initialized from pretrained model* 意味着 XXX 的权重不是与模型的其余部分一起预训练的。您需要使用下游微调任务来训练这些权重。

警告*来自 XXX 的权重在 YYY 中未使用*表示层 XXX 未被 YYY 使用，因此这些权重被丢弃。

示例：

```py
>>> from transformers import BertConfig, TFBertModel

>>> # Download model and configuration from huggingface.co and cache.
>>> model = TFBertModel.from_pretrained("bert-base-uncased")
>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
>>> model = TFBertModel.from_pretrained("./test/saved_model/")
>>> # Update configuration during loading.
>>> model = TFBertModel.from_pretrained("bert-base-uncased", output_attentions=True)
>>> assert model.config.output_attentions == True
>>> # Loading from a Pytorch model file instead of a TensorFlow checkpoint (slower, for example purposes, not runnable).
>>> config = BertConfig.from_json_file("./pt_model/my_pt_model_config.json")
>>> model = TFBertModel.from_pretrained("./pt_model/my_pytorch_model.bin", from_pt=True, config=config)
```

#### `get_bias`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1931)

```py
( ) → export const metadata = 'undefined';tf.Variable
```

返回

`tf.Variable`

表示偏置的权重，如果不是 LM 模型则为 None。

附加到 LM head 的偏置的字典。键表示偏置属性的名称。

#### `get_head_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1168)

```py
( head_mask: tf.Tensor | None num_hidden_layers: int )
```

参数

+   `head_mask` (`tf.Tensor`，形状为 `[num_heads]` 或 `[num_hidden_layers x num_heads]`，*可选*) — 指示我们是否应保留头部的掩码（保留为 1.0，丢弃为 0.0）。

+   `num_hidden_layers` (`int`) — 模型中的隐藏层数量。

如果需要，准备头部掩码。

#### `get_input_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1315)

```py
( ) → export const metadata = 'undefined';tf.Variable
```

返回

`tf.Variable`

将词汇映射到隐藏状态的嵌入层。

返回模型的输入嵌入层。

#### `get_lm_head`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1964)

```py
( ) → export const metadata = 'undefined';tf.keras.layers.Layer
```

返回

`tf.keras.layers.Layer`

如果模型有 LM head 层，则为 LM head 层，否则为 None。

LM Head 层。该方法必须被所有具有 lm head 的模型覆盖。

#### `get_output_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1871)

```py
( ) → export const metadata = 'undefined';tf.Variable
```

返回

`tf.Variable`

将词汇映射到隐藏状态的新权重。

返回模型的输出嵌入

#### `get_output_layer_with_bias`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1908)

```py
( ) → export const metadata = 'undefined';tf.keras.layers.Layer
```

返回

`tf.keras.layers.Layer`

处理偏置的层，如果不是 LM 模型则为 None。

获取处理偏置属性的层，如果模型具有将权重绑定到嵌入的 LM head

#### `get_prefix_bias_name`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1921)

```py
( ) → export const metadata = 'undefined';str
```

返回

`str`

偏置的连接前缀名称。

从模型名称到父层的偏置的连接前缀名称

#### `load_repo_checkpoint`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1342)

```py
( repo_path_or_name ) → export const metadata = 'undefined';dict
```

参数

+   `repo_path_or_name` (`str`) — 可以是 Hub 中您的 {object} 的存储库名称，也可以是本地文件夹的路径（在这种情况下，存储库将使用该本地文件夹的名称）。

返回

`dict`

来自检查点的额外元数据字典，通常是“时代”计数。

从存储库加载已保存的检查点（模型权重和优化器状态）。返回检查点生成时的当前时代计数。

#### `prepare_tf_dataset`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1391)

```py
( dataset: 'datasets.Dataset' batch_size: int = 8 shuffle: bool = True tokenizer: Optional['PreTrainedTokenizerBase'] = None collate_fn: Optional[Callable] = None collate_fn_args: Optional[Dict[str, Any]] = None drop_remainder: Optional[bool] = None prefetch: bool = True ) → export const metadata = 'undefined';Dataset
```

参数

+   `dataset` (`Any`) — 要包装为 `tf.data.Dataset` 的 [~`datasets.Dataset`]。

+   `batch_size` (`int`，默认为 8) — 要返回的批次大小。

+   `shuffle` (`bool`，默认为 `True`) — 是否以随机顺序返回数据集中的样本。通常对于训练数据集为 `True`，对于验证/测试数据集为 `False`。

+   `tokenizer`（PreTrainedTokenizerBase，*可选*） — 用于填充样本以创建批次的 `PreTrainedTokenizer`。如果传递了特定的 `collate_fn`，则不会产生影响。

+   `collate_fn` (`Callable`，*可选*) — 一个将数据集中的样本整理成单个批次的函数。如果未提供 `tokenizer`，则默认为 `DefaultDataCollator`，如果传递了 `tokenizer`，则为 `DataCollatorWithPadding`。

+   `collate_fn_args` (`Dict[str, Any]`, *可选*) — 传递给`collate_fn`的参数字典，以及样本列表。

+   `drop_remainder` (`bool`, *可选*) — 是否丢弃最后一个批次，如果批次大小不能整除数据集长度。默认设置与`shuffle`相同。

+   `prefetch` (`bool`, 默认为 `True`) — 是否在`tf.data`管道的末尾添加预取。这几乎总是有利于性能，但在边缘情况下可以禁用。

返回

`Dataset`

一个准备传递给 Keras API 的`tf.data.Dataset`。

将 HuggingFace [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)包装为带有整理和批处理的`tf.data.Dataset`。此方法旨在创建一个“即插即用”的数据集，可以直接传递给 Keras 方法，如`fit()`，而无需进一步修改。如果数据集中的列与模型的输入名称不匹配，该方法将删除这些列。如果您想指定要返回的列名，而不是使用与此模型匹配的名称，我们建议使用`Dataset.to_tf_dataset()`。

#### `prune_heads`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2311)

```py
( heads_to_prune )
```

参数

+   `heads_to_prune` (`Dict[int, List[int]]`) — 键为选定的层索引（`int`）的字典，相关值为要在该层中修剪的头部列表（`int`列表）。例如，{1: [0, 2], 2: [2, 3]}将在第 1 层修剪头部 0 和 2，在第 2 层修剪头部 2 和 3。

修剪基础模型的头部。

#### `register_for_auto_class`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3176)

```py
( auto_class = 'TFAutoModel' )
```

参数

+   `auto_class` (`str` 或 `type`, *可选*, 默认为 `"TFAutoModel"`) — 要注册此新模型的自动类。

使用给定的自动类注册此类。这应仅用于自定义模型，因为库中的模型已经与自动类映射。

此 API 是实验性的，可能在下一个版本中有一些轻微的破坏性更改。

#### `resize_token_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1973)

```py
( new_num_tokens: Optional[int] = None ) → export const metadata = 'undefined';tf.Variable or tf.keras.layers.Embedding
```

参数

+   `new_num_tokens` (`int`, *可选*) — 嵌入矩阵中的新标记数量。增加大小将在末尾添加新初始化的向量。减小大小将从末尾删除向量。如果未提供或为`None`，则只返回指向输入标记的指针，而不执行任何操作。

返回

`tf.Variable` 或 `tf.keras.layers.Embedding`

模型的输入标记的指针。

如果`new_num_tokens != config.vocab_size`，则调整模型的输入标记嵌入矩阵大小。

如果模型类具有`tie_weights()`方法，则在之后处理权重嵌入。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2323)

```py
( save_directory saved_model = False version = 1 push_to_hub = False signatures = None max_shard_size: Union[int, str] = '10GB' create_pr: bool = False safe_serialization: bool = False token: Optional[Union[str, bool]] = None **kwargs )
```

参数

+   `save_directory` (`str`) — 要保存到的目录。如果不存在，将创建该目录。

+   `saved_model` (`bool`, *可选*, 默认为 `False`) — 是否还要将模型保存为 saved model 格式。

+   `version` (`int`, *可选*, 默认为 1) — 已保存模型的版本。为了能够被 TensorFlow Serving 正确加载，保存的模型需要进行版本化，详细信息请参阅官方文档[`www.tensorflow.org/tfx/serving/serving_basic`](https://www.tensorflow.org/tfx/serving/serving_basic)

+   `push_to_hub` (`bool`, *可选*, 默认为 `False`) — 是否在保存后将模型推送到 Hugging Face 模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`名称）。

+   `signatures` (`dict` 或 `tf.function`, *可选*) — 用于 serving 的模型签名。这将传递给 model.save()的`signatures`参数。

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为 `"10GB"`) - 在分片之前的检查点的最大大小。然后，检查点分片将小于此大小。如果表示为字符串，需要是数字后跟一个单位（如 `"5MB"`）。

    如果模型的单个权重大于 `max_shard_size`，它将在自己的检查点分片中，该分片将大于 `max_shard_size`。

+   `create_pr` (`bool`, *可选*, 默认为 `False`) - 是否创建带有上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *可选*, 默认为 `False`) - 是否使用 `safetensors` 或传统的 TensorFlow 方式（使用 `h5`）保存模型。

+   `token` (`str` 或 `bool`, *可选*) - 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。

+   `kwargs` (`Dict[str, Any]`, *可选*) - 传递给 push_to_hub() 方法的额外关键字参数。

将模型及其配置文件保存到目录中，以便可以使用 from_pretrained() 类方法重新加载。

#### `serving`

```py
( inputs )
```

参数

+   用于提供模型的方法。没有特定的签名，但将作为具体的专业化 -

+   使用 `save_pretrained` 保存时的 `functions`。 - 输入（`Dict[str, tf.Tensor]`）：保存模型的输入，作为张量字典。

#### `serving_output`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1277)

```py
( output )
```

准备保存模型的输出。如果需要特定的服务修改，可以进行覆盖。

#### `set_bias`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1948)

```py
( value )
```

参数

+   `value` (`Dict[tf.Variable]`) - 附加到 LM 头部的所有新偏置。

设置 LM 头部中的所有偏置。

#### `set_input_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1851)

```py
( value )
```

参数

+   `value` (`tf.Variable`) - 将隐藏状态映射到词汇表的新权重。

设置模型的输入嵌入

#### `set_output_embeddings`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1891)

```py
( value )
```

参数

+   `value` (`tf.Variable`) - 将隐藏状态映射到词汇表的新权重。

设置模型的输出嵌入

#### `test_step`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1687)

```py
( data )
```

对 Keras 默认的 `train_step` 进行修改，正确处理模型输出与标签的匹配，并支持直接在损失输出头上进行训练。此外，它确保适当时将输入键复制到标签中。当使用虚拟损失时，它还会将标签键复制到输入字典中，以确保它们在前向传递期间对模型可用。

#### `train_step`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1579)

```py
( data )
```

对 Keras 默认的 `train_step` 进行修改，正确处理模型输出与标签的匹配，并支持直接在损失输出头上进行训练。此外，它确保适当时将输入键复制到标签中。当使用虚拟损失时，它还会将标签键复制到输入字典中，以确保它们在前向传递期间对模型可用。

## TFModelUtilsMixin

### `class transformers.modeling_tf_utils.TFModelUtilsMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L104)

```py
( )
```

用于 `tf.keras.Model` 的一些实用程序，可用作混合。

#### `num_parameters`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L109)

```py
( only_trainable: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `only_trainable` (`bool`, *optional*, 默认为 `False`) — 是否仅返回可训练参数的数量

返回

`int`

参数数量。

获取模型中的（可选的可训练）参数数量。

## FlaxPreTrainedModel

### `class transformers.FlaxPreTrainedModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L166)

```py
( config: PretrainedConfig module: Module input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True )
```

所有模型的基类。

FlaxPreTrainedModel 负责存储模型的配置，并处理加载、下载和保存模型的方法。

类属性（由派生类覆盖）：

+   `config_class` (PretrainedConfig) — 用作此模型架构的配置类的 PretrainedConfig 的子类。

+   `base_model_prefix` (`str`) — 一个字符串，指示派生类中基础模型关联的属性，该派生类在基础模型之上添加模块。

+   `main_input_name` (`str`) — 模型的主要输入的名称（通常为 NLP 模型的 `input_ids`，视觉模型的 `pixel_values` 和语音模型的 `input_values`）。

#### `push_to_hub`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id` (`str`) — 您要将模型推送到的存储库的名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir` (`bool`, *optional*) — 是否使用临时目录存储在推送到 Hub 之前保存的文件。如果没有名为 `repo_id` 的目录，则默认为 `True`，否则为 `False`。

+   `commit_message` (`str`, *optional*) — 推送时要提交的消息。默认为 `"Upload model"`。

+   `private` (`bool`, *optional*) — 是否创建的存储库应为私有。

+   `token` (`bool` 或 `str`, *optional*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。如果未指定 `repo_url`，则默认为 `True`。

+   `max_shard_size` (`int` 或 `str`, *optional*, 默认为 `"5GB"`) — 仅适用于模型。在被分片之前的检查点的最大大小。然后检查点将被分成小于此大小的每个部分。如果表示为字符串，需要是数字后跟一个单位（如 `"5MB"`）。我们将其默认为 `"5GB"`，以便用户可以在免费的 Google Colab 实例上轻松加载模型，而不会出现任何 CPU OOM 问题。

+   `create_pr` (`bool`, *optional*, 默认为 `False`) — 是否创建一个带有上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *optional*, 默认为 `True`) — 是否将模型权重转换为 safetensors 格式以进行更安全的序列化。

+   `revision` (`str`, *optional*) — 要将上传的文件推送到的分支。

+   `commit_description` (`str`, *optional*) — 将要创建的提交的描述

+   `tags` (`List[str]`, *optional*) — 要推送到 Hub 上的标签列表。

上传模型检查点到 🤗 Model Hub。

示例：

```py
from transformers import FlaxAutoModel

model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Push the model to your namespace with the name "my-finetuned-bert".
model.push_to_hub("my-finetuned-bert")

# Push the model to an organization with the name "my-finetuned-bert".
model.push_to_hub("huggingface/my-finetuned-bert")
```

#### `can_generate`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L506)

```py
( )
```

返回此模型是否可以使用 `.generate()` 生成序列。返回：`bool`: 此模型是否可以使用 `.generate()` 生成序列。

#### `from_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L518)

```py
( pretrained_model_name_or_path: Union dtype: dtype = <class 'jax.numpy.float32'> *model_args config: Union = None cache_dir: Union = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str`或`os.PathLike`) — 可以是：

    +   一个字符串，托管在 huggingface.co 模型存储库中的预训练模型的*模型 ID*。有效的模型 ID 可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个包含使用 save_pretrained()保存的模型权重的*目录*路径，例如，`./my_model_directory/`。

    +   *pt 索引检查点文件*的路径或 URL（例如，`./tf_model/model.ckpt.index`）。在这种情况下，`from_pt`应设置为`True`。

+   `dtype` (`jax.numpy.dtype`, *optional*, 默认为 `jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和`jax.numpy.bfloat16`（在 TPU 上）之一。

    这可以用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅 to_fp16()和 to_bf16()。

+   `model_args`（位置参数序列，*optional*） — 所有剩余的位置参数将传递给底层模型的`__init__`方法。

+   `config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) — 可以是：

    +   从 PretrainedConfig 派生的类的实例，

    +   一个作为 from_pretrained()输入有效的字符串或路径。

    要使用的模型配置，而不是自动加载的配置。当以下情况自动加载配置时：

    +   模型是库提供的模型（使用预训练模型的*模型 ID*字符串加载）。

    +   模型是使用 save_pretrained()保存的，并通过提供保存目录重新加载。

    +   通过提供本地目录作为`pretrained_model_name_or_path`加载模型，并在目录中找到名为*config.json*的配置 JSON 文件。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 如果不应使用标准缓存，则应将下载的预训练模型配置缓存在其中的目录路径。

+   `from_pt` (`bool`, *optional*, 默认为 `False`) — 从 PyTorch 检查点保存文件加载模型权重（请参阅`pretrained_model_name_or_path`参数的文档字符串）。

+   `ignore_mismatched_sizes` (`bool`, *optional*, 默认为 `False`) — 如果检查点中的某些权重与模型的权重大小不同，是否引发错误（例如，如果您从具有 3 个标签的检查点实例化具有 10 个标签的模型）。

+   `force_download` (`bool`, *optional*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，将尝试恢复下载。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only(bool,` *optional*, 默认为 `False`) — 是否仅查看本地文件（即，不尝试下载模型）。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行 `huggingface-cli login` 时生成的令牌 (存储在 `~/.huggingface` 中)。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交 ID，因为我们在 huggingface.co 上使用基于 git 的系统来存储模型和其他工件，所以 `revision` 可以是 git 允许的任何标识符。

从预训练模型配置实例化一个预训练的 flax 模型。

警告 *来自 XXX 的权重未从预训练模型初始化* 意味着 XXX 的权重不是与模型的其余部分一起预训练的。您需要使用下游微调任务来训练这些权重。

警告 *来自 XXX 的权重在 YYY 中未使用* 意味着层 XXX 在 YYY 中未被使用，因此这些权重被丢弃。

示例：

```py
>>> from transformers import BertConfig, FlaxBertModel

>>> # Download model and configuration from huggingface.co and cache.
>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
>>> model = FlaxBertModel.from_pretrained("./test/saved_model/")
>>> # Loading from a PyTorch checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).
>>> config = BertConfig.from_json_file("./pt_model/config.json")
>>> model = FlaxBertModel.from_pretrained("./pt_model/pytorch_model.bin", from_pt=True, config=config)
```

#### `load_flax_sharded_weights`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L459)

```py
( shard_files ) → export const metadata = 'undefined';Dict
```

参数

+   `shard_files` (`List[str]` — 要加载的分片文件列表。

返回

`Dict`

模型参数的嵌套字典，符合 flax 模型的预期格式：`{'model': {'params': {'...'}}}`。

这与 `flax.serialization.from_bytes` 相同 (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes)，但适用于分片检查点。

这种加载效率很高：每个检查点分片都会逐个加载到 RAM 中，并在加载到模型后删除。

#### `register_for_auto_class`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1226)

```py
( auto_class = 'FlaxAutoModel' )
```

参数

+   `auto_class` (`str` 或 `type`, *可选*, 默认为 `"FlaxAutoModel"`) — 用于注册这个新模型的自动类。

使用给定的自动类注册此类。这应该仅用于自定义模型，因为库中的模型已经与自动类映射。

此 API 是实验性的，可能在下一个版本中有一些轻微的破坏性更改。

#### `save_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1088)

```py
( save_directory: Union params = None push_to_hub = False max_shard_size = '10GB' token: Union = None safe_serialization: bool = False **kwargs )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 要保存到的目录。如果不存在，将创建。

+   `push_to_hub` (`bool`, *可选*, 默认为 `False`) — 保存模型后是否将其推送到 Hugging Face 模型中心。您可以使用 `repo_id` 指定要推送到的存储库 (将默认为您的命名空间中的 `save_directory` 名称)。

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为 `"10GB"`) — 在分片之前检查点的最大大小。检查点分片将小于此大小。如果表示为字符串，需要是数字后跟一个单位 (如 `"5MB"`)。

    如果模型的单个权重大于 `max_shard_size`，它将在自己的检查点分片中，该分片将大于 `max_shard_size`。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行 `huggingface-cli login` 时生成的令牌 (存储在 `~/.huggingface` 中)。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给 push_to_hub() 方法的额外关键字参数。

+   `safe_serialization` (`bool`, *可选*, 默认为 `False`) — 是否使用 `safetensors` 或通过 msgpack 保存模型。

将模型及其配置文件保存到目录中，以便可以使用 `from_pretrained()` 类方法重新加载。

#### `to_bf16`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L329)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的 `PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 与 `params` 树具有相同结构的 `PyTree`。叶子应为布尔值，对于要转换的参数应为 `True`，对于要跳过的参数应为 `False`。

将浮点 `params` 转换为 `jax.numpy.bfloat16`。这将返回一个新的 `params` 树，不会直接在原地转换 `params`。

此方法可在 TPU 上使用，显式将模型参数转换为 bfloat16 精度，以进行完全的半精度训练或以 bfloat16 保存权重以用于推理，以节省内存并提高速度。

示例：

```py
>>> from transformers import FlaxBertModel

>>> # load model
>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision
>>> model.params = model.to_bf16(model.params)
>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)
>>> # then pass the mask as follows
>>> from flax import traverse_util

>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> flat_params = traverse_util.flatten_dict(model.params)
>>> mask = {
...     path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
...     for path in flat_params
... }
>>> mask = traverse_util.unflatten_dict(mask)
>>> model.params = model.to_bf16(model.params, mask)
```

#### `to_fp16`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L395)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的 `PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 与 `params` 树具有相同结构的 `PyTree`。叶子应为布尔值，对于要转换的参数应为 `True`，对于要跳过的参数应为 `False`。

将浮点 `params` 转换为 `jax.numpy.float16`。这将返回一个新的 `params` 树，不会直接在原地转换 `params`。

此方法可在 GPU 上使用，显式将模型参数转换为 float16 精度，以进行完全的半精度训练或以 float16 保存权重以用于推理，以节省内存并提高速度。

示例：

```py
>>> from transformers import FlaxBertModel

>>> # load model
>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> # By default, the model params will be in fp32, to cast these to float16
>>> model.params = model.to_fp16(model.params)
>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)
>>> # then pass the mask as follows
>>> from flax import traverse_util

>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> flat_params = traverse_util.flatten_dict(model.params)
>>> mask = {
...     path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
...     for path in flat_params
... }
>>> mask = traverse_util.unflatten_dict(mask)
>>> model.params = model.to_fp16(model.params, mask)
```

#### `to_fp32`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L368)

```py
( params: Union mask: Any = None )
```

参数

+   `params` (`Union[Dict, FrozenDict]`) — 模型参数的 `PyTree`。

+   `mask` (`Union[Dict, FrozenDict]`) — 与 `params` 树具有相同结构的 `PyTree`。叶子应为布尔值，对于要转换的参数应为 `True`，对于要跳过的参数应为 `False`。

将浮点 `params` 转换为 `jax.numpy.float32`。此方法可用于显式将模型参数转换为 fp32 精度。这将返回一个新的 `params` 树，不会直接在原地转换 `params`。

示例：

```py
>>> from transformers import FlaxBertModel

>>> # Download model and configuration from huggingface.co
>>> model = FlaxBertModel.from_pretrained("bert-base-cased")
>>> # By default, the model params will be in fp32, to illustrate the use of this method,
>>> # we'll first cast to fp16 and back to fp32
>>> model.params = model.to_f16(model.params)
>>> # now cast back to fp32
>>> model.params = model.to_fp32(model.params)
```

## 推送到 Hub

### `class transformers.utils.PushToHubMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L639)

```py
( )
```

一个包含将模型或分词器推送到 Hub 的功能的 Mixin。

#### `push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id` (`str`) — 您要将 {object} 推送到的存储库的名称。在推送到给定组织时，应包含您的组织名称。

+   `use_temp_dir` (`bool`，*可选*) — 是否使用临时目录存储在推送到 Hub 之前保存的文件。如果没有名为 `repo_id` 的目录，则默认为 `True`，否则为 `False`。

+   `commit_message` (`str`，*可选*) — 推送时要提交的消息。默认为 `"Upload {object}"`。

+   `private` (`bool`，*可选*) — 创建的存储库是否应为私有。

+   `token` (`bool` 或 `str`，*可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface`）。如果未指定 `repo_url`，则默认为 `True`。

+   `max_shard_size` (`int` 或 `str`，*可选*，默认为 `"5GB"`) — 仅适用于模型。在被分片之前的检查点的最大大小。然后检查点将被分成小于此大小的每个分片。如果表示为字符串，需要是数字后跟一个单位（如 `"5MB"`）。我们将其默认为 `"5GB"`，以便用户可以在免费的 Google Colab 实例上轻松加载模型，而不会出现任何 CPU OOM 问题。

+   `create_pr` (`bool`，*可选*，默认为 `False`) — 是否创建一个带有上传文件的 PR 或直接提交。

+   `safe_serialization` (`bool`, *可选*，默认为`True`) — 是否将模型权重转换为 safetensors 格式以进行更安全的序列化。

+   `revision` (`str`, *可选*) — 要将上传的文件推送到的分支。

+   `commit_description` (`str`, *可选*) — 将创建的提交描述

+   `tags` (`List[str]`, *可选*) — 要推送到中心的标签列表。

将{object_files}上传到🤗模型中心。

示例：

```py
from transformers import {object_class}

{object} = {object_class}.from_pretrained("bert-base-cased")

# Push the {object} to your namespace with the name "my-finetuned-bert".
{object}.push_to_hub("my-finetuned-bert")

# Push the {object} to an organization with the name "my-finetuned-bert".
{object}.push_to_hub("huggingface/my-finetuned-bert")
```

## 分片检查点

#### `transformers.modeling_utils.load_sharded_checkpoint`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L415)

```py
( model folder strict = True prefer_safe = True ) → export const metadata = 'undefined';NamedTuple
```

参数

+   `model` (`torch.nn.Module`) — 要加载检查点的模型。

+   `folder` (`str`或`os.PathLike`) — 包含分片检查点的文件夹路径。

+   `strict` (`bool`, *可选*，默认为`True`) — 是否严格执行模型状态字典中的键与分片检查点中的键匹配。

+   `prefer_safe` (`bool`, *可选*，默认为`False`) — 如果检查点中同时存在 safetensors 和 PyTorch 保存文件，并且`prefer_safe`为 True，则将加载 safetensors 文件。否则，尽可能加载 PyTorch 文件。

返回

`NamedTuple`

一个带有`missing_keys`和`unexpected_keys`字段的命名元组

+   `missing_keys`是一个包含缺失键的字符串列表

+   `unexpected_keys`是一个包含意外键的字符串列表

这与[`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)相同，但适用于分片检查点。

这种加载效率很高：每个检查点分片都会逐个在 RAM 中加载，加载到模型后会被删除。
