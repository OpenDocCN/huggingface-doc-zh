["```py\n>>> import torch\n>>> from transformers import BertTokenizer, VisualBertModel\n\n>>> model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n>>> inputs = tokenizer(\"What is the man eating?\", return_tensors=\"pt\")\n>>> # this is a custom function that returns the visual embeddings given the image path\n>>> visual_embeds = get_visual_embeddings(image_path)\n\n>>> visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n>>> visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n>>> inputs.update(\n...     {\n...         \"visual_embeds\": visual_embeds,\n...         \"visual_token_type_ids\": visual_token_type_ids,\n...         \"visual_attention_mask\": visual_attention_mask,\n...     }\n... )\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( vocab_size = 30522 hidden_size = 768 visual_embedding_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 bypass_transformer = False special_visual_initialize = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import VisualBertConfig, VisualBertModel\n\n>>> # Initializing a VisualBERT visualbert-vqa-coco-pre style configuration\n>>> configuration = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n\n>>> # Initializing a model (with random weights) from the visualbert-vqa-coco-pre style configuration\n>>> model = VisualBertModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image.\nfrom transformers import AutoTokenizer, VisualBertModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n\ninputs = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")\nvisual_embeds = get_visual_embeddings(image).unsqueeze(0)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n\ninputs.update(\n    {\n        \"visual_embeds\": visual_embeds,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"visual_attention_mask\": visual_attention_mask,\n    }\n)\n\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None sentence_image_labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\nfrom transformers import AutoTokenizer, VisualBertForPreTraining\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertForPreTraining.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n\ninputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\nvisual_embeds = get_visual_embeddings(image).unsqueeze(0)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n\ninputs.update(\n    {\n        \"visual_embeds\": visual_embeds,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"visual_attention_mask\": visual_attention_mask,\n    }\n)\nmax_length = inputs[\"input_ids\"].shape[-1] + visual_embeds.shape[-2]\nlabels = tokenizer(\n    \"The capital of France is Paris.\", return_tensors=\"pt\", padding=\"max_length\", max_length=max_length\n)[\"input_ids\"]\nsentence_image_labels = torch.tensor(1).unsqueeze(0)  # Batch_size\n\noutputs = model(**inputs, labels=labels, sentence_image_labels=sentence_image_labels)\nloss = outputs.loss\nprediction_logits = outputs.prediction_logits\nseq_relationship_logits = outputs.seq_relationship_logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\nfrom transformers import AutoTokenizer, VisualBertForQuestionAnswering\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n\ntext = \"Who is eating the apple?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nvisual_embeds = get_visual_embeddings(image).unsqueeze(0)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n\ninputs.update(\n    {\n        \"visual_embeds\": visual_embeds,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"visual_attention_mask\": visual_attention_mask,\n    }\n)\n\nlabels = torch.tensor([[0.0, 1.0]]).unsqueeze(0)  # Batch size 1, Num labels 2\n\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nscores = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\nfrom transformers import AutoTokenizer, VisualBertForMultipleChoice\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertForMultipleChoice.from_pretrained(\"uclanlp/visualbert-vcr\")\n\nprompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\nchoice0 = \"It is eaten with a fork and a knife.\"\nchoice1 = \"It is eaten while held in the hand.\"\n\nvisual_embeds = get_visual_embeddings(image)\n# (batch_size, num_choices, visual_seq_length, visual_embedding_dim)\nvisual_embeds = visual_embeds.expand(1, 2, *visual_embeds.shape)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n\nlabels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\nencoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors=\"pt\", padding=True)\n# batch size is 1\ninputs_dict = {k: v.unsqueeze(0) for k, v in encoding.items()}\ninputs_dict.update(\n    {\n        \"visual_embeds\": visual_embeds,\n        \"visual_attention_mask\": visual_attention_mask,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"labels\": labels,\n    }\n)\noutputs = model(**inputs_dict)\n\nloss = outputs.loss\nlogits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\nfrom transformers import AutoTokenizer, VisualBertForVisualReasoning\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertForVisualReasoning.from_pretrained(\"uclanlp/visualbert-nlvr2\")\n\ntext = \"Who is eating the apple?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nvisual_embeds = get_visual_embeddings(image).unsqueeze(0)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n\ninputs.update(\n    {\n        \"visual_embeds\": visual_embeds,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"visual_attention_mask\": visual_attention_mask,\n    }\n)\n\nlabels = torch.tensor(1).unsqueeze(0)  # Batch size 1, Num choices 2\n\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nscores = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None region_to_phrase_position: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.\nfrom transformers import AutoTokenizer, VisualBertForRegionToPhraseAlignment\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = VisualBertForRegionToPhraseAlignment.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n\ntext = \"Who is eating the apple?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nvisual_embeds = get_visual_embeddings(image).unsqueeze(0)\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\nregion_to_phrase_position = torch.ones((1, inputs[\"input_ids\"].shape[-1] + visual_embeds.shape[-2]))\n\ninputs.update(\n    {\n        \"region_to_phrase_position\": region_to_phrase_position,\n        \"visual_embeds\": visual_embeds,\n        \"visual_token_type_ids\": visual_token_type_ids,\n        \"visual_attention_mask\": visual_attention_mask,\n    }\n)\n\nlabels = torch.ones(\n    (1, inputs[\"input_ids\"].shape[-1] + visual_embeds.shape[-2], visual_embeds.shape[-2])\n)  # Batch size 1\n\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nscores = outputs.logits\n```"]