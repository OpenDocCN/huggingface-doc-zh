- en: Export to TFLite
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导出到 TFLite
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tflite](https://huggingface.co/docs/transformers/v4.37.2/en/tflite)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tflite](https://huggingface.co/docs/transformers/v4.37.2/en/tflite)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[TensorFlow Lite](https://www.tensorflow.org/lite/guide) is a lightweight framework
    for deploying machine learning models on resource-constrained devices, such as
    mobile phones, embedded systems, and Internet of Things (IoT) devices. TFLite
    is designed to optimize and run models efficiently on these devices with limited
    computational power, memory, and power consumption. A TensorFlow Lite model is
    represented in a special efficient portable format identified by the `.tflite`
    file extension.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[TensorFlow Lite](https://www.tensorflow.org/lite/guide) 是一个轻量级框架，用于在资源受限的设备上部署机器学习模型，例如移动电话、嵌入式系统和物联网（IoT）设备。TFLite
    旨在优化并在这些计算能力、内存和功耗有限的设备上高效运行模型。TensorFlow Lite 模型以一种特殊的高效可移植格式表示，通过 `.tflite`
    文件扩展名进行识别。'
- en: 🤗 Optimum offers functionality to export 🤗 Transformers models to TFLite through
    the `exporters.tflite` module. For the list of supported model architectures,
    please refer to [🤗 Optimum documentation](https://huggingface.co/docs/optimum/exporters/tflite/overview).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Optimum 提供了通过 `exporters.tflite` 模块将 🤗 Transformers 模型导出到 TFLite 的功能。有关支持的模型架构列表，请参考
    [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/tflite/overview)。
- en: 'To export a model to TFLite, install the required dependencies:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型导出到 TFLite，请安装所需的依赖项：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To check out all available arguments, refer to the [🤗 Optimum docs](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model),
    or view help in command line:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有可用参数，请参考 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model)，或在命令行中查看帮助：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To export a model’s checkpoint from the 🤗 Hub, for example, `bert-base-uncased`,
    run the following command:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要从 🤗 Hub 导出模型的检查点，比如 `bert-base-uncased`，请运行以下命令：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the logs indicating progress and showing where the resulting
    `model.tflite` is saved, like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到指示进度并显示结果 `model.tflite` 保存位置的日志。
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The example above illustrates exporting a checkpoint from 🤗 Hub. When exporting
    a local model, first make sure that you saved both the model’s weights and tokenizer
    files in the same directory (`local_path`). When using CLI, pass the `local_path`
    to the `model` argument instead of the checkpoint name on 🤗 Hub.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例说明了从 🤗 Hub 导出检查点。在导出本地模型时，首先确保您将模型的权重和分词器文件保存在同一个目录（`local_path`）中。在使用
    CLI 时，将 `local_path` 传递给 `model` 参数，而不是在 🤗 Hub 上的检查点名称。
