- en: Glossary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语表
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/glossary](https://huggingface.co/learn/deep-rl-course/unit3/glossary)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unit3/glossary](https://huggingface.co/learn/deep-rl-course/unit3/glossary)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcomed!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个由社区创建的术语表。欢迎贡献！
- en: '**Tabular Method:** Type of problem in which the state and action spaces are
    small enough to approximate value functions to be represented as arrays and tables.
    **Q-learning** is an example of tabular method since a table is used to represent
    the value for different state-action pairs.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格方法：**状态和动作空间足够小，可以将值函数近似表示为数组和表格的问题类型。**Q学习**是表格方法的一个例子，因为使用表格来表示不同状态-动作对的值。'
- en: '**Deep Q-Learning:** Method that trains a neural network to approximate, given
    a state, the different **Q-values** for each possible action at that state. It
    is used to solve problems when observational space is too big to apply a tabular
    Q-Learning approach.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度Q学习：**训练神经网络来近似给定状态下每个可能动作的不同**Q值**。它用于解决观察空间太大无法应用表格Q学习方法的问题。'
- en: '**Temporal Limitation** is a difficulty presented when the environment state
    is represented by frames. A frame by itself does not provide temporal information.
    In order to obtain temporal information, we need to **stack** a number of frames
    together.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间限制**是当环境状态由帧表示时出现的困难。单独的帧不提供时间信息。为了获得时间信息，我们需要将一定数量的帧**堆叠**在一起。'
- en: '**Phases of Deep Q-Learning:**'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度Q学习的阶段：**'
- en: '**Sampling:** Actions are performed, and observed experience tuples are stored
    in a **replay memory**.'
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样：**执行动作，并将观察到的经验元组存储在**回放内存**中。'
- en: '**Training:** Batches of tuples are selected randomly and the neural network
    updates its weights using gradient descent.'
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练：**随机选择元组的批次，并使用梯度下降更新神经网络的权重。'
- en: '**Solutions to stabilize Deep Q-Learning:**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定深度Q学习的解决方案：**'
- en: '**Experience Replay:** A replay memory is created to save experiences samples
    that can be reused during training. This allows the agent to learn from the same
    experiences multiple times. Also, it helps the agent avoid forgetting previous
    experiences as it gets new ones.'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验回放：**创建一个回放内存来保存可以在训练过程中重复使用的经验样本。这使得代理可以多次从相同的经验中学习。此外，它帮助代理避免忘记之前的经验，因为它获得了新的经验。'
- en: '**Random sampling** from replay buffer allows to remove correlation in the
    observation sequences and prevents action values from oscillating or diverging
    catastrophically.'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从回放缓冲区中随机抽样**可以消除观察序列中的相关性，并防止动作值振荡或灾难性分歧。'
- en: '**Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate
    the discounted optimal **Q-value** of the next state by using Bellman equation.
    The problem is that the same network weights are used to calculate the **Q-Target**
    and the **Q-value**. This means that everytime we are modifying the **Q-value**,
    the **Q-Target** also moves with it. To avoid this issue, a separate network with
    fixed parameters is used for estimating the Temporal Difference Target. The target
    network is updated by copying parameters from our Deep Q-Network after certain
    **C steps**.'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**固定Q-目标：**为了计算**Q-目标**，我们需要使用贝尔曼方程估计下一个状态的折扣最优**Q值**。问题在于计算**Q-目标**和**Q值**时使用相同的网络权重。这意味着每次修改**Q值**时，**Q-目标**也会随之移动。为了避免这个问题，使用具有固定参数的单独网络来估计时间差异目标。目标网络通过从我们的深度Q网络复制参数来更新，在一定的**C步**之后。'
- en: '**Double DQN:** Method to handle **overestimation** of **Q-Values**. This solution
    uses two networks to decouple the action selection from the target **Value generation**:'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双重DQN：**处理**Q值**的**高估**的方法。此解决方案使用两个网络来解耦动作选择和目标**值生成**：'
- en: '**DQN Network** to select the best action to take for the next state (the action
    with the highest **Q-Value**)'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN网络**选择下一个状态的最佳动作（具有最高**Q值**的动作）'
- en: '**Target Network** to calculate the target **Q-Value** of taking that action
    at the next state. This approach reduces the **Q-Values** overestimation, it helps
    to train faster and have more stable learning.'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标网络**用于计算在下一个状态采取该动作的目标**Q值**。这种方法减少了**Q值**的高估，有助于更快地训练并具有更稳定的学习。'
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想改进课程，您可以[发起一个拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)
- en: 'This glossary was made possible thanks to:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语表得以实现，感谢：
- en: '[Dario Paez](https://github.com/dario248)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dario Paez](https://github.com/dario248)'
