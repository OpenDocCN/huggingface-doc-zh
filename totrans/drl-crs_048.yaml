- en: Glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/glossary](https://huggingface.co/learn/deep-rl-course/unit3/glossary)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcomed!
  prefs: []
  type: TYPE_NORMAL
- en: '**Tabular Method:** Type of problem in which the state and action spaces are
    small enough to approximate value functions to be represented as arrays and tables.
    **Q-learning** is an example of tabular method since a table is used to represent
    the value for different state-action pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Q-Learning:** Method that trains a neural network to approximate, given
    a state, the different **Q-values** for each possible action at that state. It
    is used to solve problems when observational space is too big to apply a tabular
    Q-Learning approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal Limitation** is a difficulty presented when the environment state
    is represented by frames. A frame by itself does not provide temporal information.
    In order to obtain temporal information, we need to **stack** a number of frames
    together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Phases of Deep Q-Learning:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling:** Actions are performed, and observed experience tuples are stored
    in a **replay memory**.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training:** Batches of tuples are selected randomly and the neural network
    updates its weights using gradient descent.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solutions to stabilize Deep Q-Learning:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experience Replay:** A replay memory is created to save experiences samples
    that can be reused during training. This allows the agent to learn from the same
    experiences multiple times. Also, it helps the agent avoid forgetting previous
    experiences as it gets new ones.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random sampling** from replay buffer allows to remove correlation in the
    observation sequences and prevents action values from oscillating or diverging
    catastrophically.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate
    the discounted optimal **Q-value** of the next state by using Bellman equation.
    The problem is that the same network weights are used to calculate the **Q-Target**
    and the **Q-value**. This means that everytime we are modifying the **Q-value**,
    the **Q-Target** also moves with it. To avoid this issue, a separate network with
    fixed parameters is used for estimating the Temporal Difference Target. The target
    network is updated by copying parameters from our Deep Q-Network after certain
    **C steps**.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double DQN:** Method to handle **overestimation** of **Q-Values**. This solution
    uses two networks to decouple the action selection from the target **Value generation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DQN Network** to select the best action to take for the next state (the action
    with the highest **Q-Value**)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target Network** to calculate the target **Q-Value** of taking that action
    at the next state. This approach reduces the **Q-Values** overestimation, it helps
    to train faster and have more stable learning.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  prefs: []
  type: TYPE_NORMAL
- en: 'This glossary was made possible thanks to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dario Paez](https://github.com/dario248)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
