# Pop2Piano

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano)

[![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/sweetcocoa/pop2piano)

## 概述

Pop2Piano模型由Jongho Choi和Kyogu Lee在[Pop2Piano：基于流行音频的钢琴翻奏生成](https://arxiv.org/abs/2211.00895)中提出。

流行音乐的钢琴翻奏广受欢迎，但从音乐中生成它们并不是一项简单的任务。这需要对弹奏钢琴有很高的专业知识，同时还要了解歌曲的不同特征和旋律。通过Pop2Piano，您可以直接从歌曲的音频波形生成翻奏。这是第一个直接从流行音频生成钢琴翻奏的模型，而无需旋律和和弦提取模块。

Pop2Piano是基于[T5](https://arxiv.org/pdf/1910.10683.pdf)的编码器-解码器Transformer模型。输入音频被转换为其波形并传递给编码器，编码器将其转换为潜在表示。解码器使用这些潜在表示以自回归方式生成令牌id。每个令牌id对应于四种不同的令牌类型之一：时间、速度、音符和“特殊”。然后将令牌id解码为其等效的MIDI文件。

论文摘要如下：

*许多人喜欢流行音乐的钢琴翻奏。然而，自动生成流行音乐的钢琴翻奏的任务仍然未被充分研究。部分原因是缺乏同步的{流行音乐，钢琴翻奏}数据对，这使得应用最新的数据密集型基于深度学习的方法具有挑战性。为了利用数据驱动方法的力量，我们使用自动化流水线制作了大量配对和同步的{流行音乐，钢琴翻奏}数据。在本文中，我们提出了Pop2Piano，这是一个Transformer网络，可以根据流行音乐的波形生成钢琴翻奏。据我们所知，这是第一个可以直接从流行音频生成钢琴翻奏的模型，而无需使用旋律和和弦提取模块。我们展示了使用我们的数据集训练的Pop2Piano能够生成合理的钢琴翻奏。*

此模型由[Susnato Dhar](https://huggingface.co/susnato)贡献。原始代码可以在[此处](https://github.com/sweetcocoa/pop2piano)找到。

## 使用提示

+   要使用Pop2Piano，您需要安装🤗 Transformers库，以及以下第三方模块：

```py
pip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy
```

请注意，您可能需要在安装后重新启动运行时。

+   Pop2Piano是一种基于编码器-解码器的模型，类似于T5。

+   Pop2Piano可用于为给定音频序列生成midi音频文件。

+   在`Pop2PianoForConditionalGeneration.generate()`中选择不同的作曲家可以产生不同结果的多样性。

+   在加载音频文件时将采样率设置为44.1 kHz可以获得良好的性能。

+   尽管Pop2Piano主要是在韩国流行音乐上进行训练的，但它在其他西方流行音乐或嘻哈歌曲上也表现不错。

## 示例

+   使用HuggingFace数据集的示例：

```py
>>> from datasets import load_dataset
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")
>>> ds = load_dataset("sweetcocoa/pop2piano_ci", split="test")

>>> inputs = processor(
...     audio=ds["audio"][0]["array"], sampling_rate=ds["audio"][0]["sampling_rate"], return_tensors="pt"
... )
>>> model_output = model.generate(input_features=inputs["input_features"], composer="composer1")
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"][0]
>>> tokenizer_output.write("./Outputs/midi_output.mid")
```

+   使用您自己的音频文件示例：

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> audio, sr = librosa.load("<your_audio_file_here>", sr=44100)  # feel free to change the sr to a suitable value.
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = processor(audio=audio, sampling_rate=sr, return_tensors="pt")
>>> model_output = model.generate(input_features=inputs["input_features"], composer="composer1")
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"][0]
>>> tokenizer_output.write("./Outputs/midi_output.mid")
```

+   批量处理多个音频文件的示例：

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> # feel free to change the sr to a suitable value.
>>> audio1, sr1 = librosa.load("<your_first_audio_file_here>", sr=44100)  
>>> audio2, sr2 = librosa.load("<your_second_audio_file_here>", sr=44100)
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = processor(audio=[audio1, audio2], sampling_rate=[sr1, sr2], return_attention_mask=True, return_tensors="pt")
>>> # Since we now generating in batch(2 audios) we must pass the attention_mask
>>> model_output = model.generate(
...     input_features=inputs["input_features"],
...     attention_mask=inputs["attention_mask"],
...     composer="composer1",
... )
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"]

>>> # Since we now have 2 generated MIDI files
>>> tokenizer_output[0].write("./Outputs/midi_output1.mid")
>>> tokenizer_output[1].write("./Outputs/midi_output2.mid")
```

+   批量处理多个音频文件的示例（使用`Pop2PianoFeatureExtractor`和`Pop2PianoTokenizer`）：

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer

>>> # feel free to change the sr to a suitable value.
>>> audio1, sr1 = librosa.load("<your_first_audio_file_here>", sr=44100)  
>>> audio2, sr2 = librosa.load("<your_second_audio_file_here>", sr=44100)
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> feature_extractor = Pop2PianoFeatureExtractor.from_pretrained("sweetcocoa/pop2piano")
>>> tokenizer = Pop2PianoTokenizer.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = feature_extractor(
...     audio=[audio1, audio2], 
...     sampling_rate=[sr1, sr2], 
...     return_attention_mask=True, 
...     return_tensors="pt",
... )
>>> # Since we now generating in batch(2 audios) we must pass the attention_mask
>>> model_output = model.generate(
...     input_features=inputs["input_features"],
...     attention_mask=inputs["attention_mask"],
...     composer="composer1",
... )
>>> tokenizer_output = tokenizer.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"]

>>> # Since we now have 2 generated MIDI files
>>> tokenizer_output[0].write("./Outputs/midi_output1.mid")
>>> tokenizer_output[1].write("./Outputs/midi_output2.mid")
```

## Pop2PianoConfig

### `class transformers.Pop2PianoConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/configuration_pop2piano.py#L29)

```py
( vocab_size = 2400 composer_vocab_size = 21 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder = True use_cache = True pad_token_id = 0 eos_token_id = 1 dense_act_fn = 'relu' **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为2400）- `Pop2PianoForConditionalGeneration`模型的词汇量。定义了在调用[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)时可以表示的不同令牌数量。

+   `composer_vocab_size`（`int`，*可选*，默认为21）- 表示作曲家的数量。

+   `d_model` (`int`, *可选*, 默认为512) — 编码器层和池化层的大小。

+   `d_kv` (`int`, *可选*, 默认为64) — 每个注意力头中键、查询、值投影的大小。投影层的`inner_dim`将定义为`num_heads * d_kv`。

+   `d_ff` (`int`, *可选*, 默认为2048) — 每个`Pop2PianoBlock`中间级前馈层的大小。

+   `num_layers` (`int`, *可选*, 默认为6) — Transformer编码器中的隐藏层数量。

+   `num_decoder_layers` (`int`, *可选*) — Transformer解码器中的隐藏层数量。如果未设置，将使用与`num_layers`相同的值。

+   `num_heads` (`int`, *可选*, 默认为8) — Transformer编码器中每个注意力层的注意力头数量。

+   `relative_attention_num_buckets` (`int`, *可选*, 默认为32) — 每个注意力层使用的桶数量。

+   `relative_attention_max_distance` (`int`, *可选*, 默认为128) — 用于桶分离的较长序列的最大距离。

+   `dropout_rate` (`float`, *可选*, 默认为0.1) — 所有dropout层的比率。

+   `layer_norm_epsilon` (`float`, *可选*, 默认为1e-6) — 层归一化层使用的epsilon。

+   `initializer_factor` (`float`, *可选*, 默认为1.0) — 初始化所有权重矩阵的因子（应保持为1.0，用于内部初始化测试）。

+   `feed_forward_proj` (`string`, *可选*, 默认为`"gated-gelu"`) — 要使用的前馈层类型。应为`"relu"`或`"gated-gelu"`之一。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `dense_act_fn` (`string`, *可选*, 默认为`"relu"`) — 用于`Pop2PianoDenseActDense`和`Pop2PianoDenseGatedActDense`中的激活函数类型。

这是用于存储[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)配置的配置类。根据指定的参数实例化一个Pop2PianoForConditionalGeneration模型，定义模型架构。使用默认值实例化配置将产生类似于Pop2Piano [sweetcocoa/pop2piano](https://huggingface.co/sweetcocoa/pop2piano)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

## Pop2PianoFeatureExtractor

### `class transformers.Pop2PianoFeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L5)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

将自身作为函数调用。

## Pop2PianoForConditionalGeneration

### `class transformers.Pop2PianoForConditionalGeneration`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1009)

```py
( config: Pop2PianoConfig )
```

参数

+   `config` ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Pop2Piano模型在顶部带有`语言建模`头。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `前向`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1109)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None input_features: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。Pop2Piano是一个带有相对位置嵌入的模型，因此您应该能够在右侧和左侧填充输入。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。[什么是输入ID？](../glossary#input-ids)要了解有关如何为预训练准备`input_ids`的更多信息，请查看[Pop2Piano训练](./Pop2Piano#training)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`之间：

    +   对于`未屏蔽`的标记，

    +   0表示标记为`屏蔽`的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。[什么是解码器输入ID？](../glossary#decoder-input-ids)Pop2Piano使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，则只需选择最后的`decoder_input_ids`输入（请参阅`past_key_values`）。要了解如何准备

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）— 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。默认情况下还将使用因果掩码。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使编码器中自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`之间：

    +   1表示头部未被`屏蔽`，

    +   0表示头部被`屏蔽`。

+   `decoder_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使解码器中自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`之间：

    +   1表示头部未被`屏蔽`，

    +   0表示头部被`屏蔽`。

+   `cross_attn_head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于使解码器中交叉注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`之间：

    +   1表示头部未被`屏蔽`，

    +   0表示头部被`屏蔽`。

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — 元组包括（`last_hidden_state`，可选：*hidden_states*，可选：*attentions*）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组包含形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的4个张量） — 包含注意力块的预计算的键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果要更好地控制如何将`input_ids`索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。

+   `input_features` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 执行与`inputs_embeds`相同的任务。如果不存在`inputs_embeds`但存在`input_features`，则将`input_features`视为`inputs_embeds`。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在`[-100, 0, ..., config.vocab_size - 1]`内。所有标签设置为`-100`的将被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`内的。

返回

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig)）和输入而异的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，在提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回 — 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可以用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回 — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    解码器在每一层的输出隐藏状态以及初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回 — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    编码器在每一层的输出隐藏状态以及初始嵌入输出。

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1217)

```py
( input_features attention_mask = None composer = 'composer1' generation_config = None **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `input_features` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) — 这是由 `Pop2PianoFeatureExtractor` 生成的音频的特征化版本。attention_mask — 对于批量生成，`input_features` 被填充以使所有示例具有相同的形状。`attention_mask` 有助于确定哪些区域被填充，哪些没有被填充。

    +   1 用于 `未填充` 的标记。

    +   0 用于 `填充` 的标记。

+   `composer` (`str`, *optional*, defaults to `"composer1"`) — 传递给 `Pop2PianoConcatEmbeddingToMel` 的值，用于为每个 `"composer"` 生成不同的嵌入。请确保 `composer` 值在 `generation_config` 的 `composer_to_feature_token` 中存在。例如，请参阅 [https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json](https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json)。

+   `generation_config` (`~generation.GenerationConfig`, *optional*) — 用作生成调用的基本参数化的生成配置。传递给 generate 的 `**kwargs` 匹配 `generation_config` 的属性将覆盖它们。如果未提供 `generation_config`，将使用默认值，其加载优先级如下：1) 从 `generation_config.json` 模型文件中，如果存在；2) 从模型配置中。请注意，未指定的参数将继承 [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) 的默认值，其文档应该被检查以参数化生成。kwargs — `generate_config` 的特定参数化和/或将转发到模型的 `forward` 函数的其他模型特定 kwargs。如果模型是编码器-解码器模型，则编码器特定的 kwargs 不应该有前缀，解码器特定的 kwargs 应该以 *decoder_* 为前缀。

返回

[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 或 `torch.LongTensor`

一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果 `return_dict_in_generate=True` 或当 `config.return_dict_in_generate=True` 时）或一个 `torch.FloatTensor`。由于 Pop2Piano 是一个编码器-解码器模型（`model.config.is_encoder_decoder=True`），可能的 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 类型为：

+   [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),

+   [GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)

为 MIDI 输出生成标记 ID。

大多数控制生成的参数都在 `generation_config` 中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过传递相应的参数到 generate() 来覆盖任何 `generation_config`，例如 `.generate(inputs, num_beams=4, do_sample=True)`。有关生成策略和代码示例的概述，请查看[以下指南](./generation_strategies)。

## Pop2PianoTokenizer

### `class transformers.Pop2PianoTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L12)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

将自身作为函数调用。

## Pop2PianoProcessor

### `class transformers.Pop2PianoProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L19)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

将自身作为函数调用。
