# Pop2Piano

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano)

[![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/sweetcocoa/pop2piano)

## æ¦‚è¿°

Pop2Pianoæ¨¡å‹ç”±Jongho Choiå’ŒKyogu Leeåœ¨[Pop2Pianoï¼šåŸºäºæµè¡ŒéŸ³é¢‘çš„é’¢ç´ç¿»å¥ç”Ÿæˆ](https://arxiv.org/abs/2211.00895)ä¸­æå‡ºã€‚

æµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥å¹¿å—æ¬¢è¿ï¼Œä½†ä»éŸ³ä¹ä¸­ç”Ÿæˆå®ƒä»¬å¹¶ä¸æ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ã€‚è¿™éœ€è¦å¯¹å¼¹å¥é’¢ç´æœ‰å¾ˆé«˜çš„ä¸“ä¸šçŸ¥è¯†ï¼ŒåŒæ—¶è¿˜è¦äº†è§£æ­Œæ›²çš„ä¸åŒç‰¹å¾å’Œæ—‹å¾‹ã€‚é€šè¿‡Pop2Pianoï¼Œæ‚¨å¯ä»¥ç›´æ¥ä»æ­Œæ›²çš„éŸ³é¢‘æ³¢å½¢ç”Ÿæˆç¿»å¥ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç›´æ¥ä»æµè¡ŒéŸ³é¢‘ç”Ÿæˆé’¢ç´ç¿»å¥çš„æ¨¡å‹ï¼Œè€Œæ— éœ€æ—‹å¾‹å’Œå’Œå¼¦æå–æ¨¡å—ã€‚

Pop2Pianoæ˜¯åŸºäº[T5](https://arxiv.org/pdf/1910.10683.pdf)çš„ç¼–ç å™¨-è§£ç å™¨Transformeræ¨¡å‹ã€‚è¾“å…¥éŸ³é¢‘è¢«è½¬æ¢ä¸ºå…¶æ³¢å½¢å¹¶ä¼ é€’ç»™ç¼–ç å™¨ï¼Œç¼–ç å™¨å°†å…¶è½¬æ¢ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚è§£ç å™¨ä½¿ç”¨è¿™äº›æ½œåœ¨è¡¨ç¤ºä»¥è‡ªå›å½’æ–¹å¼ç”Ÿæˆä»¤ç‰Œidã€‚æ¯ä¸ªä»¤ç‰Œidå¯¹åº”äºå››ç§ä¸åŒçš„ä»¤ç‰Œç±»å‹ä¹‹ä¸€ï¼šæ—¶é—´ã€é€Ÿåº¦ã€éŸ³ç¬¦å’Œâ€œç‰¹æ®Šâ€ã€‚ç„¶åå°†ä»¤ç‰Œidè§£ç ä¸ºå…¶ç­‰æ•ˆçš„MIDIæ–‡ä»¶ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*è®¸å¤šäººå–œæ¬¢æµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨ç”Ÿæˆæµè¡ŒéŸ³ä¹çš„é’¢ç´ç¿»å¥çš„ä»»åŠ¡ä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ã€‚éƒ¨åˆ†åŸå› æ˜¯ç¼ºä¹åŒæ­¥çš„{æµè¡ŒéŸ³ä¹ï¼Œé’¢ç´ç¿»å¥}æ•°æ®å¯¹ï¼Œè¿™ä½¿å¾—åº”ç”¨æœ€æ–°çš„æ•°æ®å¯†é›†å‹åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†åˆ©ç”¨æ•°æ®é©±åŠ¨æ–¹æ³•çš„åŠ›é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿åˆ¶ä½œäº†å¤§é‡é…å¯¹å’ŒåŒæ­¥çš„{æµè¡ŒéŸ³ä¹ï¼Œé’¢ç´ç¿»å¥}æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Pop2Pianoï¼Œè¿™æ˜¯ä¸€ä¸ªTransformerç½‘ç»œï¼Œå¯ä»¥æ ¹æ®æµè¡ŒéŸ³ä¹çš„æ³¢å½¢ç”Ÿæˆé’¢ç´ç¿»å¥ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯ä»¥ç›´æ¥ä»æµè¡ŒéŸ³é¢‘ç”Ÿæˆé’¢ç´ç¿»å¥çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä½¿ç”¨æ—‹å¾‹å’Œå’Œå¼¦æå–æ¨¡å—ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†è®­ç»ƒçš„Pop2Pianoèƒ½å¤Ÿç”Ÿæˆåˆç†çš„é’¢ç´ç¿»å¥ã€‚*

æ­¤æ¨¡å‹ç”±[Susnato Dhar](https://huggingface.co/susnato)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/sweetcocoa/pop2piano)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   è¦ä½¿ç”¨Pop2Pianoï¼Œæ‚¨éœ€è¦å®‰è£…ğŸ¤— Transformersåº“ï¼Œä»¥åŠä»¥ä¸‹ç¬¬ä¸‰æ–¹æ¨¡å—ï¼š

```py
pip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨å®‰è£…åé‡æ–°å¯åŠ¨è¿è¡Œæ—¶ã€‚

+   Pop2Pianoæ˜¯ä¸€ç§åŸºäºç¼–ç å™¨-è§£ç å™¨çš„æ¨¡å‹ï¼Œç±»ä¼¼äºT5ã€‚

+   Pop2Pianoå¯ç”¨äºä¸ºç»™å®šéŸ³é¢‘åºåˆ—ç”ŸæˆmidiéŸ³é¢‘æ–‡ä»¶ã€‚

+   åœ¨`Pop2PianoForConditionalGeneration.generate()`ä¸­é€‰æ‹©ä¸åŒçš„ä½œæ›²å®¶å¯ä»¥äº§ç”Ÿä¸åŒç»“æœçš„å¤šæ ·æ€§ã€‚

+   åœ¨åŠ è½½éŸ³é¢‘æ–‡ä»¶æ—¶å°†é‡‡æ ·ç‡è®¾ç½®ä¸º44.1 kHzå¯ä»¥è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚

+   å°½ç®¡Pop2Pianoä¸»è¦æ˜¯åœ¨éŸ©å›½æµè¡ŒéŸ³ä¹ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†å®ƒåœ¨å…¶ä»–è¥¿æ–¹æµè¡ŒéŸ³ä¹æˆ–å˜»å“ˆæ­Œæ›²ä¸Šä¹Ÿè¡¨ç°ä¸é”™ã€‚

## ç¤ºä¾‹

+   ä½¿ç”¨HuggingFaceæ•°æ®é›†çš„ç¤ºä¾‹ï¼š

```py
>>> from datasets import load_dataset
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")
>>> ds = load_dataset("sweetcocoa/pop2piano_ci", split="test")

>>> inputs = processor(
...     audio=ds["audio"][0]["array"], sampling_rate=ds["audio"][0]["sampling_rate"], return_tensors="pt"
... )
>>> model_output = model.generate(input_features=inputs["input_features"], composer="composer1")
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"][0]
>>> tokenizer_output.write("./Outputs/midi_output.mid")
```

+   ä½¿ç”¨æ‚¨è‡ªå·±çš„éŸ³é¢‘æ–‡ä»¶ç¤ºä¾‹ï¼š

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> audio, sr = librosa.load("<your_audio_file_here>", sr=44100)  # feel free to change the sr to a suitable value.
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = processor(audio=audio, sampling_rate=sr, return_tensors="pt")
>>> model_output = model.generate(input_features=inputs["input_features"], composer="composer1")
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"][0]
>>> tokenizer_output.write("./Outputs/midi_output.mid")
```

+   æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç¤ºä¾‹ï¼š

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoProcessor

>>> # feel free to change the sr to a suitable value.
>>> audio1, sr1 = librosa.load("<your_first_audio_file_here>", sr=44100)  
>>> audio2, sr2 = librosa.load("<your_second_audio_file_here>", sr=44100)
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> processor = Pop2PianoProcessor.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = processor(audio=[audio1, audio2], sampling_rate=[sr1, sr2], return_attention_mask=True, return_tensors="pt")
>>> # Since we now generating in batch(2 audios) we must pass the attention_mask
>>> model_output = model.generate(
...     input_features=inputs["input_features"],
...     attention_mask=inputs["attention_mask"],
...     composer="composer1",
... )
>>> tokenizer_output = processor.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"]

>>> # Since we now have 2 generated MIDI files
>>> tokenizer_output[0].write("./Outputs/midi_output1.mid")
>>> tokenizer_output[1].write("./Outputs/midi_output2.mid")
```

+   æ‰¹é‡å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç¤ºä¾‹ï¼ˆä½¿ç”¨`Pop2PianoFeatureExtractor`å’Œ`Pop2PianoTokenizer`ï¼‰ï¼š

```py
>>> import librosa
>>> from transformers import Pop2PianoForConditionalGeneration, Pop2PianoFeatureExtractor, Pop2PianoTokenizer

>>> # feel free to change the sr to a suitable value.
>>> audio1, sr1 = librosa.load("<your_first_audio_file_here>", sr=44100)  
>>> audio2, sr2 = librosa.load("<your_second_audio_file_here>", sr=44100)
>>> model = Pop2PianoForConditionalGeneration.from_pretrained("sweetcocoa/pop2piano")
>>> feature_extractor = Pop2PianoFeatureExtractor.from_pretrained("sweetcocoa/pop2piano")
>>> tokenizer = Pop2PianoTokenizer.from_pretrained("sweetcocoa/pop2piano")

>>> inputs = feature_extractor(
...     audio=[audio1, audio2], 
...     sampling_rate=[sr1, sr2], 
...     return_attention_mask=True, 
...     return_tensors="pt",
... )
>>> # Since we now generating in batch(2 audios) we must pass the attention_mask
>>> model_output = model.generate(
...     input_features=inputs["input_features"],
...     attention_mask=inputs["attention_mask"],
...     composer="composer1",
... )
>>> tokenizer_output = tokenizer.batch_decode(
...     token_ids=model_output, feature_extractor_output=inputs
... )["pretty_midi_objects"]

>>> # Since we now have 2 generated MIDI files
>>> tokenizer_output[0].write("./Outputs/midi_output1.mid")
>>> tokenizer_output[1].write("./Outputs/midi_output2.mid")
```

## Pop2PianoConfig

### `class transformers.Pop2PianoConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/configuration_pop2piano.py#L29)

```py
( vocab_size = 2400 composer_vocab_size = 21 d_model = 512 d_kv = 64 d_ff = 2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder = True use_cache = True pad_token_id = 0 eos_token_id = 1 dense_act_fn = 'relu' **kwargs )
```

å‚æ•°

+   `vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º2400ï¼‰- `Pop2PianoForConditionalGeneration`æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚

+   `composer_vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º21ï¼‰- è¡¨ç¤ºä½œæ›²å®¶çš„æ•°é‡ã€‚

+   `d_model` (`int`, *å¯é€‰*, é»˜è®¤ä¸º512) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„å¤§å°ã€‚

+   `d_kv` (`int`, *å¯é€‰*, é»˜è®¤ä¸º64) â€” æ¯ä¸ªæ³¨æ„åŠ›å¤´ä¸­é”®ã€æŸ¥è¯¢ã€å€¼æŠ•å½±çš„å¤§å°ã€‚æŠ•å½±å±‚çš„`inner_dim`å°†å®šä¹‰ä¸º`num_heads * d_kv`ã€‚

+   `d_ff` (`int`, *å¯é€‰*, é»˜è®¤ä¸º2048) â€” æ¯ä¸ª`Pop2PianoBlock`ä¸­é—´çº§å‰é¦ˆå±‚çš„å¤§å°ã€‚

+   `num_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º6) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_decoder_layers` (`int`, *å¯é€‰*) â€” Transformerè§£ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ä¸`num_layers`ç›¸åŒçš„å€¼ã€‚

+   `num_heads` (`int`, *å¯é€‰*, é»˜è®¤ä¸º8) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `relative_attention_num_buckets` (`int`, *å¯é€‰*, é»˜è®¤ä¸º32) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚ä½¿ç”¨çš„æ¡¶æ•°é‡ã€‚

+   `relative_attention_max_distance` (`int`, *å¯é€‰*, é»˜è®¤ä¸º128) â€” ç”¨äºæ¡¶åˆ†ç¦»çš„è¾ƒé•¿åºåˆ—çš„æœ€å¤§è·ç¦»ã€‚

+   `dropout_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º0.1) â€” æ‰€æœ‰dropoutå±‚çš„æ¯”ç‡ã€‚

+   `layer_norm_epsilon` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1e-6) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `initializer_factor` (`float`, *å¯é€‰*, é»˜è®¤ä¸º1.0) â€” åˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„å› å­ï¼ˆåº”ä¿æŒä¸º1.0ï¼Œç”¨äºå†…éƒ¨åˆå§‹åŒ–æµ‹è¯•ï¼‰ã€‚

+   `feed_forward_proj` (`string`, *å¯é€‰*, é»˜è®¤ä¸º`"gated-gelu"`) â€” è¦ä½¿ç”¨çš„å‰é¦ˆå±‚ç±»å‹ã€‚åº”ä¸º`"relu"`æˆ–`"gated-gelu"`ä¹‹ä¸€ã€‚

+   `use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `dense_act_fn` (`string`, *å¯é€‰*, é»˜è®¤ä¸º`"relu"`) â€” ç”¨äº`Pop2PianoDenseActDense`å’Œ`Pop2PianoDenseGatedActDense`ä¸­çš„æ¿€æ´»å‡½æ•°ç±»å‹ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªPop2PianoForConditionalGenerationæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºPop2Piano [sweetcocoa/pop2piano](https://huggingface.co/sweetcocoa/pop2piano)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## Pop2PianoFeatureExtractor

### `class transformers.Pop2PianoFeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L5)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚

## Pop2PianoForConditionalGeneration

### `class transformers.Pop2PianoForConditionalGeneration`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1009)

```py
( config: Pop2PianoConfig )
```

å‚æ•°

+   `config` ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig)) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Pop2Pianoæ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰`è¯­è¨€å»ºæ¨¡`å¤´ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `å‰å‘`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1109)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None input_features: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚Pop2Pianoæ˜¯ä¸€ä¸ªå¸¦æœ‰ç›¸å¯¹ä½ç½®åµŒå…¥çš„æ¨¡å‹ï¼Œå› æ­¤æ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨å³ä¾§å’Œå·¦ä¾§å¡«å……è¾“å…¥ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æŸ¥çœ‹[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡`input_ids`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[Pop2Pianoè®­ç»ƒ](./Pop2Piano#training)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºæ ‡è®°ä¸º`å±è”½`çš„æ ‡è®°ã€‚[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æŸ¥çœ‹[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)Pop2Pianoä½¿ç”¨`pad_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªéœ€é€‰æ‹©æœ€åçš„`decoder_input_ids`è¾“å…¥ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚è¦äº†è§£å¦‚ä½•å‡†å¤‡

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚é»˜è®¤æƒ…å†µä¸‹è¿˜å°†ä½¿ç”¨å› æœæ©ç ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚

+   `decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è§£ç å™¨ä¸­è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚

+   `cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è§£ç å™¨ä¸­äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`å±è”½`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`å±è”½`ã€‚

+   `encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œå¯é€‰ï¼š*hidden_states*ï¼Œå¯é€‰ï¼š*attentions*ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œæ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„4ä¸ªå¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœè¦æ›´å¥½åœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å¾ˆæœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” æ‰§è¡Œä¸`inputs_embeds`ç›¸åŒçš„ä»»åŠ¡ã€‚å¦‚æœä¸å­˜åœ¨`inputs_embeds`ä½†å­˜åœ¨`input_features`ï¼Œåˆ™å°†`input_features`è§†ä¸º`inputs_embeds`ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœ`decoder_input_ids`å’Œ`decoder_inputs_embeds`éƒ½æœªè®¾ç½®ï¼Œåˆ™`decoder_inputs_embeds`å–`inputs_embeds`çš„å€¼ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size - 1]`å†…ã€‚æ‰€æœ‰æ ‡ç­¾è®¾ç½®ä¸º`-100`çš„å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`å†…çš„ã€‚

è¿”å›

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*optional*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å› â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å› â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

#### `generate`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1217)

```py
( input_features attention_mask = None composer = 'composer1' generation_config = None **kwargs ) â†’ export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

å‚æ•°

+   `input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” è¿™æ˜¯ç”± `Pop2PianoFeatureExtractor` ç”Ÿæˆçš„éŸ³é¢‘çš„ç‰¹å¾åŒ–ç‰ˆæœ¬ã€‚attention_mask â€” å¯¹äºæ‰¹é‡ç”Ÿæˆï¼Œ`input_features` è¢«å¡«å……ä»¥ä½¿æ‰€æœ‰ç¤ºä¾‹å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚`attention_mask` æœ‰åŠ©äºç¡®å®šå“ªäº›åŒºåŸŸè¢«å¡«å……ï¼Œå“ªäº›æ²¡æœ‰è¢«å¡«å……ã€‚

    +   1 ç”¨äº `æœªå¡«å……` çš„æ ‡è®°ã€‚

    +   0 ç”¨äº `å¡«å……` çš„æ ‡è®°ã€‚

+   `composer` (`str`, *optional*, defaults to `"composer1"`) â€” ä¼ é€’ç»™ `Pop2PianoConcatEmbeddingToMel` çš„å€¼ï¼Œç”¨äºä¸ºæ¯ä¸ª `"composer"` ç”Ÿæˆä¸åŒçš„åµŒå…¥ã€‚è¯·ç¡®ä¿ `composer` å€¼åœ¨ `generation_config` çš„ `composer_to_feature_token` ä¸­å­˜åœ¨ã€‚ä¾‹å¦‚ï¼Œè¯·å‚é˜… [https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json](https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json)ã€‚

+   `generation_config` (`~generation.GenerationConfig`, *optional*) â€” ç”¨ä½œç”Ÿæˆè°ƒç”¨çš„åŸºæœ¬å‚æ•°åŒ–çš„ç”Ÿæˆé…ç½®ã€‚ä¼ é€’ç»™ generate çš„ `**kwargs` åŒ¹é… `generation_config` çš„å±æ€§å°†è¦†ç›–å®ƒä»¬ã€‚å¦‚æœæœªæä¾› `generation_config`ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼Œå…¶åŠ è½½ä¼˜å…ˆçº§å¦‚ä¸‹ï¼š1) ä» `generation_config.json` æ¨¡å‹æ–‡ä»¶ä¸­ï¼Œå¦‚æœå­˜åœ¨ï¼›2) ä»æ¨¡å‹é…ç½®ä¸­ã€‚è¯·æ³¨æ„ï¼ŒæœªæŒ‡å®šçš„å‚æ•°å°†ç»§æ‰¿ [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) çš„é»˜è®¤å€¼ï¼Œå…¶æ–‡æ¡£åº”è¯¥è¢«æ£€æŸ¥ä»¥å‚æ•°åŒ–ç”Ÿæˆã€‚kwargs â€” `generate_config` çš„ç‰¹å®šå‚æ•°åŒ–å’Œ/æˆ–å°†è½¬å‘åˆ°æ¨¡å‹çš„ `forward` å‡½æ•°çš„å…¶ä»–æ¨¡å‹ç‰¹å®š kwargsã€‚å¦‚æœæ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œåˆ™ç¼–ç å™¨ç‰¹å®šçš„ kwargs ä¸åº”è¯¥æœ‰å‰ç¼€ï¼Œè§£ç å™¨ç‰¹å®šçš„ kwargs åº”è¯¥ä»¥ *decoder_* ä¸ºå‰ç¼€ã€‚

è¿”å›

[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) æˆ– `torch.LongTensor`

ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)ï¼ˆå¦‚æœ `return_dict_in_generate=True` æˆ–å½“ `config.return_dict_in_generate=True` æ—¶ï¼‰æˆ–ä¸€ä¸ª `torch.FloatTensor`ã€‚ç”±äº Pop2Piano æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼ˆ`model.config.is_encoder_decoder=True`ï¼‰ï¼Œå¯èƒ½çš„ [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) ç±»å‹ä¸ºï¼š

+   [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),

+   [GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)

ä¸º MIDI è¾“å‡ºç”Ÿæˆæ ‡è®° IDã€‚

å¤§å¤šæ•°æ§åˆ¶ç”Ÿæˆçš„å‚æ•°éƒ½åœ¨ `generation_config` ä¸­è®¾ç½®ï¼Œå¦‚æœæœªä¼ é€’ï¼Œåˆ™å°†è®¾ç½®ä¸ºæ¨¡å‹çš„é»˜è®¤ç”Ÿæˆé…ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡ä¼ é€’ç›¸åº”çš„å‚æ•°åˆ° generate() æ¥è¦†ç›–ä»»ä½• `generation_config`ï¼Œä¾‹å¦‚ `.generate(inputs, num_beams=4, do_sample=True)`ã€‚æœ‰å…³ç”Ÿæˆç­–ç•¥å’Œä»£ç ç¤ºä¾‹çš„æ¦‚è¿°ï¼Œè¯·æŸ¥çœ‹[ä»¥ä¸‹æŒ‡å—](./generation_strategies)ã€‚

## Pop2PianoTokenizer

### `class transformers.Pop2PianoTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L12)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚

## Pop2PianoProcessor

### `class transformers.Pop2PianoProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L19)

```py
( *args **kwargs )
```

#### `__call__`

```py
( *args **kwargs )
```

å°†è‡ªèº«ä½œä¸ºå‡½æ•°è°ƒç”¨ã€‚
