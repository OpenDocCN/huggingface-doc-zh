- en: Use model after training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/trl/use_model](https://huggingface.co/docs/trl/use_model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/trl/v0.7.10/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/start.d9a24ea1.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/scheduler.9039eef2.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/singletons.9eef12cc.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/paths.1355483e.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/entry/app.5bef33b8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/index.ded8f90d.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/0.abccdcd8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/nodes/24.43d447c2.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/CodeBlock.8580f3e8.js">
    <link rel="modulepreload" href="/docs/trl/v0.7.10/en/_app/immutable/chunks/Heading.f027f30d.js">
  prefs: []
  type: TYPE_NORMAL
- en: Once you have trained a model using either the SFTTrainer, PPOTrainer, or DPOTrainer,
    you will have a fine-tuned model that can be used for text generation. In this
    section, we’ll walk through the process of loading the fine-tuned model and generating
    text. If you need to run an inference server with the trained model, you can explore
    libraries such as [`text-generation-inference`](https://github.com/huggingface/text-generation-inference).
  prefs: []
  type: TYPE_NORMAL
- en: Load and Generate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have fine-tuned a model fully, meaning without the use of PEFT you can
    simply load it like any other language model in transformers. E.g. the value head
    that was trained during the PPO training is no longer needed and if you load the
    model with the original transformer class it will be ignored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively you can also use the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Use Adapters PEFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also merge the adapters into the base model so you can use the model
    like a normal transformers model, however the checkpoint will be significantly
    bigger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once you have the model loaded and either merged the adapters or keep them separately
    on top you can run generation as with a normal model outlined above.
  prefs: []
  type: TYPE_NORMAL
