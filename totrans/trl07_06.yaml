- en: Use model after training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在训练后使用模型
- en: 'Original text: [https://huggingface.co/docs/trl/use_model](https://huggingface.co/docs/trl/use_model)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/trl/use_model](https://huggingface.co/docs/trl/use_model)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Once you have trained a model using either the SFTTrainer, PPOTrainer, or DPOTrainer,
    you will have a fine-tuned model that can be used for text generation. In this
    section, we’ll walk through the process of loading the fine-tuned model and generating
    text. If you need to run an inference server with the trained model, you can explore
    libraries such as [`text-generation-inference`](https://github.com/huggingface/text-generation-inference).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您使用SFTTrainer、PPOTrainer或DPOTrainer训练了模型，您将获得一个经过微调的模型，可用于文本生成。在本节中，我们将介绍加载经过微调的模型并生成文本的过程。如果您需要使用训练好的模型运行推理服务器，您可以探索诸如[`text-generation-inference`](https://github.com/huggingface/text-generation-inference)等库。
- en: Load and Generate
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和生成
- en: 'If you have fine-tuned a model fully, meaning without the use of PEFT you can
    simply load it like any other language model in transformers. E.g. the value head
    that was trained during the PPO training is no longer needed and if you load the
    model with the original transformer class it will be ignored:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经完全微调了一个模型，即没有使用PEFT，您可以像在transformers中加载任何其他语言模型一样简单地加载它。例如，在PPO训练期间训练的值头部不再需要，如果您使用原始transformer类加载模型，它将被忽略：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively you can also use the pipeline:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您也可以使用pipeline：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Use Adapters PEFT
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用适配器PEFT
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also merge the adapters into the base model so you can use the model
    like a normal transformers model, however the checkpoint will be significantly
    bigger:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将适配器合并到基础模型中，这样您就可以像使用普通的transformers模型一样使用模型，但是检查点会显著增大：
- en: '[PRE3]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once you have the model loaded and either merged the adapters or keep them separately
    on top you can run generation as with a normal model outlined above.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您加载了模型并合并了适配器，或者将它们保留在顶部，您可以像上面概述的普通模型一样运行生成。
