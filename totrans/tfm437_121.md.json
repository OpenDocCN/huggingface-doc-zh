["```py\n>>> pipe = pipeline(\"text-classification\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n```", "```py\n>>> pipe = pipeline(model=\"roberta-large-mnli\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\n```", "```py\n>>> pipe = pipeline(\"text-classification\")\n>>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\n```", "```py\nimport datasets\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```", "```py\nfrom transformers import pipeline\n\npipe = pipeline(\"text-classification\")\n\ndef data():\n    while True:\n        # This could come from a dataset, a database, a queue or HTTP request\n        # in a server\n        # Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n        # to use multiple threads to preprocess data. You can still have 1 thread that\n        # does the preprocessing while the main runs the big inference\n        yield \"This is a test\"\n\nfor out in pipe(data()):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```", "```py\n>>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n\n>>> # Sentiment analysis pipeline\n>>> analyzer = pipeline(\"sentiment-analysis\")\n\n>>> # Question answering pipeline, specifying the checkpoint identifier\n>>> oracle = pipeline(\n...     \"question-answering\", model=\"distilbert-base-cased-distilled-squad\", tokenizer=\"bert-base-cased\"\n... )\n\n>>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n>>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n```", "```py\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nimport datasets\n\ndataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\npipe = pipeline(\"text-classification\", device=0)\nfor out in pipe(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\n    print(out)\n    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n    # Exactly the same output as before, but the content are passed\n    # as batches to the model\n```", "```py\nfrom transformers import pipeline\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"text-classification\", device=0)\n\nclass MyDataset(Dataset):\n    def __len__(self):\n        return 5000\n\n    def __getitem__(self, i):\n        return \"This is a test\"\n\ndataset = MyDataset()\n\nfor batch_size in [1, 8, 64, 256]:\n    print(\"-\" * 30)\n    print(f\"Streaming batch_size={batch_size}\")\n    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n        pass\n```", "```py\n# On GTX 970\n------------------------------\nStreaming no batching\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:26<00:00, 187.52it/s]\n------------------------------\nStreaming batch_size=8\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:04<00:00, 1205.95it/s]\n------------------------------\nStreaming batch_size=64\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:02<00:00, 2478.24it/s]\n------------------------------\nStreaming batch_size=256\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 2554.43it/s]\n(diminishing returns, saturated the GPU)\n```", "```py\nclass MyDataset(Dataset):\n    def __len__(self):\n        return 5000\n\n    def __getitem__(self, i):\n        if i % 64 == 0:\n            n = 100\n        else:\n            n = 1\n        return \"This is a test\" * n\n```", "```py\n------------------------------\nStreaming no batching\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:05<00:00, 183.69it/s]\n------------------------------\nStreaming batch_size=8\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:03<00:00, 265.74it/s]\n------------------------------\nStreaming batch_size=64\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:26<00:00, 37.80it/s]\n------------------------------\nStreaming batch_size=256\n  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"/home/nicolas/src/transformers/test.py\", line 42, in <module>\n    for out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):\n....\n    q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\nRuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)\n```", "```py\npreprocessed = pipe.preprocess(inputs)\nmodel_outputs = pipe.forward(preprocessed)\noutputs = pipe.postprocess(model_outputs)\n```", "```py\nall_model_outputs = []\nfor preprocessed in pipe.preprocess(inputs):\n    model_outputs = pipe.forward(preprocessed)\n    all_model_outputs.append(model_outputs)\noutputs = pipe.postprocess(all_model_outputs)\n```", "```py\nclass MyPipeline(TextClassificationPipeline):\n    def postprocess():\n        # Your code goes here\n        scores = scores * 100\n        # And here\n\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n# or if you use *pipeline* function, then:\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(model=\"superb/wav2vec2-base-superb-ks\")\n>>> classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n[{'score': 0.997, 'label': '_unknown_'}, {'score': 0.002, 'label': 'left'}, {'score': 0.0, 'label': 'yes'}, {'score': 0.0, 'label': 'down'}, {'score': 0.0, 'label': 'stop'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(model=\"openai/whisper-base\")\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flour-fatten sauce.'}\n```", "```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(model=\"suno/bark-small\")\n>>> output = pipe(\"Hey it's HuggingFace on the phone!\")\n\n>>> audio = output[\"audio\"]\n>>> sampling_rate = output[\"sampling_rate\"]\n```", "```py\n>>> from transformers import pipeline\n\n>>> music_generator = pipeline(task=\"text-to-audio\", model=\"facebook/musicgen-small\", framework=\"pt\")\n\n>>> # diversify the music generation by adding randomness with a high temperature and set a maximum music length\n>>> generate_kwargs = {\n...     \"do_sample\": True,\n...     \"temperature\": 0.7,\n...     \"max_new_tokens\": 35,\n... }\n\n>>> outputs = music_generator(\"Techno music with high melodic riffs\", generate_kwargs=generate_kwargs)\n```", "```py\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"ashraq/esc50\")\n>>> audio = next(iter(dataset[\"train\"][\"audio\"]))[\"array\"]\n>>> classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-unfused\")\n>>> classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\n[{'score': 0.9996, 'label': 'Sound of a dog'}, {'score': 0.0004, 'label': 'Sound of vaccum cleaner'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> depth_estimator = pipeline(task=\"depth-estimation\", model=\"Intel/dpt-large\")\n>>> output = depth_estimator(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n>>> # This is a tensor with the values being the depth expressed in meters for each pixel\n>>> output[\"predicted_depth\"].shape\ntorch.Size([1, 384, 384])\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n>>> classifier(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'score': 0.442, 'label': 'macaw'}, {'score': 0.088, 'label': 'popinjay'}, {'score': 0.075, 'label': 'parrot'}, {'score': 0.073, 'label': 'parodist, lampooner'}, {'score': 0.046, 'label': 'poll, poll_parrot'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> segmenter = pipeline(model=\"facebook/detr-resnet-50-panoptic\")\n>>> segments = segmenter(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n>>> len(segments)\n2\n\n>>> segments[0][\"label\"]\n'bird'\n\n>>> segments[1][\"label\"]\n'bird'\n\n>>> type(segments[0][\"mask\"])  # This is a black and white mask showing where is the bird on the original image.\n<class 'PIL.Image.Image'>\n\n>>> segments[0][\"mask\"].size\n(768, 512)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import pipeline\n\n>>> upscaler = pipeline(\"image-to-image\", model=\"caidas/swin2SR-classical-sr-x2-64\")\n>>> img = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n>>> img = img.resize((64, 64))\n>>> upscaled_img = upscaler(img)\n>>> img.size\n(64, 64)\n\n>>> upscaled_img.size\n(144, 144)\n```", "```py\n>>> from transformers import pipeline\n\n>>> detector = pipeline(model=\"facebook/detr-resnet-50\")\n>>> detector(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'score': 0.997, 'label': 'bird', 'box': {'xmin': 69, 'ymin': 171, 'xmax': 396, 'ymax': 507}}, {'score': 0.999, 'label': 'bird', 'box': {'xmin': 398, 'ymin': 105, 'xmax': 767, 'ymax': 507}}]\n\n>>> # x, y  are expressed relative to the top left hand corner.\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(model=\"openai/clip-vit-large-patch14\")\n>>> classifier(\n...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n...     candidate_labels=[\"animals\", \"humans\", \"landscape\"],\n... )\n[{'score': 0.965, 'label': 'animals'}, {'score': 0.03, 'label': 'humans'}, {'score': 0.005, 'label': 'landscape'}]\n\n>>> classifier(\n...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n...     candidate_labels=[\"black and white\", \"photorealist\", \"painting\"],\n... )\n[{'score': 0.996, 'label': 'black and white'}, {'score': 0.003, 'label': 'photorealist'}, {'score': 0.0, 'label': 'painting'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> detector = pipeline(model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\")\n>>> detector(\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n...     candidate_labels=[\"cat\", \"couch\"],\n... )\n[{'score': 0.287, 'label': 'cat', 'box': {'xmin': 324, 'ymin': 20, 'xmax': 640, 'ymax': 373}}, {'score': 0.254, 'label': 'cat', 'box': {'xmin': 1, 'ymin': 55, 'xmax': 315, 'ymax': 472}}, {'score': 0.121, 'label': 'couch', 'box': {'xmin': 4, 'ymin': 0, 'xmax': 642, 'ymax': 476}}]\n\n>>> detector(\n...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\",\n...     candidate_labels=[\"head\", \"bird\"],\n... )\n[{'score': 0.119, 'label': 'bird', 'box': {'xmin': 71, 'ymin': 170, 'xmax': 410, 'ymax': 508}}]\n```", "```py\nconversation = Conversation(\"Going to the movies tonight - any suggestions?\")\nconversation.add_message({\"role\": \"assistant\", \"content\": \"The Big lebowski.\"})\nconversation.add_message({\"role\": \"user\", \"content\": \"Is it good?\"})\n```", "```py\n>>> from transformers import pipeline, Conversation\n# Any model with a chat template can be used in a ConversationalPipeline.\n\n>>> chatbot = pipeline(model=\"facebook/blenderbot-400M-distill\")\n>>> # Conversation objects initialized with a string will treat it as a user message\n>>> conversation = Conversation(\"I'm looking for a movie - what's your favourite one?\")\n>>> conversation = chatbot(conversation)\n>>> conversation.messages[-1][\"content\"]\n\"I don't really have a favorite movie, but I do like action movies. What about you?\"\n\n>>> conversation.add_message({\"role\": \"user\", \"content\": \"That's interesting, why do you like action movies?\"})\n>>> conversation = chatbot(conversation)\n>>> conversation.messages[-1][\"content\"]\n\" I think it's just because they're so fast-paced and action-fantastic.\"\n```", "```py\n>>> from transformers import pipeline\n\n>>> fill_masker = pipeline(model=\"bert-base-uncased\")\n>>> fill_masker(\"This is a simple [MASK].\")\n[{'score': 0.042, 'token': 3291, 'token_str': 'problem', 'sequence': 'this is a simple problem.'}, {'score': 0.031, 'token': 3160, 'token_str': 'question', 'sequence': 'this is a simple question.'}, {'score': 0.03, 'token': 8522, 'token_str': 'equation', 'sequence': 'this is a simple equation.'}, {'score': 0.027, 'token': 2028, 'token_str': 'one', 'sequence': 'this is a simple one.'}, {'score': 0.024, 'token': 3627, 'token_str': 'rule', 'sequence': 'this is a simple rule.'}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> fill_masker = pipeline(model=\"bert-base-uncased\")\n>>> tokenizer_kwargs = {\"truncation\": True}\n>>> fill_masker(\n...     \"This is a simple [MASK]. \" + \"...with a large amount of repeated text appended. \" * 100,\n...     tokenizer_kwargs=tokenizer_kwargs,\n... )\n```", "```py\n>>> from transformers import pipeline\n\n>>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n>>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n{'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n```", "```py\n# use bart in pytorch\nsummarizer = pipeline(\"summarization\")\nsummarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n\n# use t5 in tf\nsummarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\nsummarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n```", "```py\n>>> from transformers import pipeline\n\n>>> oracle = pipeline(model=\"google/tapas-base-finetuned-wtq\")\n>>> table = {\n...     \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n...     \"Stars\": [\"36542\", \"4512\", \"3934\"],\n...     \"Contributors\": [\"651\", \"77\", \"34\"],\n...     \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n... }\n>>> oracle(query=\"How many stars does the transformers repository have?\", table=table)\n{'answer': 'AVERAGE > 36542', 'coordinates': [(0, 1)], 'cells': ['36542'], 'aggregator': 'AVERAGE'}\n```", "```py\ndata = {\n    \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n    \"age\": [\"56\", \"45\", \"59\"],\n    \"number of movies\": [\"87\", \"53\", \"69\"],\n    \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n}\n```", "```py\nimport pandas as pd\n\ntable = pd.DataFrame.from_dict(data)\n```", "```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n>>> classifier(\"This movie is disgustingly good !\")\n[{'label': 'POSITIVE', 'score': 1.0}]\n\n>>> classifier(\"Director tried too much.\")\n[{'label': 'NEGATIVE', 'score': 0.996}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> generator = pipeline(model=\"gpt2\")\n>>> generator(\"I can't believe you did such a \", do_sample=False)\n[{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n\n>>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n>>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n```", "```py\n>>> from transformers import pipeline\n\n>>> generator = pipeline(model=\"mrm8488/t5-base-finetuned-question-generation-ap\")\n>>> generator(\n...     \"answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n... )\n[{'generated_text': 'question: Who created the RuPERTa-base?'}]\n```", "```py\ntext2text_generator = pipeline(\"text2text-generation\")\ntext2text_generator(\"question: What is 42 ? context: 42 is the answer to life, the universe and everything\")\n```", "```py\n>>> from transformers import pipeline\n\n>>> token_classifier = pipeline(model=\"Jean-Baptiste/camembert-ner\", aggregation_strategy=\"simple\")\n>>> sentence = \"Je m'appelle jean-baptiste et je vis \u00e0 montr\u00e9al\"\n>>> tokens = token_classifier(sentence)\n>>> tokens\n[{'entity_group': 'PER', 'score': 0.9931, 'word': 'jean-baptiste', 'start': 12, 'end': 26}, {'entity_group': 'LOC', 'score': 0.998, 'word': 'montr\u00e9al', 'start': 38, 'end': 47}]\n\n>>> token = tokens[0]\n>>> # Start and end provide an easy way to highlight words in the original text.\n>>> sentence[token[\"start\"] : token[\"end\"]]\n' jean-baptiste'\n\n>>> # Some models use the same idea to do part of speech.\n>>> syntaxer = pipeline(model=\"vblagoje/bert-english-uncased-finetuned-pos\", aggregation_strategy=\"simple\")\n>>> syntaxer(\"My name is Sarah and I live in London\")\n[{'entity_group': 'PRON', 'score': 0.999, 'word': 'my', 'start': 0, 'end': 2}, {'entity_group': 'NOUN', 'score': 0.997, 'word': 'name', 'start': 3, 'end': 7}, {'entity_group': 'AUX', 'score': 0.994, 'word': 'is', 'start': 8, 'end': 10}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'sarah', 'start': 11, 'end': 16}, {'entity_group': 'CCONJ', 'score': 0.999, 'word': 'and', 'start': 17, 'end': 20}, {'entity_group': 'PRON', 'score': 0.999, 'word': 'i', 'start': 21, 'end': 22}, {'entity_group': 'VERB', 'score': 0.998, 'word': 'live', 'start': 23, 'end': 27}, {'entity_group': 'ADP', 'score': 0.999, 'word': 'in', 'start': 28, 'end': 30}, {'entity_group': 'PROPN', 'score': 0.999, 'word': 'london', 'start': 31, 'end': 37}]\n```", "```py\nen_fr_translator = pipeline(\"translation_en_to_fr\")\nen_fr_translator(\"How old are you?\")\n```", "```py\n>>> from transformers import pipeline\n\n>>> oracle = pipeline(model=\"facebook/bart-large-mnli\")\n>>> oracle(\n...     \"I have a problem with my iphone that needs to be resolved asap!!\",\n...     candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n... )\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}\n\n>>> oracle(\n...     \"I have a problem with my iphone that needs to be resolved asap!!\",\n...     candidate_labels=[\"english\", \"german\"],\n... )\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['english', 'german'], 'scores': [0.814, 0.186]}\n```", "```py\n>>> from transformers import pipeline\n\n>>> document_qa = pipeline(model=\"impira/layoutlm-document-qa\")\n>>> document_qa(\n...     image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\",\n...     question=\"What is the invoice number?\",\n... )\n[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> extractor = pipeline(model=\"bert-base-uncased\", task=\"feature-extraction\")\n>>> result = extractor(\"This is a simple test.\", return_tensors=True)\n>>> result.shape  # This is a tensor of shape [1, sequence_lenth, hidden_dimension] representing the input string.\ntorch.Size([1, 8, 768])\n```", "```py\n>>> from transformers import pipeline\n\n>>> captioner = pipeline(model=\"ydshieh/vit-gpt2-coco-en\")\n>>> captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'generated_text': 'two birds are standing next to each other '}]\n```", "```py\n>>> from transformers import pipeline\n\n>>> generator = pipeline(model=\"facebook/sam-vit-base\", task=\"mask-generation\")\n>>> outputs = generator(\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n... )\n\n>>> outputs = generator(\n...     \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", points_per_batch=128\n... )\n```", "```py\n>>> from transformers import pipeline\n\n>>> oracle = pipeline(model=\"dandelin/vilt-b32-finetuned-vqa\")\n>>> image_url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png\"\n>>> oracle(question=\"What is she wearing ?\", image=image_url)\n[{'score': 0.948, 'answer': 'hat'}, {'score': 0.009, 'answer': 'fedora'}, {'score': 0.003, 'answer': 'clothes'}, {'score': 0.003, 'answer': 'sun hat'}, {'score': 0.002, 'answer': 'nothing'}]\n\n>>> oracle(question=\"What is she wearing ?\", image=image_url, top_k=1)\n[{'score': 0.948, 'answer': 'hat'}]\n\n>>> oracle(question=\"Is this a person ?\", image=image_url, top_k=1)\n[{'score': 0.993, 'answer': 'yes'}]\n\n>>> oracle(question=\"Is this a man ?\", image=image_url, top_k=1)\n[{'score': 0.996, 'answer': 'no'}]\n```", "```py\n# Explicitly ask for tensor allocation on CUDA device :0\npipe = pipeline(..., device=0)\nwith pipe.device_placement():\n    # Every framework specific tensor allocation will be done on the request device\n    output = pipe(...)\n```"]