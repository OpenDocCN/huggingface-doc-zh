# DDIMScheduler

> 原始文本：[`huggingface.co/docs/diffusers/api/schedulers/ddim`](https://huggingface.co/docs/diffusers/api/schedulers/ddim)

[Jiaming Song、Chenlin Meng 和 Stefano Ermon 撰写的 [去噪扩散隐式模型](https://huggingface.co/papers/2010.02502)（DDIM）。

论文摘要如下：

*去噪扩散概率模型（DDPMs）在没有对抗训练的情况下实现了高质量的图像生成，但它们需要模拟许多步骤的马尔可夫链才能生成样本。为了加速采样，我们提出了去噪扩散隐式模型（DDIMs），这是一种更高效的迭代隐式概率模型类，具有与 DDPMs 相同的训练过程。在 DDPMs 中，生成过程被定义为马尔可夫扩散过程的反向。我们构建了一类非马尔可夫扩散过程，这些过程导致相同的训练目标，但其反向过程可以更快地从中采样。我们在实证中证明，与 DDPMs 相比，DDIMs 可以在墙钟时间方面以 10× 到 50× 的速度产生高质量样本，使我们能够在计算和样本质量之间进行权衡，并且可以直接在潜在空间中执行语义上有意义的图像插值。*

本文的原始代码可以在 [ermongroup/ddim](https://github.com/ermongroup/ddim) 找到，您可以在 [tsong.me](https://tsong.me/) 上联系作者。

## 提示

论文 [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891) 声称训练和推断设置之间的不匹配导致了稳定扩散的推断生成结果不佳。为了解决这个问题，作者提出：

🧪 这是一个实验性功能！

1.  重新调整噪声计划以强制零终端信噪比（SNR）

```py
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, rescale_betas_zero_snr=True)
```

1.  使用 `v_prediction` 训练模型（将以下参数添加到 [train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py) 或 [train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py) 脚本中）

```py
--prediction_type="v_prediction"
```

1.  更改采样器以始终从最后一个时间步开始

```py
pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing")
```

1.  重新调整无分类器指导以防止过度曝光

```py
image = pipe(prompt, guidance_rescale=0.7).images[0]
```

例如：

```py
from diffusers import DiffusionPipeline, DDIMScheduler
import torch

pipe = DiffusionPipeline.from_pretrained("ptx0/pseudo-journey-v2", torch_dtype=torch.float16)
pipe.scheduler = DDIMScheduler.from_config(
    pipe.scheduler.config, rescale_betas_zero_snr=True, timestep_spacing="trailing"
)
pipe.to("cuda")

prompt = "A lion in galaxies, spirals, nebulae, stars, smoke, iridescent, intricate detail, octane render, 8k"
image = pipe(prompt, guidance_rescale=0.7).images[0]
image
```

## DDIMScheduler

### `class diffusers.DDIMScheduler`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_ddim.py#L131)

```py
( num_train_timesteps: int = 1000 beta_start: float = 0.0001 beta_end: float = 0.02 beta_schedule: str = 'linear' trained_betas: Union = None clip_sample: bool = True set_alpha_to_one: bool = True steps_offset: int = 0 prediction_type: str = 'epsilon' thresholding: bool = False dynamic_thresholding_ratio: float = 0.995 clip_sample_range: float = 1.0 sample_max_value: float = 1.0 timestep_spacing: str = 'leading' rescale_betas_zero_snr: bool = False )
```

参数

+   `num_train_timesteps` (`int`, 默认为 1000) — 训练模型的扩散步骤数。

+   `beta_start` (`float`, 默认为 0.0001) — 推断的起始 `beta` 值。

+   `beta_end` (`float`, 默认为 0.02) — 最终的 `beta` 值。

+   `beta_schedule` (`str`, 默认为 `"linear"`) — beta 计划，从 beta 范围到用于模型步进的 beta 序列的映射。可选择 `linear`、`scaled_linear` 或 `squaredcos_cap_v2`。

+   `trained_betas` (`np.ndarray`, *可选*) — 直接传递一个 beta 数组给构造函数，以绕过 `beta_start` 和 `beta_end`。

+   `clip_sample` (`bool`, 默认为 `True`) — 为了数值稳定性而裁剪预测样本。

+   `clip_sample_range` (`float`, 默认为 1.0) — 采样裁剪的最大幅度。仅当 `clip_sample=True` 时有效。

+   `set_alpha_to_one` (`bool`, 默认为 `True`) — 每个扩散步骤使用该步骤和上一个步骤的 alphas 乘积值。对于最后一步，没有上一个 alpha。当此选项为 `True` 时，前一个 alpha 乘积被固定为 `1`，否则使用步骤 0 的 alpha 值。

+   `steps_offset` (`int`, 默认为 0) — 添加到推断步骤的偏移量。您可以使用 `offset=1` 和 `set_alpha_to_one=False` 的组合，使最后一步使用步骤 0 作为上一个 alpha 乘积，就像在 Stable Diffusion 中一样。

+   `prediction_type`（`str`，默认为`epsilon`，*可选*）— 调度函数的预测类型；可以是`epsilon`（预测扩散过程的噪声），`sample`（直接预测有噪声的样本）或`v_prediction`（参见[Imagen Video](https://imagen.research.google/video/paper.pdf)论文的第 2.4 节）。

+   `thresholding`（`bool`，默认为`False`）— 是否使用“动态阈值”方法。这对于稳定扩散等潜在空间扩散模型不适用。

+   `dynamic_thresholding_ratio`（`float`，默认为 0.995）— 动态阈值方法的比率。仅在`thresholding=True`时有效。

+   `sample_max_value`（`float`，默认为 1.0）— 动态阈值的阈值值。仅在`thresholding=True`时有效。

+   `timestep_spacing`（`str`，默认为`"leading"`）— 时间步应该如何缩放。有关更多信息，请参阅[Common Diffusion Noise Schedules and Sample Steps are Flawed](https://huggingface.co/papers/2305.08891)的表 2。

+   `rescale_betas_zero_snr`（`bool`，默认为`False`）— 是否重新缩放 beta 以使终端 SNR 为零。这使模型能够生成非常明亮和暗的样本，而不是将其限制为具有中等亮度的样本。与[`--offset_noise`](https://github.com/huggingface/diffusers/blob/74fd735eb073eb1d774b1ab4154a0876eb82f055/examples/dreambooth/train_dreambooth.py#L506) loosely 相关。

`DDIMScheduler`扩展了引入非马尔可夫引导的去噪扩散概率模型（DDPMs）的过程。

该模型继承自 SchedulerMixin 和 ConfigMixin。检查超类文档，了解库为所有调度程序实现的通用方法，如加载和保存。

#### `scale_model_input`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_ddim.py#L238)

```py
( sample: FloatTensor timestep: Optional = None ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `sample`（`torch.FloatTensor`）— 输入样本。

+   `timestep`（`int`，*可选*）— 扩散链中的当前时间步。

返回

`torch.FloatTensor`

一个经过缩放的输入样本。

确保与需要根据当前时间步长缩放去噪模型输入的调度程序可以互换。

#### `set_timesteps`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_ddim.py#L299)

```py
( num_inference_steps: int device: Union = None )
```

参数

+   `num_inference_steps`（`int`）— 使用预训练模型生成样本时使用的扩散步数。

设置用于扩散链的离散时间步（在推断之前运行）。

#### `步骤`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_ddim.py#L344)

```py
( model_output: FloatTensor timestep: int sample: FloatTensor eta: float = 0.0 use_clipped_model_output: bool = False generator = None variance_noise: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';~schedulers.scheduling_utils.DDIMSchedulerOutput or tuple
```

参数

+   `model_output`（`torch.FloatTensor`）— 从学习的扩散模型直接输出。

+   `timestep`（`float`）— 扩散链中的当前离散时间步。

+   `sample`（`torch.FloatTensor`）— 扩散过程中创建的样本的当前实例。

+   `eta`（`float`）— 扩散步骤中添加噪声的噪声权重。

+   `use_clipped_model_output`（`bool`，默认为`False`）— 如果为`True`，则从剪切预测的原始样本计算“校正”的`model_output`。当`self.config.clip_sample`为`True`时，必要，因为预测的原始样本被剪切为[-1, 1]。如果没有发生剪切，“校正”的`model_output`将与提供的输入一致，`use_clipped_model_output`不起作用。

+   `generator`（`torch.Generator`，*可选*）— 随机数生成器。

+   `variance_noise`（`torch.FloatTensor`）— 通过直接提供方差本身的噪声来替代使用`generator`生成噪声。对于`CycleDiffusion`等方法非常有用。

+   `return_dict` (`bool`，*可选*，默认为`True`) — 是否返回 DDIMSchedulerOutput 或`tuple`。

返回值

`~schedulers.scheduling_utils.DDIMSchedulerOutput`或`tuple`

如果`return_dict`为`True`，则返回 DDIMSchedulerOutput，否则返回一个元组，其中第一个元素是样本张量。

通过反转 SDE 来预测上一个时间步的样本。此函数从学习模型输出（通常是预测的噪声）中传播扩散过程。

## DDIMSchedulerOutput

### `class diffusers.schedulers.scheduling_ddim.DDIMSchedulerOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/schedulers/scheduling_ddim.py#L31)

```py
( prev_sample: FloatTensor pred_original_sample: Optional = None )
```

参数

+   `prev_sample`（对于图像形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`） — 上一个时间步的计算样本`(x_{t-1})`。`prev_sample`应该在去噪循环中作为下一个模型输入使用。

+   `pred_original_sample`（对于图像形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`） — 基于当前时间步的模型输出预测的去噪样本`(x_{0})`。`pred_original_sample`可用于预览进展或指导。

调度程序的`step`函数输出的输出类。
