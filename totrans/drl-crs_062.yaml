- en: (Optional) the Policy Gradient Theorem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem](https://huggingface.co/learn/deep-rl-course/unit4/pg-theorem)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this optional section where we’re **going to study how we differentiate the
    objective function that we will use to approximate the policy gradient**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first recap our different formulas:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The Objective function
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Return](../Images/91e5d228c6d4b4dfdbd4a60c7ae8fa63.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'The probability of a trajectory (given that action comes from<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​):'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Probability](../Images/4b4baebaf1b080cb2852e25cd8686dec.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: 'So we have: <math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><msub><mo>∑</mo><mi>τ</mi></msub><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">\nabla_\theta J(\theta) = \nabla_\theta \sum_{\tau}P(\tau;\theta)R(\tau)</annotation></semantics></math>∇θ​J(θ)=∇θ​∑τ​P(τ;θ)R(τ)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the gradient of the sum as the sum of the gradient: <math><semantics><mrow><mo>=</mo><msub><mo>∑</mo><mi>τ</mi></msub><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">= \sum_{\tau}
    \nabla_\theta P(\tau;\theta)R(\tau)</annotation></semantics></math> =∑τ​∇θ​P(τ;θ)R(τ)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We then multiply every term in the sum by<math><semantics><mrow><mfrac><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\frac{P(\tau;\theta)}{P(\tau;\theta)}</annotation></semantics></math>P(τ;θ)P(τ;θ)​(which
    is possible since it’s = 1) <math><semantics><mrow><mo>=</mo><msub><mo>∑</mo><mi>τ</mi></msub><mfrac><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>R</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">= \sum_{\tau}
    \frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau)</annotation></semantics></math>
    =∑τ​P(τ;θ)P(τ;θ)​∇θ​P(τ;θ)R(τ)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: We can simplify further this since<math><semantics><mrow><mfrac><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mfrac><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow>
    <annotation encoding="application/x-tex">\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta
    P(\tau;\theta) = P(\tau;\theta)\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}</annotation></semantics></math>
    P(τ;θ)P(τ;θ)​∇θ​P(τ;θ)=P(τ;θ)P(τ;θ)∇θ​P(τ;θ)​ <math><semantics><mrow><mo>=</mo><msub><mo>∑</mo><mi>τ</mi></msub><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mfrac><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo stretchy="false">(</mo><mi>τ</mi><mo
    separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">= \sum_{\tau} P(\tau;\theta) \frac{\nabla_\theta
    P(\tau;\theta)}{P(\tau;\theta)}R(\tau)</annotation></semantics></math> =∑τ​P(τ;θ)P(τ;θ)∇θ​P(τ;θ)​R(τ)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: We can then use the *derivative log trick* (also called *likelihood ratio trick*
    or *REINFORCE trick*), a simple rule in calculus that implies that<math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>x</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>f</mi><mo
    stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>x</mi></msub><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo
    stretchy="false">)</mo></mrow><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo
    stretchy="false">)</mo></mrow></mfrac></mrow> <annotation encoding="application/x-tex">\nabla_x
    log f(x) = \frac{\nabla_x f(x)}{f(x)}</annotation></semantics></math> ∇x​logf(x)=f(x)∇x​f(x)​
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: So given we have<math><semantics><mrow><mfrac><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation
    encoding="application/x-tex">\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}</annotation></semantics></math>
    P(τ;θ)∇θ​P(τ;θ)​ we transform it as<math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau|\theta)</annotation></semantics></math> ∇θ​logP(τ∣θ)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'So this is our likelihood policy gradient: <math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msub><mo>∑</mo><mi>τ</mi></msub><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>R</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mo stretchy="false">)</mo></mrow> <annotation
    encoding="application/x-tex">\nabla_\theta J(\theta) = \sum_{\tau} P(\tau;\theta)
    \nabla_\theta log P(\tau;\theta) R(\tau)</annotation></semantics></math> ∇θ​J(θ)=∑τ​P(τ;θ)∇θ​logP(τ;θ)R(τ)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是我们的似然策略梯度：∇θ​J(θ)=∑τ​P(τ;θ)∇θ​logP(τ;θ)R(τ)
- en: Thanks for this new formula, we can estimate the gradient using trajectory samples
    (we can approximate the likelihood ratio policy gradient with sample-based estimate
    if you prefer). <math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mi>R</mi><mo stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_\theta
    J(\theta) = \frac{1}{m} \sum^{m}_{i=1} \nabla_\theta log P(\tau^{(i)};\theta)R(\tau^{(i)})</annotation></semantics></math>∇θ​J(θ)=m1​∑i=1m​∇θ​logP(τ(i);θ)R(τ(i))
    where each<math><semantics><mrow><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\tau^{(i)}</annotation></semantics></math>τ(i)
    is a sampled trajectory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢这个新公式，我们可以使用轨迹样本来估计梯度（如果您愿意，我们可以用基于样本的估计来近似似然比策略梯度）。其中每个τ(i)是一个采样轨迹。
- en: 'But we still have some mathematics work to do there: we need to simplify<math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><mi>τ</mi><mi mathvariant="normal">∣</mi><mi>θ</mi><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau|\theta)</annotation></semantics></math> ∇θ​logP(τ∣θ)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们仍然有一些数学工作要做：我们需要简化∇θ​logP(τ∣θ)
- en: 'We know that: <math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo
    stretchy="false">[</mo><mi>μ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><msubsup><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>P</mi><mo
    stretchy="false">(</mo><msubsup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation
    encoding="application/x-tex">\nabla_\theta log P(\tau^{(i)};\theta)= \nabla_\theta
    log[ \mu(s_0) \prod_{t=0}^{H} P(s_{t+1}^{(i)}|s_{t}^{(i)}, a_{t}^{(i)}) \pi_\theta(a_{t}^{(i)}|s_{t}^{(i)})]</annotation></semantics></math>∇θ​logP(τ(i);θ)=∇θ​log[μ(s0​)∏t=0H​P(st+1(i)​∣st(i)​,at(i)​)πθ​(at(i)​∣st(i)​)]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Where<math><semantics><mrow><mi>μ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mu(s_0)</annotation></semantics></math>μ(s0​)
    is the initial state distribution and<math><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msubsup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">P(s_{t+1}^{(i)}|s_{t}^{(i)},
    a_{t}^{(i)})</annotation></semantics></math> P(st+1(i)​∣st(i)​,at(i)​) is the
    state transition dynamics of the MDP.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the log of a product is equal to the sum of the logs: <math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mrow><mo
    fence="true">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>μ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo
    stretchy="false">)</mo><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msubsup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau^{(i)};\theta)= \nabla_\theta \left[log \mu(s_0) + \sum\limits_{t=0}^{H}log
    P(s_{t+1}^{(i)}|s_{t}^{(i)} a_{t}^{(i)}) + \sum\limits_{t=0}^{H}log \pi_\theta(a_{t}^{(i)}|s_{t}^{(i)})\right]</annotation></semantics></math>
    ∇θ​logP(τ(i);θ)=∇θ​[logμ(s0​)+t=0∑H​logP(st+1(i)​∣st(i)​at(i)​)+t=0∑H​logπθ​(at(i)​∣st(i)​)]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道一个乘积的对数等于对数的和：∇θ​logP(τ(i);θ)=∇θ​[logμ(s0)+t=0∑HlogP(st+1(i)∣st(i)at(i))+t=0∑Hlogπθ(at(i)∣st(i))]
- en: 'We also know that the gradient of the sum is equal to the sum of gradient:
    <math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>μ</mi><mo
    stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msubsup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><mo>+</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau^{(i)};\theta)=\nabla_\theta log\mu(s_0) + \nabla_\theta \sum\limits_{t=0}^{H}
    log P(s_{t+1}^{(i)}|s_{t}^{(i)} a_{t}^{(i)}) + \nabla_\theta \sum\limits_{t=0}^{H}
    log \pi_\theta(a_{t}^{(i)}|s_{t}^{(i)})</annotation></semantics></math> ∇θ​logP(τ(i);θ)=∇θ​logμ(s0​)+∇θ​t=0∑H​logP(st+1(i)​∣st(i)​at(i)​)+∇θ​t=0∑H​logπθ​(at(i)​∣st(i)​)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Since neither initial state distribution or state transition dynamics of the
    MDP are dependent of<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ,
    the derivate of both terms are 0\. So we can remove them:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Since:<math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msubsup><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi
    mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><msubsup><mi>a</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla_\theta
    \sum_{t=0}^{H} log P(s_{t+1}^{(i)}|s_{t}^{(i)} a_{t}^{(i)}) = 0</annotation></semantics></math>
    ∇θ​∑t=0H​logP(st+1(i)​∣st(i)​at(i)​)=0 and<math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>μ</mi><mo
    stretchy="false">(</mo><msub><mi>s</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow>
    <annotation encoding="application/x-tex">\nabla_\theta \mu(s_0) = 0</annotation></semantics></math>∇θ​μ(s0​)=0
    <math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau^{(i)};\theta) = \nabla_\theta \sum_{t=0}^{H} log \pi_\theta(a_{t}^{(i)}|s_{t}^{(i)})</annotation></semantics></math>∇θ​logP(τ(i);θ)=∇θ​∑t=0H​logπθ​(at(i)​∣st(i)​)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下公式，我们可以将总和的梯度重写为梯度的总和：
- en: 'We can rewrite the gradient of the sum as the sum of gradients: <math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo
    stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msup><mo separator="true">;</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\nabla_\theta
    log P(\tau^{(i)};\theta)= \sum_{t=0}^{H} \nabla_\theta log \pi_\theta(a_{t}^{(i)}|s_{t}^{(i)})</annotation></semantics></math>
    ∇θ​logP(τ(i);θ)=∑t=0H​∇θ​logπθ​(at(i)​∣st(i)​)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∇θ​logP(τ(i);θ)=∑t=0H​∇θ​logπθ​(at(i)​∣st(i​))
- en: 'So, the final formula for estimating the policy gradient is: <math><semantics><mrow><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo
    stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>g</mi><mo>^</mo></mover><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>H</mi></msubsup><msub><mi
    mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><msubsup><mi>a</mi><mi>t</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo
    stretchy="false">)</mo></mrow></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo
    stretchy="false">)</mo><mi>R</mi><mo stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo
    stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo
    stretchy="false">)</mo></mrow> <annotation encoding="application/x-tex">\nabla_{\theta}
    J(\theta) = \hat{g} = \frac{1}{m} \sum^{m}_{i=1} \sum^{H}_{t=0} \nabla_\theta
    \log \pi_\theta(a^{(i)}_{t} | s_{t}^{(i)})R(\tau^{(i)})</annotation></semantics></math>
    ∇θ​J(θ)=g^​=m1​∑i=1m​∑t=0H​∇θ​logπθ​(at(i)​∣st(i)​)R(τ(i))'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，估计策略梯度的最终公式是：∇θ J(θ) = g^ = 1/m ∑i=1m ∑t=0H ∇θ log πθ(a^(i)_t | s_t^(i))R(τ^(i))
