# 生成

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation)

每个框架都有一个用于文本生成的`GenerationMixin`类中实现的生成方法：

+   PyTorch的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)在[GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)中实现。

+   TensorFlow的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)在[TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin)中实现。

+   Flax/JAX的[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)在[FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin)中实现。

无论您选择哪个框架，您都可以使用[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)类实例对生成方法进行参数化。请参考此类以获取完整的生成参数列表，这些参数控制生成方法的行为。

要了解如何检查模型的生成配置，了解默认值，如何临时更改参数以及如何创建和保存自定义生成配置，请参考[文本生成策略指南](../generation_strategies)。该指南还解释了如何使用相关功能，如标记流。

## GenerationConfig

### `class transformers.GenerationConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L40)

```py
( **kwargs )
```

控制输出长度的参数

+   `max_length`（`int`，*可选*，默认为20）— 生成的标记可以具有的最大长度。对应于输入提示的长度+`max_new_tokens`。如果也设置了`max_new_tokens`，则其效果将被覆盖。

+   `max_new_tokens`（`int`，*可选*）— 生成的最大标记数，忽略提示中的标记数。

+   `min_length`（`int`，*可选*，默认为0）— 要生成的序列的最小长度。对应于输入提示的长度+`min_new_tokens`。如果也设置了`min_new_tokens`，则其效果将被覆盖。

+   `min_new_tokens`（`int`，*可选*）— 生成的最小标记数，忽略提示中的标记数。

+   `early_stopping`（`bool`或`str`，*可选*，默认为`False`）— 控制基于束搜索的方法的停止条件，如束搜索。它接受以下值：`True`，表示一旦有`num_beams`个完整候选项就停止生成；`False`，应用启发式方法，当很难找到更好的候选项时停止生成；`"never"`，束搜索过程仅在不能有更好的候选项时停止（经典的束搜索算法）。

+   `max_time(float,` *可选*) — 允许计算运行的最长时间（以秒为单位）。在分配的时间已过后，生成仍将完成当前传递。

控制生成策略使用的参数

+   `do_sample`（`bool`，*可选*，默认为`False`）— 是否使用采样；否则使用贪婪解码。

+   `num_beams`（`int`，*可选*，默认为1）— 用于束搜索的束数。1表示不进行束搜索。

+   `num_beam_groups`（`int`，*可选*，默认为1）— 将`num_beams`分成多个组以确保不同组的束之间的多样性。有关更多详细信息，请参阅[此论文](https://arxiv.org/pdf/1610.02424.pdf)。

+   `penalty_alpha` (`float`, *optional*) — 这些值平衡了对比搜索解码中模型置信度和退化惩罚。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应使用过去的最后键/值注意力（如果适用于模型）来加快解码速度。

用于操纵模型输出logits的参数

+   `temperature` (`float`, *optional*, defaults to 1.0) — 用于调节下一个标记概率的值。

+   `top_k` (`int`, *optional*, defaults to 50) — 要保留的最高概率词汇标记的数量，用于进行top-k过滤。

+   `top_p` (`float`, *optional*, defaults to 1.0) — 如果设置为小于1的浮点数，则仅保留概率加起来达到`top_p`或更高的最可能标记集合以进行生成。

+   `typical_p` (`float`, *optional*, defaults to 1.0) — 本地典型性衡量了预测下一个目标标记的条件概率与预测随机下一个标记的条件概率之间的相似程度，给定已生成的部分文本。如果设置为小于1的浮点数，则保留概率加起来达到`typical_p`或更高的最典型标记集合以进行生成。有关更多详细信息，请参见[此论文](https://arxiv.org/pdf/2202.00666.pdf)。

+   `epsilon_cutoff` (`float`, *optional*, defaults to 0.0) — 如果设置为0和1之间的浮点数，只有条件概率大于`epsilon_cutoff`的标记才会被采样。在论文中，建议的值范围从3e-4到9e-4，取决于模型的大小。有关更多详细信息，请参见[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。

+   `eta_cutoff` (`float`, *optional*, defaults to 0.0) — Eta采样是局部典型采样和epsilon采样的混合。如果设置为0和1之间的浮点数，仅当一个标记大于`eta_cutoff`或`sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits))`时才考虑。后一项直观上是下一个标记概率的期望，乘以`sqrt(eta_cutoff)`。在论文中，建议的值范围从3e-4到2e-3，取决于模型的大小。有关更多详细信息，请参见[截断采样作为语言模型去平滑](https://arxiv.org/abs/2210.15191)。

+   `diversity_penalty` (`float`, *optional*, defaults to 0.0) — 如果一个beam在特定时间生成与其他组中的任何beam相同的标记，则从该beam的得分中减去此值。请注意，只有在启用`group beam search`时，`diversity_penalty`才有效。

+   `repetition_penalty` (`float`, *optional*, defaults to 1.0) — 重复惩罚的参数。1.0表示没有惩罚。有关更多详细信息，请参见[此论文](https://arxiv.org/pdf/1909.05858.pdf)。

+   `encoder_repetition_penalty` (`float`, *optional*, defaults to 1.0) — 编码器重复惩罚的参数。对于不在原始输入中的序列施加指数惩罚。1.0表示没有惩罚。

+   `length_penalty` (`float`, *optional*, defaults to 1.0) — 用于基于beam的生成的长度的指数惩罚。它作为指数应用于序列长度，然后用于分割序列的得分。由于得分是序列的对数似然（即负数），`length_penalty` > 0.0 会促进更长的序列，而`length_penalty` < 0.0 会鼓励更短的序列。

+   `no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — 如果设置为大于0的整数，那么该大小的所有ngram只能出现一次。

+   `bad_words_ids(List[List[int]],` *optional*) — 不允许生成的标记id列表。查看[NoBadWordsLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoBadWordsLogitsProcessor)以获取更多文档和示例。

+   `force_words_ids(List[List[int]]` 或 `List[List[List[int]]]`, *optional*) — 必须生成的标记id列表。如果给定 `List[List[int]]`，则将其视为必须包含的简单单词列表，与 `bad_words_ids` 相反。如果给定 `List[List[List[int]]]`，这将触发一个[分离约束](https://github.com/huggingface/transformers/issues/14081)，其中可以允许每个单词的不同形式。

+   `renormalize_logits` (`bool`, *optional*, 默认为 `False`) — 在应用所有标记处理器或包装器（包括自定义的）后是否重新归一化标记。强烈建议将此标志设置为 `True`，因为搜索算法假定得分标记已归一化，但某些标记处理器或包装器会破坏归一化。

+   `constraints` (`List[Constraint]`, *optional*) — 可以添加到生成中的自定义约束，以确保输出将包含由 `Constraint` 对象定义的某些标记的使用，以最合理的方式。

+   `forced_bos_token_id` (`int`, *optional*, 默认为 `model.config.forced_bos_token_id`) — 在 `decoder_start_token_id` 后强制作为第一个生成的标记的标记id。对于像 [mBART](../model_doc/mbart) 这样的多语言模型很有用，其中第一个生成的标记需要是目标语言标记。

+   `forced_eos_token_id` (`Union[int, List[int]]`, *optional*, 默认为 `model.config.forced_eos_token_id`) — 当达到 `max_length` 时，强制作为最后生成的标记的标记id。可选择使用列表设置多个 *end-of-sequence* 标记。

+   `remove_invalid_values` (`bool`, *optional*, 默认为 `model.config.remove_invalid_values`) — 是否删除模型可能的 *nan* 和 *inf* 输出，以防止生成方法崩溃。请注意，使用 `remove_invalid_values` 可以减慢生成速度。

+   `exponential_decay_length_penalty` (`tuple(int, float)`, *optional*) — 此元组在生成一定数量的标记后添加指数增长的长度惩罚。元组应包含：`(start_index, decay_factor)`，其中 `start_index` 表示惩罚开始的位置，`decay_factor` 表示指数衰减的因子。

+   `suppress_tokens` (`List[int]`, *optional*) — 生成时将被抑制的标记列表。`SupressTokens` 标记处理器将将它们的对数概率设置为 `-inf`，以便它们不被抽样。

+   `begin_suppress_tokens` (`List[int]`, *optional*) — 生成开始时将被抑制的标记列表。`SupressBeginTokens` 标记处理器将将它们的对数概率设置为 `-inf`，以便它们不被抽样。

+   `forced_decoder_ids` (`List[List[int]]`, *optional*) — 一对整数的列表，指示在抽样之前将强制执行的生成索引到标记索引的映射。例如，`[[1, 123]]` 表示第二个生成的标记将始终是索引为 123 的标记。

+   `sequence_bias` (`Dict[Tuple[int], float]`, *optional*)) — 将标记序列映射到其偏差项的字典。正偏差增加选择该序列的几率，而负偏差则相反。查看[SequenceBiasLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SequenceBiasLogitsProcessor)以获取更多文档和示例。

+   `guidance_scale` (`float`, *optional*) — 分类器自由引导（CFG）的引导比例。通过设置 `guidance_scale > 1` 启用 CFG。更高的引导比例鼓励模型生成与输入提示更紧密相关的样本，通常以牺牲质量为代价。

+   `low_memory` (`bool`, *optional*) — 用于对比搜索的顺序topk开关，以减少内存峰值。与对比搜索一起使用。

`generate` 输出变量的定义参数

+   `num_return_sequences(int,` *optional*, 默认为 1) — 每个批次中每个元素独立计算返回序列的数量。

+   `output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*, defaults to `False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `output_scores` (`bool`, *optional*, defaults to `False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量中的 `scores`。

+   `return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

可在生成时使用的特殊标记

+   `pad_token_id` (`int`, *optional*) — *填充*标记的 id。

+   `bos_token_id` (`int`, *optional*) — *序列开始*标记的 id。

+   `eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的 id。可选择使用列表设置多个*序列结束*标记。

专属于编码器-解码器模型的生成参数

+   `encoder_no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — 如果设置为大于 0 的整数，则在 `encoder_input_ids` 中出现的该大小的所有 n 元组不能出现在 `decoder_input_ids` 中。

+   `decoder_start_token_id` (`int`, *optional*) — 如果编码器-解码器模型开始解码时使用与 *bos* 不同的标记，则为该标记的 id。

专属于 [助手生成](https

+   `num_assistant_tokens` (`int`, *optional*, defaults to 5) — 定义助手模型在每次迭代之前生成的*推测标记*数量。`num_assistant_tokens` 的值越高，生成的*推测性*越强：如果助手模型表现良好，可以实现更大的加速，如果助手模型需要大量修正，则实现的加速度较低。

+   `num_assistant_tokens_schedule` (`str`, *optional*, defaults to `"heuristic"`) — 定义推断期间最大助手标记应该如何更改的计划。

    +   `"_heuristic_`: 当所有*推测*标记都正确时，将 `num_assistant_tokens` 增加 2，否则减少 1

    +   `"constant"`: 在生成期间 `num_assistant_tokens` 保持不变

通配符

用于生成任务配置的类。`generate` 调用支持以下文本解码器、文本到文本、语音到文本和视觉到文本模型的生成方法：

+   通过调用 [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search) 实现*贪婪解码*，如果 `num_beams=1` 且 `do_sample=False`

+   通过调用 [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search) 实现*对比搜索*，如果 `penalty_alpha>0.` 且 `top_k>1`

+   通过调用 [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample) 实现*多项式采样*，如果 `num_beams=1` 且 `do_sample=True`

+   通过调用 [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search) 实现*束搜索解码*，如果 `num_beams>1` 且 `do_sample=False`

+   通过调用 [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample) 实现*束搜索多项式采样*，如果 `num_beams>1` 且 `do_sample=True`

+   通过调用 [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search) 实现*多样束搜索解码*，如果 `num_beams>1` 且 `num_beam_groups>1`

+   通过调用 [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search) 实现*受限束搜索解码*，如果 `constraints!=None` 或 `force_words_ids!=None`

+   通过将`assistant_model`传递给`.generate()`来进行*辅助解码*调用`assisted_decoding()`。

您不需要直接调用上述任何方法。将自定义参数值传递给‘.generate()‘。要了解更多关于解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。

这些标志中的大部分控制生成的对数或停止标准。确保您查看[生成相关类](https://huggingface.co/docs/transformers/internal/generation_utils)以获取可能操作的完整描述，以及它们用法的示例。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L605)

```py
( pretrained_model_name: Union config_file_name: Union = None cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs ) → export const metadata = 'undefined';GenerationConfig
```

参数

+   `pretrained_model_name` (`str`或`os.PathLike`) — 这可以是：

    +   一个字符串，预训练模型配置的*模型ID*，托管在huggingface.co模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或命名空间在用户或组织名称下，如`dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)方法保存的配置文件，例如，`./my_model_directory/`。

+   `config_file_name` (`str`或`os.PathLike`，*可选*，默认为`"generation_config.json"`) — 要从`pretrained_model_name`加载的生成配置JSON文件的名称。

+   `cache_dir` (`str`或`os.PathLike`，*可选*) — 下载预训练模型配置文件时应缓存的目录路径，如果不应使用标准缓存。

+   `force_download` (`bool`，*可选*，默认为`False`) — 是否强制（重新）下载配置文件并覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`，*可选*，默认为`False`) — 是否删除接收不完整的文件。如果存在这样的文件，则尝试恢复下载。

+   `proxies` (`Dict[str, str]`，*可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理服务器在每个请求上使用。

+   `token` (`str`或`bool`，*可选*) — 用作远程文件的HTTP令牌的令牌。如果为`True`，或未指定，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。

+   `revision` (`str`，*可选*，默认为`"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交ID，因为我们在huggingface.co上使用基于git的系统来存储模型和其他工件，所以`revision`可以是git允许的任何标识符。

    要测试您在Hub上提交的拉取请求，可以传递`revision=“refs/pr/<pr_number>“。</pr_number>

+   `return_unused_kwargs` (`bool`，*可选*，默认为`False`) — 如果为`False`，则此函数仅返回最终的配置对象。

    如果为`True`，则此函数返回一个`Tuple(config, unused_kwargs)`，其中*unused_kwargs*是一个字典，其中键/值对的键不是配置属性：即，未使用来更新`config`的`kwargs`的部分，否则将被忽略。

+   `subfolder` (`str`，*可选*，默认为`""`) — 如果相关文件位于huggingface.co模型存储库的子文件夹中，您可以在此处指定文件夹名称。

+   `kwargs` (`Dict[str, Any]`，*可选*) — 任何键的kwargs值，这些键是配置属性，将用于覆盖加载的值。关于键/值对的行为，其键*不*是配置属性，由`return_unused_kwargs`关键字参数控制。

返回

[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)

从这个预训练模型实例化的配置对象。

从生成配置文件实例化[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。

示例：

```py
>>> from transformers import GenerationConfig

>>> # Download configuration from huggingface.co and cache.
>>> generation_config = GenerationConfig.from_pretrained("gpt2")

>>> # E.g. config was saved using *save_pretrained('./test/saved_model/')*
>>> generation_config.save_pretrained("./test/saved_model/")
>>> generation_config = GenerationConfig.from_pretrained("./test/saved_model/")

>>> # You can also specify configuration names to your generation configuration file
>>> generation_config.save_pretrained("./test/saved_model/", config_file_name="my_configuration.json")
>>> generation_config = GenerationConfig.from_pretrained("./test/saved_model/", "my_configuration.json")

>>> # If you'd like to try a minor variation to an existing configuration, you can also pass generation
>>> # arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored
>>> generation_config, unused_kwargs = GenerationConfig.from_pretrained(
...     "gpt2", top_k=1, foo=False, do_sample=True, return_unused_kwargs=True
... )
>>> generation_config.top_k
1

>>> unused_kwargs
{'foo': False}
```

#### `from_model_config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L927)

```py
( model_config: PretrainedConfig ) → export const metadata = 'undefined';GenerationConfig
```

参数

+   `model_config` (`PretrainedConfig`) — 将用于实例化生成配置的模型配置。

返回

[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)

从这些参数实例化的配置对象。

从[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)实例化[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。此函数可用于将可能包含生成参数的旧[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)对象转换为独立的[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L529)

```py
( save_directory: Union config_file_name: Union = None push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory` (`str`或`os.PathLike`) — 将保存配置JSON文件的目录（如果不存在，将创建）。

+   `config_file_name` (`str`或`os.PathLike`, *可选*, 默认为`"generation_config.json"`) — 要保存在`save_directory`中的生成配置JSON文件的名称。

+   `push_to_hub` (`bool`, *可选*, 默认为 `False`) — 在保存模型后是否将其推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`的名称）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。

将生成配置对象保存到目录`save_directory`，以便可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained)类方法重新加载它。

## GenerationMixin

### `class transformers.GenerationMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L321)

```py
( )
```

一个包含自回归文本生成所有函数的类，可作为[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)中的混合类使用。

该类公开[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，可用于：

+   通过调用[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)实现*贪婪解码*，如果`num_beams=1`且`do_sample=False`

+   通过调用[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)实现*对比搜索*，如果`penalty_alpha>0`且`top_k>1`

+   通过调用[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)实现*多项式采样*，如果`num_beams=1`且`do_sample=True`

+   通过调用[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)实现*束搜索解码*，如果`num_beams>1`且`do_sample=False`

+   通过调用[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)实现*束搜索多项式采样*，如果`num_beams>1`且`do_sample=True`

+   通过调用[group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)实现*多样化的束搜索解码*，如果`num_beams>1`且`num_beam_groups>1`

+   通过调用[constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)实现*受限束搜索解码*，如果`constraints!=None`或`force_words_ids!=None`

您不需要直接调用上述任何方法。而是将自定义参数值传递给“generate”。要了解更多有关解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1173)

```py
( inputs: Optional = None generation_config: Optional = None logits_processor: Optional = None stopping_criteria: Optional = None prefix_allowed_tokens_fn: Optional = None synced_gpus: Optional = None assistant_model: Optional = None streamer: Optional = None negative_prompt_ids: Optional = None negative_prompt_attention_mask: Optional = None **kwargs ) → export const metadata = 'undefined';ModelOutput or torch.LongTensor
```

参数

+   `inputs`（根据模态性质变化的`torch.Tensor`，*可选*）— 用作生成提示或模型输入到编码器的序列。如果为`None`，则该方法将使用`bos_token_id`和批量大小为1进行初始化。对于仅解码器模型，`inputs`应该是`input_ids`格式。对于编码器-解码器模型，*inputs*可以表示任何`input_ids`、`input_values`、`input_features`或`pixel_values`之一。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）— 用作生成调用的基本参数化的生成配置。传递给generate的`**kwargs`与`generation_config`的属性匹配将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。

+   `logits_processor`（`LogitsProcessorList`，*可选*）— 自定义logits处理器，用于补充从参数和生成配置构建的默认logits处理器。如果传递了已经使用参数或生成配置创建的logit处理器，则会引发错误。此功能适用于高级用户。

+   `stopping_criteria`（`StoppingCriteriaList`，*可选*）— 自定义停止标准，用于补充从参数和生成配置构建的默认停止标准。如果传递了已经使用参数或生成配置创建的停止标准，则会引发错误。如果您的停止标准取决于`scores`输入，请确保将`return_dict_in_generate=True, output_scores=True`传递给`generate`。此功能适用于高级用户。

+   `prefix_allowed_tokens_fn`（`Callable[[int, torch.Tensor], List[int]]`，*可选*）— 如果提供了此函数，则将束搜索限制为每个步骤仅允许的令牌。如果未提供，则不应用任何约束。此函数接受2个参数：批次ID `batch_id` 和 `input_ids`。它必须返回一个列表，其中包含下一代步骤的允许令牌，条件是批次ID `batch_id` 和先前生成的令牌 `inputs_ids`。此参数对于基于前缀的受限生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `synced_gpus`（`bool`，*可选*）— 是否继续运行while循环直到max_length。除非被覆盖，否则在DeepSpeed ZeRO Stage 3多GPU环境下，此标志将设置为`True`，以避免在其他GPU生成之前一个GPU完成生成时挂起。否则将设置为`False`。

+   `assistant_model`（`PreTrainedModel`，*可选*）— 一个可以用来加速生成的助手模型。助手模型必须具有完全相同的分词器。当使用助手模型预测候选令牌比使用您调用generate的模型进行生成要快得多时，加速就会实现。因此，助手模型应该要小得多。

+   `streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成的序列的Streamer对象。生成的令牌通过`streamer.put(token_ids)`传递，Streamer负责任何进一步的处理。

+   `negative_prompt_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 一些处理器（如CFG）需要的负提示。批量大小必须与输入批量大小匹配。这是一个实验性功能，可能在未来版本中会有破坏性的API更改。

+   `negative_prompt_attention_mask`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于`negative_prompt_ids`的Attention_mask。

+   `kwargs`（`Dict[str, Any]`，*可选*）— `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。如果模型是编码器-解码器模型，则编码器特定的kwargs不应该有前缀，解码器特定的kwargs应该以*decoder_*为前缀。

返回

[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)或`torch.LongTensor`

一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或当`config.return_dict_in_generate=True`时）或一个`torch.FloatTensor`。

如果模型*不是*编码器-解码器模型（`model.config.is_encoder_decoder=False`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型有：

+   [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)

+   [GenerateBeamDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamDecoderOnlyOutput)

如果模型是一个编码器-解码器模型（`model.config.is_encoder_decoder=True`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型有：

+   [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),

+   [GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)

为具有语言建模头的模型生成令牌id序列。

大多数控制生成的参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给generate()来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

#### `compute_transition_scores`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L919)

```py
( sequences: Tensor scores: Tuple beam_indices: Optional = None normalize_logits: bool = False ) → export const metadata = 'undefined';torch.Tensor
```

参数

+   `sequences`（`torch.LongTensor`）— 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则较短。

+   `scores`（`tuple(torch.FloatTensor)`）— 每个生成步骤中每个词汇令牌的转移分数。由条件于先前生成令牌的对数softmax的令牌的对数概率组成的波束转移分数元组，其中每个张量的形状为`(batch_size*num_beams, config.vocab_size)`，最多有`max_new_tokens`个元素（每个生成的令牌一个元素）。

+   `beam_indices`（`torch.LongTensor`，*可选*）— 在每个生成步骤生成的标记id的波束索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.LongTensor`。仅在生成时`num_beams>1`时需要。

+   `normalize_logits`（`bool`，*可选*，默认为`False`）— 是否对logits进行归一化（由于历史原因，可能未归一化）。

返回

`torch.Tensor`

形状为`(batch_size*num_return_sequences, sequence_length)`的`torch.Tensor`，包含转换分数（logits）

计算给定生成分数的序列的转换分数（如果使用波束搜索，则还有波束索引）。这是一种方便的方法，在生成时快速获取所选标记的分数。

示例：

```py
>>> from transformers import GPT2Tokenizer, AutoModelForCausalLM
>>> import numpy as np

>>> tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer.pad_token_id = tokenizer.eos_token_id
>>> inputs = tokenizer(["Today is"], return_tensors="pt")

>>> # Example 1: Print the scores for each token generated with Greedy Search
>>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)
>>> transition_scores = model.compute_transition_scores(
...     outputs.sequences, outputs.scores, normalize_logits=True
... )
>>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for
>>> # encoder-decoder models, like BART or T5.
>>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
>>> generated_tokens = outputs.sequences[:, input_length:]
>>> for tok, score in zip(generated_tokens[0], transition_scores[0]):
...     # | token | token string | logits | probability
...     print(f"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}")
|   262 |  the     | -1.414 | 24.33%
|  1110 |  day     | -2.609 | 7.36%
|   618 |  when    | -2.010 | 13.40%
|   356 |  we      | -1.859 | 15.58%
|   460 |  can     | -2.508 | 8.14%

>>> # Example 2: Reconstruct the sequence scores from Beam Search
>>> outputs = model.generate(
...     **inputs,
...     max_new_tokens=5,
...     num_beams=4,
...     num_return_sequences=4,
...     return_dict_in_generate=True,
...     output_scores=True,
... )
>>> transition_scores = model.compute_transition_scores(
...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False
... )
>>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.
>>> # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the
>>> # use case, you might want to recompute it with `normalize_logits=True`.
>>> # Tip 2: the output length does NOT include the input length
>>> output_length = np.sum(transition_scores.numpy() < 0, axis=1)
>>> length_penalty = model.generation_config.length_penalty
>>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)
>>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))
True
```

#### `greedy_search`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2172)

```py
( input_ids: LongTensor logits_processor: Optional = None stopping_criteria: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False streamer: Optional = None **model_kwargs )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 用作生成提示的序列。

+   `logits_processor`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)类的实例列表，用于修改应用于每个生成步骤的语言建模头的预测分数。

+   `stopping_criteria`（`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)类的实例列表，用于告知生成循环是否应该停止。

+   `max_length`（`int`，*可选*，默认为20）— **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。

+   `pad_token_id`（`int`，*可选*）— *填充*标记的id。

+   `eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。

+   `output_attentions`（`bool`，*可选*，默认为`False`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*，默认为`False`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `output_scores`（`bool`，*可选*，默认为`False`）— 是否返回预测分数。有关更多详细信息，请参见返回的张量下的`scores`。

+   `return_dict_in_generate`（`bool`，*可选*，默认为`False`）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行while循环直到max_length（对于ZeRO阶段3需要）

+   `streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。model_kwargs — 附加的特定于模型的关键字参数将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**贪婪解码**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForCausalLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     StoppingCriteriaList,
...     MaxLengthCriteria,
... )

>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")

>>> # set pad_token_id to eos_token_id because GPT2 does not have a PAD token
>>> model.generation_config.pad_token_id = model.generation_config.eos_token_id

>>> input_prompt = "It might be possible to"
>>> input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),
...     ]
... )
>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

>>> outputs = model.greedy_search(
...     input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
... )

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["It might be possible to get a better understanding of the nature of the problem, but it's not"]
```

#### `sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2433)

```py
( input_ids: LongTensor logits_processor: Optional = None stopping_criteria: Optional = None logits_warper: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False streamer: Optional = None **model_kwargs ) → export const metadata = 'undefined';GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput or torch.LongTensor
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 用作生成提示的序列。

+   `logits_processor`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改应用于每个生成步骤的语言建模头的预测分数的类派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)的实例列表。

+   `stopping_criteria`（`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告知生成循环是否应停止的类派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)的实例列表。

+   `logits_warper`（`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的类派生自[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)的实例列表。

+   `max_length`（`int`，*可选*，默认为20）— **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。

+   `pad_token_id`（`int`，*可选*）— *填充*标记的id。

+   `eos_token_id`（`Union[int, List[int]]`，*可选*）— *结束序列*标记的id。可选择使用列表设置多个*结束序列*标记。

+   `output_attentions`（`bool`，*可选*，默认为`False`）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*，默认为`False`）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `output_scores`（`bool`，*可选*，默认为`False`）— 是否返回预测分数。有关更多详细信息，请参见返回张量下的`scores`。

+   `return_dict_in_generate`（`bool`，*可选*，默认为`False`）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）— 是否继续运行while循环直到max_length（需要ZeRO阶段3）

+   `streamer`（`BaseStreamer`，*可选*）— 将用于流式传输生成的序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。model_kwargs — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

返回

[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)或`torch.LongTensor`

包含生成标记的`torch.LongTensor`（默认行为）或一个[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)，如果`model.config.is_encoder_decoder=False`且`return_dict_in_generate=True`，或一个[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)，如果`model.config.is_encoder_decoder=True`。

使用**多项式采样**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForCausalLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     TopKLogitsWarper,
...     TemperatureLogitsWarper,
...     StoppingCriteriaList,
...     MaxLengthCriteria,
... )
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
>>> model = AutoModelForCausalLM.from_pretrained("gpt2")

>>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token
>>> model.config.pad_token_id = model.config.eos_token_id
>>> model.generation_config.pad_token_id = model.config.eos_token_id

>>> input_prompt = "Today is a beautiful day, and"
>>> input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(15, eos_token_id=model.generation_config.eos_token_id),
...     ]
... )
>>> # instantiate logits processors
>>> logits_warper = LogitsProcessorList(
...     [
...         TopKLogitsWarper(50),
...         TemperatureLogitsWarper(0.7),
...     ]
... )

>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

>>> torch.manual_seed(0)
>>> outputs = model.sample(
...     input_ids,
...     logits_processor=logits_processor,
...     logits_warper=logits_warper,
...     stopping_criteria=stopping_criteria,
... )

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Today is a beautiful day, and we must do everything possible to make it a day of celebration.']
```

#### `beam_search`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2743)

```py
( input_ids: LongTensor beam_scorer: BeamScorer logits_processor: Optional = None stopping_criteria: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False **model_kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。

+   `beam_scorer` (`BeamScorer`) — 一个派生自[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。

+   `logits_processor` (`LogitsProcessorList`，*可选*）— [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。从[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)派生的类的实例列表，用于修改应用于每个生成步骤的语言建模头的预测分数。

+   `stopping_criteria` (`StoppingCriteriaList`，*可选*）— [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。从[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)派生的类的实例列表，用于告知生成循环是否应该停止。

+   `max_length` (`int`，*可选*，默认为20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成的标记数量。要生成的序列的最大长度。

+   `pad_token_id` (`int`，*可选*）— *填充* 标记的id。

+   `eos_token_id` (`Union[int, List[int]]`，*可选*）— *序列结束* 标记的id。可选择使用列表设置多个*序列结束* 标记。

+   `output_attentions` (`bool`，*可选*，默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*，默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `output_scores` (`bool`，*可选*，默认为`False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量下的`scores`。

+   `return_dict_in_generate` (`bool`，*可选*，默认为`False`) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `synced_gpus` (`bool`，*可选*，默认为`False`) — 是否继续运行 while 循环直到达到 max_length（ZeRO 阶段 3 所需）model_kwargs — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**beam search解码**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)。请改用`generate()`。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     BeamSearchScorer,
... )
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("t5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

>>> encoder_input_str = "translate English to German: How old are you?"
>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

>>> # lets run beam search using 3 beams
>>> num_beams = 3
>>> # define decoder start token ids
>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
>>> input_ids = input_ids * model.config.decoder_start_token_id

>>> # add encoder_outputs to model keyword arguments
>>> model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

>>> # instantiate beam scorer
>>> beam_scorer = BeamSearchScorer(
...     batch_size=1,
...     num_beams=num_beams,
...     device=model.device,
... )

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
...     ]
... )

>>> outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Wie alt bist du?']
```

#### `beam_sample`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3074)

```py
( input_ids: LongTensor beam_scorer: BeamScorer logits_processor: Optional = None stopping_criteria: Optional = None logits_warper: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False **model_kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 用作生成提示的序列。

+   `beam_scorer` (`BeamScorer`) — [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的派生实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。

+   `logits_processor` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改每个生成步骤应用的语言建模头的预测分数的从[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)派生类的实例列表。

+   `stopping_criteria` (`StoppingCriteriaList`, *optional*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告知生成循环是否应该停止的从[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)派生类的实例列表。

+   `logits_warper` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的扭曲的从[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)派生类的实例列表。

+   `max_length` (`int`, *optional*, defaults to 20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成的标记数量。要生成的序列的最大长度。

+   `pad_token_id` (`int`, *optional*) — *填充*标记的id。

+   `eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。

+   `output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。有关更多细节，请参见返回的张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*, defaults to `False`) — 是否返回所有层的隐藏状态。有关更多细节，请参见返回的张量中的`hidden_states`。

+   `output_scores` (`bool`, *optional*, defaults to `False`) — 是否返回预测分数。有关更多细节，请参见返回的张量中的`scores`。

+   `return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `synced_gpus` (`bool`, *optional*, defaults to `False`) — 是否继续运行while循环直到达到`max_length`（对于ZeRO阶段3需要）。`model_kwargs` — 额外的特定于模型的kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**波束搜索多项式抽样**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     TopKLogitsWarper,
...     TemperatureLogitsWarper,
...     BeamSearchScorer,
... )
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("t5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

>>> encoder_input_str = "translate English to German: How old are you?"
>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

>>> # lets run beam search using 3 beams
>>> num_beams = 3
>>> # define decoder start token ids
>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
>>> input_ids = input_ids * model.config.decoder_start_token_id

>>> # add encoder_outputs to model keyword arguments
>>> model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

>>> # instantiate beam scorer
>>> beam_scorer = BeamSearchScorer(
...     batch_size=1,
...     max_length=model.config.max_length,
...     num_beams=num_beams,
...     device=model.device,
... )

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
... )
>>> # instantiate logits processors
>>> logits_warper = LogitsProcessorList(
...     [
...         TopKLogitsWarper(50),
...         TemperatureLogitsWarper(0.7),
...     ]
... )

>>> outputs = model.beam_sample(
...     input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
... )

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Wie alt bist du?']
```

#### `对比搜索`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1715)

```py
( input_ids: LongTensor top_k: Optional = 1 penalty_alpha: Optional = 0 logits_processor: Optional = None logits_warper: Optional = None stopping_criteria: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False streamer: Optional = None sequential: Optional = None **model_kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。

+   `top_k` (`int`, *可选*，默认为1) — 用于重新排名对比搜索的候选集的大小

+   `penalty_alpha` (`float`, *可选*，默认为0) — 对比搜索的退化惩罚；当大于0时激活

+   `logits_processor` (`LogitsProcessorList`, *可选*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)类的实例列表，用于修改每个生成步骤中应用的语言建模头的预测分数。

+   `logits_warper` (`LogitsProcessorList`, *可选*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。派生自[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)类的实例列表，用于在每个生成步骤之前应用于多项式抽样的语言建模头的预测分数分布。

+   `stopping_criteria` (`StoppingCriteriaList`, *可选*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)类的实例列表，用于告知生成循环是否应该停止。

+   `pad_token_id` (`int`, *可选*) — *填充*标记的id。

+   `eos_token_id` (`Union[int, List[int]]`, *可选*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。

+   `output_attentions` (`bool`, *可选*，默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*，默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `output_scores` (`bool`, *可选*，默认为`False`) — 是否返回预测分数。有关更多详细信息，请查看返回张量下的`scores`。

+   `return_dict_in_generate` (`bool`, *可选*，默认为`False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `synced_gpus` (`bool`, *可选*，默认为`False`) — 是否继续运行while循环直到max_length（需要ZeRO阶段3）

+   `streamer` (`BaseStreamer`, *可选*) — 将用于流式传输生成序列的Streamer对象。生成的标记通过`streamer.put(token_ids)`传递，streamer负责任何进一步处理。

+   `sequential` (`bool`, *optional*) — 如果为True，则将topk隐藏状态计算从并行切换到顺序以减少内存。model_kwargs — 附加的模型特定关键字参数将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**对比搜索**为具有语言建模头的模型生成标记id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForCausalLM,
...     StoppingCriteriaList,
...     MaxLengthCriteria,
... )

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
>>> model = AutoModelForCausalLM.from_pretrained("facebook/opt-125m")
>>> # set pad_token_id to eos_token_id because OPT does not have a PAD token
>>> model.config.pad_token_id = model.config.eos_token_id
>>> input_prompt = "DeepMind Company is"
>>> input_ids = tokenizer(input_prompt, return_tensors="pt")
>>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=64)])
>>> outputs = model.contrastive_search(
...     **input_ids, penalty_alpha=0.6, top_k=4, stopping_criteria=stopping_criteria
... )
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['DeepMind Company is a company that focuses on the development and commercialization of artificial intelligence (AI). DeepMind’s mission is to help people understand and solve problems that are difficult to solve in the world today.\n\nIn this post, we talk about the benefits of deep learning in business and how it']
```

#### `group_beam_search`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3411)

```py
( input_ids: LongTensor beam_scorer: BeamScorer logits_processor: Optional = None stopping_criteria: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: bool = False **model_kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 用作生成提示的序列。

+   `beam_scorer` (`BeamScorer`) — 一个派生自[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的实例，定义了在生成过程中如何构建、存储和排序beam假设。有关更多信息，请阅读[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)的文档。

+   `logits_processor` (`LogitsProcessorList`, *optional*) — 一个[LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList)的实例。用于修改应用于每个生成步骤的语言建模头的预测分数的类派生自[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)的实例列表。

+   `stopping_criteria` (`StoppingCriteriaList`, *optional*) — 一个[StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList)的实例。用于告诉生成循环是否应该停止的类派生自[StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)的实例列表。

+   `max_length` (`int`, *optional*, 默认为20) — **已弃用**。直接使用`logits_processor`或`stopping_criteria`来限制生成标记的数量。要生成的序列的最大长度。

+   `pad_token_id` (`int`, *optional*) — *填充*标记的id。

+   `eos_token_id` (`Union[int, List[int]]`, *optional*) — *序列结束*标记的id。可选择使用列表设置多个*序列结束*标记。

+   `output_attentions` (`bool`, *optional*, 默认为`False`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*, 默认为`False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `output_scores` (`bool`, *optional*, 默认为`False`) — 是否返回预测分数。有关更多详细信息，请参阅返回张量下的`scores`。

+   `return_dict_in_generate` (`bool`, *optional*, 默认为`False`) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `synced_gpus` (`bool`, *optional*, 默认为`False`) — 是否继续运行while循环直到max_length（需要ZeRO阶段3）

    model_kwargs — 附加的模型特定kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**多样化束搜索解码**为具有语言建模头的模型生成令牌id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用 [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)。请改用 generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     HammingDiversityLogitsProcessor,
...     BeamSearchScorer,
... )
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("t5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

>>> encoder_input_str = "translate English to German: How old are you?"
>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

>>> # lets run diverse beam search using 6 beams
>>> num_beams = 6
>>> # define decoder start token ids
>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
>>> input_ids = input_ids * model.config.decoder_start_token_id

>>> # add encoder_outputs to model keyword arguments
>>> model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

>>> # instantiate beam scorer
>>> beam_scorer = BeamSearchScorer(
...     batch_size=1,
...     max_length=model.config.max_length,
...     num_beams=num_beams,
...     device=model.device,
...     num_beam_groups=3,
... )

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [
...         HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
...     ]
... )

>>> outputs = model.group_beam_search(
...     input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
... )

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Wie alt bist du?']
```

#### `constrained_beam_search`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3796)

```py
( input_ids: LongTensor constrained_beam_scorer: ConstrainedBeamSearchScorer logits_processor: Optional = None stopping_criteria: Optional = None max_length: Optional = None pad_token_id: Optional = None eos_token_id: Union = None output_attentions: Optional = None output_hidden_states: Optional = None output_scores: Optional = None return_dict_in_generate: Optional = None synced_gpus: Optional = None **model_kwargs )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 用作生成提示的序列。

+   `constrained_beam_scorer` (`ConstrainedBeamSearchScorer`) — 表示如何在生成过程中构建、存储和排序束假设的 [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer) 的派生实例，同时满足一系列正面约束。有关更多信息，请阅读 [ConstrainedBeamSearchScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer) 的文档。

+   `logits_processor` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList) 的实例。用于修改每个生成步骤应用的语言建模头的预测分数的类派生实例的列表。

+   `stopping_criteria` (`StoppingCriteriaList`, *optional*) — [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList) 的实例。用于告知生成循环是否应该停止的类派生实例的列表。

+   `logits_warper` (`LogitsProcessorList`, *optional*) — [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList) 的实例。用于在每个生成步骤的多项式抽样之前应用于语言建模头的预测分数分布的类派生实例的列表。

+   `max_length` (`int`, *optional*, 默认为 20) — **已弃用**。直接使用 `logits_processor` 或 `stopping_criteria` 来限制生成的令牌数量。要生成的序列的最大长度。

+   `pad_token_id` (`int`, *optional*) — *填充* 令牌的id。

+   `eos_token_id` (`Union[int, List[int]]`, *optional*) — *结束序列* 令牌的id。可选择使用列表设置多个 *结束序列* 令牌。

+   `output_attentions` (`bool`, *optional*, 默认为 `False`) — 是否返回所有注意力层的注意力张量。有关更多细节，请参见返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*, 默认为 `False`) — 是否返回所有层的隐藏状态。有关更多细节，请参见返回张量中的 `hidden_states`。

+   `output_scores` (`bool`, *optional*, 默认为 `False`) — 是否返回预测分数。有关更多细节，请参见返回张量中的 `scores`。

+   `return_dict_in_generate` (`bool`, *optional*, 默认为 `False`) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `synced_gpus`（`bool`，*可选*，默认为`False`）- 是否继续运行while循环直到max_length（需要ZeRO阶段3）model_kwargs-其他特定于模型的kwargs将被转发到模型的`forward`函数。如果模型是编码器-解码器模型，则kwargs应包括`encoder_outputs`。

使用**受限束搜索解码**为具有语言建模头的模型生成令牌id序列，可用于文本解码器、文本到文本、语音到文本和视觉到文本模型。

在大多数情况下，您不需要直接调用[constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)。请改用generate()。有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

示例：

```py
>>> from transformers import (
...     AutoTokenizer,
...     AutoModelForSeq2SeqLM,
...     LogitsProcessorList,
...     MinLengthLogitsProcessor,
...     ConstrainedBeamSearchScorer,
...     PhrasalConstraint,
... )
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("t5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

>>> encoder_input_str = "translate English to German: How old are you?"
>>> encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

>>> # lets run beam search using 3 beams
>>> num_beams = 3
>>> # define decoder start token ids
>>> input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
>>> input_ids = input_ids * model.config.decoder_start_token_id

>>> # add encoder_outputs to model keyword arguments
>>> model_kwargs = {
...     "encoder_outputs": model.get_encoder()(
...         encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
...     )
... }

>>> constraint_str = "Sie"
>>> constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
>>> constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]

>>> # instantiate beam scorer
>>> beam_scorer = ConstrainedBeamSearchScorer(
...     batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
... )

>>> # instantiate logits processors
>>> logits_processor = LogitsProcessorList(
...     [
...         MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
...     ]
... )

>>> outputs = model.constrained_beam_search(
...     input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
... )

>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Wie alt sind Sie?']
```

## TFGenerationMixin

### `class transformers.TFGenerationMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L444)

```py
( )
```

包含支持生成的所有函数的类，可用作[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)中的mixin。

该类公开[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)，可用于：

+   *贪婪解码*，如果`num_beams=1`且`do_sample=False`，则调用`greedy_search()`

+   *对比搜索*，如果`penalty_alpha>0`和`top_k>1`，则调用`contrastive_search()`

+   *多项式采样*，如果`num_beams=1`且`do_sample=True`，则调用`sample()`

+   *束搜索解码*，如果`num_beams>1`，则调用`beam_search()`

您不需要直接调用上述任何方法。请将自定义参数值传递给“generate”而不是。要了解有关解码策略的更多信息，请参考[文本生成策略指南](../generation_strategies)。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L645)

```py
( inputs: Optional = None generation_config: Optional = None logits_processor: Optional = None seed = None **kwargs ) → export const metadata = 'undefined';ModelOutput or tf.Tensor
```

参数

+   `inputs`（`tf.Tensor`，根据模态性质而变化的形状，*可选*）- 用作生成提示或作为编码器的模型输入的序列。如果为`None`，则该方法将使用`bos_token_id`和批量大小为1进行初始化。对于仅解码器模型，`inputs`应为`input_ids`格式。对于编码器-解码器模型，*inputs*可以表示`input_ids`、`input_values`、`input_features`或`pixel_values`中的任何一个。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）- 用作生成调用的基本参数化的生成配置。传递给生成的`**kwargs`与`generation_config`的属性匹配将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。

+   `logits_processor`（`LogitsProcessorList`，*可选*）- 自定义logits处理器，补充从参数和生成配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或生成配置创建，则会抛出错误。此功能适用于高级用户。

+   `seed`（`List[int]`，*可选*）- 用于控制采样的随机种子，包含两个整数，在`do_sample`为`True`时使用。请参阅`tf.random`中无状态函数的`seed`参数。

+   `kwargs` (`Dict[str, Any]`, *optional*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。如果模型是编码器-解码器模型，则不应该为编码器特定kwargs添加前缀，而应该为解码器特定kwargs添加前缀*decoder_*。

返回

[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 或 `tf.Tensor`

一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)（如果`return_dict_in_generate=True`或`config.return_dict_in_generate=True`）或一个`tf.Tensor`。

如果模型*不是*编码器-解码器模型（`model.config.is_encoder_decoder=False`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型为：

+   [TFGreedySearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchDecoderOnlyOutput),

+   [TFSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleDecoderOnlyOutput),

+   [TFBeamSearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchDecoderOnlyOutput),

+   [TFBeamSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleDecoderOnlyOutput)

如果模型是编码器-解码器模型（`model.config.is_encoder_decoder=True`），可能的[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)类型为：

+   [TFGreedySearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchEncoderDecoderOutput),

+   [TFSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleEncoderDecoderOutput),

+   [TFBeamSearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchEncoderDecoderOutput),

+   [TFBeamSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleEncoderDecoderOutput)

为具有语言建模头的模型生成token id序列。

大多数控制生成的参数都在`generation_config`中设置，如果未传递，则将设置为模型的默认生成配置。您可以通过将相应的参数传递给generate来覆盖任何`generation_config`，例如`.generate(inputs, num_beams=4, do_sample=True)`。

有关生成策略和代码示例的概述，请查看[以下指南](../generation_strategies)。

#### `compute_transition_scores`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L477)

```py
( sequences: Tensor scores: Tuple beam_indices: Optional = None normalize_logits: bool = False ) → export const metadata = 'undefined';tf.Tensor
```

参数

+   `sequences` (`tf.Tensor`) — 生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短一些。

+   `scores` (`tuple(tf.Tensor)`) — 每个生成步骤中每个词汇标记的转移分数。Beam转移分数由tokens的log概率组成，条件是先前生成的tokens的log softmax。形状为`(batch_size*num_beams, config.vocab_size)`的`tf.Tensor`元组，最多包含`max_new_tokens`个元素（每个生成的token一个元素）。

+   `beam_indices` (`tf.Tensor`, *optional*) — 每个生成步骤中生成的token id的beam索引。形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`。在生成时如果`num_beams>1`则是必需的。

+   `normalize_logits` (`bool`, *optional*, defaults to `False`) — 是否对logits进行归一化（由于历史原因，可能未归一化）。

返回

`tf.Tensor`

一个形状为`(batch_size*num_return_sequences, sequence_length)`的`tf.Tensor`，包含转移分数（logits）

计算给定生成分数的序列的转移分数（以及如果使用了束搜索，则为束索引）。这是一个方便的方法，在生成时快速获取所选标记的分数。

示例：

```py
>>> from transformers import GPT2Tokenizer, TFAutoModelForCausalLM
>>> import numpy as np

>>> tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
>>> model = TFAutoModelForCausalLM.from_pretrained("gpt2")
>>> tokenizer.pad_token_id = tokenizer.eos_token_id
>>> inputs = tokenizer(["Today is"], return_tensors="tf")

>>> # Example 1: Print the scores for each token generated with Greedy Search
>>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)
>>> transition_scores = model.compute_transition_scores(
...     outputs.sequences, outputs.scores, normalize_logits=True
... )
>>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for
>>> # encoder-decoder models, like BART or T5.
>>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]
>>> generated_tokens = outputs.sequences[:, input_length:]
>>> for tok, score in zip(generated_tokens[0], transition_scores[0]):
...     # | token | token string | logits | probability
...     print(f"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}")
|   262 |  the     | -1.413 | 24.33%
|  1110 |  day     | -2.609 | 7.36%
|   618 |  when    | -2.009 | 13.41%
|   356 |  we      | -1.859 | 15.58%
|   460 |  can     | -2.508 | 8.14%

>>> # Example 2: Reconstruct the sequence scores from Beam Search
>>> outputs = model.generate(
...     **inputs,
...     max_new_tokens=5,
...     num_beams=4,
...     num_return_sequences=4,
...     return_dict_in_generate=True,
...     output_scores=True,
... )
>>> transition_scores = model.compute_transition_scores(
...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False
... )
>>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.
>>> # Tip: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the
>>> # use case, you might want to recompute it with `normalize_logits=True`.
>>> output_length = input_length + np.sum(transition_scores.numpy() < 0, axis=1)
>>> length_penalty = model.generation_config.length_penalty
>>> reconstructed_scores = np.sum(transition_scores, axis=1) / (output_length**length_penalty)
>>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))
True
```

## FlaxGenerationMixin

### `class transformers.FlaxGenerationMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L129)

```py
( )
```

包含用于自回归文本生成的所有函数的类，可作为[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)中的mixin使用。

该类公开了[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)，可用于：

+   *贪婪解码*，如果`num_beams=1`且`do_sample=False`，则调用`_greedy_search()`

+   *多项式采样*，如果`num_beams=1`且`do_sample=True`，则调用`_sample()`。

+   *束搜索解码*，如果`num_beams>1`且`do_sample=False`，则调用`_beam_search()`

您无需直接调用上述任何方法。而是将自定义参数值传递给“generate”。要了解更多关于解码策略的信息，请参考[文本生成策略指南](../generation_strategies)。

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L267)

```py
( input_ids: Array generation_config: Optional = None prng_key: Optional = None trace: bool = True params: Optional = None logits_processor: Optional = None **kwargs )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）—用作生成提示的序列。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）—用作生成调用的基本参数化的生成配置。传递给generate的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，则将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。

+   `trace`（`bool`，*可选*，默认为`True`）—是否跟踪生成。设置`trace=False`仅用于调试，会导致运行时间明显变慢。

+   `params`（`Dict[str, jnp.ndarray]`，*可选*）—可选择传递模型参数。对于并行化生成可能会有用。

+   `logits_processor`（`FlaxLogitsProcessorList`，*可选*）—自定义logits处理器，补充了从参数和生成配置构建的默认logits处理器。如果传递了已经使用参数或生成配置创建的logit处理器，则会抛出错误。此功能旨在供高级用户使用。

+   `kwargs`（`Dict[str, Any]`，*可选*）—`generate_config`的临时参数化和/或将转发给模型的`forward`函数的其他特定于模型的kwargs。如果模型是编码器-解码器模型，则不应添加编码器特定的kwargs前缀，而应为解码器特定的kwargs添加*decoder_*前缀。

为具有语言建模头的模型生成令牌id序列。
