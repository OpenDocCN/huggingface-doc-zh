- en: From Q-Learning to Deep Q-Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit3/from-q-to-dqn](https://huggingface.co/learn/deep-rl-course/unit3/from-q-to-dqn)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: We learned that **Q-Learning is an algorithm we use to train our Q-Function**,
    an **action-value function** that determines the value of being at a particular
    state and taking a specific action at that state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-function](../Images/d98598351e812f60049067f862f79c69.png)'
  prefs: []
  type: TYPE_IMG
- en: The **Q comes from “the Quality” of that action at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: Internally, our Q-function is encoded by **a Q-table, a table where each cell
    corresponds to a state-action pair value.** Think of this Q-table as **the memory
    or cheat sheet of our Q-function.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that Q-Learning is a *tabular method*. This becomes a problem
    if the states and actions spaces **are not small enough to be represented efficiently
    by arrays and tables**. In other words: it is **not scalable**. Q-Learning worked
    well with small state space environments like:'
  prefs: []
  type: TYPE_NORMAL
- en: FrozenLake, we had 16 states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taxi-v3, we had 500 states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But think of what we’re going to do today: we will train an agent to learn
    to play Space Invaders, a more complex game, using the frames as input.'
  prefs: []
  type: TYPE_NORMAL
- en: As **[Nikita Melkozerov mentioned](https://twitter.com/meln1k), Atari environments** have
    an observation space with a shape of (210, 160, 3)*, containing values ranging
    from 0 to 255 so that gives us<math><semantics><mrow><mn>25</mn><msup><mn>6</mn><mrow><mn>210</mn><mo>×</mo><mn>160</mn><mo>×</mo><mn>3</mn></mrow></msup><mo>=</mo><mn>25</mn><msup><mn>6</mn><mn>100800</mn></msup></mrow><annotation
    encoding="application/x-tex">256^{210 \times 160 \times 3} = 256^{100800}</annotation></semantics></math>256210×160×3=256100800
    possible observations (for comparison, we have approximately<math><semantics><mrow><mn>1</mn><msup><mn>0</mn><mn>80</mn></msup></mrow><annotation
    encoding="application/x-tex">10^{80}</annotation></semantics></math>1080 atoms
    in the observable universe).
  prefs: []
  type: TYPE_NORMAL
- en: A single frame in Atari is composed of an image of 210x160 pixels. Given that
    the images are in color (RGB), there are 3 channels. This is why the shape is
    (210, 160, 3). For each pixel, the value can go from 0 to 255.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Atari State Space](../Images/51e288086cae4a16b8a0ca90888a8264.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the state space is gigantic; due to this, creating and updating a
    Q-table for that environment would not be efficient. In this case, the best idea
    is to approximate the Q-values using a parametrized Q-function <math><semantics><mrow><msub><mi>Q</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q_{\theta}(s,a)</annotation></semantics></math>Qθ​(s,a)
    .
  prefs: []
  type: TYPE_NORMAL
- en: This neural network will approximate, given a state, the different Q-values
    for each possible action at that state. And that’s exactly what Deep Q-Learning
    does.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Q Learning](../Images/1e4c6acfdf811be054c82941f53e5853.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand Deep Q-Learning, let’s dive deeper into the Deep Q-Network.
  prefs: []
  type: TYPE_NORMAL
