# LayoutXLM

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutxlm)

## 概述

LayoutXLM是由Yiheng Xu、Tengchao Lv、Lei Cui、Guoxin Wang、Yijuan Lu、Dinei Florencio、Cha Zhang、Furu Wei提出的[LayoutXLM: 多模态预训练用于多语言视觉丰富文档理解](https://arxiv.org/abs/2104.08836)。它是在53种语言上训练的[LayoutLMv2模型](https://arxiv.org/abs/2012.14740)的多语言扩展。

该论文的摘要如下：

*最近，使用文本、布局和图像进行多模态预训练已经在视觉丰富文档理解任务中取得了SOTA性能，这表明跨不同模态的联合学习具有巨大潜力。在本文中，我们提出了LayoutXLM，这是一个用于多语言文档理解的多模态预训练模型，旨在消除视觉丰富文档理解的语言障碍。为了准确评估LayoutXLM，我们还介绍了一个名为XFUN的多语言表单理解基准数据集，其中包括7种语言（中文、日文、西班牙文、法文、意大利文、德文、葡萄牙文）的表单理解样本，并为每种语言手动标记了键值对。实验结果表明，LayoutXLM模型在XFUN数据集上明显优于现有的SOTA跨语言预训练模型。*

该模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/microsoft/unilm)找到。

## 使用提示和示例

可以直接将LayoutXLM的权重插入到LayoutLMv2模型中，如下所示：

```py
from transformers import LayoutLMv2Model

model = LayoutLMv2Model.from_pretrained("microsoft/layoutxlm-base")
```

请注意，LayoutXLM有自己的分词器，基于[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)。您可以按以下方式初始化它：

```py
from transformers import LayoutXLMTokenizer

tokenizer = LayoutXLMTokenizer.from_pretrained("microsoft/layoutxlm-base")
```

与LayoutLMv2类似，您可以使用[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)（内部应用[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)和[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)/[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)）来为模型准备所有数据。

由于LayoutXLM的架构等同于LayoutLMv2，可以参考[LayoutLMv2的文档页面](layoutlmv2)获取所有提示、代码示例和笔记本。

## LayoutXLMTokenizer

### `class transformers.LayoutXLMTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L149)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件路径。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 在预训练期间使用的序列开始标记。可用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *可选*, 默认为 `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类（整个序列的分类，而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为这个标记。

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。

+   `sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`) — 用于特殊[SEP]标记的边界框。

+   `pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。

+   `pad_token_label` (`int`, *optional*, defaults to -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。

+   `only_label_first_subword` (`bool`, *optional*, defaults to `True`) — 是否仅标记第一个子词，如果提供了单词标签。

+   `sp_model_kwargs` (`dict`, *optional*) — 将传递给`SentencePieceProcessor.__init__()`方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可以用于设置：

    +   `enable_sampling`：启用子词正则化。

    +   `nbest_size`：unigram的采样参数。对于BPE-Dropout无效。

        +   `nbest_size = {0,1}`：不执行采样。

        +   `nbest_size > 1`：从nbest_size结果中采样。

        +   `nbest_size < 0`：假设nbest_size是无限的，并使用前向过滤和后向采样算法从所有假设（格）中采样。

    +   `alpha`：用于unigram采样的平滑参数，以及BPE-dropout的合并操作的丢弃概率。

+   `sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。

改编自[RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)和[XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。

这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。

`__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L442)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题）或一个字符串列表的列表（单词批次）。

+   `text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预分词的字符串）。

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级边界框。每个边界框应该被归一化为0-1000的比例。

+   `word_labels` (`List[int]`, `List[List[int]]`, *optional*) — 单词级整数标签（用于标记分类任务，如FUNSD、CORD）。

+   `add_special_tokens` (`bool`, *optional*, defaults to `True`) — 是否对序列进行编码，相对于其模型使用特殊标记。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, 默认为 `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度，使用参数`max_length`指定，或者填充到模型的最大可接受输入长度，如果未提供该参数。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, 默认为 `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则逐个截断token，从一对序列中最长的序列中删除一个token。

    +   `'only_first'`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则仅截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度，使用参数`max_length`指定，或者截断到模型的最大可接受输入长度，如果未提供该参数。如果提供了一对序列（或一批对序列），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出序列长度大于模型最大可接受输入大小的批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *optional*, 默认为 0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出token将包含截断序列末尾的一些token，以提供截断和溢出序列之间的一些重叠。该参数的值定义重叠token的数量。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`：返回TensorFlow `tf.constant`对象。

    +   `'pt'`：返回PyTorch `torch.Tensor`对象。

    +   `'np'`：返回Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *optional*) — 是否返回token类型ID。如果保持默认设置，将根据特定分词器的默认设置返回token类型ID，由`return_outputs`属性定义。

    [什么是token类型ID？](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认设置返回注意力掩码，由`return_outputs`属性定义。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *optional*, 默认为 `False`) — 是否返回溢出的标记序列。如果提供了一对输入ID序列（或一批对），并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的标记。

+   `return_special_tokens_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回特殊标记掩码信息。

+   `return_offsets_mapping` (`bool`, *optional*, 默认为 `False`) — 是否返回每个标记的 `(char_start, char_end)`。

    仅在继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 的快速标记器上可用，如果使用Python的标记器，此方法将引发 `NotImplementedError`。

+   `return_length` (`bool`, *optional*, 默认为 `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *optional*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()` 方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的标记ID列表。

    [什么是输入ID？](../glossary#input-ids)

+   `bbox` — 要提供给模型的边界框列表。

+   `token_type_ids` — 要提供给模型的标记类型ID列表（当 `return_token_type_ids=True` 或 *`token_type_ids`* 在 `self.model_input_names` 中时）。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当 `return_attention_mask=True` 或 *`attention_mask`* 在 `self.model_input_names` 中时）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `labels` — 要提供给模型的标签列表（当指定 `word_labels` 时）。

+   `overflowing_tokens` — 溢出的标记序列列表（当指定 `max_length` 并且 `return_overflowing_tokens=True` 时）。

+   `num_truncated_tokens` — 被截断的标记数（当指定 `max_length` 并且 `return_overflowing_tokens=True` 时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当 `add_special_tokens=True` 和 `return_special_tokens_mask=True` 时）。

+   `length` — 输入的长度（当 `return_length=True` 时）。

标记化和准备模型的一个或多个序列或一个或多个序列对的主要方法，具有单词级归一化的边界框和可选标签。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L314)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 要添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *optional*) — 可选的第二个序列对的ID列表。

返回

`List[int]`

具有适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。XLM-RoBERTa序列的格式如下：

+   单个序列：`<s> X </s>`

+   序列对：`<s> A </s></s> B </s>`

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L340)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *optional*) — 可选的第二个序列对的ID列表。

+   `already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化为模型。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。在使用分词器的`prepare_for_model`方法添加特殊标记时调用此方法。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L368)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个ID列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建用于序列对分类任务的掩码。XLM-RoBERTa不使用标记类型ID，因此返回零的列表。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm.py#L425)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LayoutXLMTokenizerFast

### `class transformers.LayoutXLMTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L152)

```py
( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' cls_token_box = [0, 0, 0, 0] sep_token_box = [1000, 1000, 1000, 1000] pad_token_box = [0, 0, 0, 0] pad_token_label = -100 only_label_first_subword = True **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件路径。

+   `bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开始标记。可以用作序列分类器标记。

    在使用特殊标记构建序列时，这不是用于序列开始的标记。使用的标记是`cls_token`。

+   `eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在构建来自多个序列的序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于序列分类时使用的分类器标记（对整个序列进行分类，而不是对每个标记进行分类）。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `pad_token` (`str`, *optional*, defaults to `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `mask_token` (`str`, *optional*, defaults to `"<mask>"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[CLS]标记的边界框。

+   `sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`) — 用于特殊[SEP]标记的边界框。

+   `pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) — 用于特殊[PAD]标记的边界框。

+   `pad_token_label` (`int`, *optional*, defaults to -100) — 用于填充标记的标签。默认为-100，这是PyTorch的CrossEntropyLoss的`ignore_index`。

+   `only_label_first_subword` (`bool`, *optional*, defaults to `True`) — 是否仅标记第一个子词，如果提供了单词标签。

+   `additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED", "</s>NOTUSED"]`) — 分词器使用的额外特殊标记。

构建一个“快速” LayoutXLM 分词器（由 HuggingFace 的 *tokenizers* 库支持）。改编自 [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer) 和 [XLNetTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlnet#transformers.XLNetTokenizer)。基于 [BPE](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models)。

此分词器继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py#L272)

```py
( text: Union text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列可以是一个字符串，一个字符串列表（单个示例的单词或一批示例的问题），或一个字符串列表的列表（单词批次）。

+   `text_pair` (`List[str]`, `List[List[str]]`) — 要编码的序列或序列批次。每个序列应该是一个字符串列表（预分词的字符串）。

+   `boxes` (`List[List[int]]`, `List[List[List[int]]]`) — 单词级别的边界框。每个边界框应标准化为 0-1000 的比例。

+   `word_labels` (`List[int]`, `List[List[int]]`, *可选*) — 单词级别的整数标签（用于诸如 FUNSD、CORD 等标记分类任务）。

+   `add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 是否使用相对于其模型的特殊标记对序列进行编码。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到指定的最大长度，该长度由参数 `max_length` 指定，或者填充到模型可接受的最大输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）: 无填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则将逐个标记截断，从该对中最长的序列中删除一个标记。

    +   `'only_first'`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。

    +   `'only_second'`: 截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）: 无截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数中需要最大长度。如果模型没有特定的最大输入长度（如 XLNet）截断/填充到最大长度将被禁用。

+   `stride` (`int`, *optional*, 默认为 0) — 如果设置为一个数字，并且与 `max_length` 一起使用，当 `return_overflowing_tokens=True` 时返回的溢出 token 将包含截断序列末尾的一些 token，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠 token 的数量。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。这对于在具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上启用 Tensor Cores 特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `return_token_type_ids` (`bool`, *optional*) — 是否返回 token 类型 ID。如果保持默认设置，将根据特定分词器的默认值返回 token 类型 ID，由 `return_outputs` 属性定义。

    [什么是 token 类型 ID？](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由 `return_outputs` 属性定义。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *optional*, 默认为 `False`) — 是否返回溢出的 token 序列。如果提供了一对输入 id 序列（或一批对）并且 `truncation_strategy = longest_first` 或 `True`，则会引发错误，而不是返回溢出的 token。

+   `return_special_tokens_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回特殊 token 掩码信息。

+   `return_offsets_mapping` (`bool`, *optional*, 默认为 `False`) — 是否返回每个 token 的 `(char_start, char_end)`。

    这仅适用于继承自 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 的快速分词器，如果使用 Python 的分词器，此方法将引发 `NotImplementedError`。

+   `return_length` (`bool`, *optional*, 默认为 `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *optional*, 默认为 `True`) — 是否打印更多信息和警告。 **kwargs — 传递给 `self.tokenize()` 方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

一个包含以下字段的 [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的 token id 列表。

    [什么是输入 ID？](../glossary#input-ids)

+   `bbox` — 要提供给模型的边界框列表。

+   `token_type_ids` — 要提供给模型的 token 类型 id 列表（当 `return_token_type_ids=True` 或 *`token_type_ids`* 在 `self.model_input_names` 中时）。

    [什么是 token 类型 ID？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些 token 应该被模型关注的索引列表（当 `return_attention_mask=True` 或 *`attention_mask`* 在 `self.model_input_names` 中时）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `labels` — 要提供给模型的标签列表（当指定 `word_labels` 时）。

+   `overflowing_tokens` — 溢出的 token 序列列表（当指定 `max_length` 并且 `return_overflowing_tokens=True` 时）。

+   `num_truncated_tokens` — 截断的标记数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`且`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）。

主要方法是对一个或多个序列或一个或多个序列对进行标记化和准备模型，其中包含单词级别的归一化边界框和可选标签。

## LayoutXLMProcessor

### `class transformers.LayoutXLMProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L26)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（`LayoutLMv2ImageProcessor`，*可选*） — [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)的实例。图像处理器是必需的输入。

+   `tokenizer`（`LayoutXLMTokenizer`或`LayoutXLMTokenizerFast`，*可选*） — [LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)或[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)的实例。标记器是必需的输入。

构建一个LayoutXLM处理器，将LayoutXLM图像处理器和LayoutXLM标记器组合成一个单一处理器。

[LayoutXLMProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor)提供了准备模型数据所需的所有功能。

它首先使用[LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)将文档图像调整为固定大小，并可选择应用OCR以获取单词和归一化边界框。然后将它们提供给[LayoutXLMTokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer)或[LayoutXLMTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast)，将单词和边界框转换为标记级别的`input_ids`、`attention_mask`、`token_type_ids`、`bbox`。可选地，可以提供整数`word_labels`，这些标签将转换为用于标记分类任务（如FUNSD、CORD）的标记级别`labels`。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutxlm/processing_layoutxlm.py#L67)

```py
( images text: Union = None text_pair: Union = None boxes: Union = None word_labels: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )
```

该方法首先将`images`参数转发给`~LayoutLMv2ImagePrpcessor.__call__`。如果`LayoutLMv2ImagePrpcessor`初始化时`apply_ocr`设置为`True`，它将获取的单词和边界框以及其他参数传递给[**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)并返回输出，同时返回调整大小后的`images`。如果`LayoutLMv2ImagePrpcessor`初始化时`apply_ocr`设置为`False`，它将用户指定的单词（`text`/`text_pair`）和`boxes`以及其他参数传递给[__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer.__call__)并返回输出，同时返回调整大小后的`images`。

更多信息请参考上述两种方法的文档字符串。
