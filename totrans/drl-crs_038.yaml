- en: Glossary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/glossary](https://huggingface.co/learn/deep-rl-course/unit2/glossary)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: This is a community-created glossary. Contributions are welcomed!
  prefs: []
  type: TYPE_NORMAL
- en: Strategies to find the optimal policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Policy-based methods.** The policy is usually trained with a neural network
    to select what action to take given a state. In this case it is the neural network
    which outputs the action that the agent should take instead of using a value function.
    Depending on the experience received by the environment, the neural network will
    be re-adjusted and will provide better actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value-based methods.** In this case, a value function is trained to output
    the value of a state or a state-action pair that will represent our policy. However,
    this value doesn’t define what action the agent should take. In contrast, we need
    to specify the behavior of the agent given the output of the value function. For
    example, we could decide to adopt a policy to take the action that always leads
    to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy
    (or whatever decision the user takes) that uses the values of the value-function
    to decide the actions to take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the value-based methods, we can find two main strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The state-value function.** For each state, the state-value function is the
    expected return if the agent starts in that state and follows the policy until
    the end.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The action-value function.** In contrast to the state-value function, the
    action-value calculates for each state and action pair the expected return if
    the agent starts in that state, takes that action, and then follows the policy
    forever after.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Epsilon-greedy strategy:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Common strategy used in reinforcement learning that involves balancing exploration
    and exploitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chooses the action with the highest expected reward with a probability of 1-epsilon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chooses a random action with a probability of epsilon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epsilon is typically decreased over time to shift focus towards exploitation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greedy strategy:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Involves always choosing the action that is expected to lead to the highest
    reward, based on the current knowledge of the environment. (Only exploitation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always chooses the action with the highest expected reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does not include any exploration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be disadvantageous in environments with uncertainty or unknown optimal actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-policy vs on-policy algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Off-policy algorithms:** A different policy is used at training time and
    inference time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-policy algorithms:** The same policy is used during training and inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo and Temporal Difference learning strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Monte Carlo (MC):** Learning at the end of the episode. With Monte Carlo,
    we wait until the episode ends and then we update the value function (or policy
    function) from a complete episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal Difference (TD):** Learning at each step. With Temporal Difference
    Learning, we update the value function (or policy function) at each step without
    requiring a complete episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to improve the course, you can [open a Pull Request.](https://github.com/huggingface/deep-rl-class/pulls)
  prefs: []
  type: TYPE_NORMAL
- en: 'This glossary was made possible thanks to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ramón Rueda](https://github.com/ramon-rd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hasarindu Perera](https://github.com/hasarinduperera/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Arkady Arkhangorodsky](https://github.com/arkadyark/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
