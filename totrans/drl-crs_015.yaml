- en: Train your first Deep Reinforcement Learning Agent ü§ñ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: [![Ask
    a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you‚Äôve studied the bases of Reinforcement Learning, you‚Äôre ready to
    train your first agent and share it with the community through the Hub üî•: A Lunar
    Lander agent that will learn to land correctly on the Moon üåï'
  prefs: []
  type: TYPE_NORMAL
- en: '![LunarLander](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
  prefs: []
  type: TYPE_IMG
- en: And finally, you‚Äôll **upload this trained agent to the Hugging Face Hub ü§ó, a
    free, open platform where people can share ML models, datasets, and demos.**
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to our [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    you‚Äôll be able to compare your results with other classmates and exchange the
    best practices to improve your agent‚Äôs scores. Who will win the challenge for
    Unit 1 üèÜ?
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  prefs: []
  type: TYPE_NORMAL
- en: '**If you don‚Äôt find your model, go to the bottom of the page and click on the
    refresh button.**'
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here üëâ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  prefs: []
  type: TYPE_NORMAL
- en: So let‚Äôs get started! üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on Open In Colab button** üëá :'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 1: Train your first Deep Reinforcement Learning Agent ü§ñ'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 1 thumbnail](../Images/269d50061313727de39330c553eb4733.png)'
  prefs: []
  type: TYPE_IMG
- en: In this notebook, you‚Äôll train your **first Deep Reinforcement Learning agent**
    a Lunar Lander agent that will learn to **land correctly on the Moon üåï**. Using
    [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep
    Reinforcement Learning library, share them with the community, and experiment
    with different configurations
  prefs: []
  type: TYPE_NORMAL
- en: The environment üéÆ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The library used üìö
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We‚Äôre constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook üèÜ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Gymnasium**, the environment library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to use **Stable-Baselines3**, the deep reinforcement learning library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score üî•.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This notebook is from Deep Reinforcement Learning Course
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this free course, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: üìñ Study Deep Reinforcement Learning in **theory and practice**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü§ñ Train **agents in unique environments**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üéì **Earn a certificate of completion** by completing 80% of the assignments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more!
  prefs: []
  type: TYPE_NORMAL
- en: Check üìö the syllabus üëâ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able to¬†**send you the links when each Unit is published
    and give you information about the challenges and updates).**
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep in touch and ask questions is **to join our discord server**
    to exchange with the community and with us üëâüèª [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites üèóÔ∏è
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: üî≤ üìù **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)**
    that gives you all the **information about the course and helps you to onboard**
    ü§ó
  prefs: []
  type: TYPE_NORMAL
- en: üî≤ üìö **Develop an understanding of the foundations of Reinforcement learning**
    (MC, TD, Rewards hypothesis‚Ä¶) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).
  prefs: []
  type: TYPE_NORMAL
- en: A small recap of Deep Reinforcement Learning üìö
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let‚Äôs do a small recap on what we learned in the first Unit:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is a **computational approach to learning from actions**.
    We build an agent that learns from the environment by **interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of any RL agent is to **maximize its expected cumulative reward** (also
    called expected return) because RL is based on the *reward hypothesis*, which
    is that all goals can be described as the maximization of an expected cumulative
    reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL process is a **loop that outputs a sequence of state, action, reward,
    and next state**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the expected cumulative reward (expected return), **we discount
    the rewards**: the rewards that come sooner (at the beginning of the game) are
    more probable to happen since they are more predictable than the long-term future
    reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve an RL problem, you want to **find an optimal policy**; the policy is
    the ‚Äúbrain‚Äù of your AI that will tell us what action to take given a state. The
    optimal one is the one that gives you the actions that max the expected return.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are **two** ways to find your optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By **training your policy directly**: policy-based methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By **training a value function** that tells us the expected return the agent
    will get at each state and use this function to define our policy: value-based
    methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we spoke about Deep RL because **we introduce deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based) hence the name ‚Äúdeep.‚Äù**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs train our first Deep Reinforcement Learning agent and upload it to the
    Hub üöÄ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Get a certificate üéì
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: Set the GPU üí™
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent‚Äôs training, we‚Äôll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Install dependencies and create a virtual screen üîΩ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to install the dependencies, we‚Äôll install multiple ones.
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stable-baselines3[extra]`: The deep reinforcement learning library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face ü§ó Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things easier, we created a script to install all these dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: During the notebook, we‚Äôll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install virtual screen libraries and create and
    run a virtual screen üñ•
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To make sure the new installed libraries are used, **sometimes it‚Äôs required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so you‚Äôll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages üì¶
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One additional library we import is huggingface_hub **to be able to upload and
    download trained models from the hub**.
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face Hub ü§ó works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  prefs: []
  type: TYPE_NORMAL
- en: You can see here all the Deep reinforcement Learning models available hereüëâ
    [https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Understand Gymnasium and how it works ü§ñ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: üèã The library containing our environment is called Gymnasium. **You‚Äôll use Gymnasium
    a lot in Deep Reinforcement Learning.**
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gymnasium library provides two things:'
  prefs: []
  type: TYPE_NORMAL
- en: An interface that allows you to **create RL environments**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **collection of environments** (gym-control, atari, box2D‚Ä¶).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs look at an example, but first let‚Äôs recall the RL loop.
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At each step:'
  prefs: []
  type: TYPE_NORMAL
- en: Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the
    first frame of our game (Environment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent
    will move to the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive
    Reward +1)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With Gymnasium:'
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ We create our environment using `gymnasium.make()`
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step:'
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Get an action using our model (in our example we take a random action)
  prefs: []
  type: TYPE_NORMAL
- en: 4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and
    get
  prefs: []
  type: TYPE_NORMAL
- en: '`observation`: The new state (st+1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The reward we get after executing the action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terminated`: Indicates if the episode terminated (agent reach the terminal
    state)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncated`: Introduced with this new version, it indicates a timelimit or
    if an agent go out of bounds of the environment for instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: A dictionary that provides additional information (depends on the environment).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more explanations check this üëâ [https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
  prefs: []
  type: TYPE_NORMAL
- en: 'If the episode is terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: We reset the environment to its initial state with `observation = env.reset()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Let‚Äôs look at an example!** Make sure to read the code'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Create the LunarLander environment üåõ and understand how it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The environment üéÆ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),
    **to land correctly on the moon**. To do that, the agent needs to learn **to adapt
    its speed and position (horizontal, vertical, and angular) to land correctly.**
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: üí° A good habit when you start to use an environment is to check its documentation
  prefs: []
  type: TYPE_NORMAL
- en: üëâ [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs see what the Environment looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We see with `Observation Space Shape (8,)` that the observation is a vector
    of size 8, where each value contains different information about the lander:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pad coordinate (x)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical pad coordinate (y)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal speed (x)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical speed (y)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angular speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left leg contact point has touched the land (boolean)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the right leg contact point has touched the land (boolean)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available üéÆ:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action 0: Do nothing,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 1: Fire left orientation engine,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 2: Fire the main engine,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 3: Fire right orientation engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function (the function that will give a reward at each timestep) üí∞:'
  prefs: []
  type: TYPE_NORMAL
- en: After every step a reward is granted. The total reward of an episode is the
    **sum of the rewards for all the steps within that episode**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step, the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: Is increased/decreased the closer/further the lander is to the landing pad.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased/decreased the slower/faster the lander is moving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased the more the lander is tilted (angle not horizontal).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased by 10 points for each leg that is in contact with the ground.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.03 points each frame a side engine is firing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.3 points each frame the main engine is firing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode receive an **additional reward of -100 or +100 points for crashing
    or landing safely respectively.**
  prefs: []
  type: TYPE_NORMAL
- en: An episode is **considered a solution if it scores at least 200 points.**
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Environment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We create a vectorized environment (a method for stacking multiple independent
    environments into a single environment) of 16 environments, this way, **we‚Äôll
    have more diverse experiences during the training.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Create the Model ü§ñ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have studied our environment and we understood the problem: **being able
    to land the Lunar Lander to the Landing Pad correctly by controlling left, right
    and main orientation engine**. Now let‚Äôs build the algorithm we‚Äôre going to use
    to solve this Problem üöÄ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, we‚Äôre going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SB3 is a set of **reliable implementations of reinforcement learning algorithms
    in PyTorch**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'üí° A good habit when using a new library is to dive first on the documentation:
    [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)
    and then try some tutorials.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stable Baselines3](../Images/12dda719af36ee58d0b11fadfe3279ba.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we‚Äôre going to use SB3 **PPO**. [PPO (aka Proximal Policy
    Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning
    algorithms that you‚Äôll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO is a combination of:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Value-based reinforcement learning method*: learning an action-value function
    that will tell us the **most valuable action to take given a state and action**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy-based reinforcement learning method*: learning a policy that will **give
    us a probability distribution over actions**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 is easy to set up:'
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ You **create your environment** (in our case it was done above)
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model
    = PPO("MlpPolicy")`
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training
    timesteps
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Train the PPO agent üèÉ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs train our agent for 1,000,000 timesteps, don‚Äôt forget to use GPU on Colab.
    It will take approximately ~20min, but you can use fewer timesteps if you just
    want to try it out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training, take a ‚òï break you deserved it ü§ó
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate the agent üìà
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next step, we‚Äôll see **how to automatically evaluate and share your agent
    to compete in a leaderboard, but for now let‚Äôs do it ourselves**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí° When you evaluate your agent, you should not use your training environment
    but create an evaluation environment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million
    steps, which means that our lunar lander agent is ready to land on the moon üåõü•≥.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish our trained model on the Hub üî•
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub ü§ó with one line of code.
  prefs: []
  type: TYPE_NORMAL
- en: üìö The libraries documentation üëâ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face‚Äîx-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example of a Model Card (with Space Invaders):'
  prefs: []
  type: TYPE_NORMAL
- en: By using `package_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way:'
  prefs: []
  type: TYPE_NORMAL
- en: You can **showcase our work** üî•
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **visualize your agent playing** üëÄ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **share with the community an agent that others can use** üíæ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **access a leaderboard üèÜ to see how well your agent is performing compared
    to your classmates** üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ (If it‚Äôs not already done) create an account on Hugging Face ‚û° [https://huggingface.co/join](https://huggingface.co/join)
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don‚Äôt want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ We‚Äôre now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()`
    function
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs fill the `package_to_hub` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model`: our trained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name`: the name of the trained model that we defined in `model_save`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_architecture`: the model architecture we used, in our case PPO'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env_id`: the name of the environment, in our case `LunarLander-v2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_env`: the evaluation environment defined in eval_env'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí° **A good name is `{username}/{model_architecture}-{env_id}`**
  prefs: []
  type: TYPE_NORMAL
- en: '`commit_message`: message of the commit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Congrats ü•≥ you‚Äôve just trained and uploaded your first Deep Reinforcement Learning
    agent. The script above should have displayed a link to a model repository such
    as [https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3).
    When you go to this link, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: See a video preview of your agent at the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click ‚ÄúFiles and versions‚Äù to see all the files in the repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click ‚ÄúUse in stable-baselines3‚Äù to get a code snippet that shows how to load
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model card (`README.md` file) which gives a description of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the hood, the Hub uses git-based repositories (don‚Äôt worry if you don‚Äôt
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  prefs: []
  type: TYPE_NORMAL
- en: Compare the results of your LunarLander-v2 with your classmates using the leaderboard
    üèÜ üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: Load a saved LunarLander model from the Hub ü§ó
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to [ironbar](https://github.com/ironbar) for the contribution.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a saved model from the Hub is really easy.
  prefs: []
  type: TYPE_NORMAL
- en: You go to [https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)
    to see the list of all the Stable-baselines3 saved models.
  prefs: []
  type: TYPE_NORMAL
- en: You select one and copy its repo_id
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we just need to use load_from_hub with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The repo_id
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The filename: the saved model inside the repo and its extension (*.zip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the model I download from the Hub was trained with Gym (the former version
    of Gymnasium) we need to install shimmy a API conversion tool that will help us
    to run the environment correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shimmy Documentation: [https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs evaluate this agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Some additional challenges üèÜ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to learn **is to try things by your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  prefs: []
  type: TYPE_NORMAL
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some ideas to achieve so:'
  prefs: []
  type: TYPE_NORMAL
- en: Train more steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try different hyperparameters for `PPO`. You can see them at [https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    and try another model such as DQN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Push your new trained model** on the Hub üî•'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compare the results of your LunarLander-v2 with your classmates** using the
    [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    üèÜ'
  prefs: []
  type: TYPE_NORMAL
- en: Is moon landing too boring for you? Try to **change the environment**, why not
    use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the
    gym documentation](https://www.gymlibrary.dev/) and have fun üéâ.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Congrats on finishing this chapter! That was the biggest one, **and there was
    a lot of information.**
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre still feel confused with all these elements‚Ä¶it‚Äôs totally normal! **This
    was the same for me and for all people who studied RL.**
  prefs: []
  type: TYPE_NORMAL
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. It‚Äôs important to master these elements and have a solid foundations.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, during the course, we‚Äôre going to dive deeper into these concepts
    but **it‚Äôs better to have a good understanding of them now before diving into
    the next chapters.**
  prefs: []
  type: TYPE_NORMAL
- en: Next time, in the bonus unit 1, you‚Äôll train Huggy the Dog to fetch the stick.
  prefs: []
  type: TYPE_NORMAL
- en: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
  prefs: []
  type: TYPE_IMG
- en: Keep learning, stay awesome ü§ó
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
