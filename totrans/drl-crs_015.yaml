- en: Train your first Deep Reinforcement Learning Agent ü§ñ
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you‚Äôve studied the bases of Reinforcement Learning, you‚Äôre ready to
    train your first agent and share it with the community through the Hub üî•: A Lunar
    Lander agent that will learn to land correctly on the Moon üåï'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![LunarLander](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: And finally, you‚Äôll **upload this trained agent to the Hugging Face Hub ü§ó, a
    free, open platform where people can share ML models, datasets, and demos.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to our [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    you‚Äôll be able to compare your results with other classmates and exchange the
    best practices to improve your agent‚Äôs scores. Who will win the challenge for
    Unit 1 üèÜ?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**If you don‚Äôt find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here üëâ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: So let‚Äôs get started! üöÄ
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on Open In Colab button** üëá :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 1: Train your first Deep Reinforcement Learning Agent ü§ñ'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 1 thumbnail](../Images/269d50061313727de39330c553eb4733.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: In this notebook, you‚Äôll train your **first Deep Reinforcement Learning agent**
    a Lunar Lander agent that will learn to **land correctly on the Moon üåï**. Using
    [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep
    Reinforcement Learning library, share them with the community, and experiment
    with different configurations
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The environment üéÆ
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The library used üìö
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We‚Äôre constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook üèÜ
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to use **Stable-Baselines3**, the deep reinforcement learning library.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score üî•.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This notebook is from Deep Reinforcement Learning Course
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'In this free course, you will:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: üìñ Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü§ñ Train **agents in unique environments**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üéì **Earn a certificate of completion** by completing 80% of the assignments.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Check üìö the syllabus üëâ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able to¬†**send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep in touch and ask questions is **to join our discord server**
    to exchange with the community and with us üëâüèª [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites üèóÔ∏è
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: üî≤ üìù **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)**
    that gives you all the **information about the course and helps you to onboard**
    ü§ó
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: üî≤ üìö **Develop an understanding of the foundations of Reinforcement learning**
    (MC, TD, Rewards hypothesis‚Ä¶) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A small recap of Deep Reinforcement Learning üìö
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Let‚Äôs do a small recap on what we learned in the first Unit:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is a **computational approach to learning from actions**.
    We build an agent that learns from the environment by **interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of any RL agent is to **maximize its expected cumulative reward** (also
    called expected return) because RL is based on the *reward hypothesis*, which
    is that all goals can be described as the maximization of an expected cumulative
    reward.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL process is a **loop that outputs a sequence of state, action, reward,
    and next state**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the expected cumulative reward (expected return), **we discount
    the rewards**: the rewards that come sooner (at the beginning of the game) are
    more probable to happen since they are more predictable than the long-term future
    reward.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve an RL problem, you want to **find an optimal policy**; the policy is
    the ‚Äúbrain‚Äù of your AI that will tell us what action to take given a state. The
    optimal one is the one that gives you the actions that max the expected return.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are **two** ways to find your optimal policy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'By **training your policy directly**: policy-based methods.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By **training a value function** that tells us the expected return the agent
    will get at each state and use this function to define our policy: value-based
    methods.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we spoke about Deep RL because **we introduce deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based) hence the name ‚Äúdeep.‚Äù**
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs train our first Deep Reinforcement Learning agent and upload it to the
    Hub üöÄ
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Get a certificate üéì
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Set the GPU üí™
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent‚Äôs training, we‚Äôll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Install dependencies and create a virtual screen üîΩ
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to install the dependencies, we‚Äôll install multiple ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stable-baselines3[extra]`: The deep reinforcement learning library.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face ü§ó Hub.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things easier, we created a script to install all these dependencies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During the notebook, we‚Äôll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install virtual screen libraries and create and
    run a virtual screen üñ•
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To make sure the new installed libraries are used, **sometimes it‚Äôs required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so you‚Äôll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Import the packages üì¶
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One additional library we import is huggingface_hub **to be able to upload and
    download trained models from the hub**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face Hub ü§ó works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: You can see here all the Deep reinforcement Learning models available hereüëâ
    [https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Understand Gymnasium and how it works ü§ñ
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: üèã The library containing our environment is called Gymnasium. **You‚Äôll use Gymnasium
    a lot in Deep Reinforcement Learning.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gymnasium library provides two things:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: An interface that allows you to **create RL environments**.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **collection of environments** (gym-control, atari, box2D‚Ä¶).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs look at an example, but first let‚Äôs recall the RL loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'At each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the
    first frame of our game (Environment).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent
    will move to the right.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive
    Reward +1)*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With Gymnasium:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ We create our environment using `gymnasium.make()`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Get an action using our model (in our example we take a random action)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and
    get
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '`observation`: The new state (st+1)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The reward we get after executing the action'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terminated`: Indicates if the episode terminated (agent reach the terminal
    state)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncated`: Introduced with this new version, it indicates a timelimit or
    if an agent go out of bounds of the environment for instance.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: A dictionary that provides additional information (depends on the environment).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more explanations check this üëâ [https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'If the episode is terminated:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Let‚Äôs look at an example!** Make sure to read the code'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create the LunarLander environment üåõ and understand how it works
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The environment üéÆ
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),
    **to land correctly on the moon**. To do that, the agent needs to learn **to adapt
    its speed and position (horizontal, vertical, and angular) to land correctly.**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: üí° A good habit when you start to use an environment is to check its documentation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: üëâ [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs see what the Environment looks like:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We see with `Observation Space Shape (8,)` that the observation is a vector
    of size 8, where each value contains different information about the lander:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pad coordinate (x)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical pad coordinate (y)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal speed (x)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical speed (y)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angular speed
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left leg contact point has touched the land (boolean)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the right leg contact point has touched the land (boolean)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available üéÆ:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Action 0: Do nothing,'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 1: Fire left orientation engine,'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 2: Fire the main engine,'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 3: Fire right orientation engine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function (the function that will give a reward at each timestep) üí∞:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: After every step a reward is granted. The total reward of an episode is the
    **sum of the rewards for all the steps within that episode**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step, the reward:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Is increased/decreased the closer/further the lander is to the landing pad.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased/decreased the slower/faster the lander is moving.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased the more the lander is tilted (angle not horizontal).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased by 10 points for each leg that is in contact with the ground.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.03 points each frame a side engine is firing.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.3 points each frame the main engine is firing.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode receive an **additional reward of -100 or +100 points for crashing
    or landing safely respectively.**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: An episode is **considered a solution if it scores at least 200 points.**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Environment
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We create a vectorized environment (a method for stacking multiple independent
    environments into a single environment) of 16 environments, this way, **we‚Äôll
    have more diverse experiences during the training.**
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create the Model ü§ñ
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have studied our environment and we understood the problem: **being able
    to land the Lunar Lander to the Landing Pad correctly by controlling left, right
    and main orientation engine**. Now let‚Äôs build the algorithm we‚Äôre going to use
    to solve this Problem üöÄ.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, we‚Äôre going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SB3 is a set of **reliable implementations of reinforcement learning algorithms
    in PyTorch**.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'üí° A good habit when using a new library is to dive first on the documentation:
    [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)
    and then try some tutorials.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Stable Baselines3](../Images/12dda719af36ee58d0b11fadfe3279ba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we‚Äôre going to use SB3 **PPO**. [PPO (aka Proximal Policy
    Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning
    algorithms that you‚Äôll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO is a combination of:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Value-based reinforcement learning method*: learning an action-value function
    that will tell us the **most valuable action to take given a state and action**.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy-based reinforcement learning method*: learning a policy that will **give
    us a probability distribution over actions**.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 is easy to set up:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ You **create your environment** (in our case it was done above)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model
    = PPO("MlpPolicy")`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training
    timesteps
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Solution
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Train the PPO agent üèÉ
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs train our agent for 1,000,000 timesteps, don‚Äôt forget to use GPU on Colab.
    It will take approximately ~20min, but you can use fewer timesteps if you just
    want to try it out.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training, take a ‚òï break you deserved it ü§ó
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evaluate the agent üìà
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next step, we‚Äôll see **how to automatically evaluate and share your agent
    to compete in a leaderboard, but for now let‚Äôs do it ourselves**
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí° When you evaluate your agent, you should not use your training environment
    but create an evaluation environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Solution
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million
    steps, which means that our lunar lander agent is ready to land on the moon üåõü•≥.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish our trained model on the Hub üî•
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub ü§ó with one line of code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: üìö The libraries documentation üëâ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face‚Äîx-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example of a Model Card (with Space Invaders):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: By using `package_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'This way:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: You can **showcase our work** üî•
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **visualize your agent playing** üëÄ
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **share with the community an agent that others can use** üíæ
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **access a leaderboard üèÜ to see how well your agent is performing compared
    to your classmates** üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ (If it‚Äôs not already done) create an account on Hugging Face ‚û° [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you don‚Äôt want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ We‚Äôre now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()`
    function
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs fill the `package_to_hub` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`model`: our trained model.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name`: the name of the trained model that we defined in `model_save`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_architecture`: the model architecture we used, in our case PPO'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env_id`: the name of the environment, in our case `LunarLander-v2`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_env`: the evaluation environment defined in eval_env'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí° **A good name is `{username}/{model_architecture}-{env_id}`**
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`commit_message`: message of the commit'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Congrats ü•≥ you‚Äôve just trained and uploaded your first Deep Reinforcement Learning
    agent. The script above should have displayed a link to a model repository such
    as [https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3).
    When you go to this link, you can:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: See a video preview of your agent at the right.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click ‚ÄúFiles and versions‚Äù to see all the files in the repository.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click ‚ÄúUse in stable-baselines3‚Äù to get a code snippet that shows how to load
    the model.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÁÇπÂáª‚ÄúÂú®stable-baselines3‰∏≠‰ΩøÁî®‚Äù‰ª•Ëé∑ÂèñÊòæÁ§∫Â¶Ç‰ΩïÂä†ËΩΩÊ®°ÂûãÁöÑ‰ª£Á†ÅÁâáÊÆµ„ÄÇ
- en: A model card (`README.md` file) which gives a description of the model
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰∏Ä‰∏™Ê®°ÂûãÂç°ÁâáÔºà`README.md`Êñá‰ª∂ÔºâÔºåÂÖ∂‰∏≠ÊèèËø∞‰∫ÜÊ®°Âûã
- en: Under the hood, the Hub uses git-based repositories (don‚Äôt worry if you don‚Äôt
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ÂπïÂêéÔºåHub‰ΩøÁî®Âü∫‰∫égitÁöÑÂ≠òÂÇ®Â∫ìÔºàÂ¶ÇÊûú‰Ω†‰∏çÁü•ÈÅìgitÊòØ‰ªÄ‰πàÔºå‰∏çÁî®ÊãÖÂøÉÔºâÔºåËøôÊÑèÂë≥ÁùÄ‰Ω†ÂèØ‰ª•Âú®ÂÆûÈ™åÂíåÊîπËøõ‰ª£ÁêÜÊó∂Êõ¥Êñ∞Ê®°ÂûãÁöÑÊñ∞ÁâàÊú¨„ÄÇ
- en: Compare the results of your LunarLander-v2 with your classmates using the leaderboard
    üèÜ üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ΩøÁî®ÊéíË°åÊ¶úÊØîËæÉ‰Ω†ÁöÑLunarLander-v2ÁöÑÁªìÊûú‰∏é‰Ω†ÁöÑÂêåÂ≠¶üèÜüëâ[https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: Load a saved LunarLander model from the Hub ü§ó
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ‰ªéHubÂä†ËΩΩ‰øùÂ≠òÁöÑLunarLanderÊ®°Âûãü§ó
- en: Thanks to [ironbar](https://github.com/ironbar) for the contribution.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÑüË∞¢[ironbar](https://github.com/ironbar)ÁöÑË¥°ÁåÆ„ÄÇ
- en: Loading a saved model from the Hub is really easy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ªéHubÂä†ËΩΩ‰øùÂ≠òÁöÑÊ®°ÂûãÈùûÂ∏∏ÂÆπÊòì„ÄÇ
- en: You go to [https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)
    to see the list of all the Stable-baselines3 saved models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ‰Ω†ÂèØ‰ª•Âéª[https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)Êü•ÁúãÊâÄÊúâStable-baselines3‰øùÂ≠òÊ®°ÂûãÁöÑÂàóË°®„ÄÇ
- en: You select one and copy its repo_id
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ÈÄâÊã©‰∏Ä‰∏™Âπ∂Â§çÂà∂ÂÖ∂repo_id
- en: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
- en: 'Then we just need to use load_from_hub with:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ÁÑ∂ÂêéÊàë‰ª¨Âè™ÈúÄË¶Å‰ΩøÁî®load_from_hubÔºö
- en: The repo_id
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo and its extension (*.zip)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êñá‰ª∂ÂêçÔºöÂ≠òÂÇ®Â∫ì‰∏≠‰øùÂ≠òÁöÑÊ®°ÂûãÂèäÂÖ∂Êâ©Â±ïÂêçÔºà*.zipÔºâ
- en: Because the model I download from the Hub was trained with Gym (the former version
    of Gymnasium) we need to install shimmy a API conversion tool that will help us
    to run the environment correctly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Âõ†‰∏∫Êàë‰ªéHub‰∏ãËΩΩÁöÑÊ®°ÂûãÊòØ‰ΩøÁî®GymÔºàGymnasiumÁöÑÂâçË∫´ÔºâËÆ≠ÁªÉÁöÑÔºåÊâÄ‰ª•Êàë‰ª¨ÈúÄË¶ÅÂÆâË£ÖshimmyÔºåËøôÊòØ‰∏Ä‰∏™APIËΩ¨Êç¢Â∑•ÂÖ∑ÔºåÂ∞ÜÂ∏ÆÂä©Êàë‰ª¨Ê≠£Á°ÆËøêË°åÁéØÂ¢É„ÄÇ
- en: 'Shimmy Documentation: [https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ShimmyÊñáÊ°£Ôºö[https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let‚Äôs evaluate this agent:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ËÆ©Êàë‰ª¨ËØÑ‰º∞Ëøô‰∏™‰ª£ÁêÜÔºö
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Some additional challenges üèÜ
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÊåëÊàòüèÜ
- en: The best way to learn **is to try things by your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Â≠¶‰π†ÁöÑÊúÄ‰Ω≥ÊñπÂºè**ÊòØËá™Â∑±Â∞ùËØï**ÔºÅÊ≠£Â¶Ç‰Ω†ÊâÄÁúãÂà∞ÁöÑÔºåÂΩìÂâçÁöÑ‰ª£ÁêÜË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™Âª∫ËÆÆÔºå‰Ω†ÂèØ‰ª•ËÆ≠ÁªÉÊõ¥Â§öÊ≠•È™§„ÄÇÈÄöËøá100‰∏áÊ≠•ÔºåÊàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∫õÂæàÂ•ΩÁöÑÁªìÊûúÔºÅ
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®[ÊéíË°åÊ¶ú](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)‰∏≠Ôºå‰Ω†‰ºöÊâæÂà∞‰Ω†ÁöÑ‰ª£ÁêÜ„ÄÇ‰Ω†ËÉΩËææÂà∞Ê¶úÈ¶ñÂêóÔºü
- en: 'Here are some ideas to achieve so:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂÆûÁé∞ÁöÑÊÉ≥Ê≥ïÔºö
- en: Train more steps
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ËÆ≠ÁªÉÊõ¥Â§öÊ≠•È™§
- en: Try different hyperparameters for `PPO`. You can see them at [https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â∞ùËØï‰∏çÂêåÁöÑ`PPO`Ë∂ÖÂèÇÊï∞„ÄÇ‰Ω†ÂèØ‰ª•Âú®[https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)ÁúãÂà∞ÂÆÉ‰ª¨„ÄÇ
- en: Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    and try another model such as DQN.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êü•Áúã[Stable-Baselines3ÊñáÊ°£](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)ÔºåÂ∞ùËØïÂè¶‰∏Ä‰∏™Ê®°ÂûãÔºåÂ¶ÇDQN„ÄÇ
- en: '**Push your new trained model** on the Hub üî•'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Â∞Ü‰Ω†ÁöÑÊñ∞ËÆ≠ÁªÉÊ®°ÂûãÊé®ÈÄÅÂà∞Hub** üî•'
- en: '**Compare the results of your LunarLander-v2 with your classmates** using the
    [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    üèÜ'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**‰ΩøÁî®[ÊéíË°åÊ¶ú](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)‰∏é‰Ω†ÁöÑÂêåÂ≠¶ÊØîËæÉ‰Ω†ÁöÑLunarLander-v2ÁöÑÁªìÊûú**
    üèÜ'
- en: Is moon landing too boring for you? Try to **change the environment**, why not
    use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the
    gym documentation](https://www.gymlibrary.dev/) and have fun üéâ.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ÂØπ‰Ω†Êù•ËØ¥ÔºåÁôªÊúàÂ§™Êó†ËÅä‰∫ÜÂêóÔºüÂ∞ùËØï**ÊîπÂèòÁéØÂ¢É**Ôºå‰∏∫‰ªÄ‰πà‰∏ç‰ΩøÁî®MountainCar-v0ÔºåCartPole-v1ÊàñCarRacing-v0ÔºüÊü•ÁúãÂÆÉ‰ª¨ÁöÑÂ∑•‰ΩúÊñπÂºè[‰ΩøÁî®gymÊñáÊ°£](https://www.gymlibrary.dev/)Âπ∂‰∫´Âèó‰πêË∂£üéâ„ÄÇ
- en: '* * *'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats on finishing this chapter! That was the biggest one, **and there was
    a lot of information.**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÅ≠Âñú‰Ω†ÂÆåÊàê‰∫ÜËøô‰∏ÄÁ´†ÔºÅËøôÊòØÊúÄÂ§ßÁöÑ‰∏ÄÁ´†Ôºå**ÂåÖÂê´‰∫ÜÂæàÂ§ö‰ø°ÊÅØ„ÄÇ**
- en: If you‚Äôre still feel confused with all these elements‚Ä¶it‚Äôs totally normal! **This
    was the same for me and for all people who studied RL.**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Â¶ÇÊûú‰Ω†‰ªçÁÑ∂ÊÑüÂà∞Âõ∞ÊÉëÔºåËøô‰∫õÂÖÉÁ¥†ÂØπÊàëÂíåÊâÄÊúâÂ≠¶‰π†RLÁöÑ‰∫∫Êù•ËØ¥ÈÉΩÊòØÊ≠£Â∏∏ÁöÑÔºÅ
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. It‚Äôs important to master these elements and have a solid foundations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ÁªßÁª≠‰πãÂâçËä±Êó∂Èó¥ÁúüÊ≠£**ÊéåÊè°ÊùêÊñôÂπ∂Â∞ùËØïÈ¢ùÂ§ñÁöÑÊåëÊàò**„ÄÇÊéåÊè°Ëøô‰∫õÂÖÉÁ¥†Âπ∂Âª∫Á´ãÂùöÂÆûÁöÑÂü∫Á°ÄÊòØÂæàÈáçË¶ÅÁöÑ„ÄÇ
- en: Naturally, during the course, we‚Äôre going to dive deeper into these concepts
    but **it‚Äôs better to have a good understanding of them now before diving into
    the next chapters.**
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ËØæÁ®ã‰∏≠ÔºåÊàë‰ª¨Â∞ÜÊõ¥Ê∑±ÂÖ•Âú∞Êé¢ËÆ®Ëøô‰∫õÊ¶ÇÂøµÔºå‰ΩÜ**ÊúÄÂ•ΩÁé∞Âú®Â∞±ÂØπÂÆÉ‰ª¨Êúâ‰∏Ä‰∏™ËâØÂ•ΩÁöÑÁêÜËß£ÔºåÁÑ∂ÂêéÂÜçÊ∑±ÂÖ•‰∏ã‰∏ÄÁ´†ËäÇ„ÄÇ**
- en: Next time, in the bonus unit 1, you‚Äôll train Huggy the Dog to fetch the stick.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ‰∏ãÊ¨°ÔºåÂú®Â•ñÂä±ÂçïÂÖÉ1‰∏≠Ôºå‰Ω†Â∞ÜËÆ≠ÁªÉHuggy the DogÂéªÊé•Ê£çÂ≠ê„ÄÇ
- en: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
- en: Keep learning, stay awesome ü§ó
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÁªßÁª≠Â≠¶‰π†Ôºå‰øùÊåÅÊ£íÊ£íÁöÑü§ó
