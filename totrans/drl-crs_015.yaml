- en: Train your first Deep Reinforcement Learning Agent ğŸ¤–
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ğŸ¤–
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![æé—®](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
- en: 'Now that youâ€™ve studied the bases of Reinforcement Learning, youâ€™re ready to
    train your first agent and share it with the community through the Hub ğŸ”¥: A Lunar
    Lander agent that will learn to land correctly on the Moon ğŸŒ•'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»å­¦ä¹ äº†å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†ï¼Œä½ å¯ä»¥å¼€å§‹è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªä»£ç†ï¼Œå¹¶é€šè¿‡Hubä¸ç¤¾åŒºåˆ†äº«ğŸ”¥ï¼šä¸€ä¸ªæœˆçƒç€é™†å™¨ä»£ç†ï¼Œå°†å­¦ä¼šæ­£ç¡®ç€é™†åœ¨æœˆçƒä¸ŠğŸŒ•
- en: '![LunarLander](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![æœˆçƒç€é™†å™¨](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
- en: And finally, youâ€™ll **upload this trained agent to the Hugging Face Hub ğŸ¤—, a
    free, open platform where people can share ML models, datasets, and demos.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½ å°†**ä¸Šä¼ è¿™ä¸ªè®­ç»ƒå¥½çš„ä»£ç†åˆ°Hugging Face Hub ğŸ¤—ï¼Œè¿™æ˜¯ä¸€ä¸ªå…è´¹ã€å¼€æ”¾çš„å¹³å°ï¼Œäººä»¬å¯ä»¥åœ¨è¿™é‡Œåˆ†äº«æœºå™¨å­¦ä¹ æ¨¡å‹ã€æ•°æ®é›†å’Œæ¼”ç¤ºã€‚**
- en: Thanks to our [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    youâ€™ll be able to compare your results with other classmates and exchange the
    best practices to improve your agentâ€™s scores. Who will win the challenge for
    Unit 1 ğŸ†?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æˆ‘ä»¬çš„[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ï¼Œä½ å°†èƒ½å¤Ÿä¸å…¶ä»–åŒå­¦æ¯”è¾ƒä½ çš„ç»“æœï¼Œå¹¶äº¤æµæœ€ä½³å®è·µï¼Œä»¥æé«˜ä½ çš„ä»£ç†åˆ†æ•°ã€‚è°å°†èµ¢å¾—Unit
    1çš„æŒ‘æˆ˜ğŸ†ï¼Ÿ
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯è¿™ä¸ª[è®¤è¯æµç¨‹](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)çš„å®è·µæ€§ï¼Œä½ éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubï¼Œå¹¶**è·å¾—>=200çš„ç»“æœ**ã€‚
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰¾åˆ°ä½ çš„ç»“æœï¼Œå»[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)æ‰¾åˆ°ä½ çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**
- en: '**If you donâ€™t find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœæ‰¾ä¸åˆ°ä½ çš„æ¨¡å‹ï¼Œè¯·è½¬åˆ°é¡µé¢åº•éƒ¨å¹¶ç‚¹å‡»åˆ·æ–°æŒ‰é’®ã€‚**'
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®¤è¯æµç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: And you can check your progress here ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥ä½ çš„è¿›åº¦ğŸ‘‰[https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
- en: So letâ€™s get started! ğŸš€
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ï¼ğŸš€
- en: '**To start the hands-on click on Open In Colab button** ğŸ‘‡ :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¦å¼€å§‹å®è·µï¼Œè¯·ç‚¹å‡»â€œåœ¨Colabä¸­æ‰“å¼€â€æŒ‰é’®**ğŸ‘‡ï¼š'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼ºçƒˆå»ºè®®å­¦ç”Ÿä½¿ç”¨Google Colabè¿›è¡Œå®è·µç»ƒä¹ ï¼Œè€Œä¸æ˜¯åœ¨ä¸ªäººç”µè„‘ä¸Šè¿è¡Œã€‚
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨Google Colabï¼Œ**ä½ å¯ä»¥ä¸“æ³¨äºå­¦ä¹ å’Œå®éªŒï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¾ç½®ç¯å¢ƒçš„æŠ€æœ¯ç»†èŠ‚**ã€‚
- en: 'Unit 1: Train your first Deep Reinforcement Learning Agent ğŸ¤–'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unit 1ï¼šè®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ğŸ¤–
- en: '![Unit 1 thumbnail](../Images/269d50061313727de39330c553eb4733.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Unit 1ç¼©ç•¥å›¾](../Images/269d50061313727de39330c553eb4733.png)'
- en: In this notebook, youâ€™ll train your **first Deep Reinforcement Learning agent**
    a Lunar Lander agent that will learn to **land correctly on the Moon ğŸŒ•**. Using
    [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep
    Reinforcement Learning library, share them with the community, and experiment
    with different configurations
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œä½ å°†è®­ç»ƒä½ çš„**ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†**ï¼Œä¸€ä¸ªæœˆçƒç€é™†å™¨ä»£ç†ï¼Œå°†å­¦ä¼š**åœ¨æœˆçƒä¸Šæ­£ç¡®ç€é™†ğŸŒ•**ã€‚ä½¿ç”¨[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)è¿™ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ï¼Œä¸ç¤¾åŒºåˆ†äº«ï¼Œå¹¶å°è¯•ä¸åŒçš„é…ç½®
- en: The environment ğŸ®
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¯å¢ƒğŸ®
- en: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
- en: The library used ğŸ“š
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„åº“ğŸ“š
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
- en: Weâ€™re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœä½ åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­å‘ç°äº†ä¸€äº›é—®é¢˜**ï¼Œè¯·åœ¨Github Repoä¸Š[æå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚
- en: Objectives of this notebook ğŸ†
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†
- en: 'At the end of the notebook, you will:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬çš„ç»“å°¾ï¼Œä½ å°†ï¼š
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿä½¿ç”¨**Gymnasium**ï¼Œè¿™æ˜¯ä¸€ä¸ªç¯å¢ƒåº“ã€‚
- en: Be able to use **Stable-Baselines3**, the deep reinforcement learning library.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿä½¿ç”¨**Stable-Baselines3**ï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score ğŸ”¥.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿ**å°†è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°Hub**ï¼Œå¹¶é™„ä¸Šä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œä¸€ä¸ªè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚
- en: This notebook is from Deep Reinforcement Learning Course
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¬”è®°æœ¬æ¥è‡ªæ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹æ’å›¾](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
- en: 'In this free course, you will:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é—¨å…è´¹è¯¾ç¨‹ä¸­ï¼Œæ‚¨å°†ï¼š
- en: ğŸ“– Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“– ç†è®ºå’Œå®è·µä¸Šå­¦ä¹ æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚
- en: ğŸ§‘â€ğŸ’» Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ§‘â€ğŸ’» å­¦ä¹ ä½¿ç”¨è‘—åçš„æ·±åº¦RLåº“ï¼Œå¦‚Stable Baselines3ã€RL Baselines3 Zooã€CleanRLå’ŒSample Factory
    2.0ã€‚
- en: ğŸ¤– Train **agents in unique environments**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ¤– åœ¨ç‹¬ç‰¹ç¯å¢ƒä¸­**è®­ç»ƒä»£ç†**
- en: ğŸ“ **Earn a certificate of completion** by completing 80% of the assignments.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“ é€šè¿‡å®Œæˆ80%çš„ä½œä¸š**è·å¾—å®Œæˆè¯ä¹¦**ã€‚
- en: And more!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥åŠæ›´å¤šï¼
- en: Check ğŸ“š the syllabus ğŸ‘‰ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ğŸ“šè¯¾ç¨‹å¤§çº²ğŸ‘‰[https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
- en: Donâ€™t forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able toÂ **send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦å¿˜è®°**[æ³¨å†Œè¯¾ç¨‹](http://eepurl.com/ic5ZUD)**ï¼ˆæˆ‘ä»¬æ­£åœ¨æ”¶é›†æ‚¨çš„ç”µå­é‚®ä»¶ä»¥ä¾¿**åœ¨æ¯ä¸ªå•å…ƒå‘å¸ƒæ—¶å‘æ‚¨å‘é€é“¾æ¥å¹¶æä¾›æœ‰å…³æŒ‘æˆ˜å’Œæ›´æ–°çš„ä¿¡æ¯ï¼‰**ã€‚
- en: The best way to keep in touch and ask questions is **to join our discord server**
    to exchange with the community and with us ğŸ‘‰ğŸ» [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿æŒè”ç³»å¹¶æé—®çš„æœ€ä½³æ–¹å¼æ˜¯**åŠ å…¥æˆ‘ä»¬çš„discordæœåŠ¡å™¨**ä¸ç¤¾åŒºå’Œæˆ‘ä»¬äº¤æµğŸ‘‰ğŸ»[https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
- en: Prerequisites ğŸ—ï¸
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆå†³æ¡ä»¶ğŸ—ï¸
- en: 'Before diving into the notebook, you need to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š
- en: ğŸ”² ğŸ“ **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)**
    that gives you all the **information about the course and helps you to onboard**
    ğŸ¤—
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”²ğŸ“ é€šè¿‡[é˜…è¯»ç¬¬0å•å…ƒ](https://huggingface.co/deep-rl-course/unit0/introduction) **è·å–æœ‰å…³è¯¾ç¨‹çš„æ‰€æœ‰ä¿¡æ¯å¹¶å¸®åŠ©æ‚¨å…¥é—¨**ğŸ¤—
- en: ğŸ”² ğŸ“š **Develop an understanding of the foundations of Reinforcement learning**
    (MC, TD, Rewards hypothesisâ€¦) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”²ğŸ“š é€šè¿‡[é˜…è¯»ç¬¬1å•å…ƒ](https://huggingface.co/deep-rl-course/unit1/introduction) **äº†è§£å¼ºåŒ–å­¦ä¹ åŸºç¡€**ï¼ˆMCã€TDã€å¥–åŠ±å‡è®¾...ï¼‰ã€‚
- en: A small recap of Deep Reinforcement Learning ğŸ“š
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å°ç»“ğŸ“š
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![RLè¿‡ç¨‹](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: 'Letâ€™s do a small recap on what we learned in the first Unit:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç®€è¦å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åœ¨ç¬¬ä¸€ä¸ªå•å…ƒå­¦åˆ°çš„å†…å®¹ï¼š
- en: Reinforcement Learning is a **computational approach to learning from actions**.
    We build an agent that learns from the environment by **interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§**ä»è¡ŒåŠ¨ä¸­å­¦ä¹ çš„è®¡ç®—æ–¹æ³•**ã€‚æˆ‘ä»¬æ„å»ºä¸€ä¸ªä»£ç†ï¼Œé€šè¿‡**é€šè¿‡è¯•é”™ä¸ç¯å¢ƒäº’åŠ¨**å¹¶æ¥æ”¶å¥–åŠ±ï¼ˆè´Ÿé¢æˆ–æ­£é¢ï¼‰ä½œä¸ºåé¦ˆæ¥å­¦ä¹ ã€‚
- en: The goal of any RL agent is to **maximize its expected cumulative reward** (also
    called expected return) because RL is based on the *reward hypothesis*, which
    is that all goals can be described as the maximization of an expected cumulative
    reward.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»»ä½•RLä»£ç†çš„ç›®æ ‡æ˜¯**æœ€å¤§åŒ–å…¶é¢„æœŸç´¯ç§¯å¥–åŠ±**ï¼ˆä¹Ÿç§°ä¸ºé¢„æœŸå›æŠ¥ï¼‰ï¼Œå› ä¸ºRLåŸºäº*å¥–åŠ±å‡è®¾*ï¼Œå³æ‰€æœ‰ç›®æ ‡éƒ½å¯ä»¥æè¿°ä¸ºæœ€å¤§åŒ–é¢„æœŸç´¯ç§¯å¥–åŠ±ã€‚
- en: The RL process is a **loop that outputs a sequence of state, action, reward,
    and next state**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLè¿‡ç¨‹æ˜¯ä¸€ä¸ª**è¾“å‡ºçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€åºåˆ—çš„å¾ªç¯**ã€‚
- en: 'To calculate the expected cumulative reward (expected return), **we discount
    the rewards**: the rewards that come sooner (at the beginning of the game) are
    more probable to happen since they are more predictable than the long-term future
    reward.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—é¢„æœŸç´¯ç§¯å¥–åŠ±ï¼ˆé¢„æœŸå›æŠ¥ï¼‰ï¼Œ**æˆ‘ä»¬æŠ˜æ‰£å¥–åŠ±**ï¼šè¶Šæ—©è·å¾—çš„å¥–åŠ±ï¼ˆåœ¨æ¸¸æˆå¼€å§‹æ—¶ï¼‰æ›´æœ‰å¯èƒ½å‘ç”Ÿï¼Œå› ä¸ºå®ƒä»¬æ¯”é•¿æœŸæœªæ¥å¥–åŠ±æ›´å¯é¢„æµ‹ã€‚
- en: To solve an RL problem, you want to **find an optimal policy**; the policy is
    the â€œbrainâ€ of your AI that will tell us what action to take given a state. The
    optimal one is the one that gives you the actions that max the expected return.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è§£å†³RLé—®é¢˜ï¼Œæ‚¨å¸Œæœ›**æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥**ï¼›ç­–ç•¥æ˜¯æ‚¨çš„AIçš„â€œå¤§è„‘â€ï¼Œå°†å‘Šè¯‰æˆ‘ä»¬åœ¨ç»™å®šçŠ¶æ€ä¸‹åº”è¯¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ã€‚æœ€ä½³ç­–ç•¥æ˜¯ä½¿æ‚¨çš„è¡ŒåŠ¨æœ€å¤§åŒ–é¢„æœŸå›æŠ¥çš„ç­–ç•¥ã€‚
- en: 'There are **two** ways to find your optimal policy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰**ä¸¤ç§**æ–¹æ³•å¯ä»¥æ‰¾åˆ°æ‚¨çš„æœ€ä¼˜ç­–ç•¥ï¼š
- en: 'By **training your policy directly**: policy-based methods.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡**ç›´æ¥è®­ç»ƒæ‚¨çš„ç­–ç•¥**ï¼šåŸºäºç­–ç•¥çš„æ–¹æ³•ã€‚
- en: 'By **training a value function** that tells us the expected return the agent
    will get at each state and use this function to define our policy: value-based
    methods.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡**è®­ç»ƒä¸€ä¸ªå€¼å‡½æ•°**å‘Šè¯‰æˆ‘ä»¬ä»£ç†åœ¨æ¯ä¸ªçŠ¶æ€å°†è·å¾—çš„é¢„æœŸå›æŠ¥ï¼Œå¹¶ä½¿ç”¨æ­¤å‡½æ•°å®šä¹‰æˆ‘ä»¬çš„ç­–ç•¥ï¼šåŸºäºä»·å€¼çš„æ–¹æ³•ã€‚
- en: Finally, we spoke about Deep RL because **we introduce deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based) hence the name â€œdeep.â€**
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è°ˆåˆ°äº†æ·±åº¦RLï¼Œå› ä¸º**æˆ‘ä»¬å¼•å…¥äº†æ·±åº¦ç¥ç»ç½‘ç»œæ¥ä¼°è®¡è¦é‡‡å–çš„è¡ŒåŠ¨ï¼ˆåŸºäºç­–ç•¥ï¼‰æˆ–ä¼°è®¡çŠ¶æ€çš„ä»·å€¼ï¼ˆåŸºäºä»·å€¼ï¼‰ï¼Œå› æ­¤å¾—åâ€œæ·±åº¦â€ã€‚**
- en: Letâ€™s train our first Deep Reinforcement Learning agent and upload it to the
    Hub ğŸš€
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†å¹¶å°†å…¶ä¸Šä¼ åˆ°HubğŸš€
- en: Get a certificate ğŸ“
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è·å¾—è¯ä¹¦ğŸ“
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè¿›è¡Œ[è®¤è¯æµç¨‹](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ°Hubå¹¶**è·å¾—>=200çš„ç»“æœ**ã€‚
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·è½¬åˆ°[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)å¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³è®¤è¯æµç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: Set the GPU ğŸ’ª
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®GPUğŸ’ª
- en: To **accelerate the agentâ€™s training, weâ€™ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†**åŠ é€Ÿä»£ç†çš„è®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨GPU**ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œè½¬åˆ°`è¿è¡Œæ—¶ > æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![GPUæ­¥éª¤1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![GPU æ­¥éª¤ 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Install dependencies and create a virtual screen ğŸ”½
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®‰è£…ä¾èµ–å¹¶åˆ›å»ºè™šæ‹Ÿå±å¹•ğŸ”½
- en: The first step is to install the dependencies, weâ€™ll install multiple ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯å®‰è£…ä¾èµ–é¡¹ï¼Œæˆ‘ä»¬å°†å®‰è£…å¤šä¸ªã€‚
- en: '`gymnasium[box2d]`: Contains the LunarLander-v2 environment ğŸŒ›'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gymnasium[box2d]`ï¼šåŒ…å« LunarLander-v2 ç¯å¢ƒğŸŒ›'
- en: '`stable-baselines3[extra]`: The deep reinforcement learning library.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stable-baselines3[extra]`ï¼šæ·±åº¦å¼ºåŒ–å­¦ä¹ åº“ã€‚'
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face ğŸ¤— Hub.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_sb3`ï¼šç”¨äº Stable-baselines3 åŠ è½½å’Œä¸Šä¼ æ¨¡å‹çš„é¢å¤–ä»£ç æ¥è‡ª Hugging Face ğŸ¤— Hubã€‚'
- en: To make things easier, we created a script to install all these dependencies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–äº‹æƒ…ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè„šæœ¬æ¥å®‰è£…æ‰€æœ‰è¿™äº›ä¾èµ–é¡¹ã€‚
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During the notebook, weâ€™ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªé‡æ’­è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨ colab ä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚
- en: Hence the following cell will install virtual screen libraries and create and
    run a virtual screen ğŸ–¥
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ¥ä¸‹æ¥çš„å•å…ƒæ ¼å°†å®‰è£…è™šæ‹Ÿå±å¹•åº“å¹¶åˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿå±å¹•ğŸ–¥
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To make sure the new installed libraries are used, **sometimes itâ€™s required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so youâ€™ll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿æ–°å®‰è£…çš„åº“è¢«ä½¿ç”¨ï¼Œ**æœ‰æ—¶éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬è¿è¡Œæ—¶**ã€‚ä¸‹ä¸€ä¸ªå•å…ƒæ ¼å°†å¼ºåˆ¶**è¿è¡Œæ—¶å´©æºƒï¼Œå› æ­¤æ‚¨éœ€è¦é‡æ–°è¿æ¥å¹¶ä»è¿™é‡Œå¼€å§‹è¿è¡Œä»£ç **ã€‚é€šè¿‡è¿™ä¸ªæŠ€å·§ï¼Œ**æˆ‘ä»¬å°†èƒ½å¤Ÿè¿è¡Œæˆ‘ä»¬çš„è™šæ‹Ÿå±å¹•**ã€‚
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Import the packages ğŸ“¦
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥åŒ…ğŸ“¦
- en: One additional library we import is huggingface_hub **to be able to upload and
    download trained models from the hub**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¼å…¥çš„å¦ä¸€ä¸ªåº“æ˜¯ huggingface_hub **ä»¥ä¾¿èƒ½å¤Ÿä» hub ä¸Šä¼ å’Œä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹**ã€‚
- en: The Hugging Face Hub ğŸ¤— works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub ğŸ¤— ä½œä¸ºä¸€ä¸ªä¸­å¿ƒåœ°æ–¹ï¼Œä»»ä½•äººéƒ½å¯ä»¥åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€æŒ‡æ ‡ã€å¯è§†åŒ–å’Œå…¶ä»–åŠŸèƒ½ï¼Œè®©æ‚¨å¯ä»¥è½»æ¾ä¸ä»–äººåˆä½œã€‚
- en: You can see here all the Deep reinforcement Learning models available hereğŸ‘‰
    [https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ‰€æœ‰å¯ç”¨çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ğŸ‘‰[https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Understand Gymnasium and how it works ğŸ¤–
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº†è§£ Gymnasium åŠå…¶å·¥ä½œåŸç†ğŸ¤–
- en: ğŸ‹ The library containing our environment is called Gymnasium. **Youâ€™ll use Gymnasium
    a lot in Deep Reinforcement Learning.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ åŒ…å«æˆ‘ä»¬ç¯å¢ƒçš„åº“ç§°ä¸º Gymnasiumã€‚**åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ‚¨å°†ç»å¸¸ä½¿ç”¨ Gymnasiumã€‚**
- en: Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium æ˜¯ç”± Farama Foundation ç»´æŠ¤çš„ Gym åº“çš„**æ–°ç‰ˆæœ¬**[ï¼ˆhttps://farama.org/ï¼‰ã€‚
- en: 'The Gymnasium library provides two things:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium åº“æä¾›ä¸¤ä¸ªä¸œè¥¿ï¼š
- en: An interface that allows you to **create RL environments**.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…è®¸æ‚¨**åˆ›å»º RL ç¯å¢ƒ**çš„æ¥å£ã€‚
- en: A **collection of environments** (gym-control, atari, box2Dâ€¦).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ç»„ç¯å¢ƒï¼ˆgym-controlã€atariã€box2D...ï¼‰ã€‚
- en: Letâ€™s look at an example, but first letâ€™s recall the RL loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ RL å¾ªç¯ã€‚
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![RL è¿‡ç¨‹](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: 'At each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ï¼š
- en: Our Agent receivesÂ a **state (S0)**Â from theÂ **Environment**Â â€” we receive the
    first frame of our game (Environment).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ä»£ç†ä»**ç¯å¢ƒ**æ¥æ”¶ä¸€ä¸ª**çŠ¶æ€ï¼ˆS0ï¼‰**â€”â€”æˆ‘ä»¬æ¥æ”¶åˆ°æ¸¸æˆçš„ç¬¬ä¸€å¸§ï¼ˆç¯å¢ƒï¼‰ã€‚
- en: Based on thatÂ **state (S0),**Â the Agent takes anÂ **action (A0)**Â â€” our Agent
    will move to the right.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºé‚£ä¸ª**çŠ¶æ€ï¼ˆS0ï¼‰**ï¼Œä»£ç†é‡‡å–ä¸€ä¸ª**åŠ¨ä½œï¼ˆA0ï¼‰**â€”â€”æˆ‘ä»¬çš„ä»£ç†å°†å‘å³ç§»åŠ¨ã€‚
- en: The environment transitions to aÂ **new**Â **state (S1)**Â â€” new frame.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒè½¬æ¢åˆ°ä¸€ä¸ª**æ–°çš„çŠ¶æ€ï¼ˆS1ï¼‰**â€”â€”æ–°çš„å¸§ã€‚
- en: The environment gives someÂ **reward (R1)**Â to the Agent â€” weâ€™re not deadÂ *(Positive
    Reward +1)*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¯å¢ƒç»™ä»£ç†ä¸€äº›**å¥–åŠ±ï¼ˆR1ï¼‰**â€”â€”æˆ‘ä»¬è¿˜æ²¡æœ‰æ­»äº¡*(æ­£å¥–åŠ± +1)*ã€‚
- en: 'With Gymnasium:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Gymnasiumï¼š
- en: 1ï¸âƒ£ We create our environment using `gymnasium.make()`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æˆ‘ä»¬ä½¿ç”¨ `gymnasium.make()` åˆ›å»ºæˆ‘ä»¬çš„ç¯å¢ƒ
- en: 2ï¸âƒ£ We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ æˆ‘ä»¬ä½¿ç”¨ `observation = env.reset()` å°†ç¯å¢ƒé‡ç½®ä¸ºåˆå§‹çŠ¶æ€
- en: 'At each step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€æ­¥ï¼š
- en: 3ï¸âƒ£ Get an action using our model (in our example we take a random action)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è·å–ä¸€ä¸ªåŠ¨ä½œï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­æˆ‘ä»¬æ‰§è¡Œä¸€ä¸ªéšæœºåŠ¨ä½œï¼‰
- en: 4ï¸âƒ£ Using `env.step(action)`, we perform this action in the environment and
    get
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 4ï¸âƒ£ ä½¿ç”¨ `env.step(action)`ï¼Œæˆ‘ä»¬åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæ­¤åŠ¨ä½œå¹¶è·å–
- en: '`observation`: The new state (st+1)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`ï¼šæ–°çŠ¶æ€ï¼ˆst+1ï¼‰'
- en: '`reward`: The reward we get after executing the action'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`ï¼šæ‰§è¡ŒåŠ¨ä½œåæˆ‘ä»¬è·å¾—çš„å¥–åŠ±'
- en: '`terminated`: Indicates if the episode terminated (agent reach the terminal
    state)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terminated`ï¼šæŒ‡ç¤ºå‰§é›†æ˜¯å¦ç»ˆæ­¢ï¼ˆä»£ç†åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€ï¼‰'
- en: '`truncated`: Introduced with this new version, it indicates a timelimit or
    if an agent go out of bounds of the environment for instance.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncated`ï¼šåœ¨è¿™ä¸ªæ–°ç‰ˆæœ¬ä¸­å¼•å…¥ï¼Œå®ƒæŒ‡ç¤ºä¸€ä¸ªæ—¶é—´é™åˆ¶æˆ–è€…å¦‚æœä»£ç†è¶…å‡ºç¯å¢ƒçš„è¾¹ç•Œã€‚'
- en: '`info`: A dictionary that provides additional information (depends on the environment).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`ï¼šæä¾›é¢å¤–ä¿¡æ¯çš„å­—å…¸ï¼ˆå–å†³äºç¯å¢ƒï¼‰ã€‚'
- en: For more explanations check this ğŸ‘‰ [https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šè§£é‡Šï¼Œè¯·æŸ¥çœ‹æ­¤é“¾æ¥ğŸ‘‰[https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
- en: 'If the episode is terminated:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå‰§é›†ç»ˆæ­¢ï¼š
- en: We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ `observation = env.reset()` å°†ç¯å¢ƒé‡ç½®ä¸ºåˆå§‹çŠ¶æ€
- en: '**Letâ€™s look at an example!** Make sure to read the code'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼** ç¡®ä¿é˜…è¯»ä»£ç '
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create the LunarLander environment ğŸŒ› and understand how it works
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»º LunarLander ç¯å¢ƒğŸŒ›å¹¶äº†è§£å…¶å·¥ä½œåŸç†
- en: The environment ğŸ®
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç¯å¢ƒğŸ®
- en: In this first tutorial, weâ€™re going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),
    **to land correctly on the moon**. To do that, the agent needs to learn **to adapt
    its speed and position (horizontal, vertical, and angular) to land correctly.**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¡ A good habit when you start to use an environment is to check its documentation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ‘‰ [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s see what the Environment looks like:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We see with `Observation Space Shape (8,)` that the observation is a vector
    of size 8, where each value contains different information about the lander:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pad coordinate (x)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical pad coordinate (y)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal speed (x)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical speed (y)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angular speed
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left leg contact point has touched the land (boolean)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the right leg contact point has touched the land (boolean)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available ğŸ®:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Action 0: Do nothing,'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 1: Fire left orientation engine,'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 2: Fire the main engine,'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 3: Fire right orientation engine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function (the function that will give a reward at each timestep) ğŸ’°:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: After every step a reward is granted. The total reward of an episode is the
    **sum of the rewards for all the steps within that episode**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step, the reward:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Is increased/decreased the closer/further the lander is to the landing pad.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased/decreased the slower/faster the lander is moving.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased the more the lander is tilted (angle not horizontal).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased by 10 points for each leg that is in contact with the ground.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.03 points each frame a side engine is firing.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.3 points each frame the main engine is firing.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode receive an **additional reward of -100 or +100 points for crashing
    or landing safely respectively.**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: An episode is **considered a solution if it scores at least 200 points.**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Environment
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We create a vectorized environment (a method for stacking multiple independent
    environments into a single environment) of 16 environments, this way, **weâ€™ll
    have more diverse experiences during the training.**
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create the Model ğŸ¤–
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have studied our environment and we understood the problem: **being able
    to land the Lunar Lander to the Landing Pad correctly by controlling left, right
    and main orientation engine**. Now letâ€™s build the algorithm weâ€™re going to use
    to solve this Problem ğŸš€.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, weâ€™re going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SB3 is a set of **reliable implementations of reinforcement learning algorithms
    in PyTorch**.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ’¡ A good habit when using a new library is to dive first on the documentation:
    [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)
    and then try some tutorials.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Stable Baselines3](../Images/12dda719af36ee58d0b11fadfe3279ba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, weâ€™re going to use SB3 **PPO**. [PPO (aka Proximal Policy
    Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning
    algorithms that youâ€™ll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO is a combination of:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Value-based reinforcement learning method*: learning an action-value function
    that will tell us the **most valuable action to take given a state and action**.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy-based reinforcement learning method*: learning a policy that will **give
    us a probability distribution over actions**.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 is easy to set up:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ You **create your environment** (in our case it was done above)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ You define the **model you want to use and instantiate this model** `model
    = PPO("MlpPolicy")`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ You **train the agent** with `model.learn` and define the number of training
    timesteps
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Solution
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Train the PPO agent ğŸƒ
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒPPOä»£ç† ğŸƒ
- en: Letâ€™s train our agent for 1,000,000 timesteps, donâ€™t forget to use GPU on Colab.
    It will take approximately ~20min, but you can use fewer timesteps if you just
    want to try it out.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„ä»£ç†è®­ç»ƒ1,000,000ä¸ªæ—¶é—´æ­¥ï¼Œä¸è¦å¿˜è®°åœ¨Colabä¸Šä½¿ç”¨GPUã€‚è¿™å°†å¤§çº¦éœ€è¦~20åˆ†é’Ÿï¼Œä½†å¦‚æœæ‚¨åªæƒ³å°è¯•ä¸€ä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›´å°‘çš„æ—¶é—´æ­¥æ•°ã€‚
- en: During the training, take a â˜• break you deserved it ğŸ¤—
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœŸé—´ï¼Œä¼‘æ¯ä¸€ä¸‹ï¼Œæ‚¨å€¼å¾—æ‹¥æœ‰ ğŸ¤—
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evaluate the agent ğŸ“ˆ
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°ä»£ç† ğŸ“ˆ
- en: Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°å¾—å°†ç¯å¢ƒåŒ…è£…åœ¨[ç›‘è§†å™¨](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html)ä¸­ã€‚
- en: Now that our Lunar Lander agent is trained ğŸš€, we need to **check its performance**.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æœˆçƒç€é™†å™¨ä»£ç†å·²ç»è®­ç»ƒå¥½äº† ğŸš€ï¼Œæˆ‘ä»¬éœ€è¦**æ£€æŸ¥å…¶æ€§èƒ½**ã€‚
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable-Baselines3æä¾›äº†ä¸€ä¸ªæ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼š`evaluate_policy`ã€‚
- en: To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦å¡«å†™è¯¥éƒ¨åˆ†ï¼Œæ‚¨éœ€è¦[æŸ¥çœ‹æ–‡æ¡£](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
- en: In the next step, weâ€™ll see **how to automatically evaluate and share your agent
    to compete in a leaderboard, but for now letâ€™s do it ourselves**
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°**å¦‚ä½•è‡ªåŠ¨è¯„ä¼°å’Œåˆ†äº«æ‚¨çš„ä»£ç†ä»¥å‚åŠ æ’è¡Œæ¦œæ¯”èµ›ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬è‡ªå·±æ¥åš**
- en: ğŸ’¡ When you evaluate your agent, you should not use your training environment
    but create an evaluation environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å½“æ‚¨è¯„ä¼°æ‚¨çš„ä»£ç†æ—¶ï¼Œæ‚¨ä¸åº”è¯¥ä½¿ç”¨è®­ç»ƒç¯å¢ƒï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªè¯„ä¼°ç¯å¢ƒã€‚
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Solution
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million
    steps, which means that our lunar lander agent is ready to land on the moon ğŸŒ›ğŸ¥³.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒ1ç™¾ä¸‡æ­¥åï¼Œæˆ‘å¾—åˆ°çš„å¹³å‡å¥–åŠ±æ˜¯`200.20 +/- 20.80`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æœˆçƒç€é™†å™¨ä»£ç†å·²ç»å‡†å¤‡å¥½åœ¨æœˆçƒä¸Šç€é™† ğŸŒ›ğŸ¥³ã€‚
- en: Publish our trained model on the Hub ğŸ”¥
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Hubä¸Šå‘å¸ƒæˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ ğŸ”¥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub ğŸ¤— with one line of code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°åœ¨è®­ç»ƒåå–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°hub ğŸ¤—ã€‚
- en: ğŸ“š The libraries documentation ğŸ‘‰ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“š åº“çš„æ–‡æ¡£ ğŸ‘‰ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
- en: 'Hereâ€™s an example of a Model Card (with Space Invaders):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ¨¡å‹å¡çš„ç¤ºä¾‹ï¼ˆå¸¦æœ‰å¤ªç©ºä¾µç•¥è€…ï¼‰ï¼š
- en: By using `package_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨`package_to_hub` **æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•å›æ”¾ã€ç”Ÿæˆä»£ç†çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ°hub**ã€‚
- en: 'This way:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼š
- en: You can **showcase our work** ğŸ”¥
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥
- en: You can **visualize your agent playing** ğŸ‘€
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å¯è§†åŒ–æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ** ğŸ‘€
- en: You can **share with the community an agent that others can use** ğŸ’¾
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«å…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†** ğŸ’¾
- en: You can **access a leaderboard ğŸ† to see how well your agent is performing compared
    to your classmates** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹æ‚¨çš„ä»£ç†ç›¸å¯¹äºåŒå­¦è¡¨ç°å¦‚ä½•** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¿˜æœ‰ä¸‰ä¸ªæ­¥éª¤è¦éµå¾ªï¼š
- en: 1ï¸âƒ£ (If itâ€™s not already done) create an account on Hugging Face â¡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ (å¦‚æœè¿˜æ²¡æœ‰) åœ¨Hugging Faceä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ· â¡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦ä»Hugging Faceç½‘ç«™å­˜å‚¨æ‚¨çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **å…·æœ‰å†™å…¥æƒé™**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ›å»ºHFä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤åˆ¶ä»¤ç‰Œ
- en: Run the cell below and paste the token
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼å¹¶ç²˜è´´ä»¤ç‰Œ
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you donâ€™t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`
- en: 3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using `package_to_hub()`
    function
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨`package_to_hub()`å‡½æ•°å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥
- en: 'Letâ€™s fill the `package_to_hub` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¡«å†™`package_to_hub`å‡½æ•°ï¼š
- en: '`model`: our trained model.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼šæˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚'
- en: '`model_name`: the name of the trained model that we defined in `model_save`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`ï¼šæˆ‘ä»¬åœ¨`model_save`ä¸­å®šä¹‰çš„è®­ç»ƒæ¨¡å‹çš„åç§°'
- en: '`model_architecture`: the model architecture we used, in our case PPO'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_architecture`ï¼šæˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹æ¶æ„ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯PPO'
- en: '`env_id`: the name of the environment, in our case `LunarLander-v2`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env_id`ï¼šç¯å¢ƒçš„åç§°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯`LunarLander-v2`'
- en: '`eval_env`: the evaluation environment defined in eval_env'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_env`ï¼šåœ¨eval_envä¸­å®šä¹‰çš„è¯„ä¼°ç¯å¢ƒ'
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`ï¼šå°†è¦åˆ›å»º/æ›´æ–°çš„Hugging Face Hubå­˜å‚¨åº“çš„åç§°`(repo_id = {username}/{repo_name})`'
- en: ğŸ’¡ **A good name is `{username}/{model_architecture}-{env_id}`**
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ **ä¸€ä¸ªå¥½çš„åç§°æ˜¯`{username}/{model_architecture}-{env_id}`**
- en: '`commit_message`: message of the commit'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`ï¼šæäº¤çš„æ¶ˆæ¯'
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Congrats ğŸ¥³ youâ€™ve just trained and uploaded your first Deep Reinforcement Learning
    agent. The script above should have displayed a link to a model repository such
    as [https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3).
    When you go to this link, you can:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œ ğŸ¥³ æ‚¨åˆšåˆšè®­ç»ƒå¹¶ä¸Šä¼ äº†æ‚¨çš„ç¬¬ä¸€ä¸ªæ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚ä¸Šé¢çš„è„šæœ¬åº”è¯¥æ˜¾ç¤ºäº†ä¸€ä¸ªæŒ‡å‘æ¨¡å‹å­˜å‚¨åº“çš„é“¾æ¥ï¼Œä¾‹å¦‚[https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3)ã€‚å½“æ‚¨è®¿é—®æ­¤é“¾æ¥æ—¶ï¼Œæ‚¨å¯ä»¥ï¼š
- en: See a video preview of your agent at the right.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å³ä¾§æŸ¥çœ‹ä»£ç†çš„è§†é¢‘é¢„è§ˆã€‚
- en: Click â€œFiles and versionsâ€ to see all the files in the repository.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‚¹å‡»â€œæ–‡ä»¶å’Œç‰ˆæœ¬â€ä»¥æŸ¥çœ‹å­˜å‚¨åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚
- en: Click â€œUse in stable-baselines3â€ to get a code snippet that shows how to load
    the model.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‚¹å‡»â€œåœ¨stable-baselines3ä¸­ä½¿ç”¨â€ä»¥è·å–æ˜¾ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹çš„ä»£ç ç‰‡æ®µã€‚
- en: A model card (`README.md` file) which gives a description of the model
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹å¡ç‰‡ï¼ˆ`README.md`æ–‡ä»¶ï¼‰ï¼Œå…¶ä¸­æè¿°äº†æ¨¡å‹
- en: Under the hood, the Hub uses git-based repositories (donâ€™t worry if you donâ€™t
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹•åï¼ŒHubä½¿ç”¨åŸºäºgitçš„å­˜å‚¨åº“ï¼ˆå¦‚æœä½ ä¸çŸ¥é“gitæ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨å®éªŒå’Œæ”¹è¿›ä»£ç†æ—¶æ›´æ–°æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚
- en: Compare the results of your LunarLander-v2 with your classmates using the leaderboard
    ğŸ† ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ’è¡Œæ¦œæ¯”è¾ƒä½ çš„LunarLander-v2çš„ç»“æœä¸ä½ çš„åŒå­¦ğŸ†ğŸ‘‰[https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: Load a saved LunarLander model from the Hub ğŸ¤—
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»HubåŠ è½½ä¿å­˜çš„LunarLanderæ¨¡å‹ğŸ¤—
- en: Thanks to [ironbar](https://github.com/ironbar) for the contribution.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢[ironbar](https://github.com/ironbar)çš„è´¡çŒ®ã€‚
- en: Loading a saved model from the Hub is really easy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä»HubåŠ è½½ä¿å­˜çš„æ¨¡å‹éå¸¸å®¹æ˜“ã€‚
- en: You go to [https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)
    to see the list of all the Stable-baselines3 saved models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å»[https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)æŸ¥çœ‹æ‰€æœ‰Stable-baselines3ä¿å­˜æ¨¡å‹çš„åˆ—è¡¨ã€‚
- en: You select one and copy its repo_id
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªå¹¶å¤åˆ¶å…¶repo_id
- en: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
- en: 'Then we just need to use load_from_hub with:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨load_from_hubï¼š
- en: The repo_id
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo and its extension (*.zip)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡ä»¶åï¼šå­˜å‚¨åº“ä¸­ä¿å­˜çš„æ¨¡å‹åŠå…¶æ‰©å±•åï¼ˆ*.zipï¼‰
- en: Because the model I download from the Hub was trained with Gym (the former version
    of Gymnasium) we need to install shimmy a API conversion tool that will help us
    to run the environment correctly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»Hubä¸‹è½½çš„æ¨¡å‹æ˜¯ä½¿ç”¨Gymï¼ˆGymnasiumçš„å‰èº«ï¼‰è®­ç»ƒçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å®‰è£…shimmyï¼Œè¿™æ˜¯ä¸€ä¸ªAPIè½¬æ¢å·¥å…·ï¼Œå°†å¸®åŠ©æˆ‘ä»¬æ­£ç¡®è¿è¡Œç¯å¢ƒã€‚
- en: 'Shimmy Documentation: [https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Shimmyæ–‡æ¡£ï¼š[https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Letâ€™s evaluate this agent:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯„ä¼°è¿™ä¸ªä»£ç†ï¼š
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Some additional challenges ğŸ†
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ğŸ†
- en: The best way to learn **is to try things by your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€ä½³æ–¹å¼**æ˜¯è‡ªå·±å°è¯•**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œä½ å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚é€šè¿‡100ä¸‡æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›å¾ˆå¥½çš„ç»“æœï¼
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°ä½ çš„ä»£ç†ã€‚ä½ èƒ½è¾¾åˆ°æ¦œé¦–å—ï¼Ÿ
- en: 'Here are some ideas to achieve so:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›å®ç°çš„æƒ³æ³•ï¼š
- en: Train more steps
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ›´å¤šæ­¥éª¤
- en: Try different hyperparameters for `PPO`. You can see them at [https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä¸åŒçš„`PPO`è¶…å‚æ•°ã€‚ä½ å¯ä»¥åœ¨[https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)çœ‹åˆ°å®ƒä»¬ã€‚
- en: Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    and try another model such as DQN.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[Stable-Baselines3æ–‡æ¡£](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)ï¼Œå°è¯•å¦ä¸€ä¸ªæ¨¡å‹ï¼Œå¦‚DQNã€‚
- en: '**Push your new trained model** on the Hub ğŸ”¥'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†ä½ çš„æ–°è®­ç»ƒæ¨¡å‹æ¨é€åˆ°Hub** ğŸ”¥'
- en: '**Compare the results of your LunarLander-v2 with your classmates** using the
    [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    ğŸ†'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸ä½ çš„åŒå­¦æ¯”è¾ƒä½ çš„LunarLander-v2çš„ç»“æœ**
    ğŸ†'
- en: Is moon landing too boring for you? Try to **change the environment**, why not
    use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the
    gym documentation](https://www.gymlibrary.dev/) and have fun ğŸ‰.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä½ æ¥è¯´ï¼Œç™»æœˆå¤ªæ— èŠäº†å—ï¼Ÿå°è¯•**æ”¹å˜ç¯å¢ƒ**ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨MountainCar-v0ï¼ŒCartPole-v1æˆ–CarRacing-v0ï¼ŸæŸ¥çœ‹å®ƒä»¬çš„å·¥ä½œæ–¹å¼[ä½¿ç”¨gymæ–‡æ¡£](https://www.gymlibrary.dev/)å¹¶äº«å—ä¹è¶£ğŸ‰ã€‚
- en: '* * *'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats on finishing this chapter! That was the biggest one, **and there was
    a lot of information.**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ å®Œæˆäº†è¿™ä¸€ç« ï¼è¿™æ˜¯æœ€å¤§çš„ä¸€ç« ï¼Œ**åŒ…å«äº†å¾ˆå¤šä¿¡æ¯ã€‚**
- en: If youâ€™re still feel confused with all these elementsâ€¦itâ€™s totally normal! **This
    was the same for me and for all people who studied RL.**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»ç„¶æ„Ÿåˆ°å›°æƒ‘ï¼Œè¿™äº›å…ƒç´ å¯¹æˆ‘å’Œæ‰€æœ‰å­¦ä¹ RLçš„äººæ¥è¯´éƒ½æ˜¯æ­£å¸¸çš„ï¼
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. Itâ€™s important to master these elements and have a solid foundations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­ä¹‹å‰èŠ±æ—¶é—´çœŸæ­£**æŒæ¡ææ–™å¹¶å°è¯•é¢å¤–çš„æŒ‘æˆ˜**ã€‚æŒæ¡è¿™äº›å…ƒç´ å¹¶å»ºç«‹åšå®çš„åŸºç¡€æ˜¯å¾ˆé‡è¦çš„ã€‚
- en: Naturally, during the course, weâ€™re going to dive deeper into these concepts
    but **itâ€™s better to have a good understanding of them now before diving into
    the next chapters.**
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨è¿™äº›æ¦‚å¿µï¼Œä½†**æœ€å¥½ç°åœ¨å°±å¯¹å®ƒä»¬æœ‰ä¸€ä¸ªè‰¯å¥½çš„ç†è§£ï¼Œç„¶åå†æ·±å…¥ä¸‹ä¸€ç« èŠ‚ã€‚**
- en: Next time, in the bonus unit 1, youâ€™ll train Huggy the Dog to fetch the stick.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡ï¼Œåœ¨å¥–åŠ±å•å…ƒ1ä¸­ï¼Œä½ å°†è®­ç»ƒHuggy the Dogå»æ¥æ£å­ã€‚
- en: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
- en: Keep learning, stay awesome ğŸ¤—
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»§ç»­å­¦ä¹ ï¼Œä¿æŒæ£’æ£’çš„ğŸ¤—
