- en: Train your first Deep Reinforcement Learning Agent ğŸ¤–
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that youâ€™ve studied the bases of Reinforcement Learning, youâ€™re ready to
    train your first agent and share it with the community through the Hub ğŸ”¥: A Lunar
    Lander agent that will learn to land correctly on the Moon ğŸŒ•'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![LunarLander](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: And finally, youâ€™ll **upload this trained agent to the Hugging Face Hub ğŸ¤—, a
    free, open platform where people can share ML models, datasets, and demos.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to our [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    youâ€™ll be able to compare your results with other classmates and exchange the
    best practices to improve your agentâ€™s scores. Who will win the challenge for
    Unit 1 ğŸ†?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**If you donâ€™t find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: So letâ€™s get started! ğŸš€
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on Open In Colab button** ğŸ‘‡ :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 1: Train your first Deep Reinforcement Learning Agent ğŸ¤–'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 1 thumbnail](../Images/269d50061313727de39330c553eb4733.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: In this notebook, youâ€™ll train your **first Deep Reinforcement Learning agent**
    a Lunar Lander agent that will learn to **land correctly on the Moon ğŸŒ•**. Using
    [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep
    Reinforcement Learning library, share them with the community, and experiment
    with different configurations
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The environment ğŸ®
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The library used ğŸ“š
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weâ€™re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook ğŸ†
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to use **Stable-Baselines3**, the deep reinforcement learning library.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score ğŸ”¥.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This notebook is from Deep Reinforcement Learning Course
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'In this free course, you will:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“– Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ§‘â€ğŸ’» Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ¤– Train **agents in unique environments**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ“ **Earn a certificate of completion** by completing 80% of the assignments.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Check ğŸ“š the syllabus ğŸ‘‰ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Donâ€™t forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able toÂ **send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep in touch and ask questions is **to join our discord server**
    to exchange with the community and with us ğŸ‘‰ğŸ» [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites ğŸ—ï¸
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ”² ğŸ“ **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)**
    that gives you all the **information about the course and helps you to onboard**
    ğŸ¤—
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ”² ğŸ“š **Develop an understanding of the foundations of Reinforcement learning**
    (MC, TD, Rewards hypothesisâ€¦) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A small recap of Deep Reinforcement Learning ğŸ“š
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s do a small recap on what we learned in the first Unit:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning is a **computational approach to learning from actions**.
    We build an agent that learns from the environment by **interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of any RL agent is to **maximize its expected cumulative reward** (also
    called expected return) because RL is based on the *reward hypothesis*, which
    is that all goals can be described as the maximization of an expected cumulative
    reward.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RL process is a **loop that outputs a sequence of state, action, reward,
    and next state**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the expected cumulative reward (expected return), **we discount
    the rewards**: the rewards that come sooner (at the beginning of the game) are
    more probable to happen since they are more predictable than the long-term future
    reward.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve an RL problem, you want to **find an optimal policy**; the policy is
    the â€œbrainâ€ of your AI that will tell us what action to take given a state. The
    optimal one is the one that gives you the actions that max the expected return.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are **two** ways to find your optimal policy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'By **training your policy directly**: policy-based methods.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By **training a value function** that tells us the expected return the agent
    will get at each state and use this function to define our policy: value-based
    methods.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we spoke about Deep RL because **we introduce deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based) hence the name â€œdeep.â€**
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Letâ€™s train our first Deep Reinforcement Learning agent and upload it to the
    Hub ğŸš€
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Get a certificate ğŸ“
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Set the GPU ğŸ’ª
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agentâ€™s training, weâ€™ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Install dependencies and create a virtual screen ğŸ”½
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to install the dependencies, weâ€™ll install multiple ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium[box2d]`: Contains the LunarLander-v2 environment ğŸŒ›'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stable-baselines3[extra]`: The deep reinforcement learning library.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face ğŸ¤— Hub.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To make things easier, we created a script to install all these dependencies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During the notebook, weâ€™ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install virtual screen libraries and create and
    run a virtual screen ğŸ–¥
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To make sure the new installed libraries are used, **sometimes itâ€™s required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so youâ€™ll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Import the packages ğŸ“¦
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One additional library we import is huggingface_hub **to be able to upload and
    download trained models from the hub**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face Hub ğŸ¤— works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: You can see here all the Deep reinforcement Learning models available hereğŸ‘‰
    [https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Understand Gymnasium and how it works ğŸ¤–
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ğŸ‹ The library containing our environment is called Gymnasium. **Youâ€™ll use Gymnasium
    a lot in Deep Reinforcement Learning.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gymnasium library provides two things:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: An interface that allows you to **create RL environments**.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **collection of environments** (gym-control, atari, box2Dâ€¦).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Letâ€™s look at an example, but first letâ€™s recall the RL loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'At each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Our Agent receivesÂ a **state (S0)**Â from theÂ **Environment**Â â€” we receive the
    first frame of our game (Environment).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on thatÂ **state (S0),**Â the Agent takes anÂ **action (A0)**Â â€” our Agent
    will move to the right.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment transitions to aÂ **new**Â **state (S1)**Â â€” new frame.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives someÂ **reward (R1)**Â to the Agent â€” weâ€™re not deadÂ *(Positive
    Reward +1)*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With Gymnasium:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ We create our environment using `gymnasium.make()`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ Get an action using our model (in our example we take a random action)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 4ï¸âƒ£ Using `env.step(action)`, we perform this action in the environment and
    get
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '`observation`: The new state (st+1)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The reward we get after executing the action'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`terminated`: Indicates if the episode terminated (agent reach the terminal
    state)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncated`: Introduced with this new version, it indicates a timelimit or
    if an agent go out of bounds of the environment for instance.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: A dictionary that provides additional information (depends on the environment).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more explanations check this ğŸ‘‰ [https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'If the episode is terminated:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Letâ€™s look at an example!** Make sure to read the code'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create the LunarLander environment ğŸŒ› and understand how it works
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The environment ğŸ®
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this first tutorial, weâ€™re going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),
    **to land correctly on the moon**. To do that, the agent needs to learn **to adapt
    its speed and position (horizontal, vertical, and angular) to land correctly.**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ’¡ A good habit when you start to use an environment is to check its documentation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ‘‰ [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s see what the Environment looks like:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We see with `Observation Space Shape (8,)` that the observation is a vector
    of size 8, where each value contains different information about the lander:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pad coordinate (x)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical pad coordinate (y)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal speed (x)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical speed (y)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angular speed
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left leg contact point has touched the land (boolean)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the right leg contact point has touched the land (boolean)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available ğŸ®:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Action 0: Do nothing,'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 1: Fire left orientation engine,'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 2: Fire the main engine,'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 3: Fire right orientation engine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function (the function that will give a reward at each timestep) ğŸ’°:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: After every step a reward is granted. The total reward of an episode is the
    **sum of the rewards for all the steps within that episode**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step, the reward:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Is increased/decreased the closer/further the lander is to the landing pad.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased/decreased the slower/faster the lander is moving.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased the more the lander is tilted (angle not horizontal).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased by 10 points for each leg that is in contact with the ground.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.03 points each frame a side engine is firing.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.3 points each frame the main engine is firing.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode receive an **additional reward of -100 or +100 points for crashing
    or landing safely respectively.**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: An episode is **considered a solution if it scores at least 200 points.**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Environment
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We create a vectorized environment (a method for stacking multiple independent
    environments into a single environment) of 16 environments, this way, **weâ€™ll
    have more diverse experiences during the training.**
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create the Model ğŸ¤–
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have studied our environment and we understood the problem: **being able
    to land the Lunar Lander to the Landing Pad correctly by controlling left, right
    and main orientation engine**. Now letâ€™s build the algorithm weâ€™re going to use
    to solve this Problem ğŸš€.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, weâ€™re going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SB3 is a set of **reliable implementations of reinforcement learning algorithms
    in PyTorch**.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'ğŸ’¡ A good habit when using a new library is to dive first on the documentation:
    [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)
    and then try some tutorials.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Stable Baselines3](../Images/12dda719af36ee58d0b11fadfe3279ba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, weâ€™re going to use SB3 **PPO**. [PPO (aka Proximal Policy
    Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning
    algorithms that youâ€™ll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO is a combination of:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Value-based reinforcement learning method*: learning an action-value function
    that will tell us the **most valuable action to take given a state and action**.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy-based reinforcement learning method*: learning a policy that will **give
    us a probability distribution over actions**.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 is easy to set up:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ You **create your environment** (in our case it was done above)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ You define the **model you want to use and instantiate this model** `model
    = PPO("MlpPolicy")`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ You **train the agent** with `model.learn` and define the number of training
    timesteps
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Solution
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Train the PPO agent ğŸƒ
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Letâ€™s train our agent for 1,000,000 timesteps, donâ€™t forget to use GPU on Colab.
    It will take approximately ~20min, but you can use fewer timesteps if you just
    want to try it out.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training, take a â˜• break you deserved it ğŸ¤—
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evaluate the agent ğŸ“ˆ
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that our Lunar Lander agent is trained ğŸš€, we need to **check its performance**.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next step, weâ€™ll see **how to automatically evaluate and share your agent
    to compete in a leaderboard, but for now letâ€™s do it ourselves**
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ’¡ When you evaluate your agent, you should not use your training environment
    but create an evaluation environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Solution
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million
    steps, which means that our lunar lander agent is ready to land on the moon ğŸŒ›ğŸ¥³.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish our trained model on the Hub ğŸ”¥
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub ğŸ¤— with one line of code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“š The libraries documentation ğŸ‘‰ [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-faceâ€”x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s an example of a Model Card (with Space Invaders):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: By using `package_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'This way:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: You can **showcase our work** ğŸ”¥
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **visualize your agent playing** ğŸ‘€
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **share with the community an agent that others can use** ğŸ’¾
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can **access a leaderboard ğŸ† to see how well your agent is performing compared
    to your classmates** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ (If itâ€™s not already done) create an account on Hugging Face â¡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you donâ€™t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using `package_to_hub()`
    function
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s fill the `package_to_hub` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '`model`: our trained model.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name`: the name of the trained model that we defined in `model_save`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_architecture`: the model architecture we used, in our case PPO'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env_id`: the name of the environment, in our case `LunarLander-v2`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_env`: the evaluation environment defined in eval_env'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ’¡ **A good name is `{username}/{model_architecture}-{env_id}`**
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`commit_message`: message of the commit'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Congrats ğŸ¥³ youâ€™ve just trained and uploaded your first Deep Reinforcement Learning
    agent. The script above should have displayed a link to a model repository such
    as [https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3).
    When you go to this link, you can:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: See a video preview of your agent at the right.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click â€œFiles and versionsâ€ to see all the files in the repository.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click â€œUse in stable-baselines3â€ to get a code snippet that shows how to load
    the model.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‚¹å‡»â€œåœ¨stable-baselines3ä¸­ä½¿ç”¨â€ä»¥è·å–æ˜¾ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹çš„ä»£ç ç‰‡æ®µã€‚
- en: A model card (`README.md` file) which gives a description of the model
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹å¡ç‰‡ï¼ˆ`README.md`æ–‡ä»¶ï¼‰ï¼Œå…¶ä¸­æè¿°äº†æ¨¡å‹
- en: Under the hood, the Hub uses git-based repositories (donâ€™t worry if you donâ€™t
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹•åï¼ŒHubä½¿ç”¨åŸºäºgitçš„å­˜å‚¨åº“ï¼ˆå¦‚æœä½ ä¸çŸ¥é“gitæ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨å®éªŒå’Œæ”¹è¿›ä»£ç†æ—¶æ›´æ–°æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚
- en: Compare the results of your LunarLander-v2 with your classmates using the leaderboard
    ğŸ† ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ’è¡Œæ¦œæ¯”è¾ƒä½ çš„LunarLander-v2çš„ç»“æœä¸ä½ çš„åŒå­¦ğŸ†ğŸ‘‰[https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: Load a saved LunarLander model from the Hub ğŸ¤—
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»HubåŠ è½½ä¿å­˜çš„LunarLanderæ¨¡å‹ğŸ¤—
- en: Thanks to [ironbar](https://github.com/ironbar) for the contribution.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢[ironbar](https://github.com/ironbar)çš„è´¡çŒ®ã€‚
- en: Loading a saved model from the Hub is really easy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ä»HubåŠ è½½ä¿å­˜çš„æ¨¡å‹éå¸¸å®¹æ˜“ã€‚
- en: You go to [https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)
    to see the list of all the Stable-baselines3 saved models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å»[https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)æŸ¥çœ‹æ‰€æœ‰Stable-baselines3ä¿å­˜æ¨¡å‹çš„åˆ—è¡¨ã€‚
- en: You select one and copy its repo_id
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªå¹¶å¤åˆ¶å…¶repo_id
- en: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
- en: 'Then we just need to use load_from_hub with:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨load_from_hubï¼š
- en: The repo_id
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo and its extension (*.zip)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡ä»¶åï¼šå­˜å‚¨åº“ä¸­ä¿å­˜çš„æ¨¡å‹åŠå…¶æ‰©å±•åï¼ˆ*.zipï¼‰
- en: Because the model I download from the Hub was trained with Gym (the former version
    of Gymnasium) we need to install shimmy a API conversion tool that will help us
    to run the environment correctly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»Hubä¸‹è½½çš„æ¨¡å‹æ˜¯ä½¿ç”¨Gymï¼ˆGymnasiumçš„å‰èº«ï¼‰è®­ç»ƒçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å®‰è£…shimmyï¼Œè¿™æ˜¯ä¸€ä¸ªAPIè½¬æ¢å·¥å…·ï¼Œå°†å¸®åŠ©æˆ‘ä»¬æ­£ç¡®è¿è¡Œç¯å¢ƒã€‚
- en: 'Shimmy Documentation: [https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Shimmyæ–‡æ¡£ï¼š[https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Letâ€™s evaluate this agent:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯„ä¼°è¿™ä¸ªä»£ç†ï¼š
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Some additional challenges ğŸ†
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ğŸ†
- en: The best way to learn **is to try things by your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€ä½³æ–¹å¼**æ˜¯è‡ªå·±å°è¯•**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œä½ å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚é€šè¿‡100ä¸‡æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›å¾ˆå¥½çš„ç»“æœï¼
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°ä½ çš„ä»£ç†ã€‚ä½ èƒ½è¾¾åˆ°æ¦œé¦–å—ï¼Ÿ
- en: 'Here are some ideas to achieve so:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›å®ç°çš„æƒ³æ³•ï¼š
- en: Train more steps
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ›´å¤šæ­¥éª¤
- en: Try different hyperparameters for `PPO`. You can see them at [https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä¸åŒçš„`PPO`è¶…å‚æ•°ã€‚ä½ å¯ä»¥åœ¨[https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)çœ‹åˆ°å®ƒä»¬ã€‚
- en: Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    and try another model such as DQN.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[Stable-Baselines3æ–‡æ¡£](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)ï¼Œå°è¯•å¦ä¸€ä¸ªæ¨¡å‹ï¼Œå¦‚DQNã€‚
- en: '**Push your new trained model** on the Hub ğŸ”¥'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†ä½ çš„æ–°è®­ç»ƒæ¨¡å‹æ¨é€åˆ°Hub** ğŸ”¥'
- en: '**Compare the results of your LunarLander-v2 with your classmates** using the
    [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    ğŸ†'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸ä½ çš„åŒå­¦æ¯”è¾ƒä½ çš„LunarLander-v2çš„ç»“æœ**
    ğŸ†'
- en: Is moon landing too boring for you? Try to **change the environment**, why not
    use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the
    gym documentation](https://www.gymlibrary.dev/) and have fun ğŸ‰.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä½ æ¥è¯´ï¼Œç™»æœˆå¤ªæ— èŠäº†å—ï¼Ÿå°è¯•**æ”¹å˜ç¯å¢ƒ**ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨MountainCar-v0ï¼ŒCartPole-v1æˆ–CarRacing-v0ï¼ŸæŸ¥çœ‹å®ƒä»¬çš„å·¥ä½œæ–¹å¼[ä½¿ç”¨gymæ–‡æ¡£](https://www.gymlibrary.dev/)å¹¶äº«å—ä¹è¶£ğŸ‰ã€‚
- en: '* * *'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats on finishing this chapter! That was the biggest one, **and there was
    a lot of information.**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ å®Œæˆäº†è¿™ä¸€ç« ï¼è¿™æ˜¯æœ€å¤§çš„ä¸€ç« ï¼Œ**åŒ…å«äº†å¾ˆå¤šä¿¡æ¯ã€‚**
- en: If youâ€™re still feel confused with all these elementsâ€¦itâ€™s totally normal! **This
    was the same for me and for all people who studied RL.**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»ç„¶æ„Ÿåˆ°å›°æƒ‘ï¼Œè¿™äº›å…ƒç´ å¯¹æˆ‘å’Œæ‰€æœ‰å­¦ä¹ RLçš„äººæ¥è¯´éƒ½æ˜¯æ­£å¸¸çš„ï¼
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. Itâ€™s important to master these elements and have a solid foundations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­ä¹‹å‰èŠ±æ—¶é—´çœŸæ­£**æŒæ¡ææ–™å¹¶å°è¯•é¢å¤–çš„æŒ‘æˆ˜**ã€‚æŒæ¡è¿™äº›å…ƒç´ å¹¶å»ºç«‹åšå®çš„åŸºç¡€æ˜¯å¾ˆé‡è¦çš„ã€‚
- en: Naturally, during the course, weâ€™re going to dive deeper into these concepts
    but **itâ€™s better to have a good understanding of them now before diving into
    the next chapters.**
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ›´æ·±å…¥åœ°æ¢è®¨è¿™äº›æ¦‚å¿µï¼Œä½†**æœ€å¥½ç°åœ¨å°±å¯¹å®ƒä»¬æœ‰ä¸€ä¸ªè‰¯å¥½çš„ç†è§£ï¼Œç„¶åå†æ·±å…¥ä¸‹ä¸€ç« èŠ‚ã€‚**
- en: Next time, in the bonus unit 1, youâ€™ll train Huggy the Dog to fetch the stick.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡ï¼Œåœ¨å¥–åŠ±å•å…ƒ1ä¸­ï¼Œä½ å°†è®­ç»ƒHuggy the Dogå»æ¥æ£å­ã€‚
- en: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
- en: Keep learning, stay awesome ğŸ¤—
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»§ç»­å­¦ä¹ ï¼Œä¿æŒæ£’æ£’çš„ğŸ¤—
