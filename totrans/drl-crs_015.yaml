- en: Train your first Deep Reinforcement Learning Agent 🤖
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练你的第一个深度强化学习代理🤖
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit1/hands-on](https://huggingface.co/learn/deep-rl-course/unit1/hands-on)
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![提问](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit1/unit1.ipynb)'
- en: 'Now that you’ve studied the bases of Reinforcement Learning, you’re ready to
    train your first agent and share it with the community through the Hub 🔥: A Lunar
    Lander agent that will learn to land correctly on the Moon 🌕'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了强化学习的基础知识，你可以开始训练你的第一个代理，并通过Hub与社区分享🔥：一个月球着陆器代理，将学会正确着陆在月球上🌕
- en: '![LunarLander](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![月球着陆器](../Images/2cd989d7bf01c3c770ed0607b8bfdd59.png)'
- en: And finally, you’ll **upload this trained agent to the Hugging Face Hub 🤗, a
    free, open platform where people can share ML models, datasets, and demos.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将**上传这个训练好的代理到Hugging Face Hub 🤗，这是一个免费、开放的平台，人们可以在这里分享机器学习模型、数据集和演示。**
- en: Thanks to our [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    you’ll be able to compare your results with other classmates and exchange the
    best practices to improve your agent’s scores. Who will win the challenge for
    Unit 1 🏆?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们的[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)，你将能够与其他同学比较你的结果，并交流最佳实践，以提高你的代理分数。谁将赢得Unit
    1的挑战🏆？
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这个[认证流程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)的实践性，你需要将训练好的模型推送到Hub，并**获得>=200的结果**。
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到你的结果，去[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)找到你的模型，**结果=平均奖励-奖励的标准差**
- en: '**If you don’t find your model, go to the bottom of the page and click on the
    refresh button.**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果找不到你的模型，请转到页面底部并点击刷新按钮。**'
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证流程的更多信息，请查看此部分👉[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: And you can check your progress here 👉 [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里检查你的进度👉[https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
- en: So let’s get started! 🚀
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！🚀
- en: '**To start the hands-on click on Open In Colab button** 👇 :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**要开始实践，请点击“在Colab中打开”按钮**👇：'
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![在Colab中打开](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit1/unit1.ipynb)'
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议学生使用Google Colab进行实践练习，而不是在个人电脑上运行。
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Google Colab，**你可以专注于学习和实验，而不必担心设置环境的技术细节**。
- en: 'Unit 1: Train your first Deep Reinforcement Learning Agent 🤖'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Unit 1：训练你的第一个深度强化学习代理🤖
- en: '![Unit 1 thumbnail](../Images/269d50061313727de39330c553eb4733.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Unit 1缩略图](../Images/269d50061313727de39330c553eb4733.png)'
- en: In this notebook, you’ll train your **first Deep Reinforcement Learning agent**
    a Lunar Lander agent that will learn to **land correctly on the Moon 🌕**. Using
    [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep
    Reinforcement Learning library, share them with the community, and experiment
    with different configurations
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个笔记本中，你将训练你的**第一个深度强化学习代理**，一个月球着陆器代理，将学会**在月球上正确着陆🌕**。使用[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)这个深度强化学习库，与社区分享，并尝试不同的配置
- en: The environment 🎮
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境🎮
- en: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)'
- en: The library used 📚
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用的库📚
- en: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)'
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断努力改进我们的教程，所以**如果你在这个笔记本中发现了一些问题**，请在Github Repo上[提出问题](https://github.com/huggingface/deep-rl-class/issues)。
- en: Objectives of this notebook 🏆
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本笔记本的目标🏆
- en: 'At the end of the notebook, you will:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的结尾，你将：
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用**Gymnasium**，这是一个环境库。
- en: Be able to use **Stable-Baselines3**, the deep reinforcement learning library.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用**Stable-Baselines3**，这是一个深度强化学习库。
- en: Be able to **push your trained agent to the Hub** with a nice video replay and
    an evaluation score 🔥.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够**将训练好的代理推送到Hub**，并附上一个漂亮的视频回放和一个评估分数🔥。
- en: This notebook is from Deep Reinforcement Learning Course
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个笔记本来自深度强化学习课程
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![深度强化学习课程插图](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
- en: 'In this free course, you will:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这门免费课程中，您将：
- en: 📖 Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📖 理论和实践上学习深度强化学习。
- en: 🧑‍💻 Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🧑‍💻 学习使用著名的深度RL库，如Stable Baselines3、RL Baselines3 Zoo、CleanRL和Sample Factory
    2.0。
- en: 🤖 Train **agents in unique environments**
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🤖 在独特环境中**训练代理**
- en: 🎓 **Earn a certificate of completion** by completing 80% of the assignments.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🎓 通过完成80%的作业**获得完成证书**。
- en: And more!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以及更多！
- en: Check 📚 the syllabus 👉 [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 查看📚课程大纲👉[https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
- en: Don’t forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able to **send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记**[注册课程](http://eepurl.com/ic5ZUD)**（我们正在收集您的电子邮件以便**在每个单元发布时向您发送链接并提供有关挑战和更新的信息）**。
- en: The best way to keep in touch and ask questions is **to join our discord server**
    to exchange with the community and with us 👉🏻 [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 保持联系并提问的最佳方式是**加入我们的discord服务器**与社区和我们交流👉🏻[https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
- en: Prerequisites 🏗️
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件🏗️
- en: 'Before diving into the notebook, you need to:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入笔记本之前，您需要：
- en: 🔲 📝 **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)**
    that gives you all the **information about the course and helps you to onboard**
    🤗
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 🔲📝 通过[阅读第0单元](https://huggingface.co/deep-rl-course/unit0/introduction) **获取有关课程的所有信息并帮助您入门**🤗
- en: 🔲 📚 **Develop an understanding of the foundations of Reinforcement learning**
    (MC, TD, Rewards hypothesis…) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 🔲📚 通过[阅读第1单元](https://huggingface.co/deep-rl-course/unit1/introduction) **了解强化学习基础**（MC、TD、奖励假设...）。
- en: A small recap of Deep Reinforcement Learning 📚
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习的小结📚
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![RL过程](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: 'Let’s do a small recap on what we learned in the first Unit:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下我们在第一个单元学到的内容：
- en: Reinforcement Learning is a **computational approach to learning from actions**.
    We build an agent that learns from the environment by **interacting with it through
    trial and error** and receiving rewards (negative or positive) as feedback.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是一种**从行动中学习的计算方法**。我们构建一个代理，通过**通过试错与环境互动**并接收奖励（负面或正面）作为反馈来学习。
- en: The goal of any RL agent is to **maximize its expected cumulative reward** (also
    called expected return) because RL is based on the *reward hypothesis*, which
    is that all goals can be described as the maximization of an expected cumulative
    reward.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何RL代理的目标是**最大化其预期累积奖励**（也称为预期回报），因为RL基于*奖励假设*，即所有目标都可以描述为最大化预期累积奖励。
- en: The RL process is a **loop that outputs a sequence of state, action, reward,
    and next state**.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL过程是一个**输出状态、动作、奖励和下一个状态序列的循环**。
- en: 'To calculate the expected cumulative reward (expected return), **we discount
    the rewards**: the rewards that come sooner (at the beginning of the game) are
    more probable to happen since they are more predictable than the long-term future
    reward.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了计算预期累积奖励（预期回报），**我们折扣奖励**：越早获得的奖励（在游戏开始时）更有可能发生，因为它们比长期未来奖励更可预测。
- en: To solve an RL problem, you want to **find an optimal policy**; the policy is
    the “brain” of your AI that will tell us what action to take given a state. The
    optimal one is the one that gives you the actions that max the expected return.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要解决RL问题，您希望**找到一个最优策略**；策略是您的AI的“大脑”，将告诉我们在给定状态下应该采取什么行动。最佳策略是使您的行动最大化预期回报的策略。
- en: 'There are **two** ways to find your optimal policy:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有**两种**方法可以找到您的最优策略：
- en: 'By **training your policy directly**: policy-based methods.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**直接训练您的策略**：基于策略的方法。
- en: 'By **training a value function** that tells us the expected return the agent
    will get at each state and use this function to define our policy: value-based
    methods.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**训练一个值函数**告诉我们代理在每个状态将获得的预期回报，并使用此函数定义我们的策略：基于价值的方法。
- en: Finally, we spoke about Deep RL because **we introduce deep neural networks
    to estimate the action to take (policy-based) or to estimate the value of a state
    (value-based) hence the name “deep.”**
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们谈到了深度RL，因为**我们引入了深度神经网络来估计要采取的行动（基于策略）或估计状态的价值（基于价值），因此得名“深度”。**
- en: Let’s train our first Deep Reinforcement Learning agent and upload it to the
    Hub 🚀
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们训练我们的第一个深度强化学习代理并将其上传到Hub🚀
- en: Get a certificate 🎓
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获得证书🎓
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained model to the Hub and **get a result of >= 200**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一点，进行[认证流程](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)，您需要将训练好的模型推送到Hub并**获得>=200的结果**。
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到您的结果，请转到[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)并找到您的模型，**结果=平均奖励-奖励的标准差**
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证流程的更多信息，请查看此部分👉[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: Set the GPU 💪
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置GPU💪
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了**加速代理的训练，我们将使用GPU**。要做到这一点，转到`运行时 > 更改运行时类型`
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![GPU步骤1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
- en: '`Hardware Accelerator > GPU`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件加速器 > GPU
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![GPU 步骤 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
- en: Install dependencies and create a virtual screen 🔽
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装依赖并创建虚拟屏幕🔽
- en: The first step is to install the dependencies, we’ll install multiple ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装依赖项，我们将安装多个。
- en: '`gymnasium[box2d]`: Contains the LunarLander-v2 environment 🌛'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gymnasium[box2d]`：包含 LunarLander-v2 环境🌛'
- en: '`stable-baselines3[extra]`: The deep reinforcement learning library.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stable-baselines3[extra]`：深度强化学习库。'
- en: '`huggingface_sb3`: Additional code for Stable-baselines3 to load and upload
    models from the Hugging Face 🤗 Hub.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huggingface_sb3`：用于 Stable-baselines3 加载和上传模型的额外代码来自 Hugging Face 🤗 Hub。'
- en: To make things easier, we created a script to install all these dependencies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化事情，我们创建了一个脚本来安装所有这些依赖项。
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During the notebook, we’ll need to generate a replay video. To do so, with colab,
    **we need to have a virtual screen to be able to render the environment** (and
    thus record the frames).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们需要生成一个重播视频。为此，在 colab 中，**我们需要有一个虚拟屏幕来渲染环境**（从而记录帧）。
- en: Hence the following cell will install virtual screen libraries and create and
    run a virtual screen 🖥
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，接下来的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕🖥
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To make sure the new installed libraries are used, **sometimes it’s required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so you’ll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保新安装的库被使用，**有时需要重新启动笔记本运行时**。下一个单元格将强制**运行时崩溃，因此您需要重新连接并从这里开始运行代码**。通过这个技巧，**我们将能够运行我们的虚拟屏幕**。
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Import the packages 📦
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入包📦
- en: One additional library we import is huggingface_hub **to be able to upload and
    download trained models from the hub**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入的另一个库是 huggingface_hub **以便能够从 hub 上传和下载训练好的模型**。
- en: The Hugging Face Hub 🤗 works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub 🤗 作为一个中心地方，任何人都可以分享和探索模型和数据集。它具有版本控制、指标、可视化和其他功能，让您可以轻松与他人合作。
- en: You can see here all the Deep reinforcement Learning models available here👉
    [https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里看到所有可用的深度强化学习模型👉[https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads](https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads)
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Understand Gymnasium and how it works 🤖
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 Gymnasium 及其工作原理🤖
- en: 🏋 The library containing our environment is called Gymnasium. **You’ll use Gymnasium
    a lot in Deep Reinforcement Learning.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋 包含我们环境的库称为 Gymnasium。**在深度强化学习中，您将经常使用 Gymnasium。**
- en: Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium 是由 Farama Foundation 维护的 Gym 库的**新版本**[（https://farama.org/）。
- en: 'The Gymnasium library provides two things:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium 库提供两个东西：
- en: An interface that allows you to **create RL environments**.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个允许您**创建 RL 环境**的接口。
- en: A **collection of environments** (gym-control, atari, box2D…).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组环境（gym-control、atari、box2D...）。
- en: Let’s look at an example, but first let’s recall the RL loop.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，但首先让我们回顾一下 RL 循环。
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![RL 过程](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: 'At each step:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步：
- en: Our Agent receives a **state (S0)** from the **Environment** — we receive the
    first frame of our game (Environment).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的代理从**环境**接收一个**状态（S0）**——我们接收到游戏的第一帧（环境）。
- en: Based on that **state (S0),** the Agent takes an **action (A0)** — our Agent
    will move to the right.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于那个**状态（S0）**，代理采取一个**动作（A0）**——我们的代理将向右移动。
- en: The environment transitions to a **new** **state (S1)** — new frame.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境转换到一个**新的状态（S1）**——新的帧。
- en: The environment gives some **reward (R1)** to the Agent — we’re not dead *(Positive
    Reward +1)*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境给代理一些**奖励（R1）**——我们还没有死亡*(正奖励 +1)*。
- en: 'With Gymnasium:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Gymnasium：
- en: 1️⃣ We create our environment using `gymnasium.make()`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 我们使用 `gymnasium.make()` 创建我们的环境
- en: 2️⃣ We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 我们使用 `observation = env.reset()` 将环境重置为初始状态
- en: 'At each step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步：
- en: 3️⃣ Get an action using our model (in our example we take a random action)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 使用我们的模型获取一个动作（在我们的示例中我们执行一个随机动作）
- en: 4️⃣ Using `env.step(action)`, we perform this action in the environment and
    get
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 4️⃣ 使用 `env.step(action)`，我们在环境中执行此动作并获取
- en: '`observation`: The new state (st+1)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observation`：新状态（st+1）'
- en: '`reward`: The reward we get after executing the action'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`：执行动作后我们获得的奖励'
- en: '`terminated`: Indicates if the episode terminated (agent reach the terminal
    state)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`terminated`：指示剧集是否终止（代理到达终止状态）'
- en: '`truncated`: Introduced with this new version, it indicates a timelimit or
    if an agent go out of bounds of the environment for instance.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncated`：在这个新版本中引入，它指示一个时间限制或者如果代理超出环境的边界。'
- en: '`info`: A dictionary that provides additional information (depends on the environment).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`：提供额外信息的字典（取决于环境）。'
- en: For more explanations check this 👉 [https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多解释，请查看此链接👉[https://gymnasium.farama.org/api/env/#gymnasium.Env.step](https://gymnasium.farama.org/api/env/#gymnasium.Env.step)
- en: 'If the episode is terminated:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果剧集终止：
- en: We reset the environment to its initial state with `observation = env.reset()`
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `observation = env.reset()` 将环境重置为初始状态
- en: '**Let’s look at an example!** Make sure to read the code'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们看一个例子！** 确保阅读代码'
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Create the LunarLander environment 🌛 and understand how it works
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 LunarLander 环境🌛并了解其工作原理
- en: The environment 🎮
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境🎮
- en: In this first tutorial, we’re going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),
    **to land correctly on the moon**. To do that, the agent needs to learn **to adapt
    its speed and position (horizontal, vertical, and angular) to land correctly.**
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 💡 A good habit when you start to use an environment is to check its documentation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 👉 [https://gymnasium.farama.org/environments/box2d/lunar_lander/](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what the Environment looks like:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We see with `Observation Space Shape (8,)` that the observation is a vector
    of size 8, where each value contains different information about the lander:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pad coordinate (x)
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical pad coordinate (y)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal speed (x)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertical speed (y)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angle
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angular speed
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the left leg contact point has touched the land (boolean)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the right leg contact point has touched the land (boolean)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available 🎮:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Action 0: Do nothing,'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 1: Fire left orientation engine,'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 2: Fire the main engine,'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action 3: Fire right orientation engine.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reward function (the function that will give a reward at each timestep) 💰:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: After every step a reward is granted. The total reward of an episode is the
    **sum of the rewards for all the steps within that episode**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'For each step, the reward:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Is increased/decreased the closer/further the lander is to the landing pad.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased/decreased the slower/faster the lander is moving.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased the more the lander is tilted (angle not horizontal).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is increased by 10 points for each leg that is in contact with the ground.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.03 points each frame a side engine is firing.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is decreased by 0.3 points each frame the main engine is firing.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode receive an **additional reward of -100 or +100 points for crashing
    or landing safely respectively.**
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: An episode is **considered a solution if it scores at least 200 points.**
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized Environment
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We create a vectorized environment (a method for stacking multiple independent
    environments into a single environment) of 16 environments, this way, **we’ll
    have more diverse experiences during the training.**
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Create the Model 🤖
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have studied our environment and we understood the problem: **being able
    to land the Lunar Lander to the Landing Pad correctly by controlling left, right
    and main orientation engine**. Now let’s build the algorithm we’re going to use
    to solve this Problem 🚀.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, we’re going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SB3 is a set of **reliable implementations of reinforcement learning algorithms
    in PyTorch**.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '💡 A good habit when using a new library is to dive first on the documentation:
    [https://stable-baselines3.readthedocs.io/en/master/](https://stable-baselines3.readthedocs.io/en/master/)
    and then try some tutorials.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![Stable Baselines3](../Images/12dda719af36ee58d0b11fadfe3279ba.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we’re going to use SB3 **PPO**. [PPO (aka Proximal Policy
    Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning
    algorithms that you’ll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO is a combination of:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*Value-based reinforcement learning method*: learning an action-value function
    that will tell us the **most valuable action to take given a state and action**.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Policy-based reinforcement learning method*: learning a policy that will **give
    us a probability distribution over actions**.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable-Baselines3 is easy to set up:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ You **create your environment** (in our case it was done above)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ You define the **model you want to use and instantiate this model** `model
    = PPO("MlpPolicy")`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ You **train the agent** with `model.learn` and define the number of training
    timesteps
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Solution
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Train the PPO agent 🏃
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练PPO代理 🏃
- en: Let’s train our agent for 1,000,000 timesteps, don’t forget to use GPU on Colab.
    It will take approximately ~20min, but you can use fewer timesteps if you just
    want to try it out.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们为我们的代理训练1,000,000个时间步，不要忘记在Colab上使用GPU。这将大约需要~20分钟，但如果您只想尝试一下，您可以使用更少的时间步数。
- en: During the training, take a ☕ break you deserved it 🤗
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间，休息一下，您值得拥有 🤗
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Solution
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evaluate the agent 📈
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估代理 📈
- en: Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记得将环境包装在[监视器](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html)中。
- en: Now that our Lunar Lander agent is trained 🚀, we need to **check its performance**.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们的月球着陆器代理已经训练好了 🚀，我们需要**检查其性能**。
- en: 'Stable-Baselines3 provides a method to do that: `evaluate_policy`.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stable-Baselines3提供了一个方法来做到这一点：`evaluate_policy`。
- en: To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要填写该部分，您需要[查看文档](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)
- en: In the next step, we’ll see **how to automatically evaluate and share your agent
    to compete in a leaderboard, but for now let’s do it ourselves**
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将看到**如何自动评估和分享您的代理以参加排行榜比赛，但现在让我们自己来做**
- en: 💡 When you evaluate your agent, you should not use your training environment
    but create an evaluation environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 当您评估您的代理时，您不应该使用训练环境，而是创建一个评估环境。
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Solution
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In my case, I got a mean reward is `200.20 +/- 20.80` after training for 1 million
    steps, which means that our lunar lander agent is ready to land on the moon 🌛🥳.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我的情况下，训练1百万步后，我得到的平均奖励是`200.20 +/- 20.80`，这意味着我们的月球着陆器代理已经准备好在月球上着陆 🌛🥳。
- en: Publish our trained model on the Hub 🔥
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Hub上发布我们训练好的模型 🔥
- en: Now that we saw we got good results after the training, we can publish our trained
    model on the hub 🤗 with one line of code.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到在训练后取得了良好的结果，我们可以用一行代码将我们训练好的模型发布到hub 🤗。
- en: 📚 The libraries documentation 👉 [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face—x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 📚 库的文档 👉 [https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face—x-stable-baselines3-v20](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)
- en: 'Here’s an example of a Model Card (with Space Invaders):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个模型卡的示例（带有太空侵略者）：
- en: By using `package_to_hub` **you evaluate, record a replay, generate a model
    card of your agent and push it to the hub**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`package_to_hub` **您可以评估、记录回放、生成代理的模型卡并将其推送到hub**。
- en: 'This way:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这样：
- en: You can **showcase our work** 🔥
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**展示我们的工作** 🔥
- en: You can **visualize your agent playing** 👀
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**可视化您的代理进行游戏** 👀
- en: You can **share with the community an agent that others can use** 💾
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**与社区分享其他人可以使用的代理** 💾
- en: You can **access a leaderboard 🏆 to see how well your agent is performing compared
    to your classmates** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以**访问排行榜 🏆 查看您的代理相对于同学表现如何** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够与社区分享您的模型，还有三个步骤要遵循：
- en: 1️⃣ (If it’s not already done) create an account on Hugging Face ➡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ (如果还没有) 在Hugging Face上创建一个帐户 ➡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2️⃣ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 登录，然后，您需要从Hugging Face网站存储您的身份验证令牌。
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的令牌([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **具有写入权限**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![创建HF令牌](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制令牌
- en: Run the cell below and paste the token
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行下面的单元格并粘贴令牌
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you don’t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用Google Colab或Jupyter Notebook，您需要使用这个命令：`huggingface-cli login`
- en: 3️⃣ We’re now ready to push our trained agent to the 🤗 Hub 🔥 using `package_to_hub()`
    function
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 现在我们准备使用`package_to_hub()`函数将我们训练好的代理推送到🤗 Hub 🔥
- en: 'Let’s fill the `package_to_hub` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们填写`package_to_hub`函数：
- en: '`model`: our trained model.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`：我们训练好的模型。'
- en: '`model_name`: the name of the trained model that we defined in `model_save`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name`：我们在`model_save`中定义的训练模型的名称'
- en: '`model_architecture`: the model architecture we used, in our case PPO'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_architecture`：我们使用的模型架构，在我们的案例中是PPO'
- en: '`env_id`: the name of the environment, in our case `LunarLander-v2`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env_id`：环境的名称，在我们的案例中是`LunarLander-v2`'
- en: '`eval_env`: the evaluation environment defined in eval_env'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_env`：在eval_env中定义的评估环境'
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`：将要创建/更新的Hugging Face Hub存储库的名称`(repo_id = {username}/{repo_name})`'
- en: 💡 **A good name is `{username}/{model_architecture}-{env_id}`**
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 **一个好的名称是`{username}/{model_architecture}-{env_id}`**
- en: '`commit_message`: message of the commit'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`：提交的消息'
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解决方案
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Congrats 🥳 you’ve just trained and uploaded your first Deep Reinforcement Learning
    agent. The script above should have displayed a link to a model repository such
    as [https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3).
    When you go to this link, you can:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜 🥳 您刚刚训练并上传了您的第一个深度强化学习代理。上面的脚本应该显示了一个指向模型存储库的链接，例如[https://huggingface.co/osanseviero/test_sb3](https://huggingface.co/osanseviero/test_sb3)。当您访问此链接时，您可以：
- en: See a video preview of your agent at the right.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在右侧查看代理的视频预览。
- en: Click “Files and versions” to see all the files in the repository.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“文件和版本”以查看存储库中的所有文件。
- en: Click “Use in stable-baselines3” to get a code snippet that shows how to load
    the model.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击“在stable-baselines3中使用”以获取显示如何加载模型的代码片段。
- en: A model card (`README.md` file) which gives a description of the model
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型卡片（`README.md`文件），其中描述了模型
- en: Under the hood, the Hub uses git-based repositories (don’t worry if you don’t
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，Hub使用基于git的存储库（如果你不知道git是什么，不用担心），这意味着你可以在实验和改进代理时更新模型的新版本。
- en: Compare the results of your LunarLander-v2 with your classmates using the leaderboard
    🏆 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用排行榜比较你的LunarLander-v2的结果与你的同学🏆👉[https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: Load a saved LunarLander model from the Hub 🤗
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Hub加载保存的LunarLander模型🤗
- en: Thanks to [ironbar](https://github.com/ironbar) for the contribution.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[ironbar](https://github.com/ironbar)的贡献。
- en: Loading a saved model from the Hub is really easy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 从Hub加载保存的模型非常容易。
- en: You go to [https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)
    to see the list of all the Stable-baselines3 saved models.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以去[https://huggingface.co/models?library=stable-baselines3](https://huggingface.co/models?library=stable-baselines3)查看所有Stable-baselines3保存模型的列表。
- en: You select one and copy its repo_id
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个并复制其repo_id
- en: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![Copy-id](../Images/f3fbf7e375946adf2b0df2b8be31be10.png)'
- en: 'Then we just need to use load_from_hub with:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们只需要使用load_from_hub：
- en: The repo_id
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo and its extension (*.zip)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件名：存储库中保存的模型及其扩展名（*.zip）
- en: Because the model I download from the Hub was trained with Gym (the former version
    of Gymnasium) we need to install shimmy a API conversion tool that will help us
    to run the environment correctly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我从Hub下载的模型是使用Gym（Gymnasium的前身）训练的，所以我们需要安装shimmy，这是一个API转换工具，将帮助我们正确运行环境。
- en: 'Shimmy Documentation: [https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Shimmy文档：[https://github.com/Farama-Foundation/Shimmy](https://github.com/Farama-Foundation/Shimmy)
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s evaluate this agent:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估这个代理：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Some additional challenges 🏆
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些额外的挑战🏆
- en: The best way to learn **is to try things by your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的最佳方式**是自己尝试**！正如你所看到的，当前的代理表现不佳。作为第一个建议，你可以训练更多步骤。通过100万步，我们看到了一些很好的结果！
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)中，你会找到你的代理。你能达到榜首吗？
- en: 'Here are some ideas to achieve so:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些实现的想法：
- en: Train more steps
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练更多步骤
- en: Try different hyperparameters for `PPO`. You can see them at [https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的`PPO`超参数。你可以在[https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters)看到它们。
- en: Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)
    and try another model such as DQN.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看[Stable-Baselines3文档](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)，尝试另一个模型，如DQN。
- en: '**Push your new trained model** on the Hub 🔥'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将你的新训练模型推送到Hub** 🔥'
- en: '**Compare the results of your LunarLander-v2 with your classmates** using the
    [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    🏆'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用[排行榜](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)与你的同学比较你的LunarLander-v2的结果**
    🏆'
- en: Is moon landing too boring for you? Try to **change the environment**, why not
    use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the
    gym documentation](https://www.gymlibrary.dev/) and have fun 🎉.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对你来说，登月太无聊了吗？尝试**改变环境**，为什么不使用MountainCar-v0，CartPole-v1或CarRacing-v0？查看它们的工作方式[使用gym文档](https://www.gymlibrary.dev/)并享受乐趣🎉。
- en: '* * *'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats on finishing this chapter! That was the biggest one, **and there was
    a lot of information.**
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了这一章！这是最大的一章，**包含了很多信息。**
- en: If you’re still feel confused with all these elements…it’s totally normal! **This
    was the same for me and for all people who studied RL.**
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然感到困惑，这些元素对我和所有学习RL的人来说都是正常的！
- en: Take time to really **grasp the material before continuing and try the additional
    challenges**. It’s important to master these elements and have a solid foundations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前花时间真正**掌握材料并尝试额外的挑战**。掌握这些元素并建立坚实的基础是很重要的。
- en: Naturally, during the course, we’re going to dive deeper into these concepts
    but **it’s better to have a good understanding of them now before diving into
    the next chapters.**
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在课程中，我们将更深入地探讨这些概念，但**最好现在就对它们有一个良好的理解，然后再深入下一章节。**
- en: Next time, in the bonus unit 1, you’ll train Huggy the Dog to fetch the stick.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下次，在奖励单元1中，你将训练Huggy the Dog去接棍子。
- en: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![Huggy](../Images/3fff0107ad50440533e843a81416a46f.png)'
- en: Keep learning, stay awesome 🤗
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 继续学习，保持棒棒的🤗
