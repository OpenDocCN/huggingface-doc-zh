["```py\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\ndevice = \"cuda\"\nmodel_id = \"gpt2-large\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n```", "```py\nfrom datasets import load_dataset\n\ntest = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\nencodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n```", "```py\nimport torch\nfrom tqdm import tqdm\n\nmax_length = model.config.n_positions\nstride = 512\nseq_len = encodings.input_ids.size(1)\n\nnlls = []\nprev_end_loc = 0\nfor begin_loc in tqdm(range(0, seq_len, stride)):\n    end_loc = min(begin_loc + max_length, seq_len)\n    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n    target_ids = input_ids.clone()\n    target_ids[:, :-trg_len] = -100\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n\n        # loss is calculated using CrossEntropyLoss which averages over valid labels\n        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n        # to the left by 1.\n        neg_log_likelihood = outputs.loss\n\n    nlls.append(neg_log_likelihood)\n\n    prev_end_loc = end_loc\n    if end_loc == seq_len:\n        break\n\nppl = torch.exp(torch.stack(nlls).mean())\n```"]