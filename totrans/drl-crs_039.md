# å®è·µ

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/learn/deep-rl-course/unit2/hands-on](https://huggingface.co/learn/deep-rl-course/unit2/hands-on)

[![æé—®](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord) [![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb)

ç°åœ¨æˆ‘ä»¬å·²ç»å­¦ä¹ äº†Qå­¦ä¹ ç®—æ³•ï¼Œè®©æˆ‘ä»¬ä»å¤´å¼€å§‹å®ç°å®ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªç¯å¢ƒä¸­è®­ç»ƒæˆ‘ä»¬çš„Qå­¦ä¹ ä»£ç†ï¼š

1.  [å†°æ¹–-v1ï¼ˆéæ»‘å’Œæ»‘ç‰ˆæœ¬ï¼‰](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) â˜ƒï¸ï¼šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„ä»£ç†å°†éœ€è¦**ä»èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰åˆ°ç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰**åªèƒ½åœ¨å†°å†»çš„ç“·ç –ï¼ˆFï¼‰ä¸Šè¡Œèµ°ï¼Œå¹¶é¿å¼€æ´ï¼ˆHï¼‰ã€‚

1.  [ä¸€ä¸ªè‡ªåŠ¨å‡ºç§Ÿè½¦](https://gymnasium.farama.org/environments/toy_text/taxi/) ğŸš– éœ€è¦**å­¦ä¼šå¯¼èˆª**åŸå¸‚ï¼Œ**å°†ä¹˜å®¢ä»Aç‚¹è¿é€åˆ°Bç‚¹**ã€‚

![ç¯å¢ƒ](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)

æ„Ÿè°¢[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ï¼Œæ‚¨å°†èƒ½å¤Ÿå°†æ‚¨çš„ç»“æœä¸å…¶ä»–åŒå­¦è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶äº¤æµæœ€ä½³å®è·µä»¥æé«˜æ‚¨çš„ä»£ç†åˆ†æ•°ã€‚è°å°†èµ¢å¾—ç¬¬2å•å…ƒçš„æŒ‘æˆ˜ï¼Ÿ

è¦éªŒè¯è¿™ä¸ªå®è·µå¯¹[è®¤è¯è¿‡ç¨‹](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒæœ‰ç´ çš„å‡ºç§Ÿè½¦æ¨¡å‹æ¨é€åˆ°Hubï¼Œå¹¶**è·å¾—>= 4.5çš„ç»“æœ**ã€‚

è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·è½¬åˆ°[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)å¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**

æœ‰å…³è®¤è¯è¿‡ç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)

æ‚¨å¯ä»¥åœ¨è¿™é‡Œæ£€æŸ¥æ‚¨çš„è¿›åº¦ğŸ‘‰[https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)

**è¦å¼€å§‹å®è·µï¼Œè¯·ç‚¹å‡»â€œåœ¨Colabä¸­æ‰“å¼€â€æŒ‰é’®**ğŸ‘‡ï¼š

[![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb)

æˆ‘ä»¬å¼ºçƒˆå»ºè®®å­¦ç”Ÿä½¿ç”¨Google Colabè¿›è¡Œå®è·µç»ƒä¹ ï¼Œè€Œä¸æ˜¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šè¿è¡Œå®ƒä»¬ã€‚

é€šè¿‡ä½¿ç”¨Google Colabï¼Œ**æ‚¨å¯ä»¥ä¸“æ³¨äºå­¦ä¹ å’Œå®éªŒï¼Œè€Œä¸å¿…æ‹…å¿ƒè®¾ç½®ç¯å¢ƒçš„æŠ€æœ¯ç»†èŠ‚**ã€‚

# ç¬¬2å•å…ƒï¼šä½¿ç”¨FrozenLake-v1 â›„å’ŒTaxi-v3 ğŸš•çš„Qå­¦ä¹ 

![ç¬¬2å•å…ƒç¼©ç•¥å›¾](../Images/0b9bbdbce6b297349ae6de9075b71340.png)

åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œ**æ‚¨å°†ä»å¤´å¼€å§‹ç¼–å†™æ‚¨çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†**æ¥ç©å†°æ¹– â„ï¸ ä½¿ç”¨Qå­¦ä¹ ï¼Œä¸ç¤¾åŒºåˆ†äº«ï¼Œå¹¶å°è¯•ä¸åŒçš„é…ç½®ã€‚

â¬‡ï¸ è¿™æ˜¯æ‚¨å°†åœ¨å‡ åˆ†é’Ÿå†…å®ç°çš„ç¤ºä¾‹ã€‚ â¬‡ï¸

![ç¯å¢ƒ](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)

### ğŸ® ç¯å¢ƒï¼š

+   [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

+   [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)

### ğŸ“š RLåº“ï¼š

+   Pythonå’ŒNumPy

+   [ä½“è‚²é¦†](https://gymnasium.farama.org/)

æˆ‘ä»¬ä¸æ–­åŠªåŠ›æ”¹è¿›æˆ‘ä»¬çš„æ•™ç¨‹ï¼Œæ‰€ä»¥**å¦‚æœæ‚¨åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­å‘ç°ä¸€äº›é—®é¢˜**ï¼Œè¯·[åœ¨GitHub Repoä¸Šæå‡ºé—®é¢˜](https://github.com/huggingface/deep-rl-class/issues)ã€‚

## æœ¬ç¬”è®°æœ¬çš„ç›®æ ‡ğŸ†

åœ¨ç¬”è®°æœ¬çš„æœ«å°¾ï¼Œæ‚¨å°†ï¼š

+   èƒ½å¤Ÿä½¿ç”¨**ä½“è‚²é¦†**ï¼Œè¿™ä¸ªç¯å¢ƒåº“ã€‚

+   èƒ½å¤Ÿä»å¤´å¼€å§‹ç¼–å†™Qå­¦ä¹ ä»£ç†ã€‚

+   èƒ½å¤Ÿ**å°†æ‚¨è®­ç»ƒæœ‰ç´ çš„ä»£ç†å’Œä»£ç æ¨é€åˆ°Hub**ï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ¼‚äº®çš„è§†é¢‘å›æ”¾å’Œè¯„ä¼°åˆ†æ•°ğŸ”¥ã€‚

## è¿™ä¸ªç¬”è®°æœ¬æ¥è‡ªæ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹

![Deep RLè¯¾ç¨‹æ’å›¾](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)

åœ¨è¿™é—¨å…è´¹è¯¾ç¨‹ä¸­ï¼Œæ‚¨å°†ï¼š

+   ğŸ“– å­¦ä¹ æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„**ç†è®ºå’Œå®è·µ**ã€‚

+   ğŸ§‘â€ğŸ’» å­¦ä¹ ä½¿ç”¨è‘—åçš„æ·±åº¦RLåº“ï¼Œå¦‚Stable Baselines3ã€RL Baselines3 Zooã€CleanRLå’ŒSample Factory 2.0ã€‚

+   ğŸ¤– åœ¨ç‹¬ç‰¹ç¯å¢ƒä¸­**è®­ç»ƒä»£ç†**

æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ğŸ“šè¯¾ç¨‹å¤§çº²ğŸ‘‰[https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)

ä¸è¦å¿˜è®°**[æ³¨å†Œè¯¾ç¨‹](http://eepurl.com/ic5ZUD)**ï¼ˆæˆ‘ä»¬æ­£åœ¨æ”¶é›†æ‚¨çš„ç”µå­é‚®ä»¶ï¼Œä»¥ä¾¿èƒ½å¤Ÿ**åœ¨æ¯ä¸ªå•å…ƒå‘å¸ƒæ—¶å‘æ‚¨å‘é€é“¾æ¥ï¼Œå¹¶å‘æ‚¨æä¾›æœ‰å…³æŒ‘æˆ˜å’Œæ›´æ–°çš„ä¿¡æ¯ï¼‰**ã€‚

ä¿æŒè”ç³»çš„æœ€ä½³æ–¹å¼æ˜¯åŠ å…¥æˆ‘ä»¬çš„discordæœåŠ¡å™¨ï¼Œä¸ç¤¾åŒºå’Œæˆ‘ä»¬äº¤æµğŸ‘‰ğŸ»[https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)

## å…ˆå†³æ¡ä»¶ğŸ—ï¸

åœ¨å¼€å§‹ä½¿ç”¨ç¬”è®°æœ¬ä¹‹å‰ï¼Œæ‚¨éœ€è¦ï¼š

ğŸ”² ğŸ“š **é€šè¿‡é˜…è¯»ç¬¬2å•å…ƒå­¦ä¹ [Qå­¦ä¹ ](https://huggingface.co/deep-rl-course/unit2/introduction)** ğŸ¤—

## Q-Learningçš„å°ç»“

*Qå­¦ä¹ * **æ˜¯å¼ºåŒ–å­¦ä¹ ç®—æ³•**ï¼š

+   è®­ç»ƒ*Qå‡½æ•°*ï¼Œä¸€ä¸ª**åŠ¨ä½œå€¼å‡½æ•°**ï¼Œç”±ä¸€ä¸ªåŒ…å«æ‰€æœ‰çŠ¶æ€-åŠ¨ä½œå¯¹å€¼çš„*Qè¡¨*åœ¨å†…å­˜ä¸­ç¼–ç ã€‚

+   ç»™å®šä¸€ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œï¼Œæˆ‘ä»¬çš„Qå‡½æ•°**å°†åœ¨Qè¡¨ä¸­æœç´¢ç›¸åº”çš„å€¼ã€‚**

![Qå‡½æ•°](../Images/c6f51357ba01781edc9f3041b33e5be4.png)

+   å½“è®­ç»ƒå®Œæˆæ—¶ï¼Œ**æˆ‘ä»¬æœ‰ä¸€ä¸ªæœ€ä¼˜çš„Qå‡½æ•°ï¼Œå› æ­¤æœ‰ä¸€ä¸ªæœ€ä¼˜çš„Qè¡¨ã€‚**

+   å¦‚æœæˆ‘ä»¬**æœ‰ä¸€ä¸ªæœ€ä¼˜çš„Qå‡½æ•°**ï¼Œæˆ‘ä»¬å°±æœ‰ä¸€ä¸ªæœ€ä¼˜çš„ç­–ç•¥ï¼Œå› ä¸ºæˆ‘ä»¬**çŸ¥é“å¯¹äºæ¯ä¸ªçŠ¶æ€ï¼Œè¦é‡‡å–çš„æœ€ä½³è¡ŒåŠ¨ã€‚**

![é“¾æ¥å€¼ç­–ç•¥](../Images/06e7785cc764e6109bfc6c89005a4d92.png)

ä½†æ˜¯ï¼Œåœ¨å¼€å§‹æ—¶ï¼Œ**æˆ‘ä»¬çš„Qè¡¨æ˜¯æ— ç”¨çš„ï¼Œå› ä¸ºå®ƒä¸ºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ç»™å‡ºä»»æ„å€¼ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†Qè¡¨åˆå§‹åŒ–ä¸º0å€¼ï¼‰**ã€‚ä½†æ˜¯ï¼Œéšç€æˆ‘ä»¬æ¢ç´¢ç¯å¢ƒå¹¶æ›´æ–°æˆ‘ä»¬çš„Qè¡¨ï¼Œå®ƒå°†ç»™å‡ºè¶Šæ¥è¶Šå¥½çš„è¿‘ä¼¼å€¼

![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)

è¿™æ˜¯Qå­¦ä¹ çš„ä¼ªä»£ç ï¼š

![Qå­¦ä¹ ](../Images/e98aadd735672374a66857c170d3b2ce.png)

# è®©æˆ‘ä»¬ç¼–å†™æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•ğŸš€

è¦éªŒè¯è¿™ä¸ªå®è·µé¡¹ç›®çš„[è®¤è¯æµç¨‹](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)ï¼Œæ‚¨éœ€è¦å°†è®­ç»ƒè¿‡çš„Taxiæ¨¡å‹æ¨é€åˆ°Hubï¼Œå¹¶**è·å¾—>=4.5çš„ç»“æœ**ã€‚

è¦æ‰¾åˆ°æ‚¨çš„ç»“æœï¼Œè¯·è½¬åˆ°[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)å¹¶æ‰¾åˆ°æ‚¨çš„æ¨¡å‹ï¼Œ**ç»“æœ=å¹³å‡å¥–åŠ±-å¥–åŠ±çš„æ ‡å‡†å·®**

æœ‰å…³è®¤è¯æµç¨‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤éƒ¨åˆ†ğŸ‘‰[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)

## å®‰è£…ä¾èµ–é¡¹å¹¶åˆ›å»ºè™šæ‹Ÿæ˜¾ç¤ºğŸ”½

åœ¨ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªé‡æ’­è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œåœ¨Colabä¸­ï¼Œ**æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªè™šæ‹Ÿå±å¹•æ¥æ¸²æŸ“ç¯å¢ƒ**ï¼ˆä»è€Œè®°å½•å¸§ï¼‰ã€‚

å› æ­¤ï¼Œä»¥ä¸‹å•å…ƒå°†å®‰è£…åº“å¹¶åˆ›å»ºå’Œè¿è¡Œè™šæ‹Ÿå±å¹•ğŸ–¥

æˆ‘ä»¬å°†å®‰è£…å¤šä¸ªï¼š

+   `gymnasium`ï¼šåŒ…å«FrozenLake-v1 â›„å’ŒTaxi-v3 ğŸš•ç¯å¢ƒã€‚

+   `pygame`ï¼šç”¨äºFrozenLake-v1å’ŒTaxi-v3ç”¨æˆ·ç•Œé¢ã€‚

+   `numpy`ï¼šç”¨äºå¤„ç†æˆ‘ä»¬çš„Qè¡¨ã€‚

Hugging Face Hub ğŸ¤—ä½œä¸ºä¸€ä¸ªä¸­å¿ƒåŒ–çš„åœ°æ–¹ï¼Œä»»ä½•äººéƒ½å¯ä»¥åˆ†äº«å’Œæ¢ç´¢æ¨¡å‹å’Œæ•°æ®é›†ã€‚å®ƒå…·æœ‰ç‰ˆæœ¬æ§åˆ¶ã€æŒ‡æ ‡ã€å¯è§†åŒ–å’Œå…¶ä»–åŠŸèƒ½ï¼Œè®©æ‚¨å¯ä»¥è½»æ¾ä¸ä»–äººåˆä½œã€‚

æ‚¨å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ·±åº¦RLæ¨¡å‹ï¼ˆå¦‚æœå®ƒä»¬ä½¿ç”¨Qå­¦ä¹ ï¼‰ğŸ‘‰[https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)

```py
pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt
```

```py
sudo apt-get update
sudo apt-get install -y python3-opengl
apt install ffmpeg xvfb
pip3 install pyvirtualdisplay
```

ä¸ºäº†ç¡®ä¿æ–°å®‰è£…çš„åº“è¢«ä½¿ç”¨ï¼Œ**æœ‰æ—¶éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬è¿è¡Œæ—¶**ã€‚ä¸‹ä¸€ä¸ªå•å…ƒæ ¼å°†å¼ºåˆ¶**è¿è¡Œæ—¶å´©æºƒï¼Œå› æ­¤æ‚¨éœ€è¦é‡æ–°è¿æ¥å¹¶ä»è¿™é‡Œå¼€å§‹è¿è¡Œä»£ç **ã€‚é€šè¿‡è¿™ä¸ªæŠ€å·§ï¼Œ**æˆ‘ä»¬å°†èƒ½å¤Ÿè¿è¡Œæˆ‘ä»¬çš„è™šæ‹Ÿå±å¹•**ã€‚

```py
import os

os.kill(os.getpid(), 9)
```

```py
# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()
```

## å¯¼å…¥åŒ…ğŸ“¦

é™¤äº†å®‰è£…çš„åº“ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ï¼š

+   `random`ï¼šç”Ÿæˆéšæœºæ•°ï¼ˆå¯¹epsilon-è´ªå©ªç­–ç•¥æœ‰ç”¨ï¼‰ã€‚

+   `imageio`ï¼šç”Ÿæˆé‡æ’­è§†é¢‘ã€‚

```py
import numpy as np
import gymnasium as gym
import random
import imageio
import os
import tqdm

import pickle5 as pickle
from tqdm.notebook import tqdm
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™æˆ‘ä»¬çš„Qå­¦ä¹ ç®—æ³•ğŸ”¥

# ç¬¬1éƒ¨åˆ†ï¼šå†°å†»æ¹–â›„ï¼ˆéæ»‘åŠ¨ç‰ˆæœ¬ï¼‰

## åˆ›å»ºå’Œç†è§£å†°å†»æ¹–ç¯å¢ƒâ›„ï¼ˆhttps://gymnasium.farama.org/environments/toy_text/frozen_lake/ï¼‰

* * *

ğŸ’¡ å½“æ‚¨å¼€å§‹ä½¿ç”¨ç¯å¢ƒæ—¶ï¼Œå…»æˆä¸€ä¸ªå¥½ä¹ æƒ¯æ˜¯æ£€æŸ¥å…¶æ–‡æ¡£

ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)

* * *

æˆ‘ä»¬å°†è®­ç»ƒæˆ‘ä»¬çš„Qå­¦ä¹ ä»£ç†**ä»èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰å¯¼èˆªåˆ°ç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰ï¼Œåªèƒ½åœ¨å†°å†»ç“·ç –ï¼ˆFï¼‰ä¸Šè¡Œèµ°ï¼Œé¿å…æ‰å…¥æ´ä¸­ï¼ˆHï¼‰**ã€‚

æˆ‘ä»¬å¯ä»¥æœ‰ä¸¤ç§ç¯å¢ƒå¤§å°ï¼š

+   `map_name="4x4"`ï¼šä¸€ä¸ª4x4ç½‘æ ¼ç‰ˆæœ¬

+   `map_name="8x8"`ï¼šä¸€ä¸ª8x8ç½‘æ ¼ç‰ˆæœ¬

ç¯å¢ƒæœ‰ä¸¤ç§æ¨¡å¼ï¼š

+   `is_slippery=False`ï¼šç”±äºå†°å†»æ¹–çš„éæ»‘åŠ¨æ€§è´¨ï¼Œä»£ç†å§‹ç»ˆ**æœç€é¢„æœŸæ–¹å‘ç§»åŠ¨**ã€‚

+   `is_slippery=True`ï¼šç”±äºå†°å†»æ¹–çš„æ»‘åŠ¨æ€§è´¨ï¼Œä»£ç†**å¯èƒ½ä¸æ€»æ˜¯æœç€é¢„æœŸæ–¹å‘ç§»åŠ¨**ï¼ˆéšæœºçš„ï¼‰ã€‚

ç°åœ¨è®©æˆ‘ä»¬ä¿æŒç®€å•ï¼Œä½¿ç”¨4x4åœ°å›¾å’Œéæ»‘åŠ¨ã€‚æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªåä¸º`render_mode`çš„å‚æ•°ï¼ŒæŒ‡å®šç¯å¢ƒåº”å¦‚ä½•å¯è§†åŒ–ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå› ä¸ºæˆ‘ä»¬**å¸Œæœ›åœ¨æœ€åè®°å½•ç¯å¢ƒçš„è§†é¢‘ï¼Œæˆ‘ä»¬éœ€è¦å°†render_modeè®¾ç½®ä¸ºrgb_array**ã€‚

å¦‚[æ–‡æ¡£ä¸­æ‰€è¿°](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)ï¼Œâ€œrgb_arrayâ€ï¼šè¿”å›ä»£è¡¨ç¯å¢ƒå½“å‰çŠ¶æ€çš„å•ä¸ªå¸§ã€‚ä¸€ä¸ªå¸§æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆxï¼Œyï¼Œ3ï¼‰çš„np.ndarrayï¼Œè¡¨ç¤ºxä¹˜yåƒç´ å›¾åƒçš„RGBå€¼ã€‚

```py
# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode="rgb_array"
env = gym.make()  # TODO use the correct parameters
```

### è§£å†³æ–¹æ¡ˆ

```py
env = gym.make("FrozenLake-v1", map_name="4x4", is_slippery=False, render_mode="rgb_array")
```

æ‚¨å¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ç½‘æ ¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
desc=["SFFF", "FHFH", "FFFH", "HFFG"]
gym.make('FrozenLake-v1', desc=desc, is_slippery=True)
```

ä½†æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨é»˜è®¤ç¯å¢ƒã€‚

### è®©æˆ‘ä»¬çœ‹çœ‹ç¯å¢ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ï¼š

```py
# We create our environment with gym.make("<name_of_the_environment>")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).
print("_____OBSERVATION SPACE_____ \n")
print("Observation Space", env.observation_space)
print("Sample observation", env.observation_space.sample())  # Get a random observation
```

æˆ‘ä»¬çœ‹åˆ°`Observation Space Shape Discrete(16)`ï¼Œè§‚å¯Ÿæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤º**ä»£ç†çš„å½“å‰ä½ç½®ä¸ºcurrent_row * ncols + current_colï¼ˆå…¶ä¸­è¡Œå’Œåˆ—éƒ½ä»0å¼€å§‹ï¼‰**ã€‚

ä¾‹å¦‚ï¼Œ4x4åœ°å›¾ä¸­çš„ç›®æ ‡ä½ç½®å¯ä»¥è®¡ç®—å¦‚ä¸‹ï¼š3 * 4 + 3 = 15ã€‚å¯èƒ½çš„è§‚å¯Ÿæ¬¡æ•°å–å†³äºåœ°å›¾çš„å¤§å°ã€‚**ä¾‹å¦‚ï¼Œ4x4åœ°å›¾æœ‰16ç§å¯èƒ½çš„è§‚å¯Ÿã€‚**

ä¾‹å¦‚ï¼Œè¿™å°±æ˜¯çŠ¶æ€=0çš„æ ·å­ï¼š

![FrozenLake](../Images/5d72cc95a82a64293bb4212444acec50.png)

```py
print("\n _____ACTION SPACE_____ \n")
print("Action Space Shape", env.action_space.n)
print("Action Space Sample", env.action_space.sample())  # Take a random action
```

åŠ¨ä½œç©ºé—´ï¼ˆä»£ç†å¯ä»¥é‡‡å–çš„å¯èƒ½åŠ¨ä½œé›†ï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰4ä¸ªå¯ç”¨åŠ¨ä½œğŸ®ï¼š

+   0ï¼šå‘å·¦ç§»åŠ¨

+   1ï¼šå‘ä¸‹ç§»åŠ¨

+   2ï¼šå‘å³ç§»åŠ¨

+   3ï¼šå‘ä¸Šç§»åŠ¨

å¥–åŠ±å‡½æ•°ğŸ’°ï¼š

+   è¾¾åˆ°ç›®æ ‡ï¼š+1

+   åˆ°è¾¾æ´ï¼š0

+   åˆ°è¾¾å†°å†»ï¼š0

## åˆ›å»ºå’Œåˆå§‹åŒ–Qè¡¨ğŸ—„ï¸

ï¼ˆğŸ‘€ ä¼ªä»£ç çš„ç¬¬1æ­¥ï¼‰

![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)

ç°åœ¨æ˜¯åˆå§‹åŒ–æˆ‘ä»¬çš„Qè¡¨çš„æ—¶å€™äº†ï¼è¦çŸ¥é“è¦ä½¿ç”¨å¤šå°‘è¡Œï¼ˆçŠ¶æ€ï¼‰å’Œåˆ—ï¼ˆåŠ¨ä½œï¼‰ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´ã€‚æˆ‘ä»¬å·²ç»çŸ¥é“å®ƒä»¬çš„å€¼äº†ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä»¥ç¼–ç¨‹æ–¹å¼è·å–å®ƒä»¬ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æ¨å¹¿åˆ°ä¸åŒçš„ç¯å¢ƒã€‚Gymä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æ–¹æ³•ï¼š`env.action_space.n`å’Œ`env.observation_space.n`

```py
state_space =
print("There are ", state_space, " possible states")

action_space =
print("There are ", action_space, " possible actions")
```

```py
# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)
def initialize_q_table(state_space, action_space):
  Qtable =
  return Qtable
```

```py
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

### è§£å†³æ–¹æ¡ˆ

```py
state_space = env.observation_space.n
print("There are ", state_space, " possible states")

action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

```py
# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros
def initialize_q_table(state_space, action_space):
    Qtable = np.zeros((state_space, action_space))
    return Qtable
```

```py
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

## å®šä¹‰è´ªå©ªç­–ç•¥ğŸ¤–

è¯·è®°ä½ï¼Œç”±äºQå­¦ä¹ æ˜¯ä¸€ç§**ç¦»ç­–ç•¥**ç®—æ³•ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§ç­–ç•¥ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨**è¡ŒåŠ¨å’Œæ›´æ–°å€¼å‡½æ•°æ—¶ä½¿ç”¨ä¸åŒçš„ç­–ç•¥**ã€‚

+   Epsilon-è´ªå©ªç­–ç•¥ï¼ˆè¡ŒåŠ¨ç­–ç•¥ï¼‰

+   è´ªå©ªç­–ç•¥ï¼ˆæ›´æ–°ç­–ç•¥ï¼‰

è´ªå©ªç­–ç•¥ä¹Ÿå°†æ˜¯Qå­¦ä¹ ä»£ç†å®Œæˆè®­ç»ƒåçš„æœ€ç»ˆç­–ç•¥ã€‚è´ªå©ªç­–ç•¥ç”¨äºä½¿ç”¨Qè¡¨é€‰æ‹©åŠ¨ä½œã€‚

![Q-Learning](../Images/ce691ce98ae89b58669eb975be3f446c.png)

```py
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action =

  return action
```

#### è§£å†³æ–¹æ¡ˆ

```py
def greedy_policy(Qtable, state):
    # Exploitation: take the action with the highest state, action value
    action = np.argmax(Qtable[state][:])

    return action
```

## å®šä¹‰epsilon-greedyç­–ç•¥ ğŸ¤–

Epsilon-greedyæ˜¯å¤„ç†æ¢ç´¢/åˆ©ç”¨æƒè¡¡çš„è®­ç»ƒç­–ç•¥ã€‚

ä½¿ç”¨epsilon-greedyçš„æƒ³æ³•ï¼š

+   ä»¥*æ¦‚ç‡1-É›*ï¼š**æˆ‘ä»¬è¿›è¡Œåˆ©ç”¨**ï¼ˆå³æˆ‘ä»¬çš„ä»£ç†é€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œå¯¹å€¼çš„åŠ¨ä½œï¼‰ã€‚

+   ä»¥*æ¦‚ç‡É›*ï¼šæˆ‘ä»¬è¿›è¡Œ**æ¢ç´¢**ï¼ˆå°è¯•éšæœºåŠ¨ä½œï¼‰ã€‚

éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬é€æ¸**å‡å°‘epsilonå€¼ï¼Œå› ä¸ºæˆ‘ä»¬å°†éœ€è¦è¶Šæ¥è¶Šå°‘çš„æ¢ç´¢å’Œæ›´å¤šçš„åˆ©ç”¨**ã€‚

![Q-Learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)

```py
def epsilon_greedy_policy(Qtable, state, epsilon):
  # Randomly generate a number between 0 and 1
  random_num =
  # if random_num > greater than epsilon --> exploitation
  if random_num > epsilon:
    # Take the action with the highest value given a state
    # np.argmax can be useful here
    action =
  # else --> exploration
  else:
    action = # Take a random action

  return action
```

#### è§£å†³æ–¹æ¡ˆ

```py
def epsilon_greedy_policy(Qtable, state, epsilon):
    # Randomly generate a number between 0 and 1
    random_num = random.uniform(0, 1)
    # if random_num > greater than epsilon --> exploitation
    if random_num > epsilon:
        # Take the action with the highest value given a state
        # np.argmax can be useful here
        action = greedy_policy(Qtable, state)
    # else --> exploration
    else:
        action = env.action_space.sample()

    return action
```

## å®šä¹‰è¶…å‚æ•° âš™ï¸

ä¸æ¢ç´¢ç›¸å…³çš„è¶…å‚æ•°æ˜¯æœ€é‡è¦çš„ä¹‹ä¸€ã€‚

+   æˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„ä»£ç†**æ¢ç´¢è¶³å¤Ÿçš„çŠ¶æ€ç©ºé—´**ä»¥å­¦ä¹ ä¸€ä¸ªè‰¯å¥½çš„å€¼è¿‘ä¼¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€æ¸å‡å°‘epsilonçš„è¡°å‡ã€‚

+   å¦‚æœæ‚¨å°†epsilonå‡å°‘å¾—å¤ªå¿«ï¼ˆè¡°å‡ç‡å¤ªé«˜ï¼‰ï¼Œ**æ‚¨ä¼šå†’ç€ä»£ç†è¢«å¡ä½çš„é£é™©**ï¼Œå› ä¸ºæ‚¨çš„ä»£ç†æ²¡æœ‰æ¢ç´¢è¶³å¤Ÿçš„çŠ¶æ€ç©ºé—´ï¼Œå› æ­¤æ— æ³•è§£å†³é—®é¢˜ã€‚

```py
# Training parameters
n_training_episodes = 10000  # Total training episodes
learning_rate = 0.7  # Learning rate

# Evaluation parameters
n_eval_episodes = 100  # Total number of test episodes

# Environment parameters
env_id = "FrozenLake-v1"  # Name of the environment
max_steps = 99  # Max steps per episode
gamma = 0.95  # Discounting rate
eval_seed = []  # The evaluation seed of the environment

# Exploration parameters
max_epsilon = 1.0  # Exploration probability at start
min_epsilon = 0.05  # Minimum exploration probability
decay_rate = 0.0005  # Exponential decay rate for exploration prob
```

## åˆ›å»ºè®­ç»ƒå¾ªç¯æ–¹æ³•

![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)

è®­ç»ƒå¾ªç¯å¦‚ä¸‹ï¼š

```py
For episode in the total of training episodes:

Reduce epsilon (since we need less and less exploration)
Reset the environment

  For step in max timesteps:
    Choose the action At using epsilon greedy policy
    Take the action (a) and observe the outcome state(s') and reward (r)
    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
    If done, finish the episode
    Our next state is the new state
```

```py
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
  for episode in tqdm(range(n_training_episodes)):
    # Reduce epsilon (because we need less and less exploration)
    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)
    # Reset the environment
    state, info = env.reset()
    step = 0
    terminated = False
    truncated = False

    # repeat
    for step in range(max_steps):
      # Choose the action At using epsilon greedy policy
      action =

      # Take action At and observe Rt+1 and St+1
      # Take the action (a) and observe the outcome state(s') and reward (r)
      new_state, reward, terminated, truncated, info =

      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
      Qtable[state][action] =

      # If terminated or truncated finish the episode
      if terminated or truncated:
        break

      # Our next state is the new state
      state = new_state
  return Qtable
```

#### è§£å†³æ–¹æ¡ˆ

```py
def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):
    for episode in tqdm(range(n_training_episodes)):
        # Reduce epsilon (because we need less and less exploration)
        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)
        # Reset the environment
        state, info = env.reset()
        step = 0
        terminated = False
        truncated = False

        # repeat
        for step in range(max_steps):
            # Choose the action At using epsilon greedy policy
            action = epsilon_greedy_policy(Qtable, state, epsilon)

            # Take action At and observe Rt+1 and St+1
            # Take the action (a) and observe the outcome state(s') and reward (r)
            new_state, reward, terminated, truncated, info = env.step(action)

            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
            Qtable[state][action] = Qtable[state][action] + learning_rate * (
                reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action]
            )

            # If terminated or truncated finish the episode
            if terminated or truncated:
                break

            # Our next state is the new state
            state = new_state
    return Qtable
```

## è®­ç»ƒQ-Learningä»£ç† ğŸƒ

```py
Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)
```

## è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„Q-Learningè¡¨ç°å¦‚ä½• ğŸ‘€

```py
Qtable_frozenlake
```

## è¯„ä¼°æ–¹æ³• ğŸ“

+   æˆ‘ä»¬å®šä¹‰äº†è¦ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•æ¥æµ‹è¯•æˆ‘ä»¬çš„Q-Learningä»£ç†ã€‚

```py
def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):
    """
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param Q: The Q-table
    :param seed: The evaluation seed array (for taxi-v3)
    """
    episode_rewards = []
    for episode in tqdm(range(n_eval_episodes)):
        if seed:
            state, info = env.reset(seed=seed[episode])
        else:
            state, info = env.reset()
        step = 0
        truncated = False
        terminated = False
        total_rewards_ep = 0

        for step in range(max_steps):
            # Take the action (index) that have the maximum expected future reward given that state
            action = greedy_policy(Q, state)
            new_state, reward, terminated, truncated, info = env.step(action)
            total_rewards_ep += reward

            if terminated or truncated:
                break
            state = new_state
        episode_rewards.append(total_rewards_ep)
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)

    return mean_reward, std_reward
```

## è¯„ä¼°æˆ‘ä»¬çš„Q-Learningä»£ç† ğŸ“ˆ

+   é€šå¸¸ï¼Œæ‚¨åº”è¯¥æœ‰ä¸€ä¸ªå¹³å‡å¥–åŠ±ä¸º1.0

+   ç”±äºçŠ¶æ€ç©ºé—´éå¸¸å°ï¼ˆ16ï¼‰ï¼Œ**ç¯å¢ƒç›¸å¯¹å®¹æ˜“**ã€‚æ‚¨å¯ä»¥å°è¯•[ç”¨æ»‘åŠ¨ç‰ˆæœ¬æ›¿æ¢å®ƒ](https://www.gymlibrary.dev/environments/toy_text/frozen_lake)ï¼Œè¿™ä¼šå¼•å…¥éšæœºæ€§ï¼Œä½¿ç¯å¢ƒæ›´åŠ å¤æ‚ã€‚

```py
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}")
```

## å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ”¥

ç°åœ¨æˆ‘ä»¬åœ¨è®­ç»ƒåçœ‹åˆ°äº†è‰¯å¥½çš„ç»“æœï¼Œ**æˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ¤—**ã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ªæ¨¡å‹å¡çš„ç¤ºä¾‹ï¼š

![æ¨¡å‹å¡](../Images/9832a45306ed2f863e30acb21be3060d.png)

åœ¨å¹•åï¼ŒHubä½¿ç”¨åŸºäºgitçš„å­˜å‚¨åº“ï¼ˆå¦‚æœæ‚¨ä¸çŸ¥é“gitæ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥éšç€å®éªŒå’Œæ”¹è¿›ä»£ç†æ›´æ–°æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚

#### ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 

```py
from huggingface_hub import HfApi, snapshot_download
from huggingface_hub.repocard import metadata_eval_result, metadata_save

from pathlib import Path
import datetime
import json
```

```py
def record_video(env, Qtable, out_directory, fps=1):
    """
    Generate a replay video of the agent
    :param env
    :param Qtable: Qtable of our agent
    :param out_directory
    :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)
    """
    images = []
    terminated = False
    truncated = False
    state, info = env.reset(seed=random.randint(0, 500))
    img = env.render()
    images.append(img)
    while not terminated or truncated:
        # Take the action (index) that have the maximum expected future reward given that state
        action = np.argmax(Qtable[state][:])
        state, reward, terminated, truncated, info = env.step(
            action
        )  # We directly put next_state = state for recording logic
        img = env.render()
        images.append(img)
    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)
```

```py
def push_to_hub(repo_id, model, env, video_fps=1, local_repo_path="hub"):
    """
    Evaluate, Generate a video and Upload a model to Hugging Face Hub.
    This method does the complete pipeline:
    - It evaluates the model
    - It generates the model card
    - It generates a replay video of the agent
    - It pushes everything to the Hub

    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub
    :param env
    :param video_fps: how many frame per seconds to record our video replay
    (with taxi-v3 and frozenlake-v1 we use 1)
    :param local_repo_path: where the local repository is
    """
    _, repo_name = repo_id.split("/")

    eval_env = env
    api = HfApi()

    # Step 1: Create the repo
    repo_url = api.create_repo(
        repo_id=repo_id,
        exist_ok=True,
    )

    # Step 2: Download files
    repo_local_path = Path(snapshot_download(repo_id=repo_id))

    # Step 3: Save the model
    if env.spec.kwargs.get("map_name"):
        model["map_name"] = env.spec.kwargs.get("map_name")
        if env.spec.kwargs.get("is_slippery", "") == False:
            model["slippery"] = False

    # Pickle the model
    with open((repo_local_path) / "q-learning.pkl", "wb") as f:
        pickle.dump(model, f)

    # Step 4: Evaluate the model and build JSON with evaluation metrics
    mean_reward, std_reward = evaluate_agent(
        eval_env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"]
    )

    evaluate_data = {
        "env_id": model["env_id"],
        "mean_reward": mean_reward,
        "n_eval_episodes": model["n_eval_episodes"],
        "eval_datetime": datetime.datetime.now().isoformat(),
    }

    # Write a JSON file called "results.json" that will contain the
    # evaluation results
    with open(repo_local_path / "results.json", "w") as outfile:
        json.dump(evaluate_data, outfile)

    # Step 5: Create the model card
    env_name = model["env_id"]
    if env.spec.kwargs.get("map_name"):
        env_name += "-" + env.spec.kwargs.get("map_name")

    if env.spec.kwargs.get("is_slippery", "") == False:
        env_name += "-" + "no_slippery"

    metadata = {}
    metadata["tags"] = [env_name, "q-learning", "reinforcement-learning", "custom-implementation"]

    # Add metrics
    eval = metadata_eval_result(
        model_pretty_name=repo_name,
        task_pretty_name="reinforcement-learning",
        task_id="reinforcement-learning",
        metrics_pretty_name="mean_reward",
        metrics_id="mean_reward",
        metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
        dataset_pretty_name=env_name,
        dataset_id=env_name,
    )

    # Merges both dictionaries
    metadata = {**metadata, **eval}

    model_card = f"""
  # **Q-Learning** Agent playing1 **{env_id}**
  This is a trained model of a **Q-Learning** agent playing **{env_id}** .

  ## Usage

  model = load_from_hub(repo_id="{repo_id}", filename="q-learning.pkl")

  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)
  env = gym.make(model["env_id"])
  """

    evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])

    readme_path = repo_local_path / "README.md"
    readme = ""
    print(readme_path.exists())
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
            readme = f.read()
    else:
        readme = model_card

    with readme_path.open("w", encoding="utf-8") as f:
        f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

    # Step 6: Record a video
    video_path = repo_local_path / "replay.mp4"
    record_video(env, model["qtable"], video_path, video_fps)

    # Step 7\. Push everything to the Hub
    api.upload_folder(
        repo_id=repo_id,
        folder_path=repo_local_path,
        path_in_repo=".",
    )

    print("Your model is pushed to the Hub. You can view your model here: ", repo_url)
```

### ã€‚

é€šè¿‡ä½¿ç”¨`push_to_hub` **æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ”¾ã€ç”Ÿæˆæ‚¨çš„ä»£ç†çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚

è¿™æ ·ï¼š

+   æ‚¨å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥

+   æ‚¨å¯ä»¥**å¯è§†åŒ–æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ** ğŸ‘€

+   æ‚¨å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«ä¸€ä¸ªå…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†** ğŸ’¾

+   æ‚¨å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹æ‚¨çš„ä»£ç†ä¸åŒå­¦ç›¸æ¯”çš„è¡¨ç°å¦‚ä½•** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)

ä¸ºäº†èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¿˜æœ‰ä¸‰ä¸ªæ­¥éª¤è¦éµå¾ªï¼š

1ï¸âƒ£ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ªHFè´¦æˆ· â¡ [https://huggingface.co/join](https://huggingface.co/join)

2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦ä»Hugging Faceç½‘ç«™å­˜å‚¨æ‚¨çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚

+   åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œï¼ˆ[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)ï¼‰**å…·æœ‰å†™å…¥æƒé™**

![åˆ›å»ºHFä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)

```py
from huggingface_hub import notebook_login

notebook_login()
```

å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰

3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨`push_to_hub()`å‡½æ•°å°†è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥

+   è®©æˆ‘ä»¬åˆ›å»º**åŒ…å«è¶…å‚æ•°å’ŒQè¡¨çš„æ¨¡å‹å­—å…¸**ã€‚

```py
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,
    "learning_rate": learning_rate,
    "gamma": gamma,
    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,
    "qtable": Qtable_frozenlake,
}
```

è®©æˆ‘ä»¬å¡«å†™`push_to_hub`å‡½æ•°ï¼š

+   `repo_id`ï¼šå°†è¦åˆ›å»º/æ›´æ–°çš„Hugging Face Hubå­˜å‚¨åº“çš„åç§°ï¼ˆ`repo_id = {username}/{repo_name}`ï¼‰ğŸ’¡ ä¸€ä¸ªå¥½çš„`repo_id`æ˜¯`{username}/q-{env_id}`

+   `model`ï¼šåŒ…å«è¶…å‚æ•°å’ŒQè¡¨çš„æ¨¡å‹å­—å…¸ã€‚

+   `env`ï¼šç¯å¢ƒã€‚

+   `commit_message`ï¼šæäº¤çš„æ¶ˆæ¯

```py
model
```

```py
username = ""  # FILL THIS
repo_name = "q-FrozenLake-v1-4x4-noSlippery"
push_to_hub(repo_id=f"{username}/{repo_name}", model=model, env=env)
```

æ­å–œğŸ¥³ï¼Œä½ åˆšåˆšä»å¤´å¼€å§‹å®ç°ã€è®­ç»ƒå’Œä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚FrozenLake-v1 no_slipperyæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¯å¢ƒï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´éš¾çš„ğŸ”¥ã€‚

# ç¬¬2éƒ¨åˆ†ï¼šTaxi-v3 ğŸš–

## åˆ›å»ºå¹¶ç†è§£Taxi-v3 ğŸš•

* * *

ğŸ’¡ å½“ä½ å¼€å§‹ä½¿ç”¨ä¸€ä¸ªç¯å¢ƒæ—¶ï¼Œå…»æˆä¸€ä¸ªå¥½ä¹ æƒ¯æ˜¯æŸ¥çœ‹å…¶æ–‡æ¡£

ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)

* * *

åœ¨`Taxi-v3` ğŸš•ä¸­ï¼Œç½‘æ ¼ä¸–ç•Œä¸­æœ‰å››ä¸ªæŒ‡å®šä½ç½®ï¼Œåˆ†åˆ«ç”¨Rï¼ˆçº¢è‰²ï¼‰ã€Gï¼ˆç»¿è‰²ï¼‰ã€Yï¼ˆé»„è‰²ï¼‰å’ŒBï¼ˆè“è‰²ï¼‰è¡¨ç¤ºã€‚

å½“å‰§é›†å¼€å§‹æ—¶ï¼Œ**å‡ºç§Ÿè½¦ä»éšæœºæ–¹æ ¼å¼€å§‹**ï¼Œä¹˜å®¢åœ¨éšæœºä½ç½®ã€‚å‡ºç§Ÿè½¦é©¶å‘ä¹˜å®¢çš„ä½ç½®ï¼Œ**æ¥è½½ä¹˜å®¢**ï¼Œé©¶å‘ä¹˜å®¢çš„ç›®çš„åœ°ï¼ˆå¦å¤–å››ä¸ªæŒ‡å®šä½ç½®ä¹‹ä¸€ï¼‰ï¼Œç„¶å**æ”¾ä¸‹ä¹˜å®¢**ã€‚ä¸€æ—¦ä¹˜å®¢ä¸‹è½¦ï¼Œå‰§é›†ç»“æŸã€‚

![å‡ºç§Ÿè½¦](../Images/dcffe584c3d93a6b2668209fe5b78373.png)

```py
env = gym.make("Taxi-v3", render_mode="rgb_array")
```

æœ‰**500ä¸ªç¦»æ•£çŠ¶æ€ï¼Œå› ä¸ºæœ‰25ä¸ªå‡ºç§Ÿè½¦ä½ç½®ï¼Œ5ä¸ªä¹˜å®¢å¯èƒ½çš„ä½ç½®**ï¼ˆåŒ…æ‹¬ä¹˜å®¢åœ¨å‡ºç§Ÿè½¦ä¸Šçš„æƒ…å†µï¼‰ï¼Œä»¥åŠ**4ä¸ªç›®çš„åœ°ä½ç½®**ã€‚

```py
state_space = env.observation_space.n
print("There are ", state_space, " possible states")
```

```py
action_space = env.action_space.n
print("There are ", action_space, " possible actions")
```

åŠ¨ä½œç©ºé—´ï¼ˆä»£ç†å¯ä»¥é‡‡å–çš„å¯èƒ½åŠ¨ä½œé›†ï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰**6ä¸ªå¯ç”¨åŠ¨ä½œğŸ®**ï¼š

+   0ï¼šå‘å—ç§»åŠ¨

+   1ï¼šå‘åŒ—ç§»åŠ¨

+   2ï¼šå‘ä¸œç§»åŠ¨

+   3ï¼šå‘è¥¿ç§»åŠ¨

+   4ï¼šæ¥è½½ä¹˜å®¢

+   5ï¼šæ”¾ä¸‹ä¹˜å®¢

å¥–åŠ±å‡½æ•°ğŸ’°ï¼š

+   æ¯æ­¥-1ï¼Œé™¤éè§¦å‘å…¶ä»–å¥–åŠ±ã€‚

+   +20 é€è¾¾ä¹˜å®¢ã€‚

+   -10æ‰§è¡Œâ€œæ¥è½½â€å’Œâ€œæ”¾ä¸‹â€åŠ¨ä½œéæ³•ã€‚

```py
# Create our Q table with state_size rows and action_size columns (500x6)
Qtable_taxi = initialize_q_table(state_space, action_space)
print(Qtable_taxi)
print("Q-table shape: ", Qtable_taxi.shape)
```

## å®šä¹‰è¶…å‚æ•°âš™ï¸

âš  ä¸è¦ä¿®æ”¹EVAL_SEEDï¼ševal_seedæ•°ç»„**å…è®¸æˆ‘ä»¬ä¸ºæ¯ä¸ªåŒå­¦è¯„ä¼°æ‚¨çš„ä»£ç†ï¼Œä½¿å‡ºç§Ÿè½¦çš„èµ·å§‹ä½ç½®ç›¸åŒ**

```py
# Training parameters
n_training_episodes = 25000  # Total training episodes
learning_rate = 0.7  # Learning rate

# Evaluation parameters
n_eval_episodes = 100  # Total number of test episodes

# DO NOT MODIFY EVAL_SEED
eval_seed = [
    16,
    54,
    165,
    177,
    191,
    191,
    120,
    80,
    149,
    178,
    48,
    38,
    6,
    125,
    174,
    73,
    50,
    172,
    100,
    148,
    146,
    6,
    25,
    40,
    68,
    148,
    49,
    167,
    9,
    97,
    164,
    176,
    61,
    7,
    54,
    55,
    161,
    131,
    184,
    51,
    170,
    12,
    120,
    113,
    95,
    126,
    51,
    98,
    36,
    135,
    54,
    82,
    45,
    95,
    89,
    59,
    95,
    124,
    9,
    113,
    58,
    85,
    51,
    134,
    121,
    169,
    105,
    21,
    30,
    11,
    50,
    65,
    12,
    43,
    82,
    145,
    152,
    97,
    106,
    55,
    31,
    85,
    38,
    112,
    102,
    168,
    123,
    97,
    21,
    83,
    158,
    26,
    80,
    63,
    5,
    81,
    32,
    11,
    28,
    148,
]  # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position
# Each seed has a specific starting state

# Environment parameters
env_id = "Taxi-v3"  # Name of the environment
max_steps = 99  # Max steps per episode
gamma = 0.95  # Discounting rate

# Exploration parameters
max_epsilon = 1.0  # Exploration probability at start
min_epsilon = 0.05  # Minimum exploration probability
decay_rate = 0.005  # Exponential decay rate for exploration prob
```

## è®­ç»ƒæˆ‘ä»¬çš„Qå­¦ä¹ ä»£ç†ğŸƒ

```py
Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)
Qtable_taxi
```

## åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ğŸ’¾å¹¶å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°HubğŸ”¥

+   æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°ä»¥å®ç°å¯é‡ç°æ€§å’ŒQè¡¨ã€‚

```py
model = {
    "env_id": env_id,
    "max_steps": max_steps,
    "n_training_episodes": n_training_episodes,
    "n_eval_episodes": n_eval_episodes,
    "eval_seed": eval_seed,
    "learning_rate": learning_rate,
    "gamma": gamma,
    "max_epsilon": max_epsilon,
    "min_epsilon": min_epsilon,
    "decay_rate": decay_rate,
    "qtable": Qtable_taxi,
}
```

```py
username = ""  # FILL THIS
repo_name = ""  # FILL THIS
push_to_hub(repo_id=f"{username}/{repo_name}", model=model, env=env)
```

ç°åœ¨å®ƒåœ¨Hubä¸Šï¼Œä½ å¯ä»¥é€šè¿‡æ’è¡Œæ¦œä¸åŒå­¦ä»¬æ¯”è¾ƒä½ çš„Taxi-v3çš„ç»“æœğŸ†ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)

![å‡ºç§Ÿè½¦æ’è¡Œæ¦œ](../Images/0d0ce61b026fca34f96441a33217667d.png)

# ç¬¬3éƒ¨åˆ†ï¼šä»HubåŠ è½½ğŸ”½

Hugging Face Hub ğŸ¤— ä»¤äººæƒŠå¹çš„åœ°æ–¹åœ¨äºä½ å¯ä»¥è½»æ¾åœ°åŠ è½½ç¤¾åŒºä¸­å¼ºå¤§çš„æ¨¡å‹ã€‚

ä»HubåŠ è½½å·²ä¿å­˜çš„æ¨¡å‹éå¸¸å®¹æ˜“ï¼š

1.  ä½ å¯ä»¥å‰å¾€[https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)æŸ¥çœ‹æ‰€æœ‰ä¿å­˜çš„q-learningæ¨¡å‹åˆ—è¡¨ã€‚

1.  ä½ é€‰æ‹©ä¸€ä¸ªå¹¶å¤åˆ¶å…¶repo_id

![å¤åˆ¶id](../Images/38b2f545a7fd317518e1c20d339f44d7.png)

1.  ç„¶åæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨`load_from_hub`ï¼š

+   repo_id

+   æ–‡ä»¶åï¼šå­˜å‚¨åœ¨å­˜å‚¨åº“ä¸­çš„å·²ä¿å­˜æ¨¡å‹ã€‚

#### ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 

```py
from urllib.error import HTTPError

from huggingface_hub import hf_hub_download

def load_from_hub(repo_id: str, filename: str) -> str:
    """
    Download a model from Hugging Face Hub.
    :param repo_id: id of the model repository from the Hugging Face Hub
    :param filename: name of the model zip file from the repository
    """
    # Get the model from the Hub, download and cache the model on your local disk
    pickle_model = hf_hub_download(repo_id=repo_id, filename=filename)

    with open(pickle_model, "rb") as f:
        downloaded_model_file = pickle.load(f)

    return downloaded_model_file
```

### ã€‚

```py
model = load_from_hub(repo_id="ThomasSimonini/q-Taxi-v3", filename="q-learning.pkl")  # Try to use another model

print(model)
env = gym.make(model["env_id"])

evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
```

```py
model = load_from_hub(
    repo_id="ThomasSimonini/q-FrozenLake-v1-no-slippery", filename="q-learning.pkl"
)  # Try to use another model

env = gym.make(model["env_id"], is_slippery=False)

evaluate_agent(env, model["max_steps"], model["n_eval_episodes"], model["qtable"], model["eval_seed"])
```

## ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ğŸ†

å­¦ä¹ çš„æœ€ä½³æ–¹å¼æ˜¯**å°è¯•è‡ªå·±åŠ¨æ‰‹**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œä½ å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚è¿›è¡Œ100ä¸‡æ­¥è®­ç»ƒåï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›å¾ˆå¥½çš„ç»“æœï¼

åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°ä½ çš„ä»£ç†ã€‚ä½ èƒ½ç™»é¡¶å—ï¼Ÿ

ä»¥ä¸‹æ˜¯ä¸€äº›æå‡æ’è¡Œæ¦œçš„æƒ³æ³•ï¼š

+   è®­ç»ƒæ›´å¤šæ­¥éª¤

+   é€šè¿‡æŸ¥çœ‹åŒå­¦ä»¬çš„å·¥ä½œï¼Œå°è¯•ä¸åŒçš„è¶…å‚æ•°ã€‚

+   **å°†ä½ çš„æ–°è®­ç»ƒæ¨¡å‹æ¨é€**åˆ°HubğŸ”¥

åœ¨å†°ä¸Šè¡Œèµ°å’Œå¼€å‡ºç§Ÿè½¦å¯¹ä½ æ¥è¯´å¤ªæ— èŠäº†å—ï¼Ÿå°è¯•**æ”¹å˜ç¯å¢ƒ**ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨FrozenLake-v1æ»‘åŠ¨ç‰ˆæœ¬ï¼ŸæŸ¥çœ‹å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„[ä½¿ç”¨gymnasiumæ–‡æ¡£](https://gymnasium.farama.org/)å¹¶ç©å¾—å¼€å¿ƒğŸ‰ã€‚

* * *

æ­å–œğŸ¥³ï¼Œä½ åˆšåˆšå®ç°ã€è®­ç»ƒå’Œä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚

ç†è§£Qå­¦ä¹ æ˜¯ç†è§£åŸºäºä»·å€¼çš„æ–¹æ³•çš„é‡è¦ä¸€æ­¥ã€‚

åœ¨ä¸‹ä¸€ä¸ªä½¿ç”¨æ·±åº¦ Q å­¦ä¹ çš„å•å…ƒä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°åˆ›å»ºå’Œæ›´æ–° Q è¡¨æ˜¯ä¸€ä¸ªä¸é”™çš„ç­–ç•¥ï¼Œ**ä½†æ˜¯ï¼Œè¿™å¹¶ä¸å…·æœ‰å¯æ‰©å±•æ€§ã€‚**

ä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸‹ä½ åˆ›å»ºäº†ä¸€ä¸ªå­¦ä¹ ç©æ¯ç­çš„ä»£ç†ç¨‹åºã€‚

![æ¯ç­](../Images/5692612f8572824879d3b76eb2ec21b4.png)

æ¯ç­æ˜¯ä¸€ä¸ªåºå¤§çš„ç¯å¢ƒï¼Œå…·æœ‰å·¨å¤§çš„çŠ¶æ€ç©ºé—´ï¼ˆæ•°ç™¾ä¸‡ä¸ªä¸åŒçš„çŠ¶æ€ï¼‰ã€‚ä¸ºè¯¥ç¯å¢ƒåˆ›å»ºå’Œæ›´æ–° Q è¡¨å°†ä¸é«˜æ•ˆã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­å­¦ä¹ æ·±åº¦ Q å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§ç®—æ³•ï¼Œ**æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ï¼Œç»™å®šä¸€ä¸ªçŠ¶æ€ï¼Œæ¯ä¸ªåŠ¨ä½œçš„ä¸åŒ Q å€¼ã€‚**

![ç¯å¢ƒ](../Images/bf441b005cda192d0dc86eb42475aeb3.png)

åœ¨ç¬¬ä¸‰å•å…ƒè§ï¼ğŸ”¥

## ç»§ç»­å­¦ä¹ ï¼Œä¿æŒæ£’æ£’çš„ ğŸ¤—
