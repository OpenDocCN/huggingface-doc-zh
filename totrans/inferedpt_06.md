# 自动缩放

> 原文链接：[https://huggingface.co/docs/inference-endpoints/autoscaling](https://huggingface.co/docs/inference-endpoints/autoscaling)

自动缩放允许您根据流量和加速器利用率动态调整运行模型的端点副本数量。通过利用自动缩放，您可以无缝处理不同的工作负载，同时优化成本并确保高可用性。

## 缩放标准

自动缩放过程是基于加速器利用率指标触发的。根据所使用的加速器类型，缩放的标准有所不同：

+   **CPU 加速器**：当所有副本的平均 CPU 利用率达到 80% 时，将添加一个新副本。

+   **GPU 加速器**：当所有副本在 2 分钟窗口内的平均 GPU 利用率达到 80% 时，将添加一个新副本。

值得注意的是，扩展过程每分钟进行一次，而缩减过程需要 2 分钟。这种频率确保了自动缩放系统的响应性和稳定性之间的平衡，在扩展或缩减后稳定时间为 300 秒。

## 有效自动缩放的考虑因素

虽然自动缩放提供了方便的资源管理，但应该牢记一些考虑因素以确保其有效性：

+   **模型初始化时间**：在新副本初始化期间，模型将被下载并加载到内存中。如果您的副本初始化时间较长，则自动缩放可能不太有效。这是因为在此期间，平均 GPU 利用率可能会低于阈值，从而触发端点的自动缩减。

+   **企业计划控制**：如果您拥有[企业计划](https://huggingface.co/inference-endpoints/enterprise)，您可以完全控制自动缩放定义。这使您可以根据特定要求自定义缩放阈值、行为和标准。

## 缩放至 0

推理端点还支持将副本缩放至 0，这意味着在没有传入流量时将副本数量减少至 0。此功能基于请求模式而不是加速器利用率。当端点保持空闲超过 15 分钟没有收到任何请求时，系统会自动将端点缩减至 0 个副本。要启用此功能，请转到设置页面，您会找到一个名为“自动缩放至零”的部分。

将副本缩放至 0 有助于通过在空闲时期最小化资源使用来优化成本节省。然而，重要的是要意识到，将副本缩放至 0 意味着在端点接收到新请求时会有一个冷启动期。此外，在新副本初始化时，HTTP 服务器将以状态码 `502 Bad Gateway` 响应。请注意，目前没有针对传入请求的排队系统。因此，我们建议在客户端开发自己的请求队列并进行适当的错误处理，以优化吞吐量和延迟。

冷启动期的持续时间取决于您的模型大小。建议在启用缩放至 0 和管理用户期望时考虑潜在的延迟影响。
