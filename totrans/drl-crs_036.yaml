- en: A Q-Learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand Q-Learning, let’s take a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/7bdcf448ce834a314efde42768c303f7.png)'
  prefs: []
  type: TYPE_IMG
- en: You’re a mouse in this tiny maze. You always **start at the same starting point.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is **to eat the big pile of cheese at the bottom right-hand corner** and
    avoid the poison. After all, who doesn’t like cheese?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The episode ends if we eat the poison, **eat the big pile of cheese**, or if
    we take more than five steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate is 0.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount rate (gamma) is 0.99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/c17db319504a73cec7f8f89c466d8000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The reward function goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**+0:** Going to a state with no cheese in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**+1:** Going to a state with a small cheese in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**+10:** Going to the state with the big pile of cheese.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-10:** Going to the state with the poison and thus dying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**+0** If we take more than five steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/f8fc089c4b1640dfdcc484c4f5916025.png)'
  prefs: []
  type: TYPE_IMG
- en: To train our agent to have an optimal policy (so a policy that goes right, right,
    down), **we will use the Q-Learning algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Initialize the Q-table'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/36ec86ed2fe8ef5a648fca96e67b8e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, for now, **our Q-table is useless**; we need **to train our Q-function using
    the Q-Learning algorithm.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do it for 2 training timesteps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training timestep 1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Choose an action using the Epsilon Greedy Strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because epsilon is big (= 1.0), I take a random action. In this case, I go right.
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/3b703676d561e8e0bcd1fb87c2b3aef2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 3: Perform action At, get Rt+1 and St+1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By going right, I get a small cheese, so<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = 1</annotation></semantics></math>Rt+1​=1
    and I’m in a new state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/59cd579ddd1d870f7a3df5e80ebc2b03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 4: Update Q(St, At)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now update<math><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    separator="true">,</mo><msub><mi>A</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">Q(S_t, A_t)</annotation></semantics></math>Q(St​,At​)
    using our formula.
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/039d07cc30eaeda29cabcae129111e00.png) ![Maze-Example](../Images/a6cf10447e88cd0d903730da115cb468.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training timestep 2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Choose an action using the Epsilon Greedy Strategy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**I take a random action again, since epsilon=0.99 is big**. (Notice we decay
    epsilon a little bit because, as the training progress, we want less and less
    exploration).'
  prefs: []
  type: TYPE_NORMAL
- en: I took the action ‘down’. **This is not a good action since it leads me to the
    poison.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/8a555fe2a851cda0cd8432c383e56d7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 3: Perform action At, get Rt+1 and St+1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because I ate poison, **I get<math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mo>−</mo><mn>10</mn></mrow><annotation
    encoding="application/x-tex">R_{t+1} = -10</annotation></semantics></math>Rt+1​=−10,
    and I die.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/44ccf2ebfc7c5cc6d2383ac1d60d8f4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 4: Update Q(St, At)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Maze-Example](../Images/b633bdc1c80d53eef7dad7daab4e8e1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Because we’re dead, we start a new episode. But what we see here is that, **with
    two explorations steps, my agent became smarter.**
  prefs: []
  type: TYPE_NORMAL
- en: As we continue exploring and exploiting the environment and updating Q-values
    using the TD target, the **Q-table will give us a better and better approximation.
    At the end of the training, we’ll get an estimate of the optimal Q-function.**
  prefs: []
  type: TYPE_NORMAL
