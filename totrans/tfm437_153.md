# CPMAnt

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpmant)

## 概述

CPM-Ant 是一个拥有 10B 参数的开源中文预训练语言模型（PLM）。它也是 CPM-Live 实时训练过程的第一个里程碑。训练过程具有成本效益且环保。CPM-Ant 在 CUGE 基准测试中通过增量调整取得了令人满意的结果。除了完整模型，我们还提供各种压缩版本以满足不同硬件配置的要求。[查看更多](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)

该模型由[OpenBMB](https://huggingface.co/openbmb)贡献。原始代码可在[此处](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)找到。

## 资源

+   一个关于[CPM-Live](https://github.com/OpenBMB/CPM-Live/tree/cpm-ant/cpm-live)的教程。

## CpmAntConfig

### `class transformers.CpmAntConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/configuration_cpmant.py#L29)

```py
( vocab_size: int = 30720 hidden_size: int = 4096 num_attention_heads: int = 32 dim_head: int = 128 dim_ff: int = 10240 num_hidden_layers: int = 48 dropout_p: int = 0.0 position_bias_num_buckets: int = 512 position_bias_max_distance: int = 2048 eps: int = 1e-06 init_std: float = 1.0 prompt_types: int = 32 prompt_length: int = 32 segment_types: int = 32 use_cache: bool = True **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为 30720) — CPMAnt 模型的词汇量。定义调用 CpmAntModel 时传递的`input`中可以表示的不同标记数量。

+   `hidden_size` (`int`, *可选*, 默认为 4096) — 编码器层的维度。

+   `num_attention_heads` (`int`, *可选*, 默认为 32) — Transformer 编码器中的注意力头数。

+   `dim_head` (`int`, *可选*, 默认为 128) — Transformer 编码器中每个注意力层的注意力头维度。

+   `dim_ff` (`int`, *可选*, 默认为 10240) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为 48) — Transformer 编码器的层数。

+   `dropout_p` (`float`, *可选*, 默认为 0.0) — 嵌入层、编码器中所有全连接层的 dropout 概率。

+   `position_bias_num_buckets` (`int`, *可选*, 默认为 512) — 位置偏置桶的数量。

+   `position_bias_max_distance` (`int`, *可选*, 默认为 2048) — 该模型可能被使用的最大序列长度。通常设置为一个较大的值以防万一（例如 512、1024 或 2048）。

+   `eps` (`float`, *可选*, 默认为 1e-06) — 层归一化层使用的 epsilon。

+   `init_std` (`float`, *可选*, 默认为 1.0) — 使用 std = init_std 初始化参数。

+   `prompt_types` (`int`, *可选*, 默认为 32) — 提示类型。

+   `prompt_length` (`int`, *可选*, 默认为 32) — 提示的长度。

+   `segment_types` (`int`, *可选*, 默认为 32) — 分段类型。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 是否使用缓存。

这是一个配置类，用于存储 CpmAntModel 的配置。根据指定的参数实例化一个 CPMAnt 模型，定义模型架构。使用默认值实例化配置将产生类似于 CPMAnt [openbmb/cpm-ant-10b](https://huggingface.co/openbmb/cpm-ant-10b)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

示例：

```py
>>> from transformers import CpmAntModel, CpmAntConfig

>>> # Initializing a CPMAnt cpm-ant-10b style configuration
>>> configuration = CpmAntConfig()

>>> # Initializing a model from the cpm-ant-10b style configuration
>>> model = CpmAntModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## CpmAntTokenizer

### `class transformers.CpmAntTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L88)

```py
( vocab_file bod_token = '<d>' eod_token = '</d>' bos_token = '<s>' eos_token = '</s>' pad_token = '<pad>' unk_token = '<unk>' line_token = '</n>' space_token = '</_>' padding_side = 'left' **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `bod_token`（`str`，*可选*，默认为`"<d>"`）— 文档起始标记。

+   `eod_token`（`str`，*可选*，默认为`"</d>"`）— 文档结束标记。

+   `bos_token`（`str`，*可选*，默认为`"<s>"`）— 序列起始标记。

+   `eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。

+   `pad_token`（`str`，*可选*，默认为`"<pad>"`）— 用于填充的标记。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。

+   `line_token`（`str`，*可选*，默认为`"</n>"`）— 行标记。

+   `space_token`（`str`，*可选*，默认为`"</_>"`）— 空格标记。

构建一个 CPMAnt 分词器。基于字节级字节对编码。

#### `build_inputs_with_special_tokens`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L236)

```py
( token_ids_0: List token_ids_1: List = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— 将添加特殊标记的第一个标记化序列。

+   `token_ids_1`（`List[int]`）— 将添加特殊标记的可选第二个标记化序列。

返回

`List[int]`

带有特殊标记的模型输入。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。CPMAnt 序列的格式如下：

+   单个序列：`[BOS] Sequence`。

#### `get_special_tokens_mask`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/tokenization_cpmant.py#L254)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）— ID 列表。

+   `token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 标记列表是否已经格式化为模型的特殊标记。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1 表示特殊标记，0 表示序列标记。

从没有添加特殊标记的标记列表中检索序列 ID。在使用分词器`prepare_for_model`方法添加特殊标记时调用此方法。

## CpmAntModel

### `class transformers.CpmAntModel`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L594)

```py
( config: CpmAntConfig )
```

裸 CPMAnt 模型输出原始隐藏状态，没有特定的头部。此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

参数配置（~CpmAntConfig）：具有所有初始化参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

#### `forward`

[<源代码>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L636)

```py
( input_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None past_key_values: Optional = None use_cache: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, seq_len)`的`torch.Tensor`）— 词汇表中输入序列标记的索引。

    可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递了`use_cache=True`或者当`config.use_cache=True`时返回） — 包含预计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

+   `use_cache`（`bool`，*可选*） — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。

+   `return_dict`（`bool`，*可选*） — 是否返回 ModelOutput 而不是普通元组。

返回

一个 transformers.modeling_outputs.BaseModelOutputWithPast 或者`tuple(torch.FloatTensor)`

一个 transformers.modeling_outputs.BaseModelOutputWithPast 或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时），包括各种元素，取决于配置（CpmAntConfig）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`） — 模型最后一层的隐藏状态序列。

    如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递了`use_cache=True`或者当`config.use_cache=True`时返回） — 长度为`config.n_layers`的元组`tuple(torch.FloatTensor)`，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，并且如果`config.is_encoder_decoder=True`，还有 2 个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预计算的隐藏状态（自注意力块中的键和值，以及如果`config.is_encoder_decoder=True`，则包含交叉注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层的输出隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递了`output_attentions=True`或者当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重经过注意力 softmax 后，用于计算自注意力头中的加权平均值。

CpmAntModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, CpmAntModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("openbmb/cpm-ant-10b")
>>> model = CpmAntModel.from_pretrained("openbmb/cpm-ant-10b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## CpmAntForCausalLM

### `class transformers.CpmAntForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L739)

```py
( config: CpmAntConfig )
```

CPMAnt 模型在顶部带有语言建模头（线性层，其权重与输入嵌入绑定）。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

参数配置（~CpmAntConfig）：模型配置类，包含所有初始化参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpmant/modeling_cpmant.py#L758)

```py
( input_ids: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None attention_mask: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为（batch_size，seq_len）的`torch.Tensor`） - 词汇表中输入序列标记的索引。

    可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） - 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

+   `use_cache`（`bool`，*可选*） - 如果设置为`True`，将返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*） - 是否返回所有注意力层的注意力张量。

+   `output_hidden_states`（`bool`，*可选*） - 是否返回所有层的隐藏状态。

+   `return_dict`（`bool`，*可选*） - 是否返回 ModelOutput 而不是普通元组。

    参数 - 输入 ID（形状为（batch_size，seq_len）的`torch.Tensor`）：词汇表中输入序列标记的索引。

    可以使用`CPMAntTokenizer`获取索引。有关详细信息，请参阅 PreTrainedTokenizer.encode()和 PreTrainedTokenizer.`call`()。

    什么是输入 ID？ `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回：包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。 `use_cache` (`bool`, *optional*): 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。 `output_attentions` (`bool`, *optional*): 是否返回所有注意力层的注意力张量。 `output_hidden_states` (`bool`, *optional*): 是否返回所有层的隐藏状态。 `labels` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*): 用于计算掩码语言建模损失的标签。 `return_dict` (`bool`，*optional*): 是否返回 ModelOutput 而不是普通元组。 `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*): CPMAnt 将自动处理注意力掩码，此参数是文本生成流水线的虚拟参数。

    示例 —

+   使用 CpmAntForCausalLM 进行文本生成。 —

返回

transformers.modeling_outputs.CausalLMOutputWithPast 或`torch.FloatTensor`元组。

一个 transformers.modeling_outputs.CausalLMOutputWithPast 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（CpmAntConfig）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回 — 长度为`config.n_layers`的元组，每个元组有 2 个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回 — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回 — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

CpmAntForCausalLM 前向方法，覆盖了`__call__`特殊方法。

尽管前向传播的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, CpmAntForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("openbmb/cpm-ant-10b")
>>> model = CpmAntForCausalLM.from_pretrained("openbmb/cpm-ant-10b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```
