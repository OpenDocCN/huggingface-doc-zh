- en: Distributed Inference with ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/distributed_inference](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/distributed_inference](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed inference can fall into three brackets:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†å¸ƒå¼æ¨ç†å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š
- en: Loading an entire model onto each GPU and sending chunks of a batch through
    each GPUâ€™s model copy at a time
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ•´ä¸ªæ¨¡å‹åŠ è½½åˆ°æ¯ä¸ªGPUä¸Šï¼Œå¹¶ä¸€æ¬¡å‘é€ä¸€æ‰¹æ•°æ®å—åˆ°æ¯ä¸ªGPUçš„æ¨¡å‹å‰¯æœ¬
- en: Loading parts of a model onto each GPU and processing a single input at one
    time
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹çš„éƒ¨åˆ†åŠ è½½åˆ°æ¯ä¸ªGPUä¸Šï¼Œå¹¶ä¸€æ¬¡å¤„ç†ä¸€ä¸ªè¾“å…¥
- en: Loading parts of a model onto each GPU and using what is called scheduled Pipeline
    Parallelism to combine the two prior techniques.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹çš„éƒ¨åˆ†åŠ è½½åˆ°æ¯ä¸ªGPUä¸Šï¼Œå¹¶ä½¿ç”¨æ‰€è°“çš„è®¡åˆ’ç®¡é“å¹¶è¡Œæ¥ç»“åˆè¿™ä¸¤ç§å…ˆå‰çš„æŠ€æœ¯ã€‚
- en: Weâ€™re going to go through the first and the last bracket, showcasing how to
    do each as they are more realistic scenarios.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€ä¸ªè®¨è®ºç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ‹¬å·ï¼Œå±•ç¤ºå¦‚ä½•æ‰§è¡Œæ¯ä¸ªæ‹¬å·ï¼Œå› ä¸ºå®ƒä»¬æ›´ç¬¦åˆå®é™…æƒ…å†µã€‚
- en: Sending chunks of a batch automatically to each loaded model
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨å°†ä¸€æ‰¹æ•°æ®å—å‘é€åˆ°æ¯ä¸ªåŠ è½½çš„æ¨¡å‹
- en: This is the most memory-intensive solution, as it requires each GPU to keep
    a full copy of the model in memory at a given time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€å ç”¨å†…å­˜çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒè¦æ±‚æ¯ä¸ªGPUåœ¨ç»™å®šæ—¶é—´å†…ä¿ç•™æ¨¡å‹çš„å®Œæ•´å‰¯æœ¬ã€‚
- en: Normally when doing this, users send the model to a specific device to load
    it from the CPU, and then move each prompt to a different device.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”¨æˆ·å°†æ¨¡å‹å‘é€åˆ°ç‰¹å®šè®¾å¤‡ä»¥ä»CPUåŠ è½½å®ƒï¼Œç„¶åå°†æ¯ä¸ªæç¤ºç§»åŠ¨åˆ°ä¸åŒçš„è®¾å¤‡ã€‚
- en: 'A basic pipeline using the `diffusers` library might look something like so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `diffusers` åº“åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„ç®¡é“å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Followed then by performing inference based on the specific prompt:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ ¹æ®ç‰¹å®šæç¤ºæ‰§è¡Œæ¨ç†ï¼š
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: One will notice how we have to check the rank to know what prompt to send, which
    can be a bit tedious.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬ä¼šæ³¨æ„åˆ°æˆ‘ä»¬å¿…é¡»æ£€æŸ¥ç­‰çº§ä»¥çŸ¥é“è¦å‘é€å“ªä¸ªæç¤ºï¼Œè¿™å¯èƒ½æœ‰ç‚¹ç¹çã€‚
- en: A user might then also think that with ğŸ¤— Accelerate, using the `Accelerator`
    to prepare a dataloader for such a task might also be a simple way to manage this.
    (To learn more, check out the relevant section in the [Quick Tour](../quicktour#distributed-evaluation))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åç”¨æˆ·å¯èƒ½ä¼šè®¤ä¸ºä½¿ç”¨ `Accelerator` åœ¨å‡†å¤‡æ•°æ®åŠ è½½å™¨æ—¶ä¹Ÿå¯èƒ½æ˜¯ç®¡ç†è¿™ä¸ªä»»åŠ¡çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚ï¼ˆè¦äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹[å¿«é€Ÿå…¥é—¨](../quicktour#distributed-evaluation)ä¸­çš„ç›¸å…³éƒ¨åˆ†ï¼‰
- en: 'Can it manage it? Yes. Does it add unneeded extra code however: also yes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒèƒ½å¤Ÿç®¡ç†å—ï¼Ÿæ˜¯çš„ã€‚ä½†æ˜¯å®ƒæ˜¯å¦æ·»åŠ äº†ä¸å¿…è¦çš„é¢å¤–ä»£ç ï¼šä¹Ÿæ˜¯çš„ã€‚
- en: With ğŸ¤— Accelerate, we can simplify this process by using the [Accelerator.split_between_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.split_between_processes)
    context manager (which also exists in `PartialState` and `AcceleratorState`).
    This function will automatically split whatever data you pass to it (be it a prompt,
    a set of tensors, a dictionary of the prior data, etc.) across all the processes
    (with a potential to be padded) for you to use right away.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ğŸ¤— Accelerateï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ [Accelerator.split_between_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.split_between_processes)
    ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆä¹Ÿå­˜åœ¨äº `PartialState` å’Œ `AcceleratorState` ä¸­ï¼‰æ¥ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹ã€‚è¿™ä¸ªå‡½æ•°ä¼šè‡ªåŠ¨å°†æ‚¨ä¼ é€’ç»™å®ƒçš„ä»»ä½•æ•°æ®ï¼ˆæ— è®ºæ˜¯æç¤ºã€ä¸€ç»„å¼ é‡ã€å…ˆå‰æ•°æ®çš„å­—å…¸ç­‰ï¼‰è‡ªåŠ¨åˆ†é…åˆ°æ‰€æœ‰è¿›ç¨‹ï¼ˆå¯èƒ½éœ€è¦å¡«å……ï¼‰ä¸­ï¼Œä»¥ä¾¿ç«‹å³ä½¿ç”¨ã€‚
- en: 'Letâ€™s rewrite the above example using this context manager:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨é‡æ–°ç¼–å†™ä¸Šé¢çš„ä¾‹å­ï¼š
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then to launch the code, we can use the ğŸ¤— Accelerate:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯åŠ¨ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ğŸ¤— Accelerateï¼š
- en: 'If you have generated a config file to be used using `accelerate config`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å·²ç»ç”Ÿæˆäº†ä¸€ä¸ªè¦ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ `accelerate config`ï¼š
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you have a specific config file you want to use:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰ä¸€ä¸ªç‰¹å®šçš„é…ç½®æ–‡ä»¶è¦ä½¿ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or if donâ€™t want to make any config files and launch on two GPUs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…å¦‚æœä¸æƒ³åˆ›å»ºä»»ä½•é…ç½®æ–‡ä»¶å¹¶åœ¨ä¸¤ä¸ªGPUä¸Šå¯åŠ¨ï¼š
- en: 'Note: You will get some warnings about values being guessed based on your system.
    To remove these you can do `accelerate config default` or go through `accelerate
    config` to create a config file.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ‚¨å°†æ”¶åˆ°ä¸€äº›å…³äºæ ¹æ®æ‚¨çš„ç³»ç»ŸçŒœæµ‹å€¼çš„è­¦å‘Šã€‚è¦åˆ é™¤è¿™äº›è­¦å‘Šï¼Œæ‚¨å¯ä»¥æ‰§è¡Œ `accelerate config default` æˆ–é€šè¿‡ `accelerate
    config` åˆ›å»ºä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Weâ€™ve now reduced the boilerplate code needed to split this data to a few lines
    of code quite easily.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»å°†éœ€è¦æ‹†åˆ†è¿™äº›æ•°æ®çš„æ ·æ¿ä»£ç å‡å°‘åˆ°å‡ è¡Œä»£ç ä¸­ï¼Œéå¸¸å®¹æ˜“ã€‚
- en: But what if we have an odd distribution of prompts to GPUs? For example, what
    if we have 3 prompts, but only 2 GPUs?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªå¥‡æ•°åˆ†å¸ƒçš„æç¤ºåˆ°GPUï¼Ÿä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰3ä¸ªæç¤ºï¼Œä½†åªæœ‰2ä¸ªGPUï¼Ÿ
- en: Under the context manager, the first GPU would receive the first two prompts
    and the second GPU the third, ensuring that all prompts are split and no overhead
    is needed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹ï¼Œç¬¬ä¸€ä¸ªGPUå°†æ¥æ”¶å‰ä¸¤ä¸ªæç¤ºï¼Œç¬¬äºŒä¸ªGPUå°†æ¥æ”¶ç¬¬ä¸‰ä¸ªæç¤ºï¼Œç¡®ä¿æ‰€æœ‰æç¤ºéƒ½è¢«æ‹†åˆ†ï¼Œä¸éœ€è¦é¢å¤–çš„å¼€é”€ã€‚
- en: '*However*, what if we then wanted to do something with the results of *all
    the GPUs*? (Say gather them all and perform some kind of post processing) You
    can pass in `apply_padding=True` to ensure that the lists of prompts are padded
    to the same length, with extra data being taken from the last sample. This way
    all GPUs will have the same number of prompts, and you can then gather the results.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç„¶è€Œ*ï¼Œå¦‚æœæˆ‘ä»¬æƒ³å¯¹*æ‰€æœ‰GPU*çš„ç»“æœåšä¸€äº›å¤„ç†æ€ä¹ˆåŠï¼Ÿï¼ˆæ¯”å¦‚æ”¶é›†å®ƒä»¬å¹¶æ‰§è¡ŒæŸç§åå¤„ç†ï¼‰æ‚¨å¯ä»¥ä¼ é€’ `apply_padding=True` æ¥ç¡®ä¿æç¤ºåˆ—è¡¨å¡«å……åˆ°ç›¸åŒçš„é•¿åº¦ï¼Œé¢å¤–çš„æ•°æ®å°†ä»æœ€åä¸€ä¸ªæ ·æœ¬ä¸­è·å–ã€‚è¿™æ ·æ‰€æœ‰GPUå°†æœ‰ç›¸åŒæ•°é‡çš„æç¤ºï¼Œç„¶åæ‚¨å¯ä»¥æ”¶é›†ç»“æœã€‚'
- en: This is only needed when trying to perform an action such as gathering the results,
    where the data on each device needs to be the same length. Basic inference does
    not require this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰åœ¨å°è¯•æ‰§è¡Œè¯¸å¦‚æ”¶é›†ç»“æœä¹‹ç±»çš„æ“ä½œæ—¶æ‰éœ€è¦è¿™æ ·åšï¼Œå…¶ä¸­æ¯ä¸ªè®¾å¤‡ä¸Šçš„æ•°æ®éœ€è¦å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚åŸºæœ¬æ¨ç†ä¸éœ€è¦è¿™æ ·åšã€‚
- en: 'For instance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On the first GPU, the prompts will be `["a dog", "a cat"]`, and on the second
    GPU it will be `["a chicken", "a chicken"]`. Make sure to drop the final sample,
    as it will be a duplicate of the previous one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€ä¸ªGPUä¸Šï¼Œæç¤ºå°†æ˜¯ `["a dog", "a cat"]`ï¼Œåœ¨ç¬¬äºŒä¸ªGPUä¸Šå°†æ˜¯ `["a chicken", "a chicken"]`ã€‚ç¡®ä¿ä¸¢å¼ƒæœ€åä¸€ä¸ªæ ·æœ¬ï¼Œå› ä¸ºå®ƒå°†æ˜¯å‰ä¸€ä¸ªæ ·æœ¬çš„å‰¯æœ¬ã€‚
- en: Memory-efficient pipeline parallelism (experimental)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜é«˜æ•ˆçš„ç®¡é“å¹¶è¡Œï¼ˆå®éªŒæ€§ï¼‰
- en: This next part will discuss using *pipeline parallelism*. This is an **experimental**
    API utilizing the [PiPPy library by PyTorch](https://github.com/pytorch/PiPPy/)
    as a native solution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†è®¨è®ºä½¿ç”¨*ç®¡é“å¹¶è¡Œå¤„ç†*ã€‚è¿™æ˜¯ä¸€ä¸ª**å®éªŒæ€§** APIï¼Œåˆ©ç”¨äº† [PyTorch çš„ PiPPy åº“](https://github.com/pytorch/PiPPy/)
    ä½œä¸ºæœ¬åœ°è§£å†³æ–¹æ¡ˆã€‚
- en: 'The general idea with pipeline parallelism is: say you have 4 GPUs and a model
    big enough it can be *split* on four GPUs using `device_map="auto"`. With this
    method you can send in 4 inputs at a time (for example here, any amount works)
    and each model chunk will work on an input, then receive the next input once the
    prior chunk finished, making it *much* more efficient **and faster** than the
    method described earlier. Hereâ€™s a visual taken from the PyTorch repository:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡é“å¹¶è¡Œå¤„ç†çš„ä¸€èˆ¬æ€æƒ³æ˜¯ï¼šå‡è®¾æ‚¨æœ‰ 4 ä¸ª GPU å’Œä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `device_map="auto"` åœ¨å››ä¸ª GPU ä¸Šè¿›è¡Œ*åˆ†å‰²*ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨å¯ä»¥ä¸€æ¬¡å‘é€
    4 ä¸ªè¾“å…¥ï¼ˆä¾‹å¦‚åœ¨è¿™é‡Œï¼Œä»»æ„æ•°é‡éƒ½å¯ä»¥ï¼‰ï¼Œæ¯ä¸ªæ¨¡å‹å—å°†å¤„ç†ä¸€ä¸ªè¾“å…¥ï¼Œç„¶ååœ¨å‰ä¸€ä¸ªå—å®Œæˆåæ¥æ”¶ä¸‹ä¸€ä¸ªè¾“å…¥ï¼Œä½¿å…¶æ¯”ä¹‹å‰æè¿°çš„æ–¹æ³•*æ›´*é«˜æ•ˆ **å’Œæ›´å¿«**ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ¥è‡ª
    PyTorch ä»“åº“çš„å¯è§†åŒ–ï¼š
- en: '![PiPPy example](../Images/0d96d7f73aed95a927cf10c18cd407b8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![PiPPy ç¤ºä¾‹](../Images/0d96d7f73aed95a927cf10c18cd407b8.png)'
- en: To illustrate how you can use this with Accelerate, we have created an [example
    zoo](https://github.com/huggingface/accelerate/tree/main/examples/inference) showcasing
    a number of different models and situations. In this tutorial, weâ€™ll show this
    method for GPT2 across two GPUs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜æ‚¨å¦‚ä½•åœ¨ Accelerate ä¸­ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª [ç¤ºä¾‹åŠ¨ç‰©å›­](https://github.com/huggingface/accelerate/tree/main/examples/inference)ï¼Œå±•ç¤ºäº†è®¸å¤šä¸åŒçš„æ¨¡å‹å’Œæƒ…å†µã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ä¸¤ä¸ª
    GPU ä¸Šä¸º GPT2 ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚
- en: 'Before you proceed, please make sure you have the latest pippy installed by
    running the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…äº†æœ€æ–°çš„ pippyï¼Œæ–¹æ³•æ˜¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We require at least version 0.2.0\. To confirm that you have the correct version,
    run `pip show torchpippy`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è‡³å°‘éœ€è¦ç‰ˆæœ¬ 0.2.0ã€‚è¦ç¡®è®¤æ‚¨å®‰è£…äº†æ­£ç¡®çš„ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ `pip show torchpippy`ã€‚
- en: 'Start by creating the model on the CPU:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆåœ¨ CPU ä¸Šåˆ›å»ºæ¨¡å‹ï¼š
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next youâ€™ll need to create some example inputs to use. These help PiPPy trace
    the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€äº›ç¤ºä¾‹è¾“å…¥ä»¥ä¾›ä½¿ç”¨ã€‚è¿™æœ‰åŠ©äº PiPPy è¿½è¸ªæ¨¡å‹ã€‚
- en: However you make this example will determine the relative batch size that will
    be used/passed through the model at a given time, so make sure to remember how
    many items there are!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨åˆ¶ä½œè¿™ä¸ªç¤ºä¾‹çš„æ–¹å¼å°†å†³å®šåœ¨ç»™å®šæ—¶é—´é€šè¿‡æ¨¡å‹ä¼ é€’çš„ç›¸å¯¹æ‰¹æ¬¡å¤§å°ï¼Œå› æ­¤è¯·è®°ä½æœ‰å¤šå°‘é¡¹ï¼
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next we need to actually perform the tracing and get the model ready. To do
    so, use the [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)
    function and it will fully wrap the model for pipeline parallelism automatically:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®é™…æ‰§è¡Œè¿½è¸ªå¹¶å‡†å¤‡å¥½æ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œè¯·ä½¿ç”¨ [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)
    å‡½æ•°ï¼Œå®ƒå°†è‡ªåŠ¨å®Œå…¨åŒ…è£…æ¨¡å‹ä»¥è¿›è¡Œç®¡é“å¹¶è¡Œå¤„ç†ï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are a variety of parameters you can pass through to `prepare_pippy`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ `prepare_pippy` ä¼ é€’å„ç§å‚æ•°ï¼š
- en: '`split_points` lets you determine what layers to split the model at. By default
    we use wherever `device_map="auto" declares, such as` fc`or`conv1`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_points` è®©æ‚¨ç¡®å®šåœ¨å“ªäº›å±‚ä¸Šæ‹†åˆ†æ¨¡å‹ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ `device_map="auto"` å£°æ˜çš„ä½ç½®ï¼Œä¾‹å¦‚ `fc` æˆ–
    `conv1`ã€‚'
- en: '`num_chunks` determines how the batch will be split and sent to the model itself
    (so `num_chunks=1` with four split points/four GPUs will have a naive MP where
    a single input gets passed between the four layer split points)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_chunks` å†³å®šäº†æ‰¹æ¬¡å°†å¦‚ä½•åˆ†å‰²å¹¶å‘é€åˆ°æ¨¡å‹æœ¬èº«ï¼ˆå› æ­¤ `num_chunks=1`ï¼Œæœ‰å››ä¸ªåˆ†å‰²ç‚¹/å››ä¸ª GPU å°†å…·æœ‰ä¸€ä¸ªç®€å•çš„ MPï¼Œå…¶ä¸­å•ä¸ªè¾“å…¥åœ¨å››ä¸ªå±‚åˆ†å‰²ç‚¹ä¹‹é—´ä¼ é€’ï¼‰'
- en: From here, all thatâ€™s left is to actually perform the distributed inference!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œå‰©ä¸‹çš„å°±æ˜¯å®é™…æ‰§è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼
- en: When passing inputs, we highly recommend to pass them in as a tuple of arguments.
    Using `kwargs` is supported, however, this approach is experimental.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ é€’è¾“å…¥æ—¶ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å°†å®ƒä»¬ä½œä¸ºå‚æ•°å…ƒç»„ä¼ é€’ã€‚æ”¯æŒä½¿ç”¨ `kwargs`ï¼Œä½†è¿™ç§æ–¹æ³•æ˜¯å®éªŒæ€§çš„ã€‚
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When finished all the data will be on the last process only:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæ‰€æœ‰æ•°æ®å°†ä»…åœ¨æœ€åä¸€ä¸ªè¿›ç¨‹ä¸Šï¼š
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you pass in `gather_output=True` to [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy),
    the output will be sent across to all the GPUs afterwards without needing the
    `is_last_process` check. This is `False` by default as it incurs a communication
    call.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå°† `gather_output=True` ä¼ é€’ç»™ [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)ï¼Œè¾“å‡ºå°†åœ¨ä¹‹åå‘é€åˆ°æ‰€æœ‰
    GPUï¼Œæ— éœ€è¿›è¡Œ `is_last_process` æ£€æŸ¥ã€‚é»˜è®¤æƒ…å†µä¸‹ä¸º `False`ï¼Œå› ä¸ºè¿™ä¼šäº§ç”Ÿé€šä¿¡è°ƒç”¨ã€‚
- en: And thatâ€™s it! To explore more, please check out the inference examples in the
    [Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples/inference)
    and our [documentation](../package_reference/inference) as we work to improving
    this integration.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼è¦äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹ [Accelerate ä»“åº“](https://github.com/huggingface/accelerate/tree/main/examples/inference)
    ä¸­çš„æ¨ç†ç¤ºä¾‹å’Œæˆ‘ä»¬çš„ [æ–‡æ¡£](../package_reference/inference)ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ”¹è¿›è¿™ä¸ªé›†æˆã€‚
