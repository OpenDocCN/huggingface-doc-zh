- en: Distributed Inference with 🤗 Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 🤗 Accelerate 进行分布式推理
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/distributed_inference](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/usage_guides/distributed_inference](https://huggingface.co/docs/accelerate/usage_guides/distributed_inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed inference can fall into three brackets:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式推理可以分为三个部分：
- en: Loading an entire model onto each GPU and sending chunks of a batch through
    each GPU’s model copy at a time
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将整个模型加载到每个GPU上，并一次发送一批数据块到每个GPU的模型副本
- en: Loading parts of a model onto each GPU and processing a single input at one
    time
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的部分加载到每个GPU上，并一次处理一个输入
- en: Loading parts of a model onto each GPU and using what is called scheduled Pipeline
    Parallelism to combine the two prior techniques.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的部分加载到每个GPU上，并使用所谓的计划管道并行来结合这两种先前的技术。
- en: We’re going to go through the first and the last bracket, showcasing how to
    do each as they are more realistic scenarios.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个讨论第一个和最后一个括号，展示如何执行每个括号，因为它们更符合实际情况。
- en: Sending chunks of a batch automatically to each loaded model
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动将一批数据块发送到每个加载的模型
- en: This is the most memory-intensive solution, as it requires each GPU to keep
    a full copy of the model in memory at a given time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最占用内存的解决方案，因为它要求每个GPU在给定时间内保留模型的完整副本。
- en: Normally when doing this, users send the model to a specific device to load
    it from the CPU, and then move each prompt to a different device.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在这种情况下，用户将模型发送到特定设备以从CPU加载它，然后将每个提示移动到不同的设备。
- en: 'A basic pipeline using the `diffusers` library might look something like so:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `diffusers` 库创建一个基本的管道可能看起来像这样：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Followed then by performing inference based on the specific prompt:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后根据特定提示执行推理：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: One will notice how we have to check the rank to know what prompt to send, which
    can be a bit tedious.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人们会注意到我们必须检查等级以知道要发送哪个提示，这可能有点繁琐。
- en: A user might then also think that with 🤗 Accelerate, using the `Accelerator`
    to prepare a dataloader for such a task might also be a simple way to manage this.
    (To learn more, check out the relevant section in the [Quick Tour](../quicktour#distributed-evaluation))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用户可能会认为使用 `Accelerator` 在准备数据加载器时也可能是管理这个任务的一种简单方法。（要了解更多，请查看[快速入门](../quicktour#distributed-evaluation)中的相关部分）
- en: 'Can it manage it? Yes. Does it add unneeded extra code however: also yes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 它能够管理吗？是的。但是它是否添加了不必要的额外代码：也是的。
- en: With 🤗 Accelerate, we can simplify this process by using the [Accelerator.split_between_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.split_between_processes)
    context manager (which also exists in `PartialState` and `AcceleratorState`).
    This function will automatically split whatever data you pass to it (be it a prompt,
    a set of tensors, a dictionary of the prior data, etc.) across all the processes
    (with a potential to be padded) for you to use right away.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 🤗 Accelerate，我们可以通过使用 [Accelerator.split_between_processes()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.split_between_processes)
    上下文管理器（也存在于 `PartialState` 和 `AcceleratorState` 中）来简化这个过程。这个函数会自动将您传递给它的任何数据（无论是提示、一组张量、先前数据的字典等）自动分配到所有进程（可能需要填充）中，以便立即使用。
- en: 'Let’s rewrite the above example using this context manager:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个上下文管理器重新编写上面的例子：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then to launch the code, we can use the 🤗 Accelerate:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动代码，我们可以使用 🤗 Accelerate：
- en: 'If you have generated a config file to be used using `accelerate config`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经生成了一个要使用的配置文件，请使用 `accelerate config`：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you have a specific config file you want to use:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个特定的配置文件要使用：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or if don’t want to make any config files and launch on two GPUs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如果不想创建任何配置文件并在两个GPU上启动：
- en: 'Note: You will get some warnings about values being guessed based on your system.
    To remove these you can do `accelerate config default` or go through `accelerate
    config` to create a config file.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：您将收到一些关于根据您的系统猜测值的警告。要删除这些警告，您可以执行 `accelerate config default` 或通过 `accelerate
    config` 创建一个配置文件。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ve now reduced the boilerplate code needed to split this data to a few lines
    of code quite easily.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将需要拆分这些数据的样板代码减少到几行代码中，非常容易。
- en: But what if we have an odd distribution of prompts to GPUs? For example, what
    if we have 3 prompts, but only 2 GPUs?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们有一个奇数分布的提示到GPU？例如，如果我们有3个提示，但只有2个GPU？
- en: Under the context manager, the first GPU would receive the first two prompts
    and the second GPU the third, ensuring that all prompts are split and no overhead
    is needed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文管理器下，第一个GPU将接收前两个提示，第二个GPU将接收第三个提示，确保所有提示都被拆分，不需要额外的开销。
- en: '*However*, what if we then wanted to do something with the results of *all
    the GPUs*? (Say gather them all and perform some kind of post processing) You
    can pass in `apply_padding=True` to ensure that the lists of prompts are padded
    to the same length, with extra data being taken from the last sample. This way
    all GPUs will have the same number of prompts, and you can then gather the results.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而*，如果我们想对*所有GPU*的结果做一些处理怎么办？（比如收集它们并执行某种后处理）您可以传递 `apply_padding=True` 来确保提示列表填充到相同的长度，额外的数据将从最后一个样本中获取。这样所有GPU将有相同数量的提示，然后您可以收集结果。'
- en: This is only needed when trying to perform an action such as gathering the results,
    where the data on each device needs to be the same length. Basic inference does
    not require this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在尝试执行诸如收集结果之类的操作时才需要这样做，其中每个设备上的数据需要具有相同的长度。基本推理不需要这样做。
- en: 'For instance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On the first GPU, the prompts will be `["a dog", "a cat"]`, and on the second
    GPU it will be `["a chicken", "a chicken"]`. Make sure to drop the final sample,
    as it will be a duplicate of the previous one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个GPU上，提示将是 `["a dog", "a cat"]`，在第二个GPU上将是 `["a chicken", "a chicken"]`。确保丢弃最后一个样本，因为它将是前一个样本的副本。
- en: Memory-efficient pipeline parallelism (experimental)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存高效的管道并行（实验性）
- en: This next part will discuss using *pipeline parallelism*. This is an **experimental**
    API utilizing the [PiPPy library by PyTorch](https://github.com/pytorch/PiPPy/)
    as a native solution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将讨论使用*管道并行处理*。这是一个**实验性** API，利用了 [PyTorch 的 PiPPy 库](https://github.com/pytorch/PiPPy/)
    作为本地解决方案。
- en: 'The general idea with pipeline parallelism is: say you have 4 GPUs and a model
    big enough it can be *split* on four GPUs using `device_map="auto"`. With this
    method you can send in 4 inputs at a time (for example here, any amount works)
    and each model chunk will work on an input, then receive the next input once the
    prior chunk finished, making it *much* more efficient **and faster** than the
    method described earlier. Here’s a visual taken from the PyTorch repository:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行处理的一般思想是：假设您有 4 个 GPU 和一个足够大的模型，可以使用 `device_map="auto"` 在四个 GPU 上进行*分割*。使用这种方法，您可以一次发送
    4 个输入（例如在这里，任意数量都可以），每个模型块将处理一个输入，然后在前一个块完成后接收下一个输入，使其比之前描述的方法*更*高效 **和更快**。这里有一个来自
    PyTorch 仓库的可视化：
- en: '![PiPPy example](../Images/0d96d7f73aed95a927cf10c18cd407b8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![PiPPy 示例](../Images/0d96d7f73aed95a927cf10c18cd407b8.png)'
- en: To illustrate how you can use this with Accelerate, we have created an [example
    zoo](https://github.com/huggingface/accelerate/tree/main/examples/inference) showcasing
    a number of different models and situations. In this tutorial, we’ll show this
    method for GPT2 across two GPUs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明您如何在 Accelerate 中使用这个功能，我们创建了一个 [示例动物园](https://github.com/huggingface/accelerate/tree/main/examples/inference)，展示了许多不同的模型和情况。在本教程中，我们将展示如何在两个
    GPU 上为 GPT2 使用这种方法。
- en: 'Before you proceed, please make sure you have the latest pippy installed by
    running the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已安装了最新的 pippy，方法是运行以下命令：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We require at least version 0.2.0\. To confirm that you have the correct version,
    run `pip show torchpippy`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至少需要版本 0.2.0。要确认您安装了正确的版本，请运行 `pip show torchpippy`。
- en: 'Start by creating the model on the CPU:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在 CPU 上创建模型：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next you’ll need to create some example inputs to use. These help PiPPy trace
    the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要创建一些示例输入以供使用。这有助于 PiPPy 追踪模型。
- en: However you make this example will determine the relative batch size that will
    be used/passed through the model at a given time, so make sure to remember how
    many items there are!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您制作这个示例的方式将决定在给定时间通过模型传递的相对批次大小，因此请记住有多少项！
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next we need to actually perform the tracing and get the model ready. To do
    so, use the [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)
    function and it will fully wrap the model for pipeline parallelism automatically:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要实际执行追踪并准备好模型。为此，请使用 [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)
    函数，它将自动完全包装模型以进行管道并行处理：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are a variety of parameters you can pass through to `prepare_pippy`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 `prepare_pippy` 传递各种参数：
- en: '`split_points` lets you determine what layers to split the model at. By default
    we use wherever `device_map="auto" declares, such as` fc`or`conv1`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_points` 让您确定在哪些层上拆分模型。默认情况下，我们使用 `device_map="auto"` 声明的位置，例如 `fc` 或
    `conv1`。'
- en: '`num_chunks` determines how the batch will be split and sent to the model itself
    (so `num_chunks=1` with four split points/four GPUs will have a naive MP where
    a single input gets passed between the four layer split points)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_chunks` 决定了批次将如何分割并发送到模型本身（因此 `num_chunks=1`，有四个分割点/四个 GPU 将具有一个简单的 MP，其中单个输入在四个层分割点之间传递）'
- en: From here, all that’s left is to actually perform the distributed inference!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，剩下的就是实际执行分布式推理！
- en: When passing inputs, we highly recommend to pass them in as a tuple of arguments.
    Using `kwargs` is supported, however, this approach is experimental.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递输入时，我们强烈建议将它们作为参数元组传递。支持使用 `kwargs`，但这种方法是实验性的。
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When finished all the data will be on the last process only:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，所有数据将仅在最后一个进程上：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you pass in `gather_output=True` to [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy),
    the output will be sent across to all the GPUs afterwards without needing the
    `is_last_process` check. This is `False` by default as it incurs a communication
    call.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将 `gather_output=True` 传递给 [inference.prepare_pippy()](/docs/accelerate/v0.27.2/en/package_reference/inference#accelerate.prepare_pippy)，输出将在之后发送到所有
    GPU，无需进行 `is_last_process` 检查。默认情况下为 `False`，因为这会产生通信调用。
- en: And that’s it! To explore more, please check out the inference examples in the
    [Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples/inference)
    and our [documentation](../package_reference/inference) as we work to improving
    this integration.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！要了解更多，请查看 [Accelerate 仓库](https://github.com/huggingface/accelerate/tree/main/examples/inference)
    中的推理示例和我们的 [文档](../package_reference/inference)，因为我们正在努力改进这个集成。
