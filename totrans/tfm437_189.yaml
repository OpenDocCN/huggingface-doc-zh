- en: Longformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Longformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/longformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/longformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/longformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/longformer)
- en: '[![Models](../Images/9b11f1f7f11b12c8a363e62c0873b10a.png)](https://huggingface.co/models?filter=longformer)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/longformer-base-4096-finetuned-squadv1)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![模型](../Images/9b11f1f7f11b12c8a363e62c0873b10a.png)](https://huggingface.co/models?filter=longformer)
    [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/longformer-base-4096-finetuned-squadv1)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Longformer model was presented in [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)
    by Iz Beltagy, Matthew E. Peters, Arman Cohan.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Longformer模型在[Iz Beltagy, Matthew E. Peters, Arman Cohan的《Longformer: The Long-Document
    Transformer》](https://arxiv.org/pdf/2004.05150.pdf)中提出。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Transformer-based models are unable to process long sequences due to their
    self-attention operation, which scales quadratically with the sequence length.
    To address this limitation, we introduce the Longformer with an attention mechanism
    that scales linearly with sequence length, making it easy to process documents
    of thousands of tokens or longer. Longformer’s attention mechanism is a drop-in
    replacement for the standard self-attention and combines a local windowed attention
    with a task motivated global attention. Following prior work on long-sequence
    transformers, we evaluate Longformer on character-level language modeling and
    achieve state-of-the-art results on text8 and enwik8\. In contrast to most prior
    work, we also pretrain Longformer and finetune it on a variety of downstream tasks.
    Our pretrained Longformer consistently outperforms RoBERTa on long document tasks
    and sets new state-of-the-art results on WikiHop and TriviaQA.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于Transformer的模型由于其自注意力操作而无法处理长序列，该操作随着序列长度呈二次方增长。为了解决这一限制，我们引入了Longformer，其注意力机制与序列长度呈线性增长，使其能够轻松处理数千个标记或更长文档。Longformer的注意力机制是标准自注意力的替代品，结合了局部窗口注意力和任务驱动的全局注意力。在长序列Transformer的先前工作基础上，我们在字符级语言建模上评估Longformer，并在text8和enwik8上取得了最新的成果。与大多数先前的工作相比，我们还对Longformer进行了预训练，并在各种下游任务上进行了微调。我们的预训练Longformer在长文档任务上始终优于RoBERTa，并在WikiHop和TriviaQA上取得了最新的成果。*'
- en: This model was contributed by [beltagy](https://huggingface.co/beltagy). The
    Authors’ code can be found [here](https://github.com/allenai/longformer).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[beltagy](https://huggingface.co/beltagy)贡献。作者的代码可以在[此处](https://github.com/allenai/longformer)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Since the Longformer is based on RoBERTa, it doesn’t have `token_type_ids`.
    You don’t need to indicate which token belongs to which segment. Just separate
    your segments with the separation token `tokenizer.sep_token` (or `</s>`).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Longformer基于RoBERTa，它没有`token_type_ids`。您不需要指示哪个标记属于哪个段。只需使用分隔标记`tokenizer.sep_token`（或`</s>`）分隔您的段。
- en: A transformer model replacing the attention matrices by sparse matrices to go
    faster. Often, the local context (e.g., what are the two tokens left and right?)
    is enough to take action for a given token. Some preselected input tokens are
    still given global attention, but the attention matrix has way less parameters,
    resulting in a speed-up. See the local attention section for more information.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将注意力矩阵替换为稀疏矩阵以加快速度的变压器模型。通常，局部上下文（例如，左右两个标记是什么？）足以为给定标记采取行动。仍然会给定一些预选输入标记全局关注，但是注意力矩阵的参数要少得多，从而加快速度。有关更多信息，请参阅局部注意力部分。
- en: Longformer Self Attention
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Longformer自注意力
- en: Longformer self attention employs self attention on both a “local” context and
    a “global” context. Most tokens only attend “locally” to each other meaning that
    each token attends to its<math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>w</mi></mrow><annotation
    encoding="application/x-tex">\frac{1}{2} w</annotation></semantics></math>21​w
    previous tokens and<math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>w</mi></mrow><annotation
    encoding="application/x-tex">\frac{1}{2} w</annotation></semantics></math>21​w
    succeeding tokens with<math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math>w
    being the window length as defined in `config.attention_window`. Note that `config.attention_window`
    can be of type `List` to define a different<math><semantics><mrow><mi>w</mi></mrow><annotation
    encoding="application/x-tex">w</annotation></semantics></math>w for each layer.
    A selected few tokens attend “globally” to all other tokens, as it is conventionally
    done for all tokens in `BertSelfAttention`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer自注意力同时在“局部”上下文和“全局”上下文上使用自注意力。大多数标记仅“局部”地相互关注，这意味着每个标记只关注其前<math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>w</mi></mrow><annotation
    encoding="application/x-tex">\frac{1}{2} w</annotation></semantics></math>21​w个标记和后<math><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>w</mi></mrow><annotation
    encoding="application/x-tex">\frac{1}{2} w</annotation></semantics></math>21​w个标记，其中<math><semantics><mrow><mi>w</mi></mrow><annotation
    encoding="application/x-tex">w</annotation></semantics></math>w是在`config.attention_window`中定义的窗口长度。请注意，`config.attention_window`可以是`List`类型，以定义每个层的不同<math><semantics><mrow><mi>w</mi></mrow><annotation
    encoding="application/x-tex">w</annotation></semantics></math>w。少数选定的标记对所有其他标记进行“全局”关注，就像在`BertSelfAttention`中为所有标记惯例上所做的那样。
- en: Note that “locally” and “globally” attending tokens are projected by different
    query, key and value matrices. Also note that every “locally” attending token
    not only attends to tokens within its window<math><semantics><mrow><mi>w</mi></mrow><annotation
    encoding="application/x-tex">w</annotation></semantics></math>w, but also to all
    “globally” attending tokens so that global attention is *symmetric*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“局部”和“全局”关注的标记由不同的查询、键和值矩阵进行投影。还请注意，每个“局部”关注的标记不仅关注其窗口内的标记<math><semantics><mrow><mi>w</mi></mrow><annotation
    encoding="application/x-tex">w</annotation></semantics></math>w，还关注所有“全局”关注的标记，以使全局关注是*对称*的。
- en: 'The user can define which tokens attend “locally” and which tokens attend “globally”
    by setting the tensor `global_attention_mask` at run-time appropriately. All Longformer
    models employ the following logic for `global_attention_mask`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过在运行时适当设置张量`global_attention_mask`来定义哪些令牌“局部”关注，哪些令牌“全局”关注。所有Longformer模型都采用以下逻辑来处理`global_attention_mask`：
- en: '0: the token attends “locally”,'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0：令牌“局部”关注，
- en: '1: the token attends “globally”.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1：令牌“全局”关注。
- en: For more information please also refer to [forward()](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel.forward)
    method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考[forward()](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel.forward)方法。
- en: Using Longformer self attention, the memory and time complexity of the query-key
    matmul operation, which usually represents the memory and time bottleneck, can
    be reduced from<math><semantics><mrow><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><msub><mi>n</mi><mi>s</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times n_s)</annotation></semantics></math>O(ns​×ns​) to<math><semantics><mrow><mi
    mathvariant="script">O</mi><mo stretchy="false">(</mo><msub><mi>n</mi><mi>s</mi></msub><mo>×</mo><mi>w</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{O}(n_s
    \times w)</annotation></semantics></math>O(ns​×w), with<math><semantics><mrow><msub><mi>n</mi><mi>s</mi></msub></mrow><annotation
    encoding="application/x-tex">n_s</annotation></semantics></math>ns​ being the
    sequence length and<math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math>w
    being the average window size. It is assumed that the number of “globally” attending
    tokens is insignificant as compared to the number of “locally” attending tokens.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Longformer自注意力机制，通常代表内存和时间瓶颈的查询-键matmul操作的内存和时间复杂度可以从O(ns×ns)降低到O(ns×w)，其中ns是序列长度，w是平均窗口大小。假设“全局”关注的令牌数量与“局部”关注的令牌数量相比微不足道。
- en: For more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考官方[论文](https://arxiv.org/pdf/2004.05150.pdf)。
- en: Training
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '[LongformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMaskedLM)
    is trained the exact same way [RobertaForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaForMaskedLM)
    is trained and should be used as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMaskedLM)的训练方式与[RobertaForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaForMaskedLM)完全相同，应该如下使用：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Token classification task guide](../tasks/token_classification)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[令牌分类任务指南](../tasks/token_classification)'
- en: '[Question answering task guide](../tasks/question_answering)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[问答任务指南](../tasks/question_answering)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[遮蔽语言建模任务指南](../tasks/masked_language_modeling)'
- en: '[Multiple choice task guide](../tasks/multiple_choice)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多项选择任务指南](../tasks/multiple_choice)'
- en: LongformerConfig
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerConfig
- en: '### `class transformers.LongformerConfig`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/configuration_longformer.py#L46)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/configuration_longformer.py#L46)'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Longformer model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)
    or [TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为30522）— Longformer模型的词汇量。定义了在调用[LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)或[TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel)时可以由`inputs_ids`表示的不同令牌数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`（`int`，*可选*，默认为768）— 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers`（`int`，*可选*，默认为12）— Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads`（`int`，*可选*，默认为12）— Transformer编码器中每个注意力层的注意力头数量。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size`（`int`，*可选*，默认为3072）— Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act`（`str`或`Callable`，*可选*，默认为`"gelu"`）— 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob`（`float`，*可选*，默认为0.1）— 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)
    or [TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用[LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)或[TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel)时传递的`token_type_ids`的词汇大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。'
- en: '`attention_window` (`int` or `List[int]`, *optional*, defaults to 512) — Size
    of an attention window around each token. If an `int`, use the same size for all
    layers. To specify a different window size for each layer, use a `List[int]` where
    `len(attention_window) == num_hidden_layers`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_window` (`int` or `List[int]`, *optional*, defaults to 512) — 每个标记周围的注意力窗口大小。如果是`int`，则对所有层使用相同大小。要为每个层指定不同的窗口大小，请使用`List[int]`，其中`len(attention_window)
    == num_hidden_layers`。'
- en: This is the configuration class to store the configuration of a [LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)
    or a [TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel).
    It is used to instantiate a Longformer model according to the specified arguments,
    defining the model architecture.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)或[TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel)配置的配置类。它用于根据指定的参数实例化Longformer模型，定义模型架构。
- en: This is the configuration class to store the configuration of a [LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel).
    It is used to instantiate an Longformer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LongFormer [allenai/longformer-base-4096](https://huggingface.co/allenai/longformer-base-4096)
    architecture with a sequence length 4,096.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)配置的配置类。它用于根据指定的参数实例化Longformer模型，定义模型架构。使用默认值实例化配置将产生与LongFormer
    [allenai/longformer-base-4096](https://huggingface.co/allenai/longformer-base-4096)架构相似的配置，序列长度为4,096。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: LongformerTokenizer
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerTokenizer
- en: '### `class transformers.LongformerTokenizer`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L116)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L116)'
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors` (`str`, *optional*, defaults to `"replace"`) — 解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，在从多个序列构建序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, 默认为 `"<s>"`) — 在进行序列分类（整个序列而不是每个标记的分类）时使用的分类器标记。这是构建带有特殊标记的序列时的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, 默认为 `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (Longformer tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *optional*, 默认为 `False`) — 是否在输入前添加一个初始空格。这允许将前导单词视为任何其他单词。（Longformer分词器通过前面的空格检测单词的开头）。'
- en: Constructs a Longformer tokenizer, derived from the GPT-2 tokenizer, using byte-level
    Byte-Pair-Encoding.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个Longformer分词器，从GPT-2分词器派生，使用字节级字节对编码。
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器已经训练成将空格视为标记的一部分（有点像sentencepiece），因此一个单词将
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据句子开头是否有空格，将被编码为不同的方式：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在实例化此分词器时或在对某些文本调用它时传递`add_prefix_space=True`来避免这种行为，但由于模型不是以这种方式进行预训练的，因此可能会导致性能下降。
- en: When used with `is_split_into_words=True`, this tokenizer will add a space before
    each word (even the first one).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当与`is_split_into_words=True`一起使用时，此分词器将在每个单词之前添加一个空格（甚至是第一个单词）。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L363)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L363)'
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 要添加特殊标记的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊标记的[input IDs](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A Longformer sequence has the
    following format:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊标记从序列或序列对构建用于序列分类任务的模型输入。Longformer序列的格式如下：
- en: 'single sequence: `<s> X </s>`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`<s> X </s>`
- en: 'pair of sequences: `<s> A </s></s> B </s>`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`<s> A </s></s> B </s>`
- en: '#### `convert_tokens_to_string`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L328)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L328)'
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L415)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L415)'
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 可选的第二个ID列表，用于序列对。'
- en: Returns
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 零列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. Longformer does not make use of token type ids, therefore a list of zeros
    is returned.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。Longformer不使用token类型ID，因此返回一个零列表。
- en: '#### `get_special_tokens_mask`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L388)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer.py#L388)'
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 可选的第二个ID列表，用于序列对。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — 标记列表是否已经格式化为模型的特殊标记。'
- en: Returns
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围在[0, 1]之间：1表示特殊标记，0表示序列标记。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊标记的标记列表中检索序列ID。在使用分词器`prepare_for_model`方法添加特殊标记时，会调用此方法。
- en: LongformerTokenizerFast
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerTokenizerFast
- en: '### `class transformers.LongformerTokenizerFast`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer_fast.py#L91)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer_fast.py#L91)'
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`）— 词汇表文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file`（`str`）— 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors`（`str`，*可选*，默认为`"replace"`）— 解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*可选*，默认为`"<s>"`）— 在预训练期间使用的序列开始标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。使用的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*可选*，默认为`"</s>"`）— 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。使用的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*可选*，默认为`"</s>"`）— 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`（`str`，*可选*，默认为`"<s>"`）— 在进行序列分类（整个序列的分类，而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*可选*，默认为`"<unk>"`）— 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*可选*，默认为`"<pad>"`）— 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`（`str`，*可选*，默认为`"<mask>"`）— 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) — Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (Longformer tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space`（`bool`，*可选*，默认为`False`）— 是否在输入中添加初始空格。这允许将前导单词视为任何其他单词。（Longformer分词器通过前面的空格检测单词的开头）。'
- en: '`trim_offsets` (`bool`, *optional*, defaults to `True`) — Whether the post
    processing step should trim offsets to avoid including whitespaces.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trim_offsets`（`bool`，*可选*，默认为`True`）— 后处理步骤是否应该修剪偏移量以避免包含空格。'
- en: Construct a “fast” Longformer tokenizer (backed by HuggingFace’s *tokenizers*
    library), derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”Longformer分词器（由HuggingFace的*tokenizers*库支持），派生自GPT-2分词器，使用字节级字节对编码。
- en: This tokenizer has been trained to treat spaces like parts of the tokens (a
    bit like sentencepiece) so a word will
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器已经训练过，将空格视为标记的一部分（有点像sentencepiece），所以一个单词会
- en: 'be encoded differently whether it is at the beginning of the sentence (without
    space) or not:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子开头（没有空格）或不是时，可能会以不同方式编码：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can get around that behavior by passing `add_prefix_space=True` when instantiating
    this tokenizer or when you call it on some text, but since the model was not pretrained
    this way, it might yield a decrease in performance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在实例化此分词器时或在对某些文本调用它时传递`add_prefix_space=True`来避免这种行为，但由于模型不是以这种方式进行预训练的，可能会导致性能下降。
- en: When used with `is_split_into_words=True`, this tokenizer needs to be instantiated
    with `add_prefix_space=True`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当与`is_split_into_words=True`一起使用时，需要使用`add_prefix_space=True`来实例化此分词器。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer_fast.py#L308)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/tokenization_longformer_fast.py#L308)'
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *optional*) — 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 零列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. Longformer does not make use of token type ids, therefore a list of zeros
    is returned.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。Longformer不使用令牌类型id，因此返回一个零列表。
- en: Longformer specific outputs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Longformer特定输出
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L55)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L55)'
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 每一层的`torch.FloatTensor`元组，形状为`(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`，其中`x`是具有全局注意力掩码的令牌数量。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余的`attention_window`）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌到自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）的`attention_window / 2`个值是到前（后）的令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则到`attentions`中的所有其他令牌的注意力权重设置为0，该值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 每一层的`torch.FloatTensor`元组，形状为`(batch_size, num_heads, sequence_length, x)`，其中`x`是具有全局注意力掩码的令牌数量。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自具有全局注意力的每个令牌到序列中每个令牌的注意力权重。
- en: Base class for Longformer’s outputs, with potential hidden states, local and
    global attentions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer输出的基类，具有潜在的隐藏状态、局部和全局注意力。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L98)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L98)'
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    further processed by a Linear layer and a Tanh activation function. The Linear
    layer weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 序列第一个令牌（分类令牌）的最后一层隐藏状态，进一步由线性层和Tanh激活函数处理。线性层的权重是通过预训练期间的下一个句子预测（分类）目标进行训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的本地注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个令牌到具有全局注意力的每个令牌（前`x`个值）以及到注意力窗口中的每个令牌的注意力权重（剩余的`attention_window`）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个令牌到序列中的每个令牌的注意力权重。
- en: Base class for Longformer’s outputs that also contains a pooling of the last
    hidden states.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer输出的基类，还包含最后隐藏状态的池化。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput`'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L146)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L146)'
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）—
    语言建模头的预测分数（SoftMax之前每个词汇令牌的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax后的本地注意力权重。这些是从序列中的每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余`attention_window`个值）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；应从前`x`个注意力权重中访问该值。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问该值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax后的全局注意力权重。这些是从具有全局注意力的每个令牌到序列中的每个令牌的注意力权重。
- en: Base class for masked language models outputs.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 用于掩码语言模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L192)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L192)'
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 总跨度抽取损失是开始和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度开始得分（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 跨度结束得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出的一个和每层输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax后的本地注意力权重。这些是从序列中的每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余`attention_window`个值）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌与自身的注意力权重位于索引`x
    + attention_window / 2`处，前（后）`attention_window / 2`个值是指与前（后）`attention_window
    / 2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中获取。如果一个令牌具有全局注意力，则该令牌对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中获取值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是每个令牌与序列中每个令牌的全局注意力的注意力权重。
- en: Base class for outputs of question answering Longformer models.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于问答Longformer模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L241)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L241)'
- en: '[PRE16]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出和每一层输出各一个）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是序列中每个令牌与具有全局注意力的每个令牌（前`x`个值）以及注意力窗口中的每个令牌的注意力权重（剩余的`attention_window`值）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌与自身的注意力权重位于索引`x
    + attention_window / 2`处，前（后）`attention_window / 2`个值是指与前（后）`attention_window
    / 2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中获取。如果一个令牌具有全局注意力，则该令牌对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中获取值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自每个令牌的全局注意力到序列中每个令牌的注意力权重。
- en: Base class for outputs of sentence classification models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L287)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L287)'
- en: '[PRE17]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.FloatTensor`，形状为*(1,)*，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）— *num_choices*是输入张量的第二维度。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类得分（SoftMax之前）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（嵌入输出和每层输出各一个）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌的注意力权重，分别对全局注意力的每个令牌（前`x`个值）和注意力窗口中的每个令牌进行注意力。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；值应从第一个`x`注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自每个令牌的全局注意力到序列中每个令牌的注意力权重。
- en: Base class for outputs of multiple choice Longformer models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 多选Longformer模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L335)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L335)'
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — 分类得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个标记到具有全局注意力的每个标记（前`x`个值）以及到注意力窗口中的每个标记的注意力权重（剩余的`attention_window`）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：第一个`x`个值是指文本中具有固定位置的标记，但剩余的`attention_window + 1`个值是指具有相对位置的标记：标记到自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）面的`attention_window / 2`个值是指到前（后）面的标记的注意力权重。如果注意力窗口包含具有全局注意力的标记，则相应索引处的注意力权重设置为0；值应从第一个`x`个注意力权重中获取。如果一个标记具有全局注意力，则该标记到`attentions`中的所有其他标记的注意力权重设置为0，值应从`global_attentions`中获取。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中的每个标记的注意力权重。
- en: Base class for outputs of token classification models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 用于标记分类模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutput`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L68)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L68)'
- en: '[PRE19]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — 模型最后一层的隐藏状态序列输出。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个标记到具有全局注意力的每个标记（前`x`个值）以及到注意力窗口中的每个标记的注意力权重（剩余的`attention_window`）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是指对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全局注意力在注意力softmax之后的权重，用于计算自注意力头中的加权平均值。这些是来自具有全局注意力的每个令牌到序列中每个令牌的注意力权重。
- en: Base class for Longformer’s outputs, with potential hidden states, local and
    global attentions.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer输出的基类，具有潜在的隐藏状态，本地和全局注意力。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling`'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerBaseModelOutputWithPooling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L111)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L111)'
- en: '[PRE20]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`）- 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和Tanh激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本地注意力在注意力softmax之后的权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和注意力窗口中每个令牌的注意力权重（剩余`attention_window`个值）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是指对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力SoftMax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中的每个标记的注意力权重。
- en: Base class for Longformer’s outputs that also contains a pooling of the last
    hidden states.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer输出的基类，还包含最后隐藏状态的池化。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L159)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L159)'
- en: '[PRE21]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Masked language modeling (MLM) loss.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，在提供`labels`时返回）- 掩蔽语言建模（MLM）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）-
    语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力SoftMax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个标记到具有全局注意力的每个标记（前`x`个值）以及到注意力窗口中的每个标记的注意力权重（剩余的`attention_window`）。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：第一个`x`个值指的是文本中固定位置的标记，但剩余的`attention_window + 1`个值指的是具有相对位置的标记：标记到自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）面的`attention_window / 2`个值是到前（后）面的标记的注意力权重。如果注意力窗口包含具有全局注意力的标记，则相应索引处的注意力权重设置为0；应从第一个`x`个注意力权重中访问该值。如果一个标记具有全局注意力，则该标记到`attentions`中的所有其他标记的注意力权重设置为0，应从`global_attentions`中访问该值。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力SoftMax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中的每个标记的注意力权重。
- en: Base class for masked language models outputs.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 用于掩蔽语言模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput`'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L205)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L205)'
- en: '[PRE22]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，在提供`labels`时返回）- 总跨度提取损失是起始位置和结束位置的交叉熵之和。'
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-start
    scores (before SoftMax).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 跨度开始得分（SoftMax之前）。'
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-end
    scores (before SoftMax).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 跨度结束得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。 '
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在注意力SoftMax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余`attention_window`个值）的注意力权重。 '
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力SoftMax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自具有全局注意力的每个令牌到序列中每个令牌的注意力权重。
- en: Base class for outputs of question answering Longformer models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 用于问答Longformer模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L254)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L254)'
- en: '[PRE23]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(1,)`，*可选*，当提供`labels`时返回） — 分类（如果config.num_labels==1则为回归）损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自每个令牌的全局注意力到序列中每个令牌的注意力权重。
- en: Base class for outputs of sentence classification models.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分类模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput`'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L300)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L300)'
- en: '[PRE24]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor`，形状为*(1,)*，*可选的*，当提供`labels`时返回) — 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor`，形状为`(batch_size, num_choices)`) — *num_choices*是输入张量的第二维度。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类得分（SoftMax之前）。
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则对`attentions`中的所有其他令牌的注意力权重设置为0，值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中每个标记的注意力权重。
- en: Base class for outputs of multiple choice models.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 多选模型输出的基类。
- en: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput`'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L348)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L348)'
- en: '[PRE25]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`tf.Tensor`）—
    分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（嵌入输出的一个和每层输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层输出的模型隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是序列中每个标记到具有全局注意力的每个标记（前`x`个值）和到注意力窗口中的每个标记（剩余`attention_window`个值）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。请注意，前`x`个值是指文本中固定位置的标记，但剩余的`attention_window + 1`个值是指相对位置的标记：标记自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）的`attention_window / 2`个值是指前（后）的标记的注意力权重。如果注意力窗口包含具有全局注意力的标记，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个标记具有全局注意力，则`attentions`中对所有其他标记的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`tf.Tensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中每个标记的注意力权重。
- en: Base class for outputs of token classification models.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 标记分类模型输出的基类。
- en: PytorchHide Pytorch content
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: LongformerModel
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerModel
- en: '### `class transformers.LongformerModel`'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1521)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1521)'
- en: '[PRE26]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Longformer Model outputting raw hidden-states without any specific
    head on top.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 Longformer 模型输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。 '
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: 'This class copied code from [RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)
    and overwrote standard self-attention with longformer self-attention to provide
    the ability to process long sequences following the self-attention approach described
    in [Longformer: the Long-Document Transformer](https://arxiv.org/abs/2004.05150)
    by Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer self-attention combines
    a local (sliding window) and global attention to extend to long documents without
    the O(n^2) increase in memory and compute.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '这个类从[RobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaModel)复制了代码，并用
    Longformer 自注意力覆盖了标准的自注意力，以提供处理长序列的能力，遵循[Longformer: 长文档 Transformer](https://arxiv.org/abs/2004.05150)中描述的自注意力方法，作者是
    Iz Beltagy、Matthew E. Peters 和 Arman Cohan。Longformer 自注意力结合了局部（滑动窗口）和全局注意力，以扩展到长文档，而不会导致内存和计算的
    O(n^2) 增加。'
- en: The self-attention module `LongformerSelfAttention` implemented here supports
    the combination of local and global attention but it lacks support for autoregressive
    attention and dilated attention. Autoregressive and dilated attention are more
    relevant for autoregressive language modeling than finetuning on downstream tasks.
    Future release will add support for autoregressive attention, but the support
    for dilated attention requires a custom CUDA kernel to be memory and compute efficient.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实现的自注意力模块`LongformerSelfAttention`支持局部和全局注意力的组合，但不支持自回归注意力和扩张注意力。自回归和扩张注意力对于自回归语言建模比微调下游任务更相关。未来的版本将添加对自回归注意力的支持，但对扩张注意力的支持需要自定义
    CUDA 内核以提高内存和计算效率。
- en: '#### `forward`'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1638)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1638)'
- en: '[PRE27]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）
    — 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间。'
- en: 1 for tokens that are `not masked`,
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to decide the attention given on each token, local attention
    or global attention. Tokens with global attention attends to all other tokens,
    and all other tokens attend to them. This is important for task-specific finetuning
    because it makes the model more flexible at representing the task. For example,
    for classification, the ~~token should be given global attention. For QA, all
    question tokens should also have global attention. Please refer to the [Longformer
    paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected
    in `[0, 1]`:~~'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）
    — 决定每个标记上的注意力分配，局部注意力或全局注意力。具有全局注意力的标记会关注所有其他标记，而所有其他标记也会关注它们。这对于任务特定的微调很重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予该标记全局注意力。对于问答，所有问题标记也应该具有全局注意力。请参考[Longformer
    论文](https://arxiv.org/abs/2004.05150)获取更多细节。掩码值选在`[0, 1]`之间：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示全局注意力（关注所有其他标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — 在编码器中，用于使注意力模块的特定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — 在解码器中，用于使注意力模块的特定头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被屏蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被屏蔽。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]`
    中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将
    `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerBaseModelOutputWithPooling)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时），包含根据配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列的输出。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    further processed by a Linear layer and a Tanh activation function. The Linear
    layer weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — 序列第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层和 Tanh 激活函数处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个标记到具有全局注意力的每个标记（前`x`个值）和到注意力窗口中的每个标记（剩余的`attention_window`）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。注意，前`x`个值是指文本中具有固定位置的标记，但剩余的`attention_window + 1`个值是指具有相对位置的标记：标记到自身的注意力权重位于索引`x
    + attention_window / 2`处，前（后）的`attention_window / 2`个值是指到前（后）的`attention_window
    / 2`个标记的注意力权重。如果注意力窗口包含具有全局注意力的标记，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个标记具有全局注意力，则到`attentions`中的所有其他标记的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个标记到序列中的每个标记的注意力权重。
- en: The [LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE28]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: LongformerForMaskedLM
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerForMaskedLM
- en: '### `class transformers.LongformerForMaskedLM`'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1762)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1762)'
- en: '[PRE29]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Longformer Model with a `language modeling` head on top.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有`语言建模`头的Longformer模型。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1781)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1781)'
- en: '[PRE30]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是input IDs？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是attention masks？](../glossary#attention-mask)'
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to decide the attention given on each token, local attention
    or global attention. Tokens with global attention attends to all other tokens,
    and all other tokens attend to them. This is important for task-specific finetuning
    because it makes the model more flexible at representing the task. For example,
    for classification, the ~~token should be given global attention. For QA, all
    question tokens should also have global attention. Please refer to the [Longformer
    paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected
    in `[0, 1]`:~~'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于决定每个标记的注意力分配，局部注意力或全局注意力。具有全局注意力的标记关注所有其他标记，所有其他标记也关注它们。这对于任务特定的微调很重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予~~标记全局注意力。对于问答，所有问题标记也应该具有全局注意力。请参考[Longformer
    paper](https://arxiv.org/abs/2004.05150)获取更多详细信息。掩码值选在`[0, 1]`之间：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示全局注意力（关注所有其他标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — 用于使编码器中的注意力模块中的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩盖`，
- en: 0 indicates the head is `masked`.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部`被掩盖`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — 用于使解码器中的注意力模块中的选定头部失效的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩盖`，
- en: 0 indicates the head is `masked`.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部`被掩盖`。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选在`[0, 1]`之间：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围在`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是position IDs？](../glossary#position-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用于计算掩盖语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩盖），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的标记'
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Used to hide legacy
    arguments that have been deprecated.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, any]`，可选，默认为*{}*）— 用于隐藏已被弃用的旧参数。'
- en: Returns
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）—
    语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个标记到具有全局注意力的每个标记（前`x`个值）以及到注意力窗口中的每个标记（剩余的`attention_window`值）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：第一个`x`个值是指文本中具有固定位置的标记，但剩余的`attention_window + 1`个值是指具有相对位置的标记：一个标记到自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是指到前（后）`attention_window /
    2`个标记的注意力权重。如果注意力窗口包含具有全局注意力的标记，则相应索引处的注意力权重设置为0；该值应从第一个`x`个注意力权重中获取。如果一个标记具有全局注意力，则到`attentions`中所有其他标记的注意力权重设置为0，该值应从`global_attentions`中获取。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的标记数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是从具有全局注意力的每个标记到序列中的每个标记的注意力权重。
- en: The [LongformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Mask filling example:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 填充掩码示例：
- en: '[PRE31]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let’s try a very long input.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个非常长的输入。
- en: '[PRE32]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: LongformerForSequenceClassification
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerForSequenceClassification
- en: '### `class transformers.LongformerForSequenceClassification`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1870)'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1870)'
- en: '[PRE33]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Longformer Model transformer with a sequence classification/regression head
    on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer模型变换器，顶部带有一个序列分类/回归头（池化输出的顶部的线性层），例如用于GLUE任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1889)'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1889)'
- en: '[PRE34]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获得。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的蒙版。蒙版值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-484
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力蒙版？](../glossary#attention-mask)'
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to decide the attention given on each token, local attention
    or global attention. Tokens with global attention attends to all other tokens,
    and all other tokens attend to them. This is important for task-specific finetuning
    because it makes the model more flexible at representing the task. For example,
    for classification, the ~~token should be given global attention. For QA, all
    question tokens should also have global attention. Please refer to the [Longformer
    paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected
    in `[0, 1]`:~~'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于决定每个标记上的注意力的蒙版，局部注意力或全局注意力。具有全局注意力的标记会关注所有其他标记，而所有其他标记会关注它们。这对于特定任务微调很重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予全局关注。对于问答，所有问题标记也应该有全局关注。请参阅[Longformer论文](https://arxiv.org/abs/2004.05150)以获取更多详细信息。蒙版值选择在`[0,
    1]`之间：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示全局注意力（关注所有其他标记的标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`，形状为`(num_layers, num_heads)`，*可选*) — 用于在编码器中使注意力模块的选定头部失效的蒙版。蒙版值选择在`[0,
    1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-490
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩盖`，
- en: 0 indicates the head is `masked`.
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`torch.Tensor`，形状为`(num_layers, num_heads)`，*可选*) — 用于在解码器中使注意力模块的选定头部失效的蒙版。蒙版值选择在`[0,
    1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩盖`，
- en: 0 indicates the head is `masked`.
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是token type IDs？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]`
    中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是position IDs？](../glossary#position-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权来将
    `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算序列分类/回归损失的标签。索引应在
    `[0, ..., config.num_labels - 1]` 中。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果
    `config.num_labels > 1`，则计算分类损失（交叉熵）。'
- en: Returns
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerSequenceClassifierOutput)
    或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供 `labels` 时返回)
    — 分类（如果 `config.num_labels==1` 则为回归）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    分类（如果 `config.num_labels==1` 则为回归）分数（SoftMax 之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递了 `output_hidden_states=True`
    或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length,
    hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递了 `output_attentions=True`
    或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length,
    x + attention_window + 1)` 的 `torch.FloatTensor` 元组（每个层一个），其中 `x` 是具有全局注意力掩码的标记数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力 softmax 之后的本地注意力权重。这些是从序列中的每个标记到具有全局注意力的每个标记（前 `x` 个值）和到注意力窗口中的每个标记（剩余的
    `attention_window`）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌到自身的注意力权重位于索引`x
    + attention_window / 2`，前（后）`attention_window / 2`个值是到前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；应从前`x`个注意力权重中访问该值。如果一个令牌具有全局注意力，则`attentions`中对所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自具有全局注意力的每个令牌到序列中每个令牌的注意力权重。
- en: The [LongformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example of single-label classification:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 单标签分类示例：
- en: '[PRE35]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Example of multi-label classification:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类示例：
- en: '[PRE36]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: LongformerForMultipleChoice
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerForMultipleChoice
- en: '### `class transformers.LongformerForMultipleChoice`'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerForMultipleChoice`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2222)'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2222)'
- en: '[PRE37]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Longformer Model with a multiple choice classification head on top (a linear
    layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer模型在顶部具有多选分类头（池化输出顶部的线性层和softmax），例如用于RocStories/SWAG任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2240)'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2240)'
- en: '[PRE38]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, num_choices, sequence_length)`)
    — 词汇表中输入序列令牌的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, num_choices, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-541
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示标记是`未掩码`，
- en: 0 for tokens that are `masked`.
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示标记是`掩码`。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask`（形状为`(batch_size, num_choices, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于决定每个标记上给予的注意力，局部注意力或全局注意力。具有全局注意力的标记会关注所有其他标记，而所有其他标记也会关注它们。这对于任务特定的微调非常重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予全局注意力。对于问答，所有问题标记也应该具有全局注意力。有关更多详细信息，请参考[Longformer
    paper](https://arxiv.org/abs/2004.05150)。在`[0, 1]`中选择的掩码值：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-545
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-546
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示全局注意力（关注所有其他标记的标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于在编码器中使注意力模块的选定头部失效的掩码。在`[0,
    1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-548
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`未掩码`。
- en: 0 indicates the head is `masked`.
  id: totrans-549
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`掩码`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于在解码器中使注意力模块的选定头部失效的掩码。在`[0,
    1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部是`未掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-552
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部是`掩码`。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-554
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-555
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是标记类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, num_choices, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the multiple choice classification loss. Indices should be in `[0,
    ..., num_choices-1]` where `num_choices` is the size of the second dimension of
    the input tensors. (See `input_ids` above)'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算多项选择分类损失的标签。索引应在`[0,
    ..., num_choices-1]`范围内，其中`num_choices`是输入张量第二维的大小。（参见上面的`input_ids`）'
- en: Returns
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerMultipleChoiceModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）- *num_choices*是输入张量的第二维。（参见上面的*input_ids*）。'
- en: Classification scores (before SoftMax).
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类得分（SoftMax之前）。
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组，其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是来自序列中每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余的`attention_window`值）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-574
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`，前`attention_window / 2`（后续）值是指到前`attention_window / 2`（后续）个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则到`attentions`中的所有其他令牌的注意力权重设置为0，值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组，其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是来自具有全局注意力的每个令牌到序列中每个令牌的注意力权重。
- en: The [LongformerForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE39]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: LongformerForTokenClassification
  id: totrans-581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerForTokenClassification
- en: '### `class transformers.LongformerForTokenClassification`'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerForTokenClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2134)'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2134)'
- en: '[PRE40]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Longformer Model with a token classification head on top (a linear layer on
    top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer 模型，顶部带有一个标记分类头部（隐藏状态输出顶部的线性层），例如用于命名实体识别（NER）任务。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头部等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2153)'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2153)'
- en: '[PRE41]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选定在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被掩码的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被掩码的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to decide the attention given on each token, local attention
    or global attention. Tokens with global attention attends to all other tokens,
    and all other tokens attend to them. This is important for task-specific finetuning
    because it makes the model more flexible at representing the task. For example,
    for classification, the ~~token should be given global attention. For QA, all
    question tokens should also have global attention. Please refer to the [Longformer
    paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected
    in `[0, 1]`:~~'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.FloatTensor`，*可选*）—
    用于决定每个标记上给予的注意力，局部注意力或全局注意力。具有全局注意力的标记会关注所有其他标记，而所有其他标记会关注它们。这对于任务特定的微调很重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予全局注意力。对于问答，所有问题标记也应该有全局注意力。有关更多详细信息，请参阅[Longformer
    论文](https://arxiv.org/abs/2004.05150)。掩码值选定在 `[0, 1]`：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-602
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于全局注意力（关注所有其他标记的标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为 `(num_layers, num_heads)` 的 `torch.Tensor`，*可选*）— 用于在编码器中使注意力模块中的选定头部失效的掩码。掩码值选定在
    `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码。
- en: 0 indicates the head is `masked`.
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为 `(num_layers, num_heads)` 的 `torch.Tensor`，*可选*）— 用于在解码器中使注意力模块中的选定头部失效的掩码。掩码值选定在
    `[0, 1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-608
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩码，
- en: 0 indicates the head is `masked`.
  id: totrans-609
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩码。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选定在 `[0, 1]`：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-611
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应于 *句子 A* 标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-612
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应于 *句子 B* 标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列令牌的位置索引。选择范围在`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算令牌分类损失的标签。索引应在`[0,
    ..., config.num_labels - 1]`范围内。'
- en: Returns
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput)或者`tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerTokenClassifierOutput)或者一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`时）包括不同的元素，取决于配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification loss.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）—
    分类分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数量。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的本地注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个令牌到具有全局注意力的每个令牌（前`x`个值）以及到注意力窗口中的每个令牌（剩余的`attention_window`）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`values)。请注意，前`x`个值是指文本中具有固定位置的令牌，但剩余的`attention_window + 1`个值是指具有相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`处，前（后）`attention_window / 2`个值是对前（后）`attention_window /
    2`个令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；该值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则`attentions`中对所有其他令牌的注意力权重设置为0，应从`global_attentions`中访问这些值。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—形状为`(batch_size,
    num_heads, sequence_length, x)`的`torch.FloatTensor`元组（每层一个），其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是每个具有全局注意力的令牌对序列中的每个令牌的注意力权重。
- en: The [LongformerForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前后处理步骤，而后者会默默忽略它们。
- en: 'Example:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE42]'
  id: totrans-636
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: LongformerForQuestionAnswering
  id: totrans-637
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LongformerForQuestionAnswering
- en: '### `class transformers.LongformerForQuestionAnswering`'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.LongformerForQuestionAnswering`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1997)'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L1997)'
- en: '[PRE43]'
  id: totrans-640
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Longformer Model with a span classification head on top for extractive question-answering
    tasks like SQuAD / TriviaQA (a linear layers on top of the hidden-states output
    to compute `span start logits` and `span end logits`).
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer模型在顶部具有一个跨度分类头，用于提取式问答任务，如SQuAD / TriviaQA（在隐藏状态输出的顶部添加线性层以计算`span
    start logits`和`span end logits`）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2015)'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_longformer.py#L2015)'
- en: '[PRE44]'
  id: totrans-648
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—词汇表中输入序列令牌的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-654
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-655
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`global_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to decide the attention given on each token, local attention
    or global attention. Tokens with global attention attends to all other tokens,
    and all other tokens attend to them. This is important for task-specific finetuning
    because it makes the model more flexible at representing the task. For example,
    for classification, the ~~token should be given global attention. For QA, all
    question tokens should also have global attention. Please refer to the [Longformer
    paper](https://arxiv.org/abs/2004.05150) for more details. Mask values selected
    in `[0, 1]`:~~'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于决定每个标记上给出的注意力，局部注意力或全局注意力。具有全局注意力的标记关注所有其他标记，所有其他标记也关注它们。这对于任务特定的微调非常重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予~~标记全局注意力。对于问答，所有问题标记也应该具有全局注意力。有关更多详细信息，请参阅[Longformer
    paper](https://arxiv.org/abs/2004.05150)。选择的掩码值为`[0, 1]`：~~'
- en: 0 for local attention (a sliding window attention),
  id: totrans-658
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于局部注意力（滑动窗口注意力），
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-659
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于全局注意力（关注所有其他标记，所有其他标记也关注它们）。
- en: '`head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the encoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于在编码器中使注意力模块中的选定头部失效的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-661
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-662
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*)
    — Mask to nullify selected heads of the attention modules in the decoder. Mask
    values selected in `[0, 1]`:'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(num_layers, num_heads)`的`torch.Tensor`，*可选*）— 用于在解码器中使注意力模块中的选定头部失效的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 indicates the head is `not masked`,
  id: totrans-664
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-665
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被`masked`。
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-667
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 对应一个*句子A*标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-668
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 对应一个*句子B*标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    — Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记跨度开始位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。'
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) —
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记跨度结束位置（索引）的标签。位置被夹紧到序列的长度（`sequence_length`）。超出序列范围的位置不会用于计算损失。'
- en: Returns
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_longformer.LongformerQuestionAnsweringModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig)）和输入而异的各种元素。'
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 总跨度提取损失是起始和结束位置的交叉熵之和。'
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-start scores (before SoftMax).'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`） — 跨度起始分数（SoftMax之前）。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Span-end scores (before SoftMax).'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`） — 跨度结束分数（SoftMax之前）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组，其中一个用于嵌入的输出，另一个用于每一层的输出。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-685
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x + attention_window + 1)`的`torch.FloatTensor`元组，其中`x`是具有全局注意力掩码的令牌数。'
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-687
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的局部注意力权重，用于计算自注意力头中的加权平均值。这些是从序列中的每个令牌到具有全局注意力的每个令牌（前`x`个值）和到注意力窗口中的每个令牌（剩余`attention_window`）的注意力权重。
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-688
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1`值）。请注意，前`x`个值是指文本中固定位置的令牌，但剩余的`attention_window + 1`个值是指相对位置的令牌：令牌对自身的注意力权重位于索引`x
    + attention_window / 2`处，前（后）的`attention_window / 2`个值是指前（后）的令牌的注意力权重。如果注意力窗口包含具有全局注意力的令牌，则相应索引处的注意力权重设置为0；值应从前`x`个注意力权重中访问。如果一个令牌具有全局注意力，则`attentions`中所有其他令牌的注意力权重设置为0，值应从`global_attentions`中访问。
- en: '`global_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, x)`, where `x` is the number of tokens with global attention
    mask.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, x)`的`torch.FloatTensor`元组，其中`x`是具有全局注意力掩码的令牌数。'
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-690
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的全局注意力权重，用于计算自注意力头中的加权平均值。这些是具有全局注意力的每个令牌到序列中的每个令牌的注意力权重。
- en: The [LongformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '[LongformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE45]'
  id: totrans-694
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: TensorFlowHide TensorFlow content
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow内容
- en: TFLongformerModel
  id: totrans-696
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFLongformerModel
- en: '### `class transformers.TFLongformerModel`'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFLongformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2113)'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2113)'
- en: '[PRE46]'
  id: totrans-699
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法来加载模型权重。'
- en: The bare Longformer Model outputting raw hidden-states without any specific
    head on top.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Longformer模型输出原始隐藏状态，没有特定的头部。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档，了解所有与一般使用和行为相关的事项。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入作为关键字参数（类似于PyTorch模型），或者
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有输入作为列表、元组或字典在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该“只需工作” -
    只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个`input_ids`的张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个包含一个或多个与文档字符串中给定输入名称相关联的输入张量的字典：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！
- en: 'This class copies code from [TFRobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.TFRobertaModel)
    and overwrites standard self-attention with longformer self-attention to provide
    the ability to process long sequences following the self-attention approach described
    in [Longformer: the Long-Document Transformer](https://arxiv.org/abs/2004.05150)
    by Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer self-attention combines
    a local (sliding window) and global attention to extend to long documents without
    the O(n^2) increase in memory and compute.'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '这个类从[TFRobertaModel](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.TFRobertaModel)复制代码，并用Longformer自注意力覆盖标准的自注意力，以提供处理长序列的能力，遵循[Iz
    Beltagy, Matthew E. Peters, and Arman Cohan撰写的《Longformer: the Long-Document Transformer》中描述的自注意力方法。Longformer自注意力结合了局部（滑动窗口）和全局注意力，以扩展到长文档，而不会增加O(n^2)的内存和计算量。'
- en: The self-attention module `TFLongformerSelfAttention` implemented here supports
    the combination of local and global attention but it lacks support for autoregressive
    attention and dilated attention. Autoregressive and dilated attention are more
    relevant for autoregressive language modeling than finetuning on downstream tasks.
    Future release will add support for autoregressive attention, but the support
    for dilated attention requires a custom CUDA kernel to be memory and compute efficient.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实现的自注意力模块`TFLongformerSelfAttention`支持局部和全局注意力的组合，但不支持自回归注意力和扩张注意力。自回归和扩张注意力对于自回归语言建模比下游任务的微调更相关。未来的版本将添加对自回归注意力的支持，但对扩张注意力的支持需要一个自定义CUDA内核，以实现内存和计算的高效性。
- en: '#### `call`'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2139)'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2139)'
- en: '[PRE47]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-720
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-721
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-723
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-724
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`np.ndarray`或`tf.Tensor`，形状为`(encoder_layers, encoder_attention_heads)`，*可选*)
    — 用于使注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：'
- en: 1 indicates the head is `not masked`,
  id: totrans-727
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`，
- en: 0 indicates the head is `masked`.
  id: totrans-728
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attention_mask` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 用于决定每个标记上给予的注意力，局部注意力或全局注意力的掩码。具有全局注意力的标记会关注所有其他标记，而所有其他标记会关注它们。这对于任务特定的微调很重要，因为它使模型在表示任务时更加灵活。例如，对于分类，应该给予全局注意力。对于问答，所有问题标记也应该有全局注意力。有关更多详细信息，请参阅[Longformer论文](https://arxiv.org/abs/2004.05150)。在`[0,
    1]`中选择的掩码值：'
- en: 0 for local attention (a sliding window attention),
  id: totrans-730
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于局部注意力（滑动窗口注意力），为0。
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-731
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示全局注意力（关注所有其他标记，所有其他标记也关注它们）。
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：'
- en: 0 corresponds to a *sentence A* token,
  id: totrans-733
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0对应于一个*句子A*的标记，
- en: 1 corresponds to a *sentence B* token.
  id: totrans-734
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1对应于一个*句子B*的标记。
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[令牌类型ID是什么？](../glossary#token-type-ids)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`np.ndarray`或`tf.Tensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。这个参数只能在急切模式下使用，在图模式下，将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。这个参数只能在急切模式下使用，在图模式下，将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。这个参数可以在急切模式下使用，在图模式下，该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFLongformerModel](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: TFLongformerForMaskedLM
  id: totrans-745
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLongformerForMaskedLM`'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2180)'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-748
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longformer Model with a `language modeling` head on top.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2201)'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-767
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-770
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-771
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-772
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-774
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-775
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for local attention (a sliding window attention),
  id: totrans-777
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-778
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-780
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-781
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-782
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-784
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerMaskedLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Masked language modeling (MLM) loss.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-797
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-799
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-800
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-802
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLongformerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-806
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-807
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: TFLongformerForQuestionAnswering
  id: totrans-808
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLongformerForQuestionAnswering`'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2275)'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-811
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longformer Model with a span classification head on top for extractive question-answering
    tasks like SQuAD / TriviaQA (a linear layer on top of the hidden-states output
    to compute `span start logits` and `span end logits`).
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2298)'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-827
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-831
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-833
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-834
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-835
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-837
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-838
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for local attention (a sliding window attention),
  id: totrans-840
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-841
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-843
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-844
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-845
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-847
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the start of the labelled span for computing the token
    classification loss. Positions are clamped to the length of the sequence (*sequence_length*).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels
    for position (index) of the end of the labelled span for computing the token classification
    loss. Positions are clamped to the length of the sequence (*sequence_length*).
    Position outside of the sequence are not taken into account for computing the
    loss.'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerQuestionAnsweringModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Total span extraction loss is the sum of a Cross-Entropy for the start
    and end positions.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-start
    scores (before SoftMax).'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Span-end
    scores (before SoftMax).'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-865
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-867
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLongformerForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: TFLongformerForSequenceClassification
  id: totrans-873
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLongformerForSequenceClassification`'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2454)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longformer Model transformer with a sequence classification/regression head
    on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2473)'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-892
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Parameters
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-895
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-896
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-898
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-899
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-900
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-902
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-903
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for local attention (a sliding window attention),
  id: totrans-905
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-906
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-908
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-909
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-910
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-912
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-924
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-926
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-927
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-929
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLongformerForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-933
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-934
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: TFLongformerForTokenClassification
  id: totrans-935
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLongformerForTokenClassification`'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2694)'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-938
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longformer Model with a token classification head on top (a linear layer on
    top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2717)'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-954
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Parameters
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-957
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-958
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-960
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-961
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-962
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-964
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-965
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Mask to decide the attention given on each token,
    local attention or global attention. Tokens with global attention attends to all
    other tokens, and all other tokens attend to them. This is important for task-specific
    finetuning because it makes the model more flexible at representing the task.
    For example, for classification, the ~~token should be given global attention.
    For QA, all question tokens should also have global attention. Please refer to
    the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details. Mask
    values selected in `[0, 1]`:~~'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for local attention (a sliding window attention),
  id: totrans-967
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-968
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-970
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-971
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-972
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-974
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerTokenClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    — Classification scores (before SoftMax).'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-987
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-989
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-990
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-992
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLongformerForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerForTokenClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-996
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-997
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: TFLongformerForMultipleChoice
  id: totrans-998
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFLongformerForMultipleChoice`'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2568)'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Parameters
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Longformer Model with a multiple choice classification head on top (a linear
    layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/longformer/modeling_tf_longformer.py#L2597)'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-1020
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-1021
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-1023
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-1024
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-1025
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-1027
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-1028
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size,
    num_choices, sequence_length)`, *optional*) — Mask to decide the attention given
    on each token, local attention or global attention. Tokens with global attention
    attends to all other tokens, and all other tokens attend to them. This is important
    for task-specific finetuning because it makes the model more flexible at representing
    the task. For example, for classification, the ~~token should be given global
    attention. For QA, all question tokens should also have global attention. Please
    refer to the [Longformer paper](https://arxiv.org/abs/2004.05150) for more details.
    Mask values selected in `[0, 1]`:~~'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for local attention (a sliding window attention),
  id: totrans-1030
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for global attention (tokens that attend to all other tokens, and all other
    tokens attend to them).
  id: totrans-1031
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Segment token indices to indicate first and second
    portions of the inputs. Indices are selected in `[0, 1]`:'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  id: totrans-1033
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  id: totrans-1034
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-1035
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Indices of positions of each input sequence tokens
    in the position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-1037
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`
    where `num_choices` is the size of the second dimension of the input tensors.
    (See `input_ids` above)'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.models.longformer.modeling_tf_longformer.TFLongformerMultipleChoiceModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LongformerConfig](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.LongformerConfig))
    and inputs.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape *(1,)*, *optional*, returned when `labels` is
    provided) — Classification loss.'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, num_choices)`) — *num_choices*
    is the second dimension of the input tensors. (see *input_ids* above).'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification scores (before SoftMax).
  id: totrans-1049
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-1051
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x + attention_window
    + 1)`, where `x` is the number of tokens with global attention mask.'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token in the sequence to every token with global attention (first `x` values)
    and to every token in the attention window (remaining `attention_window
  id: totrans-1053
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '1`values). Note that the first`x`values refer to tokens with fixed positions
    in the text, but the remaining`attention_window + 1`values refer to tokens with
    relative positions: the attention weight of a token to itself is located at index`x
    + attention_window / 2`and the`attention_window / 2`preceding (succeeding) values
    are the attention weights to the`attention_window / 2`preceding (succeeding) tokens.
    If the attention window contains a token with global attention, the attention
    weight at the corresponding index is set to 0; the value should be accessed from
    the first`x`attention weights. If a token has global attention, the attention
    weights to all other tokens in`attentions`is set to 0, the values should be accessed
    from`global_attentions`.'
  id: totrans-1054
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, x)`, where
    `x` is the number of tokens with global attention mask.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Global attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads. Those are the attention weights from every
    token with global attention to every token in the sequence.
  id: totrans-1056
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFLongformerForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice)
    forward method, overrides the `__call__` special method.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-1060
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
