# K-Diffusion

> 原文：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/k_diffusion`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/k_diffusion)

[k-diffusion](https://github.com/crowsonkb/k-diffusion)是由[Katherine Crowson](https://github.com/crowsonkb/)创建的流行库。我们提供`StableDiffusionKDiffusionPipeline`和`StableDiffusionXLKDiffusionPipeline`，允许您使用 k-diffusion 的采样器运行 Stable DIffusion。

请注意，大多数 k-diffusion 的采样器都在 Diffusers 中实现，我们建议使用现有的调度程序。您可以在 Diffusers 中找到 k-diffusion 采样器和调度程序之间的映射[here](https://huggingface.co/docs/diffusers/api/schedulers/overview)

## StableDiffusionKDiffusionPipeline

### `class diffusers.StableDiffusionKDiffusionPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py#L50)

```py
( vae text_encoder tokenizer unet scheduler safety_checker feature_extractor requires_safety_checker: bool = True )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，以及从潜在表示中解码图像。

+   `text_encoder` (`CLIPTextModel`) — 冻结的文本编码器。 Stable Diffusion 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体来说是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `tokenizer` (`CLIPTokenizer`) — 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `unet` (UNet2DConditionModel) — 有条件的 U-Net 架构，用于去噪编码图像的潜在表示。

+   `scheduler` (SchedulerMixin) — 用于与`unet`结合使用以去噪编码图像潜在表示的调度程序。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成的图像是否可能被视为具有冒犯性或有害性的分类模块。请参考[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)获取详细信息。

+   `feature_extractor` (`CLIPImageProcessor`) — 从生成的图像中提取特征的模型，用作`safety_checker`的输入。

使用 Stable Diffusion 进行文本到图像生成的流程。

此模型继承自 DiffusionPipeline。检查超类文档以获取库为所有流程实现的通用方法（如下载或保存、在特定设备上运行等）。

该流程还继承以下加载方法：

+   load_textual_inversion() 用于加载文本反转嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

这是一个实验性的流程，未来可能会发生变化。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_k_diffusion.py#L181)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）- 要编码的提示设备-（`torch.device`）：torch 设备

+   `num_images_per_prompt`（`int`）- 每个提示应生成的图像数量

+   `do_classifier_free_guidance`（`bool`）- 是否使用无分类器指导。

+   `negative_prompt`（`str`或`List[str]`，*可选*）- 不用来指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）- 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）- 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成 negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）- 将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip`（`int`，*可选*）- 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionXLKDiffusionPipeline

### `class diffusers.StableDiffusionXLKDiffusionPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L92)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True )
```

参数

+   `vae`（AutoencoderKL）- 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder`（`CLIPTextModel`）- 冻结的文本编码器。 Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `text_encoder_2`（`CLIPTextModelWithProjection`）- 第二个冻结的文本编码器。 Stable Diffusion XL 使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)的文本和池部分，具体是[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)变体。

+   `tokenizer`（`CLIPTokenizer`）- 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `tokenizer_2`（`CLIPTokenizer`）- 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的第二个分词器。

+   `unet`（UNet2DConditionModel）- 有条件的 U-Net 架构，用于去噪编码图像潜变量。

+   `scheduler`（SchedulerMixin）- 与`unet`结合使用以去噪编码图像潜变量的调度程序。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

+   `force_zeros_for_empty_prompt`（`bool`，*可选*，默认为`"True"`）- 是否强制负提示嵌入始终设置为 0。还请参阅`stabilityai/stable-diffusion-xl-base-1-0`的配置。

使用 Stable Diffusion XL 和 k-diffusion 进行文本到图像生成的流水线。

此模型继承自 DiffusionPipeline。查看超类文档，了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。

管道还继承以下加载方法：

+   用于加载文本反演嵌入的 load_textual_inversion()

+   用于加载`.ckpt`文件的 from_single_file()

+   用于加载 LoRA 权重的 load_lora_weights()

+   `save_lora_weights()`用于保存 LoRA 权重

+   加载 IP 适配器

#### `disable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L609)

```py
( )
```

如果已启用，则禁用 FreeU 机制。

#### `disable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L208)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L225)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L586)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段 1 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 阶段 2 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 阶段 1 的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 阶段 2 的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L200)

```py
( )
```

启用切片 VAE 解码。当启用此选项时，VAE 将将输入张量分割成片段，以便在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L216)

```py
( )
```

启用平铺 VAE 解码。当启用此选项时，VAE 将将输入张量分割成瓦片，以便在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L233)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 要编码的提示

+   `prompt_2`（`str`或`List[str]`，*可选*）— 要发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器中都使用`prompt`设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt`（`int`）— 每个提示应生成的图像数量

+   `do_classifier_free_guidance`（`bool`）— 是否使用无分类器引导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用来引导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用引导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。

+   `negative_prompt_2`（`str`或`List[str]`，*可选*）— 用于发送到`tokenizer_2`和`text_encoder_2`的不用来引导图像生成的提示或提示。如果未定义，则在两个文本编码器中都使用`negative_prompt`

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负面提示嵌入。

+   `pooled_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化文本嵌入。

+   `negative_pooled_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成池化的负面提示嵌入。

+   `lora_scale`（`float`，*可选*）— 将应用于文本编码器的所有 LoRA 层的 lora 比例，如果加载了 LoRA 层。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时要从 CLIP 中跳过的层数。值为 1 意味着将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L614)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（`bool`，默认为`True`）— 在 UNet 上应用融合。

+   `vae`（`bool`，默认为`True`）— 在 VAE 上应用融合。

启用融合的 QKV 投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此 API 为🧪实验性质。

#### `unfuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_k_diffusion/pipeline_stable_diffusion_xl_k_diffusion.py#L645)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（`bool`，默认为`True`）— 在 UNet 上应用融合。

+   `vae`（`bool`，默认为`True`）— 在 VAE 上应用融合。

如果启用，禁用 QKV 投影融合。

此 API 为🧪实验性质。
