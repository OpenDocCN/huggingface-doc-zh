- en: Soft prompts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è½¯æç¤º
- en: 'Original text: [https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Training large pretrained language models is very time-consuming and compute-intensive.
    As they continue to grow in size, there is increasing interest in more efficient
    training methods such as *prompting*. Prompting primes a frozen pretrained model
    for a specific downstream task by including a text prompt that describes the task
    or even demonstrates an example of the task. With prompting, you can avoid fully
    training a separate model for each downstream task, and use the same frozen pretrained
    model instead. This is a lot easier because you can use the same model for several
    different tasks, and it is significantly more efficient to train and store a smaller
    set of prompt parameters than to train all the modelâ€™s parameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éå¸¸è€—æ—¶å’Œè®¡ç®—å¯†é›†ã€‚éšç€å®ƒä»¬ä¸æ–­å¢å¤§ï¼Œäººä»¬å¯¹æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚*æç¤º*ï¼‰è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æç¤ºé€šè¿‡åŒ…å«æè¿°ä»»åŠ¡æˆ–ç”šè‡³æ¼”ç¤ºä»»åŠ¡ç¤ºä¾‹çš„æ–‡æœ¬æç¤ºæ¥ä¸ºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡å‡†å¤‡ä¸€ä¸ªå†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡æç¤ºï¼Œæ‚¨å¯ä»¥é¿å…ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡å®Œå…¨è®­ç»ƒä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨ç›¸åŒçš„å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™æ ·åšæ›´å®¹æ˜“ï¼Œå› ä¸ºæ‚¨å¯ä»¥ä¸ºå¤šä¸ªä¸åŒä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼Œå¹¶ä¸”è®­ç»ƒå’Œå­˜å‚¨ä¸€å°ç»„æç¤ºå‚æ•°è¦æ¯”è®­ç»ƒæ‰€æœ‰æ¨¡å‹å‚æ•°è¦é«˜æ•ˆå¾—å¤šã€‚
- en: 'There are two categories of prompting methods:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºæ–¹æ³•æœ‰ä¸¤ç±»ï¼š
- en: hard prompts are manually handcrafted text prompts with discrete input tokens;
    the downside is that it requires a lot of effort to create a good prompt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡¬æç¤ºæ˜¯æ‰‹å·¥åˆ¶ä½œçš„å…·æœ‰ç¦»æ•£è¾“å…¥æ ‡è®°çš„æ–‡æœ¬æç¤ºï¼›ç¼ºç‚¹æ˜¯éœ€è¦å¤§é‡åŠªåŠ›æ¥åˆ›å»ºä¸€ä¸ªå¥½çš„æç¤º
- en: soft prompts are learnable tensors concatenated with the input embeddings that
    can be optimized to a dataset; the downside is that they arenâ€™t human readable
    because you arenâ€™t matching these â€œvirtual tokensâ€ to the embeddings of a real
    word
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è½¯æç¤ºæ˜¯å¯å­¦ä¹ çš„å¼ é‡ï¼Œä¸è¾“å…¥åµŒå…¥è¿æ¥åœ¨ä¸€èµ·ï¼Œå¯ä»¥ä¼˜åŒ–åˆ°æ•°æ®é›†ï¼›ç¼ºç‚¹æ˜¯å®ƒä»¬ä¸æ˜“é˜…è¯»ï¼Œå› ä¸ºä½ æ²¡æœ‰å°†è¿™äº›â€œè™šæ‹Ÿæ ‡è®°â€ä¸çœŸå®å•è¯çš„åµŒå…¥åŒ¹é…èµ·æ¥
- en: 'This conceptual guide provides a brief overview of the soft prompt methods
    included in ğŸ¤— PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt
    tuning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¦‚å¿µæŒ‡å—æä¾›äº†åŒ…å«åœ¨ğŸ¤— PEFTä¸­çš„è½¯æç¤ºæ–¹æ³•çš„ç®€è¦æ¦‚è¿°ï¼šæç¤ºè°ƒæ•´ï¼Œå‰ç¼€è°ƒæ•´ï¼ŒPè°ƒæ•´å’Œå¤šä»»åŠ¡æç¤ºè°ƒæ•´ã€‚
- en: Prompt tuning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºè°ƒæ•´
- en: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)Only train and store a significantly
    smaller set of task-specific prompt parameters [(image source)](https://hf.co/papers/2104.08691).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)åªè®­ç»ƒå’Œå­˜å‚¨ä¸€ç»„æ˜¾è‘—è¾ƒå°çš„ä»»åŠ¡ç‰¹å®šæç¤ºå‚æ•°[(å›¾ç‰‡æ¥æº)](https://hf.co/papers/2104.08691)ã€‚'
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification
    tasks on T5 models, and all downstream tasks are cast as a text generation task.
    For example, sequence classification usually assigns a single class label to a
    sequence of text. By casting it as a text generation task, the tokens that make
    up the class label are *generated*. Prompts are added to the input as a series
    of tokens. Typically, the model parameters are fixed which means the prompt tokens
    are also fixed by the model parameters.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[æç¤ºè°ƒæ•´](https://hf.co/papers/2104.08691)æ˜¯ä¸ºT5æ¨¡å‹ä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å¼€å‘çš„ï¼Œæ‰€æœ‰ä¸‹æ¸¸ä»»åŠ¡éƒ½è¢«è§†ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåºåˆ—åˆ†ç±»é€šå¸¸å°†å•ä¸ªç±»æ ‡ç­¾åˆ†é…ç»™ä¸€ç³»åˆ—æ–‡æœ¬ã€‚é€šè¿‡å°†å…¶è§†ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ„æˆç±»æ ‡ç­¾çš„æ ‡è®°è¢«*ç”Ÿæˆ*ã€‚æç¤ºè¢«æ·»åŠ åˆ°è¾“å…¥ä¸­ä½œä¸ºä¸€ç³»åˆ—æ ‡è®°ã€‚é€šå¸¸ï¼Œæ¨¡å‹å‚æ•°æ˜¯å›ºå®šçš„ï¼Œè¿™æ„å‘³ç€æç¤ºæ ‡è®°ä¹Ÿç”±æ¨¡å‹å‚æ•°å›ºå®šã€‚'
- en: The key idea behind prompt tuning is that prompt tokens have their own parameters
    that are updated independently. This means you can keep the pretrained modelâ€™s
    parameters frozen, and only update the gradients of the prompt token embeddings.
    The results are comparable to the traditional method of training the entire model,
    and prompt tuning performance scales as model size increases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºè°ƒæ•´çš„å…³é”®æ€æƒ³æ˜¯æç¤ºæ ‡è®°æœ‰è‡ªå·±çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°æ˜¯ç‹¬ç«‹æ›´æ–°çš„ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥ä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°å†»ç»“ï¼Œåªæ›´æ–°æç¤ºæ ‡è®°åµŒå…¥çš„æ¢¯åº¦ã€‚ç»“æœä¸ä¼ ç»Ÿçš„è®­ç»ƒæ•´ä¸ªæ¨¡å‹çš„æ–¹æ³•ç›¸å½“ï¼Œæç¤ºè°ƒæ•´çš„æ€§èƒ½éšç€æ¨¡å‹å¤§å°çš„å¢åŠ è€Œæé«˜ã€‚
- en: Take a look at [Prompt tuning for causal language modeling](../task_guides/clm-prompt-tuning)
    for a step-by-step guide on how to train a model with prompt tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[å› æœè¯­è¨€å»ºæ¨¡çš„æç¤ºè°ƒæ•´](../task_guides/clm-prompt-tuning)ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨æç¤ºè°ƒæ•´è®­ç»ƒæ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚
- en: Prefix tuning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰ç¼€è°ƒæ•´
- en: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)Optimize the prefix parameters
    for each task [(image source)](https://hf.co/papers/2101.00190).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)ä¸ºæ¯ä¸ªä»»åŠ¡ä¼˜åŒ–å‰ç¼€å‚æ•°[(å›¾ç‰‡æ¥æº)](https://hf.co/papers/2101.00190)ã€‚'
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language
    generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix
    tuning also prepends a sequence of task-specific vectors to the input that can
    be trained and updated while keeping the rest of the pretrained modelâ€™s parameters
    frozen.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[å‰ç¼€è°ƒæ•´](https://hf.co/papers/2101.00190)æ˜¯ä¸ºGPTæ¨¡å‹ä¸Šçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ä»»åŠ¡è®¾è®¡çš„ã€‚å®ƒä¸æç¤ºè°ƒæ•´éå¸¸ç›¸ä¼¼ï¼›å‰ç¼€è°ƒæ•´è¿˜åœ¨è¾“å…¥ä¹‹å‰æ·»åŠ äº†ä¸€ç³»åˆ—ä»»åŠ¡ç‰¹å®šå‘é‡ï¼Œå¯ä»¥åœ¨ä¿æŒå…¶ä½™é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å†»ç»“çš„åŒæ—¶è¿›è¡Œè®­ç»ƒå’Œæ›´æ–°ã€‚'
- en: The main difference is that the prefix parameters are inserted in **all** of
    the model layers, whereas prompt tuning only adds the prompt parameters to the
    model input embeddings. The prefix parameters are also optimized by a separate
    feed-forward network (FFN) instead of training directly on the soft prompts because
    it causes instability and hurts performance. The FFN is discarded after updating
    the soft prompts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦åŒºåˆ«åœ¨äºå‰ç¼€å‚æ•°æ’å…¥åœ¨**æ‰€æœ‰**æ¨¡å‹å±‚ä¸­ï¼Œè€Œæç¤ºè°ƒæ•´ä»…å°†æç¤ºå‚æ•°æ·»åŠ åˆ°æ¨¡å‹è¾“å…¥åµŒå…¥ä¸­ã€‚å‰ç¼€å‚æ•°ä¹Ÿé€šè¿‡å•ç‹¬çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨è½¯æç¤ºä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ä¸ç¨³å®šæ€§å¹¶å½±å“æ€§èƒ½ã€‚åœ¨æ›´æ–°è½¯æç¤ºåï¼ŒFFNè¢«ä¸¢å¼ƒã€‚
- en: As a result, the authors found that prefix tuning demonstrates comparable performance
    to fully finetuning a model, despite having 1000x fewer parameters, and it performs
    even better in low-data settings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½œè€…å‘ç°å‰ç¼€è°ƒæ•´è¡¨ç°ä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸å½“ï¼Œå°½ç®¡å‚æ•°å°‘äº†1000å€ï¼Œè€Œä¸”åœ¨ä½æ•°æ®æƒ…å†µä¸‹è¡¨ç°å¾—æ›´å¥½ã€‚
- en: Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning)
    for a step-by-step guide on how to train a model with prefix tuning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[æ¡ä»¶ç”Ÿæˆçš„å‰ç¼€è°ƒæ•´](../task_guides/seq2seq-prefix-tuning)ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨å‰ç¼€è°ƒæ•´è®­ç»ƒæ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚
- en: P-tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P-tuning
- en: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)Prompt tokens can be inserted
    anywhere in the input sequence, and they are optimized by a prompt encoder [(image
    source)](https://hf.co/papers/2103.10385).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)æç¤ºæ ‡è®°å¯ä»¥æ’å…¥è¾“å…¥åºåˆ—çš„ä»»ä½•ä½ç½®ï¼Œå¹¶é€šè¿‡æç¤ºç¼–ç å™¨è¿›è¡Œä¼˜åŒ–[(å›¾ç‰‡æ¥æº)](https://hf.co/papers/2103.10385)ã€‚'
- en: '[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language
    understanding (NLU) tasks and all language models. It is another variation of
    a soft prompt method; P-tuning also adds a trainable embedding tensor that can
    be optimized to find better prompts, and it uses a prompt encoder (a bidirectional
    long-short term memory network or LSTM) to optimize the prompt parameters. Unlike
    prefix tuning though:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[P-tuning](https://hf.co/papers/2103.10385)è®¾è®¡ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡å’Œæ‰€æœ‰è¯­è¨€æ¨¡å‹ã€‚è¿™æ˜¯è½¯æç¤ºæ–¹æ³•çš„å¦ä¸€ç§å˜ä½“ï¼›P-tuningè¿˜æ·»åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„åµŒå…¥å¼ é‡ï¼Œå¯ä»¥ä¼˜åŒ–ä»¥æ‰¾åˆ°æ›´å¥½çš„æç¤ºï¼Œå¹¶ä½¿ç”¨æç¤ºç¼–ç å™¨ï¼ˆåŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œæˆ–LSTMï¼‰æ¥ä¼˜åŒ–æç¤ºå‚æ•°ã€‚ä¸è¿‡ï¼Œä¸å‰ç¼€è°ƒæ•´ä¸åŒï¼š'
- en: the prompt tokens can be inserted anywhere in the input sequence, and it isnâ€™t
    restricted to only the beginning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æç¤ºæ ‡è®°å¯ä»¥æ’å…¥è¾“å…¥åºåˆ—çš„ä»»ä½•ä½ç½®ï¼Œä¸ä»…é™äºå¼€å¤´
- en: the prompt tokens are only added to the input instead of adding them to every
    layer of the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æç¤ºæ ‡è®°ä»…æ·»åŠ åˆ°è¾“å…¥ä¸­ï¼Œè€Œä¸æ˜¯æ·»åŠ åˆ°æ¨¡å‹çš„æ¯ä¸€å±‚ã€‚
- en: introducing *anchor* tokens can improve performance because they indicate characteristics
    of a component in the input sequence
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼•å…¥*é”š*æ ‡è®°å¯ä»¥æé«˜æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬æŒ‡ç¤ºè¾“å…¥åºåˆ—ä¸­ç»„ä»¶çš„ç‰¹å¾
- en: The results suggest that P-tuning is more efficient than manually crafting prompts,
    and it enables GPT-like models to compete with BERT-like models on NLU tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼ŒP-tuningæ¯”æ‰‹åŠ¨åˆ¶ä½œæç¤ºæ›´æœ‰æ•ˆï¼Œå¹¶ä½¿ç±»ä¼¼GPTçš„æ¨¡å‹èƒ½å¤Ÿåœ¨NLUä»»åŠ¡ä¸Šä¸ç±»ä¼¼BERTçš„æ¨¡å‹ç«äº‰ã€‚
- en: Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification)
    for a step-by-step guide on how to train a model with P-tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[ç”¨äºåºåˆ—åˆ†ç±»çš„P-tuning](../task_guides/ptuning-seq-classification)ä»¥äº†è§£å¦‚ä½•ä½¿ç”¨P-tuningè®­ç»ƒæ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚
- en: Multitask prompt tuning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šä»»åŠ¡æç¤ºè°ƒæ•´
- en: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[Multitask prompt tuning
    enables parameter-efficient transfer learning](https://hf.co/papers/2103.10385).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[å¤šä»»åŠ¡æç¤ºè°ƒæ•´å®ç°å‚æ•°é«˜æ•ˆçš„è¿ç§»å­¦ä¹ ](https://hf.co/papers/2103.10385)ã€‚'
- en: '[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single
    prompt from data for multiple task types that can be shared for different target
    tasks. Other existing approaches learn a separate soft prompt for each task that
    need to be retrieved or aggregated for adaptation to target tasks. MPT consists
    of two stages:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¤šä»»åŠ¡æç¤ºè°ƒæ•´ï¼ˆMPTï¼‰](https://hf.co/papers/2103.10385)ä»æ•°æ®ä¸­ä¸ºå¤šç§ä»»åŠ¡ç±»å‹å­¦ä¹ å•ä¸ªæç¤ºï¼Œå¯ä»¥å…±äº«ç»™ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ã€‚å…¶ä»–ç°æœ‰æ–¹æ³•å­¦ä¹ ä¸ºæ¯ä¸ªä»»åŠ¡å•ç‹¬çš„è½¯æç¤ºï¼Œéœ€è¦æ£€ç´¢æˆ–èšåˆä»¥é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚MPTåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼š'
- en: source training - for each task, its soft prompt is decomposed into task-specific
    vectors. The task-specific vectors are multiplied together to form another matrix
    W, and the Hadamard product is used between W and a shared prompt matrix P to
    generate a task-specific prompt matrix. The task-specific prompts are distilled
    into a single prompt matrix that is shared across all tasks. This prompt is trained
    with multitask training.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æºè®­ç»ƒ - å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œå…¶è½¯æç¤ºè¢«åˆ†è§£ä¸ºç‰¹å®šäºä»»åŠ¡çš„å‘é‡ã€‚è¿™äº›ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ç›¸ä¹˜å½¢æˆå¦ä¸€ä¸ªçŸ©é˜µWï¼Œå¹¶ä¸”åœ¨Wå’Œå…±äº«æç¤ºçŸ©é˜µPä¹‹é—´ä½¿ç”¨Hadamardä¹˜ç§¯ç”Ÿæˆç‰¹å®šäºä»»åŠ¡çš„æç¤ºçŸ©é˜µã€‚ç‰¹å®šäºä»»åŠ¡çš„æç¤ºè¢«æç‚¼æˆä¸€ä¸ªåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å…±äº«çš„å•ä¸ªæç¤ºçŸ©é˜µã€‚è¿™ä¸ªæç¤ºé€šè¿‡å¤šä»»åŠ¡è®­ç»ƒè¿›è¡Œè®­ç»ƒã€‚
- en: target adaptation - to adapt the single prompt for a target task, a target prompt
    is initialized and expressed as the Hadamard product of the shared prompt matrix
    and the task-specific low-rank prompt matrix.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›®æ ‡é€‚åº” - ä¸ºäº†ä½¿å•ä¸ªæç¤ºé€‚åº”ç›®æ ‡ä»»åŠ¡ï¼Œåˆå§‹åŒ–ç›®æ ‡æç¤ºå¹¶å°†å…¶è¡¨ç¤ºä¸ºå…±äº«æç¤ºçŸ©é˜µå’Œç‰¹å®šäºä»»åŠ¡çš„ä½ç§©æç¤ºçŸ©é˜µçš„Hadamardä¹˜ç§¯ã€‚
- en: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[Prompt decomposition](https://hf.co/papers/2103.10385).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[æç¤ºåˆ†è§£](https://hf.co/papers/2103.10385)ã€‚'
