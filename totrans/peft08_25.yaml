- en: Soft prompts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软提示
- en: 'Original text: [https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Training large pretrained language models is very time-consuming and compute-intensive.
    As they continue to grow in size, there is increasing interest in more efficient
    training methods such as *prompting*. Prompting primes a frozen pretrained model
    for a specific downstream task by including a text prompt that describes the task
    or even demonstrates an example of the task. With prompting, you can avoid fully
    training a separate model for each downstream task, and use the same frozen pretrained
    model instead. This is a lot easier because you can use the same model for several
    different tasks, and it is significantly more efficient to train and store a smaller
    set of prompt parameters than to train all the model’s parameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型预训练语言模型非常耗时和计算密集。随着它们不断增大，人们对更高效的训练方法（如*提示*）越来越感兴趣。提示通过包含描述任务或甚至演示任务示例的文本提示来为特定下游任务准备一个冻结的预训练模型。通过提示，您可以避免为每个下游任务完全训练一个单独的模型，而是使用相同的冻结的预训练模型。这样做更容易，因为您可以为多个不同任务使用相同的模型，并且训练和存储一小组提示参数要比训练所有模型参数要高效得多。
- en: 'There are two categories of prompting methods:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 提示方法有两类：
- en: hard prompts are manually handcrafted text prompts with discrete input tokens;
    the downside is that it requires a lot of effort to create a good prompt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬提示是手工制作的具有离散输入标记的文本提示；缺点是需要大量努力来创建一个好的提示
- en: soft prompts are learnable tensors concatenated with the input embeddings that
    can be optimized to a dataset; the downside is that they aren’t human readable
    because you aren’t matching these “virtual tokens” to the embeddings of a real
    word
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软提示是可学习的张量，与输入嵌入连接在一起，可以优化到数据集；缺点是它们不易阅读，因为你没有将这些“虚拟标记”与真实单词的嵌入匹配起来
- en: 'This conceptual guide provides a brief overview of the soft prompt methods
    included in 🤗 PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt
    tuning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念指南提供了包含在🤗 PEFT中的软提示方法的简要概述：提示调整，前缀调整，P调整和多任务提示调整。
- en: Prompt tuning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示调整
- en: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)Only train and store a significantly
    smaller set of task-specific prompt parameters [(image source)](https://hf.co/papers/2104.08691).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)只训练和存储一组显著较小的任务特定提示参数[(图片来源)](https://hf.co/papers/2104.08691)。'
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification
    tasks on T5 models, and all downstream tasks are cast as a text generation task.
    For example, sequence classification usually assigns a single class label to a
    sequence of text. By casting it as a text generation task, the tokens that make
    up the class label are *generated*. Prompts are added to the input as a series
    of tokens. Typically, the model parameters are fixed which means the prompt tokens
    are also fixed by the model parameters.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示调整](https://hf.co/papers/2104.08691)是为T5模型上的文本分类任务开发的，所有下游任务都被视为文本生成任务。例如，序列分类通常将单个类标签分配给一系列文本。通过将其视为文本生成任务，构成类标签的标记被*生成*。提示被添加到输入中作为一系列标记。通常，模型参数是固定的，这意味着提示标记也由模型参数固定。'
- en: The key idea behind prompt tuning is that prompt tokens have their own parameters
    that are updated independently. This means you can keep the pretrained model’s
    parameters frozen, and only update the gradients of the prompt token embeddings.
    The results are comparable to the traditional method of training the entire model,
    and prompt tuning performance scales as model size increases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 提示调整的关键思想是提示标记有自己的参数，这些参数是独立更新的。这意味着您可以保持预训练模型的参数冻结，只更新提示标记嵌入的梯度。结果与传统的训练整个模型的方法相当，提示调整的性能随着模型大小的增加而提高。
- en: Take a look at [Prompt tuning for causal language modeling](../task_guides/clm-prompt-tuning)
    for a step-by-step guide on how to train a model with prompt tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[因果语言建模的提示调整](../task_guides/clm-prompt-tuning)以了解如何使用提示调整训练模型的逐步指南。
- en: Prefix tuning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀调整
- en: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)Optimize the prefix parameters
    for each task [(image source)](https://hf.co/papers/2101.00190).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)为每个任务优化前缀参数[(图片来源)](https://hf.co/papers/2101.00190)。'
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language
    generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix
    tuning also prepends a sequence of task-specific vectors to the input that can
    be trained and updated while keeping the rest of the pretrained model’s parameters
    frozen.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[前缀调整](https://hf.co/papers/2101.00190)是为GPT模型上的自然语言生成（NLG）任务设计的。它与提示调整非常相似；前缀调整还在输入之前添加了一系列任务特定向量，可以在保持其余预训练模型参数冻结的同时进行训练和更新。'
- en: The main difference is that the prefix parameters are inserted in **all** of
    the model layers, whereas prompt tuning only adds the prompt parameters to the
    model input embeddings. The prefix parameters are also optimized by a separate
    feed-forward network (FFN) instead of training directly on the soft prompts because
    it causes instability and hurts performance. The FFN is discarded after updating
    the soft prompts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于前缀参数插入在**所有**模型层中，而提示调整仅将提示参数添加到模型输入嵌入中。前缀参数也通过单独的前馈网络（FFN）进行优化，而不是直接在软提示上进行训练，因为这会导致不稳定性并影响性能。在更新软提示后，FFN被丢弃。
- en: As a result, the authors found that prefix tuning demonstrates comparable performance
    to fully finetuning a model, despite having 1000x fewer parameters, and it performs
    even better in low-data settings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者发现前缀调整表现与完全微调模型相当，尽管参数少了1000倍，而且在低数据情况下表现得更好。
- en: Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning)
    for a step-by-step guide on how to train a model with prefix tuning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[条件生成的前缀调整](../task_guides/seq2seq-prefix-tuning)以了解如何使用前缀调整训练模型的逐步指南。
- en: P-tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P-tuning
- en: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)Prompt tokens can be inserted
    anywhere in the input sequence, and they are optimized by a prompt encoder [(image
    source)](https://hf.co/papers/2103.10385).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)提示标记可以插入输入序列的任何位置，并通过提示编码器进行优化[(图片来源)](https://hf.co/papers/2103.10385)。'
- en: '[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language
    understanding (NLU) tasks and all language models. It is another variation of
    a soft prompt method; P-tuning also adds a trainable embedding tensor that can
    be optimized to find better prompts, and it uses a prompt encoder (a bidirectional
    long-short term memory network or LSTM) to optimize the prompt parameters. Unlike
    prefix tuning though:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[P-tuning](https://hf.co/papers/2103.10385)设计用于自然语言理解（NLU）任务和所有语言模型。这是软提示方法的另一种变体；P-tuning还添加了一个可训练的嵌入张量，可以优化以找到更好的提示，并使用提示编码器（双向长短期记忆网络或LSTM）来优化提示参数。不过，与前缀调整不同：'
- en: the prompt tokens can be inserted anywhere in the input sequence, and it isn’t
    restricted to only the beginning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示标记可以插入输入序列的任何位置，不仅限于开头
- en: the prompt tokens are only added to the input instead of adding them to every
    layer of the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示标记仅添加到输入中，而不是添加到模型的每一层。
- en: introducing *anchor* tokens can improve performance because they indicate characteristics
    of a component in the input sequence
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入*锚*标记可以提高性能，因为它们指示输入序列中组件的特征
- en: The results suggest that P-tuning is more efficient than manually crafting prompts,
    and it enables GPT-like models to compete with BERT-like models on NLU tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，P-tuning比手动制作提示更有效，并使类似GPT的模型能够在NLU任务上与类似BERT的模型竞争。
- en: Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification)
    for a step-by-step guide on how to train a model with P-tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[用于序列分类的P-tuning](../task_guides/ptuning-seq-classification)以了解如何使用P-tuning训练模型的逐步指南。
- en: Multitask prompt tuning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务提示调整
- en: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[Multitask prompt tuning
    enables parameter-efficient transfer learning](https://hf.co/papers/2103.10385).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[多任务提示调整实现参数高效的迁移学习](https://hf.co/papers/2103.10385)。'
- en: '[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single
    prompt from data for multiple task types that can be shared for different target
    tasks. Other existing approaches learn a separate soft prompt for each task that
    need to be retrieved or aggregated for adaptation to target tasks. MPT consists
    of two stages:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务提示调整（MPT）](https://hf.co/papers/2103.10385)从数据中为多种任务类型学习单个提示，可以共享给不同的目标任务。其他现有方法学习为每个任务单独的软提示，需要检索或聚合以适应目标任务。MPT包括两个阶段：'
- en: source training - for each task, its soft prompt is decomposed into task-specific
    vectors. The task-specific vectors are multiplied together to form another matrix
    W, and the Hadamard product is used between W and a shared prompt matrix P to
    generate a task-specific prompt matrix. The task-specific prompts are distilled
    into a single prompt matrix that is shared across all tasks. This prompt is trained
    with multitask training.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源训练 - 对于每个任务，其软提示被分解为特定于任务的向量。这些特定于任务的向量相乘形成另一个矩阵W，并且在W和共享提示矩阵P之间使用Hadamard乘积生成特定于任务的提示矩阵。特定于任务的提示被提炼成一个在所有任务中共享的单个提示矩阵。这个提示通过多任务训练进行训练。
- en: target adaptation - to adapt the single prompt for a target task, a target prompt
    is initialized and expressed as the Hadamard product of the shared prompt matrix
    and the task-specific low-rank prompt matrix.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标适应 - 为了使单个提示适应目标任务，初始化目标提示并将其表示为共享提示矩阵和特定于任务的低秩提示矩阵的Hadamard乘积。
- en: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[Prompt decomposition](https://hf.co/papers/2103.10385).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[提示分解](https://hf.co/papers/2103.10385)。'
