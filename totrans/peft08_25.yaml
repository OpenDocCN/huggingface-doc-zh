- en: Soft prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Training large pretrained language models is very time-consuming and compute-intensive.
    As they continue to grow in size, there is increasing interest in more efficient
    training methods such as *prompting*. Prompting primes a frozen pretrained model
    for a specific downstream task by including a text prompt that describes the task
    or even demonstrates an example of the task. With prompting, you can avoid fully
    training a separate model for each downstream task, and use the same frozen pretrained
    model instead. This is a lot easier because you can use the same model for several
    different tasks, and it is significantly more efficient to train and store a smaller
    set of prompt parameters than to train all the model‚Äôs parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories of prompting methods:'
  prefs: []
  type: TYPE_NORMAL
- en: hard prompts are manually handcrafted text prompts with discrete input tokens;
    the downside is that it requires a lot of effort to create a good prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: soft prompts are learnable tensors concatenated with the input embeddings that
    can be optimized to a dataset; the downside is that they aren‚Äôt human readable
    because you aren‚Äôt matching these ‚Äúvirtual tokens‚Äù to the embeddings of a real
    word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This conceptual guide provides a brief overview of the soft prompt methods
    included in ü§ó PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt
    tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)Only train and store a significantly
    smaller set of task-specific prompt parameters [(image source)](https://hf.co/papers/2104.08691).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification
    tasks on T5 models, and all downstream tasks are cast as a text generation task.
    For example, sequence classification usually assigns a single class label to a
    sequence of text. By casting it as a text generation task, the tokens that make
    up the class label are *generated*. Prompts are added to the input as a series
    of tokens. Typically, the model parameters are fixed which means the prompt tokens
    are also fixed by the model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The key idea behind prompt tuning is that prompt tokens have their own parameters
    that are updated independently. This means you can keep the pretrained model‚Äôs
    parameters frozen, and only update the gradients of the prompt token embeddings.
    The results are comparable to the traditional method of training the entire model,
    and prompt tuning performance scales as model size increases.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [Prompt tuning for causal language modeling](../task_guides/clm-prompt-tuning)
    for a step-by-step guide on how to train a model with prompt tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)Optimize the prefix parameters
    for each task [(image source)](https://hf.co/papers/2101.00190).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language
    generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix
    tuning also prepends a sequence of task-specific vectors to the input that can
    be trained and updated while keeping the rest of the pretrained model‚Äôs parameters
    frozen.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is that the prefix parameters are inserted in **all** of
    the model layers, whereas prompt tuning only adds the prompt parameters to the
    model input embeddings. The prefix parameters are also optimized by a separate
    feed-forward network (FFN) instead of training directly on the soft prompts because
    it causes instability and hurts performance. The FFN is discarded after updating
    the soft prompts.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the authors found that prefix tuning demonstrates comparable performance
    to fully finetuning a model, despite having 1000x fewer parameters, and it performs
    even better in low-data settings.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning)
    for a step-by-step guide on how to train a model with prefix tuning.
  prefs: []
  type: TYPE_NORMAL
- en: P-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)Prompt tokens can be inserted
    anywhere in the input sequence, and they are optimized by a prompt encoder [(image
    source)](https://hf.co/papers/2103.10385).'
  prefs: []
  type: TYPE_NORMAL
- en: '[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language
    understanding (NLU) tasks and all language models. It is another variation of
    a soft prompt method; P-tuning also adds a trainable embedding tensor that can
    be optimized to find better prompts, and it uses a prompt encoder (a bidirectional
    long-short term memory network or LSTM) to optimize the prompt parameters. Unlike
    prefix tuning though:'
  prefs: []
  type: TYPE_NORMAL
- en: the prompt tokens can be inserted anywhere in the input sequence, and it isn‚Äôt
    restricted to only the beginning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the prompt tokens are only added to the input instead of adding them to every
    layer of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: introducing *anchor* tokens can improve performance because they indicate characteristics
    of a component in the input sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results suggest that P-tuning is more efficient than manually crafting prompts,
    and it enables GPT-like models to compete with BERT-like models on NLU tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification)
    for a step-by-step guide on how to train a model with P-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Multitask prompt tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[Multitask prompt tuning
    enables parameter-efficient transfer learning](https://hf.co/papers/2103.10385).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single
    prompt from data for multiple task types that can be shared for different target
    tasks. Other existing approaches learn a separate soft prompt for each task that
    need to be retrieved or aggregated for adaptation to target tasks. MPT consists
    of two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: source training - for each task, its soft prompt is decomposed into task-specific
    vectors. The task-specific vectors are multiplied together to form another matrix
    W, and the Hadamard product is used between W and a shared prompt matrix P to
    generate a task-specific prompt matrix. The task-specific prompts are distilled
    into a single prompt matrix that is shared across all tasks. This prompt is trained
    with multitask training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: target adaptation - to adapt the single prompt for a target task, a target prompt
    is initialized and expressed as the Hadamard product of the shared prompt matrix
    and the task-specific low-rank prompt matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[Prompt decomposition](https://hf.co/papers/2103.10385).'
  prefs: []
  type: TYPE_NORMAL
