- en: Soft prompts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/conceptual_guides/prompting](https://huggingface.co/docs/peft/conceptual_guides/prompting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Training large pretrained language models is very time-consuming and compute-intensive.
    As they continue to grow in size, there is increasing interest in more efficient
    training methods such as *prompting*. Prompting primes a frozen pretrained model
    for a specific downstream task by including a text prompt that describes the task
    or even demonstrates an example of the task. With prompting, you can avoid fully
    training a separate model for each downstream task, and use the same frozen pretrained
    model instead. This is a lot easier because you can use the same model for several
    different tasks, and it is significantly more efficient to train and store a smaller
    set of prompt parameters than to train all the model’s parameters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two categories of prompting methods:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: hard prompts are manually handcrafted text prompts with discrete input tokens;
    the downside is that it requires a lot of effort to create a good prompt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: soft prompts are learnable tensors concatenated with the input embeddings that
    can be optimized to a dataset; the downside is that they aren’t human readable
    because you aren’t matching these “virtual tokens” to the embeddings of a real
    word
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This conceptual guide provides a brief overview of the soft prompt methods
    included in 🤗 PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt
    tuning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Prompt tuning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4f75fee4ac42ef5989e25fc4ad53c98b.png)Only train and store a significantly
    smaller set of task-specific prompt parameters [(image source)](https://hf.co/papers/2104.08691).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification
    tasks on T5 models, and all downstream tasks are cast as a text generation task.
    For example, sequence classification usually assigns a single class label to a
    sequence of text. By casting it as a text generation task, the tokens that make
    up the class label are *generated*. Prompts are added to the input as a series
    of tokens. Typically, the model parameters are fixed which means the prompt tokens
    are also fixed by the model parameters.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The key idea behind prompt tuning is that prompt tokens have their own parameters
    that are updated independently. This means you can keep the pretrained model’s
    parameters frozen, and only update the gradients of the prompt token embeddings.
    The results are comparable to the traditional method of training the entire model,
    and prompt tuning performance scales as model size increases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [Prompt tuning for causal language modeling](../task_guides/clm-prompt-tuning)
    for a step-by-step guide on how to train a model with prompt tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Prefix tuning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2b0d1a4a20587ca6c948a2771a6d0cf5.png)Optimize the prefix parameters
    for each task [(image source)](https://hf.co/papers/2101.00190).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language
    generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix
    tuning also prepends a sequence of task-specific vectors to the input that can
    be trained and updated while keeping the rest of the pretrained model’s parameters
    frozen.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The main difference is that the prefix parameters are inserted in **all** of
    the model layers, whereas prompt tuning only adds the prompt parameters to the
    model input embeddings. The prefix parameters are also optimized by a separate
    feed-forward network (FFN) instead of training directly on the soft prompts because
    it causes instability and hurts performance. The FFN is discarded after updating
    the soft prompts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the authors found that prefix tuning demonstrates comparable performance
    to fully finetuning a model, despite having 1000x fewer parameters, and it performs
    even better in low-data settings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning)
    for a step-by-step guide on how to train a model with prefix tuning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[条件生成的前缀调整](../task_guides/seq2seq-prefix-tuning)以了解如何使用前缀调整训练模型的逐步指南。
- en: P-tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P-tuning
- en: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)Prompt tokens can be inserted
    anywhere in the input sequence, and they are optimized by a prompt encoder [(image
    source)](https://hf.co/papers/2103.10385).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/b5eee8a03e099efb53338b38a47b469a.png)提示标记可以插入输入序列的任何位置，并通过提示编码器进行优化[(图片来源)](https://hf.co/papers/2103.10385)。'
- en: '[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language
    understanding (NLU) tasks and all language models. It is another variation of
    a soft prompt method; P-tuning also adds a trainable embedding tensor that can
    be optimized to find better prompts, and it uses a prompt encoder (a bidirectional
    long-short term memory network or LSTM) to optimize the prompt parameters. Unlike
    prefix tuning though:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[P-tuning](https://hf.co/papers/2103.10385)设计用于自然语言理解（NLU）任务和所有语言模型。这是软提示方法的另一种变体；P-tuning还添加了一个可训练的嵌入张量，可以优化以找到更好的提示，并使用提示编码器（双向长短期记忆网络或LSTM）来优化提示参数。不过，与前缀调整不同：'
- en: the prompt tokens can be inserted anywhere in the input sequence, and it isn’t
    restricted to only the beginning
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示标记可以插入输入序列的任何位置，不仅限于开头
- en: the prompt tokens are only added to the input instead of adding them to every
    layer of the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示标记仅添加到输入中，而不是添加到模型的每一层。
- en: introducing *anchor* tokens can improve performance because they indicate characteristics
    of a component in the input sequence
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入*锚*标记可以提高性能，因为它们指示输入序列中组件的特征
- en: The results suggest that P-tuning is more efficient than manually crafting prompts,
    and it enables GPT-like models to compete with BERT-like models on NLU tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，P-tuning比手动制作提示更有效，并使类似GPT的模型能够在NLU任务上与类似BERT的模型竞争。
- en: Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification)
    for a step-by-step guide on how to train a model with P-tuning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[用于序列分类的P-tuning](../task_guides/ptuning-seq-classification)以了解如何使用P-tuning训练模型的逐步指南。
- en: Multitask prompt tuning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务提示调整
- en: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[Multitask prompt tuning
    enables parameter-efficient transfer learning](https://hf.co/papers/2103.10385).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e09e7855dd11d53f2d40bea40b744f1b.png)[多任务提示调整实现参数高效的迁移学习](https://hf.co/papers/2103.10385)。'
- en: '[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single
    prompt from data for multiple task types that can be shared for different target
    tasks. Other existing approaches learn a separate soft prompt for each task that
    need to be retrieved or aggregated for adaptation to target tasks. MPT consists
    of two stages:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[多任务提示调整（MPT）](https://hf.co/papers/2103.10385)从数据中为多种任务类型学习单个提示，可以共享给不同的目标任务。其他现有方法学习为每个任务单独的软提示，需要检索或聚合以适应目标任务。MPT包括两个阶段：'
- en: source training - for each task, its soft prompt is decomposed into task-specific
    vectors. The task-specific vectors are multiplied together to form another matrix
    W, and the Hadamard product is used between W and a shared prompt matrix P to
    generate a task-specific prompt matrix. The task-specific prompts are distilled
    into a single prompt matrix that is shared across all tasks. This prompt is trained
    with multitask training.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 源训练 - 对于每个任务，其软提示被分解为特定于任务的向量。这些特定于任务的向量相乘形成另一个矩阵W，并且在W和共享提示矩阵P之间使用Hadamard乘积生成特定于任务的提示矩阵。特定于任务的提示被提炼成一个在所有任务中共享的单个提示矩阵。这个提示通过多任务训练进行训练。
- en: target adaptation - to adapt the single prompt for a target task, a target prompt
    is initialized and expressed as the Hadamard product of the shared prompt matrix
    and the task-specific low-rank prompt matrix.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标适应 - 为了使单个提示适应目标任务，初始化目标提示并将其表示为共享提示矩阵和特定于任务的低秩提示矩阵的Hadamard乘积。
- en: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[Prompt decomposition](https://hf.co/papers/2103.10385).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/9029ab9061d837a205b7c7d95a5fb720.png)[提示分解](https://hf.co/papers/2103.10385)。'
