# VisualBERT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/visual_bert](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/visual_bert)

## 概述

VisualBERT模型是由Liunian Harold Li、Mark Yatskar、Da Yin、Cho-Jui Hsieh、Kai-Wei Chang在[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557)中提出的。VisualBERT是一个在各种（图像，文本）对上训练的神经网络。

论文摘要如下：

*我们提出了VisualBERT，一个简单灵活的框架，用于建模广泛的视觉和语言任务。VisualBERT由一堆Transformer层组成，通过自注意力隐式对齐输入文本的元素和相关输入图像中的区域。我们进一步提出了两个基于视觉的语言模型目标，用于在图像标题数据上预训练VisualBERT。对包括VQA、VCR、NLVR2和Flickr30K在内的四个视觉和语言任务的实验表明，VisualBERT在简化的同时优于或与最先进的模型相媲美。进一步的分析表明，VisualBERT可以将语言元素与图像区域联系起来，而无需任何明确的监督，并且甚至对句法关系敏感，例如跟踪动词和与其参数对应的图像区域之间的关联。*

此模型由[gchhablani](https://huggingface.co/gchhablani)贡献。原始代码可以在[这里](https://github.com/uclanlp/visualbert)找到。

## 使用提示

1.  提供的大多数检查点适用于[VisualBertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForPreTraining)配置。提供的其他检查点是用于下游任务微调的检查点 - VQA（'visualbert-vqa'）、VCR（'visualbert-vcr'）、NLVR2（'visualbert-nlvr2'）。因此，如果您不是在进行这些下游任务，建议使用预训练检查点。

1.  对于VCR任务，作者使用了一个经过微调的检测器来生成视觉嵌入，对于所有的检查点。我们不会将检测器及其权重作为软件包的一部分提供，但它将在研究项目中提供，并且状态可以直接加载到提供的检测器中。

VisualBERT是一个多模态视觉和语言模型。它可用于视觉问答、多项选择、视觉推理和区域到短语对应任务。VisualBERT使用类似BERT的变压器来为图像-文本对准备嵌入。然后将文本和视觉特征投影到具有相同维度的潜在空间中。

要将图像馈送到模型中，必须通过预训练的对象检测器传递每个图像，并提取区域和边界框。作者使用通过将这些区域通过预训练的CNN（如ResNet）传递后生成的特征作为视觉嵌入。他们还添加了绝对位置嵌入，并将生成的向量序列馈送到标准的BERT模型中。文本输入在嵌入层的前面与视觉嵌入连接，并且预期由[CLS]和[SEP]标记限定，就像BERT一样。段ID也必须适当设置为文本和视觉部分。

使用[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)对文本进行编码。必须使用自定义检测器/图像处理器来获取视觉嵌入。以下示例笔记本展示了如何使用类似Detectron的模型与VisualBERT一起使用：

+   [VisualBERT VQA演示笔记本](https://github.com/huggingface/transformers/tree/main/examples/research_projects/visual_bert)：此笔记本包含VisualBERT VQA的示例。

+   [为VisualBERT生成嵌入（Colab笔记本）](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing)：此笔记本包含如何生成视觉嵌入的示例。

以下示例显示如何使用 [VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel) 获取最后一个隐藏状态：

```py
>>> import torch
>>> from transformers import BertTokenizer, VisualBertModel

>>> model = VisualBertModel.from_pretrained("uclanlp/visualbert-vqa-coco-pre")
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

>>> inputs = tokenizer("What is the man eating?", return_tensors="pt")
>>> # this is a custom function that returns the visual embeddings given the image path
>>> visual_embeds = get_visual_embeddings(image_path)

>>> visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
>>> visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)
>>> inputs.update(
...     {
...         "visual_embeds": visual_embeds,
...         "visual_token_type_ids": visual_token_type_ids,
...         "visual_attention_mask": visual_attention_mask,
...     }
... )
>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
```

## VisualBertConfig

### `class transformers.VisualBertConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/configuration_visual_bert.py#L43)

```py
( vocab_size = 30522 hidden_size = 768 visual_embedding_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 bypass_transformer = False special_visual_initialize = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 30522) — VisualBERT 模型的词汇表大小。定义了在调用 [VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel) 时可以由 `inputs_ids` 表示的不同标记数量。模型的词汇表大小。定义了可以由传递给 [VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel) 的 forward 方法的 `inputs_ids` 表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `visual_embedding_dim` (`int`, *optional*, defaults to 512) — 要传递给模型的视觉嵌入的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"selu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout 比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 模型可能使用的最大序列长度。通常设置为较大的值（例如 512、1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用 [VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel) 时传递的 `token_type_ids` 的词汇表大小。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `bypass_transformer` (`bool`, *optional*, defaults to `False`) — 模型是否应绕过 Transformer 处理视觉嵌入。如果设置为 `True`，模型直接将来自 `VisualBertEmbeddings` 的视觉嵌入与来自 transformers 的文本输出连接起来，然后传递给自注意力层。

+   `special_visual_initialize` (`bool`, *optional*, defaults to `True`) — 视觉标记类型和位置类型嵌入权重是否应该与文本标记类型和正向类型嵌入相同初始化。当设置为 `True` 时，文本标记类型和位置类型嵌入的权重将复制到相应的视觉嵌入层。

这是用于存储 [VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel) 配置的配置类。根据指定的参数实例化 VisualBERT 模型，定义模型架构。使用默认值实例化配置将产生类似于 VisualBERT [uclanlp/visualbert-vqa-coco-pre](https://huggingface.co/uclanlp/visualbert-vqa-coco-pre) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import VisualBertConfig, VisualBertModel

>>> # Initializing a VisualBERT visualbert-vqa-coco-pre style configuration
>>> configuration = VisualBertConfig.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

>>> # Initializing a model (with random weights) from the visualbert-vqa-coco-pre style configuration
>>> model = VisualBertModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## VisualBertModel

### `class transformers.VisualBertModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L664)

```py
( config add_pooling_layer = True )
```

参数

+   `config`（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）— 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的VisualBert模型变压器输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存，调整输入嵌入，修剪头等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

该模型可以作为一个编码器（仅具有自注意力），遵循[Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，Lukasz Kaiser和Illia Polosukhin描述的架构](https://arxiv.org/abs/1706.03762)。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L707)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被“掩盖”的标记，

    +   0表示被“掩盖”的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    什么是标记类型ID？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    什么是位置ID？

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部未被“掩盖”，

    +   0表示头部被“掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `visual_embeds`（形状为`(batch_size, visual_seq_length, visual_embedding_dim)`的`torch.FloatTensor`，*可选*）— 视觉输入的嵌入表示，通常使用对象检测器派生。

+   `visual_attention_mask`（形状为`(batch_size, visual_seq_length)`的`torch.FloatTensor`，*可选*）— 用于避免在视觉嵌入上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。

    注意力掩码是什么？

+   VisualBERT的作者将*visual_token_type_ids*设置为所有令牌的*1*。

    令牌类型ID是什么？

+   `image_text_alignment`（形状为`(batch_size, visual_seq_length, alignment_number)`的`torch.LongTensor`，*可选*）— 用于决定视觉嵌入的位置ID的图像-文本对齐。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态的序列输出。

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入的不同元素。

+   `visual_token_type_ids`（形状为`(batch_size, visual_seq_length)`的`torch.LongTensor`，*可选*）— 段令牌索引，用于指示视觉嵌入的不同部分。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 经过用于辅助预训练任务的层进一步处理后的序列的第一个令牌（分类令牌）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类令牌。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个+每层输出的一个）。

    模型在每一层的输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头部中使用注意力softmax后的注意力权重，用于计算加权平均值。

[VisualBertModel](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image.
from transformers import AutoTokenizer, VisualBertModel
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertModel.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

inputs = tokenizer("The capital of France is Paris.", return_tensors="pt")
visual_embeds = get_visual_embeddings(image).unsqueeze(0)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

inputs.update(
    {
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
    }
)

outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
```

## VisualBertForPreTraining

### `class transformers.VisualBertForPreTraining`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L858)

```py
( config )
```

参数

+   `config` ([VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VisualBert模型在预训练期间在顶部有两个头部：一个`掩码语言建模`头部和一个`句子-图像预测（分类）`头部。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存，调整输入嵌入大小，修剪头部等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L883)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None sentence_image_labels: Optional = None ) → export const metadata = 'undefined';transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示`未屏蔽`的标记，

    +   0表示`已屏蔽`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于一个*句子A*标记，

    +   1对应于一个*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部是`未屏蔽`，

    +   0表示头部是`已屏蔽`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递一个嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `visual_embeds`（形状为`(batch_size, visual_seq_length, visual_embedding_dim)`的`torch.FloatTensor`，*可选*）— 视觉输入的嵌入表示，通常使用对象检测器派生。

+   `visual_attention_mask`（形状为`(batch_size, visual_seq_length)`的`torch.FloatTensor`，*可选*）— 用于避免在视觉嵌入上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示`未被掩码`的标记，

    +   0表示`被掩码`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_token_type_ids`（形状为`(batch_size, visual_seq_length)`的`torch.LongTensor`，*可选*）— 段标记索引，用于指示视觉嵌入的不同部分。

    [什么是标记类型ID？](../glossary#token-type-ids) VisualBERT的作者将*visual_token_type_ids*设置为*1*以表示所有标记。

+   `image_text_alignment`（形状为`(batch_size, visual_seq_length, alignment_number)`的`torch.LongTensor`，*可选*）— 用于决定视觉嵌入的位置ID的图像-文本对齐。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels`（形状为`(batch_size, total_sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`中（请参见`input_ids`文档字符串）。将索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `sentence_image_labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算句子-图像预测（分类）损失的标签。输入应为一个序列对（请参见`input_ids`文档字符串）索引应在`[0, 1]`中。

    +   0表示对于给定图像，序列B是序列A的匹配对，

    +   1表示对于给定图像，序列B是相对于A的随机序列。

返回

`transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.visual_bert.modeling_visual_bert.VisualBertForPreTrainingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包含根据配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入的不同元素。

+   `loss`（*可选*，当提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`）— 作为掩码语言建模损失和句子-图像预测（分类）损失之和的总损失。

+   `prediction_logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `seq_relationship_logits`（形状为`(batch_size, 2)`的`torch.FloatTensor`）— 句子-图像预测（分类）头的预测分数（SoftMax之前的True/False连续分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层的输出以及初始嵌入输出的隐藏状态。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力softmax后的注意力权重。

[VisualBertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForPreTraining)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
from transformers import AutoTokenizer, VisualBertForPreTraining

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForPreTraining.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")
visual_embeds = get_visual_embeddings(image).unsqueeze(0)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

inputs.update(
    {
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
    }
)
max_length = inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]
labels = tokenizer(
    "The capital of France is Paris.", return_tensors="pt", padding="max_length", max_length=max_length
)["input_ids"]
sentence_image_labels = torch.tensor(1).unsqueeze(0)  # Batch_size

outputs = model(**inputs, labels=labels, sentence_image_labels=sentence_image_labels)
loss = outputs.loss
prediction_logits = outputs.prediction_logits
seq_relationship_logits = outputs.seq_relationship_logits
```

## VisualBertForQuestionAnswering

### `class transformers.VisualBertForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1162)

```py
( config )
```

参数

+   `config`（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VisualBert模型在顶部具有分类/回归头（在汇总输出的顶部有一个dropout和一个线性层）用于VQA。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1181)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1 代表*未被掩码*的标记，

    +   0 代表*被掩码*的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子A*标记，

    +   1 对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列令牌的位置在位置嵌入中的索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `visual_embeds` (`torch.FloatTensor`，形状为`(batch_size, visual_seq_length, visual_embedding_dim)`，*可选*) — 视觉输入的嵌入表示，通常使用对象检测器派生。

+   `visual_attention_mask` (`torch.FloatTensor`，形状为`(batch_size, visual_seq_length)`，*可选*) — 避免对视觉嵌入执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_token_type_ids` (`torch.LongTensor`，形状为`(batch_size, visual_seq_length)`，*可选*) — 段令牌索引，用于指示视觉嵌入的不同部分。

    [什么是令牌类型ID？](../glossary#token-type-ids) VisualBERT的作者将*visual_token_type_ids*设置为所有令牌的*1*。

+   `image_text_alignment` (`torch.LongTensor`，形状为`(batch_size, visual_seq_length, alignment_number)`，*可选*) — 用于决定视觉嵌入的位置ID的图像-文本对齐。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, total_sequence_length)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。标签和返回的logits之间计算KLDivLoss。

返回值

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力权重之后的注意力softmax。

[VisualBertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
from transformers import AutoTokenizer, VisualBertForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForQuestionAnswering.from_pretrained("uclanlp/visualbert-vqa")

text = "Who is eating the apple?"
inputs = tokenizer(text, return_tensors="pt")
visual_embeds = get_visual_embeddings(image).unsqueeze(0)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

inputs.update(
    {
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
    }
)

labels = torch.tensor([[0.0, 1.0]]).unsqueeze(0)  # Batch size 1, Num labels 2

outputs = model(**inputs, labels=labels)
loss = outputs.loss
scores = outputs.logits
```

## VisualBertForMultipleChoice

### `class transformers.VisualBertForMultipleChoice`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1009)

```py
( config )
```

参数

+   `config`（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VisualBert模型在顶部具有多选分类头（池化输出顶部的线性层和softmax），例如用于VCR任务。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

`forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1027)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, num_choices, sequence_length)`的`torch.FloatTensor`，*可选*）— 避免在填充标记索引上执行注意力的蒙版。蒙版值在`[0, 1]`中选择：

    +   1表示未被`掩码`的标记，

    +   0表示被`掩码`的标记。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `token_type_ids`（`torch.LongTensor`，形状为`(batch_size, num_choices, sequence_length)`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1 对应一个*句子B*的标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, num_choices, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`范围内：

    +   1 表示头部`未被屏蔽`，

    +   0 表示头部`被屏蔽`。

+   `inputs_embeds`（形状为`(batch_size, num_choices, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `visual_embeds`（形状为`(batch_size, visual_seq_length, visual_embedding_dim)`的`torch.FloatTensor`，*可选*）— 视觉输入的嵌入表示，通常使用对象检测器派生。

+   `visual_attention_mask`（形状为`(batch_size, visual_seq_length)`的`torch.FloatTensor`，*可选*）— 用于避免在视觉嵌入上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：

    +   1 用于那些`未被屏蔽`的标记，

    +   0 用于`被屏蔽`的标记。

    [什么是注意力屏蔽？](../glossary#attention-mask)

+   `visual_token_type_ids`（形状为`(batch_size, visual_seq_length)`的`torch.LongTensor`，*可选*）— 段标记索引，用于指示视觉嵌入的不同部分。

    [什么是标记类型ID？](../glossary#token-type-ids) VisualBERT的作者将*visual_token_type_ids*设置为*1*，适用于所有标记。

+   `image_text_alignment`（形状为`(batch_size, visual_seq_length, alignment_number)`的`torch.LongTensor`，*可选*）— 图像文本对齐用于决定视觉嵌入的位置ID。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `标签`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算多选分类损失的标签。索引应在`[0, ..., num_choices-1]`范围内，其中`num_choices`是输入张量第二维的大小。（参见上面的`input_ids`）

返回

[transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.MultipleChoiceModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入的不同元素。

+   `loss`（形状为*(1,)*的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 分类损失。

+   `logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）— *num_choices*是输入张量的第二维。（参见上面的*input_ids*）。

    分类得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出和每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[VisualBertForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
from transformers import AutoTokenizer, VisualBertForMultipleChoice
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForMultipleChoice.from_pretrained("uclanlp/visualbert-vcr")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

visual_embeds = get_visual_embeddings(image)
# (batch_size, num_choices, visual_seq_length, visual_embedding_dim)
visual_embeds = visual_embeds.expand(1, 2, *visual_embeds.shape)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors="pt", padding=True)
# batch size is 1
inputs_dict = {k: v.unsqueeze(0) for k, v in encoding.items()}
inputs_dict.update(
    {
        "visual_embeds": visual_embeds,
        "visual_attention_mask": visual_attention_mask,
        "visual_token_type_ids": visual_token_type_ids,
        "labels": labels,
    }
)
outputs = model(**inputs_dict)

loss = outputs.loss
logits = outputs.logits
```

## VisualBertForVisualReasoning

### `class transformers.VisualBertForVisualReasoning`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1288)

```py
( config )
```

参数

+   `config` ([VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VisualBert模型在顶部具有一个序列分类头（在池化输出的顶部有一个dropout和一个线性层），用于视觉推理，例如用于NLVR任务。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1307)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`） — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 段标记索引，指示输入的第一部分和第二部分。索引选在`[0, 1]`之间：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被遮罩，

    +   0表示头部被遮罩。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `visual_embeds`（形状为`(batch_size, visual_seq_length, visual_embedding_dim)`的`torch.FloatTensor`，*可选*）— 视觉输入的嵌入表示，通常使用对象检测器派生。

+   `visual_attention_mask`（形状为`(batch_size, visual_seq_length)`的`torch.FloatTensor`，*可选*）— 用于避免对视觉嵌入执行注意力的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示未被遮罩的标记，

    +   0表示被遮罩的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_token_type_ids`（形状为`(batch_size, visual_seq_length)`的`torch.LongTensor`，*可选*）— 段标记索引，用于指示视觉嵌入的不同部分。

    [什么是标记类型ID？](../glossary#token-type-ids) VisualBERT的作者将*visual_token_type_ids*设置为*1*以表示所有标记。

+   `image_text_alignment`（形状为`(batch_size, visual_seq_length, alignment_number)`的`torch.LongTensor`，*可选*）— 用于决定视觉嵌入的位置ID的图像-文本对齐。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`中。根据这些标签计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括各种元素，取决于配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每一层的输出）。

    每一层输出的模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[VisualBertForVisualReasoning](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForVisualReasoning)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
from transformers import AutoTokenizer, VisualBertForVisualReasoning
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForVisualReasoning.from_pretrained("uclanlp/visualbert-nlvr2")

text = "Who is eating the apple?"
inputs = tokenizer(text, return_tensors="pt")
visual_embeds = get_visual_embeddings(image).unsqueeze(0)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

inputs.update(
    {
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
    }
)

labels = torch.tensor(1).unsqueeze(0)  # Batch size 1, Num choices 2

outputs = model(**inputs, labels=labels)
loss = outputs.loss
scores = outputs.logits
```

## VisualBertForRegionToPhraseAlignment

### `class transformers.VisualBertForRegionToPhraseAlignment`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1448)

```py
( config )
```

参数

+   `config`（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

VisualBert模型具有一个用于区域到短语对齐的遮蔽语言建模头部和一个位于顶部的注意力层，例如用于Flickr30实体任务。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/visual_bert/modeling_visual_bert.py#L1469)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None visual_embeds: Optional = None visual_attention_mask: Optional = None visual_token_type_ids: Optional = None image_text_alignment: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None region_to_phrase_position: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示未被`masked`的标记，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 指示输入的第一部分和第二部分的段标记索引。索引选在`[0, 1]`之间：

    +   0对应于*句子A*标记，

    +   1 对应于一个 *sentence B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在 `[0, 1]` 之间：

    +   1 表示头部是 `not masked` 的，

    +   0 表示头部是 `masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，可以直接传递嵌入表示而不是传递 `input_ids`。如果您想要更多控制权来将 `input_ids` 索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `visual_embeds` (`torch.FloatTensor`，形状为 `(batch_size, visual_seq_length, visual_embedding_dim)`，*可选*) — 视觉输入的嵌入表示，通常使用对象检测器生成。

+   `visual_attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, visual_seq_length)`，*可选*) — 用于避免在视觉嵌入上执行注意力的掩码。掩码值选择在 `[0, 1]` 之间：

    +   1 表示未被 `masked` 的标记，

    +   0 表示头部是 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, visual_seq_length)`，*可选*) — 段标记索引，用于指示视觉嵌入的不同部分。

    [什么是标记类型 ID？](../glossary#token-type-ids) VisualBERT 的作者将 *visual_token_type_ids* 设置为 *1* 以表示所有标记。

+   `image_text_alignment` (`torch.LongTensor`，形状为 `(batch_size, visual_seq_length, alignment_number)`，*可选*) — 图像-文本对齐用于决定视觉嵌入的位置 ID。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

+   `region_to_phrase_position` (`torch.LongTensor`，形状为 `(batch_size, total_sequence_length)`，*可选*) — 描述图像嵌入位置与文本标记位置对应的位置。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size, total_sequence_length, visual_sequence_length)`，*可选*) — 用于计算掩码语言建模损失的标签。KLDivLoss 是根据这些标签和注意力层的输出计算的。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，取决于配置（[VisualBertConfig](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertConfig)）和输入。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供了 `labels` 时返回) — 分类（如果 config.num_labels==1 则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, config.num_labels)`) — 分类（如果 config.num_labels==1 则为回归）得分（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.

    模型在每一层输出的隐藏状态，以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[VisualBertForRegionToPhraseAlignment](/docs/transformers/v4.37.2/en/model_doc/visual_bert#transformers.VisualBertForRegionToPhraseAlignment)的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
# Assumption: *get_visual_embeddings(image)* gets the visual embeddings of the image in the batch.
from transformers import AutoTokenizer, VisualBertForRegionToPhraseAlignment
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForRegionToPhraseAlignment.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

text = "Who is eating the apple?"
inputs = tokenizer(text, return_tensors="pt")
visual_embeds = get_visual_embeddings(image).unsqueeze(0)
visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)
region_to_phrase_position = torch.ones((1, inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]))

inputs.update(
    {
        "region_to_phrase_position": region_to_phrase_position,
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
    }
)

labels = torch.ones(
    (1, inputs["input_ids"].shape[-1] + visual_embeds.shape[-2], visual_embeds.shape[-2])
)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
scores = outputs.logits
```
