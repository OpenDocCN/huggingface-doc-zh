# Transformer模型家族

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_summary](https://huggingface.co/docs/transformers/v4.37.2/en/model_summary)

自2017年引入[原始Transformer](https://arxiv.org/abs/1706.03762)模型以来，它已经激发了许多新颖且令人兴奋的模型，超越了自然语言处理（NLP）任务。有用于[预测蛋白质的折叠结构](https://huggingface.co/blog/deep-learning-with-proteins)、[训练猎豹奔跑](https://huggingface.co/blog/train-decision-transformers)和[时间序列预测](https://huggingface.co/blog/time-series-transformers)的模型。有这么多Transformer变体可用，很容易忽略更大的画面。所有这些模型的共同之处是它们都基于原始Transformer架构。一些模型只使用编码器或解码器，而其他一些则同时使用两者。这提供了一个有用的分类法，可以对Transformer家族中的模型进行分类和检查高层次的差异，这将帮助您理解以前未遇到的Transformer。

如果您不熟悉原始Transformer模型或需要复习，请查看Hugging Face课程中的[Transformer工作原理](https://huggingface.co/course/chapter1/4?fw=pt)章节。

[https://www.youtube.com/embed/H39Z_720T5s](https://www.youtube.com/embed/H39Z_720T5s)

## 计算机视觉

[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FacQBpeFBVvrDUlzFlkejoz%2FModelscape-timeline%3Fnode-id%3D0%253A1%26t%3Dm0zJ7m2BQ9oe0WtO-1)

### 卷积网络

长时间以来，卷积网络（CNNs）一直是计算机视觉任务的主导范式，直到[视觉Transformer](https://arxiv.org/abs/2010.11929)展示了其可扩展性和效率。即使如此，CNN的一些最佳特性，如平移不变性，是如此强大（尤其对于某些任务），以至于一些Transformer在其架构中引入了卷积。[ConvNeXt](model_doc/convnext)颠倒了这种交换，并从Transformer中引入设计选择来现代化CNN。例如，ConvNeXt使用非重叠滑动窗口将图像分块化，并使用更大的内核来增加其全局感受野。ConvNeXt还做出了几个层设计选择，以提高内存效率和性能，因此它与Transformer竞争有利！

### 编码器

[视觉Transformer（ViT）](model_doc/vit)为计算机视觉任务打开了没有卷积的大门。ViT使用标准Transformer编码器，但其主要突破在于它如何处理图像。它将图像分割成固定大小的补丁，并使用它们创建嵌入，就像将句子分割成标记一样。ViT利用Transformer的高效架构展示了与当时的CNN竞争力的结果，同时需要更少的资源进行训练。ViT很快被其他视觉模型跟随，这些模型也可以处理像分割和检测这样的密集视觉任务。

其中一个模型是[Swin](model_doc/swin) Transformer。它从较小的补丁中构建分层特征图（类似于CNN👀，不同于ViT），并在更深层中将它们与相邻的补丁合并。注意力仅在局部窗口内计算，并且在注意力层之间移动窗口以创建连接以帮助模型学习更好。由于Swin Transformer可以生成分层特征图，因此它是密集预测任务（如分割和检测）的良好候选。[SegFormer](model_doc/segformer) 也使用Transformer编码器构建分层特征图，但它在顶部添加了一个简单的多层感知器（MLP）解码器，以组合所有特征图并进行预测。

其他视觉模型，如BeIT和ViTMAE，从BERT的预训练目标中汲取灵感。[BeIT](model_doc/beit) 通过*masked image modeling (MIM)* 进行预训练；图像补丁被随机屏蔽，图像也被标记为视觉标记。BeIT 被训练以预测与被屏蔽补丁对应的视觉标记。[ViTMAE](model_doc/vitmae) 有一个类似的预训练目标，只是它必须预测像素而不是视觉标记。不寻常的是，75%的图像补丁被屏蔽！解码器从被屏蔽的标记和编码的补丁中重建像素。预训练后，解码器被丢弃，编码器准备好用于下游任务。

### 解码器

仅解码器的视觉模型很少，因为大多数视觉模型依赖编码器来学习图像表示。但对于像图像生成这样的用例，解码器是一个自然的选择，正如我们从GPT-2等文本生成模型中看到的那样。[ImageGPT](model_doc/imagegpt) 使用与GPT-2相同的架构，但它不是预测序列中的下一个标记，而是预测图像中的下一个像素。除了图像生成，ImageGPT也可以进行微调以用于图像分类。

### 编码器-解码器

视觉模型通常使用编码器（也称为骨干）来提取重要的图像特征，然后将它们传递给Transformer解码器。[DETR](model_doc/detr) 有一个预训练的骨干，但它还使用完整的Transformer编码器-解码器架构进行目标检测。编码器学习图像表示，并将其与对象查询（每个对象查询是一个专注于图像中的区域或对象的学习嵌入）结合在解码器中。DETR 预测每个对象查询的边界框坐标和类别标签。

## 自然语言处理

[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FUhbQAZDlpYW5XEpdFy6GoG%2Fnlp-model-timeline%3Fnode-id%3D0%253A1%26t%3D4mZMr4r1vDEYGJ50-1)

### 编码器

[BERT](model_doc/bert) 是一个仅包含编码器的Transformer，它会随机屏蔽输入中的某些标记，以避免看到其他标记，这样可以防止其“作弊”。预训练的目标是基于上下文预测被屏蔽的标记。这使得BERT能够充分利用左右上下文来帮助学习输入的更深层和更丰富的表示。然而，BERT的预训练策略仍有改进的空间。[RoBERTa](model_doc/roberta) 通过引入一个新的预训练配方来改进这一点，该配方包括更长时间和更大批次的训练，在每个时代随机屏蔽标记，而不仅仅是在预处理期间一次，以及移除下一个句子预测目标。

提高性能的主要策略是增加模型大小。但是训练大型模型在计算上是昂贵的。减少计算成本的一种方法是使用像[DistilBERT](model_doc/distilbert)这样的较小模型。DistilBERT使用[知识蒸馏](https://arxiv.org/abs/1503.02531) - 一种压缩技术 - 来创建一个较小版本的BERT，同时保留几乎所有的语言理解能力。

然而，大多数Transformer模型继续朝着更多参数的方向发展，导致出现了专注于提高训练效率的新模型。[ALBERT](model_doc/albert)通过两种方式降低参数数量来减少内存消耗：将更大的词汇嵌入分为两个较小的矩阵，并允许层共享参数。[DeBERTa](model_doc/deberta)添加了一个解耦的注意机制，其中单词及其位置分别编码在两个向量中。注意力是从这些单独的向量计算而来，而不是从包含单词和位置嵌入的单个向量中计算。[Longformer](model_doc/longformer)也专注于使注意力更加高效，特别是用于处理具有更长序列长度的文档。它使用局部窗口注意力（仅计算围绕每个标记的固定窗口大小的注意力）和全局注意力（仅用于特定任务标记，如`[CLS]`用于分类）的组合，以创建一个稀疏的注意力矩阵，而不是完整的注意力矩阵。

### 解码器

[GPT-2](model_doc/gpt2)是一个仅解码器的Transformer，用于预测序列中的下一个单词。它会屏蔽右侧的标记，以防模型通过向前查看来“作弊”。通过在大量文本上进行预训练，GPT-2在生成文本方面表现得非常出色，即使文本有时并不准确或真实。但是GPT-2缺乏BERT预训练的双向上下文，这使得它不适用于某些任务。[XLNET](model_doc/xlnet)结合了BERT和GPT-2的预训练目标的优点，使用排列语言建模目标（PLM）使其能够双向学习。

在GPT-2之后，语言模型变得更大，现在被称为*大型语言模型（LLMs）*。如果在足够大的数据集上进行预训练，LLMs可以展示少量甚至零-shot学习。[GPT-J](model_doc/gptj)是一个具有6B参数并在400B标记上训练的LLM。GPT-J之后是[OPT](model_doc/opt)，一系列仅解码器模型，其中最大的模型为175B，并在180B标记上训练。[BLOOM](model_doc/bloom)也在同一时间发布，该系列中最大的模型有176B参数，并在46种语言和13种编程语言中训练了366B标记。

### 编码器-解码器

[BART](model_doc/bart)保留了原始的Transformer架构，但通过*文本填充*损坏修改了预训练目标，其中一些文本段被替换为单个`mask`标记。解码器预测未损坏的标记（未来标记被屏蔽），并使用编码器的隐藏状态来帮助它。[Pegasus](model_doc/pegasus)类似于BART，但Pegasus屏蔽整个句子而不是文本段。除了遮蔽语言建模，Pegasus还通过间隙句子生成（GSG）进行预训练。GSG目标屏蔽了对文档重要的整个句子，并用`mask`标记替换它们。解码器必须从剩余的句子中生成输出。[T5](model_doc/t5)是一个更独特的模型，将所有NLP任务都转化为使用特定前缀的文本到文本问题。例如，前缀`Summarize:`表示一个总结任务。T5通过监督（GLUE和SuperGLUE）训练和自监督训练（随机抽样并丢弃15%的标记）进行预训练。

## 音频

[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2Fvrchl8jDV9YwNVPWu2W0kK%2Fspeech-and-audio-model-timeline%3Fnode-id%3D0%253A1%26t%3DmM4H8pPMuK23rClL-1)

### 编码器

[Wav2Vec2](model_doc/wav2vec2)使用Transformer编码器直接从原始音频波形中学习语音表示。它通过对比任务进行预训练，以确定一组错误的语音表示中的真实语音表示。[HuBERT](model_doc/hubert)类似于Wav2Vec2，但训练过程不同。目标标签是通过聚类步骤创建的，其中相似音频片段被分配到一个成为隐藏单元的簇中。隐藏单元被映射到一个嵌入以进行预测。

### 编码器-解码器

[Speech2Text](model_doc/speech_to_text)是一个专为自动语音识别（ASR）和语音翻译设计的语音模型。该模型接受从音频波形中提取的对数梅尔滤波器特征，并预训练自回归地生成转录或翻译。[Whisper](model_doc/whisper)也是一个ASR模型，但与许多其他语音模型不同，它是在大量✨标记的✨音频转录数据上进行预训练，以实现零样本性能。数据集中还包含大量非英语语言，这意味着Whisper也可以用于资源稀缺的语言。在结构上，Whisper类似于Speech2Text。音频信号被转换为由编码器编码的对数梅尔频谱图。解码器从编码器的隐藏状态和先前的标记中自回归地生成转录。

## 多模态

[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FcX125FQHXJS2gxeICiY93p%2Fmultimodal%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)

### 编码器

[VisualBERT](model_doc/visual_bert)是一个用于视觉-语言任务的多模态模型，发布于BERT之后不久。它结合了BERT和一个预训练的目标检测系统，将图像特征提取为视觉嵌入，与文本嵌入一起传递给BERT。VisualBERT基于未屏蔽的文本和视觉嵌入预测被屏蔽的文本，并且还必须预测文本是否与图像对齐。当ViT发布时，[ViLT](model_doc/vilt)采用了ViT的架构，因为这样更容易获取图像嵌入。图像嵌入与文本嵌入一起进行处理。从那里，ViLT通过图像文本匹配、屏蔽语言建模和整词屏蔽进行预训练。

[CLIP](model_doc/clip)采用了不同的方法，对(`图像`，`文本`)进行一对预测。一个图像编码器（ViT）和一个文本编码器（Transformer）在一个包含4亿个(`图像`，`文本`)对的数据集上进行联合训练，以最大化(`图像`，`文本`)对的图像和文本嵌入之间的相似性。在预训练之后，您可以使用自然语言指示CLIP预测给定图像的文本，反之亦然。[OWL-ViT](model_doc/owlvit)在CLIP的基础上构建，将其作为零样本目标检测的骨干。在预训练之后，添加了一个目标检测头，以对(`类别`，`边界框`)对进行集合预测。

### 编码器-解码器

光学字符识别（OCR）是一个长期存在的文本识别任务，通常涉及几个组件来理解图像并生成文本。[TrOCR](model_doc/trocr)使用端到端的变换器简化了这个过程。编码器是一种ViT风格的模型，用于图像理解，并将图像处理为固定大小的补丁。解码器接受编码器的隐藏状态，并自回归地生成文本。[Donut](model_doc/donut)是一个更通用的视觉文档理解模型，不依赖于基于OCR的方法。它使用Swin变换器作为编码器，多语言BART作为解码器。Donut经过预训练，通过根据图像和文本注释预测下一个单词来阅读文本。解码器根据提示生成一个令牌序列。提示由每个下游任务的特殊令牌表示。例如，文档解析有一个特殊的`parsing`令牌，它与编码器的隐藏状态结合，将文档解析为结构化的输出格式（JSON）。

## 强化学习

[https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1](https://www.figma.com/embed?embed_host=share&url=https%3A%2F%2Fwww.figma.com%2Ffile%2FiB3Y6RvWYki7ZuKO6tNgZq%2Freinforcement-learning%3Fnode-id%3D0%253A1%26t%3DhPQwdx3HFPWJWnVf-1)

### 解码器

决策和轨迹转换器将状态、动作和奖励视为序列建模问题。[决策变换器](model_doc/decision_transformer)生成一系列动作，这些动作基于回报、过去状态和动作，导致未来期望的回报。在最后*K*个时间步中，这三种模态都被转换为令牌嵌入，并由类似GPT的模型处理，以预测未来的动作令牌。[轨迹变换器](model_doc/trajectory_transformer)也对状态、动作和奖励进行标记化，并使用GPT架构处理它们。与专注于奖励条件的决策变换器不同，轨迹变换器使用波束搜索生成未来的动作。
