# OFT

> 原文：[https://huggingface.co/docs/peft/package_reference/oft](https://huggingface.co/docs/peft/package_reference/oft)

[正交微调（OFT）](https://hf.co/papers/2306.07280)是一种用于调整文本到图像扩散模型的方法。它通过使用正交矩阵重新参数化预训练的权重矩阵，以保留预训练模型中的信息。为了减少参数数量，OFT在正交矩阵中引入了一个块对角结构。

这篇论文的摘要是：

*大型文本到图像扩散模型在从文本提示生成逼真图像方面具有令人印象深刻的能力。如何有效地引导或控制这些强大模型执行不同的下游任务成为一个重要的开放问题。为了解决这一挑战，我们引入了一种原则性的微调方法 — 正交微调（OFT），用于将文本到图像扩散模型适应到下游任务。与现有方法不同，OFT可以明确保留超球面能量，这表征了单位超球面上的神经元之间的成对关系。我们发现这一属性对于保留文本到图像扩散模型的语义生成能力至关重要。为了提高微调的稳定性，我们进一步提出了约束正交微调（COFT），它对超球面施加了额外的半径约束。具体来说，我们考虑了两个重要的微调文本到图像任务：以主题驱动的生成为目标，即在给定主题的几张图像和文本提示的情况下生成特定主题的图像，以及可控生成为目标，即使模型能够接收额外的控制信号。我们在实证上展示了我们的OFT框架在生成质量和收敛速度方面优于现有方法*。

## OFTConfig

### `class peft.OFTConfig`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/config.py#L22)

```py
( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False rank_pattern: Optional[dict] = <factory> alpha_pattern: Optional[dict] = <factory> r: int = 8 module_dropout: float = 0.0 target_modules: Union = None init_weights: bool = True layers_to_transform: Union = None layers_pattern: Optional = None modules_to_save: Optional = None coft: bool = False eps: float = 6e-05 block_share: bool = False )
```

参数

+   `r` (`int`) — OFT秩。

+   `module_dropout` (`int`) — 训练期间禁用OFT模块的丢弃概率。

+   `target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则匹配。当传递一个字符串列表时，将执行精确匹配或检查模块名称是否以任何传递的字符串结尾。如果指定为'all-linear'，则选择所有线性模块，不包括输出层。如果未指定，则根据模型架构选择模块。如果架构未知，将引发错误 — 在这种情况下，您应该手动指定目标模块。

+   `init_weights` (`bool`) — 是否执行OFT权重的初始化。

+   `layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递了一个整数列表，它将应用适配器到此列表中指定的层索引。如果传递了一个单个整数，它将在此索引处的层上应用转换。

+   `layers_pattern` (`str`) — 层模式名称，仅在`layers_to_transform`与`None`不同的情况下使用。

+   `rank_pattern` (`dict`) — 层名称或正则表达式到与`r`指定的默认秩不同的秩的映射。

+   `modules_to_save` (`List[str]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。

+   `coft` (`bool`) — 是否使用OFT的约束变体，默认为关闭。

+   `eps` (`float`) — COFT的控制强度。旋转的自由度。仅当`coft`设置为True时才有效。

+   `block_share` (`bool`) — 是否在块之间共享OFT参数。默认为`False`。

这是用于存储[OFTModel](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTModel)配置的配置类。

## OFTModel

### `class peft.OFTModel`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/model.py#L26)

```py
( model config adapter_name ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 适配器调谐层将附加到的模型。

+   `config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig)) — OFT模型的配置。

+   `adapter_name` (`str`) — 适配器的名称，默认为`"default"`。

返回

`torch.nn.Module`

OFT模型。

从预训练模型创建正交微调模型。该方法在[https://arxiv.org/abs/2306.07280](https://arxiv.org/abs/2306.07280)中有描述。

示例:

```py
>>> from diffusers import StableDiffusionPipeline
>>> from peft import OFTModel, OFTConfig

>>> config_te = OFTConfig(
...     r=8,
...     target_modules=["k_proj", "q_proj", "v_proj", "out_proj", "fc1", "fc2"],
...     module_dropout=0.0,
...     init_weights=True,
... )
>>> config_unet = OFTConfig(
...     r=8,
...     target_modules=[
...         "proj_in",
...         "proj_out",
...         "to_k",
...         "to_q",
...         "to_v",
...         "to_out.0",
...         "ff.net.0.proj",
...         "ff.net.2",
...     ],
...     module_dropout=0.0,
...     init_weights=True,
... )

>>> model = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
>>> model.text_encoder = OFTModel(model.text_encoder, config_te, "default")
>>> model.unet = OFTModel(model.unet, config_unet, "default")
```

**属性**:

+   `model` (`~torch.nn.Module`) — 要适配的模型。

+   `peft_config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig)): OFT模型的配置。
