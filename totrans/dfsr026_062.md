# ControlNet

> 原文链接：[`huggingface.co/docs/diffusers/training/controlnet`](https://huggingface.co/docs/diffusers/training/controlnet)

[ControlNet](https://hf.co/papers/2302.05543)模型是在另一个预训练模型之上训练的适配器。它通过使用额外的输入图像对模型进行条件化，从而允许更大程度上控制图像生成。输入图像可以是 canny 边缘、深度图、人体姿势等。

如果您在具有有限 vRAM 的 GPU 上进行训练，您应该尝试在训练命令中启用`gradient_checkpointing`、`gradient_accumulation_steps`和`mixed_precision`参数。您还可以通过使用 xFormers 中的内存高效注意力来减少内存占用。JAX/Flax 训练也支持在 TPU 和 GPU 上进行高效训练，但不支持梯度检查点或 xFormers。如果您想要使用 Flax 更快地进行训练，您应该拥有大于 30GB 内存的 GPU。

本指南将探讨[train_controlnet.py](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py)训练脚本，以帮助您熟悉它，以及如何为您自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

然后转到包含训练脚本的示例文件夹，并安装您正在使用的脚本所需的依赖项：

PyTorchFlax

```py
cd examples/controlnet
pip install -r requirements.txt
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多信息。

初始化🤗 Accelerate 环境：

```py
accelerate config
```

在不选择任何配置的情况下设置默认的🤗 Accelerate 环境：

```py
accelerate config default
```

或者，如果您的环境不支持交互式 shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看创建用于训练的数据集指南，了解如何创建适用于训练脚本的数据集。

以下部分突出了训练脚本的重要部分，以便了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py)，如果您有任何问题或疑虑，请告诉我们。

## 脚本参数

训练脚本提供了许多参数，帮助您自定义训练运行。所有参数及其描述都可以在[`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train_controlnet.py#L231)函数中找到。该函数为每个参数提供了默认值，如训练批量大小和学习率，但如果您愿意，也可以在训练命令中设置自己的值。

例如，为了加快使用 fp16 格式的混合精度进行训练，请在训练命令中添加`--mixed_precision`参数：

```py
accelerate launch train_controlnet.py \
  --mixed_precision="fp16"
```

许多基本和重要参数在文本到图像训练指南中有描述，因此本指南只关注 ControlNet 的相关参数：

+   `--max_train_samples`：训练样本的数量；这可以降低以加快训练速度，但如果您想要流式传输非常大的数据集，您需要在训练命令中包含此参数和`--streaming`参数

+   `--gradient_accumulation_steps`：在向后传递之前累积的更新步骤数；这使您可以使用比 GPU 内存通常处理的更大批量大小进行训练

### Min-SNR 加权

[Min-SNR](https://huggingface.co/papers/2303.09556)加权策略可以通过重新平衡损失来帮助训练，以实现更快的收敛。训练脚本支持预测`epsilon`（噪音）或`v_prediction`，但 Min-SNR 与两种预测类型兼容。此加权策略仅受 PyTorch 支持，在 Flax 训练脚本中不可用。

添加`--snr_gamma`参数，并将其设置为推荐值 5.0：

```py
accelerate launch train_controlnet.py \
  --snr_gamma=5.0
```

## 训练脚本

与脚本参数一样，文本到图像训练指南中提供了对训练脚本的整体介绍。而本指南则着眼于 ControlNet 脚本的相关部分。

训练脚本有一个[`make_train_dataset`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train_controlnet.py#L582)函数，用于对数据集进行预处理，包括图像转换和标题标记化。您将看到，除了通常的标题标记化和图像转换之外，脚本还包括用于条件图像的转换。

如果您在 TPU 上流式传输数据集，性能可能会受到🤗数据集库的限制，该库未针对图像进行优化。为了确保最大吞吐量，建议您探索其他数据集格式，如[WebDataset](https://webdataset.github.io/webdataset/)、[TorchData](https://github.com/pytorch/data)和[TensorFlow 数据集](https://www.tensorflow.org/datasets/tfless_tfds)。

```py
conditioning_image_transforms = transforms.Compose(
    [
        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
        transforms.CenterCrop(args.resolution),
        transforms.ToTensor(),
    ]
)
```

在[`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train_controlnet.py#L713)函数中，您将找到用于加载分词器、文本编码器、调度器和模型的代码。这也是 ControlNet 模型加载的地方，可以从现有权重加载，也可以从 UNet 随机初始化：

```py
if args.controlnet_model_name_or_path:
    logger.info("Loading existing controlnet weights")
    controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)
else:
    logger.info("Initializing controlnet weights from unet")
    controlnet = ControlNetModel.from_unet(unet)
```

[优化器](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train_controlnet.py#L871)已设置为更新 ControlNet 参数：

```py
params_to_optimize = controlnet.parameters()
optimizer = optimizer_class(
    params_to_optimize,
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

最后，在[训练循环](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/controlnet/train_controlnet.py#L943)中，将条件文本嵌入和图像传递给 ControlNet 模型的下游和中间块：

```py
encoder_hidden_states = text_encoder(batch["input_ids"])[0]
controlnet_image = batch["conditioning_pixel_values"].to(dtype=weight_dtype)

down_block_res_samples, mid_block_res_sample = controlnet(
    noisy_latents,
    timesteps,
    encoder_hidden_states=encoder_hidden_states,
    controlnet_cond=controlnet_image,
    return_dict=False,
)
```

如果您想了解训练循环的工作原理，请查看理解管道、模型和调度器教程，该教程解释了去噪过程的基本模式。

## 启动脚本

现在您已经准备好启动训练脚本了！🚀

本指南使用[fusing/fill50k](https://huggingface.co/datasets/fusing/fill50k)数据集，但请记住，如果需要，您可以创建并使用自己的数据集（请参阅创建用于训练的数据集指南）。

将环境变量`MODEL_NAME`设置为 Hub 上的模型 ID 或本地模型的路径，将`OUTPUT_DIR`设置为要保存模型的位置。

下载以下图像以进行训练条件：

```py
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png
```

在启动脚本之前还有一件事！根据您拥有的 GPU，您可能需要启用某些优化来训练 ControlNet。此脚本中的默认配置需要约 38GB 的 vRAM。如果您要在多个 GPU 上进行训练，请在`accelerate launch`命令中添加`--multi_gpu`参数。

16GB12GB8GB

在 16GB GPU 上，您可以使用 bitsandbytes 8 位优化器和梯度检查点来优化您的训练运行。安装 bitsandbytes：

```py
pip install bitsandbytes
```

然后，在训练命令中添加以下参数：

```py
accelerate launch train_controlnet.py \
  --gradient_checkpointing \
  --use_8bit_adam \
```

PyTorchFlax

```py
export MODEL_DIR="runwayml/stable-diffusion-v1-5"
export OUTPUT_DIR="path/to/save/model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
 --push_to_hub
```

训练完成后，您可以使用新训练的模型进行推断！

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import torch

controlnet = ControlNetModel.from_pretrained("path/to/controlnet", torch_dtype=torch.float16)
pipeline = StableDiffusionControlNetPipeline.from_pretrained(
    "path/to/base/model", controlnet=controlnet, torch_dtype=torch.float16
).to("cuda")

control_image = load_image("./conditioning_image_1.png")
prompt = "pale golden rod circle with old lace background"

generator = torch.manual_seed(0)
image = pipe(prompt, num_inference_steps=20, generator=generator, image=control_image).images[0]
image.save("./output.png")
```

## 稳定扩散 XL

稳定扩散 XL（SDXL）是一个强大的文本到图像模型，可以生成高分辨率图像，并在其架构中添加了第二个文本编码器。使用[`train_controlnet_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet_sdxl.py)脚本来训练 SDXL 模型的 ControlNet 适配器。

有关 SDXL 训练脚本的详细讨论，请参阅 SDXL 训练指南。

## 下一步

恭喜您成功训练了自己的 ControlNet！要了解如何使用您的新模型，以下指南可能会有所帮助：

+   学习如何在各种任务中使用 ControlNet 进行推断。
