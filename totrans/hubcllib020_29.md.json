["```py\nfrom huggingface_hub import HfApi, list_models\n\n# Use root method\nmodels = list_models()\n\n# Or configure a HfApi client\nhf_api = HfApi(\n    endpoint=\"https://huggingface.co\", # Can be a Private Hub endpoint.\n    token=\"hf_xxx\", # Token is not persisted on the machine.\n)\nmodels = hf_api.list_models()\n```", "```py\n>>> from huggingface_hub import add_collection_item\n>>> collection = add_collection_item(\n...     collection_slug=\"davanstrien/climate-64f99dc2a5067f6b65531bab\",\n...     item_id=\"pierre-loic/climate-news-articles\",\n...     item_type=\"dataset\"\n... )\n>>> collection.items[-1].item_id\n\"pierre-loic/climate-news-articles\"\n# ^item got added to the collection on last position\n\n# Add item with a note\n>>> add_collection_item(\n...     collection_slug=\"davanstrien/climate-64f99dc2a5067f6b65531bab\",\n...     item_id=\"datasets/climate_fever\",\n...     item_type=\"dataset\"\n...     note=\"This dataset adopts the FEVER methodology that consists of 1,535 real-world claims regarding climate-change collected on the internet.\"\n... )\n(...)\n```", "```py\n>>> new_title = \"New title, fixing a typo\"\n>>> HfApi().rename_discussion(\n...     repo_id=\"username/repo_name\",\n...     discussion_num=34\n...     new_title=new_title\n... )\n# DiscussionStatusChange(id='deadbeef0000000', type='status-change', ...)\n\n```", "```py\n\n>>> comment = \"\"\"\n... Hello @otheruser!\n...\n... # This is a title\n...\n... **This is bold**, *this is italic* and ~this is strikethrough~\n... And [this](http://url) is a link\n... \"\"\"\n\n>>> HfApi().comment_discussion(\n...     repo_id=\"username/repo_name\",\n...     discussion_num=34\n...     comment=comment\n... )\n# DiscussionComment(id='deadbeef0000000', type='comment', ...)\n\n```", "```py\n>>> from huggingface_hub import create_collection\n>>> collection = create_collection(\n...     title=\"ICCV 2023\",\n...     description=\"Portfolio of models, papers and demos I presented at ICCV 2023\",\n... )\n>>> collection.slug\n\"username/iccv-2023-64f9a55bb3115b4f513ec026\"\n```", "```py\n>>> from huggingface_hub import HfApi, plan_multi_commits\n>>> addition_commits, deletion_commits = plan_multi_commits(\n...     operations=[\n...          CommitOperationAdd(...),\n...          CommitOperationAdd(...),\n...          CommitOperationDelete(...),\n...          CommitOperationDelete(...),\n...          CommitOperationAdd(...),\n...     ],\n... )\n>>> HfApi().create_commits_on_pr(\n...     repo_id=\"my-cool-model\",\n...     addition_commits=addition_commits,\n...     deletion_commits=deletion_commits,\n...     (...)\n...     verbose=True,\n... )\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> create_inference_endpoint(\n...     \"my-endpoint-name\",\n...     repository=\"gpt2\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"cpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"c6i\",\n... )\n>>> endpoint\nInferenceEndpoint(name='my-endpoint-name', status=\"pending\",...)\n\n# Run inference on the endpoint\n>>> endpoint.client.text_generation(...)\n\"...\"\n```", "```py\n# Start an Inference Endpoint running Zephyr-7b-beta on TGI\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> create_inference_endpoint(\n...     \"aws-zephyr-7b-beta-0486\",\n...     repository=\"HuggingFaceH4/zephyr-7b-beta\",\n...     framework=\"pytorch\",\n...     task=\"text-generation\",\n...     accelerator=\"gpu\",\n...     vendor=\"aws\",\n...     region=\"us-east-1\",\n...     type=\"protected\",\n...     instance_size=\"medium\",\n...     instance_type=\"g5.2xlarge\",\n...     custom_image={\n...         \"health_route\": \"/health\",\n...         \"env\": {\n...             \"MAX_BATCH_PREFILL_TOKENS\": \"2048\",\n...             \"MAX_INPUT_LENGTH\": \"1024\",\n...             \"MAX_TOTAL_TOKENS\": \"1512\",\n...             \"MODEL_ID\": \"/repository\"\n...         },\n...         \"url\": \"ghcr.io/huggingface/text-generation-inference:1.1.0\",\n...     },\n... )\n\n```", "```py\n>>> from huggingface_hub import delete_collection\n>>> collection = delete_collection(\"username/useless-collection-64f9a55bb3115b4f513ec026\", missing_ok=True)\n```", "```py\n>>> from huggingface_hub import get_collection, delete_collection_item\n\n# Get collection first\n>>> collection = get_collection(\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\")\n\n# Delete item based on its ID\n>>> delete_collection_item(\n...     collection_slug=\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\",\n...     item_object_id=collection.items[-1].item_object_id,\n... )\n```", "```py\n>>> from huggingface_hub import duplicate_space\n\n# Duplicate a Space to your account\n>>> duplicate_space(\"multimodalart/dreambooth-training\")\nRepoUrl('https://huggingface.co/spaces/nateraw/dreambooth-training',...)\n\n# Can set custom destination id and visibility flag.\n>>> duplicate_space(\"multimodalart/dreambooth-training\", to_id=\"my-dreambooth\", private=True)\nRepoUrl('https://huggingface.co/spaces/nateraw/my-dreambooth',...)\n```", "```py\n>>> from huggingface_hub import file_exists\n>>> file_exists(\"bigcode/starcoder\", \"config.json\")\nTrue\n>>> file_exists(\"bigcode/starcoder\", \"not-a-file\")\nFalse\n>>> file_exists(\"bigcode/not-a-repo\", \"config.json\")\nFalse\n```", "```py\n>>> from huggingface_hub import get_collection\n>>> collection = get_collection(\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\")\n>>> collection.title\n'Recent models'\n>>> len(collection.items)\n37\n>>> collection.items[0]\nCollectionItem(\n    item_object_id='651446103cd773a050bf64c2',\n    item_id='TheBloke/U-Amethyst-20B-AWQ',\n    item_type='model',\n    position=88,\n    note=None\n)\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> endpoint = api.get_inference_endpoint(\"my-text-to-image\")\n>>> endpoint\nInferenceEndpoint(name='my-text-to-image', ...)\n\n# Get status\n>>> endpoint.status\n'running'\n>>> endpoint.url\n'https://my-text-to-image.region.vendor.endpoints.huggingface.cloud'\n\n# Run inference\n>>> endpoint.client.text_to_image(...)\n```", "```py\n>>> from huggingface_hub import get_paths_info\n>>> paths_info = get_paths_info(\"allenai/c4\", [\"README.md\", \"en\"], repo_type=\"dataset\")\n>>> paths_info\n[\n    RepoFile(path='README.md', size=2379, blob_id='f84cb4c97182890fc1dbdeaf1a6a468fd27b4fff', lfs=None, last_commit=None, security=None),\n    RepoFolder(path='en', tree_id='dc943c4c40f53d02b31ced1defa7e5f438d5862e', last_commit=None)\n]\n```", "```py\n>>> from huggingface_hub import get_repo_discussions\n>>> discussions_list = list(get_repo_discussions(repo_id=\"bert-base-uncased\"))\n```", "```py\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(repo_id=\"bert-base-uncased\"):\n...     print(discussion.num, discussion.title)\n```", "```py\n# Parse repo with single weights file\n>>> metadata = get_safetensors_metadata(\"bigscience/bloomz-560m\")\n>>> metadata\nSafetensorsRepoMetadata(\n    metadata=None,\n    sharded=False,\n    weight_map={'h.0.input_layernorm.bias': 'model.safetensors', ...},\n    files_metadata={'model.safetensors': SafetensorsFileMetadata(...)}\n)\n>>> metadata.files_metadata[\"model.safetensors\"].metadata\n{'format': 'pt'}\n\n# Parse repo with sharded model\n>>> metadata = get_safetensors_metadata(\"bigscience/bloom\")\nParse safetensors files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72/72 [00:12<00:00,  5.78it/s]\n>>> metadata\nSafetensorsRepoMetadata(metadata={'total_size': 352494542848}, sharded=True, weight_map={...}, files_metadata={...})\n>>> len(metadata.files_metadata)\n72  # All safetensors files have been fetched\n\n# Parse repo with sharded model\n>>> get_safetensors_metadata(\"runwayml/stable-diffusion-v1-5\")\nNotASafetensorsRepoError: 'runwayml/stable-diffusion-v1-5' is not a safetensors repo. Couldn't find 'model.safetensors.index.json' or 'model.safetensors' files.\n```", "```py\n[  96]  .\n\u2514\u2500\u2500 [ 160]  models--julien-c--EsperBERTo-small\n    \u251c\u2500\u2500 [ 160]  blobs\n    \u2502   \u251c\u2500\u2500 [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n    \u2502   \u251c\u2500\u2500 [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\n    \u2502   \u2514\u2500\u2500 [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\n    \u251c\u2500\u2500 [  96]  refs\n    \u2502   \u2514\u2500\u2500 [  40]  main\n    \u2514\u2500\u2500 [ 128]  snapshots\n        \u251c\u2500\u2500 [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\n        \u2502   \u251c\u2500\u2500 [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\n        \u2502   \u2514\u2500\u2500 [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n        \u2514\u2500\u2500 [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\n            \u251c\u2500\u2500 [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\n            \u2514\u2500\u2500 [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\n```", "```py\n>>> from huggingface_hub import like, list_liked_repos, unlike\n>>> like(\"gpt2\")\n>>> \"gpt2\" in list_liked_repos().models\nTrue\n>>> unlike(\"gpt2\")\n>>> \"gpt2\" in list_liked_repos().models\nFalse\n```", "```py\n>>> from huggingface_hub import list_accepted_access_requests\n\n>>> requests = list_accepted_access_requests(\"meta-llama/Llama-2-7b\")\n>>> len(requests)\n411\n>>> requests[0]\n[\n    AccessRequest(\n        username='clem',\n        fullname='Clem \ud83e\udd17',\n        email='***',\n        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),\n        status='accepted',\n        fields=None,\n    ),\n    ...\n]\n```", "```py\n>>> from huggingface_hub import HfApi\n\n>>> api = HfApi()\n\n>>> # List all datasets\n>>> api.list_datasets()\n\n>>> # List only the text classification datasets\n>>> api.list_datasets(filter=\"task_categories:text-classification\")\n>>> # Using the `DatasetFilter`\n>>> filt = DatasetFilter(task_categories=\"text-classification\")\n\n>>> # List only the datasets in russian for language modeling\n>>> api.list_datasets(\n...     filter=(\"language:ru\", \"task_ids:language-modeling\")\n... )\n>>> # Using the `DatasetFilter`\n>>> filt = DatasetFilter(language=\"ru\", task_ids=\"language-modeling\")\n\n>>> api.list_datasets(filter=filt)\n```", "```py\n>>> from huggingface_hub import HfApi\n\n>>> api = HfApi()\n\n>>> # List all datasets with \"text\" in their name\n>>> api.list_datasets(search=\"text\")\n\n>>> # List all datasets with \"text\" in their name made by google\n>>> api.list_datasets(search=\"text\", author=\"google\")\n```", "```py\n>>> from huggingface_hub import list_files_info\n>>> files_info = list_files_info(\"lysandre/arxiv-nlp\", [\"README.md\", \"config.json\"])\n>>> files_info\n<generator object HfApi.list_files_info at 0x7f93b848e730>\n>>> list(files_info)\n[\n    RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),\n    RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None)\n]\n```", "```py\n>>> from huggingface_hub import list_files_info\n>>> files_info = list_files_info(\"prompthero/openjourney-v4\", expand=True)\n>>> list(files_info)\n[\n    RepoFile(\n        path='safety_checker/pytorch_model.bin',\n        size=1216064769,\n        blob_id='c8835557a0d3af583cb06c7c154b7e54a069c41d',\n        lfs={\n            'size': 1216064769,\n            'sha256': '16d28f2b37109f222cdc33620fdd262102ac32112be0352a7f77e9614b35a394',\n            'pointer_size': 135\n        },\n        last_commit={\n            'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\n            'title': 'Upload diffusers weights (#48)',\n            'date': datetime.datetime(2023, 3, 21, 10, 5, 27, tzinfo=datetime.timezone.utc)\n        },\n        security={\n            'safe': True,\n            'av_scan': {\n                'virusFound': False,\n                'virusNames': None\n            },\n            'pickle_import_scan': {\n                'highestSafetyLevel': 'innocuous',\n                'imports': [\n                    {'module': 'torch', 'name': 'FloatStorage', 'safety': 'innocuous'},\n                    {'module': 'collections', 'name': 'OrderedDict', 'safety': 'innocuous'},\n                    {'module': 'torch', 'name': 'LongStorage', 'safety': 'innocuous'},\n                    {'module': 'torch._utils', 'name': '_rebuild_tensor_v2', 'safety': 'innocuous'}\n                ]\n            }\n        }\n    ),\n    RepoFile(\n        path='scheduler/scheduler_config.json',\n        size=465,\n        blob_id='70d55e3e9679f41cbc66222831b38d5abce683dd',\n        lfs=None,\n        last_commit={\n            'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\n            'title': 'Upload diffusers weights (#48)',\n            'date': datetime.datetime(2023, 3, 21, 10, 5, 27, tzinfo=datetime.timezone.utc)},\n            security={\n                'safe': True,\n                'av_scan': {\n                    'virusFound': False,\n                    'virusNames': None\n                },\n                'pickle_import_scan': None\n            }\n    ),\n    ...\n]\n```", "```py\n>>> from huggingface_hub import list_files_info\n>>> [info.path for info in list_files_info(\"stabilityai/stable-diffusion-2\", \"vae\") if info.lfs is not None]\n['vae/diffusion_pytorch_model.bin', 'vae/diffusion_pytorch_model.safetensors']\n```", "```py\n>>> from huggingface_hub import list_files_info\n>>> [info.path for info in list_files_info(\"glue\", repo_type=\"dataset\")]\n['.gitattributes', 'README.md', 'dataset_infos.json', 'glue.py']\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> api.list_inference_endpoints()\n[InferenceEndpoint(name='my-endpoint', ...), ...]\n```", "```py\n>>> from huggingface_hub import list_liked_repos\n\n>>> likes = list_liked_repos(\"julien-c\")\n\n>>> likes.user\n\"julien-c\"\n\n>>> likes.models\n[\"osanseviero/streamlit_1.15\", \"Xhaheen/ChatGPT_HF\", ...]\n```", "```py\n>>> from huggingface_hub import HfApi\n\n>>> api = HfApi()\n\n>>> # List all models\n>>> api.list_models()\n\n>>> # List only the text classification models\n>>> api.list_models(filter=\"text-classification\")\n>>> # Using the `ModelFilter`\n>>> filt = ModelFilter(task=\"text-classification\")\n\n>>> # List only models from the AllenNLP library\n>>> api.list_models(filter=\"allennlp\")\n```", "```py\n>>> from huggingface_hub import HfApi\n\n>>> api = HfApi()\n\n>>> # List all models with \"bert\" in their name\n>>> api.list_models(search=\"bert\")\n\n>>> # List all models with \"bert\" in their name made by google\n>>> api.list_models(search=\"bert\", author=\"google\")\n```", "```py\n>>> from huggingface_hub import list_pending_access_requests, accept_access_request\n\n# List pending requests\n>>> requests = list_pending_access_requests(\"meta-llama/Llama-2-7b\")\n>>> len(requests)\n411\n>>> requests[0]\n[\n    AccessRequest(\n        username='clem',\n        fullname='Clem \ud83e\udd17',\n        email='***',\n        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),\n        status='pending',\n        fields=None,\n    ),\n    ...\n]\n\n# Accept Clem's request\n>>> accept_access_request(\"meta-llama/Llama-2-7b\", \"clem\")\n```", "```py\n>>> from huggingface_hub import list_rejected_access_requests\n\n>>> requests = list_rejected_access_requests(\"meta-llama/Llama-2-7b\")\n>>> len(requests)\n411\n>>> requests[0]\n[\n    AccessRequest(\n        username='clem',\n        fullname='Clem \ud83e\udd17',\n        email='***',\n        timestamp=datetime.datetime(2023, 11, 23, 18, 4, 53, 828000, tzinfo=datetime.timezone.utc),\n        status='rejected',\n        fields=None,\n    ),\n    ...\n]\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n\n# Commits are sorted by date (last commit first)\n>>> initial_commit = api.list_repo_commits(\"gpt2\")[-1]\n\n# Initial commit is always a system commit containing the `.gitattributes` file.\n>>> initial_commit\nGitCommitInfo(\n    commit_id='9b865efde13a30c13e0a33e536cf3e4a5a9d71d8',\n    authors=['system'],\n    created_at=datetime.datetime(2019, 2, 18, 10, 36, 15, tzinfo=datetime.timezone.utc),\n    title='initial commit',\n    message='',\n    formatted_title=None,\n    formatted_message=None\n)\n\n# Create an empty branch by deriving from initial commit\n>>> api.create_branch(\"gpt2\", \"new_empty_branch\", revision=initial_commit.commit_id)\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> api.list_repo_refs(\"gpt2\")\nGitRefs(branches=[GitRefInfo(name='main', ref='refs/heads/main', target_commit='e7da7f221d5bf496a48136c0cd264e630fe9fcc8')], converts=[], tags=[])\n\n>>> api.list_repo_refs(\"bigcode/the-stack\", repo_type='dataset')\nGitRefs(\n    branches=[\n        GitRefInfo(name='main', ref='refs/heads/main', target_commit='18edc1591d9ce72aa82f56c4431b3c969b210ae3'),\n        GitRefInfo(name='v1.1.a1', ref='refs/heads/v1.1.a1', target_commit='f9826b862d1567f3822d3d25649b0d6d22ace714')\n    ],\n    converts=[],\n    tags=[\n        GitRefInfo(name='v1.0', ref='refs/tags/v1.0', target_commit='c37a8cd1e382064d8aced5e05543c5f7753834da')\n    ]\n)\n```", "```py\n>>> from huggingface_hub import list_repo_tree\n>>> repo_tree = list_repo_tree(\"lysandre/arxiv-nlp\")\n>>> repo_tree\n<generator object HfApi.list_repo_tree at 0x7fa4088e1ac0>\n>>> list(repo_tree)\n[\n    RepoFile(path='.gitattributes', size=391, blob_id='ae8c63daedbd4206d7d40126955d4e6ab1c80f8f', lfs=None, last_commit=None, security=None),\n    RepoFile(path='README.md', size=391, blob_id='43bd404b159de6fba7c2f4d3264347668d43af25', lfs=None, last_commit=None, security=None),\n    RepoFile(path='config.json', size=554, blob_id='2f9618c3a19b9a61add74f70bfb121335aeef666', lfs=None, last_commit=None, security=None),\n    RepoFile(\n        path='flax_model.msgpack', size=497764107, blob_id='8095a62ccb4d806da7666fcda07467e2d150218e',\n        lfs={'size': 497764107, 'sha256': 'd88b0d6a6ff9c3f8151f9d3228f57092aaea997f09af009eefd7373a77b5abb9', 'pointer_size': 134}, last_commit=None, security=None\n    ),\n    RepoFile(path='merges.txt', size=456318, blob_id='226b0752cac7789c48f0cb3ec53eda48b7be36cc', lfs=None, last_commit=None, security=None),\n    RepoFile(\n        path='pytorch_model.bin', size=548123560, blob_id='64eaa9c526867e404b68f2c5d66fd78e27026523',\n        lfs={'size': 548123560, 'sha256': '9be78edb5b928eba33aa88f431551348f7466ba9f5ef3daf1d552398722a5436', 'pointer_size': 134}, last_commit=None, security=None\n    ),\n    RepoFile(path='vocab.json', size=898669, blob_id='b00361fece0387ca34b4b8b8539ed830d644dbeb', lfs=None, last_commit=None, security=None)]\n]\n```", "```py\n>>> from huggingface_hub import list_repo_tree\n>>> repo_tree = list_repo_tree(\"prompthero/openjourney-v4\", expand=True)\n>>> list(repo_tree)\n[\n    RepoFolder(\n        path='feature_extractor',\n        tree_id='aa536c4ea18073388b5b0bc791057a7296a00398',\n        last_commit={\n            'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\n            'title': 'Upload diffusers weights (#48)',\n            'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\n        }\n    ),\n    RepoFolder(\n        path='safety_checker',\n        tree_id='65aef9d787e5557373fdf714d6c34d4fcdd70440',\n        last_commit={\n            'oid': '47b62b20b20e06b9de610e840282b7e6c3d51190',\n            'title': 'Upload diffusers weights (#48)',\n            'date': datetime.datetime(2023, 3, 21, 9, 5, 27, tzinfo=datetime.timezone.utc)\n        }\n    ),\n    RepoFile(\n        path='model_index.json',\n        size=582,\n        blob_id='d3d7c1e8c3e78eeb1640b8e2041ee256e24c9ee1',\n        lfs=None,\n        last_commit={\n            'oid': 'b195ed2d503f3eb29637050a886d77bd81d35f0e',\n            'title': 'Fix deprecation warning by changing `CLIPFeatureExtractor` to `CLIPImageProcessor`. (#54)',\n            'date': datetime.datetime(2023, 5, 15, 21, 41, 59, tzinfo=datetime.timezone.utc)\n        },\n        security={\n            'safe': True,\n            'av_scan': {'virusFound': False, 'virusNames': None},\n            'pickle_import_scan': None\n        }\n    )\n    ...\n]\n```", "```py\n>>> from huggingface_hub import CommitOperationAdd, preupload_lfs_files, create_commit, create_repo\n\n>>> repo_id = create_repo(\"test_preupload\").repo_id\n\n# Generate and preupload LFS files one by one\n>>> operations = [] # List of all `CommitOperationAdd` objects that will be generated\n>>> for i in range(5):\n...     content = ... # generate binary content\n...     addition = CommitOperationAdd(path_in_repo=f\"shard_{i}_of_5.bin\", path_or_fileobj=content)\n...     preupload_lfs_files(repo_id, additions=[addition]) # upload + free memory\n...     operations.append(addition)\n\n# Create commit\n>>> create_commit(repo_id, operations=operations, commit_message=\"Commit all shards\")\n```", "```py\n>>> new_title = \"New title, fixing a typo\"\n>>> HfApi().rename_discussion(\n...     repo_id=\"username/repo_name\",\n...     discussion_num=34\n...     new_title=new_title\n... )\n# DiscussionTitleChange(id='deadbeef0000000', type='title-change', ...)\n\n```", "```py\n>>> from huggingface_hub import repo_exists\n>>> repo_exists(\"huggingface/transformers\")\nTrue\n>>> repo_exists(\"huggingface/not-a-repo\")\nFalse\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n>>> future = api.run_as_future(api.whoami) # instant\n>>> future.done()\nFalse\n>>> future.result() # wait until complete and return result\n(...)\n>>> future.done()\nTrue\n```", "```py\n>>> from huggingface_hub import HfApi\n>>> api = HfApi()\n\n# Create repo\n>>> repo_id = api.create_repo(\"test-squash\").repo_id\n\n# Make a lot of commits.\n>>> api.upload_file(repo_id=repo_id, path_in_repo=\"file.txt\", path_or_fileobj=b\"content\")\n>>> api.upload_file(repo_id=repo_id, path_in_repo=\"lfs.bin\", path_or_fileobj=b\"content\")\n>>> api.upload_file(repo_id=repo_id, path_in_repo=\"file.txt\", path_or_fileobj=b\"another_content\")\n\n# Squash history\n>>> api.super_squash_history(repo_id=repo_id)\n```", "```py\n>>> from huggingface_hub import like, list_liked_repos, unlike\n>>> like(\"gpt2\")\n>>> \"gpt2\" in list_liked_repos().models\nTrue\n>>> unlike(\"gpt2\")\n>>> \"gpt2\" in list_liked_repos().models\nFalse\n```", "```py\n>>> from huggingface_hub import get_collection, update_collection_item\n\n# Get collection first\n>>> collection = get_collection(\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\")\n\n# Update item based on its ID (add note + update position)\n>>> update_collection_item(\n...     collection_slug=\"TheBloke/recent-models-64f9a55bb3115b4f513ec026\",\n...     item_object_id=collection.items[-1].item_object_id,\n...     note=\"Newly updated model!\"\n...     position=0,\n... )\n```", "```py\n>>> from huggingface_hub import update_collection_metadata\n>>> collection = update_collection_metadata(\n...     collection_slug=\"username/iccv-2023-64f9a55bb3115b4f513ec026\",\n...     title=\"ICCV Oct. 2023\"\n...     description=\"Portfolio of models, datasets, papers and demos I presented at ICCV Oct. 2023\",\n...     private=False,\n...     theme=\"pink\",\n... )\n>>> collection.slug\n\"username/iccv-oct-2023-64f9a55bb3115b4f513ec026\"\n# ^collection slug got updated but not the trailing ID\n```", "```py\n>>> from huggingface_hub import upload_file\n\n>>> with open(\"./local/filepath\", \"rb\") as fobj:\n...     upload_file(\n...         path_or_fileobj=fileobj,\n...         path_in_repo=\"remote/file/path.h5\",\n...         repo_id=\"username/my-dataset\",\n...         repo_type=\"dataset\",\n...         token=\"my_token\",\n...     )\n\"https://huggingface.co/datasets/username/my-dataset/blob/main/remote/file/path.h5\"\n\n>>> upload_file(\n...     path_or_fileobj=\".\\\\local\\\\file\\\\path\",\n...     path_in_repo=\"remote/file/path.h5\",\n...     repo_id=\"username/my-model\",\n...     token=\"my_token\",\n... )\n\"https://huggingface.co/username/my-model/blob/main/remote/file/path.h5\"\n\n>>> upload_file(\n...     path_or_fileobj=\".\\\\local\\\\file\\\\path\",\n...     path_in_repo=\"remote/file/path.h5\",\n...     repo_id=\"username/my-model\",\n...     token=\"my_token\",\n...     create_pr=True,\n... )\n\"https://huggingface.co/username/my-model/blob/refs%2Fpr%2F1/remote/file/path.h5\"\n```", "```py\n# Upload checkpoints folder except the log files\n>>> upload_folder(\n...     folder_path=\"local/checkpoints\",\n...     path_in_repo=\"remote/experiment/checkpoints\",\n...     repo_id=\"username/my-dataset\",\n...     repo_type=\"datasets\",\n...     token=\"my_token\",\n...     ignore_patterns=\"**/logs/*.txt\",\n... )\n# \"https://huggingface.co/datasets/username/my-dataset/tree/main/remote/experiment/checkpoints\"\n\n# Upload checkpoints folder including logs while deleting existing logs from the repo\n# Useful if you don't know exactly which log files have already being pushed\n>>> upload_folder(\n...     folder_path=\"local/checkpoints\",\n...     path_in_repo=\"remote/experiment/checkpoints\",\n...     repo_id=\"username/my-dataset\",\n...     repo_type=\"datasets\",\n...     token=\"my_token\",\n...     delete_patterns=\"**/logs/*.txt\",\n... )\n\"https://huggingface.co/datasets/username/my-dataset/tree/main/remote/experiment/checkpoints\"\n\n# Upload checkpoints folder while creating a PR\n>>> upload_folder(\n...     folder_path=\"local/checkpoints\",\n...     path_in_repo=\"remote/experiment/checkpoints\",\n...     repo_id=\"username/my-dataset\",\n...     repo_type=\"datasets\",\n...     token=\"my_token\",\n...     create_pr=True,\n... )\n\"https://huggingface.co/datasets/username/my-dataset/tree/refs%2Fpr%2F1/remote/experiment/checkpoints\"\n\n```", "```py\n>>> from huggingface_hub import HfApi, plan_multi_commits\n>>> addition_commits, deletion_commits = plan_multi_commits(\n...     operations=[\n...          CommitOperationAdd(...),\n...          CommitOperationAdd(...),\n...          CommitOperationDelete(...),\n...          CommitOperationDelete(...),\n...          CommitOperationAdd(...),\n...     ],\n... )\n>>> HfApi().create_commits_on_pr(\n...     repo_id=\"my-cool-model\",\n...     addition_commits=addition_commits,\n...     deletion_commits=deletion_commits,\n...     (...)\n...     verbose=True,\n... )\n```", "```py\n>>> RepoUrl('https://huggingface.co/gpt2')\nRepoUrl('https://huggingface.co/gpt2', endpoint='https://huggingface.co', repo_type='model', repo_id='gpt2')\n\n>>> RepoUrl('https://hub-ci.huggingface.co/datasets/dummy_user/dummy_dataset', endpoint='https://hub-ci.huggingface.co')\nRepoUrl('https://hub-ci.huggingface.co/datasets/dummy_user/dummy_dataset', endpoint='https://hub-ci.huggingface.co', repo_type='dataset', repo_id='dummy_user/dummy_dataset')\n\n>>> RepoUrl('hf://datasets/my-user/my-dataset')\nRepoUrl('hf://datasets/my-user/my-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='user/dataset')\n\n>>> HfApi.create_repo(\"dummy_model\")\nRepoUrl('https://huggingface.co/Wauplin/dummy_model', endpoint='https://huggingface.co', repo_type='model', repo_id='Wauplin/dummy_model')\n```", "```py\n>>> operation = CommitOperationAdd(\n...        path_in_repo=\"remote/dir/weights.h5\",\n...        path_or_fileobj=\"./local/weights.h5\",\n... )\nCommitOperationAdd(path_in_repo='remote/dir/weights.h5', path_or_fileobj='./local/weights.h5')\n\n>>> with operation.as_file() as file:\n...     content = file.read()\n\n>>> with operation.as_file(with_tqdm=True) as file:\n...     while True:\n...         data = file.read(1024)\n...         if not data:\n...              break\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.19k/8.19k [00:02<00:00, 3.72kB/s]\n\n>>> with operation.as_file(with_tqdm=True) as file:\n...     requests.put(..., data=file)\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8.19k/8.19k [00:02<00:00, 3.72kB/s]\n```", "```py\n>>> from pathlib import Path\n>>> from huggingface_hub import CommitScheduler\n\n# Scheduler uploads every 10 minutes\n>>> csv_path = Path(\"watched_folder/data.csv\")\n>>> CommitScheduler(repo_id=\"test_scheduler\", repo_type=\"dataset\", folder_path=csv_path.parent, every=10)\n\n>>> with csv_path.open(\"a\") as f:\n...     f.write(\"first line\")\n\n# Some time later (...)\n>>> with csv_path.open(\"a\") as f:\n...     f.write(\"second line\")\n```", "```py\n>>> from huggingface_hub import DatasetFilter\n\n>>> # Using author\n>>> new_filter = DatasetFilter(author=\"facebook\")\n\n>>> # Using benchmark\n>>> new_filter = DatasetFilter(benchmark=\"raft\")\n\n>>> # Using dataset_name\n>>> new_filter = DatasetFilter(dataset_name=\"wikineural\")\n\n>>> # Using language_creator\n>>> new_filter = DatasetFilter(language_creator=\"crowdsourced\")\n\n>>> # Using language\n>>> new_filter = DatasetFilter(language=\"en\")\n\n>>> # Using multilinguality\n>>> new_filter = DatasetFilter(multilinguality=\"multilingual\")\n\n>>> # Using size_categories\n>>> new_filter = DatasetFilter(size_categories=\"100K<n<1M\")\n\n>>> # Using task_categories\n>>> new_filter = DatasetFilter(task_categories=\"audio_classification\")\n\n>>> # Using task_ids\n>>> new_filter = DatasetFilter(task_ids=\"paraphrase\")\n```", "```py\n>>> from huggingface_hub import ModelFilter\n\n>>> # For the author_or_organization\n>>> new_filter = ModelFilter(author_or_organization=\"facebook\")\n\n>>> # For the library\n>>> new_filter = ModelFilter(library=\"pytorch\")\n\n>>> # For the language\n>>> new_filter = ModelFilter(language=\"french\")\n\n>>> # For the model_name\n>>> new_filter = ModelFilter(model_name=\"bert\")\n\n>>> # For the task\n>>> new_filter = ModelFilter(task=\"text-classification\")\n\n>>> from huggingface_hub import HfApi\n\n>>> api = HfApi()\n# To list model tags\n\n>>> new_filter = ModelFilter(tags=\"benchmark:raft\")\n\n>>> # Related to the dataset\n>>> new_filter = ModelFilter(trained_dataset=\"common_voice\")\n```"]