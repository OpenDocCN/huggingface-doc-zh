# RAG

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag)

[![Models](../Images/8d827715f6e5e46ae48b9169a2927210.png)](https://huggingface.co/models?filter=rag)

## 概述

检索增强生成（“RAG”）模型结合了预训练的密集检索（DPR）和序列到序列模型的能力。RAG模型检索文档，将其传递给seq2seq模型，然后进行边缘化以生成输出。检索器和seq2seq模块是从预训练模型初始化的，并进行联合微调，使得检索和生成都能够适应下游任务。

基于论文[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela。

论文摘要如下：

*已经证明，大型预训练语言模型在其参数中存储了事实知识，并且在下游自然语言处理任务上进行微调时取得了最先进的结果。然而，它们访问和精确操作知识的能力仍然有限，因此在知识密集型任务上，它们的性能落后于特定任务的架构。此外，为它们的决策提供来源并更新它们的世界知识仍然是一个开放的研究问题。具有可微分访问机制到显式非参数化内存的预训练模型可以解决这个问题，但迄今为止仅用于提取式下游任务进行了调查。我们探索了一种用于检索增强生成（RAG）的通用微调配方 — 这些模型结合了预训练的参数化和非参数化内存进行语言生成。我们介绍了RAG模型，其中参数化内存是一个预训练的seq2seq模型，非参数化内存是维基百科的密集向量索引，通过预训练的神经检索器访问。我们比较了两种RAG公式，一种是在整个生成序列中条件于相同的检索段落，另一种可以使用每个标记的不同段落。我们在广泛的知识密集型自然语言处理任务上对我们的模型进行微调和评估，并在三个开放领域问答任务上取得了最先进的成绩，优于参数化seq2seq模型和特定任务的检索和提取架构。对于语言生成任务，我们发现RAG模型生成比最先进的仅参数化seq2seq基线更具体、多样化和事实性的语言。*

此模型由[ola13](https://huggingface.co/ola13)贡献。

## 使用提示

检索增强生成（“RAG”）模型结合了预训练的密集检索（DPR）和Seq2Seq模型的能力。RAG模型检索文档，将其传递给seq2seq模型，然后进行边缘化以生成输出。检索器和seq2seq模块是从预训练模型初始化的，并进行联合微调，使得检索和生成都能够适应下游任务。

## RagConfig

### `class transformers.RagConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L80)

```py
( vocab_size = None is_encoder_decoder = True prefix = None bos_token_id = None pad_token_id = None eos_token_id = None decoder_start_token_id = None title_sep = ' / ' doc_sep = ' // ' n_docs = 5 max_combined_length = 300 retrieval_vector_size = 768 retrieval_batch_size = 8 dataset = 'wiki_dpr' dataset_split = 'train' index_name = 'compressed' index_path = None passages_path = None use_dummy_dataset = False reduce_loss = False label_smoothing = 0.0 do_deduplication = True exclude_bos_score = False do_marginalize = False output_retrieved = False use_cache = True forced_eos_token_id = None **kwargs )
```

参数

+   `title_sep` (`str`, *可选*, 默认为`" / "`) — 在调用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)时插入在标题和检索到的文档文本之间的分隔符。

+   `doc_sep` (`str`, *可选*, 默认为`" // "`) — 在调用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)时插入在检索到的文档文本和原始输入之间的分隔符。

+   `n_docs` (`int`, *可选*, 默认为5) — 要检索的文档数量。

+   `max_combined_length` (`int`, *可选*, 默认为300) — `__call__()`返回的上下文化输入的最大长度。

+   `retrieval_vector_size` (`int`, *可选*, 默认为768) — 由[RagRetriever](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagRetriever)索引的文档嵌入的维度。

+   `retrieval_batch_size` (`int`, *可选*, 默认为8) — 检索批量大小，定义为同时发出给封装的faiss索引的查询数量[RagRetriever](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagRetriever)。

+   `dataset` (`str`, *可选*, 默认为`"wiki_dpr"`) — 在HuggingFace Datasets中索引数据集的数据集标识符（使用`datasets.list_datasets()`列出所有可用数据集和ID）。

+   `dataset_split` (`str`, *可选*, 默认为`"train"`) — 要加载的`dataset`的哪个拆分。

+   `index_name` (`str`, *可选*, 默认为`"compressed"`) — 与`dataset`关联的索引的索引名称。可以在`"legacy"`、`"exact"`和`"compressed"`之间进行选择。

+   `index_path` (`str`, *可选*) — 磁盘上序列化faiss索引的路径。

+   `passages_path` (`str`, *可选*) — 与faiss索引兼容的文本段落的路径。如果使用`LegacyIndex`，则需要。

+   `use_dummy_dataset` (`bool`, *可选*, 默认为`False`) — 是否加载由`dataset`指定的数据集的“虚拟”变体。 

+   `label_smoothing` (`float`, *可选*, 默认为0.0) — 仅在`return_loss`设置为`True`时相关。控制损失计算中标签平滑的`epsilon`参数值。如果设置为0，则不执行标签平滑。

+   `do_marginalize` (`bool`, *可选*, 默认为`False`) — 如果为`True`，则通过使用`torch.nn.functional.log_softmax`对所有文档的logits进行边际化。

+   `reduce_loss` (`bool`, *可选*, 默认为`False`) — 是否使用`torch.Tensor.sum`操作减少NLL损失。

+   `do_deduplication` (`bool`, *可选*, 默认为`True`) — 是否对给定输入的不同上下文文档的生成进行去重。如果在使用分布式后端进行训练时使用，必须将其设置为`False`。

+   `exclude_bos_score` (`bool`, *可选*, 默认为`False`) — 在计算损失时是否忽略BOS标记。

+   `output_retrieved(bool,` *可选*, 默认为`False`) — 如果设置为`True`，则返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。查看返回的张量以获取更多详细信息。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `forced_eos_token_id` (`int`, *可选*) — 当达到`max_length`时，要强制作为最后生成的标记的标记ID。通常设置为`eos_token_id`。

[RagConfig](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagConfig) 存储了*RagModel*的配置。配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/zh/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/zh/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

#### `from_question_encoder_generator_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L169)

```py
( question_encoder_config: PretrainedConfig generator_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';EncoderDecoderConfig
```

返回

[EncoderDecoderConfig](/docs/transformers/v4.37.2/zh/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)

配置对象的一个实例

从预训练的编码器模型配置和解码器模型配置实例化一个[EncoderDecoderConfig](/docs/transformers/v4.37.2/zh/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)（或派生类）。

## RagTokenizer

### `class transformers.RagTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/tokenization_rag.py#L28)

```py
( question_encoder generator )
```

## Rag特定输出

### `class transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L38)

```py
( loss: Optional = None logits: FloatTensor = None doc_scores: FloatTensor = None past_key_values: Optional = None retrieved_doc_embeds: Optional = None retrieved_doc_ids: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None question_encoder_last_hidden_state: Optional = None question_enc_hidden_states: Optional = None question_enc_attentions: Optional = None generator_enc_last_hidden_state: Optional = None generator_enc_hidden_states: Optional = None generator_enc_attentions: Optional = None generator_dec_hidden_states: Optional = None generator_dec_attentions: Optional = None generator_cross_attentions: Optional = None )
```

参数

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回） — 语言建模损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边际化。

+   `doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。

+   `past_key_values` (`List[torch.FloatTensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, 当*output_retrieved=True*时返回） — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`, *optional*, 当*output_retrieved=True*时返回） — 检索器检索到的嵌入文档的索引。

+   `context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理得到的输入id。

+   `context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型问题编码器汇总输出的最后一层的隐藏状态序列。

+   `question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    问题编码器每一层的输出隐藏状态以及初始嵌入输出。

+   `question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

    生成器编码器每一层的隐藏状态以及初始嵌入输出。

+   `generator_enc_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。

    生成器解码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `generator_dec_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

用于检索器增强边际化模型输出的基类。

### `class transformers.models.rag.modeling_rag.RetrievAugLMOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L133)

```py
( logits: FloatTensor = None doc_scores: FloatTensor = None past_key_values: Optional = None retrieved_doc_embeds: Optional = None retrieved_doc_ids: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None question_encoder_last_hidden_state: Optional = None question_enc_hidden_states: Optional = None question_enc_attentions: Optional = None generator_enc_last_hidden_state: Optional = None generator_enc_hidden_states: Optional = None generator_enc_attentions: Optional = None generator_dec_hidden_states: Optional = None generator_dec_attentions: Optional = None generator_cross_attentions: Optional = None )
```

参数

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边际化。

+   `doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。

+   `past_key_values` (`List[torch.FloatTensor]`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `retrieved_doc_embeds` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs, hidden_size)`，*可选*，当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids` (`torch.LongTensor`，形状为`(batch_size, config.n_docs)`，*可选*，当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档的索引。

+   `context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索文档和检索器的问题编码器`input_ids`后处理得到的输入id。

+   `context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索文档和检索器的问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 问题编码器最后一层的隐藏状态序列模型的汇聚输出。

+   `question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。

    问题编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。

    生成器编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。

    生成器解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

## RagRetriever

### `class transformers.RagRetriever`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L337)

```py
( config question_encoder_tokenizer generator_tokenizer index = None init_retrieval = True )
```

参数

+   `config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）— 用于RAG模型的Retriever的配置。包含指示要构建哪个`Index`的参数。您可以使用`config.index_name="custom"`加载自己的自定义数据集，或者使用数据集库中的一个规范的数据集（默认）例如`config.index_name="wiki_dpr"`。

+   `question_encoder_tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）— 用于标记化问题的分词器。它用于解码问题，然后使用生成器的分词器。

+   `generator_tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）— 用于RagModel生成器部分的分词器。

+   `index`（`Index`，可选，默认为配置中定义的索引）— 如果指定，则使用此索引，而不是使用配置构建的索引

用于从向量查询获取文档的检索器。它检索文档嵌入以及文档内容，并将它们格式化以供RagModel使用。

示例：

```py
>>> # To load the default "wiki_dpr" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')
>>> from transformers import RagRetriever

>>> retriever = RagRetriever.from_pretrained(
...     "facebook/dpr-ctx_encoder-single-nq-base", dataset="wiki_dpr", index_name="compressed"
... )

>>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py
>>> from transformers import RagRetriever

>>> dataset = (
...     ...
... )  # dataset must be a datasets.Datasets object with columns "title", "text" and "embeddings", and it must have a faiss index
>>> retriever = RagRetriever.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", indexed_dataset=dataset)

>>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py
>>> from transformers import RagRetriever

>>> dataset_path = "path/to/my/dataset"  # dataset saved via *dataset.save_to_disk(...)*
>>> index_path = "path/to/my/index.faiss"  # faiss index saved via *dataset.get_index("embeddings").save(...)*
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/dpr-ctx_encoder-single-nq-base",
...     index_name="custom",
...     passages_path=dataset_path,
...     index_path=index_path,
... )

>>> # To load the legacy index built originally for Rag's paper
>>> from transformers import RagRetriever

>>> retriever = RagRetriever.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", index_name="legacy")
```

#### `init_retrieval`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L471)

```py
( )
```

检索器初始化函数。将索引加载到内存中。

#### `postprocess_docs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L479)

```py
( docs input_strings prefix n_docs return_tensors = None ) → export const metadata = 'undefined';tuple(tensors)
```

参数

+   `docs`（`dict`）— 检索到的文档。

+   `input_strings`（`str`）— 由`preprocess_query`解码的输入字符串。

+   `prefix`（`str`）— 添加到每个输入开头的前缀，通常与基于T5的模型一起使用。

返回

`tuple(tensors)`

一个包含两个元素的元组：上下文化的`input_ids`和一个兼容的`attention_mask`。

后处理检索到的`docs`并将它们与`input_strings`组合。

#### `retrieve`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L551)

```py
( question_hidden_states: ndarray n_docs: int ) → export const metadata = 'undefined';Tuple[np.ndarray, np.ndarray, List[dict]]
```

参数

+   `question_hidden_states`（形状为`(batch_size, vector_size)`的`np.ndarray`）— 一批要检索的查询向量。

+   `n_docs`（`int`）— 每个查询检索的文档数。

返回

`Tuple[np.ndarray, np.ndarray, List[dict]]`

一个包含以下对象的元组：

+   `retrieved_doc_embeds`（形状为`(batch_size, n_docs, dim)`的`np.ndarray`）— 检索到的文档的检索嵌入，每个查询一个。

+   `doc_ids`（形状为`(batch_size, n_docs)`的`np.ndarray`）— 索引中文档的id

+   `doc_dicts`（`List[dict]`）：每个查询的`retrieved_doc_embeds`示例。

为指定的`question_hidden_states`检索文档。

PytorchHide Pytorch content

## RagModel

### `class transformers.RagModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L489)

```py
( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )
```

参数

+   `config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

+   `question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）— 与`retriever`封装的faiss索引兼容的编码器模型。

+   `generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）— 用作RAG架构中生成器的seq2seq模型。

+   `retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）— 一个检索器类，封装了一个faiss索引，用于获取当前输入的上下文文档。

[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)的前向方法覆盖了`__call__`特殊方法。

虽然前向传播的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。

RAG是一个seq2seq模型，它包含两个核心组件：一个问题编码器和一个生成器。在前向传播过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将这些文档添加到输入中。这样的上下文化输入被传递给生成器。

问题编码器可以是任何*自编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出结合在多个步骤中使用---请参阅示例以获取更多详细信息。该模型兼容任何*自编码*模型作为`question_encoder`，任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L533)

```py
( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None doc_scores: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None n_docs: Optional = None ) → export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`（batch_size，sequence_length）`的`torch.LongTensor`）—词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器标记器。使用该标记器类获取索引。

    什么是输入ID？

+   `attention_mask`（形状为`（batch_size，sequence_length）`的`torch.Tensor`，*可选*）—避免对填充标记索引执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于未被`masked`的标记，值为1，

    +   对于被`masked`的标记，值为0。

    什么是注意力掩码？

+   `encoder_outputs`（`tuple（tuple（torch.FloatTensor）`，*可选*）—元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`（batch_size，n_docs * sequence_length，hidden_size）`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码过程中由（[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)）模型使用。

+   `decoder_input_ids` (`torch.LongTensor`，形状为`(batch_size, target_sequence_length)`，*可选*) — 为生成任务提供。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。

+   `decoder_attention_mask` (`torch.BoolTensor`，形状为`(batch_size, target_sequence_length)`，*可选*) — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`) — 元组包含两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。

+   `doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。

+   `context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache` (`bool`，*可选*，默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `n_docs` (`int`，*可选*，默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。

返回值

[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。

+   `doc_scores`（`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`）— 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

+   `past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `retrieved_doc_embeds`（`torch.FloatTensor`，形状为`(batch_size, config.n_docs, hidden_size)`，*可选*，当*output_retrieved=True*时返回）— 由检索器检索的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids`（`torch.LongTensor`，形状为`(batch_size, config.n_docs)`，*可选*，当*output_retrieved=True*时返回）— 检索器检索的嵌入文档的索引。

+   `context_input_ids`（`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索的文档和问题编码器input_ids后处理得到的输入id。

+   `context_attention_mask`（`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索的文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 模型问题编码器输出的最后一层的隐藏状态序列。

+   `question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    问题编码器在每一层的输出隐藏状态加上初始嵌入输出。

+   `question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器编码器在每一层的输出隐藏状态加上初始嵌入输出。

+   `generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器解码器在每一层的隐藏状态加上初始嵌入输出。

+   `generator_dec_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RagRetriever, RagModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-token-base")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-token-base", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = RagModel.from_pretrained("facebook/rag-token-base", retriever=retriever)

>>> inputs = tokenizer("How many people live in Paris?", return_tensors="pt")
>>> outputs = model(input_ids=inputs["input_ids"])
```

## RagSequenceForGeneration

### `class transformers.RagSequenceForGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L730)

```py
( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )
```

参数

+   `config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

+   `question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）- 与`检索器`封装的faiss索引兼容的编码器模型。

+   `generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）- 在RAG架构中用作生成器的seq2seq模型。

+   `检索器`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）- 一个封装了faiss索引的检索器类，用于获取当前输入的上下文文档。

[RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

RAG-sequence模型实现。它在前向传递中执行RAG-sequence特定的边际化。

RAG是一个seq2seq模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递期间，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入之前。这样的上下文化输入被传递给生成器。

问题编码器可以是任何*自动编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，或与检索器的输出结合使用多步骤---查看更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L765)

```py
( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None exclude_bos_score: Optional = None reduce_loss: Optional = None labels: Optional = None n_docs: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类获取索引。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于被“masked”掉的标记为1，

    +   对于被`masked`掉的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括(`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`)。形状为`(batch_size, n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码期间由（[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)）模型使用。

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 用于生成任务。默认为`None`，根据您使用的RAG实例的生成模型的指示构建。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`）— 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和底层生成器的`past_key_values`。可用于加速解码。`past_key_values`在解码期间由（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型使用。

+   `doc_scores`（形状为`(batch_size, config.n_docs)`的`torch.FloatTensor`）- 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。

+   `context_input_ids`（`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）- 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）- 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache`（`bool`，*可选*，默认为`True`）- 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。

+   `output_attentions`（布尔值，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量中的`attentions`。

+   `output_hidden_states`（布尔值，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量中的`hidden_states`。

+   `output_retrieved`（布尔值，*可选*）- 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要生成答案的文档数量。

+   `exclude_bos_score`（布尔值，*可选*）- 仅在传递了`labels`时相关。如果为`True`，在计算损失时将忽略BOS标记的得分。

+   `reduce_loss`（`bool`，*可选*）- 仅在传递了`labels`时相关。如果为`True`，则使用`torch.Tensor.sum`操作减少NLL损失。

+   `kwargs`（`Dict[str, any]`，可选，默认为*{}*）- 遗留字典，模型可以使用*generate()*函数。

返回

[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或`tuple(torch.FloatTensor)`

一个[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）- 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。

+   `doc_scores`（形状为`(batch_size, config.n_docs)`的`torch.FloatTensor`）- 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

+   `past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `retrieved_doc_embeds`（形状为`(batch_size, config.n_docs, hidden_size)`的`torch.FloatTensor`，*可选*，当`output_retrieved=True`时返回）- 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids`（形状为`(batch_size, config.n_docs)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）- 检索器检索到的嵌入文档的索引。

+   `context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）- 从检索到的文档和问题编码器输入id后处理得到的输入id。

+   `context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）- 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 问题编码器最后一层的隐藏状态序列，模型的池化输出。

+   `question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。

    问题编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。

    生成器编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。

    生成器解码器在每一层的输出的隐藏状态加上初始嵌入输出。

+   `generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的交叉注意力权重，在注意力softmax之后使用，用于计算交叉注意力头中的加权平均值。

[RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例:

```py
>>> from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-sequence-nq")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-sequence-nq", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = RagSequenceForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever)

>>> inputs = tokenizer("How many people live in Paris?", return_tensors="pt")
>>> targets = tokenizer(text_target="In Paris, there are 10 million people.", return_tensors="pt")
>>> input_ids = inputs["input_ids"]
>>> labels = targets["input_ids"]
>>> outputs = model(input_ids=input_ids, labels=labels)

>>> # or use retriever separately
>>> model = RagSequenceForGeneration.from_pretrained("facebook/rag-sequence-nq", use_dummy_dataset=True)
>>> # 1\. Encode
>>> question_hidden_states = model.question_encoder(input_ids)[0]
>>> # 2\. Retrieve
>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors="pt")
>>> doc_scores = torch.bmm(
...     question_hidden_states.unsqueeze(1), docs_dict["retrieved_doc_embeds"].float().transpose(1, 2)
... ).squeeze(1)
>>> # 3\. Forward to generator
>>> outputs = model(
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
...     decoder_input_ids=labels,
... )
```

`generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L906)

```py
( input_ids: Optional = None attention_mask: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None do_deduplication: Optional = None num_return_sequences: Optional = None num_beams: Optional = None n_docs: Optional = None **model_kwargs ) → export const metadata = 'undefined';torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被掩盖的标记为1，

    +   对于被掩盖的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 在*output_retrieved=True*时返回) — 从检索到的文档和问题编码器input_ids经过后处理得到的输入ID。

+   `context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 在*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。

    如果模型未初始化为`retriever`或未给出`input_ids`，则必须在前向传递中提供`context_input_ids`和`context_attention_mask`。它们由`__call__()`返回。

+   `doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

    如果模型未初始化为`retriever`或未给出`input_ids`，则必须在前向传递中提供`doc_scores`。`doc_scores`由`__call__()`返回。

+   `do_deduplication` (`bool`, *optional*) — 是否对给定输入的不同上下文文档的生成进行去重。如果在使用分布式后端进行训练时使用，必须将其设置为`False`。

+   `num_return_sequences(int,` *optional*, 默认为1) — 每个批次元素的独立计算返回序列的数量。请注意，这不是我们传递给`generator`的`[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`函数的值，其中我们将`num_return_sequences`设置为`num_beams`。

+   `num_beams` (`int`, *optional*, defaults to 1) — Beam search的beam数量。1表示没有beam search。

+   `n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要生成答案的文档数量。

+   `kwargs`（`Dict[str, Any]`，*可选*）- 额外的kwargs将传递给[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)。

返回

形状为`(batch_size * num_return_sequences, sequence_length)`的`torch.LongTensor`

生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

实现RAG序列“彻底”解码。阅读[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)文档，了解如何设置其他生成输入参数的更多信息。

## RagTokenForGeneration

### `class transformers.RagTokenForGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1128)

```py
( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )
```

参数

+   `config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

+   `question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）- 与`retriever`封装的faiss索引兼容的编码器模型。

+   `generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）- 在RAG架构中用作生成器的seq2seq模型。

+   `retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）- 封装了一个faiss索引的检索器类，用于获取当前输入的上下文文档。

[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

RAG-token模型实现。它在前向传递中执行RAG-token特定的边际化。

RAG是一个seq2seq模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递期间，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入中。这样的上下文化输入被传递给生成器。

问题编码器可以是任何*自动编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出结合在多个步骤中使用---有关更多详细信息，请参见示例。该模型与任何*自动编码*模型兼容，作为`question_encoder`，以及任何带有语言模型头的*seq2seq*模型，作为`generator`。已经使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`进行测试。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1234)

```py
( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None do_marginalize: Optional = None reduce_loss: Optional = None labels: Optional = None n_docs: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类来获取索引。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的蒙版。选择在`[0, 1]`中的蒙版值：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`(batch_size, n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码期间，由([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))模型使用。

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 用于生成任务。默认为`None`，根据您使用的生成器模型与您的RAG实例的指令构建。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）- 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果蒙版也将默认使用。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`）- 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))模型中使用。

+   `doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。

+   `context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *可选*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,*可选*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `n_docs` (`int`, *可选*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。

+   `do_marginalize` (`bool`, *可选*) — 如果为`True`，则通过使用`torch.nn.functional.log_softmax`对所有文档进行边际化。

+   `reduce_loss` (`bool`, *可选*) — 仅在传递了`labels`时相关。如果为`True`，则使用`torch.Tensor.sum`操作减少NLL损失。

+   `kwargs` (`Dict[str, any]`, 可选，默认为*{}*) — 遗留字典，模型可以使用*generate()*函数。

返回

[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入而异的各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当提供`labels`时返回) — 语言建模损失。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能对每个词汇标记在所有文档上进行边际化。

+   `doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。

+   `past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `retrieved_doc_embeds`（形状为`(batch_size, config.n_docs, hidden_size)`的`torch.FloatTensor`，*可选*，当*output_retrieved=True*时返回）- 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids`（形状为`(batch_size, config.n_docs)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）- 检索器检索到的嵌入文档的索引。

+   `context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）- 从检索到的文档和问题编码器输入id经过后处理得到的输入id。

+   `context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）- 从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 问题编码器最后一层输出的隐藏状态序列，模型的汇总输出。

+   `question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    问题编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器解码器每层输出的隐藏状态以及初始嵌入输出。

+   `generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_cross_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-token-nq")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-token-nq", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = RagTokenForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever)

>>> inputs = tokenizer("How many people live in Paris?", return_tensors="pt")
>>> targets = tokenizer(text_target="In Paris, there are 10 million people.", return_tensors="pt")
>>> input_ids = inputs["input_ids"]
>>> labels = targets["input_ids"]
>>> outputs = model(input_ids=input_ids, labels=labels)

>>> # or use retriever separately
>>> model = RagTokenForGeneration.from_pretrained("facebook/rag-token-nq", use_dummy_dataset=True)
>>> # 1\. Encode
>>> question_hidden_states = model.question_encoder(input_ids)[0]
>>> # 2\. Retrieve
>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors="pt")
>>> doc_scores = torch.bmm(
...     question_hidden_states.unsqueeze(1), docs_dict["retrieved_doc_embeds"].float().transpose(1, 2)
... ).squeeze(1)
>>> # 3\. Forward to generator
>>> outputs = model(
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
...     decoder_input_ids=labels,
... )

>>> # or directly generate
>>> generated = model.generate(
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
... )
>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1375)

```py
( input_ids: Optional = None attention_mask: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None n_docs: Optional = None generation_config: Optional = None prefix_allowed_tokens_fn: Callable = None logits_processor: Optional = [] stopping_criteria: Optional = [] **kwargs ) → export const metadata = 'undefined';torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `n_docs` (`int`，*可选*，默认为`config.n_docs`) — 要检索的文档数量和/或要为其生成答案的文档数量。

+   `generation_config` (`~generation.GenerationConfig`, *optional*) — 用作生成调用的基本参数化的生成配置。传递给生成的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，将使用默认配置，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以对生成进行参数化。

+   `prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*) — 如果提供，此函数将每一步的beam搜索限制为仅允许的标记。如果未提供，则不应用约束。此函数接受2个参数`inputs_ids`和批次ID`batch_id`。它必须返回一个列表，其中包含下一代步的允许标记，条件是先前生成的标记`inputs_ids`和批次ID`batch_id`。此参数对于受前缀约束的生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。

+   `logits_processor` (`LogitsProcessorList`, *optional*) — 自定义logits处理器，补充从参数和模型配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或模型配置创建，则会引发错误。

+   `stopping_criteria` (`StoppingCriteriaList`, *optional*) — 自定义停止标准，补充从参数和模型配置构建的默认停止标准。如果传递的停止标准已经使用参数或模型配置创建，则会引发错误。

+   `kwargs` (`Dict[str, Any]`, *optional*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。

返回

形状为`(batch_size * num_return_sequences, sequence_length)`的`torch.LongTensor`

生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

实现了RAG标记解码。

TensorFlow隐藏了TensorFlow内容

## TFRagModel

### `class transformers.TFRagModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L496)

```py
( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None load_weight_prefix: Optional[str] = None **kwargs )
```

参数

+   `config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)) — 与`retriever`封装的faiss索引兼容的编码器模型。

+   `generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)) — 在RAG架构中用作生成器的seq2seq模型。

+   `retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)) — 封装了用于获取当前输入上下文文档的faiss索引的检索器类。

[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

RAG是一个序列到序列模型，包含两个核心组件：一个问题编码器和一个生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将这些文档添加到输入之前。这样的上下文化输入被传递给生成器。

问题编码器可以是任何*自动编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成初始化，或者与检索器的输出结合在多个步骤中使用---请参阅更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，以及任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)作为`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)作为`generator`。

该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

该模型目前仅在急切模式下完全支持，并且可能无法以SavedModel格式导出。

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L547)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[Union[np.ndarray, tf.Tensor]]] | None = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None output_retrieved: bool | None = None n_docs: int | None = None return_dict: bool | None = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`） — 词汇表中输入序列标记的索引。用于初始化模型的[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类获取索引。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*） — 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   1表示未被“掩码”（masked）的标记，

    +   对于被“掩码”（masked）的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_outputs`（元组（元组（`tf.Tensor`）的形状，*可选*） — 元组由（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）组成。形状为`(batch_size, n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码过程中由（[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)）模型使用。

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`tf.Tensor`，*可选*） — 用于生成任务。默认为`None`，根据您正在使用的RAG实例的生成器模型的说明进行构造。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）- 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

+   `past_key_values`（`tuple(tuple(tf.Tensor))`）- 元组包含两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。

+   `doc_scores`（形状为`(batch_size, config.n_docs)`的`tf.Tensor`）- 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。

+   `context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）- 从检索文档和问题编码器`input_ids`后处理的输入ID。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）：从检索文档和问题编码器`input_ids`后处理的注意力掩码。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache`（`bool`，*可选*，默认为`True`）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `output_retrieved`（`bool`，*可选*）- 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `return_dict`（`bool`，*可选*）- 是否返回`TFRetrievAugLMOutput`而不是普通元组。

+   `n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要为其生成答案的文档数量。

返回

`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput`或`tuple(tf.Tensor)`

一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入而异的各种元素。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）- 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。

+   `past_key_values`（`List[tf.Tensor]`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。

+   `doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

+   `retrieved_doc_embeds` (`tf.Tensor`，形状为`(batch_size, config.n_docs, hidden_size)`，*optional*，当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`，*optional*，当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档的索引。

+   `context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*optional*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器输入ids后处理得到的输入ids。

+   `context_attention_mask` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*optional*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 问题编码器池化输出模型最后一层的隐藏状态序列。

+   `question_enc_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。

    问题编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `question_enc_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。

    生成器编码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `generator_enc_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。

    生成器解码器在每一层输出的隐藏状态以及初始嵌入输出。

+   `generator_dec_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RagRetriever, TFRagModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-token-base")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-token-base", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = TFRagModel.from_pretrained("facebook/rag-token-base", retriever=retriever, from_pt=True)

>>> input_dict = tokenizer.prepare_seq2seq_batch(
...     "How many people live in Paris?", "In Paris, there are 10 million people.", return_tensors="tf"
... )
>>> input_ids = input_dict["input_ids"]
>>> outputs = model(input_ids)
```

## TFRagSequenceForGeneration

### `class transformers.TFRagSequenceForGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1313)

```py
( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None **kwargs )
```

参数

+   `config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)) — 与`retriever`封装的faiss索引兼容的编码器模型。

+   `generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)) — 用作RAG架构中生成器的序列到序列模型。

+   `retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)) — 一个检索器类，封装了一个faiss索引，用于获取当前输入的上下文文档。

[TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

TF RAG-sequence模型实现。它在前向传递中执行RAG-sequence特定的边际化。

RAG是一个序列到序列模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入中。这样的上下文化输入被传递给生成器。

问题编码器可以是任何*自动编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*序列到序列*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出组合在多个步骤中使用---查看更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，兼容任何带有语言模型头的*序列到序列*模型作为`generator`。已经测试过使用[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)作为`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)作为`generator`。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

该模型目前处于开发状态，现在仅完全支持即时模式，并且可能无法以SavedModel格式导出。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1366)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None output_retrieved: Optional[bool] = None n_docs: Optional[int] = None exclude_bos_score: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None reduce_loss: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`tf.Tensor`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定兼容的生成器分词器。使用该分词器类获取这些索引。

+   `attention_mask` (`tf.Tensor`的形状为`(batch_size, sequence_length)`，*可选*) — 避免对填充标记索引执行注意力的掩码。选择在`[0, 1]`中的掩码值:

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码?](../glossary#attention-mask)

+   `encoder_outputs` (`tuple(tuple(tf.Tensor)`, *可选*) — 元组包括(`generator_enc_last_hidden_state`, *可选*: `generator_enc_hidden_states`, *可选*: `generator_enc_attentions`)。形状为`(batch_size, n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码期间，由([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))模型使用。

+   `decoder_input_ids` (`tf.Tensor`的形状为`(batch_size, target_sequence_length)`，*可选*) — 用于生成任务。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。

+   `decoder_attention_mask` (`torch.BoolTensor`的形状为`(batch_size, target_sequence_length)`，*可选*) — 默认行为: 生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `past_key_values` (`tuple(tuple(tf.Tensor))`) — 元组包括RAG模型的`encoder_outputs`（参见`encoder_outputs`）和底层生成器的`past_key_values`两个元素。可用于加速解码。在解码期间，`past_key_values`在([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))模型中使用。

+   `doc_scores` (`tf.Tensor`的形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未初始化为`retriever`，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，详细信息请参见示例。

+   `context_input_ids` (`tf.Tensor`的形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。

    如果模型未初始化为`retriever`，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。context_attention_mask (`tf.Tensor`的形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回): 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

    如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `output_retrieved(bool,` *optional*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `return_dict` (`bool`, *optional*) — 是否返回`TFRetrievAugLMOutput`而不是普通元组。

+   `n_docs` (`int`, *optional*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。

+   `exclude_bos_score` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则在计算损失时忽略BOS标记的分数。

+   `labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 根据Rag-Sequence模型公式计算交叉熵分类损失的标签。有关Rag-Sequence公式的详细信息，请参见[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)第2.1节。索引应在`[0, ..., config.vocab_size - 1]`范围内。

+   `reduce_loss` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则使用`tf.Tensor.sum`操作减少NLL损失。

+   `kwargs` (`Dict[str, any]`, optional, 默认为*{}*) — 遗留字典，模型可以使用*generate()*函数所需。

返回

`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或`tuple(tf.Tensor)`

一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含预先计算的隐藏状态（解码器中的键和值在注意力块中）的`tf.Tensor`，可用于加速顺序解码（请参见`past_key_values`输入）。 

+   `doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。

+   `retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, returned when *output_retrieved=True*) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`, *optional*, returned when *output_retrieved=True*) — 由检索器检索的嵌入文档的索引。

+   `context_input_ids`（`tf.Tensor`（int32）形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索到的文档和问题编码器input_ids后处理得到的输入id。

+   `context_attention_mask`（`tf.Tensor`（int32）形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 问题编码器最后一层的隐藏状态序列模型的池化输出。

+   `question_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    问题编码器在每一层的输出加上初始嵌入输出的隐藏状态。

+   `question_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器编码器在每一层的输出加上初始嵌入输出的隐藏状态。

+   `generator_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器解码器在每一层的输出加上初始嵌入输出的隐藏状态。

+   `generator_dec_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, RagRetriever, TFRagSequenceForGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-sequence-nq")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-sequence-nq", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = TFRagSequenceForGeneration.from_pretrained(
...     "facebook/rag-sequence-nq", retriever=retriever, from_pt=True
... )

>>> input_dict = tokenizer.prepare_seq2seq_batch(
...     "How many people live in Paris?", "In Paris, there are 10 million people.", return_tensors="tf"
... )
>>> outputs = model(input_dict, output_retrieved=True)

>>> # or use retriever separately
>>> # 1\. Encode
>>> input_ids = input_dict["input_ids"]
>>> question_hidden_states = model.question_encoder(input_ids)[0]
>>> # 2\. Retrieve
>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors="tf")
>>> doc_scores = tf.squeeze(
...     tf.matmul(
...         tf.expand_dims(question_hidden_states, axis=1), docs_dict["retrieved_doc_embeds"], transpose_b=True
...     ),
...     axis=1,
... )
>>> # 3\. Forward to generator
>>> outputs = model(
...     inputs=None,
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
...     decoder_input_ids=input_dict["labels"],
... )

>>> # or directly generate
>>> generated = model.generate(
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
... )
>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1601)

```py
( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None context_input_ids = None context_attention_mask = None doc_scores = None do_deduplication = None num_return_sequences = None num_beams = None n_docs = None **model_kwargs ) → export const metadata = 'undefined';tf.Tensor of shape (batch_size * num_return_sequences, sequence_length)
```

参数

+   `input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。

+   `attention_mask` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：- 1表示**未屏蔽**的标记，- 0表示**已屏蔽**的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索文档和问题编码器input_ids后处理得到的输入ID。

+   `context_attention_mask` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索文档和问题编码器input_ids后处理得到的注意力掩码。如果模型未使用`retriever`初始化或未提供`input_ids`，则必须在前向传递中提供`context_input_ids`和`context_attention_mask`。它们由`__call__()`返回。

+   `doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化或未提供`input_ids`，则必须在前向传递中提供`doc_scores`。`doc_scores`由`__call__()`返回。

+   `do_deduplication` (`bool`, *可选*) — 是否对给定输入的不同上下文文档生成进行去重。如果在使用分布式后端进行训练时，必须将其设置为`False`。

+   `num_return_sequences(int,` *可选*，默认为1) — 每个批次元素的独立计算返回序列的数量。请注意，这不是我们传递给`generator`的`[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`函数的值，其中我们将`num_return_sequences`设置为`num_beams`。

+   `num_beams` (`int`, *可选*, 默认为1) — Beam搜索的数量。1表示没有beam搜索。

+   `n_docs` (`int`，*可选*，默认为`config.n_docs`) — 要检索的文档数量和/或要为其生成答案的文档数量。

+   `kwargs` (`Dict[str, Any]`，*可选*) — 额外的kwargs将传递给[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)

返回

`tf.Tensor`，形状为`(batch_size * num_return_sequences, sequence_length)`

生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

实现了RAG序列“彻底”解码。阅读[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)文档以获取有关如何设置其他生成输入参数的更多信息

## TFRagTokenForGeneration

### `class transformers.TFRagTokenForGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L733)

```py
( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None **kwargs )
```

参数

+   `config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `question_encoder`（[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel））- 与`retriever`封装的faiss索引兼容的编码器模型。

+   `generator`（[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel））- 在RAG架构中用作生成器的seq2seq模型。

+   `retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever））- 封装了一个faiss索引的检索器类，用于获取当前输入的上下文文档。

[TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

TF RAG-token模型实现。它在前向传递中执行RAG-token特定的边际化。

RAG是一个序列到序列模型，封装了两个核心组件：问题编码器和生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入之前。这样上下文化的输入被传递给生成器。

问题编码器可以是任何*自编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。

该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行初始化以进行端到端生成，或者与检索器的输出组合在多个步骤中使用---请参阅示例以获取更多详细信息。该模型与*自编码*模型兼容，如`question_encoder`，以及具有语言模型头部的*seq2seq*模型，如`generator`。已经测试了将[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)用作`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)用作`generator`。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

该模型目前处于开发状态，因为它现在仅在急切模式下完全支持，并且可能无法以SavedModel格式导出。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L852)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[Union[np.ndarray, tf.Tensor]]] | None = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None output_retrieved: bool | None = None n_docs: int | None = None do_marginalize: bool | None = None labels: np.ndarray | tf.Tensor | None = None reduce_loss: bool | None = None return_dict: bool | None = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）- 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类来获取这些索引。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记，值为1。

    +   对于被`masked`的标记，值为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_outputs` (`tuple(tuple(tf.Tensor)`，*可选*) — 元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`(batch_size, n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。

    在解码期间由（[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)）模型使用。

+   `decoder_input_ids` (`tf.Tensor`，形状为`(batch_size, target_sequence_length)`，*可选*) — 用于生成任务。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。

+   `decoder_attention_mask` (`torch.BoolTensor`，形状为`(batch_size, target_sequence_length)`，*可选*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。默认情况下还将使用因果掩码。

+   `past_key_values` (`tuple(tuple(tf.Tensor))`) — 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。

+   `doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。

+   `context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，在*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理的输入ID。

    如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）：从检索文档和问题编码器`input_ids`后处理的注意力掩码。

    如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。

+   `use_cache` (`bool`，*可选*，默认为`True`) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量中的`hidden_states`。

+   `output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。

+   `return_dict` (`bool`，*可选*) — 是否返回`TFRetrievAugLMOutput`而不是普通元组。

+   `n_docs` (`int`, *optional*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。

+   `do_marginalize` (`bool`，*可选*) — 如果为`True`，通过使用`torch.nn.functional.log_softmax`将对数归一化到所有文档上。

+   `labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 根据Rag-Token模型公式计算交叉熵分类损失的标签。有关Rag-Token公式的详细信息，请参阅[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)第2.1节。索引应在`[0, ..., config.vocab_size - 1]`范围内。

+   `reduce_loss` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则使用`tf.Tensor.sum`操作减少NLL损失。

+   `kwargs` (`Dict[str, any]`, optional, 默认为*{}*) — 旧字典，模型可以使用*generate()*函数所需。

返回

`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或`tuple(tf.Tensor)`

一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。

+   `logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含可以用于加速顺序解码的解码器的预计算隐藏状态（注意块中的键和值）（参见`past_key_values`输入）。

+   `doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

+   `retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, 当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。

+   `retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`, *optional*, 当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档的索引。

+   `context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器输入ids后处理得到的输入ids。

+   `context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

+   `question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 模型问题编码器输出的最后一层的隐藏状态序列。

+   `question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    问题编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 模型生成器编码器最后一层的隐藏状态序列。

+   `generator_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器编码器在每一层的隐藏状态加上初始嵌入输出。

+   `generator_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `generator_dec_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    生成器解码器在每一层的隐藏状态加上初始嵌入输出。

+   `generator_dec_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import tensorflow as tf
>>> from transformers import AutoTokenizer, RagRetriever, TFRagTokenForGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/rag-token-nq")
>>> retriever = RagRetriever.from_pretrained(
...     "facebook/rag-token-nq", index_name="exact", use_dummy_dataset=True
... )
>>> # initialize with RagRetriever to do everything in one forward call
>>> model = TFRagTokenForGeneration.from_pretrained("facebook/rag-token-nq", retriever=retriever, from_pt=True)

>>> input_dict = tokenizer.prepare_seq2seq_batch(
...     "How many people live in Paris?", "In Paris, there are 10 million people.", return_tensors="tf"
... )
>>> outputs = model(input_dict, output_retrieved=True)

>>> # or use retriever separately
>>> # 1\. Encode
>>> input_ids = input_dict["input_ids"]
>>> question_hidden_states = model.question_encoder(input_ids)[0]
>>> # 2\. Retrieve
>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors="tf")
>>> doc_scores = tf.squeeze(
...     tf.matmul(
...         tf.expand_dims(question_hidden_states, axis=1), docs_dict["retrieved_doc_embeds"], transpose_b=True
...     ),
...     axis=1,
... )
>>> # 3\. Forward to generator
>>> outputs = model(
...     inputs=None,
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
...     decoder_input_ids=input_dict["labels"],
... )

>>> # or directly generate
>>> generated = model.generate(
...     context_input_ids=docs_dict["context_input_ids"],
...     context_attention_mask=docs_dict["context_attention_mask"],
...     doc_scores=doc_scores,
... )
>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1007)

```py
( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None context_input_ids = None context_attention_mask = None doc_scores = None n_docs = None generation_config = None logits_processor = [] **kwargs ) → export const metadata = 'undefined';tf.Tensor of shape (batch_size * num_return_sequences, sequence_length)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于未被`masked`的标记，值为1，

    +   对于被`masked`的标记，值为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，当*output_retrieved=True*时返回）— 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，当*output_retrieved=True*时返回）— 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。

    如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。

+   `doc_scores`（形状为`(batch_size, config.n_docs)`的`tf.Tensor`）- 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。

    如果模型未使用`retriever`初始化，则必须提供`context_input_ids`进行前向传递。`context_input_ids`由`__call__()`返回。

+   `n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要为其生成答案的文档数量。

+   `generation_config`（`~generation.GenerationConfig`，*可选*）- 用作生成调用的基本参数化的生成配置。传递给生成匹配`generation_config`属性的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。

+   `logits_processor`（`TFLogitsProcessorList`，*可选*）- 自定义logits处理器，补充从参数和模型配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或模型配置创建，则会抛出错误。

+   `kwargs`（`Dict[str, Any]`，*可选*）- `generate_config`的特定于模型的参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。

返回

`tf.Tensor`的形状为`(batch_size * num_return_sequences, sequence_length)`

生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。

实现TFRAG令牌解码。
