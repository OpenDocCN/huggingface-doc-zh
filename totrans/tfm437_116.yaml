- en: Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The base classes [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel),
    [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel),
    and [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    implement the common methods for loading/saving a model either from a local file
    or directory, or from a pretrained model configuration provided by the library
    (downloaded from HuggingFace’s AWS S3 repository).
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    and [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    also implement a few methods which are common among all the models to:'
  prefs: []
  type: TYPE_NORMAL
- en: resize the input token embeddings when new tokens are added to the vocabulary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prune the attention heads of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other methods that are common to each model are defined in [ModuleUtilsMixin](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin)
    (for the PyTorch models) and `~modeling_tf_utils.TFModuleUtilsMixin` (for the
    TensorFlow models) or for text generation, [GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin)
    (for the PyTorch models), [TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin)
    (for the TensorFlow models) and [FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin)
    (for the Flax/JAX models).
  prefs: []
  type: TYPE_NORMAL
- en: PreTrainedModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1157)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models as well as a few methods common to all
    models to:'
  prefs: []
  type: TYPE_NORMAL
- en: resize the input embeddings,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prune heads in the self-attention heads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class attributes (overridden by derived classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_tf_weights` (`Callable`) — A python *method* for loading a TensorFlow
    checkpoint in a PyTorch model, taking as arguments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An instance of the model on which to load the TensorFlow checkpoint.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`PreTrainedConfig`) — An instance of the configuration associated
    to the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path` (`str`) — A path to the TensorFlow checkpoint.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base_model_prefix` (`str`) — A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_parallelizable` (`bool`) — A flag indicating whether this model supports
    model parallelization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main_input_name` (`str`) — The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload model"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — The description of the commit that
    will be created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` (`List[str]`, *optional*) — List of tags to push on the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the model file to the 🤗 Model Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add_model_tags`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1270)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tags` (`Union[List[str], str]`) — The desired tags to inject in the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add custom tags into the model that gets pushed to the Hugging Face Hub. Will
    not overwrite existing tags in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#### `can_generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1438)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`bool`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether this model can generate sequences with `.generate()`.
  prefs: []
  type: TYPE_NORMAL
- en: Returns whether this model can generate sequences with `.generate()`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_input_require_grads`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1582)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Removes the `_require_grads_hook`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_input_require_grads`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1571)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Enables the gradients for the input embeddings. This is useful for fine-tuning
    adapter weights while keeping the model weights fixed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2617)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`, *optional*) — Can
    be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_tf` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts
    and loading the PyTorch model afterwards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a model folder containing a *flax checkpoint file* in *.msgpack*
    format (e.g, `./flax_model/` containing `flax_model.msgpack`). In this case, `from_flax`
    should be set to `True`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`None` if you are both providing the configuration and state dictionary (resp.
    with keyword arguments `config` and `state_dict`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_args` (sequence of positional arguments, *optional*) — All remaining
    positional arguments will be passed to the underlying model’s `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) — Can be
    either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a string or path valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state_dict` (`Dict[str, torch.Tensor]`, *optional*) — A state dictionary to
    use instead of a state dictionary loaded from saved weights file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This option can be used if you want to create a model from a pretrained configuration
    but load your own weights. In this case though, you should check if using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    is not a simpler option.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) — Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_tf` (`bool`, *optional*, defaults to `False`) — Load the model weights
    from a TensorFlow checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_flax` (`bool`, *optional*, defaults to `False`) — Load the model weights
    from a Flax checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) — Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_loading_info(bool,` *optional*, defaults to `False`) — Whether ot not
    to also return a dictionary containing missing keys, unexpected keys and error
    messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only(bool,` *optional*, defaults to `False`) — Whether or not
    to only look at local files (i.e., do not try to download the model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To test a pull request you made on the Hub, you can pass `revision=“refs/pr/<pr_number>“.</pr_number>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`mirror` (`str`, *optional*) — Mirror source to accelerate downloads in China.
    If you are from China and have an accessibility problem, you can set this option
    to resolve it. Note that we do not guarantee the timeliness or safety. Please
    refer to the mirror site for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_fast_init(bool,` *optional*, defaults to `True`) — Whether or not to disable
    fast initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One should only disable *_fast_init* to ensure backwards compatibility with
    `transformers.__version__ < 4.6.0` for seeded model initialization. This argument
    will be removed at the next major version. See [pull request 11471](https://github.com/huggingface/transformers/pull/11471)
    for more information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Parameters for big model inference
  prefs: []
  type: TYPE_NORMAL
- en: '`low_cpu_mem_usage(bool,` *optional*) — Tries to not use more than 1x model
    size in CPU memory (including peak memory) while loading the model. This is an
    experimental feature and a subject to change at any moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch_dtype` (`str` or `torch.dtype`, *optional*) — Override the default `torch.dtype`
    and load the model under a specific `dtype`. The different options are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified `dtype`,
    ignoring the model’s `config.torch_dtype` if one exists. If not specified'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: the model will get loaded in `torch.float` (fp32).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"auto"` - A `torch_dtype` entry in the `config.json` file of the model will
    be attempted to be used. If this entry isn’t found then next check the `dtype`
    of the first weight in the checkpoint that’s of a floating point type and use
    that as `dtype`. This will load the model using the `dtype` it was saved in at
    the end of the training. It can’t be used as an indicator of how the model was
    trained. Since it could be trained in one of half precision dtypes, but saved
    in fp32.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <tip>For some models the `dtype` they were trained in is unknown - you may try
    to check the model’s paper or reach out to the authors and ask them to add this
    information to the model’s card and to insert the `torch_dtype` entry in `config.json`
    on the hub.</tip>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`device_map` (`str` or `Dict[str, Union[int, str, torch.device]]` or `int`
    or `torch.device`, *optional*) — A map that specifies where each submodule should
    go. It doesn’t need to be refined to each parameter/buffer name, once a given
    module name is inside, every submodule of it will be sent to the same device.
    If we only pass the device (*e.g.*, `"cpu"`, `"cuda:1"`, `"mps"`, or a GPU ordinal
    rank like `1`) on which the model will be allocated, the device map will map the
    entire model to this device. Passing `device_map = 0` means put the whole model
    on GPU 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To have Accelerate compute the most optimized `device_map` automatically, set
    `device_map="auto"`. For more information about each option see [designing a device
    map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_memory` (`Dict`, *optional*) — A dictionary device identifier to maximum
    memory. Will default to the maximum memory available for each GPU and the available
    CPU RAM if unset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_folder` (`str` or `os.PathLike`, *optional*) — If the `device_map`
    contains any value `"disk"`, the folder where we will offload weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`offload_state_dict` (`bool`, *optional*) — If `True`, will temporarily offload
    the CPU state dict to the hard drive to avoid getting out of CPU RAM if the weight
    of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults
    to `True` when there is some disk offload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_in_8bit` (`bool`, *optional*, defaults to `False`) — If `True`, will
    convert the loaded model into mixed-8bit quantized model. To use this feature
    please install `bitsandbytes` (`pip install -U bitsandbytes`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_in_4bit` (`bool`, *optional*, defaults to `False`) — If `True`, will
    convert the loaded model into 4bit precision quantized model. To use this feature
    install the latest version of `bitsandbytes` (`pip install -U bitsandbytes`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quantization_config` (`Union[QuantizationConfigMixin,Dict]`, *optional*) —
    A dictionary of configuration parameters or a QuantizationConfigMixin object for
    quantization (e.g bitsandbytes, gptq)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) — In case the relevant files
    are located inside a subfolder of the model repo on huggingface.co, you can specify
    the folder name here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variant` (`str`, *optional*) — If specified load weights from `variant` filename,
    *e.g.* pytorch_model.<variant>.bin. `variant` is ignored when using `from_tf`
    or `from_flax`.</variant>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_safetensors` (`bool`, *optional*, defaults to `None`) — Whether or not
    to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`
    is not installed, it will be set to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (remaining dictionary of keyword arguments, *optional*) — Can be used
    to update the configuration object (after it being loaded) and initiate the model
    (e.g., `output_attentions=True`). Behaves differently depending on whether a `config`
    is provided or automatically loaded:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a configuration is provided with `config`, `**kwargs` will be directly passed
    to the underlying model’s `__init__` method (we assume all relevant updates to
    the configuration have already been done)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a configuration is not provided, `kwargs` will be first passed to the configuration
    class initialization function ([from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)).
    Each key of `kwargs` that corresponds to a configuration attribute will be used
    to override said attribute with the supplied `kwargs` value. Remaining keys that
    do not correspond to any configuration attribute will be passed to the underlying
    model’s `__init__` function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a pretrained pytorch model from a pre-trained model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The model is set in evaluation mode by default using `model.eval()` (Dropout
    modules are deactivated). To train the model, you should first set it back in
    training mode with `model.train()`.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Activate the special [“offline-mode”](https://huggingface.co/transformers/installation.html#offline-mode)
    to use this method in a firewalled environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`low_cpu_mem_usage` algorithm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an experimental function that loads the model using ~1x model size CPU
    memory
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: save which state_dict keys we have
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: drop state_dict before the model is created, since the latter takes 1x model
    size CPU memory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: after the model has been instantiated switch to the meta device all params/buffers
    that are going to be replaced from the loaded state_dict
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: load state_dict 2nd time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: replace the params/buffers from the state_dict
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Currently, it can’t handle deepspeed ZeRO stage 3 and ignores loading errors
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_input_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1588)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: A torch module mapping vocabulary to hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the model’s input embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_memory_footprint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2540)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`return_buffers` (`bool`, *optional*, defaults to `True`) — Whether to return
    the size of the buffer tensors in the computation of the memory footprint. Buffers
    are tensors that do not require gradients and not registered as parameters. E.g.
    mean and std in batch norm layers. Please see: [https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2](https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get the memory footprint of a model. This will return the memory footprint
    of the current model in bytes. Useful to benchmark the memory footprint of the
    current model and design some tests. Solution inspired from the PyTorch discussions:
    [https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2](https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_output_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1614)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: A torch module mapping hidden states to vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the model’s output embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gradient_checkpointing_disable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Deactivates gradient checkpointing for the current model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in other frameworks this feature can be referred to as “activation
    checkpointing” or “checkpoint activations”.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `gradient_checkpointing_enable`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`gradient_checkpointing_kwargs` (dict, *optional*) — Additional keyword arguments
    passed along to the `torch.utils.checkpoint.checkpoint` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activates gradient checkpointing for the current model.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in other frameworks this feature can be referred to as “activation
    checkpointing” or “checkpoint activations”.
  prefs: []
  type: TYPE_NORMAL
- en: We pass the `__call__` method of the modules instead of `forward` because `__call__`
    attaches all the hooks of the module. [https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2](https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `init_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2068)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`,
    you need to implement any initialization logic in `_init_weights`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_init`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1256)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A method executed at the end of each Transformer model initialization, to execute
    code that needs the model’s modules properly initialized (such as weight initialization).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prune_heads`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2085)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`heads_to_prune` (`Dict[int, List[int]]`) — Dictionary with keys being selected
    layer indices (`int`) and associated values being the list of heads to prune in
    said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads
    0 and 2 on layer 1 and heads 2 and 3 on layer 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prunes heads of the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `register_for_auto_class`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4427)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"AutoModel"`) — The
    auto class to register this new model with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  prefs: []
  type: TYPE_NORMAL
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `resize_token_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1786)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`new_num_tokens` (`int`, *optional*) — The new number of tokens in the embedding
    matrix. Increasing the size will add newly initialized vectors at the end. Reducing
    the size will remove vectors from the end. If not provided or `None`, just returns
    a pointer to the input tokens `torch.nn.Embedding` module of the model without
    doing anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the embedding matrix
    to a multiple of the provided value.If `new_num_tokens` is set to `None` will
    just pad the embedding to a multiple of `pad_to_multiple_of`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128\. For more details about this, or help on
    choosing the correct value for resizing, refer to this guide: [https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Embedding`'
  prefs: []
  type: TYPE_NORMAL
- en: Pointer to the input tokens Embeddings Module of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.
  prefs: []
  type: TYPE_NORMAL
- en: Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reverse_bettertransformer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4481)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
  prefs: []
  type: TYPE_NORMAL
- en: The model converted back to the original modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Reverts the transformation from [to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)
    so that the original modeling is used, for example in order to save the model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L2199)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) — Directory to which to save. Will
    be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_main_process` (`bool`, *optional*, defaults to `True`) — Whether the process
    calling this is the main process or not. Useful when in distributed training like
    TPUs and need to call this function on all processes. In this case, set `is_main_process=True`
    only on the main process to avoid race conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state_dict` (nested dictionary of `torch.Tensor`) — The state dictionary of
    the model to save. Will default to `self.state_dict()`, but can be used to only
    save parts of the model or if special precautions need to be taken when recovering
    the state dictionary of a model (like when using model parallelism).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_function` (`Callable`) — The function to use to save the state dictionary.
    Useful on distributed training like TPUs when one need to replace `torch.save`
    by another method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`). We default it to 5GB in order for models to be able
    to run easily on free-tier google colab instances without CPU OOM issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether to
    save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variant` (`str`, *optional*) — If specified, weights are saved in the format
    pytorch_model.<variant>.bin.</variant>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_peft_format` (`bool`, *optional*, defaults to `True`) — For backward
    compatibility with PEFT library, in case adapter weights are attached to the model,
    all keys of the state dict of adapters needs to be pre-pended with `base_model.model`.
    Advanced users can disable this behaviours by setting `save_peft_format` to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_input_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1601)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`nn.Module`) — A module mapping vocabulary to hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set model’s input embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tie_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1641)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Tie the weights between the input embeddings and the output embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: If the `torchscript` flag is set in the configuration, can’t handle parameter
    sharing so we are cloning the weights instead.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_bettertransformer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4453)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)'
  prefs: []
  type: TYPE_NORMAL
- en: The model converted to BetterTransformer.
  prefs: []
  type: TYPE_NORMAL
- en: Converts the model to use [PyTorch’s native attention implementation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html),
    integrated to Transformers through [Optimum library](https://huggingface.co/docs/optimum/bettertransformer/overview).
    Only a subset of all Transformers models are supported.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s attention fastpath allows to speed up inference through kernel fusions
    and the use of [nested tensors](https://pytorch.org/docs/stable/nested.html).
    Detailed benchmarks can be found in [this blog post](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `warn_if_padding_and_no_attention_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L4503)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Shows a one-time warning if the input_ids appear to contain padding and no attention
    mask was given.
  prefs: []
  type: TYPE_NORMAL
- en: Large model loading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Transformers 4.20.0, the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method has been reworked to accommodate large models using [Accelerate](https://huggingface.co/docs/accelerate/big_modeling).
    This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0\. Instead of creating the
    full model, then loading the pretrained weights inside it (which takes twice the
    size of the model in RAM, one for the randomly initialized model, one for the
    weights), there is an option to create the model as an empty shell, then only
    materialize its parameters when the pretrained weights are loaded.
  prefs: []
  type: TYPE_NORMAL
- en: This option can be activated with `low_cpu_mem_usage=True`. The model is first
    created on the Meta device (with empty weights) and the state dict is then loaded
    inside it (shard by shard in the case of a sharded checkpoint). This way the maximum
    RAM used is the full size of the model only.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Moreover, you can directly place the model on different devices if it doesn’t
    fully fit in RAM (only works for inference for now). With `device_map="auto"`,
    Accelerate will determine where to put each layer to maximize the use of your
    fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive
    if you don’t have enough GPU RAM (or CPU RAM). Even if the model is split across
    several devices, it will run as you would normally expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'When passing a `device_map`, `low_cpu_mem_usage` is automatically set to `True`,
    so you don’t need to specify it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You can inspect how the model was split across devices by looking at its `hf_device_map`
    attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also write your own device map following the same format (a dictionary
    layer name to device). It should map all parameters of the model to a given device,
    but you don’t have to detail where all the submodules of one layer go if that
    layer is entirely on the same device. For instance, the following device map would
    work properly for T0pp (as long as you have the GPU memory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Another way to minimize the memory impact of your model is to instantiate it
    at a lower precision dtype (like `torch.float16`) or use direct quantization techniques
    as described below.
  prefs: []
  type: TYPE_NORMAL
- en: Model Instantiation dtype
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Under Pytorch a model normally gets instantiated with `torch.float32` format.
    This can be an issue if one tries to load a model whose weights are in fp16, since
    it’d require twice as much memory. To overcome this limitation, you can either
    explicitly pass the desired `dtype` using `torch_dtype` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'or, if you want the model to always load in the most optimal memory pattern,
    you can use the special value `"auto"`, and then `dtype` will be automatically
    derived from the model’s weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Models instantiated from scratch can also be told which `dtype` to use with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Due to Pytorch design, this functionality is only available for floating dtypes.
  prefs: []
  type: TYPE_NORMAL
- en: ModuleUtilsMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.modeling_utils.ModuleUtilsMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L853)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: A few utilities for `torch.nn.Modules`, to be used as a mixin.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_memory_hooks`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L884)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Add a memory hook before and after each sub-module forward pass to record increase
    in memory consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Increase in memory consumption is stored in a `mem_rss_diff` attribute for each
    module and can be reset to zero with `model.reset_memory_hooks_state()`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `estimate_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`dict`) — The model inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The total number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Helper function to estimate the total number of tokens from the model inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `floating_point_ops`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — The batch size for the forward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_length` (`int`) — The number of tokens in each line of the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_embeddings` (`bool`, *optional*, defaults to `True`) — Whether or
    not to count embedding and softmax operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of floating-point operations.
  prefs: []
  type: TYPE_NORMAL
- en: Get number of (optionally, non-embeddings) floating-point operations for the
    forward and backward passes of a batch with this transformer model. Default approximation
    neglects the quadratic dependency on the number of tokens (valid if `12 * d_model
    << sequence_length`) as laid out in [this paper](https://arxiv.org/pdf/2001.08361.pdf)
    section 2.1\. Should be overridden for transformers with parameter re-use e.g.
    Albert or Universal Transformers, or if doing long-range modeling with very high
    sequence lengths.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_extended_attention_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L972)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor`) — Mask with ones indicating tokens to attend
    to, zeros for tokens to ignore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_shape` (`Tuple[int]`) — The shape of the input to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes broadcastable attention and causal masks so that future and masked tokens
    are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_head_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1024)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers
    x num_heads]`, *optional*) — The mask indicating if we should keep the heads or
    not (1.0 for keep, 0.0 for discard).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`) — The number of hidden layers in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_attention_chunked` (`bool`, *optional*, defaults to `False`) — Whether
    or not the attentions scores are computed by chunks or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the head mask if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `invert_attention_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L920)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_attention_mask` (`torch.Tensor`) — An attention mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: The inverted attention mask.
  prefs: []
  type: TYPE_NORMAL
- en: Invert an attention mask (e.g., switches 0\. and 1.).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `num_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L1062)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`only_trainable` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return only the number of trainable parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_embeddings` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return only the number of non-embeddings parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Get number of (optionally, trainable or non-embeddings) parameters in the module.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reset_memory_hooks_state`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L896)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Reset the `mem_rss_diff` attribute of each module (see [add_memory_hooks()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks)).
  prefs: []
  type: TYPE_NORMAL
- en: TFPreTrainedModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFPreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1058)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all TF models.
  prefs: []
  type: TYPE_NORMAL
- en: '[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models as well as a few methods common to all
    models to:'
  prefs: []
  type: TYPE_NORMAL
- en: resize the input embeddings,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prune heads in the self-attention heads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class attributes (overridden by derived classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base_model_prefix` (`str`) — A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main_input_name` (`str`) — The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3067)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload model"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) — Only
    applicable for models. The maximum size for a checkpoint before being sharded.
    Checkpoints shard will then be each of size lower than this size. If expressed
    as a string, needs to be digits followed by a unit (like `"5MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the model files to the 🤗 Model Hub while synchronizing a local clone
    of the repo in `repo_path_or_name`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '#### `can_generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1301)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`bool`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether this model can generate sequences with `.generate()`.
  prefs: []
  type: TYPE_NORMAL
- en: Returns whether this model can generate sequences with `.generate()`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compile`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1496)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This is a thin wrapper that sets the model’s loss output head as the loss if
    the user does not specify a loss function themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_model_card`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1791)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`output_dir` (`str` or `os.PathLike`) — The folder in which to create the model
    card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_name` (`str`, *optional*) — The name of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language` (`str`, *optional*) — The language of the model (if applicable)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`license` (`str`, *optional*) — The license of the model. Will default to the
    license of the pretrained model used, if the original model given to the `Trainer`
    comes from a repo on the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` (`str` or `List[str]`, *optional*) — Some tags to be included in the
    metadata of the model card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`finetuned_from` (`str`, *optional*) — The name of the model used to fine-tune
    this one (if applicable). Will default to the name of the repo of the original
    model given to the `Trainer` (if it comes from the Hub).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tasks` (`str` or `List[str]`, *optional*) — One or several task identifiers,
    to be included in the metadata of the model card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_tags` (`str` or `List[str]`, *optional*) — One or several dataset
    tags, to be included in the metadata of the model card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset` (`str` or `List[str]`, *optional*) — One or several dataset identifiers,
    to be included in the metadata of the model card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_args` (`str` or `List[str]`, *optional*) — One or several dataset
    arguments, to be included in the metadata of the model card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates a draft of a model card using the information available to the `Trainer`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `eager_serving`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1213)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`Dict[str, tf.Tensor]`) — The input of the saved model as a dictionary
    of tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Method used for serving the model. This method is deprecated, and will be removed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2499)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str`, *optional*) — Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a *PyTorch state_dict save file* (e.g, `./pt_model/pytorch_model.bin`).
    In this case, `from_pt` should be set to `True` and a configuration object should
    be provided as `config` argument. This loading path is slower than converting
    the PyTorch model in a TensorFlow model using the provided conversion scripts
    and loading the TensorFlow model afterwards.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`None` if you are both providing the configuration and state dictionary (resp.
    with keyword arguments `config` and `state_dict`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_args` (sequence of positional arguments, *optional*) — All remaining
    positional arguments will be passed to the underlying model’s `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`Union[PretrainedConfig, str]`, *optional*) — Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a string valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_pt` (`bool`, *optional*, defaults to `False`) — Load the model weights
    from a PyTorch state_dict save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) — Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`, *optional*) — Path to a directory in which a downloaded
    pretrained model configuration should be cached if the standard cache should not
    be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists. proxies — (`Dict[str, str],` optional`): A dictionary of proxy
    servers to use by protocol or endpoint, e.g.,` {‘http’: ‘foo.bar:3128’, ‘http://hostname’:
    ‘foo.bar:4012’}`. The proxies are used on each request. output_loading_info(`bool`,
    *optional*, defaults to` False`): Whether ot not to also return a dictionary containing
    missing keys, unexpected keys and error messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only(bool,` *optional*, defaults to `False`) — Whether or not
    to only look at local files (e.g., not try downloading the model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a pretrained TF 2.0 model from a pre-trained model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_bias`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1931)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Variable`'
  prefs: []
  type: TYPE_NORMAL
- en: The weights representing the bias, None if not an LM model.
  prefs: []
  type: TYPE_NORMAL
- en: Dict of bias attached to an LM head. The key represents the name of the bias
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_head_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1168)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` with shape `[num_heads]` or `[num_hidden_layers x
    num_heads]`, *optional*) — The mask indicating if we should keep the heads or
    not (1.0 for keep, 0.0 for discard).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`) — The number of hidden layers in the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the head mask if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_input_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1315)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Variable`'
  prefs: []
  type: TYPE_NORMAL
- en: The embeddings layer mapping vocabulary to hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the model’s input embeddings layer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_lm_head`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1964)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Layer`'
  prefs: []
  type: TYPE_NORMAL
- en: The LM head layer if the model has one, None if not.
  prefs: []
  type: TYPE_NORMAL
- en: The LM Head layer. This method must be overwritten by all the models that have
    a lm head.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_output_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1871)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Variable`'
  prefs: []
  type: TYPE_NORMAL
- en: The new weights mapping vocabulary to hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: Returns the model’s output embeddings
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_output_layer_with_bias`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1908)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras.layers.Layer`'
  prefs: []
  type: TYPE_NORMAL
- en: The layer that handles the bias, None if not an LM model.
  prefs: []
  type: TYPE_NORMAL
- en: Get the layer that handles a bias attribute in case the model has an LM head
    with weights tied to the embeddings
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_prefix_bias_name`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1921)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  prefs: []
  type: TYPE_NORMAL
- en: The _prefix name of the bias.
  prefs: []
  type: TYPE_NORMAL
- en: Get the concatenated _prefix name of the bias from the model name to the parent
    layer
  prefs: []
  type: TYPE_NORMAL
- en: '#### `load_repo_checkpoint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1342)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_path_or_name` (`str`) — Can either be a repository name for your {object}
    in the Hub or a path to a local folder (in which case the repository will have
    the name of that local folder).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`dict`'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of extra metadata from the checkpoint, most commonly an “epoch”
    count.
  prefs: []
  type: TYPE_NORMAL
- en: Loads a saved checkpoint (model weights and optimizer state) from a repo. Returns
    the current epoch count when the checkpoint was made.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prepare_tf_dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1391)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` (`Any`) — A [~`datasets.Dataset`] to be wrapped as a `tf.data.Dataset`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` (`int`, defaults to 8) — The size of batches to return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle` (`bool`, defaults to `True`) — Whether to return samples from the
    dataset in random order. Usually `True` for training datasets and `False` for
    validation/test datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase),
    *optional*) — A `PreTrainedTokenizer` that will be used to pad samples to create
    batches. Has no effect if a specific `collate_fn` is passed instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn` (`Callable`, *optional*) — A function that collates samples from
    the dataset into a single batch. Defaults to `DefaultDataCollator` if no `tokenizer`
    is supplied or `DataCollatorWithPadding` if a `tokenizer` is passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collate_fn_args` (`Dict[str, Any]`, *optional*) — A dict of arguments to pass
    to the `collate_fn` alongside the list of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_remainder` (`bool`, *optional*) — Whether to drop the final batch, if
    the batch_size does not evenly divide the dataset length. Defaults to the same
    setting as `shuffle`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefetch` (`bool`, defaults to `True`) — Whether to add prefetching to the
    end of the `tf.data` pipeline. This is almost always beneficial for performance,
    but can be disabled in edge cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dataset`'
  prefs: []
  type: TYPE_NORMAL
- en: A `tf.data.Dataset` which is ready to pass to the Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: Wraps a HuggingFace [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)
    as a `tf.data.Dataset` with collation and batching. This method is designed to
    create a “ready-to-use” dataset that can be passed directly to Keras methods like
    `fit()` without further modification. The method will drop columns from the dataset
    if they don’t match input names for the model. If you want to specify the column
    names to return rather than using the names that match this model, we recommend
    using `Dataset.to_tf_dataset()` instead.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `prune_heads`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2311)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`heads_to_prune` (`Dict[int, List[int]]`) — Dictionary with keys being selected
    layer indices (`int`) and associated values being the list of heads to prune in
    said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads
    0 and 2 on layer 1 and heads 2 and 3 on layer 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prunes heads of the base model.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `register_for_auto_class`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L3176)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"TFAutoModel"`) — The
    auto class to register this new model with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  prefs: []
  type: TYPE_NORMAL
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `resize_token_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1973)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`new_num_tokens` (`int`, *optional*) — The number of new tokens in the embedding
    matrix. Increasing the size will add newly initialized vectors at the end. Reducing
    the size will remove vectors from the end. If not provided or `None`, just returns
    a pointer to the input tokens without doing anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Variable` or `tf.keras.layers.Embedding`'
  prefs: []
  type: TYPE_NORMAL
- en: Pointer to the input tokens of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.
  prefs: []
  type: TYPE_NORMAL
- en: Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L2323)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str`) — Directory to which to save. Will be created if it
    doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`saved_model` (`bool`, *optional*, defaults to `False`) — If the model has
    to be saved in saved model format as well or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`version` (`int`, *optional*, defaults to 1) — The version of the saved model.
    A saved model needs to be versioned in order to be properly loaded by TensorFlow
    Serving as detailed in the official documentation [https://www.tensorflow.org/tfx/serving/serving_basic](https://www.tensorflow.org/tfx/serving/serving_basic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`signatures` (`dict` or `tf.function`, *optional*) — Model’s signature used
    for serving. This will be passed to the `signatures` argument of model.save().'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) — The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) — Whether to
    save the model using `safetensors` or the traditional TensorFlow way (that uses
    `h5`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `serving`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`Method` used for serving the model. Does not have a specific signature, but
    will be specialized as concrete —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`functions` when saving with `save_pretrained`. — inputs (`Dict[str, tf.Tensor]`):
    The input of the saved model as a dictionary of tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `serving_output`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1277)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Prepare the output of the saved model. Can be overridden if specific serving
    modifications are required.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_bias`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1948)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`Dict[tf.Variable]`) — All the new bias attached to an LM head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set all the bias in the LM head.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_input_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1851)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`tf.Variable`) — The new weights mapping hidden states to vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set model’s input embeddings
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_output_embeddings`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1891)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`value` (`tf.Variable`) — The new weights mapping hidden states to vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set model’s output embeddings
  prefs: []
  type: TYPE_NORMAL
- en: '#### `test_step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1687)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: A modification of Keras’s default `train_step` that correctly handles matching
    outputs to labels for our models and supports directly training on the loss output
    head. In addition, it ensures input keys are copied to the labels where appropriate.
    It will also copy label keys into the input dict when using the dummy loss, to
    ensure that they are available to the model during the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `train_step`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L1579)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: A modification of Keras’s default `train_step` that correctly handles matching
    outputs to labels for our models and supports directly training on the loss output
    head. In addition, it ensures input keys are copied to the labels where appropriate.
    It will also copy label keys into the input dict when using the dummy loss, to
    ensure that they are available to the model during the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: TFModelUtilsMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.modeling_tf_utils.TFModelUtilsMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L104)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: A few utilities for `tf.keras.Model`, to be used as a mixin.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `num_parameters`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_tf_utils.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`only_trainable` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return only the number of trainable parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`int`'
  prefs: []
  type: TYPE_NORMAL
- en: The number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Get the number of (optionally, trainable) parameters in the model.
  prefs: []
  type: TYPE_NORMAL
- en: FlaxPreTrainedModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxPreTrainedModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L166)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all models.
  prefs: []
  type: TYPE_NORMAL
- en: '[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)
    takes care of storing the configuration of the models and handles methods for
    loading, downloading and saving models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Class attributes (overridden by derived classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '`config_class` ([PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig))
    — A subclass of [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    to use as configuration class for this model architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base_model_prefix` (`str`) — A string indicating the attribute associated
    to the base model in derived classes of the same architecture adding modules on
    top of the base model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main_input_name` (`str`) — The name of the principal input to the model (often
    `input_ids` for NLP models, `pixel_values` for vision models and `input_values`
    for speech models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The name of the repository you want to push your model
    to. It should contain your organization name when pushing to a given organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload model"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — The description of the commit that
    will be created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` (`List[str]`, *optional*) — List of tags to push on the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the model checkpoint to the 🤗 Model Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '#### `can_generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L506)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns whether this model can generate sequences with `.generate()`. Returns:
    `bool`: Whether this model can generate sequences with `.generate()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L518)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) — Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A string, the *model id* of a pretrained model hosted inside a model repo on
    huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`,
    or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path to a *directory* containing model weights saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained),
    e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A path or url to a *pt index checkpoint file* (e.g, `./tf_model/model.ckpt.index`).
    In this case, `from_pt` should be set to `True`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`model_args` (sequence of positional arguments, *optional*) — All remaining
    positional arguments will be passed to the underlying model’s `__init__` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` (`Union[PretrainedConfig, str, os.PathLike]`, *optional*) — Can be
    either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an instance of a class derived from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a string or path valid as input to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configuration for the model to use instead of an automatically loaded configuration.
    Configuration can be automatically loaded when:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model is a model provided by the library (loaded with the *model id* string
    of a pretrained model).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model was saved using [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained)
    and is reloaded by supplying the save directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is loaded by supplying a local directory as `pretrained_model_name_or_path`
    and a configuration JSON file named *config.json* is found in the directory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`Union[str, os.PathLike]`, *optional*) — Path to a directory in
    which a downloaded pretrained model configuration should be cached if the standard
    cache should not be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`from_pt` (`bool`, *optional*, defaults to `False`) — Load the model weights
    from a PyTorch checkpoint save file (see docstring of `pretrained_model_name_or_path`
    argument).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_mismatched_sizes` (`bool`, *optional*, defaults to `False`) — Whether
    or not to raise an error if some of the weights from the checkpoint do not have
    the same size as the weights of the model (if for instance, you are instantiating
    a model with 10 labels from a checkpoint with 3 labels).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force the (re-)download of the model weights and configuration files, overriding
    the cached versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received files. Will attempt to resume the download if
    such a file exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}`. The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local_files_only(bool,` *optional*, defaults to `False`) — Whether or not
    to only look at local files (i.e., do not try to download the model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a pretrained flax model from a pre-trained model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not initialized from pretrained model* means that
    the weights of XXX do not come pretrained with the rest of the model. It is up
    to you to train those weights with a downstream fine-tuning task.
  prefs: []
  type: TYPE_NORMAL
- en: The warning *Weights from XXX not used in YYY* means that the layer XXX is not
    used by YYY, therefore those weights are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '#### `load_flax_sharded_weights`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L459)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`shard_files` (`List[str]` — The list of shard files to load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Dict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A nested dictionary of the model parameters, in the expected format for flax
    models : `{''model'': {''params'': {''...''}}}`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the same as `flax.serialization.from_bytes` (https:lax.readthedocs.io/en/latest/_modules/flax/serialization.html#from_bytes)
    but for a sharded checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'This load is performed efficiently: each checkpoint shard is loaded one by
    one in RAM and deleted after being loaded in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `register_for_auto_class`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1226)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`auto_class` (`str` or `type`, *optional*, defaults to `"FlaxAutoModel"`) —
    The auto class to register this new model with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Register this class with a given auto class. This should only be used for custom
    models as the ones in the library are already mapped with an auto class.
  prefs: []
  type: TYPE_NORMAL
- en: This API is experimental and may have some slight breaking changes in the next
    releases.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L1088)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) — Directory to which to save. Will
    be created if it doesn’t exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"10GB"`) — The maximum
    size for a checkpoint before being sharded. Checkpoints shard will then be each
    of size lower than this size. If expressed as a string, needs to be digits followed
    by a unit (like `"5MB"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a single weight of the model is bigger than `max_shard_size`, it will be
    in its own checkpoint shard which will be bigger than `max_shard_size`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `False`) — Whether to
    save the model using `safetensors` or through msgpack.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a model and its configuration file to a directory, so that it can be re-loaded
    using the `[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)`
    class method
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_bf16`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L329)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (`Union[Dict, FrozenDict]`) — A `PyTree` of model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask` (`Union[Dict, FrozenDict]`) — A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new
    `params` tree and does not cast the `params` in place.
  prefs: []
  type: TYPE_NORMAL
- en: This method can be used on TPU to explicitly convert the model parameters to
    bfloat16 precision to do full half-precision training or to save weights in bfloat16
    for inference in order to save memory and improve speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_fp16`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L395)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (`Union[Dict, FrozenDict]`) — A `PyTree` of model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask` (`Union[Dict, FrozenDict]`) — A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the floating-point `parmas` to `jax.numpy.float16`. This returns a new
    `params` tree and does not cast the `params` in place.
  prefs: []
  type: TYPE_NORMAL
- en: This method can be used on GPU to explicitly convert the model parameters to
    float16 precision to do full half-precision training or to save weights in float16
    for inference in order to save memory and improve speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '#### `to_fp32`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_flax_utils.py#L368)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`params` (`Union[Dict, FrozenDict]`) — A `PyTree` of model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask` (`Union[Dict, FrozenDict]`) — A `PyTree` with same structure as the
    `params` tree. The leaves should be booleans, `True` for params you want to cast,
    and should be `False` for those you want to skip'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cast the floating-point `parmas` to `jax.numpy.float32`. This method can be
    used to explicitly convert the model parameters to fp32 precision. This returns
    a new `params` tree and does not cast the `params` in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Pushing to the Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.utils.PushToHubMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L639)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: A Mixin containing the functionality to push a model or tokenizer to the hub.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `push_to_hub`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repo_id` (`str`) — The name of the repository you want to push your {object}
    to. It should contain your organization name when pushing to a given organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_temp_dir` (`bool`, *optional*) — Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_message` (`str`, *optional*) — Message to commit while pushing. Will
    default to `"Upload {object}"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`private` (`bool`, *optional*) — Whether or not the repository created should
    be private.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`bool` or `str`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) — Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) — Whether or not to create
    a PR with the uploaded files or directly commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) — Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*) — Branch to push the uploaded files to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commit_description` (`str`, *optional*) — The description of the commit that
    will be created'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` (`List[str]`, *optional*) — List of tags to push on the Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload the {object_files} to the 🤗 Model Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Sharded checkpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `transformers.modeling_utils.load_sharded_checkpoint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_utils.py#L415)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model in which to load the checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`folder` (`str` or `os.PathLike`) — A path to a folder containing the sharded
    checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strict` (`bool`, *optional`, defaults to` True`) — Whether to strictly enforce
    that the keys in the model state dict match the keys in the sharded checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefer_safe` (`bool`, *optional*, defaults to `False`) — If both safetensors
    and PyTorch save files are present in checkpoint and `prefer_safe` is True, the
    safetensors files will be loaded. Otherwise, PyTorch files are always loaded when
    possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`NamedTuple`'
  prefs: []
  type: TYPE_NORMAL
- en: A named tuple with `missing_keys` and `unexpected_keys` fields
  prefs: []
  type: TYPE_NORMAL
- en: '`missing_keys` is a list of str containing the missing keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unexpected_keys` is a list of str containing the unexpected keys'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the same as [`torch.nn.Module.load_state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)
    but for a sharded checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'This load is performed efficiently: each checkpoint shard is loaded one by
    one in RAM and deleted after being loaded in the model.'
  prefs: []
  type: TYPE_NORMAL
