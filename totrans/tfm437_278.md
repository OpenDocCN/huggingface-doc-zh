# MobileViTV2

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevitv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mobilevitv2)

## 概述

MobileViTV2模型是由Sachin Mehta和Mohammad Rastegari在[移动视觉transformers的可分离自我关注](https://arxiv.org/abs/2206.02680)中提出的。

MobileViTV2是MobileViT的第二个版本，通过用可分离自我关注替换MobileViT中的多头自我关注构建而成。

论文摘要如下：

*Mobile视觉transformers（MobileViT）可以在几个移动视觉任务中实现最先进的性能，包括分类和检测。尽管这些模型参数较少，但与基于卷积神经网络的模型相比，延迟较高。MobileViT中的主要效率瓶颈是transformers中的多头自注意力（MHA），它需要O(k2)的时间复杂度，关于标记（或补丁）k的数量。此外，MHA需要昂贵的操作（例如，批次矩阵乘法）来计算自我关注，影响资源受限设备上的延迟。本文介绍了一种具有线性复杂度的可分离自我关注方法，即O(k)。所提出方法的一个简单而有效的特点是它使用逐元素操作来计算自我关注，使其成为资源受限设备的良好选择。改进的模型MobileViTV2在几个移动视觉任务中处于领先地位，包括ImageNet对象分类和MS-COCO对象检测。MobileViTV2约有三百万参数，在ImageNet数据集上实现了75.6%的top-1准确率，比MobileViT高出约1%，同时在移动设备上运行速度快3.2倍。*

此模型由[shehan97](https://huggingface.co/shehan97)贡献。原始代码可以在[这里](https://github.com/apple/ml-cvnets)找到。

## 使用提示

+   MobileViTV2更像是CNN而不是Transformer模型。它不适用于序列数据，而是适用于图像批次。与ViT不同，没有嵌入。骨干模型输出特征图。

+   可以使用[MobileViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTImageProcessor)来为模型准备图像。请注意，如果您自己进行预处理，预训练检查点期望图像按BGR像素顺序排列（而不是RGB）。

+   可用的图像分类检查点是在[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)（也称为ILSVRC 2012，包含130万张图像和1000个类别）上预训练的。

+   分割模型使用[DeepLabV3](https://arxiv.org/abs/1706.05587)头。可用的语义分割检查点是在[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)上预训练的。

## MobileViTV2Config

### `class transformers.MobileViTV2Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/configuration_mobilevitv2.py#L34)

```py
( num_channels = 3 image_size = 256 patch_size = 2 expand_ratio = 2.0 hidden_act = 'swish' conv_kernel_size = 3 output_stride = 32 classifier_dropout_prob = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 aspp_out_channels = 512 atrous_rates = [6, 12, 18] aspp_dropout_prob = 0.1 semantic_loss_ignore_index = 255 n_attn_blocks = [2, 4, 3] base_attn_unit_dims = [128, 192, 256] width_multiplier = 1.0 ffn_multiplier = 2 attn_dropout = 0.0 ffn_dropout = 0.0 **kwargs )
```

参数

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。

+   `image_size` (`int`, *optional*, defaults to 256) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 2) — 每个补丁的大小（分辨率）。

+   `expand_ratio` (`float`, *optional*, defaults to 2.0) — MobileNetv2层的扩展因子。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"swish"`) — Transformer编码器和卷积层中的非线性激活函数（函数或字符串）。

+   `conv_kernel_size` (`int`, *optional*, defaults to 3) — MobileViTV2层中卷积核的大小。

+   `output_stride` (`int`, *optional*, defaults to 32) — 输出空间分辨率与输入图像分辨率的比率。

+   `classifier_dropout_prob` (`float`, *optional*, defaults to 0.1) — 附加分类器的丢失比率。

+   `initializer_range`（`float`，*可选*，默认为0.02）—用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps`（`float`，*可选*，默认为1e-05）—层归一化层使用的epsilon。

+   `aspp_out_channels`（`int`，*可选*，默认为512）—语义分割中ASPP层使用的输出通道数。

+   `atrous_rates`（`List[int]`，*可选*，默认为`[6, 12, 18]`）—语义分割中ASPP层中使用的扩张（atrous）因子。

+   `aspp_dropout_prob`（`float`，*可选*，默认为0.1）—语义分割中ASPP层的dropout比率。

+   `semantic_loss_ignore_index`（`int`，*可选*，默认为255）—语义分割模型的损失函数中被忽略的索引。

+   `n_attn_blocks`（`List[int]`，*可选*，默认为`[2, 4, 3]`）—每个MobileViTV2Layer中的注意力块数量。

+   `base_attn_unit_dims`（`List[int]`，*可选*，默认为`[128, 192, 256]`）—每个MobileViTV2Layer中注意力块维度的基本乘数

+   `width_multiplier`（`float`，*可选*，默认为1.0）—MobileViTV2的宽度乘数。

+   `ffn_multiplier`（`int`，*可选*，默认为2）— MobileViTV2的FFN乘数。

+   `attn_dropout`（`float`，*可选*，默认为0.0）—注意力层中的dropout。

+   `ffn_dropout`（`float`，*可选*，默认为0.0）—FFN层之间的dropout。

这是一个配置类，用于存储[MobileViTV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model)的配置。它用于根据指定的参数实例化MobileViTV2模型，定义模型架构。使用默认值实例化配置将产生类似于MobileViTV2 [apple/mobilevitv2-1.0](https://huggingface.co/apple/mobilevitv2-1.0)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import MobileViTV2Config, MobileViTV2Model

>>> # Initializing a mobilevitv2-small style configuration
>>> configuration = MobileViTV2Config()

>>> # Initializing a model from the mobilevitv2-small style configuration
>>> model = MobileViTV2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## MobileViTV2Model

### `class transformers.MobileViTV2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L650)

```py
( config: MobileViTV2Config expand_output: bool = True )
```

参数

+   `config`（[MobileViTV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config)）—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

MobileViTV2模型裸露地输出原始隐藏状态，没有特定的头部。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有内容。

前进

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L688)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[MobileViTV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config)）和输入。

+   `last_hidden_state`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 在空间维度上进行池化操作后的最后一层隐藏状态。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为嵌入的输出+每层的输出）。

    每层模型的隐藏状态以及可选的初始嵌入输出。

[MobileViTV2Model](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Model)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, MobileViTV2Model
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")
>>> model = MobileViTV2Model.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 512, 8, 8]
```

## MobileViTV2ForImageClassification

### `class transformers.MobileViTV2ForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L738)

```py
( config: MobileViTV2Config )
```

参数

+   `config`（[MobileViTV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

MobileViTV2模型，顶部带有一个图像分类头（在池化特征的顶部有一个线性层），例如用于ImageNet。

该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L763)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失）。如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[MobileViTV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 分类（如果config.num_labels==1则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`） — 分类（如果config.num_labels==1则为回归）分数（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组。模型在每个阶段输出的隐藏状态（也称为特征图）。

[MobileViTV2ForImageClassification](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, MobileViTV2ForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")
>>> model = MobileViTV2ForImageClassification.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## MobileViTV2ForSemanticSegmentation

### `class transformers.MobileViTV2ForSemanticSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L939)

```py
( config: MobileViTV2Config )
```

参数

+   `config`（[MobileViTV2Config](/docs/transformers/v4.37.2/en/model_doc/mobilevitv2#transformers.MobileViTV2Config)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

MobileViTV2模型，顶部带有语义分割头，例如用于Pascal VOC。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L956)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`） — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[MobileViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__)。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回 [ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels`（形状为 `(batch_size, height, width)` 的 `torch.LongTensor`，*可选*）— 用于计算损失的地面实例分割地图。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或 `config.return_dict=False`）包含各种元素，取决于配置（[MobileViTV2Config](/docs/transformers/v4.37.2/zh/model_doc/mobilevitv2#transformers.MobileViTV2Config)）和输入。

+   `loss`（形状为 `(1,)` 的 `torch.FloatTensor`，*可选*，当提供了 `labels` 时返回）— 分类（或回归，如果 `config.num_labels==1`）损失。

+   `logits`（形状为 `(batch_size, config.num_labels, logits_height, logits_width)` 的 `torch.FloatTensor`）— 每个像素的分类分数。

    <tip warning="{true}">返回的 logits 不一定与作为输入传递的 `pixel_values` 具有相同的大小。这是为了避免进行两次插值并在用户需要将 logits 调整为原始图像大小时丢失一些质量。您应始终检查您的 logits 形状并根据需要调整大小。</tip>

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, patch_size, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_attentions=True` 或 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, patch_size, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

[MobileViTV2ForSemanticSegmentation](/docs/transformers/v4.37.2/zh/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是此函数，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import requests
>>> import torch
>>> from PIL import Image
>>> from transformers import AutoImageProcessor, MobileViTV2ForSemanticSegmentation

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")
>>> model = MobileViTV2ForSemanticSegmentation.from_pretrained("apple/mobilevitv2-1.0-imagenet1k-256")

>>> inputs = image_processor(images=image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> # logits are of shape (batch_size, num_labels, height, width)
>>> logits = outputs.logits
```
