- en: Hands-on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit7/hands-on](https://huggingface.co/learn/deep-rl-course/unit7/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit7/hands-on](https://huggingface.co/learn/deep-rl-course/unit7/hands-on)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you learned the basics of multi-agents, you’re ready to train your
    first agents in a multi-agent system: **a 2vs2 soccer team that needs to beat
    the opponent team**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了多代理的基础知识，您可以开始训练您的第一个多代理系统中的代理：**一个2对2的足球团队需要击败对手团队**。
- en: And you’re going to participate in AI vs. AI challenges where your trained agent
    will compete against other classmates’ **agents every day and be ranked on a new
    leaderboard.**
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您将参加AI vs. AI挑战，您训练的代理将每天与其他同学的**代理竞争，并在新的排行榜上排名。**
- en: To validate this hands-on for the certification process, you just need to push
    a trained model. There **are no minimal results to attain to validate it.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证这个实践项目是否符合认证要求，您只需要推送一个经过训练的模型。**没有最低要求来验证它。**
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有关认证过程的更多信息，请查看此部分👉[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
- en: 'This hands-on will be different since to get correct results **you need to
    train your agents from 4 hours to 8 hours**. And given the risk of timeout in
    Colab, we advise you to train on your computer. You don’t need a supercomputer:
    a simple laptop is good enough for this exercise.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实践项目将会有所不同，因为为了获得正确的结果**您需要训练您的代理从4小时到8小时**。鉴于在Colab中超时的风险，我们建议您在您的计算机上进行训练。您不需要超级计算机：一台简单的笔记本电脑就足够了。
- en: Let’s get started! 🔥
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！🔥
- en: What is AI vs. AI?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是AI vs. AI？
- en: AI vs. AI is an open-source tool we developed at Hugging Face to compete agents
    on the Hub against one another in a multi-agent setting. These models are then
    ranked in a leaderboard.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: AI vs. AI是我们在Hugging Face开发的一个开源工具，用于在Hub上的代理之间进行多代理设置中的竞争。然后这些模型将在排行榜上排名。
- en: 'The idea of this tool is to have a robust evaluation tool: **by evaluating
    your agent with a lot of others, you’ll get a good idea of the quality of your
    policy.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具的想法是拥有一个强大的评估工具：**通过将您的代理与许多其他代理进行评估，您将对您的策略质量有一个很好的了解**。
- en: 'More precisely, AI vs. AI is three tools:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，AI vs. AI包括三个工具：
- en: A *matchmaking process* defining the matches (which model against which) and
    running the model fights using a background task in the Space.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*匹配过程*定义比赛（哪个模型对哪个模型）并在空间中使用后台任务运行模型对抗。
- en: 'A *leaderboard* getting the match history results and displaying the models’
    ELO ratings: [https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*排行榜*获取比赛历史结果并显示模型的ELO评分：[https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos)
- en: 'A *Space demo* to visualize your agents playing against others: [https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*Space演示*，可视化您的代理与其他代理的对战：[https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)
- en: 'In addition to these three tools, your classmate cyllum created a 🤗 SoccerTwos
    Challenge Analytics where you can check the detailed match results of a model:
    [https://huggingface.co/spaces/cyllum/soccertwos-analytics](https://huggingface.co/spaces/cyllum/soccertwos-analytics)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三个工具，您的同学cyllum创建了一个🤗 SoccerTwos Challenge Analytics，您可以在其中查看模型的详细比赛结果：[https://huggingface.co/spaces/cyllum/soccertwos-analytics](https://huggingface.co/spaces/cyllum/soccertwos-analytics)
- en: 'We’re [wrote a blog post to explain this AI vs. AI tool in detail](https://huggingface.co/blog/aivsai),
    but to give you the big picture it works this way:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们[撰写了一篇博文详细解释这个AI vs. AI工具](https://huggingface.co/blog/aivsai)，但为了让您有一个大致的了解，它的工作方式如下：
- en: Every four hours, our algorithm **fetches all the available models for a given
    environment (in our case ML-Agents-SoccerTwos).**
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每四小时，我们的算法**获取给定环境（在我们的情况下是ML-Agents-SoccerTwos）中的所有可用模型。**
- en: It creates a **queue of matches with the matchmaking algorithm.**
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用匹配算法创建一个**比赛队列。**
- en: We simulate the match in a Unity headless process and **gather the match result**
    (1 if the first model won, 0.5 if it’s a draw, 0 if the second model won) in a
    Dataset.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在Unity无头进程中模拟比赛并**收集比赛结果**（如果第一个模型赢得比赛则为1，如果是平局则为0.5，如果第二个模型赢得比赛则为0）。
- en: Then, when all matches from the matches queue are done, **we update the ELO
    score for each model and update the leaderboard.**
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，当所有比赛队列中的比赛都完成时，**我们更新每个模型的ELO分数并更新排行榜。**
- en: Competition Rules
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比赛规则
- en: 'This first AI vs. AI competition **is an experiment**: the goal is to improve
    the tool in the future with your feedback. So some **breakups can happen during
    the challenge**. But don’t worry **all the results are saved in a dataset so we
    can always restart the calculation correctly without losing information**.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个AI vs. AI比赛**是一个实验**：目标是通过您的反馈来改进未来的工具。因此在挑战中可能会发生一些**中断**。但不用担心**所有结果都保存在数据集中，因此我们可以始终正确重新启动计算而不会丢失信息**。
- en: 'In order for your model to get correctly evaluated against others you need
    to follow these rules:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您的模型能够正确地与其他模型进行评估，您需要遵循以下规则：
- en: '**You can’t change the observation space or action space of the agent.** By
    doing that your model will not work during evaluation.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**您不能更改代理的观察空间或行动空间。**这样做会导致您的模型在评估过程中无法工作。'
- en: You **can’t use a custom trainer for now,** you need to use the Unity MLAgents
    ones.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**目前不能使用自定义训练器**，您需要使用Unity MLAgents。'
- en: We provide executables to train your agents. You can also use the Unity Editor
    if you prefer **, but to avoid bugs, we advise that you use our executables**.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提供可执行文件来训练您的代理。您也可以使用Unity编辑器，如果您愿意**，但为了避免错误，我们建议您使用我们的可执行文件**。
- en: What will make the difference during this challenge are **the hyperparameters
    you choose**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个挑战中将会有什么不同的是**您选择的超参数**。
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断努力改进我们的教程，所以**如果您在这个笔记本中发现了一些问题**，请在GitHub Repo上[提出问题](https://github.com/huggingface/deep-rl-class/issues)。
- en: Chat with your classmates, share advice and ask questions on Discord
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Discord上与同学聊天，分享建议和提问
- en: We created a new channel called `ai-vs-ai-challenge` to exchange advice and
    ask questions.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`ai-vs-ai-challenge`的新频道，用于交流建议和提问。
- en: If you didn’t join the discord server yet, you can [join here](https://discord.gg/ydHrjt3WP5)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有加入discord服务器，您可以[在这里加入](https://discord.gg/ydHrjt3WP5)
- en: 'Step 0: Install MLAgents and download the correct executable'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤0：安装MLAgents并下载正确的可执行文件
- en: We advise you to use [conda](https://docs.conda.io/en/latest/) as a package
    manager and create a new environment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您使用[conda](https://docs.conda.io/en/latest/)作为包管理器并创建一个新环境。
- en: 'With conda, we create a new environment called rl with **Python 3.10.12**:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用conda，我们创建一个名为rl的新环境，其中包含**Python 3.10.12**：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To be able to train our agents correctly and push to the Hub, we need to install
    ML-Agents
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够正确训练我们的代理并推送到Hub，我们需要安装ML-Agents
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When the cloning is done (it takes 2.63 GB), we go inside the repository and
    install the package
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆完成后（需要2.63 GB），我们进入存储库并安装软件包
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, you need to install git-lfs: [https://git-lfs.com/](https://git-lfs.com/)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要安装git-lfs：[https://git-lfs.com/](https://git-lfs.com/)
- en: Now that it’s installed, we need to add the environment training executable.
    Based on your operating system you need to download one of them, unzip it and
    place it in a new folder inside `ml-agents` that you call `training-envs-executables`
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经安装好了，我们需要添加环境训练可执行文件。根据您的操作系统，您需要下载其中一个，解压缩并将其放在`ml-agents`中的一个新文件夹中，命名为`training-envs-executables`
- en: At the end your executable should be in `ml-agents/training-envs-executables/SoccerTwos`
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您的可执行文件应该在`ml-agents/training-envs-executables/SoccerTwos`中
- en: 'Windows: Download [this executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Windows：下载[这个可执行文件](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)
- en: 'Linux (Ubuntu): Download [this executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Linux（Ubuntu）：下载[这个可执行文件](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)
- en: 'Mac: Download [this executable](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)
    ⚠ For Mac you need also to call this `xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app`
    to be able to run SoccerTwos'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Mac：下载[这个可执行文件](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)
    ⚠ 对于Mac，您还需要调用`xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app`才能运行SoccerTwos
- en: 'Step 1: Understand the environment'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：了解环境
- en: The environment is called `SoccerTwos`. The Unity MLAgents Team made it. You
    can find its documentation [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境被称为`SoccerTwos`。Unity MLAgents团队制作了它。您可以在[这里](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)找到它的文档
- en: The goal in this environment **is to get the ball into the opponent’s goal while
    preventing the ball from entering your own goal.**
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，目标是**将球进入对手的球门，同时防止球进入自己的球门。**
- en: '![SoccerTwos](../Images/b8d7d800c316a50a5f64472742088b73.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![SoccerTwos](../Images/b8d7d800c316a50a5f64472742088b73.png)'
- en: This environment was made by the [Unity MLAgents Team](https://github.com/Unity-Technologies/ml-agents)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境是由[Unity MLAgents团队](https://github.com/Unity-Technologies/ml-agents)制作的
- en: The reward function
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励函数
- en: 'The reward function is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数是：
- en: '![SoccerTwos Reward](../Images/236ec2816e80441049618048e60a2b89.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![SoccerTwos Reward](../Images/236ec2816e80441049618048e60a2b89.png)'
- en: The observation space
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察空间
- en: 'The observation space is composed of vectors of size 336:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间由大小为336的向量组成：
- en: 11 ray-casts forward distributed over 120 degrees (264 state dimensions)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11个向前的射线分布在120度上（264个状态维度）
- en: 3 ray-casts backward distributed over 90 degrees (72 state dimensions)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3个向后的射线分布在90度上（72个状态维度）
- en: 'Both of these ray-casts can detect 6 objects:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个射线都可以检测到6个对象：
- en: Ball
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 球
- en: Blue Goal
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色目标
- en: Purple Goal
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紫色目标
- en: Wall
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 墙
- en: Blue Agent
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝色代理
- en: Purple Agent
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紫色代理
- en: The action space
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行动空间
- en: 'The action space is three discrete branches:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间有三个离散分支：
- en: '![SoccerTwos Action](../Images/8afca8f83c98b99eae38b8072b97e32c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![SoccerTwos Action](../Images/8afca8f83c98b99eae38b8072b97e32c.png)'
- en: 'Step 2: Understand MA-POCA'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：了解MA-POCA
- en: 'We know how to train agents to play against others: **we can use self-play.**
    This is a perfect technique for a 1vs1.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何训练代理与其他人对战：**我们可以使用自我对弈。**这是一个完美的技术，适用于1对1。
- en: But in our case we’re 2vs2, and each team has 2 agents. How then can we **train
    cooperative behavior for groups of agents?**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们的情况下，我们是2对2，每个团队有2个代理。那么我们如何**训练代理组的合作行为呢？**
- en: As explained in the [Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors),
    agents typically receive a reward as a group (+1 - penalty) when the team scores
    a goal. This implies that **every agent on the team is rewarded even if each agent
    didn’t contribute the same to the win**, which makes it difficult to learn what
    to do independently.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如[Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors)中所解释的，当团队进球时，代理通常作为一个团体收到奖励（+1
    - 罚分）。这意味着**即使每个代理没有对胜利做出相同的贡献，团队中的每个代理都会受到奖励**，这使得独立学习变得困难。
- en: The Unity MLAgents team developed the solution in a new multi-agent trainer
    called *MA-POCA (Multi-Agent POsthumous Credit Assignment)*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Unity MLAgents团队开发了一个新的多代理训练器解决方案，称为*MA-POCA（多代理后期信用分配）*。
- en: 'The idea is simple but powerful: a centralized critic **processes the states
    of all agents in the team to estimate how well each agent is doing**. Think of
    this critic as a coach.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法简单但强大：一个集中的评论者**处理团队中所有代理的状态，以估计每个代理的表现**。把这个评论者想象成一个教练。
- en: This allows each agent to **make decisions based only on what it perceives locally**,
    and **simultaneously evaluate how good its behavior is in the context of the whole
    group**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得每个代理**只基于其本地感知做出决策**，并**同时评估其行为在整个团队环境中的表现**。
- en: '![MA POCA](../Images/7ad7814daeea1597757fe72e8d0ff01a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![MA POCA](../Images/7ad7814daeea1597757fe72e8d0ff01a.png)'
- en: 'This illustrates MA-POCA’s centralized learning and decentralized execution.
    Source: [MLAgents Plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了 MA-POCA 的集中式学习和分散式执行。来源：[MLAgents Plays Dodgeball](https://blog.unity.com/technology/ml-agents-plays-dodgeball)
- en: The solution then is to use Self-Play with an MA-POCA trainer (called poca).
    The poca trainer will help us to train cooperative behavior and self-play to win
    against an opponent team.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是使用自我对弈和 MA-POCA 训练器（称为 poca）。poca 训练器将帮助我们训练合作行为和自我对弈以赢得对手团队。
- en: If you want to dive deeper into this MA-POCA algorithm, you need to read the
    paper they published [here](https://arxiv.org/pdf/2111.05992.pdf) and the sources
    we put on the additional readings section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想深入了解这个 MA-POCA 算法，您需要阅读他们发表的论文[这里](https://arxiv.org/pdf/2111.05992.pdf)以及我们放在附加阅读部分的来源。
- en: 'Step 3: Define the config file'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步：定义配置文件
- en: We already learned in [Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction)
    that in ML-Agents, you define **the training hyperparameters in `config.yaml`
    files.**
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction)中学到，在
    ML-Agents 中，您可以在 `config.yaml` 文件中定义**训练超参数。**
- en: There are multiple hyperparameters. To understand them better, you should read
    the explanations for each of them in **[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个超参数。要更好地理解它们，您应该阅读**[文档](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**中每个超参数的解释。
- en: 'The config file we’re going to use here is in `./config/poca/SoccerTwos.yaml`.
    It looks like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用的配置文件在 `./config/poca/SoccerTwos.yaml` 中。它看起来像这样：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Compared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play
    part. How you modify them can be critical in getting good results.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pyramids 或 SnowballTarget 相比，我们有一个包含自我对弈部分的新超参数。您如何修改它们可能对获得良好结果至关重要。
- en: The advice I can give you here is to check the explanation and recommended value
    for each parameters (especially self-play ones) against **[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md).**
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以给您的建议是检查每个参数的解释和推荐值（特别是自我对弈参数）与**[文档](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**进行对比。
- en: Now that you’ve modified our config file, you’re ready to train your agents.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经修改了我们的配置文件，准备好训练您的代理了。
- en: 'Step 4: Start the training'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 步：开始训练
- en: To train the agents, we need to **launch mlagents-learn and select the executable
    containing the environment.**
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代理，我们需要**启动 mlagents-learn 并选择包含环境的可执行文件。**
- en: 'We define four parameters:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了四个参数：
- en: '`mlagents-learn <config>`: the path where the hyperparameter config file is.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mlagents-learn <config>`：超参数配置文件所在路径。'
- en: '`-env`: where the environment executable is.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-env`：环境可执行文件所在位置。'
- en: '`-run_id`: the name you want to give to your training run id.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-run_id`：您要为训练运行 ID 指定的名称。'
- en: '`-no-graphics`: to not launch the visualization during the training.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-no-graphics`：在训练期间不启动可视化。'
- en: Depending on your hardware, 5M timesteps (the recommended value, but you can
    also try 10M) will take 5 to 8 hours of training. You can continue using your
    computer in the meantime, but I advise deactivating the computer standby mode
    to prevent the training from being stopped.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的硬件，500 万个时间步（推荐值，但也可以尝试 1000 万）需要 5 到 8 小时的训练时间。在此期间，您可以继续使用计算机，但我建议停用计算机待机模式以防止训练被停止。
- en: Depending on the executable you use (windows, ubuntu, mac) the training command
    will look like this (your executable path can be different so don’t hesitate to
    check before running).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的可执行文件（windows、ubuntu、mac），训练命令将如下所示（您的可执行文件路径可能不同，因此在运行之前请务必检查）。
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The executable contains 8 copies of SoccerTwos.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 可执行文件包含 8 个 SoccerTwos 的副本。
- en: ⚠️ It’s normal if you don’t see a big increase of ELO score (and even a decrease
    below 1200) before 2M timesteps, since your agents will spend most of their time
    moving randomly on the field before being able to goal.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 如果在 200 万个时间步之前看不到 ELO 分数的大幅增加（甚至在 1200 以下下降）是正常的，因为您的代理将在能够进球之前大部分时间随机移动在场地上。
- en: ⚠️ You can stop the training with Ctrl + C but beware of typing this command
    only once to stop the training since MLAgents needs to generate a final .onnx
    file before closing the run.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 您可以使用 Ctrl + C 停止训练，但要注意只键入此命令一次以停止训练，因为 MLAgents 需要在关闭运行之前生成最终的 .onnx 文件。
- en: 'Step 5: Push the agent to the Hugging Face Hub'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步：将代理推送到 Hugging Face Hub
- en: Now that we trained our agents, we’re **ready to push them to the Hub to be
    able to participate in the AI vs. AI challenge and visualize them playing on your
    browser🔥.**
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的代理，我们**准备将它们推送到 Hub，以便参加 AI 对抗赛并在浏览器上观看它们的比赛🔥。**
- en: 'To be able to share your model with the community, there are three more steps
    to follow:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够与社区共享您的模型，还有三个步骤要遵循：
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣（如果尚未完成）创建一个 HF 帐户 ➡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2️⃣ Sign in and store your authentication token from the Hugging Face website.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 登录并存储来自 Hugging Face 网站的身份验证令牌。
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)) **with
    write role**
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新令牌（[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)）**具有写入权限**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![创建 HF 令牌](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: Copy the token, run this, and paste the token
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 复制令牌，运行此命令，然后粘贴令牌
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, we need to run `mlagents-push-to-hf`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要运行 `mlagents-push-to-hf`。
- en: 'And we define four parameters:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了四个参数：
- en: '`-run-id`: the name of the training run id.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-run-id`：训练运行ID的名称。'
- en: '`-local-dir`: where the agent was saved, it’s results/<run_id name>, so in
    my case results/First Training.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-local-dir`：代理保存的位置，是results/<run_id名称>，所以在我的情况下是results/First Training。'
- en: '`-repo-id`: the name of the Hugging Face repo you want to create or update.
    It’s always <your huggingface username>/<the repo name> If the repo does not exist
    **it will be created automatically**'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`-repo-id`：您要创建或更新的Hugging Face repo的名称。它始终是<您的Hugging Face用户名>/<repo名称>如果仓库不存在**将自动创建**'
- en: '`--commit-message`: since HF repos are git repositories you need to give a
    commit message.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`--commit-message`：由于HF仓库是git存储库，您需要提供提交消息。'
- en: In my case
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If everything worked you should see this at the end of the process (but with
    a different url 😆) :'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，您应该在过程结束时看到这个（但网址不同😆）：
- en: 'Your model is pushed to the Hub. You can view your model here: [https://huggingface.co/ThomasSimonini/poca-SoccerTwos](https://huggingface.co/ThomasSimonini/poca-SoccerTwos)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型已推送到Hub。您可以在此查看您的模型：[https://huggingface.co/ThomasSimonini/poca-SoccerTwos](https://huggingface.co/ThomasSimonini/poca-SoccerTwos)
- en: It’s the link to your model. It contains a model card that explains how to use
    it, your Tensorboard, and your config file. **What’s awesome is that it’s a git
    repository, which means you can have different commits, update your repository
    with a new push, etc.**
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您模型的链接。它包含一个解释如何使用它的模型卡片，您的Tensorboard和配置文件。**令人惊奇的是，它是一个git存储库，这意味着您可以有不同的提交，使用新的推送更新您的存储库等。**
- en: 'Step 6: Verify that your model is ready for AI vs AI Challenge'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步：验证您的模型是否准备好参加AI vs AI挑战
- en: Now that your model is pushed to the Hub, **it’s going to be added automatically
    to the AI vs AI Challenge model pool.** It can take a little bit of time before
    your model is added to the leaderboard given we do a run of matches every 4h.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的模型已推送到Hub，**它将自动添加到AI vs AI挑战模型池中。**在您的模型被添加到排行榜之前可能需要一点时间，因为我们每4小时进行一次比赛。
- en: 'But to ensure that everything works perfectly you need to check:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了确保一切都能完美运行，您需要检查：
- en: 'That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag
    we use to select models to be added to the challenge pool. To do that go to your
    model and check the tags'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的模型中有这个标签：ML-Agents-SoccerTwos。这是我们用来选择要添加到挑战池中的模型的标签。要做到这一点，请转到您的模型并检查标签
- en: '![Verify](../Images/f293f2ed5fa03cd6a2cd77086f6200a5.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![验证](../Images/f293f2ed5fa03cd6a2cd77086f6200a5.png)'
- en: If it’s not the case you just need to modify the readme and add it
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是这种情况，您只需要修改自述文件并添加它
- en: '![Verify](../Images/9cec0da6fa3195c208f23e9300898d7e.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![验证](../Images/9cec0da6fa3195c208f23e9300898d7e.png)'
- en: That you have a `SoccerTwos.onnx` file
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您有一个`SoccerTwos.onnx`文件
- en: '![Verify](../Images/99478ac1cd0c73aef674a98bc5457f48.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![验证](../Images/99478ac1cd0c73aef674a98bc5457f48.png)'
- en: We strongly suggest that you create a new model when you push to the Hub if
    you want to train it again or train a new version.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议您在推送到Hub时创建一个新模型，如果您想再次训练它或训练一个新版本。
- en: 'Step 7: Visualize some match in our demo'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：在我们的演示中可视化一些匹配
- en: 'Now that your model is part of AI vs AI Challenge, **you can visualize how
    good it is compared to others**: [https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的模型是AI vs AI挑战的一部分，**您可以可视化它与其他模型的优劣**：[https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)
- en: 'In order to do that, you just need to go to this demo:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，您只需要转到此演示：
- en: Select your model as team blue (or team purple if you prefer) and another model
    to compete against. The best opponents to compare your model to are either whoever
    is on top of the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择您的模型作为蓝队（或者如果您喜欢，作为紫队），并选择另一个模型进行竞争。与您的模型进行比较的最佳对手要么是排行榜上的第一名，要么是[基准模型](https://huggingface.co/unity/MLAgents-SoccerTwos)
- en: The matches you see live are not used in the calculation of your result **but
    they are a good way to visualize how good your agent is**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您实时看到的比赛不会用于计算您的结果**但它们是可视化您的代理有多好的好方法**。
- en: 'And don’t hesitate to share the best score your agent gets on discord in the
    #rl-i-made-this channel 🔥'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 并且请不要犹豫在discord的#rl-i-made-this频道中分享您的代理获得的最佳分数🔥
