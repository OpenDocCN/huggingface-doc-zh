# 介绍

> 原文链接：[https://huggingface.co/learn/deep-rl-course/unit6/introduction](https://huggingface.co/learn/deep-rl-course/unit6/introduction)

![缩略图](../Images/9f49f2880784ef300d40b68768960852.png)

在第4单元，我们学习了我们的第一个基于策略的算法叫做 **Reinforce**。

在基于策略的方法中，**我们旨在直接优化策略而不使用价值函数**。更准确地说，Reinforce是*基于策略方法* 中的一个子类，称为*策略梯度方法*。这个子类通过**使用梯度上升来估计最优策略的权重** 直接优化策略。

我们看到Reinforce效果很好。然而，因为我们使用蒙特卡洛采样来估计回报（我们使用整个情节来计算回报），**我们在策略梯度估计中有显著的方差**。

记住，策略梯度估计是 **回报增加最陡峭的方向**。换句话说，如何更新我们的策略权重，使导致良好回报的行动有更高的被采取的概率。蒙特卡洛方差，我们将在本单元进一步研究，**导致训练速度较慢，因为我们需要大量样本来减轻它**。

所以今天我们将学习 **演员评论家方法**，这是一种混合架构，结合了基于价值和基于策略的方法，通过使用以下方法来减少方差，从而稳定训练：

+   *一个演员* 控制 **我们的代理如何行为**（基于策略的方法）

+   *一个评论家* 评估 **采取的行动有多好**（基于价值的方法）

我们将研究其中一种混合方法，优势演员评论家（A2C），**并在机器人环境中使用Stable-Baselines3训练我们的代理**。我们将训练：

+   一个机械臂 🦾 移动到正确的位置。

听起来很激动人心吗？让我们开始吧！
