# 术语表

> 原始文本：[`huggingface.co/learn/deep-rl-course/unit1/glossary`](https://huggingface.co/learn/deep-rl-course/unit1/glossary)

这是一个由社区创建的术语表。欢迎贡献！

### 代理

代理通过与周围环境的奖励和惩罚进行试错学习**做出决策**。

### 环境

环境是一个模拟的世界**代理可以通过与之互动来学习**。

### 马尔可夫性质

这意味着我们的代理所采取的行动**仅取决于当前状态，独立于过去的状态和行动**。

### 观察/状态

+   **状态**：世界状态的完整描述。

+   **观察**：环境/世界状态的部分描述。

### 行动

+   **离散行动**：有限数量的行动，比如左、右、上、下。

+   **连续行动**：行动的无限可能性；例如，在自动驾驶汽车的情况下，驾驶场景有无限可能发生的行动。

### 奖励和折扣

+   **奖励**：RL 中的基本因素。告诉代理行动是好还是坏。

+   RL 算法专注于最大化**累积奖励**。

+   **奖励假设**：RL 问题可以被制定为（累积）回报的最大化。

+   **折扣**是因为在开始时获得的奖励更有可能发生，因为它们比长期奖励更可预测。

### 任务

+   **情节性**：有一个起点和一个终点。

+   **连续**：有一个起点但没有终点。

### 探索与利用的权衡

+   **探索**：这完全是关于通过尝试随机行动来探索环境，并从环境中获得反馈/回报/奖励。

+   **利用**：这是关于利用我们对环境的了解来获得最大奖励。

+   **探索-利用权衡**：它平衡了我们想要**探索**环境和我们想要**利用**对环境的了解的程度。

### 策略

+   **策略**：它被称为代理的大脑。它告诉我们在给定状态下应该采取什么行动。

+   **最优策略**：当代理根据它行动时**最大化**了**预期回报**的策略。它是通过*训练*学习的。

### 基于策略的方法：

+   解决 RL 问题的一种方法。

+   在这种方法中，策略是直接学习的。

+   将每个状态映射到该状态的最佳对应行动。或者在该状态上可能行动集合上的概率分布。

### 基于价值的方法：

+   解决 RL 问题的另一种方法。

+   在这里，我们不是训练一个策略，而是训练一个**价值函数**，将每个状态映射到在该状态下的预期价值。

欢迎贡献 🤗

如果你想改进课程，你可以[发起一个拉取请求。](https://github.com/huggingface/deep-rl-class/pulls)

这个术语表得以实现，感谢：

+   [@lucifermorningstar1305](https://github.com/lucifermorningstar1305)

+   [@daspartho](https://github.com/daspartho)

+   [@misza222](https://github.com/misza222)
