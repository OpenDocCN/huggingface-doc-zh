# 预分词器

> 原文：[https://huggingface.co/docs/tokenizers/api/pre-tokenizers](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)

PythonRustNode

## BertPreTokenizer

### `class tokenizers.pre_tokenizers.BertPreTokenizer`

```py
( )
```

BertPreTokenizer

此预分词器在空格上分割标记，也在标点上分割。每个标点字符的出现将被单独处理。

## ByteLevel

### `class tokenizers.pre_tokenizers.ByteLevel`

```py
( add_prefix_space = True use_regex = True )
```

参数

+   `add_prefix_space` (`bool`, *可选*, 默认为 `True`) — 如果第一个单词前没有空格，则是否添加一个空格。这样我们可以将 *hello* 和 *say hello* 完全相同对待。

+   `use_regex` (`bool`, *可选*, 默认为 `True`) — 将此 *pre_tokenizer* 设置为 `False`，以防止其使用 GPT2 特定的正则表达式在空格上分割。

ByteLevel 预分词器

此预分词器负责用相应表示替换给定字符串的所有字节，以及分割成单词。

#### `alphabet`

```py
( ) → List[str]
```

返回

`List[str]`

由字母表组成的字符列表

返回此 PreTokenizer 使用的字母表。

由于 ByteLevel 的工作方式如其名称所示，在字节级别上，它将每个字节值编码为唯一的可见字符。这意味着总共有 256 个不同的字符组成此字母表。

## CharDelimiterSplit

### `class tokenizers.pre_tokenizers.CharDelimiterSplit`

```py
( )
```

此预分词器简单地在提供的字符上分割。类似于 `.split(delimiter)`

## 数字

### `class tokenizers.pre_tokenizers.Digits`

```py
( individual_digits = False )
```

参数

+   `individual_digits` (`bool`, *可选*, 默认为 `False`) —

此预分词器简单地使用数字在单独的标记中分割

如果设置为 True，数字将按以下方式分隔：

```py
"Call 123 please" -> "Call ", "1", "2", "3", " please"
```

如果设置为 False，数字将按以下方式分组：

```py
"Call 123 please" -> "Call ", "123", " please"
```

## Metaspace

### `class tokenizers.pre_tokenizers.Metaspace`

```py
( replacement = '_' add_prefix_space = True )
```

参数

+   `replacement` (`str`, *可选*, 默认为 `▁`) — 替换字符。必须正好是一个字符。默认情况下，我们使用 *▁* (U+2581) 元符号（与 SentencePiece 中相同）。

+   `add_prefix_space` (`bool`, *可选*, 默认为 `True`) — 如果第一个单词前没有空格，则是否添加一个空格。这样我们可以将 *hello* 和 *say hello* 完全相同对待。

Metaspace 预分词器

此预分词器将任何空格替换为提供的替换字符。然后尝试在这些空格上分割。

## PreTokenizer

### `class tokenizers.pre_tokenizers.PreTokenizer`

```py
( )
```

所有预分词器的基类

不应直接实例化此类。相反，任何 PreTokenizer 的实现在实例化时将返回此类的实例。

#### `pre_tokenize`

```py
( pretok )
```

参数

+   `pretok` (`~tokenizers.PreTokenizedString) -- 要应用此 :class:`~tokenizers.pre_tokenizers.PreTokenizer` 的预分词字符串

在原地对 `~tokenizers.PyPreTokenizedString` 进行预分词

此方法允许修改 `PreTokenizedString` 以跟踪预分词，并利用 `PreTokenizedString` 的功能。如果只想查看原始字符串的预分词结果，可以使用 `pre_tokenize_str()`

#### `pre_tokenize_str`

```py
( sequence ) → List[Tuple[str, Offsets]]
```

参数

+   `sequence` (`str`) — 要预分词的字符串

返回

`List[Tuple[str, Offsets]]`

包含预分词部分及其偏移量的元组列表

对给定字符串进行预分词

此方法提供了一种可视化 [PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer) 效果的方式，但它不会跟踪对齐，也不提供 `PreTokenizedString` 的所有功能。如果需要其中一些功能，可以使用 `pre_tokenize()`

## 标点

### `class tokenizers.pre_tokenizers.Punctuation`

```py
( behavior = 'isolated' )
```

参数

+   `behavior` (`SplitDelimiterBehavior`) — 分割时要使用的行为。选择: “removed”, “isolated” (默认), “merged_with_previous”, “merged_with_next”, “contiguous”

此预分词器简单地在标点上分割为单独的字符。

## Sequence

### `class tokenizers.pre_tokenizers.Sequence`

```py
( pretokenizers )
```

这个预分词器组合其他预分词器，并按顺序应用它们

## 分割

### `class tokenizers.pre_tokenizers.Split`

```py
( pattern behavior invert = False )
```

参数

+   `pattern` (`str` 或 `Regex`) — 用于分割字符串的模式。通常是一个字符串或使用*tokenizers.Regex*构建的正则表达式

+   `behavior` (`SplitDelimiterBehavior`) — 分割时要使用的行为。选择：“removed”, “isolated”, “merged_with_previous”, “merged_with_next”, “contiguous”

+   `invert` (`bool`, *可选*，默认为`False`) — 是否反转模式。

分割预分词器

这个多功能的预分词器使用提供的模式进行分割，并根据提供的行为进行操作。通过使用反转标志，可以反转模式。

## UnicodeScripts

### `class tokenizers.pre_tokenizers.UnicodeScripts`

```py
( )
```

这个预分词器根据属于不同语言家族的字符进行分割，它大致遵循[https://github.com/google/sentencepiece/blob/master/data/Scripts.txt](https://github.com/google/sentencepiece/blob/master/data/Scripts.txt)实际上，平假名和片假名与汉字融合在一起，0x30FC也是汉字。这模仿了SentencePiece Unigram的实现。

## Whitespace

### `class tokenizers.pre_tokenizers.Whitespace`

```py
( )
```

这个预分词器简单地使用以下正则表达式进行分割：`\w+|[^\w\s]+`

## WhitespaceSplit

### `class tokenizers.pre_tokenizers.WhitespaceSplit`

```py
( )
```

这个预分词器简单地在空格上分割。工作方式类似于`.split()`
