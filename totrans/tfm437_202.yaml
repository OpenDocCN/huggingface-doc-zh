- en: mLUKE
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mLUKE
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mluke](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mluke)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mluke](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mluke)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The mLUKE model was proposed in [mLUKE: The Power of Entity Representations
    in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151)
    by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It’s a multilingual extension
    of the [LUKE model](https://arxiv.org/abs/2010.01057) trained on the basis of
    XLM-RoBERTa.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'mLUKE模型是由Ryokan Ri、Ikuya Yamada和Yoshimasa Tsuruoka在[《mLUKE: 多语言预训练语言模型中实体表示的力量》](https://arxiv.org/abs/2110.08151)中提出的。它是基于XLM-RoBERTa训练的[LUKE模型](https://arxiv.org/abs/2010.01057)的多语言扩展。'
- en: It is based on XLM-RoBERTa and adds entity embeddings, which helps improve performance
    on various downstream tasks involving reasoning about entities such as named entity
    recognition, extractive question answering, relation classification, cloze-style
    knowledge completion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于XLM-RoBERTa并添加了实体嵌入，有助于提高在涉及实体推理的各种下游任务上的性能，如命名实体识别、抽取式问答、关系分类、填空式知识补全。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Recent studies have shown that multilingual pretrained language models can
    be effectively improved with cross-lingual alignment information from Wikipedia
    entities. However, existing methods only exploit entity information in pretraining
    and do not explicitly use entities in downstream tasks. In this study, we explore
    the effectiveness of leveraging entity representations for downstream cross-lingual
    tasks. We train a multilingual language model with 24 languages with entity representations
    and show the model consistently outperforms word-based pretrained models in various
    cross-lingual transfer tasks. We also analyze the model and the key insight is
    that incorporating entity representations into the input allows us to extract
    more language-agnostic features. We also evaluate the model with a multilingual
    cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits
    correct factual knowledge more likely than using only word representations.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*最近的研究表明，多语言预训练语言模型可以通过来自维基百科实体的跨语言对齐信息有效改进。然而，现有方法仅在预训练中利用实体信息，并未明确在下游任务中使用实体。在这项研究中，我们探讨了利用实体表示进行下游跨语言任务的有效性。我们使用包含实体表示的24种语言训练了一个多语言语言模型，并展示该模型在各种跨语言转移任务中始终优于基于单词的预训练模型。我们还分析了模型，关键见解是将实体表示合并到输入中使我们能够提取更多与语言无关的特征。我们还使用mLAMA数据集对模型进行了多语言填空提示任务的评估。我们展示基于实体的提示比仅使用单词表示更有可能引出正确的事实知识。*'
- en: This model was contributed by [ryo0634](https://huggingface.co/ryo0634). The
    original code can be found [here](https://github.com/studio-ousia/luke).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[ryo0634](https://huggingface.co/ryo0634)贡献。原始代码可以在[这里](https://github.com/studio-ousia/luke)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: 'One can directly plug in the weights of mLUKE into a LUKE model, like so:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接将mLUKE的权重插入LUKE模型中，如下所示：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that mLUKE has its own tokenizer, [MLukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/mluke#transformers.MLukeTokenizer).
    You can initialize it as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，mLUKE有自己的分词器，[MLukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/mluke#transformers.MLukeTokenizer)。您可以按以下方式初始化它：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As mLUKE’s architecture is equivalent to that of LUKE, one can refer to [LUKE’s
    documentation page](luke) for all tips, code examples and notebooks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于mLUKE的架构等同于LUKE，因此可以参考[LUKE的文档页面](luke)获取所有提示、代码示例和笔记本。
- en: MLukeTokenizer
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLukeTokenizer
- en: '### `class transformers.MLukeTokenizer`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.MLukeTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L147)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L147)'
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇文件的路径。'
- en: '`entity_vocab_file` (`str`) — Path to the entity vocabulary file.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_vocab_file` (`str`) — 实体词汇文件的路径。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sequence
    token that was used during pretraining. Can be used a sequence classifier token.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — 在预训练期间使用的序列开头标记。可以用作序列分类器标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the beginning of sequence. The token used is the `cls_token`.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列开头的标记。用于开头的标记是`cls_token`。
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用特殊标记构建序列时，这不是用于序列结尾的标记。用于结尾的标记是`sep_token`。
- en: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"</s>"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问答。它也用作使用特殊标记构建的序列的最后一个标记。'
- en: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"<s>"`) — 用于进行序列分类（对整个序列进行分类而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时。'
- en: '`mask_token` (`str`, *optional*, defaults to `"<mask>"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *可选*, 默认为 `"<mask>"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`task` (`str`, *optional*) — Task for which you want to prepare sequences.
    One of `"entity_classification"`, `"entity_pair_classification"`, or `"entity_span_classification"`.
    If you specify this argument, the entity sequence is automatically created based
    on the given entity span(s).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` (`str`, *可选*) — 您要准备序列的任务。其中之一是 `"entity_classification"`、`"entity_pair_classification"`
    或 `"entity_span_classification"`。如果指定此参数，则将根据给定的实体跨度自动创建实体序列。'
- en: '`max_entity_length` (`int`, *optional*, defaults to 32) — The maximum length
    of `entity_ids`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_entity_length` (`int`, *可选*, 默认为 32) — `entity_ids` 的最大长度。'
- en: '`max_mention_length` (`int`, *optional*, defaults to 30) — The maximum number
    of tokens inside an entity span.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_mention_length` (`int`, *可选*, 默认为 30) — 实体跨度内的标记的最大数量。'
- en: '`entity_token_1` (`str`, *optional*, defaults to `<ent>`) — The special token
    used to represent an entity span in a word token sequence. This token is only
    used when `task` is set to `"entity_classification"` or `"entity_pair_classification"`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_1` (`str`, *可选*, 默认为 `<ent>`) — 用于表示单词标记序列中实体跨度的特殊标记。仅当 `task`
    设置为 `"entity_classification"` 或 `"entity_pair_classification"` 时才使用此标记。'
- en: '`entity_token_2` (`str`, *optional*, defaults to `<ent2>`) — The special token
    used to represent an entity span in a word token sequence. This token is only
    used when `task` is set to `"entity_pair_classification"`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_2` (`str`, *可选*, 默认为 `<ent2>`) — 用于表示单词标记序列中实体跨度的特殊标记。仅当 `task`
    设置为 `"entity_pair_classification"` 时才使用此标记。'
- en: '`additional_special_tokens` (`List[str]`, *optional*, defaults to `["<s>NOTUSED",
    "</s>NOTUSED"]`) — Additional special tokens used by the tokenizer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *可选*, 默认为 `["<s>NOTUSED", "</s>NOTUSED"]`)
    — 标记器使用的其他特殊标记。'
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model_kwargs` (`dict`, *可选*) — 将传递给 `SentencePieceProcessor.__init__()`
    方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：'
- en: '`enable_sampling`: Enable subword regularization.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enable_sampling`: 启用子词正则化。'
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size`: 用于unigram的抽样参数。对于BPE-Dropout无效。'
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size = {0,1}`: 不执行抽样。'
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size > 1`: 从 nbest_size 结果中进行抽样。'
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向抽样算法从所有假设（格）中进行抽样。'
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: unigram抽样的平滑参数，以及BPE-dropout合并操作的丢失概率。'
- en: '`sp_model` (`SentencePieceProcessor`) — The *SentencePiece* processor that
    is used for every conversion (string, tokens and IDs).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和ID）的*SentencePiece*处理器。'
- en: Adapted from [XLMRobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer)
    and [LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer).
    Based on [SentencePiece](https://github.com/google/sentencepiece).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 改编自[XLMRobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer)和[LukeTokenizer](/docs/transformers/v4.37.2/en/model_doc/luke#transformers.LukeTokenizer)。基于[SentencePiece](https://github.com/google/sentencepiece)。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `__call__`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L402)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L402)'
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch of sequences
    to be encoded. Each sequence must be a string. Note that this tokenizer does not
    support tokenization based on pretokenized strings.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或批次序列。每个序列必须是一个字符串。请注意，此标记器不支持基于预标记字符串的标记化。'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`) — The sequence or batch
    of sequences to be encoded. Each sequence must be a string. Note that this tokenizer
    does not support tokenization based on pretokenized strings.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`) — 要编码的序列或批次序列。每个序列必须是一个字符串。请注意，此标记器不支持基于预标记字符串的标记化。'
- en: '`entity_spans` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`, *optional*)
    — The sequence or batch of sequences of entity spans to be encoded. Each sequence
    consists of tuples each with two integers denoting character-based start and end
    positions of entities. If you specify `"entity_classification"` or `"entity_pair_classification"`
    as the `task` argument in the constructor, the length of each sequence must be
    1 or 2, respectively. If you specify `entities`, the length of each sequence must
    be equal to the length of each sequence of `entities`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_spans` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`, *optional*)
    — 要编码的实体跨度序列或批次。每个序列由元组组成，每个元组包含两个整数，表示实体的基于字符的起始和结束位置。如果在构造函数中将`task`参数指定为`"entity_classification"`或`"entity_pair_classification"`，则每个序列的长度必须分别为1或2。如果指定了`entities`，则每个序列的长度必须等于`entities`的每个序列的长度。'
- en: '`entity_spans_pair` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`,
    *optional*) — The sequence or batch of sequences of entity spans to be encoded.
    Each sequence consists of tuples each with two integers denoting character-based
    start and end positions of entities. If you specify the `task` argument in the
    constructor, this argument is ignored. If you specify `entities_pair`, the length
    of each sequence must be equal to the length of each sequence of `entities_pair`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_spans_pair` (`List[Tuple[int, int]]`, `List[List[Tuple[int, int]]]`,
    *optional*) — 要编码的实体跨度序列或批次。每个序列由元组组成，每个元组包含两个整数，表示实体的基于字符的起始和结束位置。如果在构造函数中指定了`task`参数，则忽略此参数。如果指定了`entities_pair`，则每个序列的长度必须等于`entities_pair`的每个序列的长度。'
- en: '`entities` (`List[str]`, `List[List[str]]`, *optional*) — The sequence or batch
    of sequences of entities to be encoded. Each sequence consists of strings representing
    entities, i.e., special entities (e.g., [MASK]) or entity titles of Wikipedia
    (e.g., Los Angeles). This argument is ignored if you specify the `task` argument
    in the constructor. The length of each sequence must be equal to the length of
    each sequence of `entity_spans`. If you specify `entity_spans` without specifying
    this argument, the entity sequence or the batch of entity sequences is automatically
    constructed by filling it with the [MASK] entity.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities` (`List[str]`, `List[List[str]]`, *optional*) — 要编码的实体序列或批次。每个序列由表示实体的字符串组成，即特殊实体（例如[MASK]）或维基百科的实体标题（例如洛杉矶）。如果在构造函数中指定了`task`参数，则忽略此参数。每个序列的长度必须等于`entity_spans`的每个序列的长度。如果未指定此参数而指定了`entity_spans`，则实体序列或实体序列批次将通过填充[MASK]实体自动构建。'
- en: '`entities_pair` (`List[str]`, `List[List[str]]`, *optional*) — The sequence
    or batch of sequences of entities to be encoded. Each sequence consists of strings
    representing entities, i.e., special entities (e.g., [MASK]) or entity titles
    of Wikipedia (e.g., Los Angeles). This argument is ignored if you specify the
    `task` argument in the constructor. The length of each sequence must be equal
    to the length of each sequence of `entity_spans_pair`. If you specify `entity_spans_pair`
    without specifying this argument, the entity sequence or the batch of entity sequences
    is automatically constructed by filling it with the [MASK] entity.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entities_pair` (`List[str]`, `List[List[str]]`, *optional*) — 要编码的实体序列或批次。每个序列由表示实体的字符串组成，即特殊实体（例如[MASK]）或维基百科的实体标题（例如洛杉矶）。如果在构造函数中指定了`task`参数，则忽略此参数。每个序列的长度必须等于`entity_spans_pair`的每个序列的长度。如果未指定此参数而指定了`entity_spans_pair`，则实体序列或实体序列批次将通过填充[MASK]实体自动构建。'
- en: '`max_entity_length` (`int`, *optional*) — The maximum length of `entity_ids`.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_entity_length` (`int`, *optional*) — `entity_ids`的最大长度。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将逐标记截断，如果提供了一对序列（或一批对），则从最长序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: 截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供一对序列（或一批对），则仅截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: 截断到由参数`max_length`指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供一对序列（或一批对），则仅截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或`''do_not_truncate''`（默认）：无截断（即，可以输出长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`stride` (`int`, *optional*, defaults to 0) — If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *optional*, 默认为0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义重叠标记的数量。'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *optional*, 默认为`False`) — 输入是否已经预分词（例如，分成单词）。如果设置为`True`，分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于
    NER 或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于启用
    NVIDIA 硬件上的 Tensor Cores 特别有用，计算能力`>= 7.5`（Volta）。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`return_token_type_ids` (`bool`, *optional*) — Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *optional*) — 是否返回标记类型 ID。如果保持默认设置，将根据特定分词器的默认设置返回标记类型
    ID，由`return_outputs`属性定义。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 ID？](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *optional*) — 是否返回注意力遮罩。如果保持默认设置，将根据特定分词器的默认设置返回注意力遮罩，由`return_outputs`属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力遮罩？](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *optional*, 默认为`False`) — 是否返回溢出的标记序列。如果提供一对输入
    ID 序列（或一批对），并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的标记。'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return special tokens mask information.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *optional*, 默认为`False`) — 是否返回特殊标记遮罩信息。'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *optional*, 默认为`False`) — 是否返回每个标记的`(char_start,
    char_end)`。'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python’s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用
    Python 的分词器，此方法将引发`NotImplementedError`。
- en: '`return_length` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *optional*, 默认为`False`) — 是否返回编码输入的长度。'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) — Whether or not to print
    more information and warnings. **kwargs — passed to the `self.tokenize()` method'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给 `self.tokenize()`
    方法'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含以下字段的 [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：
- en: '`input_ids` — List of token ids to be fed to a model.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 模型要输入的标记 id 列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 id？](../glossary#input-ids)'
- en: '`token_type_ids` — List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *“token_type_ids”* is in `self.model_input_names`).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 要输入模型的标记类型 id 列表（当 `return_token_type_ids=True` 或者 `self.model_input_names`
    中包含 *“token_type_ids”* 时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 id？](../glossary#token-type-ids)'
- en: '`attention_mask` — List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *“attention_mask”* is
    in `self.model_input_names`).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 指定哪些标记应该被模型关注的索引列表（当 `return_attention_mask=True` 或者 `self.model_input_names`
    中包含 *“attention_mask”* 时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`entity_ids` — List of entity ids to be fed to a model.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_ids` — 要输入模型的实体 id 列表。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 id？](../glossary#input-ids)'
- en: '`entity_position_ids` — List of entity positions in the input sequence to be
    fed to a model.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_position_ids` — 输入序列中实体的位置列表，要输入模型。'
- en: '`entity_token_type_ids` — List of entity token type ids to be fed to a model
    (when `return_token_type_ids=True` or if *“entity_token_type_ids”* is in `self.model_input_names`).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_token_type_ids` — 要输入模型的实体标记类型 id 列表（当 `return_token_type_ids=True`
    或者 `self.model_input_names` 中包含 *“entity_token_type_ids”* 时）。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型 id？](../glossary#token-type-ids)'
- en: '`entity_attention_mask` — List of indices specifying which entities should
    be attended to by the model (when `return_attention_mask=True` or if *“entity_attention_mask”*
    is in `self.model_input_names`).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_attention_mask` — 指定模型应该关注哪些实体的索引列表（当 `return_attention_mask=True`
    或者 `self.model_input_names` 中包含 *“entity_attention_mask”* 时）。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`entity_start_positions` — List of the start positions of entities in the word
    token sequence (when `task="entity_span_classification"`).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_start_positions` — 单词标记序列中实体的开始位置列表（当 `task="entity_span_classification"`
    时）。'
- en: '`entity_end_positions` — List of the end positions of entities in the word
    token sequence (when `task="entity_span_classification"`).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entity_end_positions` — 单词标记序列中实体的结束位置列表（当 `task="entity_span_classification"`
    时）。'
- en: '`overflowing_tokens` — List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` — 溢出标记序列的列表（当指定了 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`num_truncated_tokens` — Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` — 被截断的标记数量（当指定了 `max_length` 并且 `return_overflowing_tokens=True`
    时）。'
- en: '`special_tokens_mask` — List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` — 包含 0 和 1 的列表，其中 1 指定添加的特殊标记，0 指定常规序列标记（当 `add_special_tokens=True`
    和 `return_special_tokens_mask=True` 时）。'
- en: '`length` — The length of the inputs (when `return_length=True`)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` — 输入的长度（当 `return_length=True` 时）'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences, depending on the task you want to prepare
    them for.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对一个或多个序列或一个或多个序列对进行标记化和准备模型的主要方法，具体取决于您要为其准备的任务。
- en: '#### `save_vocabulary`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L1526)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/mluke/tokenization_mluke.py#L1526)'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
