["```py\npip install transformers datasets evaluate\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> from datasets import load_dataset\n\n>>> swag = load_dataset(\"swag\", \"regular\")\n```", "```py\n>>> swag[\"train\"][0]\n{'ending0': 'passes by walking down the street playing their instruments.',\n 'ending1': 'has heard approaching them.',\n 'ending2': \"arrives and they're outside dancing and asleep.\",\n 'ending3': 'turns the lead singer watches the performance.',\n 'fold-ind': '3416',\n 'gold-source': 'gold',\n 'label': 0,\n 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n 'sent2': 'A drum line',\n 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n 'video-id': 'anetv_jkn6uvmqwh4'}\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n```", "```py\n>>> ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n\n>>> def preprocess_function(examples):\n...     first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n...     question_headers = examples[\"sent2\"]\n...     second_sentences = [\n...         [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n...     ]\n\n...     first_sentences = sum(first_sentences, [])\n...     second_sentences = sum(second_sentences, [])\n\n...     tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n...     return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n```", "```py\ntokenized_swag = swag.map(preprocess_function, batched=True)\n```", "```py\n>>> from dataclasses import dataclass\n>>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n>>> from typing import Optional, Union\n>>> import torch\n\n>>> @dataclass\n... class DataCollatorForMultipleChoice:\n...     \"\"\"\n...     Data collator that will dynamically pad the inputs for multiple choice received.\n...     \"\"\"\n\n...     tokenizer: PreTrainedTokenizerBase\n...     padding: Union[bool, str, PaddingStrategy] = True\n...     max_length: Optional[int] = None\n...     pad_to_multiple_of: Optional[int] = None\n\n...     def __call__(self, features):\n...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n...         labels = [feature.pop(label_name) for feature in features]\n...         batch_size = len(features)\n...         num_choices = len(features[0][\"input_ids\"])\n...         flattened_features = [\n...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n...         ]\n...         flattened_features = sum(flattened_features, [])\n\n...         batch = self.tokenizer.pad(\n...             flattened_features,\n...             padding=self.padding,\n...             max_length=self.max_length,\n...             pad_to_multiple_of=self.pad_to_multiple_of,\n...             return_tensors=\"pt\",\n...         )\n\n...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n...         return batch\n```", "```py\n>>> from dataclasses import dataclass\n>>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n>>> from typing import Optional, Union\n>>> import tensorflow as tf\n\n>>> @dataclass\n... class DataCollatorForMultipleChoice:\n...     \"\"\"\n...     Data collator that will dynamically pad the inputs for multiple choice received.\n...     \"\"\"\n\n...     tokenizer: PreTrainedTokenizerBase\n...     padding: Union[bool, str, PaddingStrategy] = True\n...     max_length: Optional[int] = None\n...     pad_to_multiple_of: Optional[int] = None\n\n...     def __call__(self, features):\n...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n...         labels = [feature.pop(label_name) for feature in features]\n...         batch_size = len(features)\n...         num_choices = len(features[0][\"input_ids\"])\n...         flattened_features = [\n...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n...         ]\n...         flattened_features = sum(flattened_features, [])\n\n...         batch = self.tokenizer.pad(\n...             flattened_features,\n...             padding=self.padding,\n...             max_length=self.max_length,\n...             pad_to_multiple_of=self.pad_to_multiple_of,\n...             return_tensors=\"tf\",\n...         )\n\n...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n...         return batch\n```", "```py\n>>> import evaluate\n\n>>> accuracy = evaluate.load(\"accuracy\")\n```", "```py\n>>> import numpy as np\n\n>>> def compute_metrics(eval_pred):\n...     predictions, labels = eval_pred\n...     predictions = np.argmax(predictions, axis=1)\n...     return accuracy.compute(predictions=predictions, references=labels)\n```", "```py\n>>> from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n```", "```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_swag_model\",\n...     evaluation_strategy=\"epoch\",\n...     save_strategy=\"epoch\",\n...     load_best_model_at_end=True,\n...     learning_rate=5e-5,\n...     per_device_train_batch_size=16,\n...     per_device_eval_batch_size=16,\n...     num_train_epochs=3,\n...     weight_decay=0.01,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=tokenized_swag[\"train\"],\n...     eval_dataset=tokenized_swag[\"validation\"],\n...     tokenizer=tokenizer,\n...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 16\n>>> num_train_epochs = 2\n>>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n>>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n```", "```py\n>>> from transformers import TFAutoModelForMultipleChoice\n\n>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n```", "```py\n>>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n>>> tf_train_set = model.prepare_tf_dataset(\n...     tokenized_swag[\"train\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_validation_set = model.prepare_tf_dataset(\n...     tokenized_swag[\"validation\"],\n...     shuffle=False,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n```", "```py\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```", "```py\n>>> from transformers.keras_callbacks import KerasMetricCallback\n\n>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n```", "```py\n>>> from transformers.keras_callbacks import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"my_awesome_model\",\n...     tokenizer=tokenizer,\n... )\n```", "```py\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```", "```py\n>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n```", "```py\n>>> prompt = \"France has a bread law, Le D\u00e9cret Pain, with strict rules on what is allowed in a traditional baguette.\"\n>>> candidate1 = \"The law does not apply to croissants and brioche.\"\n>>> candidate2 = \"The law applies to baguettes.\"\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n>>> labels = torch.tensor(0).unsqueeze(0)\n```", "```py\n>>> from transformers import AutoModelForMultipleChoice\n\n>>> model = AutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n>>> logits = outputs.logits\n```", "```py\n>>> predicted_class = logits.argmax().item()\n>>> predicted_class\n'0'\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n>>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n```", "```py\n>>> from transformers import TFAutoModelForMultipleChoice\n\n>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n>>> outputs = model(inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n>>> predicted_class\n'0'\n```"]