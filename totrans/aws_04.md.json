["```py\nfrom transformers import TrainingArguments\n-from transformers import Trainer\n+from optimum.neuron import NeuronTrainer as Trainer\ntraining_args = TrainingArguments(\n  # training arguments...\n)\n# A lot of code here\n# Initialize our Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,  # Original training arguments.\n    train_dataset=train_dataset if training_args.do_train else None,\n    eval_dataset=eval_dataset if training_args.do_eval else None,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n```", "```py\ntorchrun --nproc_per_node=2 huggingface-neuron-samples/text-classification/run_glue.py \\\n--model_name_or_path bert-base-uncased \\\n--dataset_name philschmid/emotion \\\n--do_train \\\n--do_eval \\\n--bf16 True \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir ./bert-emotion\n```", "```py\noptimum-cli export neuron \n  --model distilbert-base-uncased-finetuned-sst-2-english \\\n  --batch_size 1 \\\n  --sequence_length 32 \\\n  --auto_cast matmul \\\n  --auto_cast_type bf16 \\\n  distilbert_base_uncased_finetuned_sst2_english_neuron/\n```", "```py\nfrom transformers import AutoTokenizer\n-from transformers import AutoModelForSequenceClassification\n+from optimum.neuron import NeuronModelForSequenceClassification\n\n# PyTorch checkpoint\n-model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n+model = NeuronModelForSequenceClassification.from_pretrained(\"distilbert_base_uncased_finetuned_sst2_english_neuron\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\ninputs = tokenizer(\"Hamilton is considered to be the best musical of past years.\", return_tensors=\"pt\")\n\nlogits = model(**inputs).logits\nprint(model.config.id2label[logits.argmax().item()])\n# 'POSITIVE'\n```"]