["```py\n>>> from transformers import XCLIPTextModel, XCLIPTextConfig\n\n>>> # Initializing a XCLIPTextModel with microsoft/xclip-base-patch32 style configuration\n>>> configuration = XCLIPTextConfig()\n\n>>> # Initializing a XCLIPTextConfig from the microsoft/xclip-base-patch32 style configuration\n>>> model = XCLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import XCLIPVisionModel, XCLIPVisionConfig\n\n>>> # Initializing a XCLIPVisionModel with microsoft/xclip-base-patch32 style configuration\n>>> configuration = XCLIPVisionConfig()\n\n>>> # Initializing a XCLIPVisionModel model from the microsoft/xclip-base-patch32 style configuration\n>>> model = XCLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, AutoModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 8 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = processor(\n...     text=[\"playing sports\", \"eating spaghetti\", \"go shopping\"],\n...     videos=list(video),\n...     return_tensors=\"pt\",\n...     padding=True,\n... )\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n>>> probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities\n>>> print(probs)\ntensor([[1.9496e-04, 9.9960e-01, 2.0825e-04]])\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, AutoModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 8 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = processor(videos=list(video), return_tensors=\"pt\")\n\n>>> video_features = model.get_video_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, XCLIPTextModel\n\n>>> model = XCLIPTextModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, XCLIPVisionModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 16 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = XCLIPVisionModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n\n>>> batch_size, num_frames, num_channels, height, width = pixel_values.shape\n>>> pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n\n>>> outputs = model(pixel_values)\n>>> last_hidden_state = outputs.last_hidden_state\n```"]