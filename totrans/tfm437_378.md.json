["```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( text_config = None vision_config = None projection_dim = 512 prompt_layers = 2 prompt_alpha = 0.1 prompt_hidden_act = 'quick_gelu' prompt_num_attention_heads = 8 prompt_attention_dropout = 0.0 prompt_projection_dropout = 0.0 logit_scale_init_value = 2.6592 **kwargs )\n```", "```py\n( text_config: XCLIPTextConfig vision_config: XCLIPVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';XCLIPConfig\n```", "```py\n( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import XCLIPTextModel, XCLIPTextConfig\n\n>>> # Initializing a XCLIPTextModel with microsoft/xclip-base-patch32 style configuration\n>>> configuration = XCLIPTextConfig()\n\n>>> # Initializing a XCLIPTextConfig from the microsoft/xclip-base-patch32 style configuration\n>>> model = XCLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 mit_hidden_size = 512 mit_intermediate_size = 2048 mit_num_hidden_layers = 1 mit_num_attention_heads = 8 num_channels = 3 image_size = 224 patch_size = 32 num_frames = 8 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 drop_path_rate = 0.0 **kwargs )\n```", "```py\n>>> from transformers import XCLIPVisionModel, XCLIPVisionConfig\n\n>>> # Initializing a XCLIPVisionModel with microsoft/xclip-base-patch32 style configuration\n>>> configuration = XCLIPVisionConfig()\n\n>>> # Initializing a XCLIPVisionModel model from the microsoft/xclip-base-patch32 style configuration\n>>> model = XCLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: XCLIPConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.x_clip.modeling_x_clip.XCLIPOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, AutoModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 8 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = processor(\n...     text=[\"playing sports\", \"eating spaghetti\", \"go shopping\"],\n...     videos=list(video),\n...     return_tensors=\"pt\",\n...     padding=True,\n... )\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n>>> probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities\n>>> print(probs)\ntensor([[1.9496e-04, 9.9960e-01, 2.0825e-04]])\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';video_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, AutoModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 8 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = processor(videos=list(video), return_tensors=\"pt\")\n\n>>> video_features = model.get_video_features(**inputs)\n```", "```py\n( config: XCLIPTextConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, XCLIPTextModel\n\n>>> model = XCLIPTextModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: XCLIPVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> import av\n>>> import torch\n>>> import numpy as np\n\n>>> from transformers import AutoProcessor, XCLIPVisionModel\n>>> from huggingface_hub import hf_hub_download\n\n>>> np.random.seed(0)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample 16 frames\n>>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n>>> video = read_video_pyav(container, indices)\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n>>> model = XCLIPVisionModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n>>> pixel_values = processor(videos=list(video), return_tensors=\"pt\").pixel_values\n\n>>> batch_size, num_frames, num_channels, height, width = pixel_values.shape\n>>> pixel_values = pixel_values.reshape(-1, num_channels, height, width)\n\n>>> outputs = model(pixel_values)\n>>> last_hidden_state = outputs.last_hidden_state\n```"]