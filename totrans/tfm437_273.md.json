["```py\n( backbone_config: Optional = None feature_size: int = 256 mask_feature_size: int = 256 hidden_dim: int = 256 encoder_feedforward_dim: int = 1024 activation_function: str = 'relu' encoder_layers: int = 6 decoder_layers: int = 10 num_attention_heads: int = 8 dropout: float = 0.0 dim_feedforward: int = 2048 pre_norm: bool = False enforce_input_projection: bool = False common_stride: int = 4 ignore_value: int = 255 num_queries: int = 100 no_object_weight: float = 0.1 class_weight: float = 2.0 mask_weight: float = 5.0 dice_weight: float = 5.0 train_num_points: int = 12544 oversample_ratio: float = 3.0 importance_sample_ratio: float = 0.75 init_std: float = 0.02 init_xavier_std: float = 1.0 use_auxiliary_loss: bool = True feature_strides: List = [4, 8, 16, 32] output_auxiliary_logits: bool = None **kwargs )\n```", "```py\n>>> from transformers import Mask2FormerConfig, Mask2FormerModel\n\n>>> # Initializing a Mask2Former facebook/mask2former-swin-small-coco-instance configuration\n>>> configuration = Mask2FormerConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/mask2former-swin-small-coco-instance style configuration\n>>> model = Mask2FormerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( backbone_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';Mask2FormerConfig\n```", "```py\n( encoder_last_hidden_state: FloatTensor = None pixel_decoder_last_hidden_state: FloatTensor = None transformer_decoder_last_hidden_state: FloatTensor = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_intermediate_states: Tuple = None masks_queries_logits: Tuple = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_logits: Optional = None encoder_last_hidden_state: FloatTensor = None pixel_decoder_last_hidden_state: FloatTensor = None transformer_decoder_last_hidden_state: FloatTensor = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( config: Mask2FormerConfig )\n```", "```py\n( pixel_values: Tensor pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.mask2former.modeling_mask2former.Mask2FormerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoImageProcessor, Mask2FormerModel\n\n>>> # load image\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # load image preprocessor and Mask2FormerModel trained on COCO instance segmentation dataset\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n>>> model = Mask2FormerModel.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # model outputs last hidden states of shape (batch_size, num_queries, hidden_size)\n>>> print(outputs.transformer_decoder_last_hidden_state.shape)\ntorch.Size([1, 100, 256])\n```", "```py\n( config: Mask2FormerConfig )\n```", "```py\n( pixel_values: Tensor mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_hidden_states: Optional = None output_auxiliary_logits: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.mask2former.modeling_mask2former.Mask2FormerForUniversalSegmentationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n>>> from PIL import Image\n>>> import requests\n>>> import torch\n\n>>> # Load Mask2Former trained on COCO instance segmentation dataset\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-coco-instance\")\n>>> model = Mask2FormerForUniversalSegmentation.from_pretrained(\n...     \"facebook/mask2former-swin-small-coco-instance\"\n... )\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # Perform post-processing to get instance segmentation map\n>>> pred_instance_map = image_processor.post_process_semantic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0]\n>>> print(pred_instance_map.shape)\ntorch.Size([480, 640])\n```", "```py\n>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n>>> from PIL import Image\n>>> import requests\n>>> import torch\n\n>>> # Load Mask2Former trained on ADE20k semantic segmentation dataset\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\n>>> model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-ade-semantic\")\n\n>>> url = (\n...     \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n... )\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # Perform post-processing to get semantic segmentation map\n>>> pred_semantic_map = image_processor.post_process_semantic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0]\n>>> print(pred_semantic_map.shape)\ntorch.Size([512, 683])\n```", "```py\n>>> from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n>>> from PIL import Image\n>>> import requests\n>>> import torch\n\n>>> # Load Mask2Former trained on CityScapes panoptic segmentation dataset\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-panoptic\")\n>>> model = Mask2FormerForUniversalSegmentation.from_pretrained(\n...     \"facebook/mask2former-swin-small-cityscapes-panoptic\"\n... )\n\n>>> url = \"https://cdn-media.huggingface.co/Inference-API/Sample-results-on-the-Cityscapes-dataset-The-above-images-show-how-our-method-can-handle.png\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> # Model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # Perform post-processing to get panoptic segmentation map\n>>> pred_panoptic_map = image_processor.post_process_panoptic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0][\"segmentation\"]\n>>> print(pred_panoptic_map.shape)\ntorch.Size([338, 676])\n```", "```py\n( do_resize: bool = True size: Dict = None size_divisor: int = 32 resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None reduce_labels: bool = False **kwargs )\n```", "```py\n( images: Union segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( pixel_values_list: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( outputs target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False return_binary_maps: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```"]