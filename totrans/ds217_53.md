# 缓存

> 原始文本：[https://huggingface.co/docs/datasets/about_cache](https://huggingface.co/docs/datasets/about_cache)

缓存是🤗 Datasets 如此高效的原因之一。它存储先前下载和处理过的数据集，因此当您需要再次使用它们时，它们会直接从缓存中重新加载。这避免了需要重新下载数据集，或重新应用处理函数。即使您关闭并启动另一个 Python 会话，🤗 Datasets 也会直接从缓存中重新加载您的数据集！

## 指纹

缓存如何跟踪应用于数据集的转换？嗯，🤗 Datasets 为缓存文件分配一个指纹。指纹跟踪数据集的当前状态。初始指纹是使用 Arrow 表的哈希值计算的，或者如果数据集在磁盘上，则使用 Arrow 文件的哈希值。后续指纹通过组合先前状态的指纹和最新应用的转换的哈希值来计算。

转换是来自[处理方法](./process)指南的任何处理方法，例如[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)或[Dataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)。

这是实际指纹的样子：

```py
>>> from datasets import Dataset
>>> dataset1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})
>>> print(dataset1._fingerprint, dataset2._fingerprint)
d19493523d95e2dc 5b86abacd4b42434
```

为了使转换可哈希，它需要通过[dill](https://dill.readthedocs.io/en/latest/)或[pickle](https://docs.python.org/3/library/pickle)进行 picklable。

当您使用不可哈希的转换时，🤗 Datasets 会使用一个随机指纹，并发出警告。不可哈希的转换被认为与先前的转换不同。因此，🤗 Datasets 将重新计算所有转换。确保您的转换可以使用 pickle 或 dill 进行序列化，以避免这种情况！

当缓存被禁用时，🤗 Datasets 重新计算所有内容的一个例子是。当这种情况发生时，缓存文件会每次生成，并写入临时目录。一旦您的 Python 会话结束，临时目录中的缓存文件将被删除。这些缓存文件被分配一个随机哈希，而不是指纹。

当缓存被禁用时，使用[Dataset.save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)来保存您的转换数据集，否则会在会话结束时被删除。

## 哈希

数据集的指纹通过对传递给 `map` 的函数以及 `map` 参数（`batch_size`、`remove_columns`等）进行哈希计算来更新。

您可以使用[fingerprint.Hasher](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.fingerprint.Hasher)检查任何 Python 对象的哈希值：

```py
>>> from datasets.fingerprint import Hasher
>>> my_func = lambda example: {"length": len(example["text"])}
>>> print(Hasher.hash(my_func))
'3d35e2b3e94c81d6'
```

哈希是通过使用 `dill` pickler 转储对象并对转储的字节进行哈希计算来计算的。pickler 递归地转储您函数中使用的所有变量，因此您对函数中使用的对象进行的任何更改都将导致哈希值的更改。

如果您的某个函数在不同会话中似乎没有相同的哈希值，这意味着至少其中一个变量包含一个不确定的 Python 对象。当这种情况发生时，请随时对您发现可疑的任何对象进行哈希以尝试找到导致哈希值更改的对象。例如，如果您使用一个列表，其元素的顺序在不同会话中是不确定的，那么哈希值在不同会话中也不会相同。
