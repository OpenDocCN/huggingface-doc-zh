# 为您的任务选择指标

> 原文链接：[https://huggingface.co/docs/evaluate/choosing_a_metric](https://huggingface.co/docs/evaluate/choosing_a_metric)

**所以您已经训练好了模型，想要看看它在您选择的数据集上表现如何。从哪里开始？**

选择评估指标没有“一刀切”的方法，但一些好的指导原则要记住的是：

## 指标类别

有3个高级别的指标类别：

1.  *通用指标*，可应用于各种情况和数据集，如precision和accuracy。

1.  *任务特定的指标*，限于特定任务，如机器翻译（通常使用[BLEU](https://huggingface.co/metrics/bleu)或[ROUGE](https://huggingface.co/metrics/rouge)等指标进行评估）或命名实体识别（通常使用[seqeval](https://huggingface.co/metrics/seqeval)进行评估）。

1.  *数据集特定的指标*，旨在衡量模型在特定基准上的性能：例如，[GLUE基准](https://huggingface.co/datasets/glue)有专门的[评估指标](https://huggingface.co/metrics/glue)。

让我们看看这三种情况：

### 通用指标

在机器学习社区中使用的许多指标都相当通用，可应用于各种任务和数据集。

这适用于像[accuracy](https://huggingface.co/metrics/accuracy)和[precision](https://huggingface.co/metrics/precision)这样的指标，可用于评估标记（监督）数据集，以及[perplexity](https://huggingface.co/metrics/perplexity)这样的指标，可用于评估不同类型的（无监督）生成任务。

查看给定指标的输入结构，您可以查看其指标卡。例如，在[precision](https://huggingface.co/metrics/precision)的情况下，格式如下：

```py
>>> precision_metric = evaluate.load("precision")
>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'precision': 1.0}
```

### 任务特定的指标

像机器翻译和命名实体识别这样的流行ML任务有特定的指标，可用于比较模型。例如，针对文本生成提出了一系列不同的指标，从[BLEU](https://huggingface.co/metrics/bleu)及其衍生指标如[GoogleBLEU](https://huggingface.co/metrics/google_bleu)和[GLEU](https://huggingface.co/metrics/gleu)，还有[ROUGE](https://huggingface.co/metrics/rouge)、[MAUVE](https://huggingface.co/metrics/mauve)等。

您可以通过以下方式找到适合您任务的正确指标：

+   **查看[任务页面](https://huggingface.co/tasks)**，看看可以用于评估给定任务模型的指标。

+   **查看像[Papers With Code](https://paperswithcode.com/)这样的网站上的排行榜**（您可以按任务和数据集搜索）。

+   **阅读相关指标的指标卡**，看看哪些适合您的用例。例如，查看[BLEU指标卡](https://github.com/huggingface/evaluate/tree/main/metrics/bleu)或[SQuaD指标卡](https://github.com/huggingface/evaluate/tree/main/metrics/squad)。

+   **查看关于该主题发表的论文和博客文章**，看看它们报告了哪些指标。这可能会随时间变化，因此尽量选择最近几年的论文！

### 数据集特定的指标

一些数据集有与之相关的特定指标 - 尤其是在流行基准如[GLUE](https://huggingface.co/metrics/glue)和[SQuAD](https://huggingface.co/metrics/squad)的情况下。

💡 GLUE实际上是不同任务上不同子集的集合，因此首先您需要选择与NLI任务对应的子集，比如mnli，它被描述为“带有文本蕴涵注释的众包句对集合”

如果您正在评估您的模型在类似上面提到的基准数据集上的表现，您可以使用其专用的评估指标。确保您遵守它们要求的格式。例如，要在[SQuAD](https://huggingface.co/datasets/squad)数据集上评估您的模型，您需要将`question`和`context`输入到您的模型中，并返回`prediction_text`，然后将其与`references`（基于匹配问题的`id`）进行比较：

```py
>>> from evaluate import load
>>> squad_metric = load("squad")
>>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
>>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
>>> results = squad_metric.compute(predictions=predictions, references=references)
>>> results
{'exact_match': 100.0, 'f1': 100.0}
```

您可以通过查看“数据集预览”功能或给定数据集的数据集卡来找到数据集结构的示例，并可以根据度量卡来看如何使用其专用的评估函数。
