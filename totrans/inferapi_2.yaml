- en: ðŸ¤— Hosted Inference API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/api-inference/index](https://huggingface.co/docs/api-inference/index)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/api-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/entry/start.e0f2a481.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/scheduler.b8dd6794.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/singletons.b4b7c713.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/index.4e4eb7ec.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/paths.c627adaa.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/entry/app.c06d8cf1.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/index.1dbdfbc6.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/nodes/0.d33efbda.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/nodes/4.e5e8227f.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/Tip.e82b13f9.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/Heading.e9c64afe.js">
  prefs: []
  type: TYPE_NORMAL
- en: Test and evaluate, for free, over 150,000 publicly accessible machine learning
    models, or your own private models, via simple HTTP requests, with fast inference
    hosted on Hugging Face shared infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The Inference API is free to use, and rate limited. If you need an inference
    solution for production, check out our [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)
    service. With Inference Endpoints, you can easily deploy any machine learning
    model on dedicated and fully managed infrastructure. Select the cloud, region,
    compute instance, autoscaling range and security level to match your model, latency,
    throughput, and compliance needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Main features:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Get predictions from **150,000+ Transformers, Diffusers, or Timm models** (T5,
    Blenderbot, Bart, GPT-2, Pegasus...)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use built-in integrations with **over 20 Open-Source libraries** (spaCy, SpeechBrain,
    Keras, etc).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Switch from one model to the next by just switching the model ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload, manage and serve your **own models privately**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run Classification, Image Segmentation, Automatic Speech Recognition, NER, Conversational,
    Summarization, Translation, Question-Answering, Embeddings Extraction tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the box accelerated inference on **CPU** powered by Intel Xeon Ice Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third-party library models:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [Hub](https://huggingface.co) supports many new libraries, such as SpaCy,
    Timm, Keras, fastai, and more. You can read the full list [here](https://hf.co/docs/hub/libraries).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those models are enabled on the API thanks to some docker integration [api-inference-community](https://github.com/huggingface/api-inference-community/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please note however, that these models will not allow you ([tracking issue](https://github.com/huggingface/huggingface_hub/issues/85)):'
  prefs: []
  type: TYPE_NORMAL
- en: To get full optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run private models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get access to GPU inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are looking for custom support from the Hugging Face team
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![HuggingFace Expert Acceleration Program](../Images/4e3f8848a914f36cc180b9e654070ef1.png)](https://huggingface.co/support)'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face is trusted in production by over 10,000 companies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/6ae7702af262288012014841128c4a68.png) ![](../Images/224eaf3c23f0fc3f8f157c596cd4b70c.png)'
  prefs: []
  type: TYPE_IMG
