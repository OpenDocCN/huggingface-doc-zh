["```py\napt install python-opengl\napt install ffmpeg\napt install xvfb\npip install pyglet==1.5\npip install pyvirtualdisplay\n```", "```py\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```", "```py\npip install gym==0.22\npip install imageio-ffmpeg\npip install huggingface_hub\npip install gym[box2d]==0.22\n```", "```py\nfrom IPython.display import HTML\n\nHTML(\n    '<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n)\n```", "```py\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\n\nmsg = Printer()\n```", "```py\n# Adding HuggingFace argument\nparser.add_argument(\n    \"--repo-id\",\n    type=str,\n    default=\"ThomasSimonini/ppo-CartPole-v1\",\n    help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\",\n)\n```", "```py\ndef package_to_hub( repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None, ):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # Step 3: Evaluate the model and build JSON\n        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n        # First get datetime\n        eval_datetime = datetime.datetime.now()\n        eval_form_datetime = eval_datetime.isoformat()\n\n        evaluate_data = {\n            \"env_id\": hyperparameters.env_id,\n            \"mean_reward\": mean_reward,\n            \"std_reward\": std_reward,\n            \"n_evaluation_episodes\": 10,\n            \"eval_datetime\": eval_form_datetime,\n        }\n\n        # Write a JSON file\n        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n            json.dump(evaluate_data, outfile)\n\n        # Step 4: Generate a video\n        video_path = tmpdirname / \"replay.mp4\"\n        record_video(eval_env, model, video_path, video_fps)\n\n        # Step 5: Generate the model card\n        generated_model_card, metadata = _generate_model_card(\n            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n        )\n        _save_model_card(tmpdirname, generated_model_card, metadata)\n\n        # Step 6: Add logs if needed\n        if logs:\n            _add_logdir(tmpdirname, Path(logs))\n\n        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return repo_url\n\ndef _evaluate_agent(env, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        while done is False:\n            state = torch.Tensor(state).to(device)\n            action, _, _, _ = policy.get_action_and_value(state)\n            new_state, reward, done, info = env.step(action.cpu().numpy())\n            total_rewards_ep += reward\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n\ndef record_video(env, policy, out_directory, fps=30):\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        state = torch.Tensor(state).to(device)\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _, _, _ = policy.get_action_and_value(state)\n        state, reward, done, info = env.step(\n            action.cpu().numpy()\n        )  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n    \"\"\"\n    Generate the model card for the Hub\n    :param model_name: name of the model\n    :env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    :hyperparameters: training arguments\n    \"\"\"\n    # Step 1: Select the tags\n    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n    # Transform the hyperparams namespace to string\n    converted_dict = vars(hyperparameters)\n    converted_str = str(converted_dict)\n    converted_str = converted_str.split(\", \")\n    converted_str = \"\\n\".join(converted_str)\n\n    # Step 2: Generate the model card\n    model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  \"\"\"\n    return model_card, metadata\n\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n    \"\"\"\n    Define the tags for the model card\n    :param model_name: name of the model\n    :param env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    \"\"\"\n    metadata = {}\n    metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\",\n    ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=model_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_id,\n        dataset_id=env_id,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    return metadata\n\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\ndef _add_logdir(local_path: Path, logdir: Path):\n    \"\"\"Adds a logdir to the repository.\n    :param local_path: repository directory\n    :param logdir: logdir directory\n    \"\"\"\n    if logdir.exists() and logdir.is_dir():\n        # Add the logdir to the repository under new dir called logs\n        repo_logdir = local_path / \"logs\"\n\n        # Delete current logs if they exist\n        if repo_logdir.exists():\n            shutil.rmtree(repo_logdir)\n\n        # Copy logdir into repo logdir\n        shutil.copytree(logdir, repo_logdir)\n```", "```py\n# Create the evaluation environment\neval_env = gym.make(args.env_id)\n\npackage_to_hub(\n    repo_id=args.repo_id,\n    model=agent,  # The model we want to save\n    hyperparameters=args,\n    eval_env=gym.make(args.env_id),\n    logs=f\"runs/{run_name}\",\n)\n```", "```py\n# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy\n\nimport argparse\nimport os\nimport random\nimport time\nfrom distutils.util import strtobool\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\n\nmsg = Printer()\n\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n        help=\"the name of this experiment\")\n    parser.add_argument(\"--seed\", type=int, default=1,\n        help=\"seed of the experiment\")\n    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, cuda will be enabled by default\")\n    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n        help=\"the wandb's project name\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n        help=\"the entity (team) of wandb's project\")\n    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n\n    # Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n        help=\"the id of the environment\")\n    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n        help=\"total timesteps of the experiments\")\n    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n        help=\"the learning rate of the optimizer\")\n    parser.add_argument(\"--num-envs\", type=int, default=4,\n        help=\"the number of parallel game environments\")\n    parser.add_argument(\"--num-steps\", type=int, default=128,\n        help=\"the number of steps to run in each environment per policy rollout\")\n    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggle learning rate annealing for policy and value networks\")\n    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Use GAE for advantage computation\")\n    parser.add_argument(\"--gamma\", type=float, default=0.99,\n        help=\"the discount factor gamma\")\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n        help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n        help=\"the number of mini-batches\")\n    parser.add_argument(\"--update-epochs\", type=int, default=4,\n        help=\"the K epochs to update the policy\")\n    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles advantages normalization\")\n    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n        help=\"the surrogate clipping coefficient\")\n    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n        help=\"coefficient of the entropy\")\n    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n        help=\"coefficient of the value function\")\n    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n        help=\"the maximum norm for the gradient clipping\")\n    parser.add_argument(\"--target-kl\", type=float, default=None,\n        help=\"the target KL divergence threshold\")\n\n    # Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args\n\ndef package_to_hub( repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None, ):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\"\n    msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n    # Step 1: Clone or create the repo\n    repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        tmpdirname = Path(tmpdirname)\n\n        # Step 2: Save the model\n        torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n        # Step 3: Evaluate the model and build JSON\n        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n        # First get datetime\n        eval_datetime = datetime.datetime.now()\n        eval_form_datetime = eval_datetime.isoformat()\n\n        evaluate_data = {\n            \"env_id\": hyperparameters.env_id,\n            \"mean_reward\": mean_reward,\n            \"std_reward\": std_reward,\n            \"n_evaluation_episodes\": 10,\n            \"eval_datetime\": eval_form_datetime,\n        }\n\n        # Write a JSON file\n        with open(tmpdirname / \"results.json\", \"w\") as outfile:\n            json.dump(evaluate_data, outfile)\n\n        # Step 4: Generate a video\n        video_path = tmpdirname / \"replay.mp4\"\n        record_video(eval_env, model, video_path, video_fps)\n\n        # Step 5: Generate the model card\n        generated_model_card, metadata = _generate_model_card(\n            \"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters\n        )\n        _save_model_card(tmpdirname, generated_model_card, metadata)\n\n        # Step 6: Add logs if needed\n        if logs:\n            _add_logdir(tmpdirname, Path(logs))\n\n        msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n        repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n        msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n    return repo_url\n\ndef _evaluate_agent(env, n_eval_episodes, policy):\n    \"\"\"\n    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n    :param env: The evaluation environment\n    :param n_eval_episodes: Number of episode to evaluate the agent\n    :param policy: The agent\n    \"\"\"\n    episode_rewards = []\n    for episode in range(n_eval_episodes):\n        state = env.reset()\n        step = 0\n        done = False\n        total_rewards_ep = 0\n\n        while done is False:\n            state = torch.Tensor(state).to(device)\n            action, _, _, _ = policy.get_action_and_value(state)\n            new_state, reward, done, info = env.step(action.cpu().numpy())\n            total_rewards_ep += reward\n            if done:\n                break\n            state = new_state\n        episode_rewards.append(total_rewards_ep)\n    mean_reward = np.mean(episode_rewards)\n    std_reward = np.std(episode_rewards)\n\n    return mean_reward, std_reward\n\ndef record_video(env, policy, out_directory, fps=30):\n    images = []\n    done = False\n    state = env.reset()\n    img = env.render(mode=\"rgb_array\")\n    images.append(img)\n    while not done:\n        state = torch.Tensor(state).to(device)\n        # Take the action (index) that have the maximum expected future reward given that state\n        action, _, _, _ = policy.get_action_and_value(state)\n        state, reward, done, info = env.step(\n            action.cpu().numpy()\n        )  # We directly put next_state = state for recording logic\n        img = env.render(mode=\"rgb_array\")\n        images.append(img)\n    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n    \"\"\"\n    Generate the model card for the Hub\n    :param model_name: name of the model\n    :env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    :hyperparameters: training arguments\n    \"\"\"\n    # Step 1: Select the tags\n    metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n    # Transform the hyperparams namespace to string\n    converted_dict = vars(hyperparameters)\n    converted_str = str(converted_dict)\n    converted_str = converted_str.split(\", \")\n    converted_str = \"\\n\".join(converted_str)\n\n    # Step 2: Generate the model card\n    model_card = f\"\"\"\n  # PPO Agent Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  \"\"\"\n    return model_card, metadata\n\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n    \"\"\"\n    Define the tags for the model card\n    :param model_name: name of the model\n    :param env_id: name of the environment\n    :mean_reward: mean reward of the agent\n    :std_reward: standard deviation of the mean reward of the agent\n    \"\"\"\n    metadata = {}\n    metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"custom-implementation\",\n        \"deep-rl-course\",\n    ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=model_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_id,\n        dataset_id=env_id,\n    )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    return metadata\n\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\ndef _add_logdir(local_path: Path, logdir: Path):\n    \"\"\"Adds a logdir to the repository.\n    :param local_path: repository directory\n    :param logdir: logdir directory\n    \"\"\"\n    if logdir.exists() and logdir.is_dir():\n        # Add the logdir to the repository under new dir called logs\n        repo_logdir = local_path / \"logs\"\n\n        # Delete current logs if they exist\n        if repo_logdir.exists():\n            shutil.rmtree(repo_logdir)\n\n        # Copy logdir into repo logdir\n        shutil.copytree(logdir, repo_logdir)\n\ndef make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk\n\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n\n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n    )\n\n    # TRY NOT TO MODIFY: seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n\n    # env setup\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    agent = Agent(envs).to(device)\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n\n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n\n    for update in range(1, num_updates + 1):\n        # Annealing the rate if instructed to do so.\n        if args.anneal_lr:\n            frac = 1.0 - (update - 1.0) / num_updates\n            lrnow = frac * args.learning_rate\n            optimizer.param_groups[0][\"lr\"] = lrnow\n\n        for step in range(0, args.num_steps):\n            global_step += 1 * args.num_envs\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n\n            for item in info:\n                if \"episode\" in item.keys():\n                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n\n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).reshape(1, -1)\n            if args.gae:\n                advantages = torch.zeros_like(rewards).to(device)\n                lastgaelam = 0\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        nextvalues = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        nextvalues = values[t + 1]\n                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n                returns = advantages + values\n            else:\n                returns = torch.zeros_like(rewards).to(device)\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        next_return = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        next_return = returns[t + 1]\n                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n                advantages = returns - values\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values.reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(\n                    b_obs[mb_inds], b_actions.long()[mb_inds]\n                )\n                logratio = newlogprob - b_logprobs[mb_inds]\n                ratio = logratio.exp()\n\n                with torch.no_grad():\n                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n                    old_approx_kl = (-logratio).mean()\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n\n                mb_advantages = b_advantages[mb_inds]\n                if args.norm_adv:\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n\n                # Policy loss\n                pg_loss1 = -mb_advantages * ratio\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n\n                # Value loss\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                    v_clipped = b_values[mb_inds] + torch.clamp(\n                        newvalue - b_values[mb_inds],\n                        -args.clip_coef,\n                        args.clip_coef,\n                    )\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                    v_loss = 0.5 * v_loss_max.mean()\n                else:\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n\n                entropy_loss = entropy.mean()\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n                optimizer.step()\n\n            if args.target_kl is not None:\n                if approx_kl > args.target_kl:\n                    break\n\n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n\n    # Create the evaluation environment\n    eval_env = gym.make(args.env_id)\n\n    package_to_hub(\n        repo_id=args.repo_id,\n        model=agent,  # The model we want to save\n        hyperparameters=args,\n        eval_env=gym.make(args.env_id),\n        logs=f\"runs/{run_name}\",\n    )\n```", "```py\nfrom huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store\n```", "```py\n!python ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"YOUR_REPO_ID\" --total-timesteps=50000\n```"]