- en: What is RL? A short recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl](https://huggingface.co/learn/deep-rl-course/unit2/what-is-rl)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: In RL, we build an agent that can **make smart decisions**. For instance, an
    agent that **learns to play a video game.** Or a trading agent that **learns to
    maximize its benefits** by deciding on **what stocks to buy and when to sell.**
  prefs: []
  type: TYPE_NORMAL
- en: '![RL process](../Images/75d6b22440b9c5d7d5b653bfb590c8e0.png)'
  prefs: []
  type: TYPE_IMG
- en: To make intelligent decisions, our agent will learn from the environment by **interacting
    with it through trial and error** and receiving rewards (positive or negative) **as
    unique feedback.**
  prefs: []
  type: TYPE_NORMAL
- en: Its goal **is to maximize its expected cumulative reward** (because of the reward
    hypothesis).
  prefs: []
  type: TYPE_NORMAL
- en: '**The agent’s decision-making process is called the policy π:** given a state,
    a policy will output an action or a probability distribution over actions. That
    is, given an observation of the environment, a policy will provide an action (or
    multiple probabilities for each action) that the agent should take.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy](../Images/eb29b1cbd14496d16a36efc7ba1b5298.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Our goal is to find an optimal policy π*** , aka., a policy that leads to
    the best expected cumulative reward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And to find this optimal policy (hence solving the RL problem), there **are
    two main types of RL methods**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Policy-based methods*: **Train the policy directly** to learn which action
    to take given a state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Value-based methods*: **Train a value function** to learn **which state is
    more valuable** and use this value function **to take the action that leads to
    it.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Two RL approaches](../Images/ea1be9a83e2be724fd67093267c9958b.png)'
  prefs: []
  type: TYPE_IMG
- en: And in this unit, **we’ll dive deeper into the value-based methods.**
  prefs: []
  type: TYPE_NORMAL
