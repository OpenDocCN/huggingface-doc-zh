- en: Model training anatomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy](https://huggingface.co/docs/transformers/v4.37.2/en/model_memory_anatomy)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: To understand performance optimization techniques that one can apply to improve
    efficiency of model training speed and memory utilization, it’s helpful to get
    familiar with how GPU is utilized during training, and how compute intensity varies
    depending on an operation performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by exploring a motivating example of GPU utilization and the training
    run of a model. For the demonstration, we’ll need to install a few libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `nvidia-ml-py3` library allows us to monitor the memory usage of the models
    from within Python. You might be familiar with the `nvidia-smi` command in the
    terminal - this library allows to access the same information in Python directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create some dummy data: random token IDs between 100 and 30000 and
    binary labels for a classifier. In total, we get 512 sequences each with length
    512 and store them in a [Dataset](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset)
    with PyTorch format.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To print summary statistics for the GPU utilization and the training run with
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    we define two helper functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that we start with a free GPU memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks good: the GPU memory is not occupied as we would expect before we
    load any models. If that’s not the case on your machine make sure to stop all
    processes that are using GPU memory. However, not all free GPU memory can be used
    by the user. When a model is loaded to the GPU the kernels are also loaded, which
    can take up 1-2GB of memory. To see how much it is we load a tiny tensor into
    the GPU which triggers the kernels to be loaded as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We see that the kernels alone take up 1.3GB of GPU memory. Now let’s see how
    much space the model uses.
  prefs: []
  type: TYPE_NORMAL
- en: Load Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we load the `bert-large-uncased` model. We load the model weights directly
    to the GPU so that we can check how much space just the weights use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the model weights alone take up 1.3 GB of GPU memory. The exact
    number depends on the specific GPU you are using. Note that on newer GPUs a model
    can sometimes take up more space since the weights are loaded in an optimized
    fashion that speeds up the usage of the model. Now we can also quickly check if
    we get the same result as with `nvidia-smi` CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the same number as before and you can also see that we are using a V100
    GPU with 16GB of memory. So now we can start training the model and see how the
    GPU memory consumption changes. First, we set up a few standard training arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you plan to run multiple experiments, in order to properly clear the memory
    between experiments, restart the Python kernel between experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Memory utilization at vanilla training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    and train the model without using any GPU performance optimization techniques
    and a batch size of 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We see that already a relatively small batch size almost fills up our GPU’s
    entire memory. However, a larger batch size can often result in faster model convergence
    or better end performance. So ideally we want to tune the batch size to our model’s
    needs and not to the GPU limitations. What’s interesting is that we use much more
    memory than the size of the model. To understand a bit better why this is the
    case let’s have a look at a model’s operations and memory needs.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of Model’s Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers architecture includes 3 main groups of operations grouped below
    by compute-intensity.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tensor Contractions**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear layers and components of Multi-Head Attention all do batched **matrix-matrix
    multiplications**. These operations are the most compute-intensive part of training
    a transformer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Statistical Normalizations**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax and layer normalization are less compute-intensive than tensor contractions,
    and involve one or more **reduction operations**, the result of which is then
    applied via a map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Element-wise Operators**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These are the remaining operators: **biases, dropout, activations, and residual
    connections**. These are the least compute-intensive operations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This knowledge can be helpful to know when analyzing performance bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This summary is derived from [Data Movement Is All You Need: A Case Study on
    Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)'
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of Model’s Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve seen that training the model uses much more memory than just putting
    the model on the GPU. This is because there are many components during training
    that use GPU memory. The components on GPU memory are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: model weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: optimizer states
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: gradients
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: forward activations saved for gradient computation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: temporary buffers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: functionality-specific memory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A typical model trained in mixed precision with AdamW requires 18 bytes per
    model parameter plus activation memory. For inference there are no optimizer states
    and gradients, so we can subtract those. And thus we end up with 6 bytes per model
    parameter for mixed precision inference, plus activation memory.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the details.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Weights:**'
  prefs: []
  type: TYPE_NORMAL
- en: 4 bytes * number of parameters for fp32 training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6 bytes * number of parameters for mixed precision training (maintains a model
    in fp32 and one in fp16 in memory)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer States:**'
  prefs: []
  type: TYPE_NORMAL
- en: 8 bytes * number of parameters for normal AdamW (maintains 2 states)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 bytes * number of parameters for 8-bit AdamW optimizers like [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4 bytes * number of parameters for optimizers like SGD with momentum (maintains
    only 1 state)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradients**'
  prefs: []
  type: TYPE_NORMAL
- en: 4 bytes * number of parameters for either fp32 or mixed precision training (gradients
    are always kept in fp32)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forward Activations**'
  prefs: []
  type: TYPE_NORMAL
- en: size depends on many factors, the key ones being sequence length, hidden size
    and batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are the input and output that are being passed and returned by the forward
    and the backward functions and the forward activations saved for gradient computation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporary Memory**'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are all kinds of temporary variables which get released
    once the calculation is done, but in the moment these could require additional
    memory and could push to OOM. Therefore, when coding it’s crucial to think strategically
    about such temporary variables and sometimes to explicitly free those as soon
    as they are no longer needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Functionality-specific memory**'
  prefs: []
  type: TYPE_NORMAL
- en: Then, your software could have special memory needs. For example, when generating
    text using beam search, the software needs to maintain multiple copies of inputs
    and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**`forward` vs `backward` Execution Speed**'
  prefs: []
  type: TYPE_NORMAL
- en: For convolutions and linear layers there are 2x flops in the backward compared
    to the forward, which generally translates into ~2x slower (sometimes more, because
    sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited,
    and it’s typical for an activation to have to read more data in the backward than
    in the forward (e.g. activation forward reads once, writes once, activation backward
    reads twice, gradOutput and output of the forward, and writes once, gradInput).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are potentially a few places where we could save GPU memory
    or speed up operations. Now that you understand what affects GPU utilization and
    computation speed, refer to the [Methods and tools for efficient training on a
    single GPU](perf_train_gpu_one) documentation page to learn about performance
    optimization techniques.
  prefs: []
  type: TYPE_NORMAL
