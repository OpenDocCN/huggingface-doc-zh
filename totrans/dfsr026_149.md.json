["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: LCMScheduler safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 4 original_inference_steps: int = None timesteps: List = None guidance_scale: float = 8.5 num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> from diffusers import DiffusionPipeline\n>>> import torch\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n>>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n>>> pipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\n>>> prompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n\n>>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\n>>> num_inference_steps = 4\n>>> images = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0).images\n>>> images[0].save(\"image.png\")\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( w embedding_dim = 512 dtype = torch.float32 ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: LCMScheduler safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None image: Union = None num_inference_steps: int = 4 strength: float = 0.8 original_inference_steps: int = None timesteps: List = None guidance_scale: float = 8.5 num_images_per_prompt: Optional = 1 generator: Union = None latents: Optional = None prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> from diffusers import AutoPipelineForImage2Image\n>>> import torch\n>>> import PIL\n\n>>> pipe = AutoPipelineForImage2Image.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n>>> # To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n>>> pipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\n\n>>> prompt = \"High altitude snowy mountains\"\n>>> image = PIL.Image.open(\"./snowy_mountains.png\")\n\n>>> # Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\n>>> num_inference_steps = 4\n>>> images = pipe(\n...     prompt=prompt, image=image, num_inference_steps=num_inference_steps, guidance_scale=8.0\n... ).images\n\n>>> images[0].save(\"image.png\")\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( w embedding_dim = 512 dtype = torch.float32 ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]