- en: Mid-way Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap](https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/29.acf386da.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into Q-Learning, let’s summarize what we’ve just learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two types of value-based functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'State-value function: outputs the expected return if **the agent starts at
    a given state and acts according to the policy forever after.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action-value function: outputs the expected return if **the agent starts in
    a given state, takes a given action at that state** and then acts accordingly
    to the policy forever after.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In value-based methods, rather than learning the policy, **we define the policy
    by hand** and we learn a value function. If we have an optimal value function,
    we **will have an optimal policy.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two types of methods to update the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: With *the Monte Carlo method*, we update the value function from a complete
    episode, and so we **use the actual discounted return of this episode.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With *the TD Learning method,* we update the value function from a step, replacing
    the unknown<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​ with **an
    estimated return called the TD target.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Summary](../Images/2147e50332ca45537a5c052179c783d8.png)'
  prefs: []
  type: TYPE_IMG
