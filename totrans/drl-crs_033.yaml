- en: Mid-way Recap
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中途总结
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap](https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap](https://huggingface.co/learn/deep-rl-course/unit2/mid-way-recap)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into Q-Learning, let’s summarize what we’ve just learned.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入学习Q学习之前，让我们总结一下我们刚刚学到的内容。
- en: 'We have two types of value-based functions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种基于价值的函数：
- en: 'State-value function: outputs the expected return if **the agent starts at
    a given state and acts according to the policy forever after.**'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态值函数：输出期望的回报，如果**代理从给定状态开始，并根据策略永远行动。**
- en: 'Action-value function: outputs the expected return if **the agent starts in
    a given state, takes a given action at that state** and then acts accordingly
    to the policy forever after.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作值函数：输出期望的回报，如果**代理从给定状态开始，采取该状态下的给定动作，然后根据策略永远行动。**
- en: In value-based methods, rather than learning the policy, **we define the policy
    by hand** and we learn a value function. If we have an optimal value function,
    we **will have an optimal policy.**
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，我们不是学习策略，而是**手动定义策略**，并学习价值函数。如果我们有一个最优价值函数，我们**将有一个最优策略。**
- en: 'There are two types of methods to update the value function:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法来更新价值函数：
- en: With *the Monte Carlo method*, we update the value function from a complete
    episode, and so we **use the actual discounted return of this episode.**
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过*蒙特卡洛方法*，我们从完整的一集中更新价值函数，因此我们**使用该一集的实际折扣回报。**
- en: With *the TD Learning method,* we update the value function from a step, replacing
    the unknown<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​ with **an
    estimated return called the TD target.**
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过*TD学习方法*，我们从一步中更新价值函数，用估计的回报替换未知的<math><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub></mrow><annotation
    encoding="application/x-tex">G_t</annotation></semantics></math>Gt​，称为TD目标。
- en: '![Summary](../Images/2147e50332ca45537a5c052179c783d8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![总结](../Images/2147e50332ca45537a5c052179c783d8.png)'
