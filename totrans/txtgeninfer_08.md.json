["```py\ncurl 127.0.0.1:8080/generate \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}' \\\n    -H 'Content-Type: application/json'\n```", "```py\npip install huggingface-hub\n```", "```py\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(model=\"http://127.0.0.1:8080\")\nclient.text_generation(prompt=\"Write a code for snake game\")\n```", "```py\nfor token in client.text_generation(\"How do you make cheese?\", max_new_tokens=12, stream=True):\n    print(token)\n```", "```py\noutput = client.text_generation(prompt=\"Meaning of life is\", details=True)\nprint(output)\n\n# TextGenerationResponse(generated_text=' a complex concept that is not always clear to the individual. It is a concept that is not always', details=Details(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=20, seed=None, prefill=[], tokens=[Token(id=267, text=' a', logprob=-2.0723474, special=False), Token(id=11235, text=' complex', logprob=-3.1272552, special=False), Token(id=17908, text=' concept', logprob=-1.3632495, special=False),..))\n```", "```py\noutput = client.text_generation(prompt=\"Meaning of life is\", stream=True, details=True)\nprint(next(iter(output)))\n\n# TextGenerationStreamResponse(token=Token(id=267, text=' a', logprob=-2.0723474, special=False), generated_text=None, details=None)\n```", "```py\n{\n// rest of the model config here\n\"endpoints\": [{\"url\": \"https://HOST:PORT/generate_stream\"}]\n}\n```", "```py\npip install huggingface-hub gradio\n```", "```py\nimport gradio as gr\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(model=\"http://127.0.0.1:8080\")\n\ndef inference(message, history):\n    partial_message = \"\"\n    for token in client.text_generation(message, max_new_tokens=20, stream=True):\n        partial_message += token\n        yield partial_message\n\ngr.ChatInterface(\n    inference,\n    chatbot=gr.Chatbot(height=300),\n    textbox=gr.Textbox(placeholder=\"Chat with me!\", container=False, scale=7),\n    description=\"This is the demo for Gradio UI consuming TGI endpoint with LLaMA 7B-Chat model.\",\n    title=\"Gradio \ud83e\udd1d TGI\",\n    examples=[\"Are tomatoes vegetables?\"],\n    retry_btn=\"Retry\",\n    undo_btn=\"Undo\",\n    clear_btn=\"Clear\",\n).queue().launch()\n```", "```py\ndef inference(message, history):\n    return client.text_generation(message, max_new_tokens=20)\n```"]