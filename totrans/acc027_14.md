# 处理推断的大模型

> 原文：[`huggingface.co/docs/accelerate/usage_guides/big_modeling`](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)

🤗 Accelerate 提供的最大的进步之一是大型模型推断的概念，您可以在无法完全适应您的显卡的模型上执行*推断*。

本教程将分为两部分，展示如何同时使用🤗 Accelerate 和🤗 Transformers（更高级别的 API）来利用这个想法。

## 使用🤗 Accelerate

对于这些教程，我们将假设一个典型的工作流程，用于加载您的模型，如下所示：

```py
import torch

my_model = ModelClass(...)
state_dict = torch.load(checkpoint_file)
my_model.load_state_dict(state_dict)
```

请注意，这里我们假设`ModelClass`是一个占用比设备（无论是`mps`还是`cuda`）上可容纳的视频内存更多的模型。

第一步是初始化一个模型的空骨架，使用 init_empty_weights()上下文管理器，这样不会占用任何 RAM：

```py
from accelerate import init_empty_weights
with init_empty_weights():
    my_model = ModelClass(...)
```

目前这个`my_model`是“无参数”的，因此比直接加载到 CPU 上留下了更小的印记。

接下来我们需要加载权重到我们的模型中，以便进行推断。

为此，我们将使用 load_checkpoint_and_dispatch()，如其名称所示，将在您的空模型内加载一个检查点，并将每个层的权重分配到您可用的所有设备（GPU/MPS 和 CPU RAM）上。

要确定如何执行这个`dispatch`，通常指定`device_map="auto"`就足够了，因为🤗 Accelerate 会尝试填满 GPU(s)上的所有空间，然后加载到 CPU，最后如果内存不够，它将被加载到磁盘（绝对最慢的选项）。

有关设计自己的设备映射的更多详细信息，请参阅概念指南的本节

看下面的示例：

```py
from accelerate import load_checkpoint_and_dispatch

model = load_checkpoint_and_dispatch(
    model, checkpoint=checkpoint_file, device_map="auto"
)
```

如果有某些“块”层不应该被分割，您可以将它们作为`no_split_module_classes`传递。了解更多信息，请点击这里

此外，为了节省内存（例如如果`state_dict`无法适应 RAM），模型的权重可以分割并拆分为多个检查点文件。了解更多信息，请点击这里

现在模型已经完全分发，您可以像正常情况下使用模型进行推断：

```py
input = torch.randn(2,3)
input = input.to("cuda")
output = model(input)
```

现在每次输入通过一个层时，它将从 CPU 发送到 GPU（或从磁盘到 CPU 到 GPU），计算输出，然后将该层从 GPU 拉回到线路下方。虽然这会增加执行推断的开销，但通过这种方法，可以在您的系统上运行**任何大小的模型**，只要最大的层能够适合在您的 GPU 上。

多个 GPU 可以被利用，但这被认为是“模型并行”，因此在任何给定时刻只有一个 GPU 会处于活动状态，等待前一个 GPU 发送输出。您应该正常启动您的脚本使用`python`，而不需要`torchrun`，`accelerate launch`等。

要查看此内容的可视化表示，请查看下面的动画：

[`www.youtube-nocookie.com/embed/MWCSGj9jEAo`](https://www.youtube-nocookie.com/embed/MWCSGj9jEAo)

### 完整示例

以下是完整示例，展示我们上面执行的操作：

```py
import torch
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

with init_empty_weights():
    model = MyModel(...)

model = load_checkpoint_and_dispatch(
    model, checkpoint=checkpoint_file, device_map="auto"
)

input = torch.randn(2,3)
input = input.to("cuda")
output = model(input)
```

## 使用🤗 Transformers，🤗 Diffusers 和其他🤗开源库

支持🤗 Accelerate 大型模型推断的库包括它们的`from_pretrained`构造函数中的所有先前逻辑。

通过指定一个表示要从[🤗 Hub](https://hf.co/models)下载的模型的字符串，然后使用`device_map="auto"`以及一些额外的参数来操作。

作为一个简短的例子，我们将看看如何使用`transformers`加载 Big Science 的 T0pp 模型。

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto")
```

加载模型后，之前准备模型的初始步骤都已完成，模型已完全准备好利用机器中的所有资源。通过这些构造函数，您还可以通过指定模型加载的精度来节省*更多*内存，通过`torch_dtype`参数，例如：

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0pp", device_map="auto", torch_dtype=torch.float16)
```

要了解更多信息，请查看[这里提供的🤗 Transformers 文档](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading)。

## 接下来该去哪里

对于更详细的大模型推断，请务必查看关于它的概念指南
