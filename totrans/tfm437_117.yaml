- en: Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/text_generation)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'Each framework has a generate method for text generation implemented in their
    respective `GenerationMixin` class:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    is implemented in [GenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)
    is implemented in [TFGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax/JAX [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate)
    is implemented in [FlaxGenerationMixin](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of your framework of choice, you can parameterize the generate method
    with a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    class instance. Please refer to this class for the complete list of generation
    parameters, which control the behavior of the generation method.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to inspect a model’s generation configuration, what are the defaults,
    how to change the parameters ad hoc, and how to create and save a customized generation
    configuration, refer to the [text generation strategies guide](../generation_strategies).
    The guide also explains how to use related features, like token streaming.
  prefs: []
  type: TYPE_NORMAL
- en: GenerationConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GenerationConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L40)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters that control the length of the output
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — The maximum length the generated
    tokens can have. Corresponds to the length of the input prompt + `max_new_tokens`.
    Its effect is overridden by `max_new_tokens`, if also set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_new_tokens` (`int`, *optional*) — The maximum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_length` (`int`, *optional*, defaults to 0) — The minimum length of the
    sequence to be generated. Corresponds to the length of the input prompt + `min_new_tokens`.
    Its effect is overridden by `min_new_tokens`, if also set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_new_tokens` (`int`, *optional*) — The minimum numbers of tokens to generate,
    ignoring the number of tokens in the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_time(float,` *optional*) — The maximum amount of time you allow the computation
    to run for in seconds. generation will still finish the current pass after allocated
    time has been passed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters that control the generation strategy used
  prefs: []
  type: TYPE_NORMAL
- en: '`do_sample` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    sampling ; use greedy decoding otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`penalty_alpha` (`float`, *optional*) — The values balance the model confidence
    and the degeneration penalty in contrastive search decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should use the past last key/values attentions (if applicable to the model) to
    speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters for manipulation of the model output logits
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature` (`float`, *optional*, defaults to 1.0) — The value used to modulate
    the next token probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 50) — The number of highest probability
    vocabulary tokens to keep for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If set to float < 1, only
    the smallest set of most probable tokens with probabilities that add up to `top_p`
    or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typical_p` (`float`, *optional*, defaults to 1.0) — Local typicality measures
    how similar the conditional probability of predicting a target token next is to
    the expected conditional probability of predicting a random token next, given
    the partial text already generated. If set to float < 1, the smallest set of the
    most locally typical tokens with probabilities that add up to `typical_p` or higher
    are kept for generation. See [this paper](https://arxiv.org/pdf/2202.00666.pdf)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon_cutoff` (`float`, *optional*, defaults to 0.0) — If set to float strictly
    between 0 and 1, only tokens with a conditional probability greater than `epsilon_cutoff`
    will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending
    on the size of the model. See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta_cutoff` (`float`, *optional*, defaults to 0.0) — Eta sampling is a hybrid
    of locally typical sampling and epsilon sampling. If set to float strictly between
    0 and 1, a token is only considered if it is greater than either `eta_cutoff`
    or `sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits)))`. The latter
    term is intuitively the expected next token probability, scaled by `sqrt(eta_cutoff)`.
    In the paper, suggested values range from 3e-4 to 2e-3, depending on the size
    of the model. See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diversity_penalty` (`float`, *optional*, defaults to 0.0) — This value is
    subtracted from a beam’s score if it generates a token same as any beam from other
    group at a particular time. Note that `diversity_penalty` is only effective if
    `group beam search` is enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repetition_penalty` (`float`, *optional*, defaults to 1.0) — The parameter
    for repetition penalty. 1.0 means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_repetition_penalty` (`float`, *optional*, defaults to 1.0) — The paramater
    for encoder_repetition_penalty. An exponential penalty on sequences that are not
    in the original input. 1.0 means no penalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — If set to int >
    0, all ngrams of that size can only occur once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bad_words_ids(List[List[int]],` *optional*) — List of list of token ids that
    are not allowed to be generated. Check [NoBadWordsLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoBadWordsLogitsProcessor)
    for further documentation and examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_words_ids(List[List[int]]` or `List[List[List[int]]]`, *optional*) —
    List of token ids that must be generated. If given a `List[List[int]]`, this is
    treated as a simple list of words that must be included, the opposite to `bad_words_ids`.
    If given `List[List[List[int]]]`, this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),
    where one can allow different forms of each word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`renormalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to
    renormalize the logits after applying all the logits processors or warpers (including
    the custom ones). It’s highly recommended to set this flag to `True` as the search
    algorithms suppose the score logits are normalized but some logit processors or
    warpers break the normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constraints` (`List[Constraint]`, *optional*) — Custom constraints that can
    be added to the generation to ensure that the output will contain the use of certain
    tokens as defined by `Constraint` objects, in the most sensible way possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forced_bos_token_id` (`int`, *optional*, defaults to `model.config.forced_bos_token_id`)
    — The id of the token to force as the first generated token after the `decoder_start_token_id`.
    Useful for multilingual models like [mBART](../model_doc/mbart) where the first
    generated token needs to be the target language token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forced_eos_token_id` (`Union[int, List[int]]`, *optional*, defaults to `model.config.forced_eos_token_id`)
    — The id of the token to force as the last generated token when `max_length` is
    reached. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_invalid_values` (`bool`, *optional*, defaults to `model.config.remove_invalid_values`)
    — Whether to remove possible *nan* and *inf* outputs of the model to prevent the
    generation method to crash. Note that using `remove_invalid_values` can slow down
    generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exponential_decay_length_penalty` (`tuple(int, float)`, *optional*) — This
    Tuple adds an exponentially increasing length penalty, after a certain amount
    of tokens have been generated. The tuple shall consist of: `(start_index, decay_factor)`
    where `start_index` indicates where penalty starts and `decay_factor` represents
    the factor of exponential decay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suppress_tokens` (`List[int]`, *optional*) — A list of tokens that will be
    suppressed at generation. The `SupressTokens` logit processor will set their log
    probs to `-inf` so that they are not sampled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`begin_suppress_tokens` (`List[int]`, *optional*) — A list of tokens that will
    be suppressed at the beginning of the generation. The `SupressBeginTokens` logit
    processor will set their log probs to `-inf` so that they are not sampled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forced_decoder_ids` (`List[List[int]]`, *optional*) — A list of pairs of integers
    which indicates a mapping from generation indices to token indices that will be
    forced before sampling. For example, `[[1, 123]]` means the second generated token
    will always be a token of index 123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_bias` (`Dict[Tuple[int], float]`, *optional*)) — Dictionary that
    maps a sequence of tokens to its bias term. Positive biases increase the odds
    of the sequence being selected, while negative biases do the opposite. Check [SequenceBiasLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SequenceBiasLogitsProcessor)
    for further documentation and examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*) — The guidance scale for classifier
    free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`. Higher guidance
    scale encourages the model to generate samples that are more closely linked to
    the input prompt, usually at the expense of poorer quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`low_memory` (`bool`, *optional*) — Switch to sequential topk for contrastive
    search to reduce peak memory. Used with contrastive search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters that define the output variables of `generate`
  prefs: []
  type: TYPE_NORMAL
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special tokens that can be used at generation time
  prefs: []
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*) — The id of the *beginning-of-sequence*
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation parameters exclusive to encoder-decoder models
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_no_repeat_ngram_size` (`int`, *optional*, defaults to 0) — If set
    to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot
    occur in the `decoder_input_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_start_token_id` (`int`, *optional*) — If an encoder-decoder model
    starts decoding with a different token than *bos*, the id of that token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation parameters exclusive to [assistant generation](https
  prefs: []
  type: TYPE_NORMAL
- en: '`num_assistant_tokens` (`int`, *optional*, defaults to 5) — Defines the number
    of *speculative tokens* that shall be generated by the assistant model before
    being checked by the target model at each iteration. Higher values for `num_assistant_tokens`
    make the generation more *speculative* : If the assistant model is performant
    larger speed-ups can be reached, if the assistant model requires lots of corrections,
    lower speed-ups are reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_assistant_tokens_schedule` (`str`, *optional*, defaults to `"heuristic"`)
    — Defines the schedule at which max assistant tokens shall be changed during inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"_heuristic_`: When all *speculative* tokens are correct, increase `num_assistant_tokens`
    by 2 else reduce by 1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"constant"`: `num_assistant_tokens` stays unchanged during generation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Wild card
  prefs: []
  type: TYPE_NORMAL
- en: 'Class that holds a configuration for a generation task. A `generate` call supports
    the following generation methods for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models:'
  prefs: []
  type: TYPE_NORMAL
- en: '*greedy decoding* by calling [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    if `num_beams=1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*contrastive search* by calling [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    if `penalty_alpha>0.` and `top_k>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multinomial sampling* by calling [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    if `num_beams=1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search decoding* by calling [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    if `num_beams>1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search multinomial sampling* by calling [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    if `num_beams>1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*diverse beam-search decoding* by calling [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    if `num_beams>1` and `num_beam_groups>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*constrained beam-search decoding* by calling [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search),
    if `constraints!=None` or `force_words_ids!=None`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*assisted decoding* by calling `assisted_decoding()`, if `assistant_model`
    is passed to `.generate()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘.generate()‘. To learn more about decoding strategies refer to the
    [text generation strategies guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: A large number of these flags control the logits or the stopping criteria of
    the generation. Make sure you check the [generate-related classes](https://huggingface.co/docs/transformers/internal/generation_utils)
    for a full description of the possible manipulations, as well as examples of their
    usage.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L605)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name` (`str` or `os.PathLike`) — This can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a string, the *model id* of a pretrained model configuration hosted inside a
    model repo on huggingface.co. Valid model ids can be located at the root-level,
    like `bert-base-uncased`, or namespaced under a user or organization name, like
    `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a *directory* containing a configuration file saved using the [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained)
    method, e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_file_name` (`str` or `os.PathLike`, *optional*, defaults to `"generation_config.json"`)
    — Name of the generation configuration JSON file to be loaded from `pretrained_model_name`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str` or `os.PathLike`, *optional*) — Path to a directory in which
    a downloaded pretrained model configuration should be cached if the standard cache
    should not be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to force to (re-)download the configuration files and override the cached versions
    if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resume_download` (`bool`, *optional*, defaults to `False`) — Whether or not
    to delete incompletely received file. Attempts to resume the download if such
    a file exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`proxies` (`Dict[str, str]`, *optional*) — A dictionary of proxy servers to
    use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}.` The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token` (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (`str`, *optional*, defaults to `"main"`) — The specific model version
    to use. It can be a branch name, a tag name, or a commit id, since we use a git-based
    system for storing models and other artifacts on huggingface.co, so `revision`
    can be any identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To test a pull request you made on the Hub, you can pass `revision=“refs/pr/<pr_number>“.</pr_number>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_unused_kwargs` (`bool`, *optional*, defaults to `False`) — If `False`,
    then this function returns just the final configuration object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where
    *unused_kwargs* is a dictionary consisting of the key/value pairs whose keys are
    not configuration attributes: i.e., the part of `kwargs` which has not been used
    to update `config` and is otherwise ignored.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`subfolder` (`str`, *optional*, defaults to `""`) — In case the relevant files
    are located inside a subfolder of the model repo on huggingface.co, you can specify
    the folder name here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — The values in kwargs of any keys
    which are configuration attributes will be used to override the loaded values.
    Behavior concerning key/value pairs whose keys are *not* configuration attributes
    is controlled by the `return_unused_kwargs` keyword parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration object instantiated from this pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from a generation configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_model_config`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L927)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model_config` (`PretrainedConfig`) — The model config that will be used to
    instantiate the generation config.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: The configuration object instantiated from those parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiates a [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)
    from a [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).
    This function is useful to convert legacy [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    objects, which may contain generation parameters, into a stand-alone [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/configuration_utils.py#L529)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the configuration
    JSON file will be saved (will be created if it does not exist).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_file_name` (`str` or `os.PathLike`, *optional*, defaults to `"generation_config.json"`)
    — Name of the generation configuration JSON file to be saved in `save_directory`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a generation configuration object to the directory `save_directory`, so
    that it can be re-loaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: GenerationMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GenerationMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L321)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: A class containing all functions for auto-regressive text generation, to be
    used as a mixin in [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    which can be used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '*greedy decoding* by calling [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    if `num_beams=1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*contrastive search* by calling [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    if `penalty_alpha>0` and `top_k>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multinomial sampling* by calling [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    if `num_beams=1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search decoding* by calling [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    if `num_beams>1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search multinomial sampling* by calling [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    if `num_beams>1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*diverse beam-search decoding* by calling [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    if `num_beams>1` and `num_beam_groups>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*constrained beam-search decoding* by calling [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search),
    if `constraints!=None` or `force_words_ids!=None`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1173)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`torch.Tensor` of varying shape depending on the modality, *optional*)
    — The sequence used as a prompt for the generation or as model inputs to the encoder.
    If `None` the method initializes it with `bos_token_id` and a batch size of 1\.
    For decoder-only models `inputs` should of in the format of `input_ids`. For encoder-decoder
    models *inputs* can represent any of `input_ids`, `input_values`, `input_features`,
    or `pixel_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. If your stopping
    criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`. This feature is intended for advanced users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*) — Whether to continue running the while
    loop until max_length. Unless overridden this flag will be set to `True` under
    DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished
    generating before other GPUs. Otherwise it’ll be set to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant_model` (`PreTrainedModel`, *optional*) — An assistant model that
    can be used to accelerate generation. The assistant model must have the exact
    same tokenizer. The acceleration is achieved when forecasting candidate tokens
    with the assistent model is much faster than running generation with the model
    you’re calling generate from. As such, the assistant model should be much smaller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — The negative prompt needed for some processors such as CFG. The
    batch size must match the input batch size. This is an experimental feature, subject
    to breaking API changes in future versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_attention_mask` (`torch.LongTensor` of shape `(batch_size,
    sequence_length)`, *optional*) — Attention_mask for `negative_prompt_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamDecoderOnlyOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_transition_scores`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L919)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.LongTensor`) — The generated sequences. The second dimension
    (sequence_length) is either equal to `max_length` or shorter if all batches finished
    early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(torch.FloatTensor)`) — Transition scores for each vocabulary
    token at each generation step. Beam transition scores consisting of log probabilities
    of tokens conditioned on log softmax of previously generated tokens Tuple of `torch.FloatTensor`
    with up to `max_new_tokens` elements (one element for each generated token), with
    each tensor of shape `(batch_size*num_beams, config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices of generated
    token id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`. Only required if a `num_beams>1` at generate-time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to normalize
    the logits (which, for legacy reasons, may be unnormalized).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`
    containing the transition scores (logits)
  prefs: []
  type: TYPE_NORMAL
- en: Computes the transition scores of sequences given the generation scores (and
    beam indices, if beam search was used). This is a convenient method to quicky
    obtain the scores of the selected tokens at generation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#### `greedy_search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2172)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing. model_kwargs — Additional
    model specific keyword arguments will be forwarded to the `forward` function of
    the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **greedy decoding** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#### `sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2433)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing. model_kwargs — Additional
    model specific kwargs will be forwarded to the `forward` function of the model.
    If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),
    [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A `torch.LongTensor` containing the generated tokens (default behaviour) or
    a [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)
    if `model.config.is_encoder_decoder=False` and `return_dict_in_generate=True`
    or a [GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)
    if `model.config.is_encoder_decoder=True`.
  prefs: []
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **multinomial sampling** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#### `beam_search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L2743)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_scorer` (`BeamScorer`) — An derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **beam search decoding** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#### `beam_sample`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3074)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_scorer` (`BeamScorer`) — A derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **beam search multinomial sampling** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#### `contrastive_search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L1715)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 1) — The size of the candidate set
    that is used to re-rank for contrastive search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`penalty_alpha` (`float`, *optional*, defaults to 0) — The degeneration penalty
    for contrastive search; activate when it is larger than 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`streamer` (`BaseStreamer`, *optional*) — Streamer object that will be used
    to stream the generated sequences. Generated tokens are passed through `streamer.put(token_ids)`
    and the streamer is responsible for any further processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequential` (`bool`, *optional*) — Switches topk hidden state computation
    from parallel to sequential to reduce memory if True. model_kwargs — Additional
    model specific keyword arguments will be forwarded to the `forward` function of
    the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **contrastive search** and can be used for text-decoder, text-to-text, speech-to-text,
    and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#### `group_beam_search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3411)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_scorer` (`BeamScorer`) — An derived instance of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation.
    For more information, the documentation of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    should be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: model_kwargs — Additional model specific kwargs that will be forwarded to the
    `forward` function of the model. If model is an encoder-decoder model the kwargs
    should include `encoder_outputs`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **diverse beam search decoding** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#### `constrained_beam_search`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L3796)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constrained_beam_scorer` (`ConstrainedBeamSearchScorer`) — A derived instance
    of [BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    that defines how beam hypotheses are constructed, stored and sorted during generation,
    while satisfying a list of positive constraints. For more information, the documentation
    of [ConstrainedBeamSearchScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer)
    should be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    used to modify the prediction scores of the language modeling head applied at
    each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — An instance of [StoppingCriteriaList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteriaList).
    List of instances of class derived from [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    used to tell if the generation loop should stop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_warper` (`LogitsProcessorList`, *optional*) — An instance of [LogitsProcessorList](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessorList).
    List of instances of class derived from [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    used to warp the prediction score distribution of the language modeling head applied
    before multinomial sampling at each generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*, defaults to 20) — **DEPRECATED**. Use `logits_processor`
    or `stopping_criteria` directly to cap the number of generated tokens. The maximum
    length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers. See `attentions`
    under returned tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_scores` (`bool`, *optional*, defaults to `False`) — Whether or not
    to return the prediction scores. See `scores` under returned tensors for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) — Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) — Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3) model_kwargs
    — Additional model specific kwargs will be forwarded to the `forward` function
    of the model. If model is an encoder-decoder model the kwargs should include `encoder_outputs`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head using
    **constrained beam search decoding** and can be used for text-decoder, text-to-text,
    speech-to-text, and vision-to-text models.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you do not need to call [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search)
    directly. Use generate() instead. For an overview of generation strategies and
    code examples, check the [following guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: TFGenerationMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFGenerationMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L444)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: A class containing all of the functions supporting generation, to be used as
    a mixin in [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate),
    which can be used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '*greedy decoding* by calling `greedy_search()` if `num_beams=1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*contrastive search* by calling `contrastive_search()` if `penalty_alpha>0`
    and `top_k>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multinomial sampling* by calling `sample()` if `num_beams=1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search decoding* by calling `beam_search()` if `num_beams>1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L645)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`inputs` (`tf.Tensor` of varying shape depending on the modality, *optional*)
    — The sequence used as a prompt for the generation or as model inputs to the encoder.
    If `None` the method initializes it with `bos_token_id` and a batch size of 1\.
    For decoder-only models `inputs` should of in the format of `input_ids`. For encoder-decoder
    models *inputs* can represent any of `input_ids`, `input_values`, `input_features`,
    or `pixel_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`List[int]`, *optional*) — Random seed to control sampling, containing
    two integers, used when `do_sample` is `True`. See the `seed` argument from stateless
    functions in `tf.random`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `tf.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `tf.Tensor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[TFGreedySearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchDecoderOnlyOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleDecoderOnlyOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFBeamSearchDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchDecoderOnlyOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFBeamSampleDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleDecoderOnlyOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[TFGreedySearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFGreedySearchEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFSampleEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFBeamSearchEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSearchEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFBeamSampleEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.TFBeamSampleEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the model’s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate, e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: For an overview of generation strategies and code examples, check out the [following
    guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute_transition_scores`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L477)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)`) — Transition scores for each vocabulary token
    at each generation step. Beam transition scores consisting of log probabilities
    of tokens conditioned on log softmax of previously generated tokens Tuple of `tf.Tensor`
    with up to `max_new_tokens` elements (one element for each generated token), with
    each tensor of shape `(batch_size*num_beams, config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`tf.Tensor`, *optional*) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`. Only required if a `num_beams>1` at generate-time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_logits` (`bool`, *optional*, defaults to `False`) — Whether to normalize
    the logits (which, for legacy reasons, may be unnormalized).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor`'
  prefs: []
  type: TYPE_NORMAL
- en: A `tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`
    containing the transition scores (logits)
  prefs: []
  type: TYPE_NORMAL
- en: Computes the transition scores of sequences given the generation scores (and
    beam indices, if beam search was used). This is a convenient method to quicky
    obtain the scores of the selected tokens at generation time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: FlaxGenerationMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxGenerationMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: A class containing all functions for auto-regressive text generation, to be
    used as a mixin in [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class exposes [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate),
    which can be used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '*greedy decoding* by calling `_greedy_search()` if `num_beams=1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*multinomial sampling* by calling `_sample()` if `num_beams=1` and `do_sample=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*beam-search decoding* by calling `_beam_search()` if `num_beams>1` and `do_sample=False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You do not need to call any of the above methods directly. Pass custom parameter
    values to ‘generate’ instead. To learn more about decoding strategies refer to
    the [text generation strategies guide](../generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L267)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — The
    sequence used as a prompt for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trace` (`bool`, *optional*, defaults to `True`) — Whether to trace generation.
    Setting `trace=False` should only be used for debugging and will lead to a considerably
    slower runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params` (`Dict[str, jnp.ndarray]`, *optional*) — Optionally the model parameters
    can be passed. Can be useful for parallelized generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`FlaxLogitsProcessorList` , *optional*) — Custom logits
    processors that complement the default logits processors built from arguments
    and generation config. If a logit processor is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates sequences of token ids for models with a language modeling head.
  prefs: []
  type: TYPE_NORMAL
