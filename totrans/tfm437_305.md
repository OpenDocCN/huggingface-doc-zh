# CLAP

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap)

## 概述

CLAP模型由Yusong Wu，Ke Chen，Tianyu Zhang，Yuchen Hui，Taylor Berg-Kirkpatrick，Shlomo Dubnov在[大规模对比语言音频预训练与特征融合和关键词到标题增强](https://arxiv.org/pdf/2211.06687.pdf)中提出。

CLAP（对比语言音频预训练）是一个神经网络，训练于各种（音频，文本）对。它可以被指示来预测最相关的文本片段，给定一个音频，而不是直接为任务进行优化。CLAP模型使用SWINTransformer从对数Mel频谱图输入中获取音频特征，并使用RoBERTa模型获取文本特征。然后，文本和音频特征被投影到具有相同维度的潜在空间中。然后使用投影音频和文本特征之间的点积作为相似分数。

论文摘要如下：

*对比学习在多模态表示学习领域取得了显著的成功。在本文中，我们提出了对比语言音频预训练的流水线，通过将音频数据与自然语言描述相结合来开发音频表示。为了实现这一目标，我们首先发布了LAION-Audio-630K，一个包含来自不同数据源的633,526个音频文本对的大型集合。其次，我们通过考虑不同的音频编码器和文本编码器构建了对比语言音频预训练模型。我们将特征融合机制和关键词到标题增强纳入模型设计中，以进一步使模型能够处理长度可变的音频输入并增强性能。第三，我们进行了全面的实验来评估我们的模型在三个任务中的表现：文本到音频检索，零样本音频分类和监督音频分类。结果表明，我们的模型在文本到音频检索任务中取得了优越的性能。在音频分类任务中，该模型在零样本设置下取得了最先进的性能，并且能够获得与非零样本设置中模型结果相媲美的性能。LAION-Audio-6*

该模型由[Younes Belkada](https://huggingface.co/ybelkada)和[Arthur Zucker](https://huggingface.co/ArthurZ)贡献。原始代码可以在[这里](https://github.com/LAION-AI/Clap)找到。

## ClapConfig

### `class transformers.ClapConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L334)

```py
( text_config = None audio_config = None logit_scale_init_value = 14.285714285714285 projection_dim = 512 projection_hidden_act = 'relu' initializer_factor = 1.0 **kwargs )
```

参数

+   `text_config`（`dict`，*可选*）— 用于初始化[ClapTextConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextConfig)的配置选项字典。

+   `audio_config`（`dict`，*可选*）— 用于初始化[ClapAudioConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioConfig)的配置选项字典。

+   `logit_scale_init_value`（`float`，*可选*，默认为14.29）— *logit_scale*参数的初始值。默认值根据原始CLAP实现使用。

+   `projection_dim`（`int`，*可选*，默认为512）— 文本和音频投影层的维度。

+   `projection_hidden_act`（`str`，*可选*，默认为`"relu"`）— 投影层的激活函数。

+   `initializer_factor`（`float`，*可选*，默认为1.0）— 用于缩放模型权重初始化的因子。

+   `kwargs`（*可选*）— 关键字参数的字典。

[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)是用于存储[ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel)配置的配置类。根据指定的参数实例化一个 CLAP 模型，定义文本模型和音频模型配置。使用默认值实例化配置将产生与 CLAP [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused) 架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import ClapConfig, ClapModel

>>> # Initializing a ClapConfig with laion-ai/base style configuration
>>> configuration = ClapConfig()

>>> # Initializing a ClapModel (with random weights) from the laion-ai/base style configuration
>>> model = ClapModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a ClapConfig from a ClapTextConfig and a ClapAudioConfig
>>> from transformers import ClapTextConfig, ClapAudioConfig

>>> # Initializing a ClapText and ClapAudioConfig configuration
>>> config_text = ClapTextConfig()
>>> config_audio = ClapAudioConfig()

>>> config = ClapConfig.from_text_audio_configs(config_text, config_audio)
```

#### `from_text_audio_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L422)

```py
( text_config: ClapTextConfig audio_config: ClapAudioConfig **kwargs ) → export const metadata = 'undefined';ClapConfig
```

返回

[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)

一个配置对象的实例

从 clap 文本模型配置和 clap 音频模型配置实例化一个[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)（或派生类）。

## ClapTextConfig

### `class transformers.ClapTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L32)

```py
( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 initializer_factor = 1.0 layer_norm_eps = 1e-12 projection_dim = 512 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True projection_hidden_act = 'relu' **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为30522) — CLAP 模型的词汇表大小。定义了在调用[ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel)时可以表示的不同标记的数量。

+   `hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer 编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *可选*, 默认为12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *可选*, 默认为3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` 或 `Callable`, *可选*, 默认为`"relu"`) — 编码器和池化层中的非线性激活函数（函数或字符串）。如果是字符串，支持`"relu"`、`"relu"`、`"silu"`和`"relu_new"`。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为0.1) — 嵌入层、编码器和池化层中所有全连接层的丢弃概率。

+   `attention_probs_dropout_prob` (`float`, *可选*, 默认为0.1) — 注意力概率的丢弃比例。

+   `max_position_embeddings` (`int`, *可选*, 默认为512) — 此模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如，512或1024或2048）。

+   `type_vocab_size` (`int`, *可选*, 默认为2) — 在调用[ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel)时传递的`token_type_ids`的词汇表大小。

+   `layer_norm_eps` (`float`, *可选*, 默认为1e-12) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *可选*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`、`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)]中的*Method 4* (https://arxiv.org/abs/2009.13658)。

+   `is_decoder` (`bool`, *可选*, 默认为`False`) — 模型是否用作解码器。如果为`False`，则模型用作编码器。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。

+   `projection_hidden_act` (`str`, *可选*, 默认为`"relu"`) — 投影层中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `projection_dim` (`int`, *可选*, 默认为512) — `ClapTextModelWithProjection`的投影头的维度。

这是一个配置类，用于存储[ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel)的配置。根据指定的参数实例化一个CLAP模型，定义模型架构。使用默认值实例化配置将产生类似于CLAP [calp-hsat-fused](https://huggingface.co/laion/clap-hsat-fused)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例:

```py
>>> from transformers import ClapTextConfig, ClapTextModel

>>> # Initializing a CLAP text configuration
>>> configuration = ClapTextConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = ClapTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ClapAudioConfig

### `class transformers.ClapAudioConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/configuration_clap.py#L164)

```py
( window_size = 8 num_mel_bins = 64 spec_size = 256 hidden_act = 'gelu' patch_size = 4 patch_stride = [4, 4] num_classes = 527 hidden_size = 768 projection_dim = 512 depths = [2, 2, 6, 2] num_attention_heads = [4, 8, 16, 32] enable_fusion = False hidden_dropout_prob = 0.1 fusion_type = None patch_embed_input_channels = 1 flatten_patch_embeds = True patch_embeds_hidden_size = 96 enable_patch_layer_norm = True drop_path_rate = 0.0 attention_probs_dropout_prob = 0.0 qkv_bias = True mlp_ratio = 4.0 aff_block_r = 4 num_hidden_layers = 4 projection_hidden_act = 'relu' layer_norm_eps = 1e-05 initializer_factor = 1.0 **kwargs )
```

参数

+   `window_size` (`int`, *可选*, 默认为8) — 频谱图的图像大小

+   `num_mel_bins` (`int`, *可选*, 默认为64) — 每帧使用的mel特征数。应与`ClapProcessor`类中使用的值对应。

+   `spec_size` (`int`, *可选*, 默认为256) — 模型支持的频谱图的期望输入大小。它可以与`ClapFeatureExtractor`的输出不同，此时输入特征将被调整大小。对应于音频模型的`image_size`。

+   `hidden_act` (`str`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `patch_size` (`int`, *可选*, 默认为4) — 音频频谱图的补丁大小

+   `patch_stride` (`list`, *可选*, 默认为`[4, 4]`) — 音频频谱图的补丁步幅

+   `num_classes` (`int`, *可选*, 默认为527) — 用于头部训练的类别数

+   `hidden_size` (`int`, *可选*, 默认为768) — 音频编码器输出的隐藏大小。对应于倒数第二层输出的维度，发送到投影MLP层。

+   `projection_dim` (`int`, *可选*, 默认为512) — 投影层的隐藏大小。

+   `depths` (`list`, *可选*, 默认为`[2, 2, 6, 2]`) — 用于音频模型的Swin层的深度

+   `num_attention_heads` (`list`, *可选*, 默认为`[4, 8, 16, 32]`) — 用于音频模型的Swin层的注意力头数。

+   `enable_fusion` (`bool`, *可选*, 默认为`False`) — 是否启用补丁融合。这是作者的主要贡献，应该能够获得最佳结果。

+   `hidden_dropout_prob` (`float`, *可选*, 默认为0.1) — 编码器中所有全连接层的丢失概率。

+   `fusion_type` (`[type]`, *可选*) — 用于补丁融合的融合类型。

+   `patch_embed_input_channels` (`int`, *可选*, 默认为1) — 用于输入频谱图的通道数

+   `flatten_patch_embeds` (`bool`, *可选*, 默认为`True`) — 是否展平补丁嵌入

+   `patch_embeds_hidden_size` (`int`, *optional*, 默认为96) — 补丁嵌入的隐藏大小。它用作输出通道数。

+   `enable_patch_layer_norm` (`bool`, *optional*, 默认为`True`) — 是否启用补丁嵌入的层归一化

+   `drop_path_rate` (`float`, *optional*, 默认为0.0) — 用于补丁融合的Drop path率。

+   `attention_probs_dropout_prob` (`float`, *optional*, 默认为0.0) — 注意力概率的dropout比率。

+   `qkv_bias` (`bool`, *optional*, 默认为`True`) — 是否向查询、键、值投影添加偏置。

+   `mlp_ratio` (`float`, *optional*, 默认为4.0) — MLP隐藏维度与嵌入维度的比率。

+   `aff_block_r` (`int`, *optional*, 默认为4) — AudioFF块中使用的downsize_ratio。

+   `num_hidden_layers` (`int`, *optional*, 默认为4) — Transformer编码器中的隐藏层数量。

+   `projection_hidden_act` (`str`, *optional*, 默认为`"relu"`) — 投影层中的非线性激活函数（函数或字符串）。如果是字符串，则支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `layer_norm_eps` (`[type]`, *optional*, 默认为1e-05) — 层归一化层使用的epsilon。

+   `initializer_factor` (`float`, *optional*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1，内部用于初始化测试）。

这是用于存储[ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel)配置的配置类。根据指定的参数实例化一个CLAP音频编码器，定义模型架构。使用默认值实例化配置将产生类似于CLAP [laion/clap-htsat-fused](https://huggingface.co/laion/clap-htsat-fused)架构的音频编码器的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import ClapAudioConfig, ClapAudioModel

>>> # Initializing a ClapAudioConfig with laion/clap-htsat-fused style configuration
>>> configuration = ClapAudioConfig()

>>> # Initializing a ClapAudioModel (with random weights) from the laion/clap-htsat-fused style configuration
>>> model = ClapAudioModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ClapFeatureExtractor

### `class transformers.ClapFeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/feature_extraction_clap.py#L33)

```py
( feature_size = 64 sampling_rate = 48000 hop_length = 480 max_length_s = 10 fft_window_size = 1024 padding_value = 0.0 return_attention_mask = False frequency_min: float = 0 frequency_max: float = 14000 top_db: int = None truncation: str = 'fusion' padding: str = 'repeatpad' **kwargs )
```

参数

+   `feature_size` (`int`, *optional*, 默认为64) — 提取的Mel频谱图的特征维度。这对应于Mel滤波器的数量（`n_mels`）。

+   `sampling_rate` (`int`, *optional*, 默认为48000) — 音频文件应数字化的采样率，以赫兹（Hz）表示。这仅用于警告用户，如果输入到特征提取器的音频采样率不同。

+   `hop_length` (`int`,*optional*, 默认为480) — 用于获取Mel Spectrogram的STFT中重叠窗口的长度。音频将被分割成较小的`frames`，每个帧之间的步长为`hop_length`。

+   `max_length_s` (`int`, *optional*, 默认为10) — 模型的最大输入长度（以秒为单位）。这用于填充音频。

+   `fft_window_size` (`int`, *optional*, 默认为1024) — 应用傅立叶变换的窗口大小（以样本为单位）。这控制了频谱图的频率分辨率。400表示傅立叶变换在400个样本的窗口上计算。

+   `padding_value` (`float`, *optional*, 默认为0.0) — 用于填充音频的填充值。应对应于静音。

+   `return_attention_mask` (`bool`, *optional*, 默认为`False`) — 模型是否应返回与输入对应的注意力掩码。

+   `frequency_min` (`float`, *optional*, 默认为0) — 感兴趣的最低频率。对于低于此值的频率，将不计算STFT。

+   `frequency_max`（`float`，*可选*，默认为14000）- 感兴趣的最高频率。对于超过此值的值，STFT将不会计算。

+   `top_db`（`float`，*可选*）- 用于将mel频谱图转换为对数刻度的最高分贝值。有关更多详细信息，请参阅`audio_utils.power_to_db`函数

+   `truncation`（`str`，*可选*，默认为`"fusion"`）- 用于长音频输入的截断模式。有两种模式可用：

    +   `fusion`将使用`_random_mel_fusion`，它堆叠了来自mel频谱图的3个随机裁剪和整个mel频谱图的降采样版本。如果`config.fusion`设置为True，则较短的音频也需要返回4个mel，这将只是从填充音频中获得的原始mel的副本。

    +   `rand_trunc`将选择mel频谱图的随机裁剪。

+   `padding`（`str`，*可选*，默认为`"repeatpad"`）- 用于较短音频输入的填充模式。最初实现了三种模式：

    +   `repeatpad`：音频被重复，然后被填充以适应`max_length`。

    +   `repeat`：音频被重复，然后被裁剪以适应`max_length`

    +   `pad`：音频被填充。

构建一个CLAP特征提取器。

这个特征提取器继承自[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

这个类使用自定义的numpy实现*短时傅里叶变换*（STFT）从原始语音中提取mel滤波器组特征，这应该与pytorch的`torch.stft`等效。

#### `to_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/feature_extraction_clap.py#L138)

```py
( ) → export const metadata = 'undefined';Dict[str, Any]
```

返回

`Dict[str, Any]`

构成此配置实例的所有属性的字典，除了mel滤波器组，它们不需要被保存或打印，因为它们太长。

将此实例序列化为Python字典。

## ClapProcessor

### `class transformers.ClapProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L23)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor`（[ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)）- 音频处理器是必需的输入。

+   `tokenizer`（[RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast)）- 分词器是必需的输入。

构建一个CLAP处理器，将CLAP特征提取器和RoBerta分词器封装成一个单一处理器。

[ClapProcessor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapProcessor)提供了[ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)和[RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapProcessor.decode)。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L99)

```py
( *args **kwargs )
```

这种方法将所有参数转发给RobertaTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。请参考此方法的文档字符串以获取更多信息。

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/processing_clap.py#L106)

```py
( *args **kwargs )
```

这种方法将所有参数转发给RobertaTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## ClapModel

### `class transformers.ClapModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1920)

```py
( config: ClapConfig )
```

参数

+   `config` ([ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2050)

```py
( input_ids: Optional = None input_features: Optional = None is_longer: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将被忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [输入ID是什么？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   对于未被“掩码”的标记为1，

    +   对于被“掩码”的标记为0。

    [注意力掩码是什么？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [位置ID是什么？](../glossary#position-ids)

+   `input_features` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 输入音频特征。这应该由[ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)类返回，您也可以从[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)中检索。有关详细信息，请参阅`ClapFeatureExtractor.__call__()`。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

`transformers.models.clap.modeling_clap.ClapOutput` 或 `tuple(torch.FloatTensor)`

一个`transformers.models.clap.modeling_clap.ClapOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.clap.configuration_clap.ClapConfig'>`）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当 `return_loss` 为 `True` 时返回) — 音频-文本相似性的对比损失。

+   `logits_per_audio:(torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`) — `audio_embeds` 和 `text_embeds` 之间的缩放点积分数。这代表了音频-文本相似度分数。

+   `logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`) — `text_embeds` 和 `audio_embeds` 之间的缩放点积分数。这代表了文本-音频相似度分数。

+   `text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将 [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel) 的汇聚输出应用到投影层而获得的文本嵌入。

+   `audio_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将 [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel) 的汇聚输出应用到投影层而获得的音频嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel) 的输出。

+   `audio_model_output(BaseModelOutputWithPooling):` [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel) 的输出。

[ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoProcessor, ClapModel

>>> dataset = load_dataset("ashraq/esc50")
>>> audio_sample = dataset["train"]["audio"][0]["array"]

>>> model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
>>> processor = AutoProcessor.from_pretrained("laion/clap-htsat-unfused")

>>> input_text = ["Sound of a dog", "Sound of vaccum cleaner"]

>>> inputs = processor(text=input_text, audios=audio_sample, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_audio = outputs.logits_per_audio  # this is the audio-text similarity score
>>> probs = logits_per_audio.softmax(dim=-1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1956)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 代表 `未被掩码` 的标记，

    +   0 代表 `被掩码` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)

通过将 [ClapTextModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModel) 的汇聚输出应用到投影层而获得的文本嵌入。

[ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from transformers import AutoTokenizer, ClapModel

>>> model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
>>> tokenizer = AutoTokenizer.from_pretrained("laion/clap-htsat-unfused")

>>> inputs = tokenizer(["the sound of a cat", "the sound of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_audio_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2004)

```py
( input_features: Optional = None is_longer: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';audio_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_features` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 输入音频特征。这应该由 [ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor) 类返回，您也可以从 [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor) 中检索。有关详细信息，请参阅 `ClapFeatureExtractor.__call__()`。

+   `is_longer` (`torch.FloatTensor`，形状为 `(batch_size, 1)`，*可选*) — 音频片段是否比 `max_length` 更长。如果为 `True`，将启用特征融合以增强特征。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

返回

audio_features (`torch.FloatTensor`，形状为 `(batch_size, output_dim)`)

通过将投影层应用于 [ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel) 的汇聚输出获得的音频嵌入。

[ClapModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from transformers import AutoFeatureExtractor, ClapModel
>>> import torch

>>> model = ClapModel.from_pretrained("laion/clap-htsat-unfused")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("laion/clap-htsat-unfused")
>>> random_audio = torch.rand((16_000))
>>> inputs = feature_extractor(random_audio, return_tensors="pt")
>>> audio_features = model.get_audio_features(**inputs)
```

## ClapTextModel

### `class transformers.ClapTextModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1751)

```py
( config add_pooling_layer = True )
```

该模型可以作为编码器（仅具有自注意力）以及解码器的行为，此时在自注意力层之间添加了一层交叉注意力，遵循 *Attention is all you need*_ 中描述的架构，作者为 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser 和 Illia Polosukhin。

要使模型行为像一个解码器，需要使用配置中的 `is_decoder` 参数设置为 `True` 来初始化模型。要在 Seq2Seq 模型中使用，模型需要使用 `is_decoder` 参数和 `add_cross_attention` 都设置为 `True` 来初始化；然后预期在前向传递中输入一个 `encoder_hidden_states`。

.. _*注意力就是你所需要的*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1789)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

encoder_hidden_states（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）：编码器最后一层的隐藏状态序列。如果模型配置为解码器，则用于交叉注意力。encoder_attention_mask（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）：避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。掩码值选择在`[0, 1]`之间：

+   对于未被`masked`的标记为1，

+   对于被`masked`的标记为0。past_key_values（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组包含4个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。

如果使用`past_key_values`，用户可以选择仅输入最后一个`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。use_cache（`bool`，*可选*）：如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

## ClapTextModelWithProjection

### `class transformers.ClapTextModelWithProjection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2148)

```py
( config: ClapTextConfig )
```

参数

+   `config`（[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有顶部投影层的CLAP文本模型（在汇总输出的顶部有一个线性层）。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2170)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapTextModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.clap.modeling_clap.ClapTextModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clap.modeling_clap.ClapTextModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class 'transformers.models.clap.configuration_clap.ClapTextConfig'>`)和输入的各种元素。

+   `text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* 当模型初始化为`with_projection=True`时返回) — 通过将投影层应用于pooler_output获得的文本嵌入。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[ClapTextModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModelWithProjection)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, ClapTextModelWithProjection

>>> model = ClapTextModelWithProjection.from_pretrained("laion/clap-htsat-unfused")
>>> tokenizer = AutoTokenizer.from_pretrained("laion/clap-htsat-unfused")

>>> inputs = tokenizer(["a sound of a cat", "a sound of a dog"], padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> text_embeds = outputs.text_embeds
```

## ClapAudioModel

### `class transformers.ClapAudioModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1693)

```py
( config: ClapAudioConfig )
```

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L1706)

```py
( input_features: Optional = None is_longer: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_features` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 输入音频特征。这应该由[ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)类返回，您也可以从[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)中检索。有关详细信息，请参阅`ClapFeatureExtractor.__call__()`。

+   `is_longer` (`torch.FloatTensor`, 形状为`(batch_size, 1)`, *optional*) — 音频剪辑是否比`max_length`更长。如果为`True`，将启用特征融合以增强特征。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括各种元素，取决于配置（`<class 'transformers.models.clap.configuration_clap.ClapAudioConfig'>`）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列输出。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[ClapAudioModel](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import AutoProcessor, ClapAudioModel

>>> dataset = load_dataset("ashraq/esc50")
>>> audio_sample = dataset["train"]["audio"][0]["array"]

>>> model = ClapAudioModel.from_pretrained("laion/clap-htsat-fused")
>>> processor = AutoProcessor.from_pretrained("laion/clap-htsat-fused")

>>> inputs = processor(audios=audio_sample, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
```

## ClapAudioModelWithProjection

### `class transformers.ClapAudioModelWithProjection`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2224)

```py
( config: ClapAudioConfig )
```

参数

+   `config`（[ClapConfig](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部具有投影层的CLAP音频模型（在池化输出的顶部有一个线性层）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

前进

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clap/modeling_clap.py#L2244)

```py
( input_features: Optional = None is_longer: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clap.modeling_clap.ClapAudioModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_features`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—输入音频特征。这应该由您可以从[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)中检索的[ClapFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapFeatureExtractor)类返回。有关详细信息，请参阅`ClapFeatureExtractor.__call__()`。

+   `is_longer`（形状为`(batch_size, 1)`的`torch.FloatTensor`，*可选*）—音频剪辑是否比`max_length`更长。如果为`True`，将启用特征融合以增强特征。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

`transformers.models.clap.modeling_clap.ClapAudioModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clap.modeling_clap.ClapAudioModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（`<class 'transformers.models.clap.configuration_clap.ClapAudioConfig'>`）和输入的不同元素。

+   `audio_embeds`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）—通过将投影层应用于pooler_output获得的音频嵌入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）—模型最后一层的隐藏状态序列。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

[ClapAudioModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapAudioModelWithProjection)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from datasets import load_dataset
>>> from transformers import ClapAudioModelWithProjection, ClapProcessor

>>> model = ClapAudioModelWithProjection.from_pretrained("laion/clap-htsat-fused")
>>> processor = ClapProcessor.from_pretrained("laion/clap-htsat-fused")

>>> dataset = load_dataset("ashraq/esc50")
>>> audio_sample = dataset["train"]["audio"][0]["array"]

>>> inputs = processor(audios=audio_sample, return_tensors="pt")
>>> outputs = model(**inputs)
>>> audio_embeds = outputs.audio_embeds
```
