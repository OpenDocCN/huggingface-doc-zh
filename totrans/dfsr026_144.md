# I2VGen-XL

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl](https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl)

[I2VGen-XL：通过级联扩散模型实现高质量图像到视频合成](https://hf.co/papers/2311.04145.pdf) 作者：张世伟，王佳宇，张颖雅，赵康，袁航杰，秦志武，王翔，赵德力，周静仁。

论文摘要如下：

*视频合成最近取得了显著进展，这要归功于扩散模型的快速发展。然而，它仍然在语义准确性、清晰度和时空连续性方面遇到挑战。这主要源于文本视频数据的匮乏以及视频的复杂内在结构，使得模型难以同时确保语义和质量上的卓越性。在本报告中，我们提出了一种级联I2VGen-XL方法，通过解耦这两个因素来增强模型性能，并利用静态图像作为重要指导形式来确保输入数据的对齐。I2VGen-XL包括两个阶段：i）基础阶段通过使用两个分层编码器保证连贯的语义，并保留来自输入图像的内容，ii）细化阶段通过整合额外简短的文本来增强视频的细节，并将分辨率提高到1280×720。为了提高多样性，我们收集了约3500万个单镜头文本视频对和60亿个文本图像对来优化模型。通过这种方式，I2VGen-XL可以同时提高语义准确性、细节连续性和生成视频的清晰度。通过大量实验，我们研究了I2VGen-XL的基本原理，并将其与当前顶级方法进行了比较，从而展示了其在多样数据上的有效性。源代码和模型将在[此https URL](https://i2vgen-xl.github.io/)上公开。*

原始代码库可以在[此处](https://github.com/ali-vilab/i2vgen-xl/)找到。模型检查点可以在[此处](https://huggingface.co/ali-vilab/)找到。

确保查看调度器[指南](../../using-diffusers/schedulers)，以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。此外，要了解如何减少此管道的内存使用量，请参考[“减少内存使用量”]部分[此处](../../using-diffusers/svd#reduce-memory-usage)。

使用I2VGenXL的示例输出：

| 杰作，最佳质量，日落。![library](../Images/a2129243953299cb71d17cb43a7d368b.png) |
| --- |

## 注释

+   I2VGenXL总是使用`clip_skip`值为1。这意味着它利用了CLIP文本编码器的倒数第二层表示。

+   它可以生成与[稳定视频扩散](../../using-diffusers/svd)（SVD）质量相当的视频。

+   与SVD不同，它还接受文本提示作为输入。

+   它可以生成更高分辨率的视频。

+   当使用[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)（这是此管道的默认设置）时，推断步骤少于50步会导致糟糕的结果。

## I2VGenXLPipeline

### `class diffusers.I2VGenXLPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L109)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer image_encoder: CLIPVisionModelWithProjection feature_extractor: CLIPImageProcessor unet: I2VGenXLUNet scheduler: DDIMScheduler )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)） — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中生成图像。

+   `text_encoder` (`CLIPTextModel`) — 冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet` (`I2VGenXLUNet`) — 用于去噪编码视频潜在特征的`I2VGenXLUNet`。

+   `scheduler` ([DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)) — 用于与`unet`结合使用以去噪编码图像潜在特征的调度器。

提议的图像到视频生成管道，如[I2VGenXL](https://i2vgen-xl.github.io/)中提出的。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L594)

```py
( prompt: Union = None image: Union = None height: Optional = 704 width: Optional = 1280 target_fps: Optional = 16 num_frames: int = 16 num_inference_steps: int = 50 guidance_scale: float = 9.0 negative_prompt: Union = None eta: float = 0.0 num_videos_per_prompt: Optional = 1 decode_chunk_size: Optional = 1 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = 1 ) → export const metadata = 'undefined';pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`PIL.Image.Image` 或 `List[PIL.Image.Image]` 或 `torch.FloatTensor`) — 用于指导图像生成的图像。如果提供张量，则需要与[`CLIPImageProcessor`](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)兼容。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `target_fps` (`int`, *可选*) — 每秒帧数。在生成后将生成的图像导出到视频的速率。这也被用作生成时的“微条件”。

+   `num_frames` (`int`, *可选*) — 要生成的视频帧数。

+   `num_inference_steps` (`int`, *可选*) — 去噪步骤的数量。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成中不包括的提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）时被忽略。

+   `eta` (`float`, *可选*) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。

+   `num_videos_per_prompt` (`int`, *可选*) — 每个提示生成的图像数量。

+   `decode_chunk_size` (`int`, *可选*) — 一次解码的帧数。块大小越大，帧之间的时间一致性越高，但内存消耗也越高。默认情况下，解码器将一次解码所有帧以获得最大质量。减少`decode_chunk_size`以减少内存使用。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中预先生成的嘈杂潜在特征，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在特征张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。

+   `output_type` (`str`, *optional*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为 `True`) — 是否返回 [StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 而不是普通的元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定了，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预最终层的输出来计算提示嵌入。

返回

[pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput) 或 `tuple`

如果 `return_dict` 为 `True`，将返回 [pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput)，否则将返回一个元组，其中第一个元素是生成帧的列表。

使用[I2VGenXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/i2vgenxl#diffusers.I2VGenXLPipeline)进行图像到视频生成的管道的调用函数。

示例：

```py
>>> import torch
>>> from diffusers import I2VGenXLPipeline

>>> pipeline = I2VGenXLPipeline.from_pretrained("ali-vilab/i2vgen-xl", torch_dtype=torch.float16, variant="fp16")
>>> pipeline.enable_model_cpu_offload()

>>> image_url = "https://github.com/ali-vilab/i2vgen-xl/blob/main/data/test_images/img_0009.png?raw=true"
>>> image = load_image(image_url).convert("RGB")

>>> prompt = "Papers were floating in the air on a table in the library"
>>> negative_prompt = "Distorted, discontinuous, Ugly, blurry, low resolution, motionless, static, disfigured, disconnected limbs, Ugly faces, incomplete arms"
>>> generator = torch.manual_seed(8888)

>>> frames = pipeline(
...     prompt=prompt,
...     image=image,
...     num_inference_steps=50,
...     negative_prompt=negative_prompt,
...     guidance_scale=9.0,
...     generator=generator
... ).frames[0]
>>> video_path = export_to_gif(frames, "i2v.gif")
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L590)

```py
( )
```

如果启用，将禁用 FreeU 机制。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L176)

```py
( )
```

禁用切片 VAE 解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L193)

```py
( )
```

禁用平铺 VAE 解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L567)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 第1阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 第2阶段的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 第1阶段的缩放因子，用于放大骨干特征的贡献。

+   `b2` (`float`) — 第2阶段的缩放因子，用于放大骨干特征的贡献。

启用 FreeU 机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同流水线（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L168)

```py
( )
```

启用切片VAE解码。启用此选项时，VAE将将输入张量分割成片来计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L184)

```py
( )
```

启用平铺VAE解码。启用此选项时，VAE将将输入张量分割成瓦片，以便在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

`encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L200)

```py
( prompt device num_videos_per_prompt negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch设备

+   `num_videos_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用无分类器指导

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *optional*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip` (`int`, *optional*) — 从CLIP中跳过的层数，用于计算提示嵌入。值为1表示将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## I2VGenXLPipelineOutput

### `class diffusers.pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/i2vgen_xl/pipeline_i2vgen_xl.py#L95)

```py
( frames: Union )
```

参数

+   `frames` (`List[np.ndarray]` 或 `torch.FloatTensor`) — 去噪帧（基本上是图像）的列表，作为形状为`(height, width, num_channels)`的NumPy数组或`torch`张量。列表的长度表示视频长度（帧数）。

图像到视频管道的输出类。
