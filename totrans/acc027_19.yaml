- en: Using Local SGD with ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Local SGDä¸ğŸ¤— Accelerate
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/local_sgd](https://huggingface.co/docs/accelerate/usage_guides/local_sgd)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/local_sgd](https://huggingface.co/docs/accelerate/usage_guides/local_sgd)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Local SGD is a technique for distributed training where gradients are not synchronized
    every step. Thus, each process updates its own version of the model weights and
    after a given number of steps these weights are synchronized by averaging across
    all processes. This improves communication efficiency and can lead to substantial
    training speed up especially when a computer lacks a faster interconnect such
    as NVLink. Unlike gradient accumulation (where improving communication efficiency
    requires increasing the effective batch size), Local SGD does not require changing
    a batch size or a learning rate / schedule. However, if necessary, Local SGD can
    be combined with gradient accumulation as well.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Local SGDæ˜¯ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯ï¼Œå…¶ä¸­æ¢¯åº¦ä¸æ˜¯æ¯ä¸€æ­¥åŒæ­¥çš„ã€‚å› æ­¤ï¼Œæ¯ä¸ªè¿›ç¨‹æ›´æ–°è‡ªå·±çš„æ¨¡å‹æƒé‡ç‰ˆæœ¬ï¼Œå¹¶åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤ä¹‹åé€šè¿‡å¯¹æ‰€æœ‰è¿›ç¨‹è¿›è¡Œå¹³å‡æ¥åŒæ­¥è¿™äº›æƒé‡ã€‚è¿™æé«˜äº†é€šä¿¡æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æœºç¼ºä¹æ›´å¿«çš„äº’è¿ï¼ˆå¦‚NVLinkï¼‰æ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä¸æ¢¯åº¦ç´¯ç§¯ä¸åŒï¼ˆå…¶ä¸­æé«˜é€šä¿¡æ•ˆç‡éœ€è¦å¢åŠ æœ‰æ•ˆæ‰¹é‡å¤§å°ï¼‰ï¼ŒLocal
    SGDä¸éœ€è¦æ›´æ”¹æ‰¹é‡å¤§å°æˆ–å­¦ä¹ ç‡/è°ƒåº¦ã€‚ä½†æ˜¯ï¼Œå¦‚æœå¿…è¦ï¼ŒLocal SGDä¹Ÿå¯ä»¥ä¸æ¢¯åº¦ç´¯ç§¯ç»“åˆä½¿ç”¨ã€‚
- en: In this tutorial you will see how to quickly setup Local SGD ğŸ¤— Accelerate. Compared
    to a standard Accelerate setup, this requires only two extra lines of code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•å¿«é€Ÿè®¾ç½®Local SGD ğŸ¤— Accelerateã€‚ä¸æ ‡å‡†çš„Accelerateè®¾ç½®ç›¸æ¯”ï¼Œè¿™åªéœ€è¦é¢å¤–ä¸¤è¡Œä»£ç ã€‚
- en: 'This example will use a very simplistic PyTorch training loop that performs
    gradient accumulation every two batches:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„PyTorchè®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸¤ä¸ªæ‰¹æ¬¡æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ç´¯ç§¯ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Converting it to ğŸ¤— Accelerate
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è½¬æ¢ä¸ºğŸ¤— Accelerate
- en: 'First the code shown earlier will be converted to use ğŸ¤— Accelerate with neither
    a LocalSGD or a gradient accumulation helper:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¹‹å‰æ˜¾ç¤ºçš„ä»£ç å°†è¢«è½¬æ¢ä¸ºä½¿ç”¨ğŸ¤— Accelerateï¼Œæ—¢ä¸ä½¿ç”¨LocalSGDä¹Ÿä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯åŠ©æ‰‹ï¼š
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Letting ğŸ¤— Accelerate handle model synchronization
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©ğŸ¤— Accelerateå¤„ç†æ¨¡å‹åŒæ­¥
- en: 'All that is left now is to let ğŸ¤— Accelerate handle model parameter synchronization
    **and** the gradient accumulation for us. For simplicity let us assume we need
    to synchronize every 8 steps. This is achieved by adding one `with LocalSGD` statement
    and one call `local_sgd.step()` after every optimizer step:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å‰©ä¸‹çš„å°±æ˜¯è®©ğŸ¤— Accelerateå¤„ç†æ¨¡å‹å‚æ•°åŒæ­¥**å’Œ**æ¢¯åº¦ç´¯ç§¯ã€‚ä¸ºç®€å•èµ·è§ï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬éœ€è¦æ¯8æ­¥åŒæ­¥ä¸€æ¬¡ã€‚é€šè¿‡æ·»åŠ ä¸€ä¸ª`with LocalSGD`è¯­å¥å’Œåœ¨æ¯ä¸ªä¼˜åŒ–å™¨æ­¥éª¤ä¹‹åè°ƒç”¨`local_sgd.step()`æ¥å®ç°ï¼š
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Under the hood, the Local SGD code **disables** automatic gradient synchornization
    (but accumulation still works as expected!). Instead it averages model parameters
    every `local_sgd_steps` steps (as well as in the end of the training loop).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åº•å±‚ï¼ŒLocal SGDä»£ç **ç¦ç”¨**äº†è‡ªåŠ¨æ¢¯åº¦åŒæ­¥ï¼ˆä½†ç´¯ç§¯ä»ç„¶æŒ‰é¢„æœŸå·¥ä½œï¼ï¼‰ã€‚ç›¸åï¼Œå®ƒæ¯`local_sgd_steps`æ­¥å¹³å‡æ¨¡å‹å‚æ•°ï¼ˆä»¥åŠåœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶ï¼‰ã€‚
- en: Limitations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™åˆ¶
- en: The current implementation works only with basic multi-GPU (or multi-CPU) training
    without, e.g., [DeepSpeed.](https://github.com/microsoft/DeepSpeed).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰å®ç°ä»…é€‚ç”¨äºåŸºæœ¬çš„å¤šGPUï¼ˆæˆ–å¤šCPUï¼‰è®­ç»ƒï¼Œä¸åŒ…æ‹¬ä¾‹å¦‚[DeepSpeed.](https://github.com/microsoft/DeepSpeed)ã€‚
- en: References
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: 'Although we are not aware of the true origins of this simple approach, the
    idea of local SGD is quite old and goes back to at least:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬ä¸æ¸…æ¥šè¿™ç§ç®€å•æ–¹æ³•çš„çœŸæ­£èµ·æºï¼Œä½†æœ¬åœ°SGDçš„æƒ³æ³•éå¸¸å¤è€ï¼Œè‡³å°‘å¯ä»¥è¿½æº¯åˆ°ï¼š
- en: 'Zhang, J., De Sa, C., Mitliagkas, I., & RÃ©, C. (2016). [Parallel SGD: When
    does averaging help?. arXiv preprint arXiv:1606.07365.](https://arxiv.org/abs/1606.07365)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhang, J., De Sa, C., Mitliagkas, I., & RÃ©, C. (2016). [Parallel SGD: When
    does averaging help?. arXiv preprint arXiv:1606.07365.](https://arxiv.org/abs/1606.07365)'
- en: We credit the term Local SGD to the following paper (but there might be earlier
    references we are not aware of).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†Local SGDè¿™ä¸ªæœ¯è¯­å½’åŠŸäºä»¥ä¸‹è®ºæ–‡ï¼ˆä½†å¯èƒ½æœ‰æˆ‘ä»¬ä¸çŸ¥é“çš„æ›´æ—©çš„å‚è€ƒæ–‡çŒ®ï¼‰ã€‚
- en: Stich, Sebastian Urban. [â€œLocal SGD Converges Fast and Communicates Little.â€
    ICLR 2019-International Conference on Learning Representations. No. CONF. 2019.](https://arxiv.org/abs/1805.09767)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Stich, Sebastian Urban. [â€œLocal SGD Converges Fast and Communicates Little.â€
    ICLR 2019-International Conference on Learning Representations. No. CONF. 2019.](https://arxiv.org/abs/1805.09767)
