# ImageGPT

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt)

## æ¦‚è¿°

ImageGPTæ¨¡å‹æ˜¯ç”±Mark Chenã€Alec Radfordã€Rewon Childã€Jeffrey Wuã€Heewoo Junã€David Luanã€Ilya Sutskeveråœ¨[åƒç´ ç”Ÿæˆé¢„è®­ç»ƒ](https://openai.com/blog/image-gpt)ä¸­æå‡ºçš„ã€‚ImageGPTï¼ˆiGPTï¼‰æ˜¯ä¸€ä¸ªç±»ä¼¼äºGPT-2çš„æ¨¡å‹ï¼Œè®­ç»ƒç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªåƒç´ å€¼ï¼Œä»è€Œå®ç°æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„å›¾åƒç”Ÿæˆã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*å—åˆ°è‡ªç„¶è¯­è¨€æ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶ç±»ä¼¼çš„æ¨¡å‹æ˜¯å¦å¯ä»¥å­¦ä¹ å¯¹å›¾åƒæœ‰ç”¨çš„è¡¨ç¤ºã€‚æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªåºåˆ—Transformerï¼Œè‡ªå›å½’åœ°é¢„æµ‹åƒç´ ï¼Œè€Œä¸è€ƒè™‘2Dè¾“å…¥ç»“æ„çš„çŸ¥è¯†ã€‚å°½ç®¡åœ¨æ²¡æœ‰æ ‡ç­¾çš„ä½åˆ†è¾¨ç‡ImageNetä¸Šè®­ç»ƒï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªGPT-2è§„æ¨¡çš„æ¨¡å‹é€šè¿‡çº¿æ€§æ¢æµ‹ã€å¾®è°ƒå’Œä½æ•°æ®åˆ†ç±»æ¥è¡¡é‡ï¼Œå­¦ä¹ åˆ°äº†å¼ºå¤§çš„å›¾åƒè¡¨ç¤ºã€‚åœ¨CIFAR-10ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡çº¿æ€§æ¢æµ‹å®ç°äº†96.3%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†ç›‘ç£çš„Wide ResNetï¼Œå¹¶ä¸”é€šè¿‡å®Œå…¨å¾®è°ƒå®ç°äº†99.0%çš„å‡†ç¡®ç‡ï¼Œä¸é¡¶çº§ç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒ¹æ•Œã€‚å½“ç”¨VQVAEç¼–ç æ›¿æ¢åƒç´ æ—¶ï¼Œæˆ‘ä»¬åœ¨ImageNetä¸Šä¹Ÿä¸è‡ªç›‘ç£åŸºå‡†ç«äº‰ï¼Œé€šè¿‡çº¿æ€§æ¢æµ‹æˆ‘ä»¬çš„ç‰¹å¾å®ç°äº†69.0%çš„top-1å‡†ç¡®ç‡ã€‚*

![drawing](../Images/7dfe07e48e8dd3472559e9232735486e.png) æ–¹æ³•æ¦‚è¿°ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)ã€‚

è¯¥æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ï¼ŒåŸºäº[æ­¤é—®é¢˜](https://github.com/openai/image-gpt/issues/7)ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/openai/image-gpt)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   ImageGPTä¸[GPT-2](gpt2)å‡ ä¹å®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ä½¿ç”¨äº†ä¸åŒçš„æ¿€æ´»å‡½æ•°ï¼ˆå³â€œquick geluâ€ï¼‰ï¼Œå¹¶ä¸”å±‚å½’ä¸€åŒ–å±‚ä¸å¯¹è¾“å…¥è¿›è¡Œå‡å€¼ä¸­å¿ƒåŒ–ã€‚ImageGPTä¹Ÿæ²¡æœ‰ç»‘å®šè¾“å…¥å’Œè¾“å‡ºåµŒå…¥ã€‚

+   ç”±äºTransformerçš„æ³¨æ„æœºåˆ¶çš„æ—¶é—´å’Œå†…å­˜éœ€æ±‚ä¸åºåˆ—é•¿åº¦çš„å¹³æ–¹æˆæ¯”ä¾‹å¢é•¿ï¼Œä½œè€…åœ¨è¾ƒå°çš„è¾“å…¥åˆ†è¾¨ç‡ï¼ˆå¦‚32x32å’Œ64x64ï¼‰ä¸Šé¢„è®­ç»ƒäº†ImageGPTã€‚ç„¶è€Œï¼Œå°†ä¸€ä¸ª32x32x3=3072ä¸ªä»¤ç‰Œçš„åºåˆ—ä»0..255è¾“å…¥åˆ°Transformerä»ç„¶æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œä½œè€…å¯¹ï¼ˆR,G,Bï¼‰åƒç´ å€¼åº”ç”¨äº†k=512çš„kå‡å€¼èšç±»ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªé•¿åº¦ä¸º32*32 = 1024çš„åºåˆ—ï¼Œä½†ç°åœ¨æ˜¯0..511èŒƒå›´å†…çš„æ•´æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ç¼©çŸ­åºåˆ—é•¿åº¦çš„åŒæ—¶å¢åŠ äº†åµŒå…¥çŸ©é˜µçš„å¤§å°ã€‚æ¢å¥è¯è¯´ï¼ŒImageGPTçš„è¯æ±‡å¤§å°ä¸º512ï¼Œå†åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šçš„â€œå¥å­å¼€å¤´â€ï¼ˆSOSï¼‰ä»¤ç‰Œï¼Œç”¨äºæ¯ä¸ªåºåˆ—çš„å¼€å¤´ã€‚å¯ä»¥ä½¿ç”¨[ImageGPTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€‚

+   å°½ç®¡ImageGPTå®Œå…¨æ— ç›‘ç£é¢„è®­ç»ƒï¼ˆå³ä¸ä½¿ç”¨ä»»ä½•æ ‡ç­¾ï¼‰ï¼Œä½†å®ƒç”Ÿæˆäº†ç›¸å½“æœ‰æ•ˆçš„å›¾åƒç‰¹å¾ï¼Œå¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»ã€‚ä½œè€…è¡¨æ˜ï¼Œç½‘ç»œä¸­é—´çš„ç‰¹å¾æœ€æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥ç›´æ¥ç”¨ä½œè®­ç»ƒçº¿æ€§æ¨¡å‹ï¼ˆä¾‹å¦‚sklearn logisticå›å½’æ¨¡å‹ï¼‰ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºâ€œçº¿æ€§æ¢æµ‹â€ã€‚é€šè¿‡é¦–å…ˆå°†å›¾åƒé€šè¿‡æ¨¡å‹è½¬å‘ï¼Œç„¶åæŒ‡å®š`output_hidden_states=True`ï¼Œç„¶ååœ¨ä»»ä½•æ‚¨å–œæ¬¢çš„å±‚ä¸Šå¯¹éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æ± åŒ–ï¼Œå¯ä»¥è½»æ¾è·å–ç‰¹å¾ã€‚

+   æˆ–è€…ï¼Œå¯ä»¥åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥å¾®è°ƒæ•´ä¸ªæ¨¡å‹ï¼Œç±»ä¼¼äºBERTã€‚ä¸ºæ­¤ï¼Œå¯ä»¥ä½¿ç”¨[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)ã€‚

+   ImageGPTæœ‰ä¸åŒçš„å¤§å°ï¼šæœ‰ImageGPT-smallã€ImageGPT-mediumå’ŒImageGPT-largeã€‚ä½œè€…è¿˜è®­ç»ƒäº†ä¸€ä¸ªXLå˜ä½“ï¼Œä½†ä»–ä»¬æ²¡æœ‰å‘å¸ƒã€‚å¤§å°ä¸Šçš„å·®å¼‚æ€»ç»“åœ¨ä»¥ä¸‹è¡¨ä¸­ï¼š

| **æ¨¡å‹å˜ä½“** | **æ·±åº¦** | **éšè—å¤§å°** | **è§£ç å™¨éšè—å¤§å°** | **å‚æ•°ï¼ˆMï¼‰** | **ImageNet-1k Top 1** |
| --- | --- | --- | --- | --- | --- |
| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |
| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |
| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |
| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |
| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |
| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |

## èµ„æº

ä¸€ä¸ªå®˜æ–¹çš„Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ImageGPTã€‚

å›¾åƒåˆ†ç±»

+   ImageGPTçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT)æ‰¾åˆ°ã€‚

+   [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚

+   å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨è¿™é‡Œï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## ImageGPTConfig

### `class transformers.ImageGPTConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)

```py
( vocab_size = 513 n_positions = 1024 n_embd = 512 n_layer = 24 n_head = 8 n_inner = None activation_function = 'quick_gelu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 scale_attn_weights = True use_cache = True tie_word_embeddings = False scale_attn_by_inverse_layer_idx = False reorder_and_upcast_attn = False **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512) â€” GPT-2æ¨¡å‹çš„è¯æ±‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)æˆ–`TFImageGPTModel`æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒä»¤ç‰Œæ•°é‡ã€‚ 

+   `n_positions` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º32*32) â€” æ­¤æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512æˆ–1024æˆ–2048ï¼‰ã€‚

+   `n_embd` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º512) â€” åµŒå…¥å’Œéšè—çŠ¶æ€çš„ç»´åº¦ã€‚

+   `n_layer` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º24) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `n_head` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º8) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `n_inner` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºNone) â€” å†…éƒ¨å‰é¦ˆå±‚çš„ç»´åº¦ã€‚`None`å°†å°†å…¶è®¾ç½®ä¸ºn_embdçš„4å€ã€‚

+   `activation_function` (`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"quick_gelu"`) â€” æ¿€æ´»å‡½æ•°ï¼ˆå¯ä»¥æ˜¯src/transformers/activations.pyä¸­å®šä¹‰çš„æ¿€æ´»å‡½æ•°ä¹‹ä¸€ï¼‰ã€‚é»˜è®¤ä¸º`"quick_gelu"`ã€‚

+   `resid_pdrop` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” åµŒå…¥ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `embd_pdrop` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” åµŒå…¥çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `attn_pdrop` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.1) â€” æ³¨æ„åŠ›çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `layer_norm_epsilon` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º1e-5) â€” åœ¨å±‚å½’ä¸€åŒ–å±‚ä¸­ä½¿ç”¨çš„epsilonã€‚

+   `initializer_range` (`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `scale_attn_weights` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`) â€” é€šè¿‡é™¤ä»¥sqrt(hidden_size)æ¥ç¼©æ”¾æ³¨æ„åŠ›æƒé‡ã€‚

+   `use_cache` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

+   `scale_attn_by_inverse_layer_idx` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦é¢å¤–æŒ‰`1 / layer_idx + 1`ç¼©æ”¾æ³¨æ„åŠ›æƒé‡ã€‚

+   `reorder_and_upcast_attn` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è®¡ç®—æ³¨æ„åŠ›ï¼ˆç‚¹ç§¯ï¼‰ä¹‹å‰ç¼©æ”¾é”®ï¼ˆKï¼‰ï¼Œå¹¶åœ¨ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒæ—¶å°†æ³¨æ„åŠ›ç‚¹ç§¯/softmaxå‘ä¸Šè½¬æ¢ä¸ºfloat()ã€‚

è¿™æ˜¯é…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)æˆ–`TFImageGPTModel`çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªGPT-2æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºImageGPT [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import ImageGPTConfig, ImageGPTModel

>>> # Initializing a ImageGPT configuration
>>> configuration = ImageGPTConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = ImageGPTModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ImageGPTFeatureExtractor

### `class transformers.ImageGPTFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## ImageGPTImageProcessor

### `class transformers.ImageGPTImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)

```py
( clusters: Union = None do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_normalize: bool = True do_color_quantize: bool = True **kwargs )
```

å‚æ•°

+   `clusters` (`np.ndarray`æˆ–`List[List[int]]`, *å¯é€‰*) â€” åœ¨é¢œè‰²é‡åŒ–æ—¶è¦ä½¿ç”¨çš„é¢œè‰²ç°‡ï¼Œå½¢çŠ¶ä¸º`(n_clusters, 3)`ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`clusters`è¦†ç›–ã€‚

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å°†å›¾åƒçš„å°ºå¯¸è°ƒæ•´ä¸º`(size["height"], size["width"])`ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_resize`è¦†ç›–ã€‚

+   `size` (`Dict[str, int]` *å¯é€‰*, é»˜è®¤ä¸º`{"height" -- 256, "width": 256}`): è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`size`è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º`Resampling.BILINEAR`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œè¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`resample`è¦†ç›–ã€‚

+   `do_normalize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å°†å›¾åƒåƒç´ å€¼å½’ä¸€åŒ–ä¸º[-1, 1]ä¹‹é—´ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_normalize`è¦†ç›–ã€‚

+   `do_color_quantize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œé¢œè‰²é‡åŒ–ã€‚å¯ä»¥è¢«`preprocess`ä¸­çš„`do_color_quantize`è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ªImageGPTå›¾åƒå¤„ç†å™¨ã€‚æ­¤å›¾åƒå¤„ç†å™¨å¯ç”¨äºå°†å›¾åƒè°ƒæ•´ä¸ºè¾ƒå°åˆ†è¾¨ç‡ï¼ˆå¦‚32x32æˆ–64x64ï¼‰ï¼Œå¯¹å…¶è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ€åè¿›è¡Œé¢œè‰²é‡åŒ–ï¼Œä»¥è·å¾—â€œåƒç´ å€¼â€ï¼ˆé¢œè‰²ç°‡ï¼‰åºåˆ—ã€‚

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_normalize: bool = None do_color_quantize: Optional = None clusters: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” è¦é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›åƒç´ å€¼ä»0åˆ°255çš„å•ä¸ªå›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_normalize=False`ã€‚

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå°ºå¯¸ã€‚

+   `resample` (`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.resample`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯æšä¸¾ `PILImageResampling` ä¸­çš„ä¸€ä¸ªï¼Œä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–

+   `do_color_quantize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.do_color_quantize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œé¢œè‰²é‡åŒ–ã€‚

+   `clusters` (`np.ndarray` æˆ– `List[List[int]]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `self.clusters`) â€” ç”¨äºå¯¹å›¾åƒè¿›è¡Œé‡åŒ–çš„èšç±»ï¼Œå½¢çŠ¶ä¸º `(n_clusters, 3)`ã€‚ä»…åœ¨ `do_color_quantize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `return_tensors` (`str` æˆ– `TensorType`ï¼Œ*å¯é€‰*) â€” è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®: è¿”å›ä¸€ä¸ª `np.ndarray` åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW` æˆ– `'tf'`: è¿”å›ç±»å‹ä¸º `tf.Tensor` çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.PYTORCH` æˆ– `'pt'`: è¿”å›ç±»å‹ä¸º `torch.Tensor` çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.NUMPY` æˆ– `'np'`: è¿”å›ç±»å‹ä¸º `np.ndarray` çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.JAX` æˆ– `'jax'`: è¿”å›ç±»å‹ä¸º `jax.numpy.ndarray` çš„æ‰¹æ¬¡ã€‚

+   `data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `ChannelDimension.FIRST`) â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚

    +   `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚ä»…åœ¨ `do_color_quantize` è®¾ç½®ä¸º `False` æ—¶æœ‰æ•ˆã€‚

+   `input_data_format` (`ChannelDimension` æˆ– `str`ï¼Œ*å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"` æˆ– `ChannelDimension.FIRST`: å›¾åƒæ ¼å¼ä¸º (é€šé“æ•°, é«˜åº¦, å®½åº¦)ã€‚

    +   `"channels_last"` æˆ– `ChannelDimension.LAST`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦, é€šé“æ•°)ã€‚

    +   `"none"` æˆ– `ChannelDimension.NONE`: å›¾åƒæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)ã€‚

é¢„å¤„ç†å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡ã€‚

## ImageGPTModel

### `class transformers.ImageGPTModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)

```py
( config: ImageGPTConfig )
```

å‚æ•°

+   `config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ ImageGPT æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºæ²¡æœ‰ç‰¹å®šå¤´éƒ¨çš„åŸå§‹éšè—çŠ¶æ€ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.LongTensor`ï¼‰ â€” å¦‚æœ `past_key_values` ä¸º `None`ï¼Œåˆ™ `input_ids_length` = `sequence_length`ï¼Œå¦åˆ™ä¸º `past_key_values[0][0].shape[-2]`ï¼ˆè¾“å…¥è¿‡å»å…³é”®å€¼çŠ¶æ€çš„åºåˆ—é•¿åº¦ï¼‰ã€‚è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™åªåº”å°†æœªè®¡ç®—å…¶è¿‡å»çš„ `input_ids` ä½œä¸º `input_ids` ä¼ é€’ã€‚

    å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`Tuple[Tuple[torch.Tensor]]`ï¼‰- åŒ…å«ç”±æ¨¡å‹è®¡ç®—çš„é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚å°†è¿‡å»ç»™å®šç»™è¯¥æ¨¡å‹çš„`input_ids`ä¸åº”ä½œä¸º`input_ids`ä¼ é€’ï¼Œå› ä¸ºå®ƒä»¬å·²ç»è¢«è®¡ç®—è¿‡ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ

    +   1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯position IDsï¼Ÿ](../glossary#position-ids)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨æ˜¯`masked`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`inputs_embeds`ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨çš„æ ‡ç­¾**å·²ç»è¢«ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ã€‚ç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆè¢«`masked`ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æƒ…å†µã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªè¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œå¦‚æœ`config.is_encoder_decoder=True`è¿˜æœ‰2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œä»¥åŠåœ¨äº¤å‰æ³¨æ„åŠ›å—ä¸­å¯é€‰åœ°ä½¿ç”¨`config.is_encoder_decoder=True`ï¼‰å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`å’Œ`config.add_cross_attention=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ImageGPTModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTModel.from_pretrained("openai/imagegpt-small")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## ImageGPTForCausalImageModeling

### `class transformers.ImageGPTForCausalImageModeling`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)

```py
( config: ImageGPTConfig )
```

å‚æ•°

+   `config`ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰ â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ImageGPTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥å±‚ç»‘å®šï¼‰ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L940)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” å¦‚æœ`past_key_values`ä¸º`None`ï¼Œåˆ™`input_ids_length` = `sequence_length`ï¼Œå¦åˆ™ä¸º`past_key_values[0][0].shape[-2]`ï¼ˆè¾“å…¥è¿‡å»é”®å€¼çŠ¶æ€çš„åºåˆ—é•¿åº¦ï¼‰ã€‚è¯æ±‡ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªåº”å°†å°šæœªè®¡ç®—å…¶è¿‡å»çš„`input_ids`ä½œä¸º`input_ids`ä¼ é€’ã€‚

    å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`Tuple[Tuple[torch.Tensor]]`ï¼‰â€” åŒ…å«ç”±æ¨¡å‹è®¡ç®—çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼ˆè¯·å‚é˜…ä¸‹é¢çš„`past_key_values`è¾“å‡ºï¼‰ã€‚å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚å·²å°†å…¶è¿‡å»ç»™äºˆæ­¤æ¨¡å‹çš„`input_ids`ä¸åº”ä½œä¸º`input_ids`ä¼ é€’ï¼Œå› ä¸ºå®ƒä»¬å·²ç»è®¡ç®—è¿‡ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«â€œæ©ç›–â€çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ

    +   1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯ä»¤ç‰Œç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`inputs_embeds`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” è¯­è¨€å»ºæ¨¡çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæ ‡ç­¾**åœ¨æ¨¡å‹å†…éƒ¨è¢«ç§»ä½**ï¼Œå³æ‚¨å¯ä»¥è®¾ç½®`labels = input_ids`ï¼Œç´¢å¼•åœ¨`[-100, 0, ..., config.vocab_size]`ä¸­é€‰æ‹©ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—åœ¨`[0, ..., config.vocab_size]`ä¸­çš„æ ‡ç­¾ã€‚

è¿”å›

[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–`torch.FloatTensor`å…ƒç»„

ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼Œåˆ™æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥åŒ…å«å„ç§å…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥å±‚çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›) â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder = True`æ—¶ç›¸å…³ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

[ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ImageGPTForCausalImageModeling
>>> import torch
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTForCausalImageModeling.from_pretrained("openai/imagegpt-small")
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> model.to(device)
>>> # unconditional generation of 8 images
>>> batch_size = 4
>>> context = torch.full((batch_size, 1), model.config.vocab_size - 1)  # initialize with SOS token
>>> context = context.to(device)
>>> output = model.generate(
...     input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40
... )

>>> clusters = image_processor.clusters
>>> height = image_processor.size["height"]
>>> width = image_processor.size["width"]

>>> samples = output[:, 1:].cpu().detach().numpy()
>>> samples_img = [
...     np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [height, width, 3]).astype(np.uint8) for s in samples
... ]  # convert color cluster tokens back to pixels
>>> f, axes = plt.subplots(1, batch_size, dpi=300)

>>> for img, ax in zip(samples_img, axes):
...     ax.axis("off")
...     ax.imshow(img)
```

## ImageGPTForImageClassification

### `class transformers.ImageGPTForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1076)

```py
( config: ImageGPTConfig )
```

å‚æ•°

+   `config`ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ImageGPTæ¨¡å‹å˜å‹å™¨é¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰ã€‚[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)å¯¹éšè—çŠ¶æ€è¿›è¡Œå¹³å‡æ± åŒ–ä»¥è¿›è¡Œåˆ†ç±»ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1093)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰â€” å¦‚æœ`past_key_values`ä¸º`None`ï¼Œåˆ™`input_ids_length` = `sequence_length`ï¼Œå¦åˆ™ä¸º`past_key_values[0][0].shape[-2]`ï¼ˆè¾“å…¥è¿‡å»é”®å€¼çŠ¶æ€çš„åºåˆ—é•¿åº¦ï¼‰ã€‚è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªåº”å°†æœªè®¡ç®—å…¶è¿‡å»çš„`input_ids`ä½œä¸º`input_ids`ä¼ é€’ã€‚

    å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`Tuple[Tuple[torch.Tensor]]`ï¼‰â€” åŒ…å«ç”±æ¨¡å‹è®¡ç®—çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼ˆè¯·å‚è§ä¸‹é¢çš„`past_key_values`è¾“å‡ºï¼‰ã€‚å¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚å°†è¿‡å»ç»™å®šç»™è¯¥æ¨¡å‹çš„`input_ids`ä¸åº”ä½œä¸º`input_ids`ä¼ é€’ï¼Œå› ä¸ºå®ƒä»¬å·²ç»è®¡ç®—è¿‡ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1è¡¨ç¤ºæœªè¢«`masked`çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºè¢«`masked`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0å¯¹åº”äºä¸€ä¸ª*å¥å­A*æ ‡è®°ï¼Œ

    +   1å¯¹åº”äºä¸€ä¸ª*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

`transformers.modeling_outputs.SequenceClassifierOutputWithPast`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.modeling_outputs.SequenceClassifierOutputWithPast`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœconfig.num_labels==1åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºåŠ ä¸Šæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, ImageGPTForImageClassification
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTForImageClassification.from_pretrained("openai/imagegpt-small")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
```
