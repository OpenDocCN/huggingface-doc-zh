# ImageGPT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/imagegpt)

## 概述

ImageGPT模型是由Mark Chen、Alec Radford、Rewon Child、Jeffrey Wu、Heewoo Jun、David Luan、Ilya Sutskever在[像素生成预训练](https://openai.com/blog/image-gpt)中提出的。ImageGPT（iGPT）是一个类似于GPT-2的模型，训练用于预测下一个像素值，从而实现无条件和有条件的图像生成。

论文摘要如下：

*受到自然语言无监督表示学习进展的启发，我们研究类似的模型是否可以学习对图像有用的表示。我们训练一个序列Transformer，自回归地预测像素，而不考虑2D输入结构的知识。尽管在没有标签的低分辨率ImageNet上训练，我们发现一个GPT-2规模的模型通过线性探测、微调和低数据分类来衡量，学习到了强大的图像表示。在CIFAR-10上，我们通过线性探测实现了96.3%的准确率，超过了监督的Wide ResNet，并且通过完全微调实现了99.0%的准确率，与顶级监督预训练模型相匹敌。当用VQVAE编码替换像素时，我们在ImageNet上也与自监督基准竞争，通过线性探测我们的特征实现了69.0%的top-1准确率。*

![drawing](../Images/7dfe07e48e8dd3472559e9232735486e.png) 方法概述。摘自[原始论文](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)。

该模型由[nielsr](https://huggingface.co/nielsr)贡献，基于[此问题](https://github.com/openai/image-gpt/issues/7)。原始代码可以在[这里](https://github.com/openai/image-gpt)找到。

## 使用提示

+   ImageGPT与[GPT-2](gpt2)几乎完全相同，唯一的区别是使用了不同的激活函数（即“quick gelu”），并且层归一化层不对输入进行均值中心化。ImageGPT也没有绑定输入和输出嵌入。

+   由于Transformer的注意机制的时间和内存需求与序列长度的平方成比例增长，作者在较小的输入分辨率（如32x32和64x64）上预训练了ImageGPT。然而，将一个32x32x3=3072个令牌的序列从0..255输入到Transformer仍然是不可行的。因此，作者对（R,G,B）像素值应用了k=512的k均值聚类。这样，我们只有一个长度为32*32 = 1024的序列，但现在是0..511范围内的整数。因此，我们在缩短序列长度的同时增加了嵌入矩阵的大小。换句话说，ImageGPT的词汇大小为512，再加上一个特殊的“句子开头”（SOS）令牌，用于每个序列的开头。可以使用[ImageGPTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor)来为模型准备图像。

+   尽管ImageGPT完全无监督预训练（即不使用任何标签），但它生成了相当有效的图像特征，可用于下游任务，如图像分类。作者表明，网络中间的特征最有效，并且可以直接用作训练线性模型（例如sklearn logistic回归模型）。这也被称为“线性探测”。通过首先将图像通过模型转发，然后指定`output_hidden_states=True`，然后在任何您喜欢的层上对隐藏状态进行平均池化，可以轻松获取特征。

+   或者，可以在下游数据集上进一步微调整个模型，类似于BERT。为此，可以使用[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)。

+   ImageGPT有不同的大小：有ImageGPT-small、ImageGPT-medium和ImageGPT-large。作者还训练了一个XL变体，但他们没有发布。大小上的差异总结在以下表中：

| **模型变体** | **深度** | **隐藏大小** | **解码器隐藏大小** | **参数（M）** | **ImageNet-1k Top 1** |
| --- | --- | --- | --- | --- | --- |
| MiT-b0 | [2, 2, 2, 2] | [32, 64, 160, 256] | 256 | 3.7 | 70.5 |
| MiT-b1 | [2, 2, 2, 2] | [64, 128, 320, 512] | 256 | 14.0 | 78.7 |
| MiT-b2 | [3, 4, 6, 3] | [64, 128, 320, 512] | 768 | 25.4 | 81.6 |
| MiT-b3 | [3, 4, 18, 3] | [64, 128, 320, 512] | 768 | 45.2 | 83.1 |
| MiT-b4 | [3, 8, 27, 3] | [64, 128, 320, 512] | 768 | 62.6 | 83.6 |
| MiT-b5 | [3, 6, 40, 3] | [64, 128, 320, 512] | 768 | 82.0 | 83.8 |

## 资源

一个官方的Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用ImageGPT。

图像分类

+   ImageGPT的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ImageGPT)找到。

+   [ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。

+   另请参阅：[图像分类任务指南](../tasks/image_classification)

如果您有兴趣提交资源以包含在这里，请随时打开一个Pull Request，我们将进行审核！资源应该展示一些新东西，而不是重复现有资源。

## ImageGPTConfig

### `class transformers.ImageGPTConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/configuration_imagegpt.py#L37)

```py
( vocab_size = 513 n_positions = 1024 n_embd = 512 n_layer = 24 n_head = 8 n_inner = None activation_function = 'quick_gelu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 scale_attn_weights = True use_cache = True tie_word_embeddings = False scale_attn_by_inverse_layer_idx = False reorder_and_upcast_attn = False **kwargs )
```

参数

+   `vocab_size` (`int`，*可选*，默认为512) — GPT-2模型的词汇大小。定义了在调用[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)或`TFImageGPTModel`时可以表示的不同令牌数量。 

+   `n_positions` (`int`，*可选*，默认为32*32) — 此模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。

+   `n_embd` (`int`，*可选*，默认为512) — 嵌入和隐藏状态的维度。

+   `n_layer` (`int`，*可选*，默认为24) — Transformer编码器中的隐藏层数。

+   `n_head` (`int`，*可选*，默认为8) — Transformer编码器中每个注意力层的注意力头数。

+   `n_inner` (`int`，*可选*，默认为None) — 内部前馈层的维度。`None`将将其设置为n_embd的4倍。

+   `activation_function` (`str`，*可选*，默认为`"quick_gelu"`) — 激活函数（可以是src/transformers/activations.py中定义的激活函数之一）。默认为`"quick_gelu"`。

+   `resid_pdrop` (`float`，*可选*，默认为0.1) — 嵌入、编码器和池化器中所有全连接层的丢失概率。

+   `embd_pdrop` (`int`，*可选*，默认为0.1) — 嵌入的丢失比率。

+   `attn_pdrop` (`float`，*可选*，默认为0.1) — 注意力的丢失比率。

+   `layer_norm_epsilon` (`float`，*可选*，默认为1e-5) — 在层归一化层中使用的epsilon。

+   `initializer_range` (`float`，*可选*，默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `scale_attn_weights` (`bool`，*可选*，默认为`True`) — 通过除以sqrt(hidden_size)来缩放注意力权重。

+   `use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `scale_attn_by_inverse_layer_idx` (`bool`, *可选*, 默认为`False`) — 是否额外按`1 / layer_idx + 1`缩放注意力权重。

+   `reorder_and_upcast_attn` (`bool`, *可选*, 默认为`False`) — 是否在计算注意力（点积）之前缩放键（K），并在使用混合精度训练时将注意力点积/softmax向上转换为float()。

这是配置类，用于存储[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)或`TFImageGPTModel`的配置。根据指定的参数实例化一个GPT-2模型，定义模型架构。使用默认值实例化配置将产生类似于ImageGPT [openai/imagegpt-small](https://huggingface.co/openai/imagegpt-small)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import ImageGPTConfig, ImageGPTModel

>>> # Initializing a ImageGPT configuration
>>> configuration = ImageGPTConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = ImageGPTModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ImageGPTFeatureExtractor

### `class transformers.ImageGPTFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

预处理一张图像或一批图像。

## ImageGPTImageProcessor

### `class transformers.ImageGPTImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L58)

```py
( clusters: Union = None do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_normalize: bool = True do_color_quantize: bool = True **kwargs )
```

参数

+   `clusters` (`np.ndarray`或`List[List[int]]`, *可选*) — 在颜色量化时要使用的颜色簇，形状为`(n_clusters, 3)`。可以被`preprocess`中的`clusters`覆盖。

+   `do_resize` (`bool`, *可选*, 默认为`True`) — 是否将图像的尺寸调整为`(size["height"], size["width"])`。可以被`preprocess`中的`do_resize`覆盖。

+   `size` (`Dict[str, int]` *可选*, 默认为`{"height" -- 256, "width": 256}`): 调整大小后的图像尺寸。可以被`preprocess`中的`size`覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为`Resampling.BILINEAR`) — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`中的`resample`覆盖。

+   `do_normalize` (`bool`, *可选*, 默认为`True`) — 是否将图像像素值归一化为[-1, 1]之间。可以被`preprocess`中的`do_normalize`覆盖。

+   `do_color_quantize` (`bool`, *可选*, 默认为`True`) — 是否对图像进行颜色量化。可以被`preprocess`中的`do_color_quantize`覆盖。

构建一个ImageGPT图像处理器。此图像处理器可用于将图像调整为较小分辨率（如32x32或64x64），对其进行归一化，最后进行颜色量化，以获得“像素值”（颜色簇）序列。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/image_processing_imagegpt.py#L175)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_normalize: bool = None do_color_quantize: Optional = None clusters: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望像素值从0到255的单个图像或图像批次。如果传入像素值在0到1之间的图像，请设置`do_normalize=False`。

+   `do_resize` (`bool`, *可选*, 默认为`self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为`self.size`) — 调整大小后的图像尺寸。

+   `resample` (`int`，*可选*，默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。可以是枚举 `PILImageResampling` 中的一个，仅在 `do_resize` 设置为 `True` 时有效。

+   `do_normalize` (`bool`，*可选*，默认为 `self.do_normalize`) — 是否对图像进行归一化

+   `do_color_quantize` (`bool`，*可选*，默认为 `self.do_color_quantize`) — 是否对图像进行颜色量化。

+   `clusters` (`np.ndarray` 或 `List[List[int]]`，*可选*，默认为 `self.clusters`) — 用于对图像进行量化的聚类，形状为 `(n_clusters, 3)`。仅在 `do_color_quantize` 设置为 `True` 时有效。

+   `return_tensors` (`str` 或 `TensorType`，*可选*) — 返回的张量类型。可以是以下之一：

    +   未设置: 返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`: 返回类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`: 返回类型为 `jax.numpy.ndarray` 的批次。

+   `data_format` (`ChannelDimension` 或 `str`，*可选*，默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。

    +   `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。仅在 `do_color_quantize` 设置为 `False` 时有效。

+   `input_data_format` (`ChannelDimension` 或 `str`，*可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像格式为 (高度, 宽度)。

预处理图像或图像批次。

## ImageGPTModel

### `class transformers.ImageGPTModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L608)

```py
( config: ImageGPTConfig )
```

参数

+   `config` ([ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 ImageGPT 模型变压器，输出没有特定头部的原始隐藏状态。

该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L645)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`） — 如果 `past_key_values` 为 `None`，则 `input_ids_length` = `sequence_length`，否则为 `past_key_values[0][0].shape[-2]`（输入过去关键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了 `past_key_values`，则只应将未计算其过去的 `input_ids` 作为 `input_ids` 传递。

    可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取索引。有关详细信息，请参见[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预先计算的隐藏状态（注意力块中的键和值），可以用于加速顺序解码。将过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    [什么是attention masks？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是position IDs？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部未被`masked`，

    +   0表示头部是`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

    如果使用`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（请参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于语言建模的标签。请注意，模型内部的标签**已经被移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（被`masked`），损失仅计算标签在`[0, ..., config.vocab_size]`中的情况。

返回

[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入的各种元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的输出的隐藏状态序列。

    如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及在交叉注意力块中可选地使用`config.is_encoder_decoder=True`）可用于加速顺序解码（查看`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

[ImageGPTModel](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ImageGPTModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTModel.from_pretrained("openai/imagegpt-small")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## ImageGPTForCausalImageModeling

### `class transformers.ImageGPTForCausalImageModeling`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L876)

```py
( config: ImageGPTConfig )
```

参数

+   `config`（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ImageGPT模型变压器，顶部带有语言建模头（线性层，其权重与输入嵌入层绑定）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入大小，修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L940)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length` = `sequence_length`，否则为`past_key_values[0][0].shape[-2]`（输入过去键值状态的序列长度）。词汇中输入序列标记的索引。

    如果使用`past_key_values`，则只应将尚未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取索引。有关详细信息，请参阅[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参阅下面的`past_key_values`输出）。可用于加速顺序解码。已将其过去给予此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被“掩盖”的标记为1，

    +   对于被“掩盖”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部无效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被“掩盖”。

    +   0表示头部被“掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

    如果使用`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（请参阅`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`的形状为`(batch_size, sequence_length)`，*optional*) — 语言建模的标签。请注意，标签**在模型内部被移位**，即您可以设置`labels = input_ids`，索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size]`中的标签。

返回

[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或`torch.FloatTensor`元组

一个[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入包含各种元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`的形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个层的输出）的形状为`(batch_size, sequence_length, hidden_size)`。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — `torch.FloatTensor`元组（每层一个）的形状为`(batch_size, num_heads, sequence_length, sequence_length)`。

    在注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`torch.FloatTensor`元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型用于编码器-解码器设置，则相关。仅在`config.is_decoder = True`时相关。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

[ImageGPTForCausalImageModeling](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ImageGPTForCausalImageModeling
>>> import torch
>>> import matplotlib.pyplot as plt
>>> import numpy as np

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTForCausalImageModeling.from_pretrained("openai/imagegpt-small")
>>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
>>> model.to(device)
>>> # unconditional generation of 8 images
>>> batch_size = 4
>>> context = torch.full((batch_size, 1), model.config.vocab_size - 1)  # initialize with SOS token
>>> context = context.to(device)
>>> output = model.generate(
...     input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40
... )

>>> clusters = image_processor.clusters
>>> height = image_processor.size["height"]
>>> width = image_processor.size["width"]

>>> samples = output[:, 1:].cpu().detach().numpy()
>>> samples_img = [
...     np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [height, width, 3]).astype(np.uint8) for s in samples
... ]  # convert color cluster tokens back to pixels
>>> f, axes = plt.subplots(1, batch_size, dpi=300)

>>> for img, ax in zip(samples_img, axes):
...     ax.axis("off")
...     ax.imshow(img)
```

## ImageGPTForImageClassification

### `class transformers.ImageGPTForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1076)

```py
( config: ImageGPTConfig )
```

参数

+   `config`（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ImageGPT模型变压器顶部带有图像分类头（线性层）。[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)对隐藏状态进行平均池化以进行分类。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/imagegpt/modeling_imagegpt.py#L1093)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs: Any ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length` = `sequence_length`，否则为`past_key_values[0][0].shape[-2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取索引。有关详细信息，请参阅[ImageGPTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参见下面的`past_key_values`输出）。可用于加速顺序解码。将过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于一个*句子A*标记，

    +   1对应于一个*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果使用了`past_key_values`，则可能只需输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels`（`torch.LongTensor`，形状为`(batch_size,)`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[ImageGPTConfig](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTConfig)）和输入的各种元素。

+   `loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回）— 分类（如果config.num_labels==1则为回归）损失。

+   `logits`（`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`）— 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[ImageGPTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ImageGPTForImageClassification
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("openai/imagegpt-small")
>>> model = ImageGPTForImageClassification.from_pretrained("openai/imagegpt-small")

>>> inputs = image_processor(images=image, return_tensors="pt")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
```
