["```py\nimport torch\nfrom peft import inject_adapter_in_model, LoraConfig\n\nclass DummyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(10, 10)\n        self.linear = torch.nn.Linear(10, 10)\n        self.lm_head = torch.nn.Linear(10, 10)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        x = self.linear(x)\n        x = self.lm_head(x)\n        return x\n\nlora_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    target_modules=[\"linear\"],\n)\n\nmodel = DummyModel()\nmodel = inject_adapter_in_model(lora_config, model)\n\ndummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\ndummy_outputs = model(dummy_inputs)\n```", "```py\nDummyModel(\n  (embedding): Embedding(10, 10)\n  (linear): Linear(\n    in_features=10, out_features=10, bias=True\n    (lora_dropout): ModuleDict(\n      (default): Dropout(p=0.1, inplace=False)\n    )\n    (lora_A): ModuleDict(\n      (default): Linear(in_features=10, out_features=64, bias=False)\n    )\n    (lora_B): ModuleDict(\n      (default): Linear(in_features=64, out_features=10, bias=False)\n    )\n    (lora_embedding_A): ParameterDict()\n    (lora_embedding_B): ParameterDict()\n  )\n  (lm_head): Linear(in_features=10, out_features=10, bias=True)\n)\n```", "```py\nfrom peft import get_peft_model_state_dict\n\npeft_state_dict = get_peft_model_state_dict(model)\nprint(peft_state_dict)\n```"]