- en: What are the policy-based methods?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods](https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal of Reinforcement learning is to **find the optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    that will maximize the expected cumulative reward**. Because Reinforcement Learning
    is based on the *reward hypothesis*: **all goals can be described as the maximization
    of the expected cumulative reward.**'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in a soccer game (where you’re going to train the agents in two
    units), the goal is to win the game. We can describe this goal in reinforcement
    learning as **maximizing the number of goals scored** (when the ball crosses the
    goal line) into your opponent’s soccer goals. And **minimizing the number of goals
    in your soccer goals**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Soccer](../Images/a9c2200aa04bae4394f998a72fe3492d.png)'
  prefs: []
  type: TYPE_IMG
- en: Value-based, Policy-based, and Actor-critic methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first unit, we saw two methods to find (or, most of the time, approximate)
    this optimal policy<math><semantics><mrow><msup><mi>π</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation
    encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  prefs: []
  type: TYPE_NORMAL
- en: In *value-based methods*, we learn a value function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is that an optimal value function leads to an optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our objective is to **minimize the loss between the predicted and target value**
    to approximate the true action-value function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a policy, but it’s implicit since it **is generated directly from the
    value function**. For instance, in Q-Learning, we used an (epsilon-)greedy policy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, in *policy-based methods*, we directly learn to approximate<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    without having to learn a value function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is **to parameterize the policy**. For instance, using a neural network<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​, this
    policy will output a probability distribution over actions (stochastic policy).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![stochastic policy](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Our objective then is **to maximize the performance of the parameterized policy
    using gradient ascent**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To do that, we control the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that will
    affect the distribution of actions over a state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Policy based](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
  prefs: []
  type: TYPE_IMG
- en: Next time, we’ll study the *actor-critic* method, which is a combination of
    value-based and policy-based methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, thanks to policy-based methods, we can directly optimize our policy<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ to
    output a probability distribution over actions<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math>πθ​(a∣s)
    that leads to the best cumulative return. To do that, we define an objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ), that
    is, the expected cumulative reward, and we **want to find the value<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that maximizes
    this objective function**.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between policy-based and policy-gradient methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Policy-gradient methods, what we’re going to study in this unit, is a subclass
    of policy-based methods. In policy-based methods, the optimization is most of
    the time *on-policy* since for each update, we only use data (trajectories) collected
    **by our most recent version of**<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between these two methods **lies on how we optimize the parameter**<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ:'
  prefs: []
  type: TYPE_NORMAL
- en: In *policy-based methods*, we search directly for the optimal policy. We can
    optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ
    **indirectly** by maximizing the local approximation of the objective function
    with techniques like hill climbing, simulated annealing, or evolution strategies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *policy-gradient methods*, because it is a subclass of the policy-based methods,
    we search directly for the optimal policy. But we optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ **directly**
    by performing the gradient ascent on the performance of the objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before diving more into how policy-gradient methods work (the objective function,
    policy gradient theorem, gradient ascent, etc.), let’s study the advantages and
    disadvantages of policy-based methods.
  prefs: []
  type: TYPE_NORMAL
