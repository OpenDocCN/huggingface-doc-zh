- en: What are the policy-based methods?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods](https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal of Reinforcement learning is to **find the optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    that will maximize the expected cumulative reward**. Because Reinforcement Learning
    is based on the *reward hypothesis*: **all goals can be described as the maximization
    of the expected cumulative reward.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in a soccer game (where you’re going to train the agents in two
    units), the goal is to win the game. We can describe this goal in reinforcement
    learning as **maximizing the number of goals scored** (when the ball crosses the
    goal line) into your opponent’s soccer goals. And **minimizing the number of goals
    in your soccer goals**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![Soccer](../Images/a9c2200aa04bae4394f998a72fe3492d.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: Value-based, Policy-based, and Actor-critic methods
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first unit, we saw two methods to find (or, most of the time, approximate)
    this optimal policy<math><semantics><mrow><msup><mi>π</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation
    encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In *value-based methods*, we learn a value function.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is that an optimal value function leads to an optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Our objective is to **minimize the loss between the predicted and target value**
    to approximate the true action-value function.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a policy, but it’s implicit since it **is generated directly from the
    value function**. For instance, in Q-Learning, we used an (epsilon-)greedy policy.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, in *policy-based methods*, we directly learn to approximate<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    without having to learn a value function.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is **to parameterize the policy**. For instance, using a neural network<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​, this
    policy will output a probability distribution over actions (stochastic policy).
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![stochastic policy](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Our objective then is **to maximize the performance of the parameterized policy
    using gradient ascent**.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To do that, we control the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that will
    affect the distribution of actions over a state.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Policy based](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Next time, we’ll study the *actor-critic* method, which is a combination of
    value-based and policy-based methods.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consequently, thanks to policy-based methods, we can directly optimize our policy<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ to
    output a probability distribution over actions<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math>πθ​(a∣s)
    that leads to the best cumulative return. To do that, we define an objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ), that
    is, the expected cumulative reward, and we **want to find the value<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that maximizes
    this objective function**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助基于策略的方法，我们可以直接优化我们的策略<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ 以输出一个动作的概率分布<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math>πθ​(a∣s)，从而获得最佳的累积回报。为此，我们定义一个目标函数<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)，即预期的累积奖励，并且**希望找到最大化这个目标函数的值<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ**。
- en: The difference between policy-based and policy-gradient methods
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于策略和策略梯度方法之间的区别
- en: Policy-gradient methods, what we’re going to study in this unit, is a subclass
    of policy-based methods. In policy-based methods, the optimization is most of
    the time *on-policy* since for each update, we only use data (trajectories) collected
    **by our most recent version of**<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法，也就是我们将在本单元中学习的方法，是基于策略的方法的一个子类。在基于策略的方法中，优化大部分时间都是*on-policy*的，因为对于每次更新，我们只使用由我们最新版本的<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ 收集的数据（轨迹）。
- en: 'The difference between these two methods **lies on how we optimize the parameter**<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法之间的区别**在于我们如何优化参数**<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ：
- en: In *policy-based methods*, we search directly for the optimal policy. We can
    optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ
    **indirectly** by maximizing the local approximation of the objective function
    with techniques like hill climbing, simulated annealing, or evolution strategies.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*基于策略的方法*中，我们直接寻找最优策略。我们可以通过像爬山、模拟退火或进化策略等技术来最大化目标函数的局部近似来**间接**优化参数<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ。
- en: In *policy-gradient methods*, because it is a subclass of the policy-based methods,
    we search directly for the optimal policy. But we optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ **directly**
    by performing the gradient ascent on the performance of the objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*策略梯度方法*中，因为它是基于策略的方法的一个子类，我们直接寻找最优策略。但我们通过在目标函数<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的性能上执行梯度上升来**直接**优化参数<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ。
- en: Before diving more into how policy-gradient methods work (the objective function,
    policy gradient theorem, gradient ascent, etc.), let’s study the advantages and
    disadvantages of policy-based methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在更深入研究策略梯度方法的工作原理（目标函数、策略梯度定理、梯度上升等）之前，让我们先研究基于策略的方法的优缺点。
