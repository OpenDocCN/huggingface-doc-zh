- en: What are the policy-based methods?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是基于策略的方法？
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods](https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods](https://huggingface.co/learn/deep-rl-course/unit4/what-are-policy-based-methods)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal of Reinforcement learning is to **find the optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    that will maximize the expected cumulative reward**. Because Reinforcement Learning
    is based on the *reward hypothesis*: **all goals can be described as the maximization
    of the expected cumulative reward.**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的主要目标是找到将最大化预期累积奖励的最优策略π∗。因为强化学习基于“奖励假设”：所有目标都可以描述为最大化预期累积奖励。
- en: For instance, in a soccer game (where you’re going to train the agents in two
    units), the goal is to win the game. We can describe this goal in reinforcement
    learning as **maximizing the number of goals scored** (when the ball crosses the
    goal line) into your opponent’s soccer goals. And **minimizing the number of goals
    in your soccer goals**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在足球比赛中（您将在两个单位中训练代理人），目标是赢得比赛。我们可以将这个目标描述为强化学习中最大化进球数（当球越过球门线时）到对手的足球门中，并最小化自己的足球门中的进球数。
- en: '![Soccer](../Images/a9c2200aa04bae4394f998a72fe3492d.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![足球](../Images/a9c2200aa04bae4394f998a72fe3492d.png)'
- en: Value-based, Policy-based, and Actor-critic methods
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于值、基于策略和演员-评论家方法
- en: In the first unit, we saw two methods to find (or, most of the time, approximate)
    this optimal policy<math><semantics><mrow><msup><mi>π</mi><mo lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation
    encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个单元中，我们看到了两种找到（或者大多数情况下，近似）最优策略π∗的方法。
- en: In *value-based methods*, we learn a value function.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于值的方法中，我们学习一个值函数。
- en: The idea is that an optimal value function leads to an optimal policy<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗.
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个想法是，一个最优值函数会导致一个最优策略π∗。
- en: Our objective is to **minimize the loss between the predicted and target value**
    to approximate the true action-value function.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的目标是最小化预测值和目标值之间的损失，以逼近真实的动作值函数。
- en: We have a policy, but it’s implicit since it **is generated directly from the
    value function**. For instance, in Q-Learning, we used an (epsilon-)greedy policy.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个政策，但它是隐含的，因为它直接从值函数生成。例如，在Q-Learning中，我们使用了一个（epsilon-）贪婪策略。
- en: On the other hand, in *policy-based methods*, we directly learn to approximate<math><semantics><mrow><msup><mi>π</mi><mo
    lspace="0em" rspace="0em">∗</mo></msup></mrow><annotation encoding="application/x-tex">\pi^{*}</annotation></semantics></math>π∗
    without having to learn a value function.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，在基于策略的方法中，我们直接学习近似π∗，而无需学习值函数。
- en: The idea is **to parameterize the policy**. For instance, using a neural network<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​, this
    policy will output a probability distribution over actions (stochastic policy).
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个想法是对策略进行参数化。例如，使用神经网络πθ，这个策略将输出动作的概率分布（随机策略）。
- en: '![stochastic policy](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![随机策略](../Images/9123df7ebedfa0c5bc669c2d2531968f.png)'
- en: Our objective then is **to maximize the performance of the parameterized policy
    using gradient ascent**.
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的目标是通过梯度上升来最大化参数化策略的性能。
- en: To do that, we control the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that will
    affect the distribution of actions over a state.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们控制将影响状态上动作分布的参数θ。
- en: '![Policy based](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![基于策略](../Images/7b4b24746a62f4244cc0e64f74bdaef3.png)'
- en: Next time, we’ll study the *actor-critic* method, which is a combination of
    value-based and policy-based methods.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下次，我们将学习演员-评论家方法，这是基于值和基于策略方法的结合。
- en: Consequently, thanks to policy-based methods, we can directly optimize our policy<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ to
    output a probability distribution over actions<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math>πθ​(a∣s)
    that leads to the best cumulative return. To do that, we define an objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ), that
    is, the expected cumulative reward, and we **want to find the value<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ that maximizes
    this objective function**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助基于策略的方法，我们可以直接优化我们的策略<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ 以输出一个动作的概率分布<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo
    stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_\theta(a|s)</annotation></semantics></math>πθ​(a∣s)，从而获得最佳的累积回报。为此，我们定义一个目标函数<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)，即预期的累积奖励，并且**希望找到最大化这个目标函数的值<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ**。
- en: The difference between policy-based and policy-gradient methods
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于策略和策略梯度方法之间的区别
- en: Policy-gradient methods, what we’re going to study in this unit, is a subclass
    of policy-based methods. In policy-based methods, the optimization is most of
    the time *on-policy* since for each update, we only use data (trajectories) collected
    **by our most recent version of**<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法，也就是我们将在本单元中学习的方法，是基于策略的方法的一个子类。在基于策略的方法中，优化大部分时间都是*on-policy*的，因为对于每次更新，我们只使用由我们最新版本的<math><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation
    encoding="application/x-tex">\pi_\theta</annotation></semantics></math>πθ​ 收集的数据（轨迹）。
- en: 'The difference between these two methods **lies on how we optimize the parameter**<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法之间的区别**在于我们如何优化参数**<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ：
- en: In *policy-based methods*, we search directly for the optimal policy. We can
    optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math>θ
    **indirectly** by maximizing the local approximation of the objective function
    with techniques like hill climbing, simulated annealing, or evolution strategies.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*基于策略的方法*中，我们直接寻找最优策略。我们可以通过像爬山、模拟退火或进化策略等技术来最大化目标函数的局部近似来**间接**优化参数<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ。
- en: In *policy-gradient methods*, because it is a subclass of the policy-based methods,
    we search directly for the optimal policy. But we optimize the parameter<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ **directly**
    by performing the gradient ascent on the performance of the objective function<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*策略梯度方法*中，因为它是基于策略的方法的一个子类，我们直接寻找最优策略。但我们通过在目标函数<math><semantics><mrow><mi>J</mi><mo
    stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">J(\theta)</annotation></semantics></math>J(θ)的性能上执行梯度上升来**直接**优化参数<math><semantics><mrow><mi>θ</mi></mrow><annotation
    encoding="application/x-tex">\theta</annotation></semantics></math>θ。
- en: Before diving more into how policy-gradient methods work (the objective function,
    policy gradient theorem, gradient ascent, etc.), let’s study the advantages and
    disadvantages of policy-based methods.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在更深入研究策略梯度方法的工作原理（目标函数、策略梯度定理、梯度上升等）之前，让我们先研究基于策略的方法的优缺点。
