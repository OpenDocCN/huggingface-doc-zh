# DiffEdit

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/using-diffusers/diffedit`](https://huggingface.co/docs/diffusers/using-diffusers/diffedit)

å›¾åƒç¼–è¾‘é€šå¸¸éœ€è¦æä¾›è¦ç¼–è¾‘çš„åŒºåŸŸçš„è’™ç‰ˆã€‚DiffEdit æ ¹æ®æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨ç”Ÿæˆè’™ç‰ˆï¼Œä»è€Œæ›´å®¹æ˜“åœ°åˆ›å»ºè’™ç‰ˆè€Œæ— éœ€å›¾åƒç¼–è¾‘è½¯ä»¶ã€‚DiffEdit ç®—æ³•åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š

1.  æ‰©æ•£æ¨¡å‹å¯¹å—æŸäº›æŸ¥è¯¢æ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬æ¡ä»¶çš„å›¾åƒè¿›è¡Œå»å™ªï¼Œä¸ºå›¾åƒçš„ä¸åŒåŒºåŸŸäº§ç”Ÿä¸åŒçš„å™ªå£°ä¼°è®¡ï¼›è¿™ç§å·®å¼‚ç”¨äºæ¨æ–­ä¸€ä¸ªè’™ç‰ˆï¼Œä»¥è¯†åˆ«éœ€è¦æ›´æ”¹ä»¥åŒ¹é…æŸ¥è¯¢æ–‡æœ¬çš„å›¾åƒåŒºåŸŸ

1.  è¾“å…¥å›¾åƒä½¿ç”¨ DDIM ç¼–ç åˆ°æ½œç©ºé—´

1.  æ½œå˜é‡é€šè¿‡æ‰©æ•£æ¨¡å‹è§£ç ï¼Œè¯¥æ¨¡å‹ä»¥æ–‡æœ¬æŸ¥è¯¢ä¸ºæ¡ä»¶ï¼Œä½¿ç”¨è’™ç‰ˆä½œä¸ºæŒ‡å¯¼ï¼Œä½¿è’™ç‰ˆå¤–çš„åƒç´ ä¿æŒä¸è¾“å…¥å›¾åƒç›¸åŒ

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ DiffEdit ç¼–è¾‘å›¾åƒï¼Œè€Œæ— éœ€æ‰‹åŠ¨åˆ›å»ºè’™ç‰ˆã€‚

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹åº“ï¼š

```py
# uncomment to install the necessary libraries in Colab
#!pip install -q diffusers transformers accelerate
```

StableDiffusionDiffEditPipeline éœ€è¦ä¸€ä¸ªå›¾åƒè’™ç‰ˆå’Œä¸€ç»„éƒ¨åˆ†åè½¬çš„æ½œå˜é‡ã€‚å›¾åƒè’™ç‰ˆæ˜¯ä» generate_mask()å‡½æ•°ç”Ÿæˆçš„ï¼Œå¹¶åŒ…æ‹¬ä¸¤ä¸ªå‚æ•°ï¼Œ`source_prompt`å’Œ`target_prompt`ã€‚è¿™äº›å‚æ•°ç¡®å®šè¦åœ¨å›¾åƒä¸­è¿›è¡Œç¼–è¾‘çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³å°†ä¸€ä¸ªè£…æ»¡*æ°´æœ*çš„ç¢—æ›´æ”¹ä¸ºä¸€ä¸ªè£…æ»¡*æ¢¨*çš„ç¢—ï¼Œåˆ™ï¼š

```py
source_prompt = "a bowl of fruits"
target_prompt = "a bowl of pears"
```

éƒ¨åˆ†åè½¬çš„æ½œå˜é‡æ˜¯é€šè¿‡ invert()å‡½æ•°ç”Ÿæˆçš„ï¼Œé€šå¸¸æœ€å¥½åŒ…å«ä¸€ä¸ªæè¿°å›¾åƒçš„`prompt`æˆ–*æ ‡é¢˜*ï¼Œä»¥å¸®åŠ©æŒ‡å¯¼åå‘æ½œå˜é‡é‡‡æ ·è¿‡ç¨‹ã€‚æ ‡é¢˜é€šå¸¸å¯ä»¥æ˜¯æ‚¨çš„`source_prompt`ï¼Œä½†ä¹Ÿå¯ä»¥å°è¯•å…¶ä»–æ–‡æœ¬æè¿°ï¼

è®©æˆ‘ä»¬åŠ è½½ç®¡é“ã€è°ƒåº¦å™¨ã€é€†è°ƒåº¦å™¨ï¼Œå¹¶å¯ç”¨ä¸€äº›ä¼˜åŒ–ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼š

```py
import torch
from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16,
    safety_checker=None,
    use_safetensors=True,
)
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()
```

åŠ è½½è¦ç¼–è¾‘çš„å›¾åƒï¼š

```py
from diffusers.utils import load_image, make_image_grid

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
raw_image
```

ä½¿ç”¨ generate_mask()å‡½æ•°ç”Ÿæˆå›¾åƒè’™ç‰ˆã€‚æ‚¨éœ€è¦ä¼ é€’`source_prompt`å’Œ`target_prompt`ä»¥æŒ‡å®šè¦åœ¨å›¾åƒä¸­è¿›è¡Œç¼–è¾‘çš„å†…å®¹ï¼š

```py
from PIL import Image

source_prompt = "a bowl of fruits"
target_prompt = "a basket of pears"
mask_image = pipeline.generate_mask(
    image=raw_image,
    source_prompt=source_prompt,
    target_prompt=target_prompt,
)
Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºåè½¬çš„æ½œå˜é‡å¹¶ä¼ é€’ä¸€ä¸ªæè¿°å›¾åƒçš„æ ‡é¢˜ï¼š

```py
inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents
```

æœ€åï¼Œå°†å›¾åƒè’™ç‰ˆå’Œåè½¬çš„æ½œå˜é‡ä¼ é€’ç»™ç®¡é“ã€‚`target_prompt`ç°åœ¨æˆä¸º`prompt`ï¼Œè€Œ`source_prompt`åˆ™ç”¨ä½œ`negative_prompt`ï¼š

```py
output_image = pipeline(
    prompt=target_prompt,
    mask_image=mask_image,
    image_latents=inv_latents,
    negative_prompt=source_prompt,
).images[0]
mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

![](img/66e8c6b1baf21e3ade0ec71d71f6eff6.png)

åŸå§‹å›¾åƒ

![](img/9cc02168a4ef69d9e1454293018a3a04.png)

ç¼–è¾‘åçš„å›¾åƒ

## ç”Ÿæˆæºå’Œç›®æ ‡åµŒå…¥

æºå’Œç›®æ ‡åµŒå…¥å¯ä»¥ä½¿ç”¨[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨åˆ›å»ºã€‚

ä»ğŸ¤— Transformers åº“åŠ è½½ Flan-T5 æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š

```py
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)
```

æä¾›ä¸€äº›åˆå§‹æ–‡æœ¬ä»¥æç¤ºæ¨¡å‹ç”Ÿæˆæºå’Œç›®æ ‡æç¤ºã€‚

```py
source_concept = "bowl"
target_concept = "basket"

source_text = f"Provide a caption for images containing a {source_concept}. "
"The captions should be in English and should be no longer than 150 characters."

target_text = f"Provide a caption for images containing a {target_concept}. "
"The captions should be in English and should be no longer than 150 characters."
```

æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå®ç”¨å‡½æ•°æ¥ç”Ÿæˆæç¤ºï¼š

```py
@torch.no_grad()
def generate_prompts(input_prompt):
    input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.to("cuda")

    outputs = model.generate(
        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10
    )
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

source_prompts = generate_prompts(source_text)
target_prompts = generate_prompts(target_text)
print(source_prompts)
print(target_prompts)
```

å¦‚æœæ‚¨å¯¹ç”Ÿæˆä¸åŒè´¨é‡æ–‡æœ¬çš„ç­–ç•¥æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[ç”Ÿæˆç­–ç•¥](https://huggingface.co/docs/transformers/main/en/generation_strategies)æŒ‡å—ã€‚

åŠ è½½ç”± StableDiffusionDiffEditPipeline ä½¿ç”¨çš„æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ä»¥å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚æ‚¨å°†ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨æ¥è®¡ç®—æ–‡æœ¬åµŒå…¥ï¼š

```py
import torch
from diffusers import StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()

@torch.no_grad()
def embed_prompts(sentences, tokenizer, text_encoder, device="cuda"):
    embeddings = []
    for sent in sentences:
        text_inputs = tokenizer(
            sent,
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids
        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]
        embeddings.append(prompt_embeds)
    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)

source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)
target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)
```

æœ€åï¼Œå°†åµŒå…¥ä¼ é€’ç»™ generate_mask()å’Œ invert()å‡½æ•°ï¼Œä»¥åŠç®¡é“æ¥ç”Ÿæˆå›¾åƒï¼š

```py
  from diffusers import DDIMInverseScheduler, DDIMScheduler
  from diffusers.utils import load_image, make_image_grid
  from PIL import Image

  pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)

  img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
  raw_image = load_image(img_url).resize((768, 768))

  mask_image = pipeline.generate_mask(
      image=raw_image,
-     source_prompt=source_prompt,
-     target_prompt=target_prompt,
+     source_prompt_embeds=source_embeds,
+     target_prompt_embeds=target_embeds,
  )

  inv_latents = pipeline.invert(
-     prompt=source_prompt,
+     prompt_embeds=source_embeds,
      image=raw_image,
  ).latents

  output_image = pipeline(
      mask_image=mask_image,
      image_latents=inv_latents,
-     prompt=target_prompt,
-     negative_prompt=source_prompt,
+     prompt_embeds=target_embeds,
+     negative_prompt_embeds=source_embeds,
  ).images[0]
  mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L")
  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

## ç”Ÿæˆåè½¬çš„æ ‡é¢˜

å½“æ‚¨å¯ä»¥ä½¿ç”¨`source_prompt`ä½œä¸ºæ ‡é¢˜æ¥å¸®åŠ©ç”Ÿæˆéƒ¨åˆ†åè½¬çš„æ½œåœ¨å†…å®¹æ—¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜ã€‚

ä»ğŸ¤— Transformers åº“ä¸­åŠ è½½ BLIP æ¨¡å‹å’Œå¤„ç†å™¨ï¼š

```py
import torch
from transformers import BlipForConditionalGeneration, BlipProcessor

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16, low_cpu_mem_usage=True)
```

åˆ›å»ºä¸€ä¸ªå®ç”¨å‡½æ•°æ¥ä»è¾“å…¥å›¾åƒç”Ÿæˆæ ‡é¢˜ï¼š

```py
@torch.no_grad()
def generate_caption(images, caption_generator, caption_processor):
    text = "a photograph of"

    inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
    caption_generator.to("cuda")
    outputs = caption_generator.generate(**inputs, max_new_tokens=128)

    # offload caption generator
    caption_generator.to("cpu")

    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption
```

åŠ è½½è¾“å…¥å›¾åƒå¹¶ä½¿ç”¨`generate_caption`å‡½æ•°ä¸ºå…¶ç”Ÿæˆæ ‡é¢˜ï¼š

```py
from diffusers.utils import load_image

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
caption = generate_caption(raw_image, model, processor)
```

![](img/66e8c6b1baf21e3ade0ec71d71f6eff6.png)

ç”Ÿæˆçš„æ ‡é¢˜ï¼š"ä¸€å¼ æ”¾åœ¨æ¡Œå­ä¸Šçš„æ°´æœç¢—çš„ç…§ç‰‡"

ç°åœ¨æ‚¨å¯ä»¥å°†æ ‡é¢˜æ”¾å…¥ invert()å‡½æ•°ä¸­ï¼Œä»¥ç”Ÿæˆéƒ¨åˆ†åè½¬çš„æ½œåœ¨å†…å®¹ï¼
