["```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```", "```py\nfrom diffusers import ControlNetModel\n\ncontrolnet = ControlNetModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    in_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    cross_attention_dim=32,\n    conditioning_embedding_out_channels=(16, 32),\n)\ncontrolnet.push_to_hub(\"my-controlnet-model\")\n```", "```py\ncontrolnet.push_to_hub(\"my-controlnet-model\", variant=\"fp16\")\n```", "```py\nmodel = ControlNetModel.from_pretrained(\"your-namespace/my-controlnet-model\")\n```", "```py\nfrom diffusers import DDIMScheduler\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\nscheduler.push_to_hub(\"my-controlnet-scheduler\")\n```", "```py\nscheduler = DDIMScheduler.from_pretrained(\"your-namepsace/my-controlnet-scheduler\")\n```", "```py\nfrom diffusers import (\n    UNet2DConditionModel,\n    AutoencoderKL,\n    DDIMScheduler,\n    StableDiffusionPipeline,\n)\nfrom transformers import CLIPTextModel, CLIPTextConfig, CLIPTokenizer\n\nunet = UNet2DConditionModel(\n    block_out_channels=(32, 64),\n    layers_per_block=2,\n    sample_size=32,\n    in_channels=4,\n    out_channels=4,\n    down_block_types=(\"DownBlock2D\", \"CrossAttnDownBlock2D\"),\n    up_block_types=(\"CrossAttnUpBlock2D\", \"UpBlock2D\"),\n    cross_attention_dim=32,\n)\n\nscheduler = DDIMScheduler(\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n)\n\nvae = AutoencoderKL(\n    block_out_channels=[32, 64],\n    in_channels=3,\n    out_channels=3,\n    down_block_types=[\"DownEncoderBlock2D\", \"DownEncoderBlock2D\"],\n    up_block_types=[\"UpDecoderBlock2D\", \"UpDecoderBlock2D\"],\n    latent_channels=4,\n)\n\ntext_encoder_config = CLIPTextConfig(\n    bos_token_id=0,\n    eos_token_id=2,\n    hidden_size=32,\n    intermediate_size=37,\n    layer_norm_eps=1e-05,\n    num_attention_heads=4,\n    num_hidden_layers=5,\n    pad_token_id=1,\n    vocab_size=1000,\n)\ntext_encoder = CLIPTextModel(text_encoder_config)\ntokenizer = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n```", "```py\ncomponents = {\n    \"unet\": unet,\n    \"scheduler\": scheduler,\n    \"vae\": vae,\n    \"text_encoder\": text_encoder,\n    \"tokenizer\": tokenizer,\n    \"safety_checker\": None,\n    \"feature_extractor\": None,\n}\n\npipeline = StableDiffusionPipeline(**components)\npipeline.push_to_hub(\"my-pipeline\")\n```", "```py\npipeline = StableDiffusionPipeline.from_pretrained(\"your-namespace/my-pipeline\")\n```", "```py\ncontrolnet.push_to_hub(\"my-controlnet-model-private\", private=True)\n```"]