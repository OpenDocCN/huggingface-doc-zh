["```py\n>>> import datasets\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n\n>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library).\n>>> text = \"This is an example text.\"\n\n>>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n>>> sample = ds[0][\"audio\"]\n\n>>> # Define processor and model.\n>>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n>>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n\n>>> # Generate processor output and model output.\n>>> processor_output = processor(raw_speech=sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], text=text, return_tensors=\"pt\")\n>>> generated_output = model.generate(**processor_output)\n```", "```py\n( text_config = None speech_config = None decoder_config = None projection_dim = 768 logit_scale_init_value = 2.6592 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import ClvpConfig, ClvpModelForConditionalGeneration\n\n>>> # Initializing a ClvpConfig with susnato/clvp_dev style configuration\n>>> configuration = ClvpConfig()\n\n>>> # Initializing a ClvpModelForConditionalGeneration (with random weights) from the susnato/clvp_dev style configuration\n>>> model = ClvpModelForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a CLVPConfig from a CLVPTextConfig, CLVPSpeechConfig and a CLVPAutoRegressiveConfig\n>>> from transformers import ClvpEncoderConfig, ClvpDecoderConfig\n\n>>> # Initializing a CLVP text, CLVP speech and CLVP decoder configuration\n>>> config_text = ClvpEncoderConfig()\n>>> config_speech = ClvpEncoderConfig()\n>>> decoder_config = ClvpDecoderConfig()\n\n>>> config = ClvpConfig.from_sub_model_configs(config_text, config_speech, decoder_config)\n```", "```py\n( text_config: ClvpEncoderConfig speech_config: ClvpEncoderConfig decoder_config: ClvpDecoderConfig **kwargs ) \u2192 export const metadata = 'undefined';ClvpConfig\n```", "```py\n( vocab_size = 256 hidden_size = 768 intermediate_size = 1536 projection_dim = 768 num_hidden_layers = 20 num_attention_heads = 12 hidden_act = 'gelu' layer_norm_eps = 1e-05 attention_dropout = 0.1 dropout = 0.1 use_rotary_embedding = True use_attention_bias = False summary_type = 'mean' initializer_factor = 1.0 bos_token_id = 255 eos_token_id = 0 **kwargs )\n```", "```py\n>>> from transformers import ClvpEncoderConfig, ClvpEncoder\n\n>>> # Initializing a ClvpEncoderConfig with susnato/clvp_dev style configuration\n>>> encoder_configuration = ClvpEncoderConfig()\n\n>>> # Initializing a ClvpEncoder (with random weights) from the susnato/clvp_dev style configuration\n>>> model = ClvpEncoder(encoder_configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_size = 8194 max_position_embeddings = 608 max_text_tokens = 404 hidden_size = 1024 num_hidden_layers = 30 num_attention_heads = 16 n_inner = None num_mel_attn_blocks = 6 activation_function = 'gelu_new' resid_pdrop = 0.1 embd_pdrop = 0.1 attention_dropout = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 summary_type = 'cls_index' summary_use_proj = True summary_activation = None summary_proj_to_labels = True summary_first_dropout = 0.1 use_cache = True bos_token_id = 8192 eos_token_id = 8193 feature_size = 80 use_attention_bias = True initializer_factor = 1.0 decoder_fixing_codes = [83, 45, 45, 248] **kwargs )\n```", "```py\n>>> from transformers import ClvpDecoderConfig, ClvpDecoder\n\n>>> # Initializing a ClvpDecoderConfig with susnato/clvp_dev style configuration\n>>> decoder_configuration = ClvpDecoderConfig()\n\n>>> # Initializing a ClvpDecoder (with random weights) from the susnato/clvp_dev style configuration\n>>> model = ClvpDecoder(decoder_configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' unk_token = '[UNK]' bos_token = '<|endoftext|>' eos_token = '[STOP]' pad_token = '[STOP]' add_prefix_space = False add_bos_token = False add_eos_token = False **kwargs )\n```", "```py\n>>> from transformers import ClvpTokenizer\n\n>>> tokenizer = ClvpTokenizer.from_pretrained(\"susnato/clvp_dev\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[62, 84, 28, 2, 179, 79]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[2, 62, 84, 28, 2, 179, 79]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( feature_size = 80 sampling_rate = 22050 default_audio_length = 6 hop_length = 256 chunk_length = 30 n_fft = 1024 padding_value = 0.0 mel_norms = None return_attention_mask = False **kwargs )\n```", "```py\n( raw_speech: Union sampling_rate: Optional = None truncation: bool = True pad_to_multiple_of: Optional = None return_tensors: Union = None return_attention_mask: Optional = True padding: Optional = 'max_length' max_length: Optional = None **kwargs )\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: ClvpConfig )\n```", "```py\n( input_ids: LongTensor = None input_features: FloatTensor = None conditioning_encoder_inputs_embeds: Optional = None text_encoder_inputs_embeds: Optional = None attention_mask: Optional = None return_loss: Optional = None output_hidden_states: Optional = None output_attentions: Optional = False return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clvp.modeling_clvp.ClvpOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import datasets\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n\n>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)\n>>> text = \"This is an example text.\"\n\n>>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n>>> _, audio, sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n\n>>> # Define processor and model\n>>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n>>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n\n>>> # processor outputs and model outputs\n>>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors=\"pt\")\n>>> outputs = model(\n...     input_ids=processor_output[\"input_ids\"],\n...     input_features=processor_output[\"input_features\"],\n...     return_dict=True,\n... )\n```", "```py\n( input_ids: LongTensor = None input_features: FloatTensor = None attention_mask: Optional = None generation_config: Optional = None pad_to_max_mel_tokens: Optional = None output_hidden_states: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';ClvpOutput or tuple\n```", "```py\n( input_ids: Optional = None text_encoder_inputs_embeds: Optional = None attention_mask: Optional = None ) \u2192 export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n\n>>> # Define the Text\n>>> text = \"This is an example text.\"\n\n>>> # Define processor and model\n>>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n>>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n\n>>> # Generate processor output and text embeds\n>>> processor_output = processor(text=text, return_tensors=\"pt\")\n>>> text_embeds = model.get_text_features(input_ids=processor_output[\"input_ids\"])\n```", "```py\n( speech_ids: Optional = None input_ids: Optional = None input_features: Optional = None conditioning_encoder_inputs_embeds: Optional = None attention_mask: Optional = None generation_config: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> import datasets\n>>> from transformers import ClvpProcessor, ClvpModelForConditionalGeneration\n\n>>> # Define the Text and Load the Audio (We are taking an audio example from HuggingFace Hub using `datasets` library)\n>>> text = \"This is an example text.\"\n>>> ds = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=22050))\n>>> _, audio, sr = ds.sort(\"id\").select(range(1))[:1][\"audio\"][0].values()\n\n>>> # Define processor and model\n>>> processor = ClvpProcessor.from_pretrained(\"susnato/clvp_dev\")\n>>> model = ClvpModelForConditionalGeneration.from_pretrained(\"susnato/clvp_dev\")\n\n>>> # Generate processor output and model output\n>>> processor_output = processor(raw_speech=audio, sampling_rate=sr, text=text, return_tensors=\"pt\")\n>>> speech_embeds = model.get_speech_features(\n...     input_ids=processor_output[\"input_ids\"], input_features=processor_output[\"input_features\"]\n... )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: ClvpDecoderConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config: ClvpConfig )\n```", "```py\n( input_ids: Optional = None inputs_embeds: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```"]