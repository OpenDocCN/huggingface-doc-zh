["```py\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> chat = [\n...    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n...    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n...    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n... ]\n\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>\"\n```", "```py\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\n>>> chat = [\n...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n...   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n... ]\n\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"HuggingFaceH4/zephyr-7b-beta\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```", "```py\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s> \n<|user|>\nHow many helicopters can a human eat in one sitting?</s> \n<|assistant|>\n```", "```py\noutputs = model.generate(tokenized_chat, max_new_tokens=128) \nprint(tokenizer.decode(outputs[0]))\n```", "```py\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s> \n<|user|>\nHow many helicopters can a human eat in one sitting?</s> \n<|assistant|>\nMatey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```", "```py\nfrom transformers import pipeline\n\npipe = pipeline(\"conversational\", \"HuggingFaceH4/zephyr-7b-beta\")\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprint(pipe(messages))\n```", "```py\nConversation id: 76d886a0-74bd-454e-9804-0467041a63dc\nsystem: You are a friendly chatbot who always responds in the style of a pirate\nuser: How many helicopters can a human eat in one sitting?\nassistant: Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```", "```py\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi there!\"},\n    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n]\n```", "```py\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n\"\"\"\n```", "```py\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\n\"\"\"\n```", "```py\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```", "```py\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```", "```py\n\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> tokenizer.default_chat_template\n\"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n```", "```py\n{% for message in messages %}  {% if message['role'] == 'user' %}  {{ ' ' }}  {% endif %}  {{ message['content'] }}  {% if not loop.last %}  {{ '  ' }}  {% endif %}  {% endfor %}  {{ eos_token }}\n```", "```py\nfor idx, message in enumerate(messages):\n    if message['role'] == 'user':\n        print(' ')\n    print(message['content'])\n    if not idx == len(messages) - 1:  # Check for the last message in the conversation\n        print('  ')\nprint(eos_token)\n```", "```py\n{% for message in messages %}  {% if message['role'] == 'user' %}  {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}  {% elif message['role'] == 'system' %}  {{ '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}  {% elif message['role'] == 'assistant' %}  {{ ' '  + message['content'] + ' ' + eos_token }}  {% endif %}  {% endfor %}\n```", "```py\n{% for message in messages %}  {% if message['role'] == 'user' %}  {{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}  {% elif message['role'] == 'system' %}  {{ '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}  {% elif message['role'] == 'assistant' %}  {{ '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}  {% endif %}  {% endfor %}\n```", "```py\ntemplate = tokenizer.chat_template\ntemplate = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\ntokenizer.chat_template = template  # Set the new template\ntokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n```", "```py\n{% for message in messages %}  {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}  {% endfor %}\n```", "```py\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```", "```py\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```", "```py\n{% for message in messages %}  {{ message['content'] }}  {% endfor %}\n```", "```py\n{% if message['role'] == 'user' %}  {{ message['content'] }}  {% endif %}\n```", "```py\n{% if loop.last and add_generation_prompt %}  {{ bos_token + 'Assistant:\\n' }}  {% endif %}\n```"]