["```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 mlp_ratio = 4 hidden_act = 'gelu' dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-06 image_size = 224 pretrain_image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True drop_path_rate = 0.0 window_block_indices = [] residual_block_indices = [] use_absolute_position_embeddings = True use_relative_position_embeddings = False window_size = 0 out_features = None out_indices = None **kwargs )\n```", "```py\n>>> from transformers import VitDetConfig, VitDetModel\n\n>>> # Initializing a VitDet configuration\n>>> configuration = VitDetConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = VitDetModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: VitDetConfig )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import VitDetConfig, VitDetModel\n>>> import torch\n\n>>> config = VitDetConfig()\n>>> model = VitDetModel(config)\n\n>>> pixel_values = torch.randn(1, 3, 224, 224)\n\n>>> with torch.no_grad():\n...     outputs = model(pixel_values)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 768, 14, 14]\n```"]