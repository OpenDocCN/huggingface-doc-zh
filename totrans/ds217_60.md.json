["```py\n( description: str = <factory> citation: str = <factory> homepage: str = <factory> license: str = <factory> features: Optional = None post_processed: Optional = None supervised_keys: Optional = None task_templates: Optional = None builder_name: Optional = None dataset_name: Optional = None config_name: Optional = None version: Union = None splits: Optional = None download_checksums: Optional = None download_size: Optional = None post_processing_size: Optional = None dataset_size: Optional = None size_in_bytes: Optional = None )\n```", "```py\n( dataset_info_dir: str fs = 'deprecated' storage_options: Optional = None )\n```", "```py\n>>> from datasets import DatasetInfo\n>>> ds_info = DatasetInfo.from_directory(\"/path/to/directory/\")\n```", "```py\n( dataset_info_dir pretty_print = False fs = 'deprecated' storage_options: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.info.write_to_directory(\"/path/to/directory/\")\n```", "```py\n( arrow_table: Table info: Optional = None split: Optional = None indices_table: Optional = None fingerprint: Optional = None )\n```", "```py\n( name: str column: Union new_fingerprint: str )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> more_text = ds[\"text\"]\n>>> ds.add_column(name=\"text_2\", column=more_text)\nDataset({\n    features: ['text', 'label', 'text_2'],\n    num_rows: 1066\n})\n```", "```py\n( item: dict new_fingerprint: str )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n>>> ds = ds.add_item(new_review)\n>>> ds[-1]\n{'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n```", "```py\n( filename: str info: Optional = None split: Optional = None indices_filename: Optional = None in_memory: bool = False )\n```", "```py\n( buffer: Buffer info: Optional = None split: Optional = None indices_buffer: Optional = None )\n```", "```py\n( df: DataFrame features: Optional = None info: Optional = None split: Optional = None preserve_index: Optional = None )\n```", "```py\n>>> ds = Dataset.from_pandas(df)\n```", "```py\n( mapping: dict features: Optional = None info: Optional = None split: Optional = None )\n```", "```py\n( generator: Callable features: Optional = None cache_dir: str = None keep_in_memory: bool = False gen_kwargs: Optional = None num_proc: Optional = None **kwargs )\n```", "```py\n>>> def gen():\n...     yield {\"text\": \"Good\", \"label\": 0}\n...     yield {\"text\": \"Bad\", \"label\": 1}\n...\n>>> ds = Dataset.from_generator(gen)\n```", "```py\n>>> def gen(shards):\n...     for shard in shards:\n...         with open(shard) as f:\n...             for line in f:\n...                 yield {\"line\": line}\n...\n>>> shards = [f\"data{i}.txt\" for i in range(32)]\n>>> ds = Dataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.data\nMemoryMappedTable\ntext: string\nlabel: int64\n----\ntext: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",\"so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .\",\"a visual spectacle full of stunning images and effects .\",\"a gentle and engrossing character study .\",\"it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .\",\"an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .\",...,\"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .\",\"ah-nuld's action hero days might be over .\",\"it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .\",\"feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .\",\"when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .\",\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\nlabel: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.cache_files\n[{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.num_columns\n2\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.num_rows\n1066\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.column_names\n['text', 'label']\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.shape\n(1066, 2)\n```", "```py\n( column: str ) \u2192 export const metadata = 'undefined';list\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.unique('label')\n[1, 0]\n```", "```py\n( new_fingerprint: Optional = None max_depth = 16 ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"squad\", split=\"train\")\n>>> ds.features\n{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n 'context': Value(dtype='string', id=None),\n 'id': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None)}\n>>> ds.flatten()\nDataset({\n    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n    num_rows: 87599\n})\n```", "```py\n( features: Features batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 num_proc: Optional = None ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset, ClassLabel, Value\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> new_features = ds.features.copy()\n>>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n>>> new_features['text'] = Value('large_string')\n>>> ds = ds.cast(new_features)\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='large_string', id=None)}\n```", "```py\n( column: str feature: Union new_fingerprint: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='string', id=None)}\n```", "```py\n( column_names: Union new_fingerprint: Optional = None ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.remove_columns('label')\nDataset({\n    features: ['text'],\n    num_rows: 1066\n})\n>>> ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0\nDataset({\n    features: [],\n    num_rows: 0\n})\n```", "```py\n( original_column_name: str new_column_name: str new_fingerprint: Optional = None ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.rename_column('label', 'label_new')\nDataset({\n    features: ['text', 'label_new'],\n    num_rows: 1066\n})\n```", "```py\n( column_mapping: Dict new_fingerprint: Optional = None ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\nDataset({\n    features: ['text_new', 'label_new'],\n    num_rows: 1066\n})\n```", "```py\n( column_names: Union new_fingerprint: Optional = None ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.select_columns(['text'])\nDataset({\n    features: ['text'],\n    num_rows: 1066\n})\n```", "```py\n( column: str include_nulls: bool = False )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"boolq\", split=\"validation\")\n>>> ds.features\n{'answer': Value(dtype='bool', id=None),\n 'passage': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None)}\n>>> ds = ds.class_encode_column('answer')\n>>> ds.features\n{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n 'passage': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None)}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.__len__\n<bound method Dataset.__len__ of Dataset({\n    features: ['text', 'label'],\n    num_rows: 1066\n})>\n```", "```py\n( )\n```", "```py\n( batch_size: int drop_last_batch: bool = False )\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\nnew formatted columns = (all columns - previously unformatted columns)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n>>> ds.set_format(type='numpy', columns=['text', 'label'])\n>>> ds.format\n{'type': 'numpy',\n'format_kwargs': {},\n'columns': ['text', 'label'],\n'output_all_columns': False}\n```", "```py\n( transform: Optional columns: Optional = None output_all_columns: bool = False )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n>>> def encode(batch):\n...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n>>> ds.set_transform(encode)\n>>> ds[0]\n{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n 1, 1]),\n 'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n         20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n         5637,  1998, 11690,  2336,  1012,   102]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0])}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n>>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n>>> ds.format\n{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': 'numpy'}\n>>> ds.reset_format()\n>>> ds.format\n{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': None}\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n>>> ds.format\n{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': None}\n>>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n>>> ds.format\n{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': 'tensorflow'}\n```", "```py\n( transform: Optional columns: Optional = None output_all_columns: bool = False )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> def encode(example):\n...     return tokenizer(example[\"text\"], padding=True, truncation=True, return_tensors='pt')\n>>> ds = ds.with_transform(encode)\n>>> ds[0]\n{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n 1, 1, 1, 1, 1]),\n 'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n         1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n         1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0])}\n```", "```py\n( key )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';int\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.cleanup_cache_files()\n10\n```", "```py\n( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False fn_kwargs: Optional = None num_proc: Optional = None suffix_template: str = '_{rank:05d}_of_{num_proc:05d}' new_fingerprint: Optional = None desc: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> def add_prefix(example):\n...     example[\"text\"] = \"Review: \" + example[\"text\"]\n...     return example\n>>> ds = ds.map(add_prefix)\n>>> ds[0:3][\"text\"]\n['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n 'Review: the soundtrack alone is worth the price of admission .',\n 'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n\n# process a batch of examples\n>>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n# set number of processors\n>>> ds = ds.map(add_prefix, num_proc=4)\n```", "```py\n( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_name: Optional = None writer_batch_size: Optional = 1000 fn_kwargs: Optional = None num_proc: Optional = None suffix_template: str = '_{rank:05d}_of_{num_proc:05d}' new_fingerprint: Optional = None desc: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.filter(lambda x: x[\"label\"] == 1)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 533\n})\n```", "```py\n( indices: Iterable keep_in_memory: bool = False indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds.select(range(4))\nDataset({\n    features: ['text', 'label'],\n    num_rows: 4\n})\n```", "```py\n( column_names: Union reverse: Union = False kind = 'deprecated' null_placement: str = 'at_end' keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten_tomatoes', split='validation')\n>>> ds['label'][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n>>> sorted_ds = ds.sort('label')\n>>> sorted_ds['label'][:10]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n>>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n>>> another_sorted_ds['label'][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```", "```py\n( seed: Optional = None generator: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 new_fingerprint: Optional = None )\n```", "```py\nmy_dataset[0]  # fast\nmy_dataset = my_dataset.shuffle(seed=42)\nmy_dataset[0]  # up to 10x slower\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\nmy_dataset[0]  # fast again\n```", "```py\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)\nfor example in enumerate(my_iterable_dataset):  # fast\n    pass\n\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\n    pass\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds['label'][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n# set a seed\n>>> shuffled_ds = ds.shuffle(seed=42)\n>>> shuffled_ds['label'][:10]\n[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n```", "```py\n( test_size: Union = None train_size: Union = None shuffle: bool = True stratify_by_column: Optional = None seed: Optional = None generator: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None train_indices_cache_file_name: Optional = None test_indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 train_new_fingerprint: Optional = None test_new_fingerprint: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 852\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 214\n    })\n})\n\n# set a seed\n>>> ds = ds.train_test_split(test_size=0.2, seed=42)\n\n# stratified split\n>>> ds = load_dataset(\"imdb\",split=\"train\")\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n>>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\")\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 20000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5000\n    })\n})\n```", "```py\n( num_shards: int index: int contiguous: bool = False keep_in_memory: bool = False indices_cache_file_name: Optional = None writer_batch_size: Optional = 1000 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n>>> ds\nDataset({\n    features: ['text', 'label'],\n    num_rows: 1066\n})\n>>> ds.shard(num_shards=2, index=0)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 533\n})\n```", "```py\n( batch_size: Optional = None columns: Union = None shuffle: bool = False collate_fn: Optional = None drop_remainder: bool = False collate_fn_args: Optional = None label_cols: Union = None prefetch: bool = True num_workers: int = 0 num_test_batches: int = 20 )\n```", "```py\n>>> ds_train = ds[\"train\"].to_tf_dataset(\n...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n...    shuffle=True,\n...    batch_size=16,\n...    collate_fn=data_collator,\n... )\n```", "```py\n( repo_id: str config_name: str = 'default' set_default: Optional = None split: Optional = None data_dir: Optional = None commit_message: Optional = None commit_description: Optional = None private: Optional = False token: Optional = None revision: Optional = None branch = 'deprecated' create_pr: Optional = False max_shard_size: Union = None num_shards: Optional = None embed_external_files: bool = True )\n```", "```py\n>>> dataset.push_to_hub(\"<organization>/<dataset_id>\")\n>>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n>>> dataset.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n>>> dataset.push_to_hub(\"<organization>/<dataset_id>\", num_shards=1024)\n```", "```py\n>>> train_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"train\")\n>>> val_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"validation\")\n>>> # later\n>>> dataset = load_dataset(\"<organization>/<dataset_id>\")\n>>> train_dataset = dataset[\"train\"]\n>>> val_dataset = dataset[\"validation\"]\n```", "```py\n>>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n>>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n>>> # later\n>>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n>>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n```", "```py\n( dataset_path: Union fs = 'deprecated' max_shard_size: Union = None num_shards: Optional = None num_proc: Optional = None storage_options: Optional = None )\n```", "```py\n>>> ds.save_to_disk(\"path/to/dataset/directory\")\n>>> ds.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n>>> ds.save_to_disk(\"path/to/dataset/directory\", num_shards=1024)\n```", "```py\n( dataset_path: str fs = 'deprecated' keep_in_memory: Optional = None storage_options: Optional = None ) \u2192 export const metadata = 'undefined';Dataset or DatasetDict\n```", "```py\n>>> ds = load_from_disk(\"path/to/dataset/directory\")\n```", "```py\n( keep_in_memory: bool = False cache_file_name: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False num_proc: Optional = None new_fingerprint: Optional = None )\n```", "```py\n( path_or_buf: Union batch_size: Optional = None num_proc: Optional = None **to_csv_kwargs ) \u2192 export const metadata = 'undefined';int\n```", "```py\n>>> ds.to_csv(\"path/to/dataset/directory\")\n```", "```py\n( batch_size: Optional = None batched: bool = False )\n```", "```py\n>>> ds.to_pandas()\n```", "```py\n( batch_size: Optional = None batched = 'deprecated' )\n```", "```py\n>>> ds.to_dict()\n```", "```py\n( path_or_buf: Union batch_size: Optional = None num_proc: Optional = None **to_json_kwargs ) \u2192 export const metadata = 'undefined';int\n```", "```py\n>>> ds.to_json(\"path/to/dataset/directory\")\n```", "```py\n( path_or_buf: Union batch_size: Optional = None **parquet_writer_kwargs ) \u2192 export const metadata = 'undefined';int\n```", "```py\n>>> ds.to_parquet(\"path/to/dataset/directory\")\n```", "```py\n( name: str con: Union batch_size: Optional = None **sql_writer_kwargs ) \u2192 export const metadata = 'undefined';int\n```", "```py\n>>> # con provided as a connection URI string\n>>> ds.to_sql(\"data\", \"sqlite:///my_own_db.sql\")\n>>> # con provided as a sqlite3 connection object\n>>> import sqlite3\n>>> con = sqlite3.connect(\"my_own_db.sql\")\n>>> with con:\n...     ds.to_sql(\"data\", con)\n```", "```py\n( num_shards: Optional = 1 )\n```", "```py\n>>> ids = ds.to_iterable_dataset()\n>>> for example in ids:\n...     pass\n```", "```py\n>>> ids = ds.to_iterable_dataset()\n>>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset\n>>> for example in ids:\n...     pass\n```", "```py\n>>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over\n>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating\n>>> for example in ids:\n...     pass\n```", "```py\n>>> import torch\n>>> ids = ds.to_iterable_dataset(num_shards=64)\n>>> ids = ids.filter(filter_fn).map(process_fn)\n>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating\n>>> for example in ids:\n...     pass\n```", "```py\n>>> import torch\n>>> ids = ds.to_iterable_dataset(num_shards=64)\n>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n>>> for example in ids:\n...     pass\n```", "```py\n>>> from datasets.distributed import split_dataset_by_node\n>>> ids = ds.to_iterable_dataset(num_shards=512)\n>>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n>>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\n>>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\n>>> for example in ids:\n...     pass\n```", "```py\n>>> ids = ds.to_iterable_dataset(num_shards=64)\n>>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n>>> for epoch in range(n_epochs):\n...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating\n...     for example in ids:\n...         pass\n```", "```py\n( column: str index_name: Optional = None device: Optional = None string_factory: Optional = None metric_type: Optional = None custom_index: Optional = None batch_size: int = 1000 train_size: Optional = None faiss_verbose: bool = False dtype = <class 'numpy.float32'> )\n```", "```py\n>>> ds = datasets.load_dataset('crime_and_punish', split='train')\n>>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n>>> ds_with_embeddings.add_faiss_index(column='embeddings')\n>>> # query\n>>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n>>> # save index\n>>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n\n>>> ds = datasets.load_dataset('crime_and_punish', split='train')\n>>> # load index\n>>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n>>> # query\n>>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n```", "```py\n( external_arrays: array index_name: str device: Optional = None string_factory: Optional = None metric_type: Optional = None custom_index: Optional = None batch_size: int = 1000 train_size: Optional = None faiss_verbose: bool = False dtype = <class 'numpy.float32'> )\n```", "```py\n( index_name: str file: Union storage_options: Optional = None )\n```", "```py\n( index_name: str file: Union device: Union = None storage_options: Optional = None )\n```", "```py\n( column: str index_name: Optional = None host: Optional = None port: Optional = None es_client: Optional = None es_index_name: Optional = None es_index_config: Optional = None )\n```", "```py\n>>> es_client = elasticsearch.Elasticsearch()\n>>> ds = datasets.load_dataset('crime_and_punish', split='train')\n>>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n>>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n```", "```py\n( index_name: str es_index_name: str host: Optional = None port: Optional = None es_client: Optional = None es_index_config: Optional = None )\n```", "```py\n( )\n```", "```py\n( index_name: str )\n```", "```py\n( index_name: str )\n```", "```py\n( index_name: str query: Union k: int = 10 **kwargs ) \u2192 export const metadata = 'undefined';(scores, indices)\n```", "```py\n( index_name: str queries: Union k: int = 10 **kwargs ) \u2192 export const metadata = 'undefined';(total_scores, total_indices)\n```", "```py\n( index_name: str query: Union k: int = 10 **kwargs ) \u2192 export const metadata = 'undefined';(scores, examples)\n```", "```py\n( index_name: str queries: Union k: int = 10 **kwargs ) \u2192 export const metadata = 'undefined';(total_scores, total_examples)\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False num_proc: Optional = None **kwargs )\n```", "```py\n>>> ds = Dataset.from_csv('path/to/dataset.csv')\n```", "```py\n( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False field: Optional = None num_proc: Optional = None **kwargs )\n```", "```py\n>>> ds = Dataset.from_json('path/to/dataset.json')\n```", "```py\n( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False columns: Optional = None num_proc: Optional = None **kwargs )\n```", "```py\n>>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n```", "```py\n( path_or_paths: Union split: Optional = None features: Optional = None cache_dir: str = None keep_in_memory: bool = False num_proc: Optional = None **kwargs )\n```", "```py\n>>> ds = Dataset.from_text('path/to/dataset.txt')\n```", "```py\n( sql: Union con: Union features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )\n```", "```py\n>>> # Fetch a database table\n>>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n>>> # Execute a SQL query on the table\n>>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n>>> # Use a Selectable object to specify the query\n>>> from sqlalchemy import select, text\n>>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n>>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n```", "```py\n( task: Union id: int = 0 )\n```", "```py\n( label2id: Dict label_column: str )\n```", "```py\n>>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n>>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n>>> # mapping to align with\n>>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n>>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n```", "```py\n( dsets: List info: Optional = None split: Optional = None axis: int = 0 )\n```", "```py\n>>> ds3 = concatenate_datasets([ds1, ds2])\n```", "```py\n( datasets: List probabilities: Optional = None seed: Optional = None info: Optional = None split: Optional = None stopping_strategy: Literal = 'first_exhausted' ) \u2192 export const metadata = 'undefined';Dataset or IterableDataset\n```", "```py\n>>> from datasets import Dataset, interleave_datasets\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[10, 0, 11, 1, 2, 20, 12, 10, 0, 1, 2, 21, 0, 11, 1, 2, 0, 1, 12, 2, 10, 0, 22]\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n>>> dataset[\"a\"]\n[10, 0, 11, 1, 2]\n>>> dataset = interleave_datasets([d1, d2, d3])\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22]\n>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22]\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22, 23, 24]})\n>>> dataset = interleave_datasets([d1, d2, d3])\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22]\n>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 10, 24]\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n>>> dataset[\"a\"]\n[10, 0, 11, 1, 2]\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[10, 0, 11, 1, 2, 20, 12, 13, ..., 0, 1, 2, 0, 24]\nFor datasets in streaming mode (iterable):\n\n>>> from datasets import load_dataset, interleave_datasets\n>>> d1 = load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", split=\"train\", streaming=True)\n>>> d2 = load_dataset(\"oscar\", \"unshuffled_deduplicated_fr\", split=\"train\", streaming=True)\n>>> dataset = interleave_datasets([d1, d2])\n>>> iterator = iter(dataset)\n>>> next(iterator)\n{'text': 'Mtendere Village was inspired by the vision...}\n>>> next(iterator)\n{'text': \"M\u00e9dia de d\u00e9bat d'id\u00e9es, de culture...}\n```", "```py\n( dataset: DatasetType rank: int world_size: int ) \u2192 export const metadata = 'undefined';Dataset or IterableDataset\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.data\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.cache_files\n{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n 'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.num_columns\n{'test': 2, 'train': 2, 'validation': 2}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.num_rows\n{'test': 1066, 'train': 8530, 'validation': 1066}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.column_names\n{'test': ['text', 'label'],\n 'train': ['text', 'label'],\n 'validation': ['text', 'label']}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.shape\n{'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n```", "```py\n( column: str ) \u2192 export const metadata = 'undefined';Dict[str, list]\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.unique(\"label\")\n{'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.cleanup_cache_files()\n{'test': 0, 'train': 0, 'validation': 0}\n```", "```py\n( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_names: Optional = None writer_batch_size: Optional = 1000 features: Optional = None disable_nullable: bool = False fn_kwargs: Optional = None num_proc: Optional = None desc: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> def add_prefix(example):\n...     example[\"text\"] = \"Review: \" + example[\"text\"]\n...     return example\n>>> ds = ds.map(add_prefix)\n>>> ds[\"train\"][0:3][\"text\"]\n['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n 'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n 'Review: effective but too-tepid biopic']\n\n# process a batch of examples\n>>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n# set number of processors\n>>> ds = ds.map(add_prefix, num_proc=4)\n```", "```py\n( function: Optional = None with_indices: bool = False with_rank: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 keep_in_memory: bool = False load_from_cache_file: Optional = None cache_file_names: Optional = None writer_batch_size: Optional = 1000 fn_kwargs: Optional = None num_proc: Optional = None desc: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.filter(lambda x: x[\"label\"] == 1)\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 4265\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 533\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 533\n    })\n})\n```", "```py\n( column_names: Union reverse: Union = False kind = 'deprecated' null_placement: str = 'at_end' keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_names: Optional = None writer_batch_size: Optional = 1000 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset('rotten_tomatoes')\n>>> ds['train']['label'][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n>>> sorted_ds = ds.sort('label')\n>>> sorted_ds['train']['label'][:10]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n>>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n>>> another_sorted_ds['train']['label'][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```", "```py\n( seeds: Union = None seed: Optional = None generators: Optional = None keep_in_memory: bool = False load_from_cache_file: Optional = None indices_cache_file_names: Optional = None writer_batch_size: Optional = 1000 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds[\"train\"][\"label\"][:10]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n# set a seed\n>>> shuffled_ds = ds.shuffle(seed=42)\n>>> shuffled_ds[\"train\"][\"label\"][:10]\n[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n>>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n>>> ds[\"train\"].format\n{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': 'numpy'}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n>>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n>>> ds[\"train\"].format\n{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': 'numpy'}\n>>> ds.reset_format()\n>>> ds[\"train\"].format\n{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': None}\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\n( type: Optional = None columns: Optional = None output_all_columns: bool = False **format_kwargs )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n>>> ds[\"train\"].format\n{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': None}\n>>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n>>> ds[\"train\"].format\n{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n 'format_kwargs': {},\n 'output_all_columns': False,\n 'type': 'tensorflow'}\n```", "```py\n( transform: Optional columns: Optional = None output_all_columns: bool = False )\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n>>> def encode(example):\n...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n>>> ds = ds.with_transform(encode)\n>>> ds[\"train\"][0]\n{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n        112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n        112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n        170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n        179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n        188,  1566,  7912, 14516,  6997,   119,   102]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0])}\n```", "```py\n( max_depth = 16 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"squad\")\n>>> ds[\"train\"].features\n{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n 'context': Value(dtype='string', id=None),\n 'id': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None)}\n>>> ds.flatten()\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n        num_rows: 10570\n    })\n})\n```", "```py\n( features: Features )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> new_features = ds[\"train\"].features.copy()\n>>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n>>> new_features['text'] = Value('large_string')\n>>> ds = ds.cast(new_features)\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='large_string', id=None)}\n```", "```py\n( column: str feature )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='string', id=None)}\n```", "```py\n( column_names: Union )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.remove_columns(\"label\")\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1066\n    })\n})\n```", "```py\n( original_column_name: str new_column_name: str )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.rename_column(\"label\", \"label_new\")\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label_new'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label_new'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label_new'],\n        num_rows: 1066\n    })\n})\n```", "```py\n( column_mapping: Dict ) \u2192 export const metadata = 'undefined';DatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\nDatasetDict({\n    train: Dataset({\n        features: ['text_new', 'label_new'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text_new', 'label_new'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text_new', 'label_new'],\n        num_rows: 1066\n    })\n})\n```", "```py\n( column_names: Union )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\")\n>>> ds.select_columns(\"text\")\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1066\n    })\n})\n```", "```py\n( column: str include_nulls: bool = False )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"boolq\")\n>>> ds[\"train\"].features\n{'answer': Value(dtype='bool', id=None),\n 'passage': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None)}\n>>> ds = ds.class_encode_column(\"answer\")\n>>> ds[\"train\"].features\n{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n 'passage': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None)}\n```", "```py\n( repo_id config_name: str = 'default' set_default: Optional = None data_dir: Optional = None commit_message: Optional = None commit_description: Optional = None private: Optional = False token: Optional = None revision: Optional = None branch = 'deprecated' create_pr: Optional = False max_shard_size: Union = None num_shards: Optional = None embed_external_files: bool = True )\n```", "```py\n>>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n>>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n>>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n>>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", num_shards={\"train\": 1024, \"test\": 8})\n```", "```py\n>>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n>>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n>>> # later\n>>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n>>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n```", "```py\n( dataset_dict_path: Union fs = 'deprecated' max_shard_size: Union = None num_shards: Optional = None num_proc: Optional = None storage_options: Optional = None )\n```", "```py\n>>> dataset_dict.save_to_disk(\"path/to/dataset/directory\")\n>>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n>>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", num_shards={\"train\": 1024, \"test\": 8})\n```", "```py\n( dataset_dict_path: Union fs = 'deprecated' keep_in_memory: Optional = None storage_options: Optional = None )\n```", "```py\n>>> ds = load_from_disk('path/to/dataset/directory')\n```", "```py\n( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )\n```", "```py\n>>> from datasets import DatasetDict\n>>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n```", "```py\n( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )\n```", "```py\n>>> from datasets import DatasetDict\n>>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n```", "```py\n( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False columns: Optional = None **kwargs )\n```", "```py\n>>> from datasets import DatasetDict\n>>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n```", "```py\n( path_or_paths: Dict features: Optional = None cache_dir: str = None keep_in_memory: bool = False **kwargs )\n```", "```py\n>>> from datasets import DatasetDict\n>>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n```", "```py\n( task: Union id: int = 0 )\n```", "```py\n( ex_iterable: _BaseExamplesIterable info: Optional = None split: Optional = None formatting: Optional = None shuffling: Optional = None distributed: Optional = None token_per_repo_id: Optional = None format_type = 'deprecated' )\n```", "```py\n( generator: Callable features: Optional = None gen_kwargs: Optional = None ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> def gen():\n...     yield {\"text\": \"Good\", \"label\": 0}\n...     yield {\"text\": \"Bad\", \"label\": 1}\n...\n>>> ds = IterableDataset.from_generator(gen)\n```", "```py\n>>> def gen(shards):\n...     for shard in shards:\n...         with open(shard) as f:\n...             for line in f:\n...                 yield {\"line\": line}\n...\n>>> shards = [f\"data{i}.txt\" for i in range(32)]\n>>> ds = IterableDataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer\n>>> from torch.utils.data import DataLoader\n>>> dataloader = DataLoader(ds.with_format(\"torch\"), num_workers=4)  # give each worker a subset of 32/4=8 shards\n```", "```py\n( column_names: Union ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> next(iter(ds))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n>>> ds = ds.remove_columns(\"label\")\n>>> next(iter(ds))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( column_names: Union ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> next(iter(ds))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n>>> ds = ds.select_columns(\"text\")\n>>> next(iter(ds))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( column: str feature: Union ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> from datasets import load_dataset, Audio\n>>> ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\", streaming=True)\n>>> ds.features\n{'audio': Audio(sampling_rate=8000, mono=True, decode=True, id=None),\n 'english_transcription': Value(dtype='string', id=None),\n 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n 'path': Value(dtype='string', id=None),\n 'transcription': Value(dtype='string', id=None)}\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> ds.features\n{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n 'english_transcription': Value(dtype='string', id=None),\n 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n 'path': Value(dtype='string', id=None),\n 'transcription': Value(dtype='string', id=None)}\n```", "```py\n( features: Features ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> new_features = ds.features.copy()\n>>> new_features[\"label\"] = ClassLabel(names=[\"bad\", \"good\"])\n>>> new_features[\"text\"] = Value(\"large_string\")\n>>> ds = ds.cast(new_features)\n>>> ds.features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='large_string', id=None)}\n```", "```py\n( )\n```", "```py\n( batch_size: int drop_last_batch: bool = False )\n```", "```py\n( function: Optional = None with_indices: bool = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 drop_last_batch: bool = False remove_columns: Union = None features: Optional = None fn_kwargs: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> def add_prefix(example):\n...     example[\"text\"] = \"Review: \" + example[\"text\"]\n...     return example\n>>> ds = ds.map(add_prefix)\n>>> list(ds.take(3))\n[{'label': 1,\n 'text': 'Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n {'label': 1, 'text': 'Review: effective but too-tepid biopic'}]\n```", "```py\n( original_column_name: str new_column_name: str ) \u2192 export const metadata = 'undefined';IterableDataset\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> next(iter(ds))\n{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n>>> ds = ds.rename_column(\"text\", \"movie_review\")\n>>> next(iter(ds))\n{'label': 1,\n 'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( function: Optional = None with_indices = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 fn_kwargs: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> ds = ds.filter(lambda x: x[\"label\"] == 0)\n>>> list(ds.take(3))\n[{'label': 0, 'movie_review': 'simplistic , silly and tedious .'},\n {'label': 0,\n 'movie_review': \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\"},\n {'label': 0,\n 'movie_review': 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]\n```", "```py\n( seed = None generator: Optional = None buffer_size: int = 1000 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> list(ds.take(3))\n[{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n {'label': 1, 'text': 'effective but too-tepid biopic'}]\n>>> shuffled_ds = ds.shuffle(seed=42)\n>>> list(shuffled_ds.take(3))\n[{'label': 1,\n 'text': \"a sports movie with action that's exciting on the field and a story you care about off it .\"},\n {'label': 1,\n 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},\n {'label': 1,\n 'text': \"sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune .\"}]\n```", "```py\n( n )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> list(ds.take(3))\n[{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n {'label': 1, 'text': 'effective but too-tepid biopic'}]\n>>> ds = ds.skip(1)\n>>> list(ds.take(3))\n[{'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n {'label': 1, 'text': 'effective but too-tepid biopic'},\n {'label': 1,\n 'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]\n```", "```py\n( n )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n>>> small_ds = ds.take(2)\n>>> list(small_ds)\n[{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'}]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( function: Optional = None with_indices: bool = False input_columns: Union = None batched: bool = False batch_size: int = 1000 drop_last_batch: bool = False remove_columns: Union = None fn_kwargs: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> def add_prefix(example):\n...     example[\"text\"] = \"Review: \" + example[\"text\"]\n...     return example\n>>> ds = ds.map(add_prefix)\n>>> next(iter(ds[\"train\"]))\n{'label': 1,\n 'text': 'Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( function: Optional = None with_indices = False input_columns: Union = None batched: bool = False batch_size: Optional = 1000 fn_kwargs: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds = ds.filter(lambda x: x[\"label\"] == 0)\n>>> list(ds[\"train\"].take(3))\n[{'label': 0, 'text': 'Review: simplistic , silly and tedious .'},\n {'label': 0,\n 'text': \"Review: it's so laddish and juvenile , only teenage boys could possibly find it funny .\"},\n {'label': 0,\n 'text': 'Review: exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]\n```", "```py\n( seed = None generator: Optional = None buffer_size: int = 1000 )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> list(ds[\"train\"].take(3))\n[{'label': 1,\n 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n {'label': 1,\n 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n {'label': 1, 'text': 'effective but too-tepid biopic'}]\n>>> ds = ds.shuffle(seed=42)\n>>> list(ds[\"train\"].take(3))\n[{'label': 1,\n 'text': \"a sports movie with action that's exciting on the field and a story you care about off it .\"},\n {'label': 1,\n 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},\n {'label': 1,\n 'text': \"sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune .\"}]\n```", "```py\n( type: Optional = None )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> def encode(example):\n...     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n>>> ds = ds.map(encode, batched=True, remove_columns=[\"text\"])\n>>> ds = ds.with_format(\"torch\")\n```", "```py\n( features: Features ) \u2192 export const metadata = 'undefined';IterableDatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> new_features = ds[\"train\"].features.copy()\n>>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n>>> new_features['text'] = Value('large_string')\n>>> ds = ds.cast(new_features)\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='large_string', id=None)}\n```", "```py\n( column: str feature: Union )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n>>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n>>> ds[\"train\"].features\n{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n 'text': Value(dtype='string', id=None)}\n```", "```py\n( column_names: Union ) \u2192 export const metadata = 'undefined';IterableDatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds = ds.remove_columns(\"label\")\n>>> next(iter(ds[\"train\"]))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( original_column_name: str new_column_name: str ) \u2192 export const metadata = 'undefined';IterableDatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds = ds.rename_column(\"text\", \"movie_review\")\n>>> next(iter(ds[\"train\"]))\n{'label': 1,\n 'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( column_mapping: Dict ) \u2192 export const metadata = 'undefined';IterableDatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds = ds.rename_columns({\"text\": \"movie_review\", \"label\": \"rating\"})\n>>> next(iter(ds[\"train\"]))\n{'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n 'rating': 1}\n```", "```py\n( column_names: Union ) \u2192 export const metadata = 'undefined';IterableDatasetDict\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n>>> ds = ds.select(\"text\")\n>>> next(iter(ds[\"train\"]))\n{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n```", "```py\n( *args **kwargs )\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n>>> copy_of_features = ds.features.copy()\n>>> copy_of_features\n{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n 'text': Value(dtype='string', id=None)}\n```", "```py\n( batch: dict token_per_repo_id: Optional = None )\n```", "```py\n( column: list column_name: str )\n```", "```py\n( example: dict token_per_repo_id: Optional = None )\n```", "```py\n( batch )\n```", "```py\n( column column_name: str )\n```", "```py\n( example )\n```", "```py\n( max_depth = 16 ) \u2192 export const metadata = 'undefined';Features\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"squad\", split=\"train\")\n>>> ds.features.flatten()\n{'answers.answer_start': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n 'answers.text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'context': Value(dtype='string', id=None),\n 'id': Value(dtype='string', id=None),\n 'question': Value(dtype='string', id=None),\n 'title': Value(dtype='string', id=None)}\n```", "```py\n( pa_schema: Schema )\n```", "```py\n( dic ) \u2192 export const metadata = 'undefined';Features\n```", "```py\n>>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})\n{'_type': Value(dtype='string', id=None)}\n```", "```py\n( other: Features )\n```", "```py\n>>> from datasets import Features, Sequence, Value\n>>> # let's say we have to features with a different order of nested fields (for a and b for example)\n>>> f1 = Features({\"root\": Sequence({\"a\": Value(\"string\"), \"b\": Value(\"string\")})})\n>>> f2 = Features({\"root\": {\"b\": Sequence(Value(\"string\")), \"a\": Sequence(Value(\"string\"))}})\n>>> assert f1.type != f2.type\n>>> # re-ordering keeps the base structure (here Sequence is defined at the root level), but make the fields order match\n>>> f1.reorder_fields_as(f2)\n{'root': Sequence(feature={'b': Value(dtype='string', id=None), 'a': Value(dtype='string', id=None)}, length=-1, id=None)}\n>>> assert f1.reorder_fields_as(f2).type == f2.type\n```", "```py\n( feature: Any length: int = -1 id: Optional = None )\n```", "```py\n>>> from datasets import Features, Sequence, Value, ClassLabel\n>>> features = Features({'post': Sequence(feature={'text': Value(dtype='string'), 'upvotes': Value(dtype='int32'), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'])})})\n>>> features\n{'post': Sequence(feature={'text': Value(dtype='string', id=None), 'upvotes': Value(dtype='int32', id=None), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'], id=None)}, length=-1, id=None)}\n```", "```py\n( num_classes: dataclasses.InitVar[typing.Optional[int]] = None names: List = None names_file: dataclasses.InitVar[typing.Optional[str]] = None id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'])})\n>>> features\n{'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'], id=None)}\n```", "```py\n( storage: Union ) \u2192 export const metadata = 'undefined';pa.Int64Array\n```", "```py\n( values: Union )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n>>> ds.features[\"label\"].int2str(0)\n'neg'\n```", "```py\n( values: Union )\n```", "```py\n>>> from datasets import load_dataset\n>>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n>>> ds.features[\"label\"].str2int('neg')\n0\n```", "```py\n( dtype: str id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'stars': Value(dtype='int32')})\n>>> features\n{'stars': Value(dtype='int32', id=None)}\n```", "```py\n( languages: List id: Optional = None )\n```", "```py\n>>> # At construction time:\n>>> datasets.features.Translation(languages=['en', 'fr', 'de'])\n>>> # During data generation:\n>>> yield {\n...         'en': 'the cat',\n...         'fr': 'le chat',\n...         'de': 'die katze'\n... }\n```", "```py\n( )\n```", "```py\n( languages: Optional = None num_languages: Optional = None id: Optional = None ) \u2192 export const metadata = 'undefined';\nlanguage or translation (variable-length 1D tf.Tensor of tf.string)\n```", "```py\n>>> # At construction time:\n>>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])\n>>> # During data generation:\n>>> yield {\n...         'en': 'the cat',\n...         'fr': ['le chat', 'la chatte,']\n...         'de': 'die katze'\n... }\n>>> # Tensor returned :\n>>> {\n...         'language': ['en', 'de', 'fr', 'fr'],\n...         'translation': ['the cat', 'die katze', 'la chatte', 'le chat'],\n... }\n```", "```py\n( )\n```", "```py\n( shape: tuple dtype: str id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})\n```", "```py\n( shape: tuple dtype: str id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})\n```", "```py\n( shape: tuple dtype: str id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})\n```", "```py\n( shape: tuple dtype: str id: Optional = None )\n```", "```py\n>>> from datasets import Features\n>>> features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})\n```", "```py\n( sampling_rate: Optional = None mono: bool = True decode: bool = True id: Optional = None )\n```", "```py\n>>> from datasets import load_dataset, Audio\n>>> ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> ds[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n     3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```", "```py\n( storage: Union ) \u2192 export const metadata = 'undefined';pa.StructArray\n```", "```py\n( value: dict token_per_repo_id: Optional = None ) \u2192 export const metadata = 'undefined';dict\n```", "```py\n( storage: StructArray ) \u2192 export const metadata = 'undefined';pa.StructArray\n```", "```py\n( value: Union ) \u2192 export const metadata = 'undefined';dict\n```", "```py\n( )\n```", "```py\n( decode: bool = True id: Optional = None )\n```", "```py\n>>> from datasets import load_dataset, Image\n>>> ds = load_dataset(\"beans\", split=\"train\")\n>>> ds.features[\"image\"]\nImage(decode=True, id=None)\n>>> ds[0][\"image\"]\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x15E52E7F0>\n>>> ds = ds.cast_column('image', Image(decode=False))\n{'bytes': None,\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/b0a21163f78769a2cf11f58dfc767fb458fc7cea5c05dccc0144a2c0f0bc1292/train/healthy/healthy_train.85.jpg'}\n```", "```py\n( storage: Union ) \u2192 export const metadata = 'undefined';pa.StructArray\n```", "```py\n( value: dict token_per_repo_id = None )\n```", "```py\n( storage: StructArray ) \u2192 export const metadata = 'undefined';pa.StructArray\n```", "```py\n( value: Union )\n```", "```py\n( )\n```", "```py\n( description: str citation: str features: Features inputs_description: str = <factory> homepage: str = <factory> license: str = <factory> codebase_urls: List = <factory> reference_urls: List = <factory> streamable: bool = False format: Optional = None metric_name: Optional = None config_name: Optional = None experiment_id: Optional = None )\n```", "```py\n( metric_info_dir )\n```", "```py\n>>> from datasets import MetricInfo\n>>> metric_info = MetricInfo.from_directory(\"/path/to/directory/\")\n```", "```py\n( metric_info_dir pretty_print = False )\n```", "```py\n>>> from datasets import load_metric\n>>> metric = load_metric(\"accuracy\")\n>>> metric.info.write_to_directory(\"/path/to/directory/\")\n```", "```py\n( config_name: Optional = None keep_in_memory: bool = False cache_dir: Optional = None num_process: int = 1 process_id: int = 0 seed: Optional = None experiment_id: Optional = None max_concurrent_cache_files: int = 10000 timeout: Union = 100 **kwargs )\n```", "```py\n( prediction = None reference = None **kwargs )\n```", "```py\n>>> from datasets import load_metric\n>>> metric = load_metric(\"accuracy\")\n>>> metric.add(predictions=model_predictions, references=labels)\n```", "```py\n( predictions = None references = None **kwargs )\n```", "```py\n>>> from datasets import load_metric\n>>> metric = load_metric(\"accuracy\")\n>>> metric.add_batch(predictions=model_prediction, references=labels)\n```", "```py\n( predictions = None references = None **kwargs )\n```", "```py\n>>> from datasets import load_metric\n>>> metric = load_metric(\"accuracy\")\n>>> accuracy = metric.compute(predictions=model_prediction, references=labels)\n```", "```py\n( download_config: Optional = None dl_manager: Optional = None )\n```", "```py\n( *args **kwargs )\n```", "```py\n>>> import datasets\n>>> s3 = datasets.filesystems.S3FileSystem(anon=True)\n>>> s3.ls('public-datasets/imdb/train')\n['dataset_info.json.json','dataset.arrow','state.json']\n```", "```py\n>>> import datasets\n>>> s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n>>> s3.ls('my-private-datasets/imdb/train')\n['dataset_info.json.json','dataset.arrow','state.json']\n```", "```py\n>>> import botocore\n>>> from datasets.filesystems import S3Filesystem\n\n>>> s3_session = botocore.session.Session(profile_name='my_profile_name')\n>>> s3 = S3FileSystem(session=s3_session)\n```", "```py\n>>> from datasets import load_from_disk\n>>> from datasets.filesystems import S3Filesystem\n\n>>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n>>> dataset = load_from_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)\n>>> print(len(dataset))\n25000\n```", "```py\n>>> from datasets import load_dataset\n>>> from datasets.filesystems import S3Filesystem\n\n>>> dataset = load_dataset(\"imdb\")\n>>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n>>> dataset.save_to_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)\n```", "```py\n( dataset_path: str )\n```", "```py\n( fs: AbstractFileSystem )\n```", "```py\n( )\n```"]