["```py\n{\n  \"base_model_name_or_path\": \"facebook/opt-350m\", #base model to apply LoRA to\n  \"bias\": \"none\",\n  \"fan_in_fan_out\": false,\n  \"inference_mode\": true,\n  \"init_lora_weights\": true,\n  \"layers_pattern\": null,\n  \"layers_to_transform\": null,\n  \"lora_alpha\": 32,\n  \"lora_dropout\": 0.05,\n  \"modules_to_save\": null,\n  \"peft_type\": \"LORA\", #PEFT method type\n  \"r\": 16,\n  \"revision\": null,\n  \"target_modules\": [\n    \"q_proj\", #model modules to apply LoRA to (query and value projection layers)\n    \"v_proj\"\n  ],\n  \"task_type\": \"CAUSAL_LM\" #type of task to train model on\n}\n```", "```py\nfrom peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n    lora_alpha=32,\n    lora_dropout=0.05\n)\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n```", "```py\nfrom peft import get_peft_model\n\nlora_model = get_peft_model(model, lora_config)\nlora_model.print_trainable_parameters()\n\"trainable params: 1,572,864 || all params: 332,769,280 || trainable%: 0.472659014678278\"\n```", "```py\n# save locally\nlora_model.save_pretrained(\"your-name/opt-350m-lora\")\n\n# push to Hub\nlora_model.push_to_hub(\"your-name/opt-350m-lora\")\n```", "```py\nfrom peft import PeftModel, PeftConfig\n\nconfig = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\")\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\nlora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\")\n```", "```py\nlora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\", is_trainable=True)\n```", "```py\nfrom peft import AutoPeftModelForCausalLM\n\nlora_model = AutoPeftModelForCausalLM.from_pretrained(\"ybelkada/opt-350m-lora\")\n```"]