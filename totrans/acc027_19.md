# ä½¿ç”¨ Local SGD ä¸ğŸ¤— Accelerate

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/accelerate/usage_guides/local_sgd`](https://huggingface.co/docs/accelerate/usage_guides/local_sgd)

Local SGD æ˜¯ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯ï¼Œå…¶ä¸­æ¢¯åº¦ä¸æ˜¯æ¯ä¸€æ­¥åŒæ­¥çš„ã€‚å› æ­¤ï¼Œæ¯ä¸ªè¿›ç¨‹æ›´æ–°è‡ªå·±çš„æ¨¡å‹æƒé‡ç‰ˆæœ¬ï¼Œå¹¶åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤ä¹‹åé€šè¿‡å¯¹æ‰€æœ‰è¿›ç¨‹è¿›è¡Œå¹³å‡æ¥åŒæ­¥è¿™äº›æƒé‡ã€‚è¿™æé«˜äº†é€šä¿¡æ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æœºç¼ºä¹æ›´å¿«çš„äº’è¿ï¼ˆå¦‚ NVLinkï¼‰æ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä¸æ¢¯åº¦ç´¯ç§¯ä¸åŒï¼ˆå…¶ä¸­æé«˜é€šä¿¡æ•ˆç‡éœ€è¦å¢åŠ æœ‰æ•ˆæ‰¹é‡å¤§å°ï¼‰ï¼ŒLocal SGD ä¸éœ€è¦æ›´æ”¹æ‰¹é‡å¤§å°æˆ–å­¦ä¹ ç‡/è°ƒåº¦ã€‚ä½†æ˜¯ï¼Œå¦‚æœå¿…è¦ï¼ŒLocal SGD ä¹Ÿå¯ä»¥ä¸æ¢¯åº¦ç´¯ç§¯ç»“åˆä½¿ç”¨ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•å¿«é€Ÿè®¾ç½® Local SGD ğŸ¤— Accelerateã€‚ä¸æ ‡å‡†çš„ Accelerate è®¾ç½®ç›¸æ¯”ï¼Œè¿™åªéœ€è¦é¢å¤–ä¸¤è¡Œä»£ç ã€‚

è¿™ä¸ªä¾‹å­å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„ PyTorch è®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸¤ä¸ªæ‰¹æ¬¡æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ç´¯ç§¯ï¼š

```py
device = "cuda"
model.to(device)

gradient_accumulation_steps = 2

for index, batch in enumerate(training_dataloader):
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model(inputs)
    loss = loss_function(outputs, targets)
    loss = loss / gradient_accumulation_steps
    loss.backward()
    if (index + 1) % gradient_accumulation_steps == 0:
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
```

## è½¬æ¢ä¸ºğŸ¤— Accelerate

é¦–å…ˆï¼Œä¹‹å‰æ˜¾ç¤ºçš„ä»£ç å°†è¢«è½¬æ¢ä¸ºä½¿ç”¨ğŸ¤— Accelerateï¼Œæ—¢ä¸ä½¿ç”¨ LocalSGD ä¹Ÿä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯åŠ©æ‰‹ï¼š

```py
+ from accelerate import Accelerator
+ accelerator = Accelerator()

+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(
+     model, optimizer, training_dataloader, scheduler
+ )

  for index, batch in enumerate(training_dataloader):
      inputs, targets = batch
-     inputs = inputs.to(device)
-     targets = targets.to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      loss = loss / gradient_accumulation_steps
+     accelerator.backward(loss)
      if (index+1) % gradient_accumulation_steps == 0:
          optimizer.step()
          scheduler.step()
```

## è®©ğŸ¤— Accelerate å¤„ç†æ¨¡å‹åŒæ­¥

ç°åœ¨å‰©ä¸‹çš„å°±æ˜¯è®©ğŸ¤— Accelerate å¤„ç†æ¨¡å‹å‚æ•°åŒæ­¥**å’Œ**æ¢¯åº¦ç´¯ç§¯ã€‚ä¸ºç®€å•èµ·è§ï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬éœ€è¦æ¯ 8 æ­¥åŒæ­¥ä¸€æ¬¡ã€‚é€šè¿‡æ·»åŠ ä¸€ä¸ª`with LocalSGD`è¯­å¥å’Œåœ¨æ¯ä¸ªä¼˜åŒ–å™¨æ­¥éª¤ä¹‹åè°ƒç”¨`local_sgd.step()`æ¥å®ç°ï¼š

```py
+local_sgd_steps=8

+with LocalSGD(accelerator=accelerator, model=model, local_sgd_steps=8, enabled=True) as local_sgd:
    for batch in training_dataloader:
        with accelerator.accumulate(model):
            inputs, targets = batch
            outputs = model(inputs)
            loss = loss_function(outputs, targets)
            accelerator.backward(loss)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
+           local_sgd.step()
```

åœ¨åº•å±‚ï¼ŒLocal SGD ä»£ç **ç¦ç”¨**äº†è‡ªåŠ¨æ¢¯åº¦åŒæ­¥ï¼ˆä½†ç´¯ç§¯ä»ç„¶æŒ‰é¢„æœŸå·¥ä½œï¼ï¼‰ã€‚ç›¸åï¼Œå®ƒæ¯`local_sgd_steps`æ­¥å¹³å‡æ¨¡å‹å‚æ•°ï¼ˆä»¥åŠåœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶ï¼‰ã€‚

## é™åˆ¶

å½“å‰å®ç°ä»…é€‚ç”¨äºåŸºæœ¬çš„å¤š GPUï¼ˆæˆ–å¤š CPUï¼‰è®­ç»ƒï¼Œä¸åŒ…æ‹¬ä¾‹å¦‚[DeepSpeed.](https://github.com/microsoft/DeepSpeed)ã€‚

## å‚è€ƒèµ„æ–™

å°½ç®¡æˆ‘ä»¬ä¸æ¸…æ¥šè¿™ç§ç®€å•æ–¹æ³•çš„çœŸæ­£èµ·æºï¼Œä½†æœ¬åœ° SGD çš„æƒ³æ³•éå¸¸å¤è€ï¼Œè‡³å°‘å¯ä»¥è¿½æº¯åˆ°ï¼š

Zhang, J., De Sa, C., Mitliagkas, I., & RÃ©, C. (2016). [Parallel SGD: When does averaging help?. arXiv preprint arXiv:1606.07365.](https://arxiv.org/abs/1606.07365)

æˆ‘ä»¬å°† Local SGD è¿™ä¸ªæœ¯è¯­å½’åŠŸäºä»¥ä¸‹è®ºæ–‡ï¼ˆä½†å¯èƒ½æœ‰æˆ‘ä»¬ä¸çŸ¥é“çš„æ›´æ—©çš„å‚è€ƒæ–‡çŒ®ï¼‰ã€‚

Stich, Sebastian Urban. [â€œLocal SGD Converges Fast and Communicates Little.â€ ICLR 2019-International Conference on Learning Representations. No. CONF. 2019.](https://arxiv.org/abs/1805.09767)
