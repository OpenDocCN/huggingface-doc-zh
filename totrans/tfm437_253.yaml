- en: BEiT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)
    by Hangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper
    that makes self-supervised pre-training of Vision Transformers (ViTs) outperform
    supervised pre-training. Rather than pre-training the model to predict the class
    of an image (as done in the [original ViT paper](https://arxiv.org/abs/2010.11929)),
    BEiT models are pre-trained to predict visual tokens from the codebook of OpenAIâ€™s
    [DALL-E model](https://arxiv.org/abs/2102.12092) given masked patches.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce a self-supervised vision representation model BEiT, which stands
    for Bidirectional Encoder representation from Image Transformers. Following BERT
    developed in the natural language processing area, we propose a masked image modeling
    task to pretrain vision Transformers. Specifically, each image has two views in
    our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens
    (i.e., discrete tokens). We first â€œtokenizeâ€ the original image into visual tokens.
    Then we randomly mask some image patches and fed them into the backbone Transformer.
    The pre-training objective is to recover the original visual tokens based on the
    corrupted image patches. After pre-training BEiT, we directly fine-tune the model
    parameters on downstream tasks by appending task layers upon the pretrained encoder.
    Experimental results on image classification and semantic segmentation show that
    our model achieves competitive results with previous pre-training methods. For
    example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly
    outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover,
    large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L
    with supervised pre-training on ImageNet-22K (85.2%).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The JAX/FLAX
    version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj).
    The original code can be found [here](https://github.com/microsoft/unilm/tree/master/beit).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BEiT models are regular Vision Transformers, but pre-trained in a self-supervised
    way rather than supervised. They outperform both the [original model (ViT)](vit)
    as well as [Data-efficient Image Transformers (DeiT)](deit) when fine-tuned on
    ImageNet-1K and CIFAR-100\. You can check out demo notebooks regarding inference
    as well as fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thereâ€™s also a demo notebook available which showcases how to combine DALL-Eâ€™s
    image tokenizer with BEiT for performing masked image modeling. You can find it
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the BEiT models expect each image to be of the same size (resolution), one
    can use [BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)
    to resize (or rescale) and normalize images for the model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the patch resolution and image resolution used during pre-training or fine-tuning
    are reflected in the name of each checkpoint. For example, `microsoft/beit-base-patch16-224`
    refers to a base-sized architecture with patch resolution of 16x16 and fine-tuning
    resolution of 224x224\. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒæœŸé—´ä½¿ç”¨çš„è¡¥ä¸åˆ†è¾¨ç‡å’Œå›¾åƒåˆ†è¾¨ç‡åæ˜ åœ¨æ¯ä¸ªæ£€æŸ¥ç‚¹çš„åç§°ä¸­ã€‚ä¾‹å¦‚ï¼Œ`microsoft/beit-base-patch16-224`æŒ‡çš„æ˜¯ä¸€ä¸ªåŸºæœ¬å¤§å°çš„æ¶æ„ï¼Œè¡¥ä¸åˆ†è¾¨ç‡ä¸º16x16ï¼Œå¾®è°ƒåˆ†è¾¨ç‡ä¸º224x224ã€‚æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨[hub](https://huggingface.co/models?search=microsoft/beit)ä¸Šæ‰¾åˆ°ã€‚
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ç”¨çš„æ£€æŸ¥ç‚¹è¦ä¹ˆï¼ˆ1ï¼‰ä»…åœ¨[ImageNet-22k](http://www.image-net.org/)ï¼ˆåŒ…å«1400ä¸‡å›¾åƒå’Œ22kç±»åˆ«ï¼‰ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œè¦ä¹ˆï¼ˆ2ï¼‰è¿˜åœ¨ImageNet-22kä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè¦ä¹ˆï¼ˆ3ï¼‰è¿˜åœ¨[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)ï¼ˆä¹Ÿç§°ä¸ºILSVRC
    2012ï¼ŒåŒ…å«130ä¸‡å›¾åƒå’Œ1000ç±»åˆ«ï¼‰ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚
- en: BEiT uses relative position embeddings, inspired by the T5 model. During pre-training,
    the authors shared the relative position bias among the several self-attention
    layers. During fine-tuning, each layerâ€™s relative position bias is initialized
    with the shared relative position bias obtained after pre-training. Note that,
    if one wants to pre-train a model from scratch, one needs to either set the `use_relative_position_bias`
    or the `use_relative_position_bias` attribute of [BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)
    to `True` in order to add position embeddings.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BEiTä½¿ç”¨ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå—T5æ¨¡å‹å¯å‘ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œä½œè€…åœ¨å‡ ä¸ªè‡ªæ³¨æ„åŠ›å±‚ä¹‹é—´å…±äº«äº†ç›¸å¯¹ä½ç½®åå·®ã€‚åœ¨å¾®è°ƒæœŸé—´ï¼Œæ¯ä¸ªå±‚çš„ç›¸å¯¹ä½ç½®åå·®éƒ½æ˜¯ç”¨é¢„è®­ç»ƒåè·å¾—çš„å…±äº«ç›¸å¯¹ä½ç½®åå·®åˆå§‹åŒ–çš„ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœè¦ä»å¤´å¼€å§‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œéœ€è¦å°†[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)çš„`use_relative_position_bias`æˆ–`use_relative_position_bias`å±æ€§è®¾ç½®ä¸º`True`ï¼Œä»¥æ·»åŠ ä½ç½®åµŒå…¥ã€‚
- en: '![drawing](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiT pre-training.
    Taken from the [original paper.](https://arxiv.org/abs/2106.08254)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç¤º](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiTé¢„è®­ç»ƒã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2106.08254)'
- en: Resources
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with BEiT.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç³»åˆ—å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºçš„åˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨BEiTã€‚
- en: Image Classification
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåˆ†ç±»
- en: '[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)
- en: '**Semantic segmentation**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¯­ä¹‰åˆ†å‰²**'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯­ä¹‰åˆ†å‰²ä»»åŠ¡æŒ‡å—](../tasks/semantic_segmentation)'
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: BEiT specific outputs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BEiTç‰¹å®šè¾“å‡º
- en: '### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Average of the last layer hidden states of the patch tokens (excluding the *[CLS]*
    token) if *config.use_mean_pooling* is set to True. If set to False, then the
    final hidden state of the *[CLS]* token will be returned.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`) â€” å¦‚æœ*config.use_mean_pooling*è®¾ç½®ä¸ºTrueï¼Œåˆ™æ˜¯è¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬*[CLS]*æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™å°†è¿”å›*[CLS]*æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: Class for outputs of [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äº[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)è¾“å‡ºçš„ç±»ã€‚
- en: '### `class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L44)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) â€” Average
    of the last layer hidden states of the patch tokens (excluding the *[CLS]* token)
    if *config.use_mean_pooling* is set to True. If set to False, then the final hidden
    state of the *[CLS]* token will be returned.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class for outputs of [FlaxBeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.FlaxBeitModel).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: BeitConfig
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitConfig`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/configuration_beit.py#L37)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 8192) â€” Vocabulary size of the
    BEiT model. Defines the number of different image tokens that can be used during
    pre-training.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) â€” The size (resolution) of
    each image.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) â€” The size (resolution) of
    each patch.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) â€” The number of input channels.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mask_token` (`bool`, *optional*, defaults to `False`) â€” Whether to use
    a mask token for masked image modeling.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    â€” Whether to use BERT-style absolute position embeddings.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use T5-style relative position embeddings in the self-attention layers.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    â€” Whether to use the same relative position embeddings across all self-attention
    layers of the Transformer.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_shared_relative_position_bias` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ Transformer
    çš„æ‰€æœ‰è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨ç›¸åŒçš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) â€” Scale to
    use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable
    layer scale.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä½¿ç”¨çš„æ¯”ä¾‹ã€‚åŸºç¡€ä¸º 0.1ï¼Œå¤§å‹ä¸º
    1e-5ã€‚è®¾ç½®ä¸º 0 ä»¥ç¦ç”¨å±‚æ¯”ä¾‹ã€‚'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) â€” Stochastic depth
    rate per sample (when applied in the main path of residual layers).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” æ¯ä¸ªæ ·æœ¬çš„éšæœºæ·±åº¦ç‡ï¼ˆåº”ç”¨äºæ®‹å·®å±‚çš„ä¸»è·¯å¾„ï¼‰ã€‚'
- en: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) â€” Whether to mean
    pool the final hidden states of the patches instead of using the final hidden
    state of the CLS token, before applying the classification head.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mean_pooling` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹è¡¥ä¸çš„æœ€ç»ˆéšè—çŠ¶æ€è¿›è¡Œå‡å€¼æ± åŒ–ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ CLS
    æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ååº”ç”¨åˆ†ç±»å¤´ã€‚'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) â€” Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[1, 2, 3, 6]`) â€” åœ¨æœ€åä¸€ä¸ªç‰¹å¾å›¾ä¸Šåº”ç”¨çš„æ± åŒ–é‡‘å­—å¡”æ¨¡å—ä¸­ä½¿ç”¨çš„æ± åŒ–æ¯”ä¾‹ã€‚'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) â€” Whether to
    use an auxiliary head during training.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è¾…åŠ©å¤´ã€‚'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) â€” Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.4) â€” è¾…åŠ©å¤´çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡ã€‚'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) â€” Number of channels
    to use in the auxiliary head.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„é€šé“æ•°ã€‚'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) â€” Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” è¾…åŠ©å¤´ä¸­è¦ä½¿ç”¨çš„å·ç§¯å±‚æ•°ã€‚'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) â€” Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨åˆ†ç±»å±‚ä¹‹å‰å°†è¾…åŠ©å¤´çš„è¾“å‡ºä¸è¾“å…¥è¿›è¡Œè¿æ¥ã€‚'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) â€” The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 255) â€” è¯­ä¹‰åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸­è¢«å¿½ç•¥çš„ç´¢å¼•ã€‚'
- en: '`out_features` (`List[str]`, *optional*) â€” If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ `"stem"`ã€`"stage1"`ã€`"stage2"`
    ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº† `out_indices`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_indices`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§
    `stage_names` å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: '`out_indices` (`List[int]`, *optional*) â€” If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *å¯é€‰*) â€” å¦‚æœç”¨ä½œéª¨å¹²ï¼Œè¦è¾“å‡ºçš„ç‰¹å¾çš„ç´¢å¼•åˆ—è¡¨ã€‚å¯ä»¥æ˜¯ 0ã€1ã€2 ç­‰ï¼ˆå–å†³äºæ¨¡å‹æœ‰å¤šå°‘é˜¶æ®µï¼‰ã€‚å¦‚æœæœªè®¾ç½®ä¸”è®¾ç½®äº†
    `out_features`ï¼Œå°†é»˜è®¤ä¸ºç›¸åº”çš„é˜¶æ®µã€‚å¦‚æœæœªè®¾ç½®ä¸”æœªè®¾ç½® `out_features`ï¼Œå°†é»˜è®¤ä¸ºæœ€åä¸€ä¸ªé˜¶æ®µã€‚å¿…é¡»æŒ‰ç…§ `stage_names`
    å±æ€§ä¸­å®šä¹‰çš„é¡ºåºã€‚'
- en: '`add_fpn` (`bool`, *optional*, defaults to `False`) â€” Whether to add a FPN
    as part of the backbone. Only relevant for `BeitBackbone`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_fpn` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°† FPN æ·»åŠ ä¸ºéª¨å¹²çš„ä¸€éƒ¨åˆ†ã€‚ä»…é€‚ç”¨äº `BeitBackbone`ã€‚'
- en: '`reshape_hidden_states` (`bool`, *optional*, defaults to `True`) â€” Whether
    to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size,
    height, width)` in case the model is used as backbone. If `False`, the feature
    maps will be 3D tensors of shape `(batch_size, seq_len, hidden_size)`. Only relevant
    for `BeitBackbone`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshape_hidden_states` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨å°†æ¨¡å‹ç”¨ä½œéª¨å¹²æ—¶å°†ç‰¹å¾å›¾é‡å¡‘ä¸ºå½¢çŠ¶ä¸º
    `(batch_size, hidden_size, height, width)` çš„4Då¼ é‡ã€‚å¦‚æœä¸º `False`ï¼Œç‰¹å¾å›¾å°†æ˜¯å½¢çŠ¶ä¸º `(batch_size,
    seq_len, hidden_size)` çš„3Då¼ é‡ã€‚ä»…é€‚ç”¨äº `BeitBackbone`ã€‚'
- en: This is the configuration class to store the configuration of a [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel).
    It is used to instantiate an BEiT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k)
    architecture.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)
    çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª BEiT æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k)
    æ¶æ„çš„é…ç½®ã€‚
- en: 'Example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: BeitFeatureExtractor
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitFeatureExtractor
- en: '### `class transformers.BeitFeatureExtractor`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `__call__`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)ï¼‰
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`ï¼ˆé•¿åº¦ä¸º `batch_size` çš„ `List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰ â€” æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå°ºå¯¸ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿åº¦ä¸º `batch_size` çš„ `List[torch.Tensor]`ï¼Œæ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äº
    `target_sizes` æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºè¯­ä¹‰ç±»åˆ« idã€‚
- en: Converts the output of [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å°† [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚
- en: BeitImageProcessor
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitImageProcessor
- en: '### `class transformers.BeitImageProcessor`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) â€” Whether to resize the
    imageâ€™s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†å›¾åƒçš„ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å°ºå¯¸è°ƒæ•´ä¸ºæŒ‡å®šçš„ `size`ã€‚å¯ä»¥é€šè¿‡ `preprocess`
    æ–¹æ³•ä¸­çš„ `do_resize` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the output image after resizing. Can be overridden by the `size`
    parameter in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 256, "width": 256}`)ï¼šè°ƒæ•´å¤§å°åçš„è¾“å‡ºå›¾åƒå°ºå¯¸ã€‚å¯ä»¥é€šè¿‡
    `preprocess` æ–¹æ³•ä¸­çš„ `size` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    â€” Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `Resampling.BICUBIC`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥é€šè¿‡
    `preprocess` æ–¹æ³•ä¸­çš„ `resample` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) â€” Whether to center
    crop the image. If the input size is smaller than `crop_size` along any edge,
    the image is padded with 0â€™s and then center cropped. Can be overridden by the
    `do_center_crop` parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¦‚æœè¾“å…¥å°ºå¯¸æ²¿ä»»ä½•è¾¹ç¼˜å°äº `crop_size`ï¼Œåˆ™å›¾åƒå°†å¡«å……ä¸º
    0ï¼Œç„¶åè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_center_crop` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 224, "width":
    224}`): Desired output size when applying center-cropping. Only has an effect
    if `do_center_crop` is set to `True`. Can be overridden by the `crop_size` parameter
    in the `preprocess` method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `{"height" -- 224, "width": 224}`)ï¼šåº”ç”¨ä¸­å¿ƒè£å‰ªæ—¶çš„æœŸæœ›è¾“å‡ºå°ºå¯¸ã€‚ä»…åœ¨
    `do_center_crop` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `crop_size` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) â€” Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` æˆ– `float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥é€šè¿‡
    `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) â€” Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥é€šè¿‡
    `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) â€” Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„
    `do_normalize` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    â€” The mean to use if normalizing the image. This is a float or list of floats
    of length of the number of channels of the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`) â€”
    å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `image_mean`
    å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    â€” The standard deviation to use if normalizing the image. This is a float or list
    of floats of length of the number of channels of the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` æˆ– `List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `IMAGENET_STANDARD_STD`) â€” å¦‚æœå¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥é€šè¿‡
    `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to reduce all label values of segmentation maps by 1\. Usually used for datasets
    where 0 is used for background, and background itself is not included in all classes
    of a dataset (e.g. ADE20k). The background label will be replaced by 255\. Can
    be overridden by the `do_reduce_labels` parameter in the `preprocess` method.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦å‡å°‘æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­å°† 0 ç”¨äºèƒŒæ™¯ï¼Œä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­ï¼ˆä¾‹å¦‚
    ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º 255ã€‚å¯ä»¥é€šè¿‡ `preprocess` æ–¹æ³•ä¸­çš„ `do_reduce_labels` å‚æ•°è¿›è¡Œè¦†ç›–ã€‚'
- en: Constructs a BEiT image processor.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»º BEiT å›¾åƒå¤„ç†å™¨ã€‚
- en: '#### `preprocess`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)'
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`images` (`ImageInput`) â€” Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) â€” é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º0åˆ°255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) â€” Whether to
    resize the image.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_resize`) â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) â€” Size of the
    image after resizing.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.size`) â€” è°ƒæ•´å¤§å°åçš„å›¾åƒå¤§å°ã€‚'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) â€” Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *å¯é€‰*, é»˜è®¤ä¸º`self.resample`) â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚è¿™å¯ä»¥æ˜¯æšä¸¾`PILImageResampling`ä¹‹ä¸€ï¼Œä»…åœ¨è®¾ç½®`do_resize=True`æ—¶æœ‰æ•ˆã€‚'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) â€”
    Whether to center crop the image.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_center_crop`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) â€”
    Size of the image after center crop. If one edge the image is smaller than `crop_size`,
    it will be padded with zeros and then cropped'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º`self.crop_size`) â€” ä¸­å¿ƒè£å‰ªåçš„å›¾åƒå¤§å°ã€‚å¦‚æœå›¾åƒçš„ä¸€æ¡è¾¹å°äº`crop_size`ï¼Œåˆ™å°†ç”¨é›¶å¡«å……ï¼Œç„¶åè£å‰ªã€‚'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) â€” Whether
    to rescale the image values between [0 - 1].'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾ä¸º[0 - 1]ã€‚'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) â€”
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *å¯é€‰*, é»˜è®¤ä¸º`self.rescale_factor`) â€” å¦‚æœè®¾ç½®`do_rescale=True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) â€” Whether
    to normalize the image.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    â€” Image mean.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º`self.image_mean`) â€” å›¾åƒå‡å€¼ã€‚'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    â€” Image standard deviation.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` æˆ– `List[float]`, *å¯é€‰*, é»˜è®¤ä¸º`self.image_std`) â€” å›¾åƒæ ‡å‡†å·®ã€‚'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `self.do_reduce_labels`)
    â€” Whether or not to reduce all label values of segmentation maps by 1\. Usually
    used for datasets where 0 is used for background, and background itself is not
    included in all classes of a dataset (e.g. ADE20k). The background label will
    be replaced by 255.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`self.do_reduce_labels`) â€” æ˜¯å¦å‡å°‘æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­ä½¿ç”¨0è¡¨ç¤ºèƒŒæ™¯ï¼Œå¹¶ä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­ï¼ˆä¾‹å¦‚ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º255ã€‚'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) â€” The type of tensors
    to return. Can be one of:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– `TensorType`, *å¯é€‰*) â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` æˆ– `''tf''`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`tf.Tensor`çš„æ‰¹é‡ã€‚'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` æˆ– `''pt''`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`torch.Tensor`çš„æ‰¹é‡ã€‚'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` æˆ– `''np''`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`np.ndarray`çš„æ‰¹é‡ã€‚'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` æˆ– `''jax''`ï¼šè¿”å›ä¸€ä¸ªç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹é‡ã€‚'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    â€” The channel dimension format for the output image. Can be one of:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º`ChannelDimension.FIRST`)
    â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) â€” The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` æˆ– `str`, *å¯é€‰*) â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` æˆ– `ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` æˆ– `ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` æˆ– `ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚'
- en: Preprocess an image or batch of images.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`outputs` ([BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation))
    â€” Raw outputs of the model.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`ï¼ˆ[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)ï¼‰
    â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) â€” List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`ï¼Œé•¿åº¦ä¸º`batch_size`ï¼Œ*å¯é€‰*) â€” ä¸æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰å¯¹åº”çš„å…ƒç»„åˆ—è¡¨ã€‚å¦‚æœæœªè®¾ç½®ï¼Œé¢„æµ‹å°†ä¸ä¼šè¢«è°ƒæ•´å¤§å°ã€‚'
- en: Returns
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: semantic_segmentation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­ä¹‰åˆ†å‰²
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]` é•¿åº¦ä¸º `batch_size`ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®æ˜¯å½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº†
    `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚'
- en: Converts the output of [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å°† [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒPyTorchã€‚
- en: PytorchHide Pytorch content
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: BeitModel
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitModel
- en: '### `class transformers.BeitModel`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)'
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Beit Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„Beitæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)'
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„
    `hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`,
    *optional*) â€” Boolean masked positions. Indicates which patches are masked (1)
    and which arenâ€™t (0).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`,
    *optional*) â€” å¸ƒå°”æ©ç ä½ç½®ã€‚æŒ‡ç¤ºå“ªäº›è¡¥ä¸è¢«æ©ç›–ï¼ˆ1ï¼‰å“ªäº›æ²¡æœ‰ï¼ˆ0ï¼‰ã€‚'
- en: Returns
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    æˆ– `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Average of the last layer hidden states of the patch tokens (excluding the *[CLS]*
    token) if *config.use_mean_pooling* is set to True. If set to False, then the
    final hidden state of the *[CLS]* token will be returned.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰- å¦‚æœ*config.use_mean_pooling*è®¾ç½®ä¸ºTrueï¼Œåˆ™æ˜¯è¡¥ä¸æ ‡è®°çš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ï¼ˆä¸åŒ…æ‹¬*[CLS]*æ ‡è®°ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™å°†è¿”å›*[CLS]*æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º
    + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)
    forward method, overrides the `__call__` special method.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: BeitForMaskedImageModeling
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitForMaskedImageModeling
- en: '### `class transformers.BeitForMaskedImageModeling`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitForMaskedImageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)'
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)ï¼‰-
    æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Beit Model transformer with a â€˜languageâ€™ modeling head on top. BEiT does masked
    image modeling by predicting visual tokens of a Vector-Quantize Variational Autoencoder
    (VQ-VAE), whereas other vision models like ViT and DeiT predict RGB pixel values.
    As a result, this class is incompatible with [AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling),
    so you will need to use [BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)
    directly if you wish to do masked image modeling with BEiT. This model is a PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and behavior.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Beitæ¨¡å‹å˜å‹å™¨é¡¶éƒ¨å¸¦æœ‰â€œè¯­è¨€â€å»ºæ¨¡å¤´ã€‚BEiTé€šè¿‡é¢„æµ‹çŸ¢é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQ-VAEï¼‰çš„è§†è§‰æ ‡è®°æ¥è¿›è¡Œé®è”½å›¾åƒå»ºæ¨¡ï¼Œè€Œå…¶ä»–è§†è§‰æ¨¡å‹å¦‚ViTå’ŒDeiTåˆ™é¢„æµ‹RGBåƒç´ å€¼ã€‚å› æ­¤ï¼Œæ­¤ç±»ä¸[AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling)ä¸å…¼å®¹ï¼Œå› æ­¤å¦‚æœè¦ä½¿ç”¨BEiTè¿›è¡Œé®è”½å›¾åƒå»ºæ¨¡ï¼Œåˆ™éœ€è¦ç›´æ¥ä½¿ç”¨[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)ã€‚'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`)
    â€” Boolean masked positions. Indicates which patches are masked (1) and which arenâ€™t
    (0).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Masked language modeling (MLM) loss.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)
    forward method, overrides the `__call__` special method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: BeitForImageClassification
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitForImageClassification`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L832)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beit Model transformer with an image classification head on top (a linear layer
    on top of the average of the final hidden states of the patch tokens) e.g. for
    ImageNet.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L852)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: BeitForSemanticSegmentation
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitForSemanticSegmentation`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1156)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k,
    CityScapes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1214)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) â€” Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    â€” Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) â€” Classification scores for each pixel.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: JAXHide JAX content
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: FlaxBeitModel
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitModel`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L741)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The bare Beit Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Returns
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) â€” Average
    of the last layer hidden states of the patch tokens (excluding the *[CLS]* token)
    if *config.use_mean_pooling* is set to True. If set to False, then the final hidden
    state of the *[CLS]* token will be returned.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: FlaxBeitForMaskedImageModeling
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitForMaskedImageModeling`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L825)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Beit Model transformer with a â€˜languageâ€™ modeling head on top (to predict visual
    tokens).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Returns
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'bool_masked_pos (`numpy.ndarray` of shape `(batch_size, num_patches)`): Boolean
    masked positions. Indicates which patches are masked (1) and which arenâ€™t (0).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: FlaxBeitForImageClassification
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitForImageClassification`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L909)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Beit Model transformer with an image classification head on top (a linear layer
    on top of the average of the final hidden states of the patch tokens) e.g. for
    ImageNet.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Returns
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
