- en: BEiT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/beit)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)
    by Hangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper
    that makes self-supervised pre-training of Vision Transformers (ViTs) outperform
    supervised pre-training. Rather than pre-training the model to predict the class
    of an image (as done in the [original ViT paper](https://arxiv.org/abs/2010.11929)),
    BEiT models are pre-trained to predict visual tokens from the codebook of OpenAI’s
    [DALL-E model](https://arxiv.org/abs/2102.12092) given masked patches.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce a self-supervised vision representation model BEiT, which stands
    for Bidirectional Encoder representation from Image Transformers. Following BERT
    developed in the natural language processing area, we propose a masked image modeling
    task to pretrain vision Transformers. Specifically, each image has two views in
    our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens
    (i.e., discrete tokens). We first “tokenize” the original image into visual tokens.
    Then we randomly mask some image patches and fed them into the backbone Transformer.
    The pre-training objective is to recover the original visual tokens based on the
    corrupted image patches. After pre-training BEiT, we directly fine-tune the model
    parameters on downstream tasks by appending task layers upon the pretrained encoder.
    Experimental results on image classification and semantic segmentation show that
    our model achieves competitive results with previous pre-training methods. For
    example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly
    outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover,
    large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L
    with supervised pre-training on ImageNet-22K (85.2%).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The JAX/FLAX
    version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj).
    The original code can be found [here](https://github.com/microsoft/unilm/tree/master/beit).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BEiT models are regular Vision Transformers, but pre-trained in a self-supervised
    way rather than supervised. They outperform both the [original model (ViT)](vit)
    as well as [Data-efficient Image Transformers (DeiT)](deit) when fine-tuned on
    ImageNet-1K and CIFAR-100\. You can check out demo notebooks regarding inference
    as well as fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)
    (you can just replace [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)
    by [BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)
    and [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)
    by [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s also a demo notebook available which showcases how to combine DALL-E’s
    image tokenizer with BEiT for performing masked image modeling. You can find it
    [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the BEiT models expect each image to be of the same size (resolution), one
    can use [BeitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitImageProcessor)
    to resize (or rescale) and normalize images for the model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the patch resolution and image resolution used during pre-training or fine-tuning
    are reflected in the name of each checkpoint. For example, `microsoft/beit-base-patch16-224`
    refers to a base-sized architecture with patch resolution of 16x16 and fine-tuning
    resolution of 224x224\. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预训练或微调期间使用的补丁分辨率和图像分辨率反映在每个检查点的名称中。例如，`microsoft/beit-base-patch16-224`指的是一个基本大小的架构，补丁分辨率为16x16，微调分辨率为224x224。所有检查点都可以在[hub](https://huggingface.co/models?search=microsoft/beit)上找到。
- en: The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/)
    (a collection of 14 million images and 22k classes) only, (2) also fine-tuned
    on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)
    (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000
    classes).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用的检查点要么（1）仅在[ImageNet-22k](http://www.image-net.org/)（包含1400万图像和22k类别）上进行了预训练，要么（2）还在ImageNet-22k上进行了微调，要么（3）还在[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)（也称为ILSVRC
    2012，包含130万图像和1000类别）上进行了微调。
- en: BEiT uses relative position embeddings, inspired by the T5 model. During pre-training,
    the authors shared the relative position bias among the several self-attention
    layers. During fine-tuning, each layer’s relative position bias is initialized
    with the shared relative position bias obtained after pre-training. Note that,
    if one wants to pre-train a model from scratch, one needs to either set the `use_relative_position_bias`
    or the `use_relative_position_bias` attribute of [BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)
    to `True` in order to add position embeddings.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BEiT使用相对位置嵌入，受T5模型启发。在预训练期间，作者在几个自注意力层之间共享了相对位置偏差。在微调期间，每个层的相对位置偏差都是用预训练后获得的共享相对位置偏差初始化的。请注意，如果要从头开始预训练模型，需要将[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)的`use_relative_position_bias`或`use_relative_position_bias`属性设置为`True`，以添加位置嵌入。
- en: '![drawing](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiT pre-training.
    Taken from the [original paper.](https://arxiv.org/abs/2106.08254)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/6740a7101e0ab9d00b7aec6fa2bd9f0c.png) BEiT预训练。摘自[原始论文。](https://arxiv.org/abs/2106.08254)'
- en: Resources
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with BEiT.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列官方Hugging Face和社区（由🌎表示）资源的列表，可帮助您开始使用BEiT。
- en: Image Classification
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类
- en: '[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)
    is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)
    and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。'
- en: 'See also: [Image classification task guide](../tasks/image_classification)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另请参阅：[图像分类任务指南](../tasks/image_classification)
- en: '**Semantic segmentation**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义分割**'
- en: '[Semantic segmentation task guide](../tasks/semantic_segmentation)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义分割任务指南](../tasks/semantic_segmentation)'
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新东西，而不是重复现有资源。
- en: BEiT specific outputs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BEiT特定输出
- en: '### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L69)'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Average of the last layer hidden states of the patch tokens (excluding the *[CLS]*
    token) if *config.use_mean_pooling* is set to True. If set to False, then the
    final hidden state of the *[CLS]* token will be returned.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 如果*config.use_mean_pooling*设置为True，则是补丁标记的最后一层隐藏状态的平均值（不包括*[CLS]*标记）。如果设置为False，则将返回*[CLS]*标记的最终隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Class for outputs of [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用于[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)输出的类。
- en: '### `class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L44)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) — Average
    of the last layer hidden states of the patch tokens (excluding the *[CLS]* token)
    if *config.use_mean_pooling* is set to True. If set to False, then the final hidden
    state of the *[CLS]* token will be returned.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class for outputs of [FlaxBeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.FlaxBeitModel).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: BeitConfig
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitConfig`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/configuration_beit.py#L37)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 8192) — Vocabulary size of the
    BEiT model. Defines the number of different image tokens that can be used during
    pre-training.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_mask_token` (`bool`, *optional*, defaults to `False`) — Whether to use
    a mask token for masked image modeling.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_absolute_position_embeddings` (`bool`, *optional*, defaults to `False`)
    — Whether to use BERT-style absolute position embeddings.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_relative_position_bias` (`bool`, *optional*, defaults to `False`) — Whether
    to use T5-style relative position embeddings in the self-attention layers.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_shared_relative_position_bias` (`bool`, *optional*, defaults to `False`)
    — Whether to use the same relative position embeddings across all self-attention
    layers of the Transformer.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_shared_relative_position_bias` (`bool`, *可选*, 默认为 `False`) — 是否在 Transformer
    的所有自注意力层中使用相同的相对位置嵌入。'
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 0.1) — Scale to
    use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable
    layer scale.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_scale_init_value` (`float`, *可选*, 默认为 0.1) — 在自注意力层中使用的比例。基础为 0.1，大型为
    1e-5。设置为 0 以禁用层比例。'
- en: '`drop_path_rate` (`float`, *optional*, defaults to 0.1) — Stochastic depth
    rate per sample (when applied in the main path of residual layers).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`drop_path_rate` (`float`, *可选*, 默认为 0.1) — 每个样本的随机深度率（应用于残差层的主路径）。'
- en: '`use_mean_pooling` (`bool`, *optional*, defaults to `True`) — Whether to mean
    pool the final hidden states of the patches instead of using the final hidden
    state of the CLS token, before applying the classification head.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_mean_pooling` (`bool`, *可选*, 默认为 `True`) — 是否对补丁的最终隐藏状态进行均值池化，而不是使用 CLS
    标记的最终隐藏状态后应用分类头。'
- en: '`pool_scales` (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`) — Pooling
    scales used in Pooling Pyramid Module applied on the last feature map.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool_scales` (`Tuple[int]`, *可选*, 默认为 `[1, 2, 3, 6]`) — 在最后一个特征图上应用的池化金字塔模块中使用的池化比例。'
- en: '`use_auxiliary_head` (`bool`, *optional*, defaults to `True`) — Whether to
    use an auxiliary head during training.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_auxiliary_head` (`bool`, *可选*, 默认为 `True`) — 是否在训练过程中使用辅助头。'
- en: '`auxiliary_loss_weight` (`float`, *optional*, defaults to 0.4) — Weight of
    the cross-entropy loss of the auxiliary head.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_loss_weight` (`float`, *可选*, 默认为 0.4) — 辅助头的交叉熵损失的权重。'
- en: '`auxiliary_channels` (`int`, *optional*, defaults to 256) — Number of channels
    to use in the auxiliary head.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_channels` (`int`, *可选*, 默认为 256) — 辅助头中要使用的通道数。'
- en: '`auxiliary_num_convs` (`int`, *optional*, defaults to 1) — Number of convolutional
    layers to use in the auxiliary head.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_num_convs` (`int`, *可选*, 默认为 1) — 辅助头中要使用的卷积层数。'
- en: '`auxiliary_concat_input` (`bool`, *optional*, defaults to `False`) — Whether
    to concatenate the output of the auxiliary head with the input before the classification
    layer.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auxiliary_concat_input` (`bool`, *可选*, 默认为 `False`) — 是否在分类层之前将辅助头的输出与输入进行连接。'
- en: '`semantic_loss_ignore_index` (`int`, *optional*, defaults to 255) — The index
    that is ignored by the loss function of the semantic segmentation model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semantic_loss_ignore_index` (`int`, *可选*, 默认为 255) — 语义分割模型的损失函数中被忽略的索引。'
- en: '`out_features` (`List[str]`, *optional*) — If used as backbone, list of features
    to output. Can be any of `"stem"`, `"stage1"`, `"stage2"`, etc. (depending on
    how many stages the model has). If unset and `out_indices` is set, will default
    to the corresponding stages. If unset and `out_indices` is unset, will default
    to the last stage. Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_features` (`List[str]`, *可选*) — 如果用作骨干，要输出的特征列表。可以是 `"stem"`、`"stage1"`、`"stage2"`
    等（取决于模型有多少阶段）。如果未设置且设置了 `out_indices`，将默认为相应的阶段。如果未设置且未设置 `out_indices`，将默认为最后一个阶段。必须按照
    `stage_names` 属性中定义的顺序。'
- en: '`out_indices` (`List[int]`, *optional*) — If used as backbone, list of indices
    of features to output. Can be any of 0, 1, 2, etc. (depending on how many stages
    the model has). If unset and `out_features` is set, will default to the corresponding
    stages. If unset and `out_features` is unset, will default to the last stage.
    Must be in the same order as defined in the `stage_names` attribute.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_indices` (`List[int]`, *可选*) — 如果用作骨干，要输出的特征的索引列表。可以是 0、1、2 等（取决于模型有多少阶段）。如果未设置且设置了
    `out_features`，将默认为相应的阶段。如果未设置且未设置 `out_features`，将默认为最后一个阶段。必须按照 `stage_names`
    属性中定义的顺序。'
- en: '`add_fpn` (`bool`, *optional*, defaults to `False`) — Whether to add a FPN
    as part of the backbone. Only relevant for `BeitBackbone`.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_fpn` (`bool`, *可选*, 默认为 `False`) — 是否将 FPN 添加为骨干的一部分。仅适用于 `BeitBackbone`。'
- en: '`reshape_hidden_states` (`bool`, *optional*, defaults to `True`) — Whether
    to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size,
    height, width)` in case the model is used as backbone. If `False`, the feature
    maps will be 3D tensors of shape `(batch_size, seq_len, hidden_size)`. Only relevant
    for `BeitBackbone`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshape_hidden_states` (`bool`, *可选*, 默认为 `True`) — 是否在将模型用作骨干时将特征图重塑为形状为
    `(batch_size, hidden_size, height, width)` 的4D张量。如果为 `False`，特征图将是形状为 `(batch_size,
    seq_len, hidden_size)` 的3D张量。仅适用于 `BeitBackbone`。'
- en: This is the configuration class to store the configuration of a [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel).
    It is used to instantiate an BEiT model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k)
    architecture.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储 [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)
    的配置。根据指定的参数实例化一个 BEiT 模型，定义模型架构。使用默认值实例化配置将产生类似于 BEiT [microsoft/beit-base-patch16-224-pt22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k)
    架构的配置。
- en: 'Example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: BeitFeatureExtractor
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitFeatureExtractor
- en: '### `class transformers.BeitFeatureExtractor`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/feature_extraction_beit.py#L26)'
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#### `__call__`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L307)'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)）
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes`（长度为 `batch_size` 的 `List[Tuple]`，*可选*） — 每个预测的请求最终尺寸（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 长度为 `batch_size` 的 `List[torch.Tensor]`，每个项目是形状为 (height, width) 的语义分割地图，对应于
    `target_sizes` 条目（如果指定）。每个 `torch.Tensor` 的每个条目对应于语义类别 id。
- en: Converts the output of [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    的输出转换为语义分割地图。仅支持 PyTorch。
- en: BeitImageProcessor
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitImageProcessor
- en: '### `class transformers.BeitImageProcessor`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L49)'
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`，*可选*，默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的 `size`。可以通过 `preprocess`
    方法中的 `do_resize` 参数进行覆盖。'
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the output image after resizing. Can be overridden by the `size`
    parameter in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]` *可选*，默认为 `{"height" -- 256, "width": 256}`)：调整大小后的输出图像尺寸。可以通过
    `preprocess` 方法中的 `size` 参数进行覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`，*可选*，默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则要使用的重采样滤波器。可以通过
    `preprocess` 方法中的 `resample` 参数进行覆盖。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image. If the input size is smaller than `crop_size` along any edge,
    the image is padded with 0’s and then center cropped. Can be overridden by the
    `do_center_crop` parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`，*可选*，默认为 `True`) — 是否对图像进行中心裁剪。如果输入尺寸沿任何边缘小于 `crop_size`，则图像将填充为
    0，然后进行中心裁剪。可以通过 `preprocess` 方法中的 `do_center_crop` 参数进行覆盖。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 224, "width":
    224}`): Desired output size when applying center-cropping. Only has an effect
    if `do_center_crop` is set to `True`. Can be overridden by the `crop_size` parameter
    in the `preprocess` method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`，*可选*，默认为 `{"height" -- 224, "width": 224}`)：应用中心裁剪时的期望输出尺寸。仅在
    `do_center_crop` 设置为 `True` 时有效。可以通过 `preprocess` 方法中的 `crop_size` 参数进行覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`，*可选*，默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。可以通过
    `preprocess` 方法中的 `rescale_factor` 参数进行覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`，*可选*，默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以通过
    `preprocess` 方法中的 `do_rescale` 参数进行覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`，*可选*，默认为 `True`) — 是否对图像进行归一化。可以通过 `preprocess` 方法中的
    `do_normalize` 参数进行覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — The mean to use if normalizing the image. This is a float or list of floats
    of length of the number of channels of the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_MEAN`) —
    如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以通过 `preprocess` 方法中的 `image_mean`
    参数进行覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — The standard deviation to use if normalizing the image. This is a float or list
    of floats of length of the number of channels of the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`，*可选*，默认为 `IMAGENET_STANDARD_STD`) — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以通过
    `preprocess` 方法中的 `image_std` 参数进行覆盖。'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `False`) — Whether or not
    to reduce all label values of segmentation maps by 1\. Usually used for datasets
    where 0 is used for background, and background itself is not included in all classes
    of a dataset (e.g. ADE20k). The background label will be replaced by 255\. Can
    be overridden by the `do_reduce_labels` parameter in the `preprocess` method.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels` (`bool`，*可选*，默认为 `False`) — 是否减少所有分割地图的标签值。通常用于数据集中将 0 用于背景，且背景本身不包含在数据集的所有类中（例如
    ADE20k）。背景标签将被替换为 255。可以通过 `preprocess` 方法中的 `do_reduce_labels` 参数进行覆盖。'
- en: Constructs a BEiT image processor.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 BEiT 图像处理器。
- en: '#### `preprocess`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L312)'
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 预处理的图像。期望单个或批量图像，像素值范围为0到255。如果传入像素值在0到1之间的图像，请设置`do_rescale=False`。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为`self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *可选*, 默认为`self.size`) — 调整大小后的图像大小。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *可选*, 默认为`self.resample`) — 调整图像大小时要使用的重采样滤波器。这可以是枚举`PILImageResampling`之一，仅在设置`do_resize=True`时有效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *可选*, 默认为`self.do_center_crop`) — 是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the image after center crop. If one edge the image is smaller than `crop_size`,
    it will be padded with zeros and then cropped'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *可选*, 默认为`self.crop_size`) — 中心裁剪后的图像大小。如果图像的一条边小于`crop_size`，则将用零填充，然后裁剪。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为`self.do_rescale`) — 是否将图像值重新缩放为[0 - 1]。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *可选*, 默认为`self.rescale_factor`) — 如果设置`do_rescale=True`，则用于重新缩放图像的重新缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为`self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *可选*, 默认为`self.image_mean`) — 图像均值。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为`self.image_std`) — 图像标准差。'
- en: '`do_reduce_labels` (`bool`, *optional*, defaults to `self.do_reduce_labels`)
    — Whether or not to reduce all label values of segmentation maps by 1\. Usually
    used for datasets where 0 is used for background, and background itself is not
    included in all classes of a dataset (e.g. ADE20k). The background label will
    be replaced by 255.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_reduce_labels` (`bool`, *可选*, 默认为`self.do_reduce_labels`) — 是否减少所有分割地图的标签值。通常用于数据集中使用0表示背景，并且背景本身不包含在数据集的所有类中（例如ADE20k）。背景标签将被替换为255。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个`np.ndarray`列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW` 或 `''tf''`：返回一个类型为`tf.Tensor`的批量。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH` 或 `''pt''`：返回一个类型为`torch.Tensor`的批量。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY` 或 `''np''`：返回一个类型为`np.ndarray`的批量。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX` 或 `''jax''`：返回一个类型为`jax.numpy.ndarray`的批量。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为`ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，将从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"` 或 `ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"` 或 `ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"` 或 `ChannelDimension.NONE`：图像以（高度，宽度）格式。'
- en: Preprocess an image or batch of images.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像或一批图像。
- en: '#### `post_process_semantic_segmentation`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_semantic_segmentation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/image_processing_beit.py#L464)'
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`outputs` ([BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation))
    — Raw outputs of the model.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`（[BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)）
    — 模型的原始输出。'
- en: '`target_sizes` (`List[Tuple]` of length `batch_size`, *optional*) — List of
    tuples corresponding to the requested final size (height, width) of each prediction.
    If unset, predictions will not be resized.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_sizes` (`List[Tuple]`，长度为`batch_size`，*可选*) — 与每个预测的请求最终大小（高度，宽度）对应的元组列表。如果未设置，预测将不会被调整大小。'
- en: Returns
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: semantic_segmentation
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割
- en: '`List[torch.Tensor]` of length `batch_size`, where each item is a semantic
    segmentation map of shape (height, width) corresponding to the target_sizes entry
    (if `target_sizes` is specified). Each entry of each `torch.Tensor` correspond
    to a semantic class id.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[torch.Tensor]` 长度为 `batch_size`，其中每个项目是形状为 (height, width) 的语义分割地图，对应于目标大小条目（如果指定了
    `target_sizes`）。每个 `torch.Tensor` 的每个条目对应于一个语义类别 id。'
- en: Converts the output of [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    into semantic segmentation maps. Only supports PyTorch.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    的输出转换为语义分割地图。仅支持PyTorch。
- en: PytorchHide Pytorch content
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: BeitModel
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitModel
- en: '### `class transformers.BeitModel`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L620)'
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare Beit Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的Beit模型变压器输出原始隐藏状态，没有特定的头部。这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L651)'
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)
    获取。有关详细信息，请参阅 [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在 `[0, 1]` 中选择：'
- en: 1 indicates the head is `not masked`,
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示头部未被掩盖，
- en: 0 indicates the head is `masked`.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示头部被掩盖。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的
    `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的
    `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通的元组。'
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`,
    *optional*) — Boolean masked positions. Indicates which patches are masked (1)
    and which aren’t (0).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`,
    *optional*) — 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。'
- en: Returns
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.models.beit.modeling_beit.BeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling)
    或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Average of the last layer hidden states of the patch tokens (excluding the *[CLS]*
    token) if *config.use_mean_pooling* is set to True. If set to False, then the
    final hidden state of the *[CLS]* token will be returned.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 如果*config.use_mean_pooling*设置为True，则是补丁标记的最后一层隐藏状态的平均值（不包括*[CLS]*标记）。如果设置为False，则将返回*[CLS]*标记的最终隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)
    forward method, overrides the `__call__` special method.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[BeitModel](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: BeitForMaskedImageModeling
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BeitForMaskedImageModeling
- en: '### `class transformers.BeitForMaskedImageModeling`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BeitForMaskedImageModeling`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L732)'
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig)）-
    模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Beit Model transformer with a ‘language’ modeling head on top. BEiT does masked
    image modeling by predicting visual tokens of a Vector-Quantize Variational Autoencoder
    (VQ-VAE), whereas other vision models like ViT and DeiT predict RGB pixel values.
    As a result, this class is incompatible with [AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling),
    so you will need to use [BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)
    directly if you wish to do masked image modeling with BEiT. This model is a PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for
    all matter related to general usage and behavior.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Beit模型变压器顶部带有“语言”建模头。BEiT通过预测矢量量化变分自动编码器（VQ-VAE）的视觉标记来进行遮蔽图像建模，而其他视觉模型如ViT和DeiT则预测RGB像素值。因此，此类与[AutoModelForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForMaskedImageModeling)不兼容，因此如果要使用BEiT进行遮蔽图像建模，则需要直接使用[BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)。此模型是PyTorch
    [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L753)'
- en: '[PRE14]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)。'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）-
    用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被遮蔽，
- en: 0 indicates the head is `masked`.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被遮蔽。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForMaskedImageModeling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForMaskedImageModeling)
    forward method, overrides the `__call__` special method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: BeitForImageClassification
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitForImageClassification`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L832)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beit Model transformer with an image classification head on top (a linear layer
    on top of the average of the final hidden states of the patch tokens) e.g. for
    ImageNet.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L852)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForImageClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: BeitForSemanticSegmentation
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeitForSemanticSegmentation`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1156)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k,
    CityScapes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_beit.py#L1214)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [BeitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitFeatureExtractor.__call__)
    for details.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*)
    — Ground truth semantic segmentation maps for computing the loss. Indices should
    be in `[0, ..., config.num_labels - 1]`. If `config.num_labels > 1`, a classification
    loss is computed (Cross-Entropy).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SemanticSegmenterOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    and inputs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height,
    logits_width)`) — Classification scores for each pixel.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <tip warning="{true}">The logits returned do not necessarily have the same size
    as the `pixel_values` passed as inputs. This is to avoid doing two interpolations
    and lose some quality when a user needs to resize the logits to the original image
    size as post-processing. You should always check your logits shape and resize
    as needed.</tip>
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, patch_size, hidden_size)`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BeitForSemanticSegmentation](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitForSemanticSegmentation)
    forward method, overrides the `__call__` special method.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: JAXHide JAX content
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: FlaxBeitModel
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitModel`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L741)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The bare Beit Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Returns
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) — Average
    of the last layer hidden states of the patch tokens (excluding the *[CLS]* token)
    if *config.use_mean_pooling* is set to True. If set to False, then the final hidden
    state of the *[CLS]* token will be returned.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: FlaxBeitForMaskedImageModeling
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitForMaskedImageModeling`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L825)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Beit Model transformer with a ‘language’ modeling head on top (to predict visual
    tokens).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Returns
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'bool_masked_pos (`numpy.ndarray` of shape `(batch_size, num_patches)`): Boolean
    masked positions. Indicates which patches are masked (1) and which aren’t (0).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: FlaxBeitForImageClassification
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxBeitForImageClassification`'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L909)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([BeitConfig](/docs/transformers/v4.37.2/en/model_doc/beit#transformers.BeitConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Beit Model transformer with an image classification head on top (a linear layer
    on top of the average of the final hidden states of the patch tokens) e.g. for
    ImageNet.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/beit/modeling_flax_beit.py#L632)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Returns
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.beit.configuration_beit.BeitConfig'>`)
    and inputs.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxBeitPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
