- en: JAX/Flax
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JAX/Flax
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to](https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to](https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Diffusers supports Flax for super fast inference on Google TPUs, such as those
    available in Colab, Kaggle or Google Cloud Platform. This guide shows you how
    to run inference with Stable Diffusion using JAX/Flax.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Diffusersæ”¯æŒFlaxåœ¨Google TPUä¸Šè¿›è¡Œè¶…å¿«æ¨ç†ï¼Œä¾‹å¦‚åœ¨Colabã€Kaggleæˆ–Google Cloud Platformä¸Šå¯ç”¨çš„TPUã€‚æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨JAX/Flaxè¿è¡Œç¨³å®šæ‰©æ•£çš„æ¨ç†ã€‚
- en: 'Before you begin, make sure you have the necessary libraries installed:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should also make sure youâ€™re using a TPU backend. While JAX does not run
    exclusively on TPUs, youâ€™ll get the best performance on a TPU because each server
    has 8 TPU accelerators working in parallel.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜åº”ç¡®ä¿æ‚¨æ­£åœ¨ä½¿ç”¨TPUåç«¯ã€‚è™½ç„¶JAXå¹¶ä¸ä¸“é—¨åœ¨TPUä¸Šè¿è¡Œï¼Œä½†åœ¨TPUä¸Šä¼šè·å¾—æœ€ä½³æ€§èƒ½ï¼Œå› ä¸ºæ¯å°æœåŠ¡å™¨éƒ½æœ‰8ä¸ªTPUåŠ é€Ÿå™¨å¹¶è¡Œå·¥ä½œã€‚
- en: 'If you are running this guide in Colab, select *Runtime* in the menu above,
    select the option *Change runtime type*, and then select *TPU* under the *Hardware
    accelerator* setting. Import JAX and quickly check whether youâ€™re using a TPU:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨åœ¨Colabä¸­è¿è¡Œæ­¤æŒ‡å—ï¼Œè¯·åœ¨ä¸Šé¢çš„èœå•ä¸­é€‰æ‹©*è¿è¡Œæ—¶*ï¼Œé€‰æ‹©*æ›´æ”¹è¿è¡Œæ—¶ç±»å‹*é€‰é¡¹ï¼Œç„¶ååœ¨*ç¡¬ä»¶åŠ é€Ÿå™¨*è®¾ç½®ä¸‹é€‰æ‹©*TPU*ã€‚å¯¼å…¥JAXå¹¶å¿«é€Ÿæ£€æŸ¥æ˜¯å¦æ­£åœ¨ä½¿ç”¨TPUï¼š
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Great, now you can import the rest of the dependencies youâ€™ll need:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‚¨å¯ä»¥å¯¼å…¥æ‰€éœ€çš„å…¶ä½™ä¾èµ–é¡¹ï¼š
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load a model
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½æ¨¡å‹
- en: Flax is a functional framework, so models are stateless and parameters are stored
    outside of them. Loading a pretrained Flax pipeline returns *both* the pipeline
    and the model weights (or parameters). In this guide, youâ€™ll use `bfloat16`, a
    more efficient half-float type that is supported by TPUs (you can also use `float32`
    for full precision if you want).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Flaxæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§æ¡†æ¶ï¼Œå› æ­¤æ¨¡å‹æ˜¯æ— çŠ¶æ€çš„ï¼Œå‚æ•°å­˜å‚¨åœ¨æ¨¡å‹ä¹‹å¤–ã€‚åŠ è½½é¢„è®­ç»ƒçš„Flaxç®¡é“ä¼šè¿”å›*ç®¡é“å’Œæ¨¡å‹æƒé‡ï¼ˆæˆ–å‚æ•°ï¼‰*ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨`bfloat16`ï¼Œè¿™æ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„åŠæµ®ç‚¹ç±»å‹ï¼Œå—åˆ°TPUæ”¯æŒï¼ˆå¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨`float32`è¿›è¡Œå®Œæ•´ç²¾åº¦ï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Inference
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: TPUs usually have 8 devices working in parallel, so letâ€™s use the same prompt
    for each device. This means you can perform inference on 8 devices at once, with
    each device generating one image. As a result, youâ€™ll get 8 images in the same
    amount of time it takes for one chip to generate a single image!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸TPUæœ‰8ä¸ªè®¾å¤‡å¹¶è¡Œå·¥ä½œï¼Œå› æ­¤è®©æˆ‘ä»¬ä¸ºæ¯ä¸ªè®¾å¤‡ä½¿ç”¨ç›¸åŒçš„æç¤ºã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åŒæ—¶åœ¨8ä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œæ¨ç†ï¼Œæ¯ä¸ªè®¾å¤‡ç”Ÿæˆä¸€ä¸ªå›¾åƒã€‚å› æ­¤ï¼Œæ‚¨å°†åœ¨ç›¸åŒçš„æ—¶é—´å†…è·å¾—8å¼ å›¾åƒï¼Œå°±åƒä¸€å—èŠ¯ç‰‡ç”Ÿæˆä¸€å¼ å•ä¸ªå›¾åƒæ‰€éœ€çš„æ—¶é—´ä¸€æ ·ï¼
- en: Learn more details in the [How does parallelization work?](#how-does-parallelization-work)
    section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[å¹¶è¡ŒåŒ–å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ](#how-does-parallelization-work)éƒ¨åˆ†äº†è§£æ›´å¤šç»†èŠ‚ã€‚
- en: After replicating the prompt, get the tokenized text ids by calling the `prepare_inputs`
    function on the pipeline. The length of the tokenized text is set to 77 tokens
    as required by the configuration of the underlying CLIP text model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤åˆ¶æç¤ºåï¼Œé€šè¿‡åœ¨ç®¡é“ä¸Šè°ƒç”¨`prepare_inputs`å‡½æ•°æ¥è·å–æ ‡è®°åŒ–æ–‡æœ¬idã€‚æ ‡è®°åŒ–æ–‡æœ¬çš„é•¿åº¦è®¾ç½®ä¸º77ä¸ªæ ‡è®°ï¼Œè¿™æ˜¯åŸºç¡€CLIPæ–‡æœ¬æ¨¡å‹é…ç½®æ‰€è¦æ±‚çš„ã€‚
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Model parameters and inputs have to be replicated across the 8 parallel devices.
    The parameters dictionary is replicated with [`flax.jax_utils.replicate`](https://flax.readthedocs.io/en/latest/api_reference/flax.jax_utils.html#flax.jax_utils.replicate)
    which traverses the dictionary and changes the shape of the weights so they are
    repeated 8 times. Arrays are replicated using `shard`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‚æ•°å’Œè¾“å…¥å¿…é¡»åœ¨8ä¸ªå¹¶è¡Œè®¾å¤‡ä¸Šå¤åˆ¶ã€‚å‚æ•°å­—å…¸ä½¿ç”¨[`flax.jax_utils.replicate`](https://flax.readthedocs.io/en/latest/api_reference/flax.jax_utils.html#flax.jax_utils.replicate)è¿›è¡Œå¤åˆ¶ï¼Œè¯¥å‡½æ•°éå†å­—å…¸å¹¶æ›´æ”¹æƒé‡çš„å½¢çŠ¶ï¼Œä½¿å…¶é‡å¤8æ¬¡ã€‚æ•°ç»„ä½¿ç”¨`shard`è¿›è¡Œå¤åˆ¶ã€‚
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This shape means each one of the 8 devices receives as an input a `jnp` array
    with shape `(1, 77)`, where `1` is the batch size per device. On TPUs with sufficient
    memory, you could have a batch size larger than `1` if you want to generate multiple
    images (per chip) at once.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå½¢çŠ¶æ„å‘³ç€æ¯ä¸ª8ä¸ªè®¾å¤‡éƒ½æ¥æ”¶ä¸€ä¸ªå½¢çŠ¶ä¸º`(1, 77)`çš„`jnp`æ•°ç»„ä½œä¸ºè¾“å…¥ï¼Œå…¶ä¸­`1`æ˜¯æ¯ä¸ªè®¾å¤‡çš„æ‰¹é‡å¤§å°ã€‚åœ¨å…·æœ‰è¶³å¤Ÿå†…å­˜çš„TPUä¸Šï¼Œå¦‚æœè¦ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå›¾åƒï¼ˆæ¯ä¸ªèŠ¯ç‰‡ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨å¤§äº`1`çš„æ‰¹é‡å¤§å°ã€‚
- en: Next, create a random number generator to pass to the generation function. This
    is standard procedure in Flax, which is very serious and opinionated about random
    numbers. All functions that deal with random numbers are expected to receive a
    generator to ensure reproducibility, even when youâ€™re training across multiple
    distributed devices.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ªéšæœºæ•°ç”Ÿæˆå™¨ä¼ é€’ç»™ç”Ÿæˆå‡½æ•°ã€‚è¿™æ˜¯Flaxä¸­çš„æ ‡å‡†ç¨‹åºï¼ŒFlaxå¯¹éšæœºæ•°éå¸¸ä¸¥è‚ƒå’Œæœ‰è§åœ°ã€‚æ‰€æœ‰å¤„ç†éšæœºæ•°çš„å‡½æ•°éƒ½åº”è¯¥æ¥æ”¶ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä»¥ç¡®ä¿å¯é‡ç°æ€§ï¼Œå³ä½¿åœ¨è·¨å¤šä¸ªåˆ†å¸ƒå¼è®¾å¤‡è¿›è¡Œè®­ç»ƒæ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: The helper function below uses a seed to initialize a random number generator.
    As long as you use the same seed, youâ€™ll get the exact same results. Feel free
    to use different seeds when exploring results later in the guide.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„è¾…åŠ©å‡½æ•°ä½¿ç”¨ç§å­æ¥åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨ã€‚åªè¦ä½¿ç”¨ç›¸åŒçš„ç§å­ï¼Œæ‚¨å°†è·å¾—å®Œå…¨ç›¸åŒçš„ç»“æœã€‚åœ¨åç»­æŒ‡å—ä¸­æ¢ç´¢ç»“æœæ—¶ï¼Œå¯ä»¥éšæ„ä½¿ç”¨ä¸åŒçš„ç§å­ã€‚
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The helper function, or `rng`, is split 8 times so each device receives a different
    generator and generates a different image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¾…åŠ©å‡½æ•°ï¼Œæˆ–`rng`ï¼Œè¢«åˆ†æˆ8æ¬¡ï¼Œå› æ­¤æ¯ä¸ªè®¾å¤‡æ¥æ”¶ä¸åŒçš„ç”Ÿæˆå™¨å¹¶ç”Ÿæˆä¸åŒçš„å›¾åƒã€‚
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To take advantage of JAXâ€™s optimized speed on a TPU, pass `jit=True` to the
    pipeline to compile the JAX code into an efficient representation and to ensure
    the model runs in parallel across the 8 devices.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åˆ©ç”¨JAXåœ¨TPUä¸Šçš„ä¼˜åŒ–é€Ÿåº¦ï¼Œå°†`jit=True`ä¼ é€’ç»™ç®¡é“ï¼Œå°†JAXä»£ç ç¼–è¯‘æˆé«˜æ•ˆçš„è¡¨ç¤ºï¼Œå¹¶ç¡®ä¿æ¨¡å‹åœ¨8ä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œè¿è¡Œã€‚
- en: You need to ensure all your inputs have the same shape in subsequent calls,
    otherwise JAX will need to recompile the code which is slower.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨éœ€è¦ç¡®ä¿æ‰€æœ‰è¾“å…¥åœ¨åç»­è°ƒç”¨ä¸­å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œå¦åˆ™JAXå°†éœ€è¦é‡æ–°ç¼–è¯‘ä»£ç ï¼Œè¿™ä¼šæ›´æ…¢ã€‚
- en: The first inference run takes more time because it needs to compile the code,
    but subsequent calls (even with different inputs) are much faster. For example,
    it took more than a minute to compile on a TPU v2-8, but then it takes about **7s**
    on a future inference run!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡æ¨ç†è¿è¡Œéœ€è¦æ›´å¤šæ—¶é—´ï¼Œå› ä¸ºéœ€è¦ç¼–è¯‘ä»£ç ï¼Œä½†åç»­è°ƒç”¨ï¼ˆå³ä½¿ä½¿ç”¨ä¸åŒçš„è¾“å…¥ï¼‰ä¼šå¿«å¾—å¤šã€‚ä¾‹å¦‚ï¼Œåœ¨TPU v2-8ä¸Šç¼–è¯‘éœ€è¦è¶…è¿‡ä¸€åˆ†é’Ÿï¼Œä½†åœ¨æœªæ¥çš„æ¨ç†è¿è¡Œä¸­å¤§çº¦éœ€è¦**7ç§’**ï¼
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The returned array has shape `(8, 1, 512, 512, 3)` which should be reshaped
    to remove the second dimension and get 8 images of `512 Ã— 512 Ã— 3`. Then you can
    use the [numpy_to_pil()](/docs/diffusers/v0.26.3/en/api/utilities#diffusers.utils.numpy_to_pil)
    function to convert the arrays into images.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›çš„æ•°ç»„å½¢çŠ¶ä¸º`(8, 1, 512, 512, 3)`ï¼Œåº”è¯¥é‡æ–°æ•´å½¢ä»¥å»é™¤ç¬¬äºŒç»´ï¼Œå¹¶è·å¾—`512 Ã— 512 Ã— 3`çš„8ä¸ªå›¾åƒã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[numpy_to_pil()](/docs/diffusers/v0.26.3/en/api/utilities#diffusers.utils.numpy_to_pil)å‡½æ•°å°†æ•°ç»„è½¬æ¢ä¸ºå›¾åƒã€‚
- en: '[PRE9]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![img](../Images/ae26c49256edf5470277710762edc407.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/ae26c49256edf5470277710762edc407.png)'
- en: Using different prompts
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒçš„æç¤ºç¬¦
- en: 'You donâ€™t necessarily have to use the same prompt on all devices. For example,
    to generate 8 different prompts:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¸ä¸€å®šéœ€è¦åœ¨æ‰€æœ‰è®¾å¤‡ä¸Šä½¿ç”¨ç›¸åŒçš„æç¤ºã€‚ä¾‹å¦‚ï¼Œè¦ç”Ÿæˆ8ä¸ªä¸åŒçš„æç¤ºï¼š
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![img](../Images/aca4bd3a07a58b9a81031e0734d47ed3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/aca4bd3a07a58b9a81031e0734d47ed3.png)'
- en: How does parallelization work?
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: The Flax pipeline in ğŸ¤— Diffusers automatically compiles the model and runs it
    in parallel on all available devices. Letâ€™s take a closer look at how that process
    works.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Diffusersä¸­çš„Flaxç®¡é“ä¼šè‡ªåŠ¨ç¼–è¯‘æ¨¡å‹å¹¶åœ¨æ‰€æœ‰å¯ç”¨è®¾å¤‡ä¸Šå¹¶è¡Œè¿è¡Œã€‚è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹è¿™ä¸ªè¿‡ç¨‹æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: JAX parallelization can be done in multiple ways. The easiest one revolves around
    using the [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)
    function to achieve single-program multiple-data (SPMD) parallelization. It means
    running several copies of the same code, each on different data inputs. More sophisticated
    approaches are possible, and you can go over to the JAX [documentation](https://jax.readthedocs.io/en/latest/index.html)
    to explore this topic in more detail if you are interested!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: JAXå¹¶è¡ŒåŒ–å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å®ç°ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨[`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)å‡½æ•°æ¥å®ç°å•ç¨‹åºå¤šæ•°æ®ï¼ˆSPMDï¼‰å¹¶è¡ŒåŒ–ã€‚è¿™æ„å‘³ç€åœ¨ä¸åŒæ•°æ®è¾“å…¥ä¸Šè¿è¡Œç›¸åŒä»£ç çš„å¤šä¸ªå‰¯æœ¬ã€‚è¿˜æœ‰æ›´å¤æ‚çš„æ–¹æ³•ï¼Œå¦‚æœæ‚¨æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹JAXçš„[æ–‡æ¡£](https://jax.readthedocs.io/en/latest/index.html)ä»¥æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸ªä¸»é¢˜ï¼
- en: '`jax.pmap` does two things:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`jax.pmap`åšäº†ä¸¤ä»¶äº‹ï¼š'
- en: Compiles (or â€`jit`sâ€) the code which is similar to `jax.jit()`. This does not
    happen when you call `pmap`, and only the first time the `pmap`ped function is
    called.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¼–è¯‘ï¼ˆæˆ–â€œ`jit`sâ€ï¼‰ç±»ä¼¼äº`jax.jit()`çš„ä»£ç ã€‚å½“æ‚¨è°ƒç”¨`pmap`æ—¶ï¼Œä¸ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œåªæœ‰ç¬¬ä¸€æ¬¡è°ƒç”¨`pmap`å‡½æ•°æ—¶æ‰ä¼šå‘ç”Ÿã€‚
- en: Ensures the compiled code runs in parallel on all available devices.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¡®ä¿ç¼–è¯‘çš„ä»£ç åœ¨æ‰€æœ‰å¯ç”¨è®¾å¤‡ä¸Šå¹¶è¡Œè¿è¡Œã€‚
- en: 'To demonstrate, call `pmap` on the pipelineâ€™s `_generate` method (this is a
    private method that generates images and may be renamed or removed in future releases
    of ğŸ¤— Diffusers):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¼”ç¤ºï¼Œåœ¨ç®¡é“çš„`_generate`æ–¹æ³•ä¸Šè°ƒç”¨`pmap`ï¼ˆè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå›¾åƒçš„ç§æœ‰æ–¹æ³•ï¼Œåœ¨æœªæ¥çš„ğŸ¤— Diffusersç‰ˆæœ¬ä¸­å¯èƒ½ä¼šè¢«é‡å‘½åæˆ–åˆ é™¤ï¼‰ï¼š
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After calling `pmap`, the prepared function `p_generate` will:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨`pmap`åï¼Œå‡†å¤‡å¥½çš„å‡½æ•°`p_generate`å°†ï¼š
- en: Make a copy of the underlying function, `pipeline._generate`, on each device.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªè®¾å¤‡ä¸Šå¤åˆ¶åŸºç¡€å‡½æ•°`pipeline._generate`ã€‚
- en: Send each device a different portion of the input arguments (this is why itâ€™s
    necessary to call the *shard* function). In this case, `prompt_ids` has shape
    `(8, 1, 77, 768)` so the array is split into 8 and each copy of `_generate` receives
    an input with shape `(1, 77, 768)`.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘æ¯ä¸ªè®¾å¤‡å‘é€ä¸åŒéƒ¨åˆ†çš„è¾“å…¥å‚æ•°ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦è°ƒç”¨*shard*å‡½æ•°ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`prompt_ids`çš„å½¢çŠ¶ä¸º`(8, 1, 77, 768)`ï¼Œå› æ­¤æ•°ç»„è¢«åˆ†æˆ8éƒ¨åˆ†ï¼Œæ¯ä¸ª`_generate`çš„å‰¯æœ¬æ¥æ”¶å½¢çŠ¶ä¸º`(1,
    77, 768)`çš„è¾“å…¥ã€‚
- en: The most important thing to pay attention to here is the batch size (1 in this
    example), and the input dimensions that make sense for your code. You donâ€™t have
    to change anything else to make the code work in parallel.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œè¦æ³¨æ„çš„æœ€é‡è¦çš„äº‹æƒ…æ˜¯æ‰¹é‡å¤§å°ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º1ï¼‰å’Œé€‚åˆæ‚¨ä»£ç çš„è¾“å…¥ç»´åº¦ã€‚æ‚¨ä¸å¿…æ›´æ”¹å…¶ä»–ä»»ä½•å†…å®¹å³å¯ä½¿ä»£ç å¹¶è¡Œå·¥ä½œã€‚
- en: The first time you call the pipeline takes more time, but the calls afterward
    are much faster. The `block_until_ready` function is used to correctly measure
    inference time because JAX uses asynchronous dispatch and returns control to the
    Python loop as soon as it can. You donâ€™t need to use that in your code; blocking
    occurs automatically when you want to use the result of a computation that has
    not yet been materialized.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ¬¡è°ƒç”¨ç®¡é“éœ€è¦æ›´å¤šæ—¶é—´ï¼Œä½†ä¹‹åçš„è°ƒç”¨é€Ÿåº¦è¦å¿«å¾—å¤šã€‚ä½¿ç”¨`block_until_ready`å‡½æ•°æ¥æ­£ç¡®æµ‹é‡æ¨ç†æ—¶é—´ï¼Œå› ä¸ºJAXä½¿ç”¨å¼‚æ­¥è°ƒåº¦ï¼Œå¹¶å°½å¿«å°†æ§åˆ¶è¿”å›ç»™Pythonå¾ªç¯ã€‚æ‚¨ä¸éœ€è¦åœ¨ä»£ç ä¸­ä½¿ç”¨å®ƒï¼›å½“æ‚¨æƒ³è¦ä½¿ç”¨å°šæœªå®ç°çš„è®¡ç®—ç»“æœæ—¶ï¼Œé˜»å¡ä¼šè‡ªåŠ¨å‘ç”Ÿã€‚
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Check your image dimensions to see if theyâ€™re correct:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ‚¨çš„å›¾åƒç»´åº¦æ˜¯å¦æ­£ç¡®ï¼š
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
