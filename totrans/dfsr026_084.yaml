- en: JAX/Flax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to](https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/187.b54d5939.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/DocNotebookDropdown.5fa27ace.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: ü§ó Diffusers supports Flax for super fast inference on Google TPUs, such as those
    available in Colab, Kaggle or Google Cloud Platform. This guide shows you how
    to run inference with Stable Diffusion using JAX/Flax.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You should also make sure you‚Äôre using a TPU backend. While JAX does not run
    exclusively on TPUs, you‚Äôll get the best performance on a TPU because each server
    has 8 TPU accelerators working in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running this guide in Colab, select *Runtime* in the menu above,
    select the option *Change runtime type*, and then select *TPU* under the *Hardware
    accelerator* setting. Import JAX and quickly check whether you‚Äôre using a TPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now you can import the rest of the dependencies you‚Äôll need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Load a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flax is a functional framework, so models are stateless and parameters are stored
    outside of them. Loading a pretrained Flax pipeline returns *both* the pipeline
    and the model weights (or parameters). In this guide, you‚Äôll use `bfloat16`, a
    more efficient half-float type that is supported by TPUs (you can also use `float32`
    for full precision if you want).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TPUs usually have 8 devices working in parallel, so let‚Äôs use the same prompt
    for each device. This means you can perform inference on 8 devices at once, with
    each device generating one image. As a result, you‚Äôll get 8 images in the same
    amount of time it takes for one chip to generate a single image!
  prefs: []
  type: TYPE_NORMAL
- en: Learn more details in the [How does parallelization work?](#how-does-parallelization-work)
    section.
  prefs: []
  type: TYPE_NORMAL
- en: After replicating the prompt, get the tokenized text ids by calling the `prepare_inputs`
    function on the pipeline. The length of the tokenized text is set to 77 tokens
    as required by the configuration of the underlying CLIP text model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Model parameters and inputs have to be replicated across the 8 parallel devices.
    The parameters dictionary is replicated with [`flax.jax_utils.replicate`](https://flax.readthedocs.io/en/latest/api_reference/flax.jax_utils.html#flax.jax_utils.replicate)
    which traverses the dictionary and changes the shape of the weights so they are
    repeated 8 times. Arrays are replicated using `shard`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This shape means each one of the 8 devices receives as an input a `jnp` array
    with shape `(1, 77)`, where `1` is the batch size per device. On TPUs with sufficient
    memory, you could have a batch size larger than `1` if you want to generate multiple
    images (per chip) at once.
  prefs: []
  type: TYPE_NORMAL
- en: Next, create a random number generator to pass to the generation function. This
    is standard procedure in Flax, which is very serious and opinionated about random
    numbers. All functions that deal with random numbers are expected to receive a
    generator to ensure reproducibility, even when you‚Äôre training across multiple
    distributed devices.
  prefs: []
  type: TYPE_NORMAL
- en: The helper function below uses a seed to initialize a random number generator.
    As long as you use the same seed, you‚Äôll get the exact same results. Feel free
    to use different seeds when exploring results later in the guide.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The helper function, or `rng`, is split 8 times so each device receives a different
    generator and generates a different image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To take advantage of JAX‚Äôs optimized speed on a TPU, pass `jit=True` to the
    pipeline to compile the JAX code into an efficient representation and to ensure
    the model runs in parallel across the 8 devices.
  prefs: []
  type: TYPE_NORMAL
- en: You need to ensure all your inputs have the same shape in subsequent calls,
    otherwise JAX will need to recompile the code which is slower.
  prefs: []
  type: TYPE_NORMAL
- en: The first inference run takes more time because it needs to compile the code,
    but subsequent calls (even with different inputs) are much faster. For example,
    it took more than a minute to compile on a TPU v2-8, but then it takes about **7s**
    on a future inference run!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The returned array has shape `(8, 1, 512, 512, 3)` which should be reshaped
    to remove the second dimension and get 8 images of `512 √ó 512 √ó 3`. Then you can
    use the [numpy_to_pil()](/docs/diffusers/v0.26.3/en/api/utilities#diffusers.utils.numpy_to_pil)
    function to convert the arrays into images.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![img](../Images/ae26c49256edf5470277710762edc407.png)'
  prefs: []
  type: TYPE_IMG
- en: Using different prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You don‚Äôt necessarily have to use the same prompt on all devices. For example,
    to generate 8 different prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![img](../Images/aca4bd3a07a58b9a81031e0734d47ed3.png)'
  prefs: []
  type: TYPE_IMG
- en: How does parallelization work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Flax pipeline in ü§ó Diffusers automatically compiles the model and runs it
    in parallel on all available devices. Let‚Äôs take a closer look at how that process
    works.
  prefs: []
  type: TYPE_NORMAL
- en: JAX parallelization can be done in multiple ways. The easiest one revolves around
    using the [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)
    function to achieve single-program multiple-data (SPMD) parallelization. It means
    running several copies of the same code, each on different data inputs. More sophisticated
    approaches are possible, and you can go over to the JAX [documentation](https://jax.readthedocs.io/en/latest/index.html)
    to explore this topic in more detail if you are interested!
  prefs: []
  type: TYPE_NORMAL
- en: '`jax.pmap` does two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Compiles (or ‚Äù`jit`s‚Äù) the code which is similar to `jax.jit()`. This does not
    happen when you call `pmap`, and only the first time the `pmap`ped function is
    called.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensures the compiled code runs in parallel on all available devices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To demonstrate, call `pmap` on the pipeline‚Äôs `_generate` method (this is a
    private method that generates images and may be renamed or removed in future releases
    of ü§ó Diffusers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After calling `pmap`, the prepared function `p_generate` will:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a copy of the underlying function, `pipeline._generate`, on each device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send each device a different portion of the input arguments (this is why it‚Äôs
    necessary to call the *shard* function). In this case, `prompt_ids` has shape
    `(8, 1, 77, 768)` so the array is split into 8 and each copy of `_generate` receives
    an input with shape `(1, 77, 768)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most important thing to pay attention to here is the batch size (1 in this
    example), and the input dimensions that make sense for your code. You don‚Äôt have
    to change anything else to make the code work in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The first time you call the pipeline takes more time, but the calls afterward
    are much faster. The `block_until_ready` function is used to correctly measure
    inference time because JAX uses asynchronous dispatch and returns control to the
    Python loop as soon as it can. You don‚Äôt need to use that in your code; blocking
    occurs automatically when you want to use the result of a computation that has
    not yet been materialized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Check your image dimensions to see if they‚Äôre correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
