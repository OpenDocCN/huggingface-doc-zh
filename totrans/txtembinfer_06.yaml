- en: Using TEI locally with CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-embeddings-inference/local_cpu](https://huggingface.co/docs/text-embeddings-inference/local_cpu)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-embeddings-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/start.f5781b4e.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/scheduler.b108d059.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/singletons.26f524d0.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/paths.e8cea87f.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/app.ca5804ae.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/index.008de539.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/0.a44871a2.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/5.fb8f7121.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/Tip.aeb15ab7.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/CodeBlock.3968c746.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/Heading.88bfeb84.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `text-embeddings-inference` locally to run it on your own machine.
    Here are the step-by-step instructions for installation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Install Rust'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Install Rust](([https://rustup.rs/](https://rustup.rs/)) on your machine by
    run the following in your terminal, then following the instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Install necessary packages'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depending on your machine’s architecture, run one of the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: For x86 Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For M1 or M2 Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Launch Text Embeddings Inference'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the installation is successfully complete, you can launch Text Embeddings
    Inference on CPU with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, you might also need the OpenSSL libraries and gcc installed.
    On Linux machines, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now you are ready to use `text-embeddings-inference` locally on your machine.
    If you want to run TEI locally with a GPU, check out the [Using TEI locally with
    GPU](local_gpu) page.
  prefs: []
  type: TYPE_NORMAL
