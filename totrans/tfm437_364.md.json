["```py\n( logits: FloatTensor = None last_hidden_state: FloatTensor = None hidden_states: Optional = None attentions: Optional = None cross_attentions: Optional = None )\n```", "```py\n( logits: FloatTensor = None cross_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None cross_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None cross_attentions: Optional = None )\n```", "```py\n( num_latents = 256 d_latents = 1280 d_model = 768 num_blocks = 1 num_self_attends_per_block = 26 num_self_attention_heads = 8 num_cross_attention_heads = 8 qk_channels = None v_channels = None cross_attention_shape_for_attention = 'kv' self_attention_widening_factor = 1 cross_attention_widening_factor = 1 hidden_act = 'gelu' attention_probs_dropout_prob = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-12 use_query_residual = True vocab_size = 262 max_position_embeddings = 2048 image_size = 56 train_size = [368, 496] num_frames = 16 audio_samples_per_frame = 1920 samples_per_patch = 16 output_shape = [1, 16, 224, 224] output_num_channels = 512 _label_trainable_num_channels = 1024 **kwargs )\n```", "```py\n>>> from transformers import PerceiverModel, PerceiverConfig\n\n>>> # Initializing a Perceiver deepmind/language-perceiver style configuration\n>>> configuration = PerceiverConfig()\n\n>>> # Initializing a model from the deepmind/language-perceiver style configuration\n>>> model = PerceiverModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( pad_token = '[PAD]' bos_token = '[BOS]' eos_token = '[EOS]' mask_token = '[MASK]' cls_token = '[CLS]' sep_token = '[SEP]' model_max_length = 2048 **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( do_center_crop: bool = True crop_size: Dict = None do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( images: Union do_center_crop: Optional = None crop_size: Optional = None do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( config: PerceiverConfig )\n```", "```py\n( config prep_type = 'conv' spatial_downsample: int = 4 temporal_downsample: int = 1 position_encoding_type: str = 'fourier' in_channels: int = 3 out_channels: int = 64 conv_after_patching: bool = False conv_after_patching_in_channels: int = 54 conv2d_use_batchnorm: bool = True concat_or_add_pos: str = 'concat' project_pos_dim: int = -1 **position_encoding_kwargs )\n```", "```py\n( config: PerceiverConfig )\n```", "```py\n( config prep_type: str = 'patches' samples_per_patch: int = 96 position_encoding_type: str = 'fourier' concat_or_add_pos: str = 'concat' out_channels = 64 project_pos_dim = -1 **position_encoding_kwargs )\n```", "```py\n( modalities: Mapping mask_probs: Optional = None min_padding_size: int = 2 )\n```", "```py\n( config )\n```", "```py\n( config: PerceiverConfig output_num_channels: int position_encoding_type: Optional = 'trainable' output_index_dims: Optional = None num_channels: Optional = 128 subsampled_index_dims: Optional = None qk_channels: Optional = None v_channels: Optional = None num_heads: Optional = 1 widening_factor: Optional = 1 use_query_residual: Optional = False concat_preprocessed_input: Optional = False final_project: Optional = True position_encoding_only: Optional = False **position_encoding_kwargs )\n```", "```py\n( config **decoder_kwargs )\n```", "```py\n( config output_image_shape output_num_channels = 2 rescale_factor = 100.0 **decoder_kwargs )\n```", "```py\n( config: PerceiverConfig output_shape: List position_encoding_type: str **decoder_kwargs )\n```", "```py\n( config: PerceiverConfig modalities: Dict num_outputs: int output_num_channels: int min_padding_size: Optional = 2 subsampled_index_dims: Optional = None **decoder_kwargs )\n```", "```py\n( in_channels: int out_channels: int )\n```", "```py\n( config: PerceiverConfig in_channels: int postproc_type: str = 'patches' )\n```", "```py\n( config: PerceiverConfig in_channels: int )\n```", "```py\n( modalities: Mapping input_is_dict: bool = False )\n```", "```py\n( config decoder = None input_preprocessor: Callable = None output_postprocessor: Callable = None )\n```", "```py\n( inputs: FloatTensor attention_mask: Optional = None subsampled_output_points: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import PerceiverConfig, PerceiverTokenizer, PerceiverImageProcessor, PerceiverModel\n>>> from transformers.models.perceiver.modeling_perceiver import (\n...     PerceiverTextPreprocessor,\n...     PerceiverImagePreprocessor,\n...     PerceiverClassificationDecoder,\n... )\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n\n>>> # EXAMPLE 1: using the Perceiver to classify texts\n>>> # - we define a TextPreprocessor, which can be used to embed tokens\n>>> # - we define a ClassificationDecoder, which can be used to decode the\n>>> # final hidden states of the latents to classification logits\n>>> # using trainable position embeddings\n>>> config = PerceiverConfig()\n>>> preprocessor = PerceiverTextPreprocessor(config)\n>>> decoder = PerceiverClassificationDecoder(\n...     config,\n...     num_channels=config.d_latents,\n...     trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n...     use_query_residual=True,\n... )\n>>> model = PerceiverModel(config, input_preprocessor=preprocessor, decoder=decoder)\n\n>>> # you can then do a forward pass as follows:\n>>> tokenizer = PerceiverTokenizer()\n>>> text = \"hello world\"\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n\n>>> with torch.no_grad():\n...     outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n\n>>> # to train, one can train the model using standard cross-entropy:\n>>> criterion = torch.nn.CrossEntropyLoss()\n\n>>> labels = torch.tensor([1])\n>>> loss = criterion(logits, labels)\n\n>>> # EXAMPLE 2: using the Perceiver to classify images\n>>> # - we define an ImagePreprocessor, which can be used to embed images\n>>> config = PerceiverConfig(image_size=224)\n>>> preprocessor = PerceiverImagePreprocessor(\n...     config,\n...     prep_type=\"conv1x1\",\n...     spatial_downsample=1,\n...     out_channels=256,\n...     position_encoding_type=\"trainable\",\n...     concat_or_add_pos=\"concat\",\n...     project_pos_dim=256,\n...     trainable_position_encoding_kwargs=dict(\n...         num_channels=256,\n...         index_dims=config.image_size**2,\n...     ),\n... )\n\n>>> model = PerceiverModel(\n...     config,\n...     input_preprocessor=preprocessor,\n...     decoder=PerceiverClassificationDecoder(\n...         config,\n...         num_channels=config.d_latents,\n...         trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n...         use_query_residual=True,\n...     ),\n... )\n\n>>> # you can then do a forward pass as follows:\n>>> image_processor = PerceiverImageProcessor()\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> with torch.no_grad():\n...     outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n\n>>> # to train, one can train the model using standard cross-entropy:\n>>> criterion = torch.nn.CrossEntropyLoss()\n\n>>> labels = torch.tensor([1])\n>>> loss = criterion(logits, labels)\n```", "```py\n( config: PerceiverConfig )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None input_ids: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PerceiverForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n>>> model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\n\n>>> # training\n>>> text = \"This is an incomplete sentence where some words are missing.\"\n>>> inputs = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n>>> # mask \" missing.\"\n>>> inputs[\"input_ids\"][0, 52:61] = tokenizer.mask_token_id\n>>> labels = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n>>> outputs = model(**inputs, labels=labels)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n19.87\n\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2048, 262]\n\n>>> # inference\n>>> text = \"This is an incomplete sentence where some words are missing.\"\n>>> encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> # mask bytes corresponding to \" missing.\". Note that the model performs much better if the masked span starts with a space.\n>>> encoding[\"input_ids\"][0, 52:61] = tokenizer.mask_token_id\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**encoding)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2048, 262]\n\n>>> masked_tokens_predictions = logits[0, 52:61].argmax(dim=-1).tolist()\n>>> tokenizer.decode(masked_tokens_predictions)\n' missing.'\n```", "```py\n( config )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None input_ids: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PerceiverForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n>>> model = PerceiverForSequenceClassification.from_pretrained(\"deepmind/language-perceiver\")\n\n>>> text = \"hello world\"\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n```", "```py\n( config )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None pixel_values: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationLearned\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-learned\")\n>>> model = PerceiverForImageClassificationLearned.from_pretrained(\"deepmind/vision-perceiver-learned\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n( config )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None pixel_values: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationFourier\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-fourier\")\n>>> model = PerceiverForImageClassificationFourier.from_pretrained(\"deepmind/vision-perceiver-fourier\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n( config )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None pixel_values: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationConvProcessing\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-conv\")\n>>> model = PerceiverForImageClassificationConvProcessing.from_pretrained(\"deepmind/vision-perceiver-conv\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n( config )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import PerceiverForOpticalFlow\n>>> import torch\n\n>>> model = PerceiverForOpticalFlow.from_pretrained(\"deepmind/optical-flow-perceiver\")\n\n>>> # in the Perceiver IO paper, the authors extract a 3 x 3 patch around each pixel,\n>>> # leading to 3 x 3 x 3 = 27 values for each pixel (as each pixel also has 3 color channels)\n>>> # patches have shape (batch_size, num_frames, num_channels, height, width)\n>>> # the authors train on resolutions of 368 x 496\n>>> patches = torch.randn(1, 2, 27, 368, 496)\n>>> outputs = model(inputs=patches)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 368, 496, 2]\n```", "```py\n( config: PerceiverConfig )\n```", "```py\n( inputs: Optional = None attention_mask: Optional = None subsampled_output_points: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import PerceiverForMultimodalAutoencoding\n>>> import torch\n>>> import numpy as np\n\n>>> # create multimodal inputs\n>>> images = torch.randn((1, 16, 3, 224, 224))\n>>> audio = torch.randn((1, 30720, 1))\n>>> inputs = dict(image=images, audio=audio, label=torch.zeros((images.shape[0], 700)))\n\n>>> model = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\")\n\n>>> # in the Perceiver IO paper, videos are auto-encoded in chunks\n>>> # each chunk subsamples different index dimensions of the image and audio modality decoder queries\n>>> nchunks = 128\n>>> image_chunk_size = np.prod((16, 224, 224)) // nchunks\n>>> audio_chunk_size = audio.shape[1] // model.config.samples_per_patch // nchunks\n>>> # process the first chunk\n>>> chunk_idx = 0\n>>> subsampling = {\n...     \"image\": torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n...     \"audio\": torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n...     \"label\": None,\n... }\n\n>>> outputs = model(inputs=inputs, subsampled_output_points=subsampling)\n>>> logits = outputs.logits\n>>> list(logits[\"audio\"].shape)\n[1, 240]\n\n>>> list(logits[\"image\"].shape)\n[1, 6272, 3]\n\n>>> list(logits[\"label\"].shape)\n[1, 700]\n```"]