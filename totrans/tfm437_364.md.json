["```py\n>>> from transformers import PerceiverModel, PerceiverConfig\n\n>>> # Initializing a Perceiver deepmind/language-perceiver style configuration\n>>> configuration = PerceiverConfig()\n\n>>> # Initializing a model from the deepmind/language-perceiver style configuration\n>>> model = PerceiverModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import PerceiverConfig, PerceiverTokenizer, PerceiverImageProcessor, PerceiverModel\n>>> from transformers.models.perceiver.modeling_perceiver import (\n...     PerceiverTextPreprocessor,\n...     PerceiverImagePreprocessor,\n...     PerceiverClassificationDecoder,\n... )\n>>> import torch\n>>> import requests\n>>> from PIL import Image\n\n>>> # EXAMPLE 1: using the Perceiver to classify texts\n>>> # - we define a TextPreprocessor, which can be used to embed tokens\n>>> # - we define a ClassificationDecoder, which can be used to decode the\n>>> # final hidden states of the latents to classification logits\n>>> # using trainable position embeddings\n>>> config = PerceiverConfig()\n>>> preprocessor = PerceiverTextPreprocessor(config)\n>>> decoder = PerceiverClassificationDecoder(\n...     config,\n...     num_channels=config.d_latents,\n...     trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n...     use_query_residual=True,\n... )\n>>> model = PerceiverModel(config, input_preprocessor=preprocessor, decoder=decoder)\n\n>>> # you can then do a forward pass as follows:\n>>> tokenizer = PerceiverTokenizer()\n>>> text = \"hello world\"\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n\n>>> with torch.no_grad():\n...     outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n\n>>> # to train, one can train the model using standard cross-entropy:\n>>> criterion = torch.nn.CrossEntropyLoss()\n\n>>> labels = torch.tensor([1])\n>>> loss = criterion(logits, labels)\n\n>>> # EXAMPLE 2: using the Perceiver to classify images\n>>> # - we define an ImagePreprocessor, which can be used to embed images\n>>> config = PerceiverConfig(image_size=224)\n>>> preprocessor = PerceiverImagePreprocessor(\n...     config,\n...     prep_type=\"conv1x1\",\n...     spatial_downsample=1,\n...     out_channels=256,\n...     position_encoding_type=\"trainable\",\n...     concat_or_add_pos=\"concat\",\n...     project_pos_dim=256,\n...     trainable_position_encoding_kwargs=dict(\n...         num_channels=256,\n...         index_dims=config.image_size**2,\n...     ),\n... )\n\n>>> model = PerceiverModel(\n...     config,\n...     input_preprocessor=preprocessor,\n...     decoder=PerceiverClassificationDecoder(\n...         config,\n...         num_channels=config.d_latents,\n...         trainable_position_encoding_kwargs=dict(num_channels=config.d_latents, index_dims=1),\n...         use_query_residual=True,\n...     ),\n... )\n\n>>> # you can then do a forward pass as follows:\n>>> image_processor = PerceiverImageProcessor()\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = image_processor(image, return_tensors=\"pt\").pixel_values\n\n>>> with torch.no_grad():\n...     outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n\n>>> # to train, one can train the model using standard cross-entropy:\n>>> criterion = torch.nn.CrossEntropyLoss()\n\n>>> labels = torch.tensor([1])\n>>> loss = criterion(logits, labels)\n```", "```py\n>>> from transformers import AutoTokenizer, PerceiverForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n>>> model = PerceiverForMaskedLM.from_pretrained(\"deepmind/language-perceiver\")\n\n>>> # training\n>>> text = \"This is an incomplete sentence where some words are missing.\"\n>>> inputs = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n>>> # mask \" missing.\"\n>>> inputs[\"input_ids\"][0, 52:61] = tokenizer.mask_token_id\n>>> labels = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n>>> outputs = model(**inputs, labels=labels)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n19.87\n\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2048, 262]\n\n>>> # inference\n>>> text = \"This is an incomplete sentence where some words are missing.\"\n>>> encoding = tokenizer(text, padding=\"max_length\", return_tensors=\"pt\")\n\n>>> # mask bytes corresponding to \" missing.\". Note that the model performs much better if the masked span starts with a space.\n>>> encoding[\"input_ids\"][0, 52:61] = tokenizer.mask_token_id\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**encoding)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2048, 262]\n\n>>> masked_tokens_predictions = logits[0, 52:61].argmax(dim=-1).tolist()\n>>> tokenizer.decode(masked_tokens_predictions)\n' missing.'\n```", "```py\n>>> from transformers import AutoTokenizer, PerceiverForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n>>> model = PerceiverForSequenceClassification.from_pretrained(\"deepmind/language-perceiver\")\n\n>>> text = \"hello world\"\n>>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 2]\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationLearned\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-learned\")\n>>> model = PerceiverForImageClassificationLearned.from_pretrained(\"deepmind/vision-perceiver-learned\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationFourier\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-fourier\")\n>>> model = PerceiverForImageClassificationFourier.from_pretrained(\"deepmind/vision-perceiver-fourier\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n>>> from transformers import AutoImageProcessor, PerceiverForImageClassificationConvProcessing\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"deepmind/vision-perceiver-conv\")\n>>> model = PerceiverForImageClassificationConvProcessing.from_pretrained(\"deepmind/vision-perceiver-conv\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> outputs = model(inputs=inputs)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 1000]\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nPredicted class: tabby, tabby cat\n```", "```py\n>>> from transformers import PerceiverForOpticalFlow\n>>> import torch\n\n>>> model = PerceiverForOpticalFlow.from_pretrained(\"deepmind/optical-flow-perceiver\")\n\n>>> # in the Perceiver IO paper, the authors extract a 3 x 3 patch around each pixel,\n>>> # leading to 3 x 3 x 3 = 27 values for each pixel (as each pixel also has 3 color channels)\n>>> # patches have shape (batch_size, num_frames, num_channels, height, width)\n>>> # the authors train on resolutions of 368 x 496\n>>> patches = torch.randn(1, 2, 27, 368, 496)\n>>> outputs = model(inputs=patches)\n>>> logits = outputs.logits\n>>> list(logits.shape)\n[1, 368, 496, 2]\n```", "```py\n>>> from transformers import PerceiverForMultimodalAutoencoding\n>>> import torch\n>>> import numpy as np\n\n>>> # create multimodal inputs\n>>> images = torch.randn((1, 16, 3, 224, 224))\n>>> audio = torch.randn((1, 30720, 1))\n>>> inputs = dict(image=images, audio=audio, label=torch.zeros((images.shape[0], 700)))\n\n>>> model = PerceiverForMultimodalAutoencoding.from_pretrained(\"deepmind/multimodal-perceiver\")\n\n>>> # in the Perceiver IO paper, videos are auto-encoded in chunks\n>>> # each chunk subsamples different index dimensions of the image and audio modality decoder queries\n>>> nchunks = 128\n>>> image_chunk_size = np.prod((16, 224, 224)) // nchunks\n>>> audio_chunk_size = audio.shape[1] // model.config.samples_per_patch // nchunks\n>>> # process the first chunk\n>>> chunk_idx = 0\n>>> subsampling = {\n...     \"image\": torch.arange(image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n...     \"audio\": torch.arange(audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n...     \"label\": None,\n... }\n\n>>> outputs = model(inputs=inputs, subsampled_output_points=subsampling)\n>>> logits = outputs.logits\n>>> list(logits[\"audio\"].shape)\n[1, 240]\n\n>>> list(logits[\"image\"].shape)\n[1, 6272, 3]\n\n>>> list(logits[\"label\"].shape)\n[1, 700]\n```"]