- en: Loading methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/package_reference/loading_methods](https://huggingface.co/docs/evaluate/package_reference/loading_methods)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'Methods for listing and loading evaluation modules:'
  prefs: []
  type: TYPE_NORMAL
- en: List
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `evaluate.list_evaluation_modules`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/inspect.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`module_type` (`str`, *optional*, default `None`) — Type of evaluation modules
    to list. Has to be one of `''metric''`, `''comparison''`, or `''measurement''`.
    If `None`, all types are listed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_community` (`bool`, *optional*, default `True`) — Include community
    modules in the list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`with_details` (`bool`, *optional*, default `False`) — Return the full details
    on the metrics instead of only the ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List all evaluation modules available on the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: Load
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `evaluate.load`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/loading.py#L689)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path` (`str`) — path to the evaluation processing script with the evaluation
    builder. Can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a local path to processing script or the directory containing the script (if
    the script has the same name as the directory), e.g. `'./metrics/rouge'` or `'./metrics/rouge/rouge.py'`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a evaluation module identifier on the HuggingFace evaluate repo e.g. `'rouge'`
    or `'bleu'` that are in either `'metrics/'`, `'comparisons/'`, or `'measurements/'`
    depending on the provided `module_type`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config_name` (`str`, optional) — selecting a configuration for the metric
    (e.g. the GLUE metric has a configuration for each subset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module_type` (`str`, default `''metric''`) — type of evaluation module, can
    be one of `''metric''`, `''comparison''`, or `''measurement''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`, optional) — for distributed evaluation: id of the process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`, optional) — for distributed evaluation: total number
    of processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (Optional str) — path to store the temporary predictions and references
    (default to *~/.cache/huggingface/evaluate/*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    metrics in distributed setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (bool) — Whether to store the temporary results in memory
    (defaults to False)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download_config` (Optional `evaluate.DownloadConfig` — specific download configuration
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`download_mode` (`DownloadMode`, default `REUSE_DATASET_IF_EXISTS`) — Download/generate
    mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revision` (Optional `Union[str, evaluate.Version]`) — if specified, the module
    will be loaded from the datasets repository at this version. By default it is
    set to the local version of the lib. Specifying a version that is different from
    your local version of the lib might cause compatibility issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load a *evaluate.EvaluationModule*.
  prefs: []
  type: TYPE_NORMAL
