# 使用 AnimateDiff 进行文本到视频生成

> 原文：[https://huggingface.co/docs/diffusers/api/pipelines/animatediff](https://huggingface.co/docs/diffusers/api/pipelines/animatediff)

## 概述

[AnimateDiff：无需特定调整即可动画化您的个性化文本到图像扩散模型](https://arxiv.org/abs/2307.04725) 作者：郭宇伟，杨策远，饶安一，王耀辉，乔宇，林大华，戴波。

该论文的摘要如下：

*随着文本到图像模型（例如 Stable Diffusion）和相应的个性化技术（如 DreamBooth 和 LoRA）的进步，每个人都可以以较低成本将他们的想象力体现为高质量图像。随后，对于进一步将生成的静态图像与运动动态相结合的图像动画技术有着巨大需求。在本报告中，我们提出了一个实用的框架，一劳永逸地为大多数现有的个性化文本到图像模型添加动画，节省了模型特定调整的工作。所提出的框架的核心是将一个新初始化的运动建模模块插入冻结的文本到图像模型中，并在视频剪辑上训练它以提炼合理的运动先验。一旦训练完成，通过简单地注入这个运动建模模块，所有从相同基础 T2I 派生的个性化版本都会立即成为产生多样化和个性化动画图像的文本驱动模型。我们在几个公共代表性的个性化文本到图像模型上进行评估，跨动漫图片和现实照片，并展示我们提出的框架帮助这些模型生成在保留输出的领域和多样性的同时产生时间上平滑的动画片段。代码和预训练权重将在 [此链接](https://animatediff.github.io/) 上公开提供。*

## 可用的流水线

| 流水线 | 任务 | 演示 |
| --- | --- | :-: |
| [AnimateDiffPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff.py) | *使用 AnimateDiff 进行文本到视频生成* |  |
| [AnimateDiffVideoToVideoPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py) | *使用 AnimateDiff 进行视频到视频生成* |  |

## 可用的检查点

Motion Adapter 检查点可以在 [guoyww](https://huggingface.co/guoyww/) 下找到。这些检查点旨在与基于 Stable Diffusion 1.4/1.5 的任何模型一起使用。

## 用法示例

### AnimateDiffPipeline

AnimateDiff 与 MotionAdapter 检查点和 Stable Diffusion 模型检查点配合使用。MotionAdapter 是一组负责在图像帧之间添加连贯运动的运动模块集合。这些模块在 Stable Diffusion UNet 中的 Resnet 和 Attention 块之后应用。

以下示例演示了如何使用 *MotionAdapter* 检查点与 Diffusers 进行基于 StableDiffusion-1.4/1.5 的推理。

```py
import torch
from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter
from diffusers.utils import export_to_gif

# Load the motion adapter
adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
# load SD 1.5 based finetuned model
model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)
scheduler = DDIMScheduler.from_pretrained(
    model_id,
    subfolder="scheduler",
    clip_sample=False,
    timestep_spacing="linspace",
    beta_schedule="linear",
    steps_offset=1,
)
pipe.scheduler = scheduler

# enable memory savings
pipe.enable_vae_slicing()
pipe.enable_model_cpu_offload()

output = pipe(
    prompt=(
        "masterpiece, bestquality, highlydetailed, ultradetailed, sunset, "
        "orange sky, warm lighting, fishing boats, ocean waves seagulls, "
        "rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, "
        "golden hour, coastal landscape, seaside scenery"
    ),
    negative_prompt="bad quality, worse quality",
    num_frames=16,
    guidance_scale=7.5,
    num_inference_steps=25,
    generator=torch.Generator("cpu").manual_seed(42),
)
frames = output.frames[0]
export_to_gif(frames, "animation.gif")

```

以下是一些示例输出：

| 杰作，最佳质量，日落。 ![杰作，最佳质量，日落](../Images/54b95c0d916bbffbc1aba35e0b746b31.png) |
| --- |

AnimateDiff 更适合与微调的 Stable Diffusion 模型一起使用。如果您计划使用可以剪辑样本的调度程序，请确保通过在调度程序中设置 `clip_sample=False` 来禁用它，因为这也可能对生成的样本产生不利影响。此外，AnimateDiff 的检查点可能对调度程序的 beta 计划敏感。我们建议将其设置为 `linear`。

### AnimateDiffVideoToVideoPipeline

AnimateDiff 还可用于生成视觉上相似的视频，或启用从初始视频开始的风格/角色/背景或其他编辑，使您能够无缝地探索创意可能性。

```py
import imageio
import requests
import torch
from diffusers import AnimateDiffVideoToVideoPipeline, DDIMScheduler, MotionAdapter
from diffusers.utils import export_to_gif
from io import BytesIO
from PIL import Image

# Load the motion adapter
adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
# load SD 1.5 based finetuned model
model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
pipe = AnimateDiffVideoToVideoPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16).to("cuda")
scheduler = DDIMScheduler.from_pretrained(
    model_id,
    subfolder="scheduler",
    clip_sample=False,
    timestep_spacing="linspace",
    beta_schedule="linear",
    steps_offset=1,
)
pipe.scheduler = scheduler

# enable memory savings
pipe.enable_vae_slicing()
pipe.enable_model_cpu_offload()

# helper function to load videos
def load_video(file_path: str):
    images = []

    if file_path.startswith(('http://', 'https://')):
        # If the file_path is a URL
        response = requests.get(file_path)
        response.raise_for_status()
        content = BytesIO(response.content)
        vid = imageio.get_reader(content)
    else:
        # Assuming it's a local file path
        vid = imageio.get_reader(file_path)

    for frame in vid:
        pil_image = Image.fromarray(frame)
        images.append(pil_image)

    return images

video = load_video("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif")

output = pipe(
    video = video,
    prompt="panda playing a guitar, on a boat, in the ocean, high quality",
    negative_prompt="bad quality, worse quality",
    guidance_scale=7.5,
    num_inference_steps=25,
    strength=0.5,
    generator=torch.Generator("cpu").manual_seed(42),
)
frames = output.frames[0]
export_to_gif(frames, "animation.gif")
```

以下是一些示例输出：

| 源视频 | 输出视频 |
| --- | --- |
| 熊猫弹吉他 ![熊猫弹吉他](../Images/8b06f535c08fd5818358e157de31162b.png) | 熊猫弹吉他 ![熊猫弹吉他](../Images/d8d633fb4e433a768b4a0d3401f7551e.png) |
| 玛格特·罗比的特写，背景中的烟花，高质量 ![closeup of margot robbie, fireworks in the background, high quality](../Images/7944cbba13143eca48cff9f5becb8432.png) | 托尼·斯塔克的特写，小罗伯特·唐尼，烟花 ![closeup of tony stark, robert downey jr, fireworks](../Images/0753d28093b180396b738602acec6fb8.png) |

## 使用 Motion LoRAs

Motion LoRAs 是一组 LoRA，与 `guoyww/animatediff-motion-adapter-v1-5-2` 检查点一起使用。这些 LoRA 负责为动画添加特定类型的运动。

```py
import torch
from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter
from diffusers.utils import export_to_gif

# Load the motion adapter
adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
# load SD 1.5 based finetuned model
model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)
pipe.load_lora_weights(
    "guoyww/animatediff-motion-lora-zoom-out", adapter_name="zoom-out"
)

scheduler = DDIMScheduler.from_pretrained(
    model_id,
    subfolder="scheduler",
    clip_sample=False,
    beta_schedule="linear",
    timestep_spacing="linspace",
    steps_offset=1,
)
pipe.scheduler = scheduler

# enable memory savings
pipe.enable_vae_slicing()
pipe.enable_model_cpu_offload()

output = pipe(
    prompt=(
        "masterpiece, bestquality, highlydetailed, ultradetailed, sunset, "
        "orange sky, warm lighting, fishing boats, ocean waves seagulls, "
        "rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, "
        "golden hour, coastal landscape, seaside scenery"
    ),
    negative_prompt="bad quality, worse quality",
    num_frames=16,
    guidance_scale=7.5,
    num_inference_steps=25,
    generator=torch.Generator("cpu").manual_seed(42),
)
frames = output.frames[0]
export_to_gif(frames, "animation.gif")

```

| 杰作，最佳质量，日落。 ![masterpiece, bestquality, sunset](../Images/29b358e4dc2b586485e19d14fb0af55e.png) |
| --- |

## 使用 PEFT 与 Motion LoRAs

您还可以利用 [PEFT](https://github.com/huggingface/peft) 后端来组合 Motion LoRA，并创建更复杂的动画。

首先安装 PEFT

```py
pip install peft
```

然后，您可以使用以下代码组合 Motion LoRAs。

```py
import torch
from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter
from diffusers.utils import export_to_gif

# Load the motion adapter
adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2", torch_dtype=torch.float16)
# load SD 1.5 based finetuned model
model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)

pipe.load_lora_weights(
    "diffusers/animatediff-motion-lora-zoom-out", adapter_name="zoom-out",
)
pipe.load_lora_weights(
    "diffusers/animatediff-motion-lora-pan-left", adapter_name="pan-left",
)
pipe.set_adapters(["zoom-out", "pan-left"], adapter_weights=[1.0, 1.0])

scheduler = DDIMScheduler.from_pretrained(
    model_id,
    subfolder="scheduler",
    clip_sample=False,
    timestep_spacing="linspace",
    beta_schedule="linear",
    steps_offset=1,
)
pipe.scheduler = scheduler

# enable memory savings
pipe.enable_vae_slicing()
pipe.enable_model_cpu_offload()

output = pipe(
    prompt=(
        "masterpiece, bestquality, highlydetailed, ultradetailed, sunset, "
        "orange sky, warm lighting, fishing boats, ocean waves seagulls, "
        "rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, "
        "golden hour, coastal landscape, seaside scenery"
    ),
    negative_prompt="bad quality, worse quality",
    num_frames=16,
    guidance_scale=7.5,
    num_inference_steps=25,
    generator=torch.Generator("cpu").manual_seed(42),
)
frames = output.frames[0]
export_to_gif(frames, "animation.gif")

```

| 杰作，最佳质量，日落。 ![masterpiece, bestquality, sunset](../Images/14236a74edbef3ceafca9ae32e3ab3d4.png) |
| --- |

## 使用 FreeInit

[FreeInit: Bridging Initialization Gap in Video Diffusion Models](https://arxiv.org/abs/2312.07537) by Tianxing Wu, Chenyang Si, Yuming Jiang, Ziqi Huang, Ziwei Liu.

FreeInit 是一种有效的方法，可以在不进行任何额外训练的情况下改善使用视频扩散模型生成的视频的时间一致性和整体质量。它可以在推断时无缝应用于 AnimateDiff、ModelScope、VideoCrafter 和其他各种视频生成模型，并通过迭代地改进潜在初始化噪声来工作。更多细节可以在论文中找到。

以下示例演示了 FreeInit 的用法。

```py
import torch
from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
from diffusers.utils import export_to_gif

adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
model_id = "SG161222/Realistic_Vision_V5.1_noVAE"
pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16).to("cuda")
pipe.scheduler = DDIMScheduler.from_pretrained(
    model_id,
    subfolder="scheduler",
    beta_schedule="linear",
    clip_sample=False,
    timestep_spacing="linspace",
    steps_offset=1
)

# enable memory savings
pipe.enable_vae_slicing()
pipe.enable_vae_tiling()

# enable FreeInit
# Refer to the enable_free_init documentation for a full list of configurable parameters
pipe.enable_free_init(method="butterworth", use_fast_sampling=True)

# run inference
output = pipe(
    prompt="a panda playing a guitar, on a boat, in the ocean, high quality",
    negative_prompt="bad quality, worse quality",
    num_frames=16,
    guidance_scale=7.5,
    num_inference_steps=20,
    generator=torch.Generator("cpu").manual_seed(666),
)

# disable FreeInit
pipe.disable_free_init()

frames = output.frames[0]
export_to_gif(frames, "animation.gif")
```

FreeInit 实际上并不是免费的 - 改进的质量是以额外计算为代价的。它需要根据启用时设置的 `num_iters` 参数进行几次额外采样。将 `use_fast_sampling` 参数设置为 `True` 可以提高整体性能（与 `use_fast_sampling=False` 时相比质量较低，但仍比普通视频生成模型有更好的结果）。

确保查看调度程序指南以了解如何探索调度程序速度和质量之间的权衡，并查看重用管道中的组件部分，以了解如何有效地将相同组件加载到多个管道中。

## AnimateDiffPipeline

### `class diffusers.AnimateDiffPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L155)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel motion_adapter: MotionAdapter scheduler: Union feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)） - 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并从潜在表示中解码图像。

+   `text_encoder`（`CLIPTextModel`） - 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer`（`CLIPTokenizer`） - 用于对文本进行标记化的 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)） - 用于创建 UNetMotionModel 以去噪编码视频潜在表示的 [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。

+   `motion_adapter`（`MotionAdapter`） - 用于与 `unet` 结合使用以去噪编码视频潜在表示的 `MotionAdapter`。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin））— 与`unet`结合使用以去噪编码图像潜在变量的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

文本到视频生成的管道。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载 LoRA 权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存 LoRA 权重

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载 IP 适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L867)

```py
( prompt: Union = None num_frames: Optional = 16 height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成视频的像素高度。

+   `width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`）— 生成视频的像素宽度。

+   `num_frames`（`int`，*可选*，默认为16）— 生成的视频帧数。默认为16帧，每秒8帧，相当于2秒的视频。

+   `num_inference_steps`（`int`，*可选*，默认为50）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的视频，但会降低推理速度。

+   `guidance_scale`（`float`，*可选*，默认为7.5）— 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）被忽略。

+   `eta`（`float`，*可选*，默认为0.0）— 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度器中被忽略。

+   `generator`（`torch.Generator`或`List[torch.Generator]`，*可选*）— 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents`（`torch.FloatTensor`，*可选*）— 从高斯分布中预先生成的噪声潜在变量，用作视频生成的输入。可用于使用不同提示微调相同的生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜在变量张量，潜在变量应具有形状`(batch_size, num_channel, num_frames, height, width)`。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`, *可选*): 可选的图像输入以与IP适配器一起使用。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成视频的输出格式。可选择`torch.FloatTensor`、`PIL.Image`或`np.array`。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将作为[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`传递的kwargs字典。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的CLIP层数。值为1表示将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — 用于`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[TextToVideoSDPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/text_to_video#diffusers.pipelines.text_to_video_synthesis.TextToVideoSDPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的帧的列表。

用于生成的管道的调用函数。

示例:

```py
>>> import torch
>>> from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
>>> from diffusers.utils import export_to_gif

>>> adapter = MotionAdapter.from_pretrained("guoyww/animatediff-motion-adapter-v1-5-2")
>>> pipe = AnimateDiffPipeline.from_pretrained("frankjoshua/toonyou_beta6", motion_adapter=adapter)
>>> pipe.scheduler = DDIMScheduler(beta_schedule="linear", steps_offset=1, clip_sample=False)
>>> output = pipe(prompt="A corgi walking in the park")
>>> frames = output.frames[0]
>>> export_to_gif(frames, "animation.gif")
```

#### `disable_free_init`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L591)

```py
( )
```

如果启用，则禁用FreeInit机制。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L539)

```py
( )
```

如果启用，则禁用FreeU机制。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L491)

```py
( )
```

禁用切片的VAE解码。如果先前启用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L508)

```py
( )
```

禁用平铺的VAE解码。如果先前启用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_free_init`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L547)

```py
( num_iters: int = 3 use_fast_sampling: bool = False method: str = 'butterworth' order: int = 4 spatial_stop_frequency: float = 0.25 temporal_stop_frequency: float = 0.25 generator: Generator = None )
```

参数

+   `num_iters` (`int`, *可选*, 默认为`3`) — FreeInit噪声重新初始化迭代次数。

+   `use_fast_sampling` (`bool`, *optional*, 默认为 `False`) — 是否加速采样过程以降低可能的质量结果。如果设置为 `True`，则启用“由粗到细采样”策略，如论文中所述。

+   `method` (`str`, *optional*, 默认为 `butterworth`) — 必须是 `butterworth`、`ideal` 或 `gaussian` 中的一个，用作FreeInit低通滤波器的滤波方法。

+   `order` (`int`, *optional*, 默认为 `4`) — `butterworth` 方法中使用的滤波器的阶数。较大的值导致 `ideal` 方法行为，而较小的值导致 `gaussian` 方法行为。

+   `spatial_stop_frequency` (`float`, *optional*, 默认为 `0.25`) — 空间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为 `d_s`。

+   `temporal_stop_frequency` (`float`, *optional*, 默认为 `0.25`) — 时间维度的归一化停止频率。必须介于0到1之间。在原始实现中称为 `d_t`。

+   `generator` (`torch.Generator`, *optional*, defaults to `0.25`) — 用于使FreeInit生成变得确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

启用FreeInit机制，如[https://arxiv.org/abs/2312.07537](https://arxiv.org/abs/2312.07537)。

此实现已经从[官方存储库](https://github.com/TianxingWu/FreeInit)进行了调整。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L516)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段1的缩放因子，用于减弱跳跃特征的贡献。这是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 阶段2的缩放因子，用于减弱跳跃特征的贡献。这是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 阶段1的缩放因子，用于放大主干特征。

+   `b2` (`float`) — 阶段2的缩放因子，用于放大主干特征的贡献。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L483)

```py
( )
```

启用切片VAE解码。当启用此选项时，VAE将输入张量分割成多个切片以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L499)

```py
( )
```

启用平铺VAE解码。当启用此选项时，VAE将输入张量分割成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff.py#L223)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` 或 `List[str]`, *optional*) — 要编码的提示设备 — (`torch.device`): torch 设备

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由引导

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示或提示。如果未定义，则必须传递 `negative_prompt_embeds`。在不使用指导时被忽略（即如果 `guidance_scale` 小于 `1`，则被忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成负文本嵌入。

+   `lora_scale` (`float`, *optional*) — 如果加载了 LoRA 层，则将应用于文本编码器的所有 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要跳过的 CLIP 层数。值为 1 表示将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## AnimateDiffVideoToVideoPipeline

### `class diffusers.AnimateDiffVideoToVideoPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L166)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel motion_adapter: MotionAdapter scheduler: Union feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示形式。

+   `text_encoder` (`CLIPTextModel`) — 冻结的文本编码器（clip-vit-large-patch14）。

+   `tokenizer` (`CLIPTokenizer`) — 用于对文本进行标记化的 [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于创建 UNetMotionModel 以去噪编码视频潜在空间的 [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)。

+   `motion_adapter` (`MotionAdapter`) — 与 `unet` 结合使用以去噪编码视频潜在空间的 `MotionAdapter`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与 `unet` 结合使用以去噪编码图像潜在空间的调度器。可以是 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) 或 [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler) 中的一个。

视频到视频生成的管道。

该模型继承自 [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承以下加载方法：

+   加载文本反演嵌入的 load_textual_inversion()

+   加载 LoRA 权重的 load_lora_weights()

+   保存 LoRA 权重的 save_lora_weights()

+   加载 IP 适配器的 load_ip_adapter()

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L726)

```py
( video: List = None prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: Optional = None guidance_scale: float = 7.5 strength: float = 0.8 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) → export const metadata = 'undefined';AnimateDiffPipelineOutput or tuple
```

参数

+   `video` (`List[PipelineImageInput]`) — 生成条件的输入视频。必须是视频的图像/帧列表。

+   `prompt` (`str` 或 `List[str]`, *optional*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `height` (`int`, *optional*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成视频的像素高度。

+   `width` (`int`, *optional*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成视频的像素宽度。

+   `num_inference_steps` (`int`, *optional*, 默认为50) — 降噪步骤的数量。更多的降噪步骤通常会导致视频质量更高，但推理速度较慢。

+   `strength` (`float`, *optional*, 默认为0.8) — 更高的强度会导致原始视频和生成视频之间的差异更大。

+   `guidance_scale` (`float`, *optional*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 用于指导图像生成中不包含的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）忽略。

+   `eta` (`float`, *optional*, 默认为0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中抽样的预生成嘈杂潜变量，用作视频生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行抽样生成潜变量张量，形状应为 `(batch_size, num_channel, num_frames, height, width)`。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`, *optional*): 用于与IP适配器一起使用的可选图像输入。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成视频的输出格式。选择 `torch.FloatTensor`、`PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回 `AnimateDiffPipelineOutput` 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给 [`AttentionProcessor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `self.processor`。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从CLIP中跳过的层数。值为1表示将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推理过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *可选*) — `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包括在您的管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

`AnimateDiffPipelineOutput` 或 `tuple`

如果 `return_dict` 为 `True`，则返回 `AnimateDiffPipelineOutput`，否则返回一个 `tuple`，其中第一个元素是包含生成帧的列表。

用于生成的管道的调用函数。

示例：

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L521)

```py
( )
```

如果启用了FreeU机制，则禁用它。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L473)

```py
( )
```

禁用切片的VAE解码。如果之前启用了 `enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L490)

```py
( )
```

禁用平铺的VAE解码。如果之前启用了 `enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L498)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 用于减弱跳跃特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 用于减弱跳跃特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 用于放大主干特征贡献的阶段1的缩放因子。

+   `b2` (`float`) — 用于放大主干特征贡献的阶段2的缩放因子。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L465)

```py
( )
```

启用切片的VAE解码。当启用此选项时，VAE将将输入张量分割成片段以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L481)

```py
( )
```

启用平铺的VAE解码。当启用此选项时，VAE将将输入张量分割成瓦片以在几个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_animatediff_video2video.py#L234)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 要编码的提示设备 —（`torch.device`）：torch设备

+   `num_images_per_prompt`（`int`）— 应该为每个提示生成的图像数量

+   `do_classifier_free_guidance`（`bool`）— 是否使用分类器自由指导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即，如果`guidance_scale`小于`1`，则被忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负面文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale`（`float`，*可选*）— 将应用于文本编码器的所有LoRA层的LoRA比例，如果加载了LoRA层。

+   `clip_skip`（`int`，*可选*）— 在计算提示嵌入时要跳过的CLIP层数。值为1意味着将使用预终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## AnimateDiffPipelineOutput

### `class diffusers.pipelines.animatediff.AnimateDiffPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/animatediff/pipeline_output.py#L11)

```py
( frames: Union )
```

参数

+   `frames`（`List[List[PIL.Image.Image]]`或`torch.Tensor`或`np.ndarray`）— 长度为`batch_size`的PIL图像列表或torch.Tensor或np.ndarray，形状为`(batch_size, num_frames, height, width, num_channels)`。

AnimateDiff管道的输出类。
