- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/datasets/how_to_metrics](https://huggingface.co/docs/datasets/how_to_metrics)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/datasets/v2.17.0/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/start.146395b0.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/scheduler.bdbef820.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/singletons.98dc5b8b.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.8a885b74.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/paths.a483fec8.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/entry/app.e612c4fb.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/index.c0aea24a.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/0.5e8dbda6.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/nodes/22.87b3b8d2.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Tip.31005f7d.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/CodeBlock.6ccca92e.js">
    <link rel="modulepreload" href="/docs/datasets/v2.17.0/en/_app/immutable/chunks/Heading.2eb892cb.js">
  prefs: []
  type: TYPE_NORMAL
- en: Metrics is deprecated in ðŸ¤— Datasets. To learn more about how to use metrics,
    take a look at the library ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)!
    In addition to metrics, you can find more tools for evaluating models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics are important for evaluating a modelâ€™s predictions. In the tutorial,
    you learned how to compute a metric over an entire evaluation set. You have also
    seen how to load a metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Add predictions and references.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute metrics using different methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write your own metric loading script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add predictions and references
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you want to add model predictions and references to a [Metric](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric)
    instance, you have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Metric.add()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.add)
    adds a single `prediction` and `reference`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metric.add_batch()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.add_batch)
    adds a batch of `predictions` and `references`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use [Metric.add_batch()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.add_batch)
    by passing it your model predictions, and the references the model predictions
    should be evaluated against:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Metrics accepts various input formats (Python lists, NumPy arrays, PyTorch tensors,
    etc.) and converts them to an appropriate format for storage and computation.
  prefs: []
  type: TYPE_NORMAL
- en: Compute scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most straightforward way to calculate a metric is to call [Metric.compute()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Metric.compute).
    But some metrics have additional arguments that allow you to modify the metrics
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s load the [SacreBLEU](https://huggingface.co/metrics/sacrebleu) metric,
    and compute it with a different smoothing method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the SacreBLEU metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the different argument methods for computing the metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the metric with the `floor` method, and a different `smooth_value`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Custom metric loading script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Write a metric loading script to use your own custom metric (or one that is
    not on the Hub). Then you can load it as usual with [load_metric()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_metric).
  prefs: []
  type: TYPE_NORMAL
- en: To help you get started, open the [SQuAD metric loading script](https://github.com/huggingface/datasets/blob/main/metrics/squad/squad.py)
    and follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Get jump started with our metric loading script [template](https://github.com/huggingface/datasets/blob/f9713d2e23813142a02f1b0e965095f528785cff/templates/new_metric_script.py)!
  prefs: []
  type: TYPE_NORMAL
- en: Add metric attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Start by adding some information about your metric in `Metric._info()`. The
    most important attributes you should specify are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MetricInfo.description` provides a brief description about your metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MetricInfo.citation` contains a BibTex citation for the metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MetricInfo.inputs_description` describes the expected inputs and outputs.
    It may also provide an example usage of the metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MetricInfo.features` defines the name and type of the predictions and references.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After youâ€™ve filled out all these fields in the template, it should look like
    the following example from the SQuAD metric script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Download metric files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your metric needs to download, or retrieve local files, you will need to
    use the `Metric._download_and_prepare()` method. For this example, letâ€™s examine
    the [BLEURT metric loading script](https://github.com/huggingface/datasets/blob/main/metrics/bleurt/bleurt.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide a dictionary of URLs that point to the metric files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If the files are stored locally, provide a dictionary of path(s) instead of
    URLs.
  prefs: []
  type: TYPE_NORMAL
- en: '`Metric._download_and_prepare()` will take the URLs and download the metric
    files specified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Compute score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`DatasetBuilder._compute` provides the actual instructions for how to compute
    a metric given the predictions and references. Now letâ€™s take a look at the [GLUE
    metric loading script](https://github.com/huggingface/datasets/blob/main/metrics/glue/glue.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide the functions for `DatasetBuilder._compute` to calculate your metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `DatasetBuilder._compute` with instructions for what metric to calculate
    for each configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once youâ€™re finished writing your metric loading script, try to load it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
