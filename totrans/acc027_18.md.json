["```py\ndevice = \"cuda\"\nmodel.to(device)\n\ngradient_accumulation_steps = 2\n\nfor index, batch in enumerate(training_dataloader):\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss = loss / gradient_accumulation_steps\n    loss.backward()\n    if (index + 1) % gradient_accumulation_steps == 0:\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```", "```py\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n\n+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n+     model, optimizer, training_dataloader, scheduler\n+ )\n\n  for index, batch in enumerate(training_dataloader):\n      inputs, targets = batch\n-     inputs = inputs.to(device)\n-     targets = targets.to(device)\n      outputs = model(inputs)\n      loss = loss_function(outputs, targets)\n      loss = loss / gradient_accumulation_steps\n+     accelerator.backward(loss)\n      if (index+1) % gradient_accumulation_steps == 0:\n          optimizer.step()\n          scheduler.step()\n          optimizer.zero_grad()\n```", "```py\n  from accelerate import Accelerator\n- accelerator = Accelerator()\n+ accelerator = Accelerator(gradient_accumulation_steps=2)\n```", "```py\n- for index, batch in enumerate(training_dataloader):\n+ for batch in training_dataloader:\n+     with accelerator.accumulate(model):\n          inputs, targets = batch\n          outputs = model(inputs)\n```", "```py\n- loss = loss / gradient_accumulation_steps\n  accelerator.backward(loss)\n- if (index+1) % gradient_accumulation_steps == 0:\n  optimizer.step()\n  scheduler.step()\n  optimizer.zero_grad()\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import GradientAccumulationPlugin\n\nplugin = GradientAccumulationPlugin(sync_with_dataloader=False)\naccelerator = Accelerator(..., gradient_accumulation_plugin=plugin)\n```", "```py\nfrom accelerate import Accelerator\naccelerator = Accelerator(gradient_accumulation_steps=2)\nmodel, optimizer, training_dataloader, scheduler = accelerator.prepare(\n    model, optimizer, training_dataloader, scheduler\n)\nfor batch in training_dataloader:\n    with accelerator.accumulate(model):\n        inputs, targets = batch\n        outputs = model(inputs)\n        loss = loss_function(outputs, targets)\n        accelerator.backward(loss)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```", "```py\nimport torch\nimport copy\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# seed\nset_seed(0)\n\n# define toy inputs and labels\nx = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8.])\ny = torch.tensor([2., 4., 6., 8., 10., 12., 14., 16.])\ngradient_accumulation_steps = 4\nbatch_size = len(x) // gradient_accumulation_steps\n\n# define dataset and dataloader\ndataset = TensorDataset(x, y)\ndataloader = DataLoader(dataset, batch_size=batch_size)\n\n# define model, optimizer and loss function\nmodel = torch.zeros((1, 1), requires_grad=True)\nmodel_clone = copy.deepcopy(model)\ncriterion = torch.nn.MSELoss()\nmodel_optimizer = torch.optim.SGD([model], lr=0.02)\naccelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\nmodel, model_optimizer, dataloader = accelerator.prepare(model, model_optimizer, dataloader)\nmodel_clone_optimizer = torch.optim.SGD([model_clone], lr=0.02)\nprint(f\"initial model weight is {model.mean().item():.5f}\")\nprint(f\"initial model weight is {model_clone.mean().item():.5f}\")\nfor i, (inputs, labels) in enumerate(dataloader):\n    with accelerator.accumulate(model):\n        inputs = inputs.view(-1, 1)\n        print(i, inputs.flatten())\n        labels = labels.view(-1, 1)\n        outputs = inputs @ model\n        loss = criterion(outputs, labels)\n        accelerator.backward(loss)\n        model_optimizer.step()\n        model_optimizer.zero_grad()\nloss = criterion(x.view(-1, 1) @ model_clone, y.view(-1, 1))\nmodel_clone_optimizer.zero_grad()\nloss.backward()\nmodel_clone_optimizer.step()\nprint(f\"w/ accumulation, the final model weight is {model.mean().item():.5f}\")\nprint(f\"w/o accumulation, the final model weight is {model_clone.mean().item():.5f}\")\n```", "```py\ninitial model weight is 0.00000\ninitial model weight is 0.00000\n0 tensor([1., 2.])\n1 tensor([3., 4.])\n2 tensor([5., 6.])\n3 tensor([7., 8.])\nw/ accumulation, the final model weight is 2.04000\nw/o accumulation, the final model weight is 2.04000\n```"]