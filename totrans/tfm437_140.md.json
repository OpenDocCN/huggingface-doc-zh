["```py\n>>> from transformers import BigBirdPegasusConfig, BigBirdPegasusModel\n\n>>> # Initializing a BigBirdPegasus bigbird-pegasus-base style configuration\n>>> configuration = BigBirdPegasusConfig()\n\n>>> # Initializing a model (with random weights) from the bigbird-pegasus-base style configuration\n>>> model = BigBirdPegasusModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdPegasusModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> model = BigBirdPegasusModel.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n\n>>> model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"The dominant sequence transduction models are based on complex recurrent or convolutional neural \"\n...     \"networks in an encoder-decoder configuration. The best performing models also connect the encoder \"\n...     \"and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \"\n...     \"based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \"\n...     \"Experiments on two machine translation tasks show these models to be superior in quality \"\n...     \"while being more parallelizable and requiring significantly less time to train.\"\n... )\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=4096, return_tensors=\"pt\", truncation=True)\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=15)\n>>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'dominant sequence models are based on recurrent or convolutional neural networks .'\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdPegasusForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> model = BigBirdPegasusForSequenceClassification.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = BigBirdPegasusForSequenceClassification.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BigBirdPegasusForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> model = BigBirdPegasusForSequenceClassification.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = BigBirdPegasusForSequenceClassification.from_pretrained(\n...     \"google/bigbird-pegasus-large-arxiv\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdPegasusForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> model = BigBirdPegasusForQuestionAnswering.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, BigBirdPegasusForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n>>> model = BigBirdPegasusForCausalLM.from_pretrained(\n...     \"google/bigbird-pegasus-large-arxiv\", add_cross_attention=False\n... )\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n```"]