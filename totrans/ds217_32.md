# åˆ›å»ºéŸ³é¢‘æ•°æ®é›†

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/audio_dataset](https://huggingface.co/docs/datasets/audio_dataset)

æ‚¨å¯ä»¥é€šè¿‡åœ¨Hugging Face Hubä¸Šåˆ›å»ºæ•°æ®é›†å­˜å‚¨åº“ä¸å›¢é˜Ÿæˆ–ç¤¾åŒºä¸­çš„ä»»ä½•äººå…±äº«æ•°æ®é›†ï¼š

```py
from datasets import load_dataset

dataset = load_dataset("<username>/my_dataset")
```

æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åˆ›å»ºå’Œå…±äº«éŸ³é¢‘æ•°æ®é›†ï¼š

+   ä½¿ç”¨[Dataset.push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.push_to_hub)åœ¨Pythonä¸­ä»æœ¬åœ°æ–‡ä»¶åˆ›å»ºéŸ³é¢‘æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ç§åªéœ€è¦åœ¨Pythonä¸­è¿›è¡Œå‡ ä¸ªæ­¥éª¤çš„ç®€å•æ–¹æ³•ã€‚

+   ä½¿ç”¨`AudioFolder`æ„å»ºå™¨åˆ›å»ºéŸ³é¢‘æ•°æ®é›†å­˜å‚¨åº“ã€‚è¿™æ˜¯ä¸€ç§æ— ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¯å¿«é€Ÿåˆ›å»ºåŒ…å«æ•°åƒä¸ªéŸ³é¢‘æ–‡ä»¶çš„éŸ³é¢‘æ•°æ®é›†ã€‚

+   é€šè¿‡ç¼–å†™åŠ è½½è„šæœ¬åˆ›å»ºéŸ³é¢‘æ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºé«˜çº§ç”¨æˆ·ï¼Œéœ€è¦æ›´å¤šçš„åŠªåŠ›å’Œç¼–ç ï¼Œä½†æ‚¨å¯ä»¥æ›´çµæ´»åœ°å®šä¹‰ã€ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†ï¼Œè¿™å¯¹äºæ›´å¤æ‚æˆ–å¤§è§„æ¨¡çš„éŸ³é¢‘æ•°æ®é›†å¯èƒ½å¾ˆæœ‰ç”¨ã€‚

æ‚¨å¯ä»¥é€šè¿‡è¦æ±‚ç”¨æˆ·é¦–å…ˆå…±äº«å…¶è”ç³»ä¿¡æ¯æ¥æ§åˆ¶å¯¹æ•°æ®é›†çš„è®¿é—®æƒé™ã€‚æŸ¥çœ‹[Hugging Face Hubä¸Šçš„Gatedæ•°æ®é›†](https://huggingface.co/docs/hub/datasets-gated)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åœ¨Hubä¸Šå¯ç”¨æ­¤åŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ã€‚

## æœ¬åœ°æ–‡ä»¶

æ‚¨å¯ä»¥ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„åŠ è½½è‡ªå·±çš„æ•°æ®é›†ã€‚ä½¿ç”¨[cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)å‡½æ•°å°†éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è½¬æ¢ä¸º[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)ç‰¹å¾ï¼š

```py
>>> audio_dataset = Dataset.from_dict({"audio": ["path/to/audio_1", "path/to/audio_2", ..., "path/to/audio_n"]}).cast_column("audio", Audio())
>>> audio_dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': 'path/to/audio_1',
 'sampling_rate': 16000}
```

ç„¶åä½¿ç”¨[Dataset.push_to_hub()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.push_to_hub)å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hugging Face Hubï¼š

```py
audio_dataset.push_to_hub("<username>/my_dataset")
```

è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«æ‚¨éŸ³é¢‘æ•°æ®é›†çš„æ•°æ®é›†å­˜å‚¨åº“ï¼š

```py
my_dataset/
â”œâ”€â”€ README.md
â””â”€â”€ data/
    â””â”€â”€ train-00000-of-00001.parquet
```

## AudioFolder

`AudioFolder`æ˜¯ä¸€ä¸ªæ•°æ®é›†æ„å»ºå™¨ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è½½åŒ…å«æ•°åƒä¸ªéŸ³é¢‘æ–‡ä»¶çš„éŸ³é¢‘æ•°æ®é›†ï¼Œè€Œæ— éœ€æ‚¨ç¼–å†™ä»»ä½•ä»£ç ã€‚åªè¦åœ¨å…ƒæ•°æ®æ–‡ä»¶ï¼ˆ`metadata.csv`/`metadata.jsonl`ï¼‰ä¸­åŒ…å«æ­¤ä¿¡æ¯ï¼Œ`AudioFolder`ä¼šè‡ªåŠ¨åŠ è½½æœ‰å…³æ•°æ®é›†çš„ä»»ä½•å…¶ä»–ä¿¡æ¯ - å¦‚è½¬å½•ã€è¯´è¯è€…å£éŸ³æˆ–è¯´è¯è€…æ„å›¾ã€‚

ğŸ’¡æŸ¥çœ‹[Split pattern hierarchy](repository_structure#split-pattern-hierarchy)ä»¥äº†è§£æœ‰å…³`AudioFolder`å¦‚ä½•æ ¹æ®æ•°æ®é›†å­˜å‚¨åº“ç»“æ„åˆ›å»ºæ•°æ®é›†æ‹†åˆ†çš„æ›´å¤šä¿¡æ¯ã€‚

åœ¨Hugging Face Hubä¸Šåˆ›å»ºä¸€ä¸ªæ•°æ®é›†å­˜å‚¨åº“ï¼Œå¹¶æŒ‰ç…§`AudioFolder`ç»“æ„ä¸Šä¼ æ‚¨çš„æ•°æ®é›†ç›®å½•ï¼š

```py
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ metadata.csv
â””â”€â”€ data/
```

`data`æ–‡ä»¶å¤¹å¯ä»¥æ˜¯ä»»ä½•æ‚¨æƒ³è¦çš„åç§°ã€‚

å¦‚æœæ•°æ®åˆ—åŒ…å«æ›´å¤æ‚çš„æ ¼å¼ï¼ˆå¦‚æµ®ç‚¹æ•°åˆ—è¡¨ï¼‰æ—¶ï¼Œå°†å…ƒæ•°æ®å­˜å‚¨ä¸º`jsonl`æ–‡ä»¶å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä»¥é¿å…è§£æé”™è¯¯æˆ–å°†å¤æ‚å€¼è¯»å–ä¸ºå­—ç¬¦ä¸²ã€‚

å…ƒæ•°æ®æ–‡ä»¶åº”åŒ…æ‹¬ä¸€ä¸ª`file_name`åˆ—ï¼Œå°†éŸ³é¢‘æ–‡ä»¶ä¸å…¶å…ƒæ•°æ®é“¾æ¥èµ·æ¥ï¼š

```py
file_name,transcription
data/first_audio_file.mp3,znowu siÄ™ duch z ciaÅ‚em zroÅ›nie w mÅ‚odocianej wstaniesz wiosnie i moÅ¼esz skutkiem tych lekÃ³w umieraÄ‡ wstawaÄ‡ wiek wiekÃ³w dalej tam byÅ‚y przestrogi jak siekaÄ‡ gÅ‚owÄ™ jak nogi
data/second_audio_file.mp3,juÅ¼ u ÅºwierzyÅ„ca podwojÃ³w krÃ³l zasiada przy nim ksiÄ…Å¼Ä™ta i panowie rada a gdzie wzniosÅ‚y krÄ…Å¼yÅ‚ ganek rycerze obok kochanek krÃ³l skinÄ…Å‚ palcem zaczÄ™to igrzysko
data/third_audio_file.mp3,pewnie kÄ™dyÅ› w obÅ‚Ä™dzie ubite minÄ™Å‚y szlaki zaczekajmy dzieÅ„ jaki poÅ›lemy szukaÄ‡ wszÄ™dzie dziÅ› jutro pewnie bÄ™dzie posÅ‚ali wszÄ™dzie sÅ‚ugi czekali dzieÅ„ i drugi gdy nic nie doczekali z pÅ‚aczem chcÄ… jechaÄ‡ dali
```

ç„¶åæ‚¨å¯ä»¥å°†æ•°æ®é›†å­˜å‚¨åœ¨è¿™æ ·çš„ç›®å½•ç»“æ„ä¸­ï¼š

```py
metadata.csv
data/first_audio_file.mp3
data/second_audio_file.mp3
data/third_audio_file.mp3

```

ç”¨æˆ·ç°åœ¨å¯ä»¥é€šè¿‡åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ä¸­æŒ‡å®š`audiofolder`å’Œåœ¨`data_dir`ä¸­æŒ‡å®šæ•°æ®é›†ç›®å½•æ¥åŠ è½½æ‚¨çš„æ•°æ®é›†å’Œç›¸å…³å…ƒæ•°æ®ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("audiofolder", data_dir="/path/to/data")
>>> dataset["train"][0]
{'audio':
    {'path': '/path/to/extracted/audio/first_audio_file.mp3',
    'array': array([ 0.00088501,  0.0012207 ,  0.00131226, ..., -0.00045776, -0.00054932, -0.00054932], dtype=float32),
    'sampling_rate': 16000},
 'transcription': 'znowu siÄ™ duch z ciaÅ‚em zroÅ›nie w mÅ‚odocianej wstaniesz wiosnie i moÅ¼esz skutkiem tych lekÃ³w umieraÄ‡ wstawaÄ‡ wiek wiekÃ³w dalej tam byÅ‚y przestrogi jak siekaÄ‡ gÅ‚owÄ™ jak nogi'
}
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`audiofolder`åŠ è½½æ¶‰åŠå¤šä¸ªæ‹†åˆ†çš„æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæ‚¨çš„æ•°æ®é›†ç›®å½•å¯èƒ½å…·æœ‰ä»¥ä¸‹ç»“æ„ï¼š

```py
data/train/first_train_audio_file.mp3
data/train/second_train_audio_file.mp3

data/test/first_test_audio_file.mp3
data/test/second_test_audio_file.mp3

```

è¯·æ³¨æ„ï¼Œå¦‚æœéŸ³é¢‘æ–‡ä»¶ä¸æ˜¯ç´§é‚»å…ƒæ•°æ®æ–‡ä»¶ï¼Œ`file_name`åˆ—åº”è¯¥æ˜¯éŸ³é¢‘æ–‡ä»¶çš„å®Œæ•´ç›¸å¯¹è·¯å¾„ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡ä»¶åã€‚

å¯¹äºæ²¡æœ‰ä»»ä½•å…³è”å…ƒæ•°æ®çš„éŸ³é¢‘æ•°æ®é›†ï¼Œ`AudioFolder`ä¼šæ ¹æ®ç›®å½•åç§°è‡ªåŠ¨æ¨æ–­æ•°æ®é›†çš„ç±»æ ‡ç­¾ã€‚è¿™å¯¹äºéŸ³é¢‘åˆ†ç±»ä»»åŠ¡å¯èƒ½å¾ˆæœ‰ç”¨ã€‚æ‚¨çš„æ•°æ®é›†ç›®å½•å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
data/train/electronic/01.mp3
data/train/punk/01.mp3

data/test/electronic/09.mp3
data/test/punk/09.mp3
```

ä½¿ç”¨`AudioFolder`åŠ è½½æ•°æ®é›†ï¼Œå®ƒå°†ä»ç›®å½•åç§°ï¼ˆè¯­è¨€IDï¼‰åˆ›å»ºä¸€ä¸ª`label`åˆ—ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("audiofolder", data_dir="/path/to/data")
>>> dataset["train"][0]
{'audio':
    {'path': '/path/to/electronic/01.mp3',
     'array': array([ 3.9714024e-07,  7.3031038e-07,  7.5640685e-07, ...,
         -1.1963668e-01, -1.1681189e-01, -1.1244172e-01], dtype=float32),
     'sampling_rate': 44100},
 'label': 0  # "electronic"
}
>>> dataset["train"][-1]
{'audio':
    {'path': '/path/to/punk/01.mp3',
     'array': array([0.15237972, 0.13222949, 0.10627693, ..., 0.41940814, 0.37578005,
         0.33717662], dtype=float32),
     'sampling_rate': 44100},
 'label': 1  # "punk"
}
```

å¦‚æœæ‰€æœ‰éŸ³é¢‘æ–‡ä»¶éƒ½åŒ…å«åœ¨ä¸€ä¸ªå•ç‹¬çš„ç›®å½•ä¸­ï¼Œæˆ–è€…å®ƒä»¬ä¸åœ¨ç›¸åŒçº§åˆ«çš„ç›®å½•ç»“æ„ä¸­ï¼Œ`label` åˆ—ä¸ä¼šè‡ªåŠ¨æ·»åŠ ã€‚å¦‚æœéœ€è¦å®ƒï¼Œè¯·æ˜¾å¼è®¾ç½® `drop_labels=False`ã€‚

ä¸€äº›éŸ³é¢‘æ•°æ®é›†ï¼Œæ¯”å¦‚åœ¨[Kaggle ç«èµ›](https://www.kaggle.com/competitions/kaggle-pog-series-s01e02/overview)ä¸­å‘ç°çš„é‚£äº›ï¼Œä¸ºæ¯ä¸ªæ‹†åˆ†å•ç‹¬æä¾›äº†å…ƒæ•°æ®æ–‡ä»¶ã€‚åªè¦æ¯ä¸ªæ‹†åˆ†çš„å…ƒæ•°æ®ç‰¹å¾ç›¸åŒï¼Œ`audiofolder` å°±å¯ä»¥ç”¨æ¥ä¸€æ¬¡åŠ è½½æ‰€æœ‰æ‹†åˆ†ã€‚å¦‚æœæ¯ä¸ªæ‹†åˆ†çš„å…ƒæ•°æ®ç‰¹å¾ä¸åŒï¼Œæ‚¨åº”è¯¥ä½¿ç”¨å•ç‹¬çš„ `load_dataset()` è°ƒç”¨æ¥åŠ è½½å®ƒä»¬ã€‚

## åŠ è½½è„šæœ¬

ç¼–å†™ä¸€ä¸ªæ•°æ®é›†åŠ è½½è„šæœ¬æ¥æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚å®ƒå®šä¹‰äº†æ•°æ®é›†çš„æ‹†åˆ†å’Œé…ç½®ï¼Œå¹¶å¤„ç†ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†ç¤ºä¾‹ã€‚è„šæœ¬çš„åç§°åº”ä¸æ‚¨çš„æ•°æ®é›†æ–‡ä»¶å¤¹æˆ–å­˜å‚¨åº“ç›¸åŒï¼š

```py
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/
```

`data` æ–‡ä»¶å¤¹å¯ä»¥æ˜¯æ‚¨æƒ³è¦çš„ä»»ä½•åç§°ï¼Œä¸ä¸€å®šæ˜¯ `data`ã€‚é™¤éæ‚¨å°†æ•°æ®é›†æ‰˜ç®¡åœ¨ Hub ä¸Šï¼Œå¦åˆ™æ­¤æ–‡ä»¶å¤¹æ˜¯å¯é€‰çš„ã€‚

è¿™ä¸ªç›®å½•ç»“æ„å…è®¸æ‚¨çš„æ•°æ®é›†åœ¨ä¸€è¡Œä¸­åŠ è½½ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("path/to/my_dataset")
```

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä¸ºéŸ³é¢‘æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œè¿™ä¸[ä¸ºæ–‡æœ¬æ•°æ®é›†åˆ›å»ºåŠ è½½è„šæœ¬](./dataset_script)æœ‰äº›ä¸åŒã€‚éŸ³é¢‘æ•°æ®é›†é€šå¸¸å­˜å‚¨åœ¨ `tar.gz` å­˜æ¡£ä¸­ï¼Œè¿™éœ€è¦ä¸€ç§ç‰¹æ®Šçš„æ–¹æ³•æ¥æ”¯æŒæµå¼æ¨¡å¼ã€‚è™½ç„¶æµå¼ä¼ è¾“ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨æ‚¨çš„éŸ³é¢‘æ•°æ®é›†ä¸­å®ç°æµå¼æ”¯æŒï¼Œå› ä¸ºæ²¡æœ‰å¤ªå¤šç£ç›˜ç©ºé—´çš„ç”¨æˆ·å¯ä»¥åœ¨ä¸ä¸‹è½½æ•°æ®é›†çš„æƒ…å†µä¸‹ä½¿ç”¨æ‚¨çš„æ•°æ®é›†ã€‚åœ¨[Stream](./stream)æŒ‡å—ä¸­äº†è§£æ›´å¤šå…³äºæµå¼ä¼ è¾“çš„ä¿¡æ¯ï¼

è¿™é‡Œæ˜¯ä¸€ä¸ªä½¿ç”¨ TAR å­˜æ¡£çš„ç¤ºä¾‹ï¼š

```py
my_dataset/
â”œâ”€â”€ README.md
â”œâ”€â”€ my_dataset.py
â””â”€â”€ data/
    â”œâ”€â”€ train.tar.gz
    â”œâ”€â”€ test.tar.gz
    â””â”€â”€ metadata.csv
```

é™¤äº†å­¦ä¹ å¦‚ä½•åˆ›å»ºä¸€ä¸ªå¯æµå¼ä¼ è¾“çš„æ•°æ®é›†ï¼Œæ‚¨è¿˜å°†å­¦ä¹ å¦‚ä½•ï¼š

+   åˆ›å»ºä¸€ä¸ªæ•°æ®é›†æ„å»ºå™¨ç±»ã€‚

+   åˆ›å»ºæ•°æ®é›†é…ç½®ã€‚

+   æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®ã€‚

+   ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†ã€‚

+   ç”Ÿæˆæ•°æ®é›†ã€‚

+   å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hubã€‚

å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰“å¼€ä¸€ä¸ªç°æœ‰çš„éŸ³é¢‘æ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œæ¯”å¦‚[Vivos](https://huggingface.co/datasets/vivos/blob/main/vivos.py)ï¼Œå¹¶è·Ÿç€åšï¼

æœ¬æŒ‡å—å±•ç¤ºäº†å¦‚ä½•å¤„ç†å­˜å‚¨åœ¨ TAR å­˜æ¡£ä¸­çš„éŸ³é¢‘æ•°æ® - è¿™æ˜¯éŸ³é¢‘æ•°æ®é›†çš„æœ€å¸¸è§æƒ…å†µã€‚æŸ¥çœ‹[minds14](https://huggingface.co/datasets/PolyAI/minds14/blob/main/minds14.py) æ•°æ®é›†ï¼Œäº†è§£ä¸€ä¸ªä½¿ç”¨ ZIP å­˜æ¡£çš„éŸ³é¢‘è„šæœ¬ç¤ºä¾‹ã€‚

ä¸ºäº†å¸®åŠ©æ‚¨å…¥é—¨ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŠ è½½è„šæœ¬[æ¨¡æ¿](https://github.com/huggingface/datasets/blob/main/templates/new_dataset_script.py)ï¼Œæ‚¨å¯ä»¥å¤åˆ¶å¹¶ç”¨ä½œèµ·ç‚¹ï¼

### åˆ›å»ºä¸€ä¸ªæ•°æ®é›†æ„å»ºå™¨ç±»

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) æ˜¯ä»å­—å…¸ç”Ÿæˆå™¨ç”Ÿæˆçš„æ•°æ®é›†çš„åŸºç±»ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæœ‰ä¸‰ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æ‚¨åˆ›å»ºæ•°æ®é›†ï¼š

+   `_info` å­˜å‚¨æœ‰å…³æ•°æ®é›†çš„ä¿¡æ¯ï¼Œå¦‚æè¿°ã€è®¸å¯è¯å’Œç‰¹å¾ã€‚

+   `_split_generators` ä¸‹è½½æ•°æ®é›†å¹¶å®šä¹‰å…¶æ‹†åˆ†ã€‚

+   `_generate_examples` ä¸ºæ¯ä¸ªæ‹†åˆ†ç”ŸæˆåŒ…å«éŸ³é¢‘æ•°æ®å’Œ `info` ä¸­æŒ‡å®šçš„å…¶ä»–ç‰¹å¾çš„æ•°æ®é›†æ ·æœ¬ã€‚

é¦–å…ˆåˆ›å»ºæ‚¨çš„æ•°æ®é›†ç±»ï¼Œä½œä¸º [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) çš„å­ç±»ï¼Œå¹¶æ·»åŠ è¿™ä¸‰ç§æ–¹æ³•ã€‚æš‚æ—¶ä¸ç”¨æ‹…å¿ƒå¡«å†™è¿™äº›æ–¹æ³•ä¸­çš„æ¯ä¸€ä¸ªï¼Œæ‚¨å°†åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªéƒ¨åˆ†ä¸­å¼€å‘å®ƒä»¬ï¼š

```py
class VivosDataset(datasets.GeneratorBasedBuilder):
    """VIVOS is a free Vietnamese speech corpus consisting of 15 hours of recording speech prepared for
    Vietnamese Automatic Speech Recognition task."""

    def _info(self):

    def _split_generators(self, dl_manager):

    def _generate_examples(self, prompts_path, path_to_clips, audio_files):

```

#### å¤šä¸ªé…ç½®

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸€ä¸ªæ•°æ®é›†å¯èƒ½æœ‰å¤šä¸ªé…ç½®ã€‚ä¾‹å¦‚ï¼Œ[LibriVox Indonesia](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia) æ•°æ®é›†æœ‰å‡ ä¸ªå¯¹åº”ä¸åŒè¯­è¨€çš„é…ç½®ã€‚

è¦åˆ›å»ºä¸åŒçš„é…ç½®ï¼Œä½¿ç”¨ [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig) ç±»æ¥åˆ›å»ºæ•°æ®é›†çš„å­ç±»ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯é…ç½®çš„ `name`ï¼Œå¿…é¡»ä¼ é€’ç»™é…ç½®çš„è¶…ç±» `__init__()`ã€‚å¦åˆ™ï¼Œæ‚¨å¯ä»¥åœ¨é…ç½®ç±»ä¸­æŒ‡å®šä»»ä½•è‡ªå®šä¹‰å‚æ•°ã€‚

```py
class LibriVoxIndonesiaConfig(datasets.BuilderConfig):
    """BuilderConfig for LibriVoxIndonesia."""

    def __init__(self, name, version, **kwargs):
        self.language = kwargs.pop("language", None)
        self.release_date = kwargs.pop("release_date", None)
        self.num_clips = kwargs.pop("num_clips", None)
        self.num_speakers = kwargs.pop("num_speakers", None)
        self.validated_hr = kwargs.pop("validated_hr", None)
        self.total_hr = kwargs.pop("total_hr", None)
        self.size_bytes = kwargs.pop("size_bytes", None)
        self.size_human = size_str(self.size_bytes)
        description = (
            f"LibriVox-Indonesia speech to text dataset in {self.language} released on {self.release_date}. "
            f"The dataset comprises {self.validated_hr} hours of transcribed speech data"
        )
        super(LibriVoxIndonesiaConfig, self).__init__(
            name=name,
            version=datasets.Version(version),
            description=description,
            **kwargs,
        )
```

åœ¨ [GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) å†…éƒ¨çš„ `BUILDER_CONFIGS` ç±»å˜é‡ä¸­å®šä¹‰æ‚¨çš„é…ç½®ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œä½œè€…ä»ä¸€ä¸ªå•ç‹¬çš„ `release_stats.py` [æ–‡ä»¶](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/blob/main/release_stats.py) ä¸­å¯¼å…¥è¯­è¨€ï¼Œç„¶åå¾ªç¯éå†æ¯ç§è¯­è¨€ä»¥åˆ›å»ºä¸€ä¸ªé…ç½®ï¼š

```py
class LibriVoxIndonesia(datasets.GeneratorBasedBuilder):
    DEFAULT_CONFIG_NAME = "all"

    BUILDER_CONFIGS = [
        LibriVoxIndonesiaConfig(
            name=lang,
            version=STATS["version"],
            language=LANGUAGES[lang],
            release_date=STATS["date"],
            num_clips=lang_stats["clips"],
            num_speakers=lang_stats["users"],
            total_hr=float(lang_stats["totalHrs"]) if lang_stats["totalHrs"] else None,
            size_bytes=int(lang_stats["size"]) if lang_stats["size"] else None,
        )
        for lang, lang_stats in STATS["locales"].items()
    ]
```

é€šå¸¸ï¼Œç”¨æˆ·éœ€è¦æŒ‡å®šä¸€ä¸ªé…ç½®æ¥åŠ è½½ [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)ï¼Œå¦åˆ™ä¼šå¼•å‘ `ValueError`ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®é»˜è®¤æ•°æ®é›†é…ç½®æ¥é¿å…è¿™ç§æƒ…å†µï¼Œä»¥åœ¨ `DEFAULT_CONFIG_NAME` ä¸­åŠ è½½ã€‚

ç°åœ¨ï¼Œå¦‚æœç”¨æˆ·æƒ³è¦åŠ è½½ Balinese (`bal`) é…ç½®ï¼Œä»–ä»¬å¯ä»¥ä½¿ç”¨é…ç½®åç§°ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("indonesian-nlp/librivox-indonesia", "bal", split="train")
```

### æ·»åŠ æ•°æ®é›†å…ƒæ•°æ®

æ·»åŠ å…³äºæ‚¨çš„æ•°æ®é›†çš„ä¿¡æ¯å¯ä»¥å¸®åŠ©ç”¨æˆ·äº†è§£æ›´å¤šã€‚è¿™äº›ä¿¡æ¯å­˜å‚¨åœ¨ [DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo) ç±»ä¸­ï¼Œè¯¥ç±»ç”± `info` æ–¹æ³•è¿”å›ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®è¿™äº›ä¿¡æ¯ï¼š

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder("vivos")
>>> ds_builder.info
```

æ‚¨å¯ä»¥åŒ…å«å¾ˆå¤šå…³äºæ•°æ®é›†çš„ä¿¡æ¯ï¼Œä½†ä¸€äº›é‡è¦çš„ä¿¡æ¯åŒ…æ‹¬ï¼š

1.  `description` æä¾›æ•°æ®é›†çš„ç®€æ˜æè¿°ã€‚

1.  `features` æŒ‡å®šæ•°æ®é›†åˆ—ç±»å‹ã€‚ç”±äºæ‚¨æ­£åœ¨åˆ›å»ºä¸€ä¸ªéŸ³é¢‘åŠ è½½è„šæœ¬ï¼Œæ‚¨éœ€è¦åŒ…æ‹¬ [Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio) ç‰¹å¾å’Œæ•°æ®é›†çš„ `sampling_rate`ã€‚

1.  `homepage` æä¾›æ•°æ®é›†ä¸»é¡µçš„é“¾æ¥ã€‚

1.  `license` æŒ‡å®šä½¿ç”¨æ•°æ®é›†çš„æƒé™ï¼Œç”±è®¸å¯è¯ç±»å‹å®šä¹‰ã€‚

1.  `citation` æ˜¯æ•°æ®é›†çš„ BibTeX å¼•ç”¨ã€‚

æ‚¨ä¼šæ³¨æ„åˆ°å¾ˆå¤šæ•°æ®é›†ä¿¡æ¯åœ¨åŠ è½½è„šæœ¬ä¸­æ—©å·²å®šä¹‰ï¼Œè¿™å¯ä»¥ä½¿é˜…è¯»æ›´åŠ å®¹æ˜“ã€‚è¿˜æœ‰å…¶ä»– `~Dataset.Features` æ‚¨å¯ä»¥è¾“å…¥ï¼Œæ‰€ä»¥ä¸€å®šè¦æŸ¥çœ‹å®Œæ•´åˆ—è¡¨å’Œ [features guide](./about_dataset_features) ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

```py
def _info(self):
    return datasets.DatasetInfo(
        description=_DESCRIPTION,
        features=datasets.Features(
            {
                "speaker_id": datasets.Value("string"),
                "path": datasets.Value("string"),
                "audio": datasets.Audio(sampling_rate=16_000),
                "sentence": datasets.Value("string"),
            }
        ),
        supervised_keys=None,
        homepage=_HOMEPAGE,
        license=_LICENSE,
        citation=_CITATION,
    )
```

### ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†

ç°åœ¨æ‚¨å·²ç»æ·»åŠ äº†ä¸€äº›å…³äºæ•°æ®é›†çš„ä¿¡æ¯ï¼Œä¸‹ä¸€æ­¥æ˜¯ä¸‹è½½æ•°æ®é›†å¹¶å®šä¹‰æ‹†åˆ†ã€‚

1.  ä½¿ç”¨ [download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download) æ–¹æ³•ä¸‹è½½ `_PROMPTS_URLS` ä¸­çš„å…ƒæ•°æ®æ–‡ä»¶å’Œ `_DATA_URL` ä¸­çš„éŸ³é¢‘ TAR å­˜æ¡£ã€‚æ­¤æ–¹æ³•è¿”å›æœ¬åœ°æ–‡ä»¶/å­˜æ¡£çš„è·¯å¾„ã€‚åœ¨æµå¼æ¨¡å¼ä¸‹ï¼Œå®ƒä¸ä¼šä¸‹è½½æ–‡ä»¶ï¼Œåªä¼šè¿”å›ä¸€ä¸ªä»ä¸­æµå¼ä¼ è¾“æ•°æ®çš„ URLã€‚æ­¤æ–¹æ³•æ¥å—ï¼š

    +   Hub æ•°æ®é›†å­˜å‚¨åº“ä¸­æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼ˆä¾‹å¦‚ï¼Œåœ¨ `data/` æ–‡ä»¶å¤¹ä¸­ï¼‰

    +   ä¸€ä¸ªæŒ‡å‘å…¶ä»–åœ°æ–¹æ‰˜ç®¡çš„æ–‡ä»¶çš„ URL

    +   ä¸€ä¸ªï¼ˆåµŒå¥—çš„ï¼‰æ–‡ä»¶åæˆ– URL çš„åˆ—è¡¨æˆ–å­—å…¸

1.  åœ¨ä¸‹è½½æ•°æ®é›†åï¼Œä½¿ç”¨ [SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator) æ¥ç»„ç»‡æ¯ä¸ªæ‹†åˆ†ä¸­çš„éŸ³é¢‘æ–‡ä»¶å’Œå¥å­æç¤ºã€‚ä¸ºæ¯ä¸ªæ‹†åˆ†å‘½åä¸€ä¸ªæ ‡å‡†åç§°ï¼Œå¦‚ï¼š`Split.TRAIN`ã€`Split.TEST` å’Œ `SPLIT.Validation`ã€‚

    åœ¨ `gen_kwargs` å‚æ•°ä¸­ï¼ŒæŒ‡å®š `prompts_path` å’Œ `path_to_clips` çš„æ–‡ä»¶è·¯å¾„ã€‚å¯¹äº `audio_files`ï¼Œæ‚¨éœ€è¦ä½¿ç”¨ [iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive) æ¥è¿­ä»£ TAR å­˜æ¡£ä¸­çš„éŸ³é¢‘æ–‡ä»¶ã€‚è¿™ä½¿å¾—æ‚¨çš„æ•°æ®é›†å¯ä»¥è¿›è¡Œæµå¼ä¼ è¾“ã€‚æ‰€æœ‰è¿™äº›æ–‡ä»¶è·¯å¾„éƒ½ä¼ é€’åˆ°ä¸‹ä¸€æ­¥ï¼Œæ‚¨å°†åœ¨é‚£é‡Œå®é™…ç”Ÿæˆæ•°æ®é›†ã€‚

```py
def _split_generators(self, dl_manager):
    """Returns SplitGenerators."""
    prompts_paths = dl_manager.download(_PROMPTS_URLS)
    archive = dl_manager.download(_DATA_URL)
    train_dir = "vivos/train"
    test_dir = "vivos/test"

    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "prompts_path": prompts_paths["train"],
                "path_to_clips": train_dir + "/waves",
                "audio_files": dl_manager.iter_archive(archive),
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.TEST,
            gen_kwargs={
                "prompts_path": prompts_paths["test"],
                "path_to_clips": test_dir + "/waves",
                "audio_files": dl_manager.iter_archive(archive),
            },
        ),
    ]
```

æ­¤å®ç°ä¸ä¼šæå–å·²ä¸‹è½½çš„å­˜æ¡£ã€‚å¦‚æœæ‚¨æƒ³è¦åœ¨ä¸‹è½½åæå–æ–‡ä»¶ï¼Œæ‚¨éœ€è¦é¢å¤–ä½¿ç”¨[extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.extract)ï¼Œè¯·å‚é˜…[(é«˜çº§) æœ¬åœ°æå– TAR å­˜æ¡£](#advanced-extract-tar-archives-locally)éƒ¨åˆ†ã€‚

### ç”Ÿæˆæ•°æ®é›†

[GeneratorBasedBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.GeneratorBasedBuilder) ç±»ä¸­çš„æœ€åä¸€ä¸ªæ–¹æ³•å®é™…ä¸Šç”Ÿæˆæ•°æ®é›†ä¸­çš„æ ·æœ¬ã€‚å®ƒæ ¹æ®`info`æ–¹æ³•ä¸­æŒ‡å®šçš„`features`ç»“æ„ç”Ÿæˆæ•°æ®é›†ã€‚å¦‚æ‚¨æ‰€è§ï¼Œ`generate_examples`æ¥å—æ¥è‡ªå‰ä¸€æ–¹æ³•çš„`prompts_path`ã€`path_to_clips`å’Œ`audio_files`ä½œä¸ºå‚æ•°ã€‚

TAR å­˜æ¡£ä¸­çš„æ–‡ä»¶æŒ‰é¡ºåºè®¿é—®å’Œç”Ÿæˆã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦é¦–å…ˆå‡†å¤‡å¥½ä¸ TAR æ–‡ä»¶ä¸­éŸ³é¢‘æ–‡ä»¶ç›¸å…³çš„å…ƒæ•°æ®ï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†å…¶ä¸ç›¸åº”çš„éŸ³é¢‘æ–‡ä»¶ä¸€èµ·ç”Ÿæˆã€‚

```py
examples = {}
with open(prompts_path, encoding="utf-8") as f:
    for row in f:
        data = row.strip().split(" ", 1)
        speaker_id = data[0].split("_")[0]
        audio_path = "/".join([path_to_clips, speaker_id, data[0] + ".wav"])
        examples[audio_path] = {
            "speaker_id": speaker_id,
            "path": audio_path,
            "sentence": data[1],
        }
```

æœ€åï¼Œåœ¨`audio_files`ä¸­è¿­ä»£æ–‡ä»¶å¹¶ä¸å…¶å¯¹åº”çš„å…ƒæ•°æ®ä¸€èµ·ç”Ÿæˆã€‚[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive) ç”Ÿæˆä¸€ä¸ªå…ƒç»„(`path`, `f`)ï¼Œå…¶ä¸­`path`æ˜¯ TAR å­˜æ¡£ä¸­æ–‡ä»¶çš„**ç›¸å¯¹**è·¯å¾„ï¼Œ`f`æ˜¯æ–‡ä»¶å¯¹è±¡æœ¬èº«ã€‚

```py
inside_clips_dir = False
id_ = 0
for path, f in audio_files:
    if path.startswith(path_to_clips):
        inside_clips_dir = True
        if path in examples:
            audio = {"path": path, "bytes": f.read()}
            yield id_, {**examples[path], "audio": audio}
            id_ += 1
    elif inside_clips_dir:
        break
```

å°†è¿™ä¸¤ä¸ªæ­¥éª¤ç»“åˆèµ·æ¥ï¼Œæ•´ä¸ª`_generate_examples`æ–¹æ³•çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```py
def _generate_examples(self, prompts_path, path_to_clips, audio_files):
    """Yields examples as (key, example) tuples."""
    examples = {}
    with open(prompts_path, encoding="utf-8") as f:
        for row in f:
            data = row.strip().split(" ", 1)
            speaker_id = data[0].split("_")[0]
            audio_path = "/".join([path_to_clips, speaker_id, data[0] + ".wav"])
            examples[audio_path] = {
                "speaker_id": speaker_id,
                "path": audio_path,
                "sentence": data[1],
            }
    inside_clips_dir = False
    id_ = 0
    for path, f in audio_files:
        if path.startswith(path_to_clips):
            inside_clips_dir = True
            if path in examples:
                audio = {"path": path, "bytes": f.read()}
                yield id_, {**examples[path], "audio": audio}
                id_ += 1
        elif inside_clips_dir:
            break
```

### å°†æ•°æ®é›†ä¸Šä¼ åˆ°Hub

ä¸€æ—¦æ‚¨çš„è„šæœ¬å‡†å¤‡å¥½äº†ï¼Œ[åˆ›å»ºä¸€ä¸ªæ•°æ®é›†å¡ç‰‡](./dataset_card)å¹¶[ä¸Šä¼ åˆ°Hub](./share)ã€‚

æ­å–œï¼Œæ‚¨ç°åœ¨å¯ä»¥ä»HubåŠ è½½æ‚¨çš„æ•°æ®é›†äº†ï¼ğŸ¥³

```py
>>> from datasets import load_dataset
>>> load_dataset("<username>/my_dataset")
```

### ï¼ˆé«˜çº§ï¼‰æœ¬åœ°æå– TAR å­˜æ¡£

åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œä¸‹è½½çš„å­˜æ¡£æ²¡æœ‰è¢«æå–ï¼Œå› æ­¤ç¤ºä¾‹ä¸åŒ…å«æœ‰å…³å®ƒä»¬åœ¨æœ¬åœ°å­˜å‚¨ä½ç½®çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£é‡Šå¦‚ä½•ä»¥æ”¯æŒæµå¼ä¼ è¾“çš„æ–¹å¼è¿›è¡Œæå–ï¼Œæˆ‘ä»¬å°†ç®€è¦ä»‹ç»[LibriVox Indonesia](https://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/blob/main/librivox-indonesia.py)åŠ è½½è„šæœ¬ã€‚

#### ä¸‹è½½å¹¶å®šä¹‰æ•°æ®é›†æ‹†åˆ†

1.  ä½¿ç”¨[download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)æ–¹æ³•ä¸‹è½½ä½äº`_AUDIO_URL`çš„éŸ³é¢‘æ•°æ®ã€‚

1.  è¦åœ¨æœ¬åœ°æå–éŸ³é¢‘ TAR å­˜æ¡£ï¼Œè¯·ä½¿ç”¨[extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.extract)ã€‚æ‚¨åªèƒ½åœ¨éæµå¼æ¨¡å¼ä¸‹ä½¿ç”¨æ­¤æ–¹æ³•ï¼ˆå½“`dl_manager.is_streaming=False`æ—¶ï¼‰ã€‚è¿™å°†è¿”å›æå–çš„å­˜æ¡£ç›®å½•çš„æœ¬åœ°è·¯å¾„ï¼š

    ```py
    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
    ```

1.  ä½¿ç”¨[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)æ–¹æ³•è¿­ä»£ä½äº`audio_path`çš„å­˜æ¡£ï¼Œå°±åƒä¸Šé¢çš„ Vivos ç¤ºä¾‹ä¸€æ ·ã€‚[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)ä¸æä¾›æœ‰å…³å­˜æ¡£ä¸­æ–‡ä»¶çš„å®Œæ•´è·¯å¾„çš„ä¿¡æ¯ï¼Œå³ä½¿å·²ç»æå–ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦åœ¨`gen_kwargs`ä¸­å°†`local_extracted_archive`è·¯å¾„ä¼ é€’ç»™ä¸‹ä¸€æ­¥ï¼Œä»¥ä¿ç•™æœ‰å…³å­˜æ¡£æå–ä½ç½®çš„ä¿¡æ¯ã€‚è¿™æ˜¯åœ¨ç”Ÿæˆç¤ºä¾‹æ—¶æ„å»ºæ­£ç¡®çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„æ‰€å¿…éœ€çš„ã€‚

æ‚¨éœ€è¦åŒæ—¶ä½¿ç”¨[download()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download)å’Œ[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)çš„ç»„åˆï¼Œå› ä¸ºæ— æ³•ç›´æ¥é€šè¿‡è·¯å¾„è®¿é—®TARå­˜æ¡£ä¸­çš„æ–‡ä»¶ã€‚ç›¸åï¼Œæ‚¨éœ€è¦è¿­ä»£å­˜æ¡£ä¸­çš„æ–‡ä»¶ï¼æ‚¨å¯ä»¥ä½¿ç”¨[download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)å’Œ[extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.extract)æ¥å¤„ç†TARå­˜æ¡£ï¼Œä½†åªèƒ½åœ¨éæµæ¨¡å¼ä¸‹è¿›è¡Œï¼Œå¦åˆ™ä¼šå‡ºé”™ã€‚

1.  ä½¿ç”¨[download_and_extract()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract)æ–¹æ³•ä¸‹è½½åœ¨`_METADATA_URL`ä¸­æŒ‡å®šçš„å…ƒæ•°æ®æ–‡ä»¶ã€‚è¯¥æ–¹æ³•ä»¥éæµæ¨¡å¼è¿”å›æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„ã€‚åœ¨æµæ¨¡å¼ä¸‹ï¼Œå®ƒä¸ä¼šåœ¨æœ¬åœ°ä¸‹è½½æ–‡ä»¶ï¼Œè€Œæ˜¯è¿”å›ç›¸åŒçš„URLã€‚

1.  ç°åœ¨ä½¿ç”¨[SplitGenerator](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.SplitGenerator)æ¥ç»„ç»‡æ¯ä¸ªæ‹†åˆ†ä¸­çš„éŸ³é¢‘æ–‡ä»¶å’Œå…ƒæ•°æ®ã€‚ä¸ºæ¯ä¸ªæ‹†åˆ†å‘½åä¸€ä¸ªæ ‡å‡†åç§°ï¼Œå¦‚ï¼š`Split.TRAIN`ã€`Split.TEST`å’Œ`SPLIT.Validation`ã€‚

    åœ¨`gen_kwargs`å‚æ•°ä¸­ï¼ŒæŒ‡å®š`local_extracted_archive`ã€`audio_files`ã€`metadata_path`å’Œ`path_to_clips`çš„æ–‡ä»¶è·¯å¾„ã€‚è¯·è®°ä½ï¼Œå¯¹äº`audio_files`ï¼Œæ‚¨éœ€è¦ä½¿ç”¨[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)æ¥è¿­ä»£TARå­˜æ¡£ä¸­çš„éŸ³é¢‘æ–‡ä»¶ã€‚è¿™å°†ä¸ºæ‚¨çš„æ•°æ®é›†å¯ç”¨æµå¼å¤„ç†ï¼æ‰€æœ‰è¿™äº›æ–‡ä»¶è·¯å¾„éƒ½ä¼ é€’åˆ°ä¸‹ä¸€æ­¥ï¼Œç”Ÿæˆæ•°æ®é›†æ ·æœ¬ã€‚

```py
def _split_generators(self, dl_manager):
    """Returns SplitGenerators."""
    dl_manager.download_config.ignore_url_params = True

    audio_path = dl_manager.download(_AUDIO_URL)
    local_extracted_archive = dl_manager.extract(audio_path) if not dl_manager.is_streaming else None
    path_to_clips = "librivox-indonesia"

    return [
        datasets.SplitGenerator(
            name=datasets.Split.TRAIN,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_train.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
        datasets.SplitGenerator(
            name=datasets.Split.TEST,
            gen_kwargs={
                "local_extracted_archive": local_extracted_archive,
                "audio_files": dl_manager.iter_archive(audio_path),
                "metadata_path": dl_manager.download_and_extract(_METADATA_URL + "/metadata_test.csv.gz"),
                "path_to_clips": path_to_clips,
            },
        ),
    ]
```

#### ç”Ÿæˆæ•°æ®é›†

åœ¨è¿™é‡Œï¼Œ`_generate_examples`æ¥å—æ¥è‡ªå…ˆå‰æ–¹æ³•çš„`local_extracted_archive`ã€`audio_files`ã€`metadata_path`å’Œ`path_to_clips`ä½œä¸ºå‚æ•°ã€‚

1.  TARæ–‡ä»¶æŒ‰é¡ºåºè®¿é—®å’Œäº§ç”Ÿã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦é¦–å…ˆå°†`metadata_path`ä¸­çš„å…ƒæ•°æ®ä¸TARæ–‡ä»¶ä¸­çš„éŸ³é¢‘æ–‡ä»¶å…³è”èµ·æ¥ï¼Œä»¥ä¾¿éšåå¯ä»¥å°†å…¶ä¸ç›¸åº”çš„éŸ³é¢‘æ–‡ä»¶ä¸€èµ·äº§ç”Ÿã€‚

    ```py
    with open(metadata_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if self.config.name == "all" or self.config.name == row["language"]:
                row["path"] = os.path.join(path_to_clips, row["path"])
                # if data is incomplete, fill with empty values
                for field in data_fields:
                    if field not in row:
                        row[field] = ""
                metadata[row["path"]] = row
    ```

1.  ç°åœ¨æ‚¨å¯ä»¥åœ¨`audio_files`å­˜æ¡£ä¸­è·å–æ–‡ä»¶ã€‚å½“æ‚¨ä½¿ç”¨[iter_archive()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadManager.iter_archive)æ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸€ä¸ªå…ƒç»„(`path`, `f`)ï¼Œå…¶ä¸­`path`æ˜¯å­˜æ¡£ä¸­æ–‡ä»¶çš„**ç›¸å¯¹è·¯å¾„**ï¼Œ`f`æ˜¯æ–‡ä»¶å¯¹è±¡æœ¬èº«ã€‚è¦è·å–æœ¬åœ°æå–æ–‡ä»¶çš„**å®Œæ•´è·¯å¾„**ï¼Œè¯·å°†å­˜æ¡£æå–åˆ°çš„ç›®å½•(`local_extracted_path`)çš„è·¯å¾„ä¸ç›¸å¯¹éŸ³é¢‘æ–‡ä»¶è·¯å¾„(`path`)è¿æ¥èµ·æ¥ï¼š

    ```py
    for path, f in audio_files:
        if path in metadata:
            result = dict(metadata[path])
            # set the audio feature and the path to the extracted file
            path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
            result["audio"] = {"path": path, "bytes": f.read()}
            result["path"] = path
            yield id_, result
            id_ += 1
    ```

å°†è¿™ä¸¤ä¸ªæ­¥éª¤ç»“åˆèµ·æ¥ï¼Œæ•´ä¸ª`_generate_examples`æ–¹æ³•åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
def _generate_examples( self,
        local_extracted_archive,
        audio_files,
        metadata_path,
        path_to_clips, ):
        """Yields examples."""
        data_fields = list(self._info().features.keys())
        metadata = {}
        with open(metadata_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if self.config.name == "all" or self.config.name == row["language"]:
                    row["path"] = os.path.join(path_to_clips, row["path"])
                    # if data is incomplete, fill with empty values
                    for field in data_fields:
                        if field not in row:
                            row[field] = ""
                    metadata[row["path"]] = row
        id_ = 0
        for path, f in audio_files:
            if path in metadata:
                result = dict(metadata[path])
                # set the audio feature and the path to the extracted file
                path = os.path.join(local_extracted_archive, path) if local_extracted_archive else path
                result["audio"] = {"path": path, "bytes": f.read()}
                result["path"] = path
                yield id_, result
                id_ += 1
```
