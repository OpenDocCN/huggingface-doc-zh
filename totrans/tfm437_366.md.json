["```py\nimport torch\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\ninput_points = [[[450, 600]]]  # 2D location of a window in the image\n\ninputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nmasks = processor.image_processor.post_process_masks(\n    outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n)\nscores = outputs.iou_scores\n```", "```py\nimport torch\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\nmask_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nsegmentation_map = Image.open(requests.get(mask_url, stream=True).raw).convert(\"RGB\")\ninput_points = [[[450, 600]]]  # 2D location of a window in the image\n\ninputs = processor(raw_image, input_points=input_points, segmentation_maps=mask, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nmasks = processor.image_processor.post_process_masks(\n    outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n)\nscores = outputs.iou_scores\n```", "```py\n( vision_config = None prompt_encoder_config = None mask_decoder_config = None initializer_range = 0.02 **kwargs )\n```", "```py\n>>> from transformers import (\n...     SamVisionConfig,\n...     SamPromptEncoderConfig,\n...     SamMaskDecoderConfig,\n...     SamModel,\n... )\n\n>>> # Initializing a SamConfig with `\"facebook/sam-vit-huge\"` style configuration\n>>> configuration = SamConfig()\n\n>>> # Initializing a SamModel (with random weights) from the `\"facebook/sam-vit-huge\"` style configuration\n>>> model = SamModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a SamConfig from a SamVisionConfig, SamPromptEncoderConfig, and SamMaskDecoderConfig\n\n>>> # Initializing SAM vision, SAM Q-Former and language model configurations\n>>> vision_config = SamVisionConfig()\n>>> prompt_encoder_config = SamPromptEncoderConfig()\n>>> mask_decoder_config = SamMaskDecoderConfig()\n\n>>> config = SamConfig(vision_config, prompt_encoder_config, mask_decoder_config)\n```", "```py\n( hidden_size = 768 output_channels = 256 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 1024 patch_size = 16 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True mlp_ratio = 4.0 use_abs_pos = True use_rel_pos = True window_size = 14 global_attn_indexes = [2, 5, 8, 11] num_pos_feats = 128 mlp_dim = None **kwargs )\n```", "```py\n( hidden_size = 256 hidden_act = 'relu' mlp_dim = 2048 num_hidden_layers = 2 num_attention_heads = 8 attention_downsample_rate = 2 num_multimask_outputs = 3 iou_head_depth = 3 iou_head_hidden_dim = 256 layer_norm_eps = 1e-06 **kwargs )\n```", "```py\n( hidden_size = 256 image_size = 1024 patch_size = 16 mask_input_channels = 16 num_point_embeddings = 4 hidden_act = 'gelu' layer_norm_eps = 1e-06 **kwargs )\n```", "```py\n( image_processor )\n```", "```py\n( do_resize: bool = True size: Dict = None mask_size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True pad_size: int = None mask_pad_size: int = None do_convert_rgb: bool = True **kwargs )\n```", "```py\n( masks iou_scores original_size cropped_box_image pred_iou_thresh = 0.88 stability_score_thresh = 0.95 mask_threshold = 0 stability_score_offset = 1 return_tensors = 'pt' )\n```", "```py\n( image target_size crop_n_layers: int = 0 overlap_ratio: float = 0.3413333333333333 points_per_crop: Optional = 32 crop_n_points_downscale_factor: Optional = 1 device: Optional = None input_data_format: Union = None return_tensors: str = 'pt' )\n```", "```py\n( image: ndarray pad_size: Dict data_format: Union = None input_data_format: Union = None **kwargs )\n```", "```py\n( all_masks all_scores all_boxes crops_nms_thresh return_tensors = 'pt' )\n```", "```py\n( masks original_sizes reshaped_input_sizes mask_threshold = 0.0 binarize = True pad_size = None return_tensors = 'pt' ) \u2192 export const metadata = 'undefined';(Union[torch.Tensor, tf.Tensor])\n```", "```py\n( images: Union segmentation_maps: Union = None do_resize: Optional = None size: Optional = None mask_size: Optional = None resample: Optional = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None pad_size: Optional = None mask_pad_size: Optional = None do_convert_rgb: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image: ndarray size: Dict resample: Resampling = <Resampling.BICUBIC: 3> data_format: Union = None input_data_format: Union = None **kwargs ) \u2192 export const metadata = 'undefined';np.ndarray\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None input_points: Optional = None input_labels: Optional = None input_boxes: Optional = None input_masks: Optional = None image_embeddings: Optional = None multimask_output: bool = True attention_similarity: Optional = None target_embedding: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs )\n```", "```py\n( config **kwargs )\n```", "```py\n( pixel_values: TFModelInputType | None = None input_points: tf.Tensor | None = None input_labels: tf.Tensor | None = None input_boxes: tf.Tensor | None = None input_masks: tf.Tensor | None = None image_embeddings: tf.Tensor | None = None multimask_output: bool = True output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False **kwargs )\n```"]