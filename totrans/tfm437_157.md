# DialoGPT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dialogpt)

## 概述

DialoGPT是由Yizhe Zhang、Siqi Sun、Michel Galley、Yen-Chun Chen、Chris Brockett、Xiang Gao、Jianfeng Gao、Jingjing Liu、Bill Dolan在[《DialoGPT:大规模生成式预训练用于对话回复生成》](https://arxiv.org/abs/1911.00536)中提出的。它是一个在Reddit上提取的147M对话式交流数据上训练的GPT2模型。

论文摘要如下：

*我们提出了一个大型、可调节的神经对话回复生成模型DialoGPT（对话生成式预训练变压器）。在Reddit评论链中提取的147M对话式交流数据上训练，跨越2005年至2017年，DialoGPT扩展了Hugging Face PyTorch变压器，以在单轮对话设置中实现接近人类的自动和人类评估性能。我们展示了利用DialoGPT的对话系统生成比强基线系统更相关、内容更丰富和上下文更一致的回复。预训练模型和训练流程已公开发布，以促进神经回复生成研究和更智能的开放领域对话系统的发展。*

原始代码可以在[这里](https://github.com/microsoft/DialoGPT)找到。

## 使用提示

+   DialoGPT是一个带有绝对位置嵌入的模型，因此通常建议在右侧填充输入而不是左侧。

+   DialoGPT在对话数据上使用因果语言建模（CLM）目标进行训练，因此在开放领域对话系统中的回复生成方面非常强大。

+   DialoGPT使用户可以仅用10行代码创建一个聊天机器人，如[DialoGPT的模型卡片](https://huggingface.co/microsoft/DialoGPT-medium)所示。

训练：

为了训练或微调DialoGPT，可以使用因果语言建模训练。引用官方论文中的话：*我们遵循OpenAI GPT-2，将多轮对话会话建模为长文本，并将生成任务构建为语言建模。我们首先将对话会话中的所有对话轮次连接成一个长文本x_1,…, x_N（N为序列长度），以结束文本标记结束。*更多信息请参考原始论文。

DialoGPT的架构基于GPT2模型，请参考[GPT2的文档页面](gpt2)获取API参考和示例。
