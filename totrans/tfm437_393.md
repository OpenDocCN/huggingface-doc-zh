# 分词器实用程序

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/tokenization_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/tokenization_utils)

此页面列出了分词器使用的所有实用函数，主要是实现 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 和 [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 之间的常见方法的类 [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase) 和混合 [SpecialTokensMixin](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin)。

大多数情况下，这些只有在研究库中的分词器代码时才有用。

## PreTrainedTokenizerBase

### `class transformers.PreTrainedTokenizerBase`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1543)

```py
( **kwargs )
```

参数

+   `model_max_length` (`int`, *可选*) — 输入到变换器模型的最大长度（以标记数计）。当使用 [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained) 加载分词器时，此值将设置为存储在 `max_model_input_sizes` 中关联模型的值（参见上文）。如果未提供值，将默认为 VERY_LARGE_INTEGER (`int(1e30)`).

+   `padding_side` (`str`, *可选*) — 模型应该应用填充的一侧。应在 ['right', 'left'] 中选择。默认值从同名类属性中选择。

+   `truncation_side` (`str`, *可选*) — 模型应该应用截断的一侧。应在 ['right', 'left'] 中选择。默认值从同名类属性中选择。

+   `chat_template` (`str`, *可选*) — 用于格式化聊天消息列表的 Jinja 模板字符串。查看 [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating) 获取完整描述。

+   `model_input_names` (`List[string]`, *可选*) — 模型前向传递接受的输入列表（如 `"token_type_ids"` 或 `"attention_mask"`）。默认值从同名类属性中选择。

+   `bos_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 代表句子开头的特殊标记。将与 `self.bos_token` 和 `self.bos_token_id` 相关联。

+   `eos_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 代表句子结尾的特殊标记。将与 `self.eos_token` 和 `self.eos_token_id` 相关联。

+   `unk_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 代表一个未知词的特殊标记。将与 `self.unk_token` 和 `self.unk_token_id` 相关联。

+   `sep_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 用于在同一输入中分隔两个不同句子的特殊标记（例如，BERT 使用）。将与 `self.sep_token` 和 `self.sep_token_id` 相关联。

+   `pad_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。将与 `self.pad_token` 和 `self.pad_token_id` 相关联。

+   `cls_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 代表输入类别的特殊标记（例如，BERT 使用）。将与 `self.cls_token` 和 `self.cls_token_id` 相关联。

+   `mask_token` (`str` 或 `tokenizers.AddedToken`, *可选*) — 代表一个被屏蔽的标记的特殊标记（用于掩码语言建模预训练目标，如 BERT）。将与 `self.mask_token` 和 `self.mask_token_id` 相关联。

+   `additional_special_tokens`（元组或`str`或`tokenizers.AddedToken`的列表，*可选*） — 附加特殊标记的元组或列表。在此处添加它们以确保在`skip_special_tokens`设置为True时解码时跳过它们。如果它们不是词汇的一部分，它们将被添加到词汇的末尾。

+   `clean_up_tokenization_spaces` (`bool`, *可选*，默认为`True`) — 模型是否应清除在分词过程中拆分输入文本时添加的空格。

+   `split_special_tokens` (`bool`, *可选*，默认为`False`) — 特殊标记是否应在分词过程中拆分。默认行为是不拆分特殊标记。这意味着如果`<s>`是`bos_token`，那么`tokenizer.tokenize("<s>") = ['<s>`]`。否则，如果`split_special_tokens=True`，那么`tokenizer.tokenize("<s>")`将给出`['<', 's', '>']`。目前仅支持`slow`分词器支持此参数。

[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)和[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的基类。

处理这两个类的共享（大部分是样板）方法。

类属性（由派生类覆盖）

+   `vocab_files_names` (`Dict[str, str]`) — 一个字典，其中键是模型所需的每个词汇文件的`__init__`关键字名称，关联值是用于保存关联文件的文件名（字符串）。

+   `pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) — 一个字典的字典，高级键是模型所需的每个词汇文件的`__init__`关键字名称，低级键是预训练模型的`short-cut-names`，关联值是关联预训练词汇文件的`url`。

+   `max_model_input_sizes` (`Dict[str, Optional[int]]`) — 一个字典，其中键是预训练模型的`short-cut-names`，关联值是该模型的序列输入的最大长度，如果模型没有最大输入大小，则为`None`。

+   `pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) — 预训练模型的`short-cut-names`作为键，关联值是一个字典，其中包含加载预训练模型时传递给tokenizer类的`__init__`方法的特定参数。

+   `model_input_names` (`List[str]`) — 模型前向传递中预期的输入列表。

+   `padding_side` (`str`) — 模型应该应用填充的默认位置。应为`'right'`或`'left'`。

+   `truncation_side` (`str`) — 模型应该应用截断的默认位置。应为`'right'`或`'left'`。

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预先分词的字符串）。如果提供的序列是字符串列表（预先分词的），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码的序列或批量序列。每个序列可以是字符串或字符串列表（预先分词的字符串）。如果提供的序列是字符串列表（预先分词的），必须设置`is_split_into_words=True`（以消除与批量序列的歧义）。

+   `text_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `text_pair_target`（`str`，`List[str]`，`List[List[str]]`，*可选*）— 要编码为目标文本的序列或批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果将序列提供为字符串列表（预分词），必须设置`is_split_into_words=True`（以消除与批次序列的歧义）。

+   `add_special_tokens`（`bool`，*可选*，默认为`True`）— 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）— 激活和控制填充。接受以下值：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者使用模型的最大可接受输入长度（如果未提供该参数）。

    +   `False`或`'do_not_pad'`（默认）：无填充（即，可以输出长度不同的序列批次）。

+   `truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`）— 激活和控制截断。接受以下值：

    +   `True`或`'longest_first'`：使用参数`max_length`指定的最大长度进行截断，或者使用模型的最大可接受输入长度（如果未提供该参数）。这将逐标记截断，如果提供了一对序列（或一批对序列），则从较长序列中删除一个标记。

    +   `'only_first'`: 使用参数`max_length`指定的最大长度进行截断，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第一个序列。

    +   `'only_second'`: 使用参数`max_length`指定的最大长度进行截断，或者使用模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：无截断（即，可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length`（`int`，*可选*）— 由截断/填充参数之一使用的最大长度。

    如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数中需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为0）— 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *optional*, defaults to `False`) — 输入是否已经预分词化（例如，已经分成单词）。如果设置为`True`，分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或令牌分类很有用。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将填充序列到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor Cores的使用特别有用。

+   `return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *optional*) — 是否返回令牌类型ID。如果保持默认设置，将根据特定分词器的默认值返回令牌类型ID，由`return_outputs`属性定义。

    [什么是token类型ID？](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — 是否返回溢出的令牌序列。如果提供了一对输入ID序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的令牌。

+   `return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) — 是否返回特殊令牌掩码信息。

+   `return_offsets_mapping` (`bool`, *optional*, defaults to `False`) — 是否返回每个令牌的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`, *optional*, defaults to `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *optional*, defaults to `True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要馈送给模型的令牌ID列表。

    [什么是输入ID？](../glossary#input-ids)

+   `token_type_ids` — 要馈送给模型的令牌类型ID列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    [什么是token类型ID？](../glossary#token-type-ids)

+   `attention_mask` — 指定模型应该关注的令牌的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `overflowing_tokens` — 溢出的令牌序列列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 被截断的令牌数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 0和1的列表，其中1指定添加的特殊令牌，0指定常规序列令牌（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

标记化和为模型准备一个或多个序列或一个或多个序列对的主要方法。

#### `apply_chat_template`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)

```py
( conversation: Union chat_template: Optional = None add_generation_prompt: bool = False tokenize: bool = True padding: bool = False truncation: bool = False max_length: Optional = None return_tensors: Union = None **tokenizer_kwargs ) → export const metadata = 'undefined';List[int]
```

参数

+   `conversation`（Union[List[Dict[str, str]], “Conversation”） — 一个Conversation对象或包含“role”和“content”键的字典列表，表示到目前为止的聊天历史。

+   `chat_template`（str，*可选*） — 用于此转换的Jinja模板。如果未传递此参数，则将使用模型的默认聊天模板。

+   `add_generation_prompt`（bool，*可选*） — 是否以指示助手消息开始的标记结束提示。当您想要从模型生成响应时，这很有用。请注意，此参数将传递给聊天模板，因此必须在模板中支持此参数才能产生任何效果。

+   `tokenize`（`bool`，默认为`True`） — 是否对输出进行标记化。如果为`False`，输出将是一个字符串。

+   `padding`（`bool`，默认为`False`） — 是否将序列填充到最大长度。如果tokenize为`False`，则无效。

+   `truncation`（`bool`，默认为`False`） — 是否在最大长度处截断序列。如果tokenize为`False`，则无效。

+   `max_length`（`int`，*可选*） — 用于填充或截断的最大长度（以标记为单位）。如果未指定，则将使用分词器的`max_length`属性作为默认值。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*） — 如果设置，将返回特定框架的张量。如果tokenize为`False`，则无效。可接受的值为：

    +   `'tf'`：返回TensorFlow `tf.Tensor`对象。

    +   `'pt'`：返回PyTorch `torch.Tensor`对象。

    +   `'np'`：返回NumPy `np.ndarray`对象。

    +   `'jax'`：返回JAX `jnp.ndarray`对象。**tokenizer_kwargs — 传递给分词器的额外kwargs。

返回

`List[int]`

表示到目前为止标记化聊天的标记id列表，包括控制标记。此输出已准备好直接传递给模型，或通过`generate()`等方法传递。

将Conversation对象或包含`"role"`和`"content"`键的字典列表转换为标记id列表。此方法旨在与聊天模型一起使用，并将读取分词器的chat_template属性以确定在转换时要使用的格式和控制标记。当chat_template为None时，将退回到在类级别指定的default_chat_template。

#### `as_target_tokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3860)

```py
( )
```

临时设置用于编码目标的分词器。对于需要为标签进行稍微不同处理的序列到序列模型相关的分词器非常有用。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`) — 标记化输入id的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`） — 是否删除解码中的特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*） — 是否清除分词空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（额外的关键字参数，*可选*） — 将传递给底层模型特定的解码方法。

返回

`List[str]`

解码的句子列表。

将标记id列表的列表转换为字符串列表，通过调用解码。

#### `batch_encode_plus`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3026)

```py
( batch_text_or_text_pairs: Union add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `batch_text_or_text_pairs`（`List[str]`，`List[Tuple[str, str]]`，`List[List[str]]`，`List[Tuple[List[str], List[str]]`，以及对于非快速分词器，还有`List[List[int]]`，`List[Tuple[List[int], List[int]]`） - 要编码的序列或序列对批次。这可以是字符串/字符串序列/整数序列列表或字符串/字符串序列/整数序列对列表（请参阅`encode_plus`中的详细信息）。

+   `add_special_tokens`（`bool`，*可选*，默认为`True`） - 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`） - 激活和控制填充。接受以下值：

    +   `True`或`'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`: 使用参数`max_length`指定的最大长度进行填充，或者如果未提供该参数，则使用模型的最大可接受输入长度。

    +   `False`或`'do_not_pad'`（默认）：不进行填充（即，可以输出长度不同的序列批次）。

+   `truncation`（`bool`，`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为`False`） - 激活和控制截断。接受以下值：

    +   `True`或`'longest_first'`: 使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则使用模型的最大可接受输入长度。这将逐个标记地截断，如果提供了一对序列（或一批序列），则从该对中最长的序列中删除一个标记。

    +   `'only_first'`：使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则使用模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则仅截断第一个序列。

    +   `'only_second'`：使用参数`max_length`指定的最大长度进行截断，或者如果未提供该参数，则使用模型的最大可接受输入长度。如果提供了一对序列（或一批序列），则仅截断第二个序列。

    +   `False`或`'do_not_truncate'`（默认）：不进行截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length`（`int`，*可选*） - 控制截断/填充参数使用的最大长度。

    如果未设置或设置为`None`，则如果截断/填充参数中的一个需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将停用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为0） - 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。此参数的值定义了重叠标记的数量。

+   `is_split_into_words`（`bool`，*可选*，默认为`False`） - 输入是否已经预分词（例如，已经分成单词）。如果设置为`True`，则分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。

+   `pad_to_multiple_of`（`int`，*可选*） — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*） — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`：返回TensorFlow `tf.constant`对象。

    +   `'pt'`：返回PyTorch `torch.Tensor`对象。

    +   `'np'`：返回Numpy `np.ndarray`对象。

+   `return_token_type_ids`（`bool`，*可选*） — 是否返回标记类型ID。如果保持默认设置，将根据特定分词器的默认值返回标记类型ID，由`return_outputs`属性定义。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `return_attention_mask`（`bool`，*可选*） — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。

    [什么是attention masks？](../glossary#attention-mask)

+   `return_overflowing_tokens`（`bool`，*可选*，默认为`False`） — 是否返回溢出的标记序列。如果提供了一对输入id序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的标记。

+   `return_special_tokens_mask`（`bool`，*可选*，默认为`False`） — 是否返回特殊标记掩码信息。

+   `return_offsets_mapping`（`bool`，*可选*，默认为`False`） — 是否返回每个标记的`(char_start, char_end)`。

    仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length`（`bool`，*可选*，默认为`False`） — 是否返回编码输入的长度。

+   `verbose`（`bool`，*可选*，默认为`True`） — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的标记id列表。

    [什么是输入ID？](../glossary#input-ids)

+   `token_type_ids` — 要提供给模型的标记类型id列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    [什么是attention masks？](../glossary#attention-mask)

+   `overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`和`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的标记数量（当指定`max_length`和`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 0和1的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

对一组序列或一组序列对进行标记化和准备模型。

此方法已弃用，应改用`__call__`。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3322)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`） — 第一个标记化序列。

+   `token_ids_1`（`List[int]`，*可选*）—第二个标记化序列。

返回

`List[int]`

带有特殊标记的模型输入。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。

此实现不添加特殊标记，应该在子类中重写此方法。

#### `clean_up_tokenization`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3803)

```py
( out_string: str ) → export const metadata = 'undefined';str
```

参数

+   `out_string`（`str`）—要清理的文本。

返回

`str`

清理后的字符串。

清理一系列简单的英语分词工件，如标点符号前的空格和缩写形式。

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3679)

```py
( tokens: List ) → export const metadata = 'undefined';str
```

参数

+   `tokens`（`List[str]`）—要连接的标记。

返回

`str`

连接的标记。

将一系列标记转换为单个字符串。最简单的方法是`" ".join(tokens)`，但我们经常希望同时删除子词标记化工件。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0`（`List[int]`）—第一个标记化序列。

+   `token_ids_1`（`List[int]`，*可选*）—第二个标记化序列。

返回

`List[int]`

标记类型ID。

创建与传递的序列相对应的标记类型ID。[什么是标记类型ID？](../glossary#token-type-ids)

如果模型有特殊的构建方式，则应在子类中重写。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) → export const metadata = 'undefined';str
```

参数

+   `token_ids`（`Union[int]`，`List[int]`，`np.ndarray`，`torch.Tensor`，`tf.Tensor`）—标记化输入ID的列表。可以使用`__call__`方法获得。

+   `skip_special_tokens`（`bool`，*可选*，默认为`False`）—是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces`（`bool`，*可选*）—是否清理分词空格。如果为`None`，将默认为`self.clean_up_tokenization_spaces`。

+   `kwargs`（额外的关键字参数，*可选*）—将传递给底层模型特定的解码方法。

返回

`str`

解码后的句子。

将ID序列转换为字符串，使用分词器和词汇表，并提供选项以删除特殊标记和清理分词空格。

类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 return_tensors: Union = None **kwargs ) → export const metadata = 'undefined';List[int], torch.Tensor, tf.Tensor or np.ndarray
```

参数

+   `text`（`str`，`List[str]`或`List[int]`）—要编码的第一个序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法标记化的字符串ID）。

+   `text_pair`（`str`，`List[str]`或`List[int]`，*可选*）—要编码的可选第二序列。这可以是一个字符串，一个字符串列表（使用`tokenize`方法标记化的字符串）或一个整数列表（使用`convert_tokens_to_ids`方法标记化的字符串ID）。

+   `add_special_tokens`（`bool`，*可选*，默认为`True`）—在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了哪些标记会自动添加到输入ID中。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）—激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即可以输出具有不同长度的序列的批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, 默认为 `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则将逐标记截断，从序列对中最长的序列中删除一个标记。

    +   `'only_first'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第一个序列。

    +   `'only_second'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列对），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *optional*, defaults to 0) — 如果设置为一个数字，并且与 `max_length` 一起使用，当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *optional*, 默认为 `False`) — 输入是否已经预分词化（例如，已经分成单词）。如果设置为 `True`，则分词器假定输入已经分成单词（例如，通过在空格上分割），它将对其进行分词。这对于 NER 或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。需要激活 `padding`。这对于启用具有计算能力 `>= 7.5`（Volta）的 NVIDIA 硬件上的 Tensor Cores 特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

    **kwargs — 传递给 `.tokenize()` 方法。

返回

`List[int]`, `torch.Tensor`, `tf.Tensor` 或 `np.ndarray`

文本的标记化 id。

将字符串转换为 id（整数）序列，使用分词器和词汇表。

与执行 `self.convert_tokens_to_ids(self.tokenize(text))` 相同。

#### `encode_plus`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2930)

```py
( text: Union text_pair: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]` 或 `List[int]`（仅用于非快速分词器）） — 要编码的第一个序列。可以是一个字符串，一个字符串列表（使用 `tokenize` 方法进行分词），或一个整数列表（使用 `convert_tokens_to_ids` 方法进行分词）。

+   `text_pair` (`str`, `List[str]` 或 `List[int]`, *可选*) — 要编码的可选第二个序列。可以是一个字符串，一个字符串列表（使用 `tokenize` 方法进行分词），或一个整数列表（使用 `convert_tokens_to_ids` 方法进行分词）。

+   `add_special_tokens` (`bool`, *可选*, 默认为 `True`) — 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens` 函数，该函数定义了自动添加到输入标识的标记。如果要自动添加 `bos` 或 `eos` 标记，则这很有用。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定的最大长度（使用参数 `max_length`）或模型可接受的最大输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不进行填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度（使用参数 `max_length`）或模型可接受的最大输入长度（如果未提供该参数）。这将逐个标记进行截断，如果提供了一对序列（或一批对序列），则从最长序列中删除一个标记。

    +   `'only_first'`：截断到指定的最大长度（使用参数 `max_length`）或模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度（使用参数 `max_length`）或模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不进行截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length` (`int`, *可选*) — 由截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *可选*, 默认为 0) — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *optional*, 默认为`False`) — 输入是否已经预分词化（例如，已分割为单词）。如果设置为`True`，分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *optional*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。

+   `return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *optional*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *optional*) — 是否返回标记类型ID。如果保持默认设置，将根据特定分词器的默认值返回标记类型ID，由`return_outputs`属性定义。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *optional*) — 是否返回注意力掩码。如果保持默认设置，将根据特定分词器的默认值返回注意力掩码，由`return_outputs`属性定义。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) — 是否返回溢出的标记序列。如果提供一对输入id序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误而不是返回溢出的标记。

+   `return_special_tokens_mask` (`bool`, *optional*, 默认为`False`) — 是否返回特殊标记掩码信息。

+   `return_offsets_mapping` (`bool`, *optional*, 默认为`False`) — 是否返回每个标记的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`, *optional*, 默认为`False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *optional*, 默认为`True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的标记id列表。

    [什么是输入ID？](../glossary#input-ids)

+   `token_type_ids` — 要提供给模型的标记类型id列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些标记应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `overflowing_tokens` — 溢出标记序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 截断的标记数（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊标记，0指定常规序列标记（当`add_special_tokens=True`和`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

对一个序列或一对序列进行标记化和准备模型。

此方法已弃用，应改用 `__call__`。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1803)

```py
( pretrained_model_name_or_path: Union *init_inputs cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` 或 `os.PathLike`) — 可以是以下之一：

    +   一个字符串，预定义tokenizer的*模型id*，托管在huggingface.co模型仓库中。有效的模型id可以位于根级别，如 `bert-base-uncased`，也可以位于用户或组织名称下，如 `dbmdz/bert-base-german-cased`。

    +   一个*目录*的路径，其中包含了tokenizer所需的词汇文件，例如使用[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)方法保存的文件，例如 `./my_model_directory/`。

    +   （已弃用，不适用于所有派生类）一个单个保存的词汇文件的路径或url（仅当tokenizer只需要一个词汇文件时，如Bert或XLNet），例如 `./my_model_directory/vocab.txt`。

+   `cache_dir` (`str` 或 `os.PathLike`, *optional*) — 下载的预定义tokenizer词汇文件应该缓存在其中的目录路径，如果不应使用标准缓存。

+   `force_download` (`bool`, *optional*, 默认为 `False`) — 是否强制（重新）下载词汇文件并覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，则尝试恢复下载。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个按协议或端点使用的代理服务器字典，例如 `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。这些代理将在每个请求中使用。

+   `token` (`str` 或 *bool*, *optional*) — 用作远程文件HTTP bearer授权的token。如果为 `True`，将使用运行 `huggingface-cli login` 时生成的token（存储在 `~/.huggingface` 中）。

+   `local_files_only` (`bool`, *optional*, 默认为 `False`) — 是否仅依赖本地文件，不尝试下载任何文件。

+   `revision` (`str`, *optional*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交ID，因为我们在huggingface.co上使用基于git的系统来存储模型和其他工件，所以 `revision` 可以是git允许的任何标识符。

+   `subfolder` (`str`, *optional*) — 如果相关文件位于huggingface.co模型仓库的子文件夹中（例如facebook/rag-token-base），请在此处指定。

+   `inputs`（额外的位置参数，*optional*） — 将传递给Tokenizer `__init__`方法。

+   `kwargs`（额外的关键字参数，*optional*） — 将传递给Tokenizer `__init__`方法。可以用于设置特殊token，如 `bos_token`、`eos_token`、`unk_token`、`sep_token`、`pad_token`、`cls_token`、`mask_token`、`additional_special_tokens`。有关更多详细信息，请参阅`__init__`中的参数。

从预定义tokenizer实例化一个[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)（或派生类）。

当您想使用私有模型时，需要传递 `token=True`。

示例：

```py
# We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer
# Download vocabulary from huggingface.co and cache.
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = BertTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = BertTokenizer.from_pretrained("./test/saved_model/")

# If the tokenizer uses a single vocabulary file, you can point directly to this file
tokenizer = BertTokenizer.from_pretrained("./test/saved_model/my_vocab.txt")

# You can link tokens to special vocabulary when instantiating
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased", unk_token="<unk>")
# You should be sure '<unk>' is in the vocabulary when doing that.
# Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)
assert tokenizer.unk_token == "<unk>"
```

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';A list of integers in the range [0, 1]
```

参数

+   `token_ids_0` (`List[int]`) — 第一个序列的id列表。

+   `token_ids_1` (`List[int]`, *optional*) — 第二个序列的id列表。

+   `already_has_special_tokens` (`bool`, *optional*, 默认为 `False`) — token列表是否已经包含了模型所需的特殊token。

返回值

一个整数列表，范围为 [0, 1]

1 代表特殊token，0 代表序列token。

从没有添加特殊标记的标记列表中检索序列id。当使用tokenizer的`prepare_for_model`或`encode_plus`方法添加特殊标记时，会调用此方法。

#### `get_vocab`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1666)

```py
( ) → export const metadata = 'undefined';Dict[str, int]
```

返回

`Dict[str, int]`

词汇表。

将词汇表作为标记到索引的字典返回。

当`token`在词汇表中时，`tokenizer.get_vocab()[token]`等同于`tokenizer.convert_tokens_to_ids(token)`。

#### `pad`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3129)

```py
( encoded_inputs: Union padding: Union = True max_length: Optional = None pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None verbose: bool = True )
```

参数

+   `encoded_inputs`（[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)，[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)列表，`Dict[str, List[int]]`，`Dict[str, List[List[int]]`或`List[Dict[str, List[int]]]`） — 标记化输入。可以表示一个输入（[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)或`Dict[str, List[int]]`）或一批标记化输入（[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)列表，*Dict[str, List[List[int]]]*或*List[Dict[str, List[int]]]*)，因此您可以在预处理期间以及在PyTorch Dataloader收集函数中使用此方法。

    你可以使用张量（numpy数组，PyTorch张量或TensorFlow张量）代替`List[int]`，请参考上面的返回类型说明。

+   `padding`（`bool`，`str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`True`） — 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True`或`'longest'`：填充到批量中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`: 填充到指定的最大长度，使用参数`max_length`或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False`或`'do_not_pad'`（默认）：无填充（即可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*） — 返回列表的最大长度和可选的填充长度（参见上文）。

+   `pad_to_multiple_of`（`int`，*可选*） — 如果设置，将填充序列到提供的值的倍数。

    这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor Cores特别有用。

+   `return_attention_mask`（`bool`，*可选*） — 是否返回注意力蒙版。如果保持默认值，将根据特定tokenizer的默认值返回注意力蒙版，由`return_outputs`属性定义。

    [注意力蒙版是什么？](../glossary#attention-mask)

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*） — 如果设置，将返回张量而不是Python整数列表。可接受的值包括：

    +   `'tf'`：返回TensorFlow `tf.constant`对象。

    +   `'pt'`：返回PyTorch `torch.Tensor`对象。

    +   `'np'`：返回Numpy `np.ndarray`对象。

+   `verbose`（`bool`，*可选*，默认为`True`） — 是否打印更多信息和警告。

将单个编码输入或批量编码输入填充到预定义长度或批量中的最大序列长度。

填充方向（左/右）填充标记id在tokenizer级别定义（使用`self.padding_side`，`self.pad_token_id`和`self.pad_token_type_id`）。

请注意，使用快速tokenizer时，使用`__call__`方法比使用编码文本的方法再调用`pad`方法更快。

如果传递的 `encoded_inputs` 是 numpy 数组、PyTorch 张量或 TensorFlow 张量的字典，则结果将使用相同的类型，除非您使用 `return_tensors` 提供不同的张量类型。对于 PyTorch 张量，您将丢失张量的特定设备。

#### `prepare_for_model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3342)

```py
( ids: List pair_ids: Optional = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True prepend_batch_axis: bool = False **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `ids`（`List[int]`） — 第一个序列的标记化输入 id。可以通过链接 `tokenize` 和 `convert_tokens_to_ids` 方法从字符串中获取。

+   `pair_ids`（`List[int]`，*可选*） — 第二个序列的标记化输入 id。可以通过链接 `tokenize` 和 `convert_tokens_to_ids` 方法从字符串中获取。

+   `add_special_tokens`（`bool`，*可选*，默认为 `True`） — 在编码序列时是否添加特殊标记。这将使用底层的 `PretrainedTokenizerBase.build_inputs_with_special_tokens` 函数，该函数定义了自动添加到输入 id 的标记。如果要自动添加 `bos` 或 `eos` 标记，这很有用。

+   `padding`（`bool`、`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `False`） — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度，该长度由参数 `max_length` 指定，或者填充到模型可接受的最大输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度的序列批次）。

+   `truncation`（`bool`、`str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)，*可选*，默认为 `False`） — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第一个序列。

    +   `'only_second'`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者截断到模型可接受的最大输入长度（如果未提供该参数）。如果提供了一对序列（或一批对序列），则只会截断第二个序列。 

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的批次）。

+   `max_length`（`int`，*可选*） — 由截断/填充参数之一使用的最大长度。

    如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数中需要最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `stride`（`int`，*可选*，默认为 0） — 如果与 `max_length` 一起设置为一个数字，则当 `return_overflowing_tokens=True` 时返回的溢出标记将包含截断序列末尾的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *可选*, 默认为 `False`) — 输入是否已经预分词（例如，已经分成单词）。如果设置为`True`，分词器会假定输入已经分成单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *可选*) — 是否返回token type IDs。如果保持默认设置，将根据特定分词器的默认设置返回token type IDs，由`return_outputs`属性定义。

    什么是token type IDs？

+   `return_attention_mask` (`bool`, *可选*) — 是否返回attention mask。如果保持默认设置，将根据特定分词器的默认设置返回attention mask，由`return_outputs`属性定义。

    什么是attention masks？

+   `return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的token序列。如果提供一对输入IDs序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误而不是返回溢出的tokens。

+   `return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊token的mask信息。

+   `return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个token的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速分词器，如果使用Python的分词器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`, *可选*, 默认为 `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

一个带有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的token ids列表。

    什么是输入IDs？

+   `token_type_ids` — 要提供给模型的token type ids列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中）。

    什么是token type IDs？

+   `attention_mask` — 指定哪些token应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中）。

    什么是attention masks？

+   `overflowing_tokens` — 溢出token序列的列表（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 被截断的token数量（当指定`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊token，0指定常规序列token（当`add_special_tokens=True`并且`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

准备输入 id 的序列，或一对输入 id 的序列，以便模型使用。它添加特殊标记，如果溢出则截断序列，同时考虑特殊标记，并管理一个移动窗口（带有用户定义的步幅）以处理溢出的标记。请注意，对于 *pair_ids* 不等于 `None` 且 *truncation_strategy = longest_first* 或 `True`，不可能返回溢出的标记。这样的参数组合将引发错误。

#### `prepare_seq2seq_batch`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3903)

```py
( src_texts: List tgt_texts: Optional = None max_length: Optional = None max_target_length: Optional = None padding: str = 'longest' return_tensors: str = None truncation: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `src_texts` (`List[str]`) — 要总结的文档或源语言文本的列表。

+   `tgt_texts` (`list`, *可选*) — 摘要或目标语言文本的列表。

+   `max_length` (`int`, *可选*) — 控制编码器输入（要总结的文档或源语言文本）的最大长度。如果未设置或设置为 `None`，并且截断/填充参数中需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。

+   `max_target_length` (`int`, *可选*) — 控制解码器输入（目标语言文本或摘要）的最大长度。如果未设置或设置为 `None`，将使用 max_length 值。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`: 填充到批次中最长的序列（如果只提供了单个序列，则不填充）。

    +   `'max_length'`: 填充到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则填充到模型可接受的最大输入长度。

    +   `False` 或 `'do_not_pad'` (默认): 不填充（即，可以输出长度不同的序列批次）。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *可选*, 默认为 `True`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批对），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批对），则仅截断第一个序列。

    +   `'only_second'`: 截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批对），则仅截断第二个序列。

    +   `False` 或 `'do_not_truncate'` (默认): 不截断（即，可以输出长度大于模型最大可接受输入大小的序列）。**kwargs — 传递给 `self.__call__` 的额外关键字参数。

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要馈送给编码器的标记id列表。

+   `attention_mask` — 指定模型应关注的哪些标记的索引列表。

+   `labels` — tgt_texts的标记id列表。

仅当传递了tgt_texts时，将返回完整的键集`[input_ids, attention_mask, labels]`。否则，`input_ids`，`attention_mask`将是唯一的键。

为翻译准备模型输入。为了获得最佳性能，请一次翻译一句话。

#### `push_to_hub`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)

```py
( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )
```

参数

+   `repo_id`（`str`）— 要将分词器推送到的存储库的名称。在推送到给定组织时，它应包含您的组织名称。

+   `use_temp_dir`（`bool`，*可选*）— 是否使用临时目录存储保存之前的文件，然后将它们推送到Hub。如果没有名为`repo_id`的目录，则默认为`True`，否则为`False`。

+   `commit_message`（`str`，*可选*）— 推送时要提交的消息。默认为`"Upload tokenizer"`。

+   `private`（`bool`，*可选*）— 创建的存储库是否应为私有。

+   `token`（`bool`或`str`，*可选*）— 用作远程文件HTTP令牌的令牌。如果为`True`，将使用运行`huggingface-cli login`时生成的令牌（存储在`~/.huggingface`中）。如果未指定`repo_url`，则默认为`True`。

+   `max_shard_size`（`int`或`str`，*可选*，默认为`"5GB"`）— 仅适用于模型。在被分片之前的检查点的最大大小。然后，检查点将分片为每个大小低于此大小的大小。如果表示为字符串，需要是数字后跟一个单位（如`"5MB"`）。我们将其默认设置为`"5GB"`，以便用户可以在免费的Google Colab实例上轻松加载模型，而不会出现CPU OOM问题。

+   `create_pr`（`bool`，*可选*，默认为`False`）— 是否创建具有上传文件的PR或直接提交。

+   `safe_serialization`（`bool`，*可选*，默认为`True`）— 是否将模型权重转换为safetensors格式以进行更安全的序列化。

+   `revision`（`str`，*可选*）— 要将上传的文件推送到的分支。

+   `commit_description`（`str`，*可选*）— 将创建的提交的描述

+   `tags`（`List[str]`，*可选*）— 要推送到Hub的标签列表。

将分词器文件上传到🤗模型Hub。

示例：

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Push the tokenizer to your namespace with the name "my-finetuned-bert".
tokenizer.push_to_hub("my-finetuned-bert")

# Push the tokenizer to an organization with the name "my-finetuned-bert".
tokenizer.push_to_hub("huggingface/my-finetuned-bert")
```

#### `register_for_auto_class`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3877)

```py
( auto_class = 'AutoTokenizer' )
```

参数

+   `auto_class`（`str`或`type`，*可选*，默认为`"AutoTokenizer"`）— 要将此新分词器注册到的自动类。

使用给定的自动类注册此类。这应仅用于自定义分词器，因为库中的分词器已经与`AutoTokenizer`映射。

此API是实验性的，可能在下一个版本中有一些轻微的破坏性更改。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2302)

```py
( save_directory: Union legacy_format: Optional = None filename_prefix: Optional = None push_to_hub: bool = False **kwargs ) → export const metadata = 'undefined';A tuple of str
```

参数

+   `save_directory`（`str`或`os.PathLike`）— 分词器将被保存的目录路径。

+   `legacy_format`（`bool`，*可选*）— 仅适用于快速分词器。如果未设置（默认），将以统一的JSON格式保存分词器，以及以传统格式保存（如果存在），即具有特定于分词器的词汇表和单独的added_tokens文件。

    如果为`False`，则只会以统一的JSON格式保存分词器。该格式与“慢速”分词器（不由*tokenizers*库提供支持）不兼容，因此无法在相应的“慢速”分词器中加载分词器。

    如果为`True`，将以传统格式保存分词器。如果“slow”分词器不存在，则会引发值错误。

+   `filename_prefix` (`str`, *可选*) — 要添加到分词器保存的文件名称前缀。

+   `push_to_hub` (`bool`, *可选*, 默认为`False`) — 保存后是否将模型推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`名称）。

+   `kwargs` (`Dict[str, Any]`, *可选*) — 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。

返回

一个`str`元组

保存的文件。

保存完整的分词器状态。

此方法确保完整的分词器可以使用`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`类方法重新加载。

警告，无。这不会保存您在实例化后对分词器应用的修改（例如，在创建后修改`tokenizer.do_lower_case`）。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2499)

```py
( save_directory: str filename_prefix: Optional = None ) → export const metadata = 'undefined';Tuple(str)
```

参数

+   `save_directory` (`str`) — 保存词汇表的目录。

+   `filename_prefix` (`str`, *可选*) — 要添加到保存文件名称的可选前缀。

返回

`Tuple(str)`

保存的文件路径。

仅保存分词器的词汇表（词汇表+添加的标记）。

此方法不会保存分词器的配置和特殊标记映射。使用`_save_pretrained()`保存分词器的整个状态。

#### `tokenize`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2517)

```py
( text: str pair: Optional = None add_special_tokens: bool = False **kwargs ) → export const metadata = 'undefined';List[str]
```

参数

+   `text` (`str`) — 要编码的序列。

+   `pair` (`str`, *可选*) — 要与第一个序列一起编码的第二个序列。

+   `add_special_tokens` (`bool`, *可选*, 默认为`False`) — 是否添加与相应模型相关的特殊标记。

+   `kwargs`（额外关键字参数，*可选*） — 将传递给底层模型特定的编码方法。详细信息请参见[`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)

返回

`List[str]`

标记列表。

将字符串转换为标记序列，用`unk_token`替换未知标记。

#### `truncate_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3478)

```py
( ids: List pair_ids: Optional = None num_tokens_to_remove: int = 0 truncation_strategy: Union = 'longest_first' stride: int = 0 ) → export const metadata = 'undefined';Tuple[List[int], List[int], List[int]]
```

参数

+   `ids` (`List[int]`) — 第一个序列的标记化输入id。可以通过链接`tokenize`和`convert_tokens_to_ids`方法从字符串中获取。

+   `pair_ids` (`List[int]`, *可选*) — 第二个序列的标记化输入id。可以通过链接`tokenize`和`convert_tokens_to_ids`方法从字符串中获取。

+   `num_tokens_to_remove` (`int`, *可选*, 默认为0) — 使用截断策略要移除的标记数。

+   `truncation_strategy` (`str`或[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *可选*, 默认为`False`) — 截断的策略。可以是：

    +   `'longest_first'`: 截断到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。这将逐个标记截断，如果提供了一对序列（或一批序列），则从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`: 截断到指定的最大长度（使用参数`max_length`）或模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批序列），则仅截断第一个序列。

    +   `'only_second'`：根据参数`max_length`指定的最大长度截断，或者根据模型的最大可接受输入长度截断。如果未提供该参数，则仅截断一对序列中的第二个序列（或一批序列对）。

    +   `'do_not_truncate'`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的批次）。

+   `stride`（`int`，*可选*，默认为0）— 如果设置为正数，返回的溢出标记将包含来自返回的主序列的一些标记。该参数的值定义了额外标记的数量。

返回

`Tuple[List[int], List[int], List[int]]`

被截断的`ids`，被截断的`pair_ids`和溢出标记的列表。注意：如果提供了一对序列（或一批序列对），则*longest_first*策略返回空的溢出标记列表。

根据策略就地截断一个序列对。

## SpecialTokensMixin

### `class transformers.SpecialTokensMixin`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L795)

```py
( verbose = False **kwargs )
```

参数

+   `bos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表句子开头的特殊标记。

+   `eos_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表句子结尾的特殊标记。

+   `unk_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表一个未知词的特殊标记。

+   `sep_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表同一输入中两个不同句子之间的特殊标记（例如BERT使用）。

+   `pad_token`（`str`或`tokenizers.AddedToken`，*可选*）— 用于使标记数组大小相同以进行批处理的特殊标记。然后将被注意机制或损失计算忽略。

+   `cls_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表输入类别的特殊标记（例如BERT使用）。

+   `mask_token`（`str`或`tokenizers.AddedToken`，*可选*）— 代表被屏蔽的标记的特殊标记（例如BERT使用）。

+   `additional_special_tokens`（元组或`str`或`tokenizers.AddedToken`的列表，*可选*）— 一组额外的标记，将被标记为`special`，这意味着如果`skip_special_tokens`设置为`True`，在解码时将被跳过。

由[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)和[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)派生的mixin，用于处理与特殊标记相关的特定行为。特别是，这个类保存了可以用于以与模型无关的方式直接访问这些特殊标记的属性，并允许设置和更新特殊标记。

#### `add_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)

```py
( special_tokens_dict: Dict replace_additional_special_tokens = True ) → export const metadata = 'undefined';int
```

参数

+   `special_tokens_dict`（字典*str*到*str*或`tokenizers.AddedToken`）— 键应该在预定义特殊属性列表中：[`bos_token`、`eos_token`、`unk_token`、`sep_token`、`pad_token`、`cls_token`、`mask_token`、`additional_special_tokens`]。

    仅当它们不在词汇表中时才会添加标记（通过检查分词器是否将`unk_token`的索引分配给它们进行测试）。

+   `replace_additional_special_tokens`（`bool`，*可选*，默认为`True`） - 如果为`True`，则现有的额外特殊标记列表将被`special_tokens_dict`中提供的列表替换。否则，`self._additional_special_tokens`只是被扩展。在前一种情况下，这些标记不会从标记器的完整词汇表中删除 - 它们只被标记为非特殊标记。请记住，这只影响解码时跳过哪些标记，而不影响`added_tokens_encoder`和`added_tokens_decoder`。这意味着以前的`additional_special_tokens`仍然是添加的标记，并且不会被模型分割。

返回

`int`

将特殊标记添加到词汇表中的数量。

将特殊标记的字典（eos、pad、cls等）添加到编码器中，并将它们链接到类属性。如果特殊标记不在词汇表中，则将它们添加到词汇表中（从当前词汇表的最后索引开始索引）。

在向词汇表中添加新标记时，您应确保还调整模型的标记嵌入矩阵，以使其嵌入矩阵与标记器匹配。

为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。

使用`add_special_tokens`将确保您的特殊标记可以以多种方式使用：

+   在解码时可以跳过特殊标记，使用`skip_special_tokens = True`。

+   特殊标记由标记器仔细处理（它们永远不会被分割），类似于`AddedTokens`。

+   您可以使用标记器类属性轻松引用特殊标记，如`tokenizer.cls_token`。这使得开发与模型无关的训练和微调脚本变得容易。

在可能的情况下，为提供的预训练模型已经注册了特殊标记（例如[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer) `cls_token`已经注册为：obj*’[CLS]’*，XLM的一个也已经注册为`'</s>'`）。

示例：

```py
# Let's see how to add a new classification token to GPT-2
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2Model.from_pretrained("gpt2")

special_tokens_dict = {"cls_token": "<CLS>"}

num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))

assert tokenizer.cls_token == "<CLS>"
```

#### `add_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)

```py
( new_tokens: Union special_tokens: bool = False ) → export const metadata = 'undefined';int
```

参数

+   `new_tokens`（`str`，`tokenizers.AddedToken`或*str*或`tokenizers.AddedToken`的列表） - 仅当这些标记尚未在词汇表中时才会添加。`tokenizers.AddedToken`包装了一个字符串标记，让您可以个性化其行为：这个标记是否只匹配一个单词，这个标记是否应该去除左侧的所有潜在空格，这个标记是否应该去除右侧的所有潜在空格等。

+   `special_tokens`（`bool`，*可选*，默认为`False`） - 可用于指定标记是否为特殊标记。这主要会改变标准化行为（例如，特殊标记如CLS或[MASK]通常不会被小写）。

    请参阅HuggingFace tokenizers库中的`tokenizers.AddedToken`的详细信息。

返回

`int`

将特殊标记添加到词汇表中的数量。

向标记器类添加一组新标记。如果新标记不在词汇表中，则将它们添加到词汇表中，索引从当前词汇表的长度开始，并且在应用标记化算法之前将被隔离。因此，标记化算法的添加标记和词汇表中的标记不会以相同的方式处理。

注意，向词汇表中添加新标记时，您应确保还调整模型的标记嵌入矩阵，以使其嵌入矩阵与标记器匹配。

为了实现这一点，请使用[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)方法。

示例：

```py
# Let's see how to increase the vocabulary of Bert model and tokenizer
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
print("We have added", num_added_toks, "tokens")
# Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.
model.resize_token_embeddings(len(tokenizer))
```

#### `sanitize_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L865)

```py
( )
```

`sanitize_special_tokens` 现在已弃用，仅用于向后兼容，并将在 transformers v5 中移除。

## 枚举和命名元组

### `class transformers.tokenization_utils_base.TruncationStrategy`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L138)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

在 [PreTrainedTokenizerBase.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 中 `truncation` 参数的可能取值。在 IDE 中用于制表完成。

### `class transformers.CharSpan`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L150)

```py
( start: int end: int )
```

参数

+   `start` (`int`) — 原始字符串中第一个字符的索引。

+   `end` (`int`) — 原始字符串中最后一个字符后面的字符的索引。

原始字符串中的字符范围。

### `class transformers.TokenSpan`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L163)

```py
( start: int end: int )
```

参数

+   `start` (`int`) — span 中第一个标记的索引。

+   `end` (`int`) — span 中最后一个标记后面的标记的索引。

编码字符串中的标记范围（标记列表）。
