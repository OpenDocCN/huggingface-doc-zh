- en: Polars
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Polars
- en: 'Original text: [https://huggingface.co/docs/datasets-server/polars](https://huggingface.co/docs/datasets-server/polars)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/datasets-server/polars](https://huggingface.co/docs/datasets-server/polars)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Polars](https://pola-rs.github.io/polars-book/user-guide/) is a fast DataFrame
    library written in Rust with Arrow as its foundation.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Polars](https://pola-rs.github.io/polars-book/user-guide/)是一个快速的DataFrame库，用Rust编写，以Arrow为基础。'
- en: 💡 Learn more about how to get the dataset URLs in the [List Parquet files](parquet)
    guide.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 了解如何在[List Parquet files](parquet)指南中获取数据集URL的更多信息。
- en: 'Let’s start by grabbing the URLs to the `train` split of the [`blog_authorship_corpus`](https://huggingface.co/datasets/blog_authorship_corpus)
    dataset from Datasets Server:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据集服务器中获取[`blog_authorship_corpus`](https://huggingface.co/datasets/blog_authorship_corpus)数据集的`train`拆分的URL：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To read from a single Parquet file, use the [`read_parquet`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html)
    function to read it into a DataFrame and then execute your query:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要从单个Parquet文件中读取，请使用[`read_parquet`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_parquet.html)函数将其读入DataFrame，然后执行您的查询：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To read multiple Parquet files - for example, if the dataset is sharded - you’ll
    need to use the [`concat`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.concat.html)
    function to concatenate the files into a single DataFrame:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取多个Parquet文件 - 例如，如果数据集被分片 - 您需要使用[`concat`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.concat.html)函数将文件连接成单个DataFrame：
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Lazy API
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒惰的API
- en: Polars offers a [lazy API](https://pola-rs.github.io/polars-book/user-guide/lazy/using/)
    that is more performant and memory-efficient for large Parquet files. The LazyFrame
    API keeps track of what you want to do, and it’ll only execute the entire query
    when you’re ready. This way, the lazy API doesn’t load everything into RAM beforehand,
    and it allows you to work with datasets larger than your available RAM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Polars提供了一个[懒惰的API](https://pola-rs.github.io/polars-book/user-guide/lazy/using/)，对于大型Parquet文件来说更高效和节省内存。LazyFrame
    API会跟踪您想要做的事情，只有在您准备好时才会执行整个查询。这样，懒惰的API不会预先将所有内容加载到RAM中，它允许您处理比可用RAM更大的数据集。
- en: 'To lazily read a Parquet file, use the [`scan_parquet`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_parquet.html)
    function instead. Then, execute the entire query with the [`collect`](https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/api/polars.LazyFrame.collect.html)
    function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要惰性地读取Parquet文件，请改用[`scan_parquet`](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_parquet.html)函数。然后，使用[`collect`](https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/api/polars.LazyFrame.collect.html)函数执行整个查询：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
