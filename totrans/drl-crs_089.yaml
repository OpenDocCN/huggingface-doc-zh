- en: Designing Multi-Agents systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计多智能体系统
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting](https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting](https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: For this section, you’re going to watch this excellent introduction to multi-agents
    made by [Brian Douglas](https://www.youtube.com/channel/UCq0imsn84ShAe9PBOFnoIrg)
    .
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，您将观看由[Brian Douglas](https://www.youtube.com/channel/UCq0imsn84ShAe9PBOFnoIrg)制作的关于多智能体的优秀介绍。
- en: '[https://www.youtube-nocookie.com/embed/qgb0gyrpiGk](https://www.youtube-nocookie.com/embed/qgb0gyrpiGk)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/qgb0gyrpiGk](https://www.youtube-nocookie.com/embed/qgb0gyrpiGk)'
- en: 'In this video, Brian talked about how to design multi-agent systems. He specifically
    took a multi-agents system of vacuum cleaners and asked: **how can can cooperate
    with each other**?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个视频中，Brian谈到了如何设计多智能体系统。他特别提到了一个多智能体吸尘器系统，并问：**它们如何可以相互合作**？
- en: We have two solutions to design this multi-agent reinforcement learning system
    (MARL).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种解决方案来设计这个多智能体强化学习系统（MARL）。
- en: Decentralized system
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分散式系统
- en: '![Decentralized](../Images/d82bc536a16b8e15791ba574b6f9f35f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Decentralized](../Images/d82bc536a16b8e15791ba574b6f9f35f.png)'
- en: 'Source: [Introduction to Multi-Agent Reinforcement Learning](https://www.youtube.com/watch?v=qgb0gyrpiGk)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[多智能体强化学习介绍](https://www.youtube.com/watch?v=qgb0gyrpiGk)
- en: In decentralized learning, **each agent is trained independently from the others**.
    In the example given, each vacuum learns to clean as many places as it can **without
    caring about what other vacuums (agents) are doing**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在分散式学习中，**每个智能体都是独立训练的**。在给定的示例中，每个吸尘器都学习尽可能多地清洁地方**而不关心其他吸尘器（智能体）在做什么**。
- en: The benefit is that **since no information is shared between agents, these vacuums
    can be designed and trained like we train single agents**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 好处在于**由于智能体之间没有共享信息，这些吸尘器可以像我们训练单个智能体一样设计和训练**。
- en: The idea here is that **our training agent will consider other agents as part
    of the environment dynamics**. Not as agents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是**我们的训练智能体将把其他智能体视为环境动态的一部分**。而不是智能体。
- en: However, the big drawback of this technique is that it will **make the environment
    non-stationary** since the underlying Markov decision process changes over time
    as other agents are also interacting in the environment. And this is problematic
    for many Reinforcement Learning algorithms **that can’t reach a global optimum
    with a non-stationary environment**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种技术的一个重大缺点是，它会**使环境变得非静止**，因为随着其他智能体也在环境中交互，潜在的马尔可夫决策过程会随时间变化。这对许多强化学习算法来说是有问题的，**因为它们无法在非静止环境中达到全局最优**。
- en: Centralized approach
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式方法
- en: '![Centralized](../Images/5eb69229013368a2d3d27ad287a597e8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Centralized](../Images/5eb69229013368a2d3d27ad287a597e8.png)'
- en: 'Source: [Introduction to Multi-Agent Reinforcement Learning](https://www.youtube.com/watch?v=qgb0gyrpiGk)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[多智能体强化学习介绍](https://www.youtube.com/watch?v=qgb0gyrpiGk)
- en: 'In this architecture, **we have a high-level process that collects agents’
    experiences**: the experience buffer. And we’ll use these experiences **to learn
    a common policy**.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，**我们有一个收集智能体经验的高级过程**：经验缓冲区。我们将使用这些经验**来学习一个共同的策略**。
- en: 'For instance, in the vacuum cleaner example, the observation will be:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在吸尘器示例中，观察将是：
- en: The coverage map of the vacuums.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吸尘器的覆盖地图。
- en: The position of all the vacuums.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有吸尘器的位置。
- en: We use that collective experience **to train a policy that will move all three
    robots in the most beneficial way as a whole**. So each robot is learning from
    their common experience. We now have a stationary environment since all the agents
    are treated as a larger entity, and they know the change of other agents’ policies
    (since it’s the same as theirs).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用这种集体经验**来训练一个策略，以最有利的方式移动所有三个机器人作为一个整体**。因此，每个机器人都从他们的共同经验中学习。现在我们有一个静止的环境，因为所有智能体都被视为一个更大的实体，并且他们知道其他智能体策略的变化（因为它与他们的相同）。
- en: 'If we recap:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下：
- en: In a *decentralized approach*, we **treat all agents independently without considering
    the existence of the other agents.**
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*分散式方法*中，我们**独立地处理所有智能体，而不考虑其他智能体的存在**。
- en: In this case, all agents **consider others agents as part of the environment**.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，所有智能体**将其他智能体视为环境的一部分**。
- en: '**It’s a non-stationarity environment condition**, so has no guarantee of convergence.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这是一个非静止环境条件**，所以没有收敛的保证。'
- en: 'In a *centralized approach*:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*集中式方法*中：
- en: A **single policy is learned from all the agents**.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从所有智能体中学习单一策略**。'
- en: Takes as input the present state of an environment and the policy outputs joint
    actions.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将环境的当前状态和策略输出联合动作作为输入。
- en: The reward is global.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励是全局的。
