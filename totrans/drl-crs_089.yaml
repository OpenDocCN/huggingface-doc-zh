- en: Designing Multi-Agents systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting](https://huggingface.co/learn/deep-rl-course/unit7/multi-agent-setting)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/76.33b8f8e0.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Youtube.8666c400.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: For this section, you’re going to watch this excellent introduction to multi-agents
    made by [Brian Douglas](https://www.youtube.com/channel/UCq0imsn84ShAe9PBOFnoIrg)
    .
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/qgb0gyrpiGk](https://www.youtube-nocookie.com/embed/qgb0gyrpiGk)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this video, Brian talked about how to design multi-agent systems. He specifically
    took a multi-agents system of vacuum cleaners and asked: **how can can cooperate
    with each other**?'
  prefs: []
  type: TYPE_NORMAL
- en: We have two solutions to design this multi-agent reinforcement learning system
    (MARL).
  prefs: []
  type: TYPE_NORMAL
- en: Decentralized system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Decentralized](../Images/d82bc536a16b8e15791ba574b6f9f35f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Introduction to Multi-Agent Reinforcement Learning](https://www.youtube.com/watch?v=qgb0gyrpiGk)'
  prefs: []
  type: TYPE_NORMAL
- en: In decentralized learning, **each agent is trained independently from the others**.
    In the example given, each vacuum learns to clean as many places as it can **without
    caring about what other vacuums (agents) are doing**.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit is that **since no information is shared between agents, these vacuums
    can be designed and trained like we train single agents**.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is that **our training agent will consider other agents as part
    of the environment dynamics**. Not as agents.
  prefs: []
  type: TYPE_NORMAL
- en: However, the big drawback of this technique is that it will **make the environment
    non-stationary** since the underlying Markov decision process changes over time
    as other agents are also interacting in the environment. And this is problematic
    for many Reinforcement Learning algorithms **that can’t reach a global optimum
    with a non-stationary environment**.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Centralized](../Images/5eb69229013368a2d3d27ad287a597e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Introduction to Multi-Agent Reinforcement Learning](https://www.youtube.com/watch?v=qgb0gyrpiGk)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this architecture, **we have a high-level process that collects agents’
    experiences**: the experience buffer. And we’ll use these experiences **to learn
    a common policy**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in the vacuum cleaner example, the observation will be:'
  prefs: []
  type: TYPE_NORMAL
- en: The coverage map of the vacuums.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The position of all the vacuums.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use that collective experience **to train a policy that will move all three
    robots in the most beneficial way as a whole**. So each robot is learning from
    their common experience. We now have a stationary environment since all the agents
    are treated as a larger entity, and they know the change of other agents’ policies
    (since it’s the same as theirs).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we recap:'
  prefs: []
  type: TYPE_NORMAL
- en: In a *decentralized approach*, we **treat all agents independently without considering
    the existence of the other agents.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, all agents **consider others agents as part of the environment**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It’s a non-stationarity environment condition**, so has no guarantee of convergence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In a *centralized approach*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **single policy is learned from all the agents**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Takes as input the present state of an environment and the policy outputs joint
    actions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is global.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
