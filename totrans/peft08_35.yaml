- en: AdaLoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/adalora](https://huggingface.co/docs/peft/package_reference/adalora)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/16.e2c92e6e.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/ExampleCodeBlock.a22db1c3.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number
    of trainable parameters to assign to weight matrices and layers, unlike LoRA,
    which distributes parameters evenly across all modules. More parameters are budgeted
    for important weight matrices and layers while less important ones receive fewer
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fine-tuning large pre-trained language models on downstream tasks has become
    an important paradigm in NLP. However, common practice fine-tunes all of the parameters
    in a pre-trained model, which becomes prohibitive when a large number of downstream
    tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental
    updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments.
    These methods often evenly distribute the budget of incremental updates across
    all pre-trained weight matrices, and overlook the varying importance of different
    weight parameters. As a consequence, the fine-tuning performance is suboptimal.
    To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter
    budget among weight matrices according to their importance score. In particular,
    AdaLoRA parameterizes the incremental updates in the form of singular value decomposition.
    Such a novel approach allows us to effectively prune the singular values of unimportant
    updates, which is essentially to reduce their parameter budget but circumvent
    intensive exact SVD computations. We conduct extensive experiments with several
    pre-trained models on natural language processing, question answering, and natural
    language generation to validate the effectiveness of AdaLoRA. Results demonstrate
    that AdaLoRA manifests notable improvement over baselines, especially in the low
    budget settings. Our code is publicly available at [https://github.com/QingruZhang/AdaLoRA](https://github.com/QingruZhang/AdaLoRA)*.'
  prefs: []
  type: TYPE_NORMAL
- en: AdaLoraConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.AdaLoraConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/config.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`target_r` (`int`) — The target average rank of incremental matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_r` (`int`) — The initial rank for each incremental matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tinit` (`int`) — The steps of initial fine-tuning warmup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tfinal` (`int`) — The step of final fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deltaT` (`int`) — The time internval between two budget allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta1` (`float`) — The hyperparameter of EMA for sensitivity smoothing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta2` (`float`) — The hyperparameter of EMA for undertainty quantification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`orth_reg_weight` (`float`) — The coefficient of orthogonal regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_step` (`int`) — The total training steps that should be specified before
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_pattern` (`list`) — The allocated rank for each weight matrix by RankAllocator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a `~peft.AdaLora`.
  prefs: []
  type: TYPE_NORMAL
- en: AdaLoraModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.AdaLoraModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([*transformers.PreTrainedModel*]) — The model to be adapted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([*AdaLoraConfig*]) — The configuration of the AdaLora model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (*str*) — The name of the adapter, defaults to *“default”*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '*torch.nn.Module*'
  prefs: []
  type: TYPE_NORMAL
- en: The AdaLora model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creates AdaLoRA (Adaptive LoRA) model from a pretrained transformers model.
    Paper: [https://openreview.net/forum?id=lq62uWRJjiY](https://openreview.net/forum?id=lq62uWRJjiY)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model` ([*transformers.PreTrainedModel*]) — The model to be adapted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([*AdaLoraConfig*]): The configuration of the AdaLora model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `update_and_allocate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L306)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`global_step` (`int`) — The current training step, it is used to calculate
    adalora budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method updates Adalora budget and mask.
  prefs: []
  type: TYPE_NORMAL
- en: This should be called in every training step after `loss.backward()` and before
    `zero_grad()`.
  prefs: []
  type: TYPE_NORMAL
- en: '`tinit`, `tfinal` and `deltaT` are handled with in the method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
