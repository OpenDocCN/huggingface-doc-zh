- en: AdaLoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaLoRA
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/adalora](https://huggingface.co/docs/peft/package_reference/adalora)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文链接](https://huggingface.co/docs/peft/package_reference/adalora)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[AdaLoRA](https://hf.co/papers/2303.10512) is a method for optimizing the number
    of trainable parameters to assign to weight matrices and layers, unlike LoRA,
    which distributes parameters evenly across all modules. More parameters are budgeted
    for important weight matrices and layers while less important ones receive fewer
    parameters.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdaLoRA](https://hf.co/papers/2303.10512) 是一种用于优化可分配给权重矩阵和层的可训练参数数量的方法，与 LoRA
    不同，LoRA 将参数均匀分配到所有模块中。对于重要的权重矩阵和层分配更多的参数预算，而对于不太重要的部分分配较少的参数。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要是：
- en: '*Fine-tuning large pre-trained language models on downstream tasks has become
    an important paradigm in NLP. However, common practice fine-tunes all of the parameters
    in a pre-trained model, which becomes prohibitive when a large number of downstream
    tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental
    updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments.
    These methods often evenly distribute the budget of incremental updates across
    all pre-trained weight matrices, and overlook the varying importance of different
    weight parameters. As a consequence, the fine-tuning performance is suboptimal.
    To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter
    budget among weight matrices according to their importance score. In particular,
    AdaLoRA parameterizes the incremental updates in the form of singular value decomposition.
    Such a novel approach allows us to effectively prune the singular values of unimportant
    updates, which is essentially to reduce their parameter budget but circumvent
    intensive exact SVD computations. We conduct extensive experiments with several
    pre-trained models on natural language processing, question answering, and natural
    language generation to validate the effectiveness of AdaLoRA. Results demonstrate
    that AdaLoRA manifests notable improvement over baselines, especially in the low
    budget settings. Our code is publicly available at [https://github.com/QingruZhang/AdaLoRA](https://github.com/QingruZhang/AdaLoRA)*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*在下游任务上对大型预训练语言模型进行微调已经成为自然语言处理中的重要范式。然而，常见做法是微调预训练模型中的所有参数，当存在大量下游任务时，这种做法变得不切实际。因此，提出了许多微调方法，以一种参数高效的方式学习预训练权重的增量更新，例如低秩增量。这些方法通常会将增量更新的预算均匀分配到所有预训练权重矩阵中，忽视了不同权重参数的重要性差异。因此，微调性能是次优的。为了弥补这一差距，我们提出了
    AdaLoRA，根据它们的重要性分数自适应地在权重矩阵之间分配参数预算。特别地，AdaLoRA 将增量更新参数化为奇异值分解的形式。这种新颖的方法使我们能够有效地修剪不重要更新的奇异值，从而减少它们的参数预算，但避免了繁重的精确
    SVD 计算。我们对自然语言处理、问答和自然语言生成中的几个预训练模型进行了广泛实验，以验证 AdaLoRA 的有效性。结果表明，AdaLoRA 在基线上表现出显著的改进，特别是在低预算设置下。我们的代码公开可用于
    [https://github.com/QingruZhang/AdaLoRA](https://github.com/QingruZhang/AdaLoRA)*。'
- en: AdaLoraConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaLoraConfig
- en: '### `class peft.AdaLoraConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.AdaLoraConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/config.py#L22)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/config.py#L22)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`target_r` (`int`) — The target average rank of incremental matrix.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_r` (`int`) — 增量矩阵的目标平均秩。'
- en: '`init_r` (`int`) — The initial rank for each incremental matrix.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_r` (`int`) — 每个增量矩阵的初始秩。'
- en: '`tinit` (`int`) — The steps of initial fine-tuning warmup.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tinit` (`int`) — 初始微调热身步骤。'
- en: '`tfinal` (`int`) — The step of final fine-tuning.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tfinal` (`int`) — 最终微调步骤。'
- en: '`deltaT` (`int`) — The time internval between two budget allocations.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deltaT` (`int`) — 两个预算分配之间的时间间隔。'
- en: '`beta1` (`float`) — The hyperparameter of EMA for sensitivity smoothing.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta1` (`float`) — 用于敏感性平滑的 EMA 超参数。'
- en: '`beta2` (`float`) — The hyperparameter of EMA for undertainty quantification.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta2` (`float`) — 用于不确定性量化的 EMA 超参数。'
- en: '`orth_reg_weight` (`float`) — The coefficient of orthogonal regularization.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orth_reg_weight` (`float`) — 正交正则化的系数。'
- en: '`total_step` (`int`) — The total training steps that should be specified before
    training.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_step` (`int`) — 在训练之前应指定的总训练步骤。'
- en: '`rank_pattern` (`list`) — The allocated rank for each weight matrix by RankAllocator.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank_pattern` (`list`) — 由 RankAllocator 为每个权重矩阵分配的秩。'
- en: This is the configuration class to store the configuration of a `~peft.AdaLora`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 `~peft.AdaLora` 配置的配置类。
- en: AdaLoraModel
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaLoraModel
- en: '### `class peft.AdaLoraModel`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.AdaLoraModel`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L35)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L35)'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` ([*transformers.PreTrainedModel*]) — The model to be adapted.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([*transformers.PreTrainedModel*]) — 要适应的模型。'
- en: '`config` ([*AdaLoraConfig*]) — The configuration of the AdaLora model.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([*AdaLoraConfig*]) — AdaLora 模型的配置。'
- en: '`adapter_name` (*str*) — The name of the adapter, defaults to *“default”*.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (*str*) — 适配器的名称，默认为 *“default”*。'
- en: Returns
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '*torch.nn.Module*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*torch.nn.Module*'
- en: The AdaLora model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: AdaLora 模型。
- en: 'Creates AdaLoRA (Adaptive LoRA) model from a pretrained transformers model.
    Paper: [https://openreview.net/forum?id=lq62uWRJjiY](https://openreview.net/forum?id=lq62uWRJjiY)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的 transformers 模型创建 AdaLoRA (自适应 LoRA) 模型。论文：[https://openreview.net/forum?id=lq62uWRJjiY](https://openreview.net/forum?id=lq62uWRJjiY)
- en: 'Example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Attributes**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**：'
- en: '`model` ([*transformers.PreTrainedModel*]) — The model to be adapted.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` ([*transformers.PreTrainedModel*]) — 要适应的模型。'
- en: '`peft_config` ([*AdaLoraConfig*]): The configuration of the AdaLora model.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` ([*AdaLoraConfig*])：AdaLora 模型的配置。'
- en: '#### `update_and_allocate`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `update_and_allocate`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L306)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/adalora/model.py#L306)'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`global_step` (`int`) — The current training step, it is used to calculate
    adalora budget.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_step` (`int`) — 当前训练步骤，用于计算 adalora 预算。'
- en: This method updates Adalora budget and mask.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法更新Adalora预算和掩码。
- en: This should be called in every training step after `loss.backward()` and before
    `zero_grad()`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在`loss.backward()`之后和`zero_grad()`之前的每个训练步骤中应该调用此方法。
- en: '`tinit`, `tfinal` and `deltaT` are handled with in the method.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`tinit`，`tfinal`和`deltaT`在该方法中处理。'
- en: 'Example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
