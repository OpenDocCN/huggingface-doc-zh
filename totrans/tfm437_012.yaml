- en: Load adapters with ðŸ¤— PEFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/316.86bd3ba7.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/DocNotebookDropdown.3e6b3817.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft)
    methods freeze the pretrained model parameters during fine-tuning and add a small
    number of trainable parameters (the adapters) on top of it. The adapters are trained
    to learn task-specific information. This approach has been shown to be very memory-efficient
    with lower compute usage while producing results comparable to a fully fine-tuned
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Adapters trained with PEFT are also usually an order of magnitude smaller than
    the full model, making it convenient to share, store, and load them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  prefs: []
  type: TYPE_IMG
- en: The adapter weights for a OPTForCausalLM model stored on the Hub are only ~6MB
    compared to the full size of the model weights, which can be ~700MB.
  prefs: []
  type: TYPE_NORMAL
- en: If youâ€™re interested in learning more about the ðŸ¤— PEFT library, check out the
    [documentation](https://huggingface.co/docs/peft/index).
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Get started by installing ðŸ¤— PEFT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to try out the brand new features, you might be interested in installing
    the library from source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Supported PEFT models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ðŸ¤— Transformers natively supports some PEFT methods, meaning you can load adapter
    weights stored locally or on the Hub and easily run or train them with a few lines
    of code. The following methods are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use other PEFT methods, such as prompt learning or prompt tuning,
    or about the ðŸ¤— PEFT library in general, please refer to the [documentation](https://huggingface.co/docs/peft/index).
  prefs: []
  type: TYPE_NORMAL
- en: Load a PEFT adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and use a PEFT adapter model from ðŸ¤— Transformers, make sure the Hub
    repository or local directory contains an `adapter_config.json` file and the adapter
    weights, as shown in the example image above. Then you can load the PEFT adapter
    model using the `AutoModelFor` class. For example, to load a PEFT adapter model
    for causal language modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: specify the PEFT model id
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: pass it to the [AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)
    class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can load a PEFT adapter with either an `AutoModelFor` class or the base
    model class like `OPTForCausalLM` or `LlamaForCausalLM`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also load a PEFT adapter by calling the `load_adapter` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Load in 8bit or 4bit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `bitsandbytes` integration supports 8bit and 4bit precision data types,
    which are useful for loading large models because it saves memory (see the `bitsandbytes`
    integration [guide](./quantization#bitsandbytes-integration) to learn more). Add
    the `load_in_8bit` or `load_in_4bit` parameters to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and set `device_map="auto"` to effectively distribute the model to your hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Add a new adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use `~peft.PeftModel.add_adapter` to add a new adapter to a model with
    an existing adapter as long as the new adapter is the same type as the current
    one. For example, if you have an existing LoRA adapter attached to a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To add a new adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can use `~peft.PeftModel.set_adapter` to set which adapter to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Enable and disable adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once youâ€™ve added an adapter to a model, you can enable or disable the adapter
    module. To enable the adapter module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To disable the adapter module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Train a PEFT adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PEFT adapters are supported by the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class so that you can train an adapter for your specific use case. It only requires
    adding a few more lines of code. For example, to train a LoRA adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with fine-tuning a model with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the [Fine-tune a pretrained model](training) tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Define your adapter configuration with the task type and hyperparameters (see
    `~peft.LoraConfig` for more details about what the hyperparameters do).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Add adapter to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now you can pass the model to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To save your trained adapter and load it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Add additional trainable layers to a PEFT adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also fine-tune additional trainable adapters on top of a model that
    has adapters attached by passing `modules_to_save` in your PEFT config. For example,
    if you want to also fine-tune the lm_head on top of a model with a LoRA adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
