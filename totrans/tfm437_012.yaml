- en: Load adapters with ğŸ¤— PEFT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— PEFTåŠ è½½é€‚é…å™¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft)
    methods freeze the pretrained model parameters during fine-tuning and add a small
    number of trainable parameters (the adapters) on top of it. The adapters are trained
    to learn task-specific information. This approach has been shown to be very memory-efficient
    with lower compute usage while producing results comparable to a fully fine-tuned
    model.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰](https://huggingface.co/blog/peft)æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å™¨ï¼‰ã€‚é€‚é…å™¨è¢«è®­ç»ƒä»¥å­¦ä¹ ç‰¹å®šä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å·²è¢«è¯æ˜åœ¨ä½¿ç”¨æ›´ä½çš„è®¡ç®—èµ„æºçš„åŒæ—¶äº§ç”Ÿä¸å®Œå…¨å¾®è°ƒæ¨¡å‹ç›¸åª²ç¾çš„ç»“æœæ—¶éå¸¸èŠ‚çœå†…å­˜ã€‚'
- en: Adapters trained with PEFT are also usually an order of magnitude smaller than
    the full model, making it convenient to share, store, and load them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PEFTè®­ç»ƒçš„é€‚é…å™¨é€šå¸¸æ¯”å®Œæ•´æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œè¿™æ ·æ–¹ä¾¿åˆ†äº«ã€å­˜å‚¨å’ŒåŠ è½½ã€‚
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
- en: The adapter weights for a OPTForCausalLM model stored on the Hub are only ~6MB
    compared to the full size of the model weights, which can be ~700MB.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å­˜å‚¨åœ¨Hubä¸Šçš„OPTForCausalLMæ¨¡å‹çš„é€‚é…å™¨æƒé‡ä»…çº¦ä¸º6MBï¼Œè€Œæ¨¡å‹æƒé‡çš„å®Œæ•´å¤§å°å¯èƒ½çº¦ä¸º700MBã€‚
- en: If youâ€™re interested in learning more about the ğŸ¤— PEFT library, check out the
    [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºğŸ¤— PEFTåº“çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: 'Get started by installing ğŸ¤— PEFT:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å®‰è£…ğŸ¤— PEFTæ¥å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to try out the brand new features, you might be interested in installing
    the library from source:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³å°è¯•å…¨æ–°çš„åŠŸèƒ½ï¼Œæ‚¨å¯èƒ½ä¼šå¯¹ä»æºä»£ç å®‰è£…åº“æ„Ÿå…´è¶£ï¼š
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Supported PEFT models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ”¯æŒçš„PEFTæ¨¡å‹
- en: 'ğŸ¤— Transformers natively supports some PEFT methods, meaning you can load adapter
    weights stored locally or on the Hub and easily run or train them with a few lines
    of code. The following methods are supported:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— TransformersåŸç”Ÿæ”¯æŒä¸€äº›PEFTæ–¹æ³•ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥åŠ è½½æœ¬åœ°æˆ–Hubä¸Šå­˜å‚¨çš„é€‚é…å™¨æƒé‡ï¼Œå¹¶ä½¿ç”¨å‡ è¡Œä»£ç è½»æ¾è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚æ”¯æŒä»¥ä¸‹æ–¹æ³•ï¼š
- en: '[Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½ç§©é€‚é…å™¨](https://huggingface.co/docs/peft/conceptual_guides/lora)'
- en: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
- en: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
- en: If you want to use other PEFT methods, such as prompt learning or prompt tuning,
    or about the ğŸ¤— PEFT library in general, please refer to the [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³ä½¿ç”¨å…¶ä»–PEFTæ–¹æ³•ï¼Œå¦‚æç¤ºå­¦ä¹ æˆ–æç¤ºè°ƒæ•´ï¼Œæˆ–è€…äº†è§£ğŸ¤— PEFTåº“çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://huggingface.co/docs/peft/index)ã€‚
- en: Load a PEFT adapter
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½PEFTé€‚é…å™¨
- en: 'To load and use a PEFT adapter model from ğŸ¤— Transformers, make sure the Hub
    repository or local directory contains an `adapter_config.json` file and the adapter
    weights, as shown in the example image above. Then you can load the PEFT adapter
    model using the `AutoModelFor` class. For example, to load a PEFT adapter model
    for causal language modeling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½å’Œä½¿ç”¨ğŸ¤— Transformersä¸­çš„PEFTé€‚é…å™¨æ¨¡å‹æ—¶ï¼Œè¯·ç¡®ä¿Hubå­˜å‚¨åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª`adapter_config.json`æ–‡ä»¶å’Œé€‚é…å™¨æƒé‡ï¼Œå¦‚ä¸Šé¢çš„ç¤ºä¾‹å›¾æ‰€ç¤ºã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»åŠ è½½PEFTé€‚é…å™¨æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦åŠ è½½ç”¨äºå› æœè¯­è¨€å»ºæ¨¡çš„PEFTé€‚é…å™¨æ¨¡å‹ï¼š
- en: specify the PEFT model id
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‡å®šPEFTæ¨¡å‹ID
- en: pass it to the [AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)
    class
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å…¶ä¼ é€’ç»™[AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)ç±»
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can load a PEFT adapter with either an `AutoModelFor` class or the base
    model class like `OPTForCausalLM` or `LlamaForCausalLM`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`AutoModelFor`ç±»æˆ–åŸºæœ¬æ¨¡å‹ç±»ï¼ˆå¦‚`OPTForCausalLM`æˆ–`LlamaForCausalLM`ï¼‰åŠ è½½PEFTé€‚é…å™¨ã€‚
- en: 'You can also load a PEFT adapter by calling the `load_adapter` method:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡è°ƒç”¨`load_adapter`æ–¹æ³•åŠ è½½PEFTé€‚é…å™¨ï¼š
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Load in 8bit or 4bit
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»¥8ä½æˆ–4ä½åŠ è½½
- en: 'The `bitsandbytes` integration supports 8bit and 4bit precision data types,
    which are useful for loading large models because it saves memory (see the `bitsandbytes`
    integration [guide](./quantization#bitsandbytes-integration) to learn more). Add
    the `load_in_8bit` or `load_in_4bit` parameters to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and set `device_map="auto"` to effectively distribute the model to your hardware:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`bitsandbytes`é›†æˆæ”¯æŒ8ä½å’Œ4ä½ç²¾åº¦æ•°æ®ç±»å‹ï¼Œå¯¹äºåŠ è½½å¤§å‹æ¨¡å‹å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒèŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…`bitsandbytes`é›†æˆ[æŒ‡å—](./quantization#bitsandbytes-integration)ä»¥äº†è§£æ›´å¤šï¼‰ã€‚å°†`load_in_8bit`æˆ–`load_in_4bit`å‚æ•°æ·»åŠ åˆ°[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)ä¸­ï¼Œå¹¶è®¾ç½®`device_map="auto"`ä»¥æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°æ‚¨çš„ç¡¬ä»¶ï¼š'
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Add a new adapter
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªæ–°é€‚é…å™¨
- en: 'You can use `~peft.PeftModel.add_adapter` to add a new adapter to a model with
    an existing adapter as long as the new adapter is the same type as the current
    one. For example, if you have an existing LoRA adapter attached to a model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨`~peft.PeftModel.add_adapter`å°†ä¸€ä¸ªæ–°é€‚é…å™¨æ·»åŠ åˆ°å…·æœ‰ç°æœ‰é€‚é…å™¨çš„æ¨¡å‹ä¸­ï¼Œåªè¦æ–°é€‚é…å™¨ä¸å½“å‰é€‚é…å™¨çš„ç±»å‹ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æœ‰ä¸€ä¸ªå·²ç»è¿æ¥åˆ°æ¨¡å‹çš„ç°æœ‰LoRAé€‚é…å™¨ï¼š
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To add a new adapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ ä¸€ä¸ªæ–°é€‚é…å™¨ï¼š
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now you can use `~peft.PeftModel.set_adapter` to set which adapter to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨`~peft.PeftModel.set_adapter`æ¥è®¾ç½®è¦ä½¿ç”¨çš„é€‚é…å™¨ï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable and disable adapters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯ç”¨å’Œç¦ç”¨é€‚é…å™¨
- en: 'Once youâ€™ve added an adapter to a model, you can enable or disable the adapter
    module. To enable the adapter module:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‚¨å‘æ¨¡å‹æ·»åŠ äº†é€‚é…å™¨ï¼Œæ‚¨å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨é€‚é…å™¨æ¨¡å—ã€‚è¦å¯ç”¨é€‚é…å™¨æ¨¡å—ï¼š
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To disable the adapter module:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¦ç”¨é€‚é…å™¨æ¨¡å—ï¼š
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Train a PEFT adapter
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªPEFTé€‚é…å™¨
- en: 'PEFT adapters are supported by the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class so that you can train an adapter for your specific use case. It only requires
    adding a few more lines of code. For example, to train a LoRA adapter:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: PEFTé€‚é…å™¨å—[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ç±»æ”¯æŒï¼Œå› æ­¤æ‚¨å¯ä»¥ä¸ºç‰¹å®šç”¨ä¾‹è®­ç»ƒä¸€ä¸ªé€‚é…å™¨ã€‚åªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ªLoRAé€‚é…å™¨ï¼š
- en: If you arenâ€™t familiar with fine-tuning a model with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the [Fine-tune a pretrained model](training) tutorial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](training)æ•™ç¨‹ã€‚
- en: Define your adapter configuration with the task type and hyperparameters (see
    `~peft.LoraConfig` for more details about what the hyperparameters do).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰æ‚¨çš„é€‚é…å™¨é…ç½®ï¼ˆæœ‰å…³è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`~peft.LoraConfig`ï¼‰ã€‚
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Add adapter to the model.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é€‚é…å™¨æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you can pass the model to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To save your trained adapter and load it back:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜æ‚¨è®­ç»ƒè¿‡çš„é€‚é…å™¨å¹¶åŠ è½½å›æ¥ï¼š
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Add additional trainable layers to a PEFT adapter
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘PEFTé€‚é…å™¨æ·»åŠ é¢å¤–çš„å¯è®­ç»ƒå±‚
- en: 'You can also fine-tune additional trainable adapters on top of a model that
    has adapters attached by passing `modules_to_save` in your PEFT config. For example,
    if you want to also fine-tune the lm_head on top of a model with a LoRA adapter:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨PEFTé…ç½®ä¸­ä¼ é€’`modules_to_save`æ¥åœ¨å·²é™„åŠ é€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨å¾®è°ƒé¢å¤–çš„å¯è®­ç»ƒé€‚é…å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³åœ¨å…·æœ‰LoRAé€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨ä¹Ÿå¾®è°ƒlm_headï¼š
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
