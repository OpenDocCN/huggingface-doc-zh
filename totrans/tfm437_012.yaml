- en: Load adapters with ğŸ¤— PEFT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/peft](https://huggingface.co/docs/transformers/v4.37.2/en/peft)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft)
    methods freeze the pretrained model parameters during fine-tuning and add a small
    number of trainable parameters (the adapters) on top of it. The adapters are trained
    to learn task-specific information. This approach has been shown to be very memory-efficient
    with lower compute usage while producing results comparable to a fully fine-tuned
    model.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Adapters trained with PEFT are also usually an order of magnitude smaller than
    the full model, making it convenient to share, store, and load them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e34ae8912ca7fcb5554d98cb511bc58.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: The adapter weights for a OPTForCausalLM model stored on the Hub are only ~6MB
    compared to the full size of the model weights, which can be ~700MB.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: If youâ€™re interested in learning more about the ğŸ¤— PEFT library, check out the
    [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Get started by installing ğŸ¤— PEFT:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to try out the brand new features, you might be interested in installing
    the library from source:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Supported PEFT models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ğŸ¤— Transformers natively supports some PEFT methods, meaning you can load adapter
    weights stored locally or on the Hub and easily run or train them with a few lines
    of code. The following methods are supported:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AdaLoRA](https://arxiv.org/abs/2303.10512)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to use other PEFT methods, such as prompt learning or prompt tuning,
    or about the ğŸ¤— PEFT library in general, please refer to the [documentation](https://huggingface.co/docs/peft/index).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Load a PEFT adapter
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To load and use a PEFT adapter model from ğŸ¤— Transformers, make sure the Hub
    repository or local directory contains an `adapter_config.json` file and the adapter
    weights, as shown in the example image above. Then you can load the PEFT adapter
    model using the `AutoModelFor` class. For example, to load a PEFT adapter model
    for causal language modeling:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: specify the PEFT model id
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: pass it to the [AutoModelForCausalLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForCausalLM)
    class
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can load a PEFT adapter with either an `AutoModelFor` class or the base
    model class like `OPTForCausalLM` or `LlamaForCausalLM`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also load a PEFT adapter by calling the `load_adapter` method:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Load in 8bit or 4bit
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `bitsandbytes` integration supports 8bit and 4bit precision data types,
    which are useful for loading large models because it saves memory (see the `bitsandbytes`
    integration [guide](./quantization#bitsandbytes-integration) to learn more). Add
    the `load_in_8bit` or `load_in_4bit` parameters to [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    and set `device_map="auto"` to effectively distribute the model to your hardware:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Add a new adapter
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use `~peft.PeftModel.add_adapter` to add a new adapter to a model with
    an existing adapter as long as the new adapter is the same type as the current
    one. For example, if you have an existing LoRA adapter attached to a model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To add a new adapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now you can use `~peft.PeftModel.set_adapter` to set which adapter to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable and disable adapters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once youâ€™ve added an adapter to a model, you can enable or disable the adapter
    module. To enable the adapter module:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To disable the adapter module:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Train a PEFT adapter
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PEFT adapters are supported by the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class so that you can train an adapter for your specific use case. It only requires
    adding a few more lines of code. For example, to train a LoRA adapter:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with fine-tuning a model with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the [Fine-tune a pretrained model](training) tutorial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹[å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹](training)æ•™ç¨‹ã€‚
- en: Define your adapter configuration with the task type and hyperparameters (see
    `~peft.LoraConfig` for more details about what the hyperparameters do).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä»»åŠ¡ç±»å‹å’Œè¶…å‚æ•°å®šä¹‰æ‚¨çš„é€‚é…å™¨é…ç½®ï¼ˆæœ‰å…³è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`~peft.LoraConfig`ï¼‰ã€‚
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Add adapter to the model.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†é€‚é…å™¨æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you can pass the model to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)!
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥å°†æ¨¡å‹ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To save your trained adapter and load it back:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜æ‚¨è®­ç»ƒè¿‡çš„é€‚é…å™¨å¹¶åŠ è½½å›æ¥ï¼š
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Add additional trainable layers to a PEFT adapter
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘PEFTé€‚é…å™¨æ·»åŠ é¢å¤–çš„å¯è®­ç»ƒå±‚
- en: 'You can also fine-tune additional trainable adapters on top of a model that
    has adapters attached by passing `modules_to_save` in your PEFT config. For example,
    if you want to also fine-tune the lm_head on top of a model with a LoRA adapter:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨PEFTé…ç½®ä¸­ä¼ é€’`modules_to_save`æ¥åœ¨å·²é™„åŠ é€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨å¾®è°ƒé¢å¤–çš„å¯è®­ç»ƒé€‚é…å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³åœ¨å…·æœ‰LoRAé€‚é…å™¨çš„æ¨¡å‹é¡¶éƒ¨ä¹Ÿå¾®è°ƒlm_headï¼š
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
