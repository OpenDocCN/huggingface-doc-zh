# 自定义扩散

> 原始文本：[`huggingface.co/docs/diffusers/training/custom_diffusion`](https://huggingface.co/docs/diffusers/training/custom_diffusion)

[Custom Diffusion](https://huggingface.co/papers/2212.04488)是一种用于个性化图像生成模型的训练技术。与文本反转、DreamBooth 和 LoRA 一样，自定义扩散只需要几个（~4-5）示例图像。这种技术通过仅训练交叉注意力层中的权重来工作，并使用特殊词来表示新学习的概念。自定义扩散是独特的，因为它还可以同时学习多个概念。

如果您在具有有限 vRAM 的 GPU 上进行训练，您应该尝试使用`--enable_xformers_memory_efficient_attention`启用 xFormers，以实现更快的训练和更低的 vRAM 要求（16GB）。为了节省更多内存，可以在训练参数中添加`--set_grads_to_none`，将梯度设置为`None`而不是零（此选项可能会导致一些问题，如果您遇到任何问题，请尝试删除此参数）。

本指南将探讨[train_custom_diffusion.py](https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/train_custom_diffusion.py)脚本，帮助您更熟悉它，以及如何为自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

导航到包含训练脚本的示例文件夹并安装所需的依赖项：

```py
cd examples/custom_diffusion
pip install -r requirements.txt
pip install clip-retrieval
```

🤗 Accelerate 是一个帮助您在多个 GPU/TPU 上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。

初始化🤗 Accelerate 环境：

```py
accelerate config
```

要设置默认的🤗 Accelerate 环境而不选择任何配置：

```py
accelerate config default
```

或者如果您的环境不支持交互式 shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看创建用于训练的数据集指南，了解如何创建适用于训练脚本的数据集。

以下部分突出显示了训练脚本的重要部分，有助于了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/custom_diffusion/train_custom_diffusion.py)，并告诉我们您是否有任何问题或疑虑。

## 脚本参数

训练脚本包含所有参数，帮助您定制训练运行。这些参数在[`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L319)函数中找到。该函数带有默认值，但如果您愿意，也可以在训练命令中设置自己的值。

例如，要更改输入图像的分辨率：

```py
accelerate launch train_custom_diffusion.py \
  --resolution=256
```

许多基本参数在 DreamBooth 训练指南中有描述，因此本指南重点介绍了自定义扩散的独特参数：

+   `--freeze_model`：冻结交叉注意力层中的键和值参数；默认值为`crossattn_kv`，但您也可以将其设置为`crossattn`以训练交叉注意力层中的所有参数

+   `--concepts_list`：要学习多个概念，请提供包含概念的 JSON 文件的路径

+   `--modifier_token`：用于表示学习概念的特殊词

+   `--initializer_token`：

### 先前的保留损失

先前保存损失是一种方法，它利用模型生成的样本来帮助模型学习如何生成更多样化的图像。由于这些生成的样本图像属于您提供的图像相同的类别，它们有助于模型保留其对类别的学习以及如何利用其已知的类别知识来生成新构图的内容。

许多有关先前保存损失的参数在 DreamBooth 培训指南中有描述。

### 正则化

自定义扩散包括使用少量真实图像训练目标图像，以防止过拟合。可以想象，当您只对少量图像进行训练时，这可能很容易做到！使用`clip_retrieval`下载 200 张真实图像。`class_prompt`应该是与目标图像相同类别。这些图像存储在`class_data_dir`中。

```py
python retrieve.py --class_prompt cat --class_data_dir real_reg/samples_cat --num_class_images 200
```

要启用正则化，请添加以下参数：

+   `--with_prior_preservation`：是否使用先前保存损失

+   `--prior_loss_weight`：控制先前保存损失对模型的影响

+   `--real_prior`：是否使用少量真实图像以防止过拟合

```py
accelerate launch train_custom_diffusion.py \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --class_data_dir="./real_reg/samples_cat" \
  --class_prompt="cat" \
  --real_prior=True \
```

## 训练脚本

自定义扩散训练脚本中的许多代码与 DreamBooth 脚本相似。本指南重点介绍与自定义扩散相关的代码。

自定义扩散训练脚本有两个数据集类：

+   [`CustomDiffusionDataset`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L165)：对图像、类图像和提示进行预处理以进行训练

+   [`PromptDataset`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L148)：准备用于生成类图像的提示

接下来，`modifier_token`被添加到分词器中，转换为标记 id，并且标记嵌入被调整大小以适应新的`modifier_token`。然后，`modifier_token`的嵌入被初始化为`initializer_token`的嵌入。文本编码器中的所有参数都被冻结，除了标记嵌入，因为这是模型试图学习与概念相关联的内容。

```py
params_to_freeze = itertools.chain(
    text_encoder.text_model.encoder.parameters(),
    text_encoder.text_model.final_layer_norm.parameters(),
    text_encoder.text_model.embeddings.position_embedding.parameters(),
)
freeze_params(params_to_freeze)
```

现在，您需要将[自定义扩散权重](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/custom_diffusion/train_custom_diffusion.py#L911C3-L911C3)添加到注意力层中。这是确保注意力权重的形状和大小正确的一个非常重要的步骤，也是为每个 UNet 块设置适当数量的注意力处理器的关键。

```py
st = unet.state_dict()
for name, _ in unet.attn_processors.items():
    cross_attention_dim = None if name.endswith("attn1.processor") else unet.config.cross_attention_dim
    if name.startswith("mid_block"):
        hidden_size = unet.config.block_out_channels[-1]
    elif name.startswith("up_blocks"):
        block_id = int(name[len("up_blocks.")])
        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]
    elif name.startswith("down_blocks"):
        block_id = int(name[len("down_blocks.")])
        hidden_size = unet.config.block_out_channels[block_id]
    layer_name = name.split(".processor")[0]
    weights = {
        "to_k_custom_diffusion.weight": st[layer_name + ".to_k.weight"],
        "to_v_custom_diffusion.weight": st[layer_name + ".to_v.weight"],
    }
    if train_q_out:
        weights["to_q_custom_diffusion.weight"] = st[layer_name + ".to_q.weight"]
        weights["to_out_custom_diffusion.0.weight"] = st[layer_name + ".to_out.0.weight"]
        weights["to_out_custom_diffusion.0.bias"] = st[layer_name + ".to_out.0.bias"]
    if cross_attention_dim is not None:
        custom_diffusion_attn_procs[name] = attention_class(
            train_kv=train_kv,
            train_q_out=train_q_out,
            hidden_size=hidden_size,
            cross_attention_dim=cross_attention_dim,
        ).to(unet.device)
        custom_diffusion_attn_procs[name].load_state_dict(weights)
    else:
        custom_diffusion_attn_procs[name] = attention_class(
            train_kv=False,
            train_q_out=False,
            hidden_size=hidden_size,
            cross_attention_dim=cross_attention_dim,
        )
del st
unet.set_attn_processor(custom_diffusion_attn_procs)
custom_diffusion_layers = AttnProcsLayers(unet.attn_processors)
```

优化器被初始化以更新交叉注意力层参数：

```py
optimizer = optimizer_class(
    itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())
    if args.modifier_token is not None
    else custom_diffusion_layers.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

在[训练循环](https://github.com/huggingface/diffusers/blob/84cd9e8d01adb47f046b1ee449fc76a0c32dc4e2/examples/custom_diffusion/train_custom_diffusion.py#L1048)中，只更新您要学习的概念的嵌入是很重要的。这意味着将所有其他标记嵌入的梯度设置为零：

```py
if args.modifier_token is not None:
    if accelerator.num_processes > 1:
        grads_text_encoder = text_encoder.module.get_input_embeddings().weight.grad
    else:
        grads_text_encoder = text_encoder.get_input_embeddings().weight.grad
    index_grads_to_zero = torch.arange(len(tokenizer)) != modifier_token_id[0]
    for i in range(len(modifier_token_id[1:])):
        index_grads_to_zero = index_grads_to_zero & (
            torch.arange(len(tokenizer)) != modifier_token_id[i]
        )
    grads_text_encoder.data[index_grads_to_zero, :] = grads_text_encoder.data[
        index_grads_to_zero, :
    ].fill_(0)
```

## 启动脚本

一旦您完成了所有更改或对默认配置感到满意，您就可以启动训练脚本了！🚀

在本指南中，您将下载并使用这些示例[猫图像](https://www.cs.cmu.edu/~custom-diffusion/assets/data.zip)。如果您愿意，也可以创建并使用自己的数据集（请参阅创建用于训练的数据集指南）。

将环境变量`MODEL_NAME`设置为 Hub 上的模型 ID 或本地模型的路径，将`INSTANCE_DIR`设置为您刚下载猫图片的路径，将`OUTPUT_DIR`设置为您想要保存模型的路径。您将使用`<new1>`作为将新学习的嵌入与之关联的特殊词。该脚本将创建并保存模型检查点和一个 pytorch_custom_diffusion_weights.bin 文件到您的存储库中。

要使用 Weights and Biases 监视训练进度，请在训练命令中添加`--report_to=wandb`参数，并使用`--validation_prompt`指定验证提示。这对于调试和保存中间结果很有用。

如果您正在训练人脸，Custom Diffusion 团队发现以下参数效果很好：

+   `--learning_rate=5e-6`

+   `--max_train_steps`可以在 1000 和 2000 之间任意选择

+   `--freeze_model=crossattn`

+   至少使用 15-20 张图片进行训练

单一概念多个概念

```py
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export OUTPUT_DIR="path-to-save-model"
export INSTANCE_DIR="./data/cat"

accelerate launch train_custom_diffusion.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --class_data_dir=./real_reg/samples_cat/ \
  --with_prior_preservation \
  --real_prior \
  --prior_loss_weight=1.0 \
  --class_prompt="cat" \
  --num_class_images=200 \
  --instance_prompt="photo of a <new1> cat"  \
  --resolution=512  \
  --train_batch_size=2  \
  --learning_rate=1e-5  \
  --lr_warmup_steps=0 \
  --max_train_steps=250 \
  --scale_lr \
  --hflip  \
  --modifier_token "<new1>" \
  --validation_prompt="<new1> cat sitting in a bucket" \
  --report_to="wandb" \
  --push_to_hub
```

训练完成后，您可以使用您的新 Custom Diffusion 模型进行推断。

单一概念多个概念

```py
import torch
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16,
).to("cuda")
pipeline.unet.load_attn_procs("path-to-save-model", weight_name="pytorch_custom_diffusion_weights.bin")
pipeline.load_textual_inversion("path-to-save-model", weight_name="<new1>.bin")

image = pipeline(
    "<new1> cat sitting in a bucket",
    num_inference_steps=100,
    guidance_scale=6.0,
    eta=1.0,
).images[0]
image.save("cat.png")
```

## 下一步

恭喜您训练了一个 Custom Diffusion 模型！🎉 要了解更多：

+   阅读[文本到图像扩散的多概念定制](https://www.cs.cmu.edu/~custom-diffusion/)博文，了解有关 Custom Diffusion 团队实验结果的更多详细信息。
