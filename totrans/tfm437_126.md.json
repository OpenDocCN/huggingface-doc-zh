["```py\npip install deepspeed\n```", "```py\npip install transformers[deepspeed]\n```", "```py\ngit clone https://github.com/microsoft/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n--global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v \\\n--disable-pip-version-check 2>&1 | tee build.log\n```", "```py\nCUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capability())\"\n```", "```py\ngit clone https://github.com/microsoft/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\npython setup.py build_ext -j8 bdist_wheel\n```", "```py\npython -c \"import torch; print(torch.cuda.get_arch_list())\"\n```", "```py\nCUDA_VISIBLE_DEVICES=0 python -c \"import torch; \\\nprint(torch.cuda.get_device_properties(torch.device('cuda')))\"\n```", "```py\n_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)\n```", "```py\ntorch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```", "```py\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```", "```py\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```", "```py\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```", "```py\n{\n  \"zero_optimization\": {\n     \"stage\": 2,\n     \"offload_optimizer\": {\n         \"device\": \"cpu\",\n         \"pin_memory\": true\n     },\n     \"allgather_partitions\": true,\n     \"allgather_bucket_size\": 2e8,\n     \"reduce_scatter\": true,\n     \"reduce_bucket_size\": 2e8,\n     \"overlap_comm\": true,\n     \"contiguous_gradients\": true\n  }\n}\n```", "```py\n    deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...\n    ```", "```py\npython -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \\\n--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json\n```", "```py\nhostname1 slots=8\nhostname2 slots=8\n```", "```py\ndeepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \\\nyour_program.py <normal cl args> --deepspeed ds_config.json\n```", "```py\n#SBATCH --job-name=test-nodes        # name\n#SBATCH --nodes=2                    # nodes\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=10           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n\nexport GPUS_PER_NODE=8\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=9901\n\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n --master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\nyour_program.py <normal cl args> --deepspeed ds_config.json'\n```", "```py\nsbatch launch.slurm\n```", "```py\n{\n  \"checkpoint\": {\n    \"use_node_local_storage\": true\n  }\n}\n```", "```py\n# DeepSpeed requires a distributed environment even when only one process is used.\n# This emulates a launcher in the notebook\nimport os\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\n\n# Now proceed as normal, plus pass the deepspeed config file\ntraining_args = TrainingArguments(..., deepspeed=\"ds_config_zero3.json\")\ntrainer = Trainer(...)\ntrainer.train()\n```", "```py\n%%bash\ncat <<'EOT' > ds_config_zero3.json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\nEOT\n```", "```py\n!git clone https://github.com/huggingface/transformers\n!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...\n```", "```py\n%%bash\n\ngit clone https://github.com/huggingface/transformers\ncd transformers\ndeepspeed examples/pytorch/translation/run_translation.py ...\n```", "```py\ngit clone https://github.com/microsoft/DeepSpeedExamples\ncd DeepSpeedExamples\nfind . -name '*json'\n```", "```py\ngrep -i Lamb $(find . -name '*json')\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n}\n```", "```py\nTrainingArguments(..., deepspeed=\"/path/to/ds_config.json\")\n```", "```py\nds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)\nTrainingArguments(..., deepspeed=ds_config_dict)\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 5e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 5e8,\n        \"contiguous_gradients\": true\n    }\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"round_robin_gradients\": true\n    }\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 0\n    }\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 1\n    }\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": true,\n            \"buffer_count\": 4,\n            \"fast_init\": false\n        },\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": true,\n            \"buffer_count\": 5,\n            \"buffer_size\": 1e8,\n            \"max_in_cpu\": 1e9\n        },\n        \"aio\": {\n            \"block_size\": 262144,\n            \"queue_depth\": 32,\n            \"thread_count\": 1,\n            \"single_submit\": false,\n            \"overlap_events\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": 3e-5,\n            \"betas\": [0.8, 0.999],\n            \"eps\": 1e-8,\n            \"weight_decay\": 3e-7\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n            \"warmup_max_lr\": 3e-5,\n            \"warmup_num_steps\": 500\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": false\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": 3e-5,\n            \"betas\": [0.8, 0.999],\n            \"eps\": 1e-8,\n            \"weight_decay\": 3e-7\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n            \"warmup_max_lr\": 3e-5,\n            \"warmup_num_steps\": 500\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": 1e6,\n        \"stage3_prefetch_bucket_size\": 0.94e6,\n        \"stage3_param_persistence_threshold\": 1e4,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": false\n}\n```", "```py\n{\n   \"optimizer\": {\n       \"type\": \"AdamW\",\n       \"params\": {\n         \"lr\": \"auto\",\n         \"betas\": \"auto\",\n         \"eps\": \"auto\",\n         \"weight_decay\": \"auto\"\n       }\n   }\n}\n```", "```py\n{\n   \"optimizer\": {\n       \"type\": \"AdamW\",\n       \"params\": {\n         \"lr\": 0.001,\n         \"betas\": [0.8, 0.999],\n         \"eps\": 1e-8,\n         \"weight_decay\": 3e-7\n       }\n   }\n}\n```", "```py\n{\n   \"zero_allow_untested_optimizer\": true\n}\n```", "```py\n{\n   \"zero_force_ds_cpu_optimizer\": false\n}\n```", "```py\n{\n   \"scheduler\": {\n         \"type\": \"WarmupLR\",\n         \"params\": {\n             \"warmup_min_lr\": \"auto\",\n             \"warmup_max_lr\": \"auto\",\n             \"warmup_num_steps\": \"auto\"\n         }\n     }\n}\n```", "```py\n{\n   \"scheduler\": {\n         \"type\": \"WarmupLR\",\n         \"params\": {\n             \"warmup_min_lr\": 0,\n             \"warmup_max_lr\": 0.001,\n             \"warmup_num_steps\": 1000\n         }\n     }\n}\n```", "```py\n{\n   \"scheduler\": {\n         \"type\": \"WarmupDecayLR\",\n         \"params\": {\n             \"last_batch_iteration\": -1,\n             \"total_num_steps\": \"auto\",\n             \"warmup_min_lr\": \"auto\",\n             \"warmup_max_lr\": \"auto\",\n             \"warmup_num_steps\": \"auto\"\n         }\n     }\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": false,\n    }\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```", "```py\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    }\n}\n```", "```py\n{\n    \"bf16\": {\n        \"enabled\": true\n    }\n}\n```", "```py\n{\n    \"communication_data_type\": \"fp32\"\n}\n```", "```py\n\"amp\": {\n    \"enabled\": \"auto\",\n    \"opt_level\": \"auto\"\n}\n```", "```py\n{\n    \"amp\": {\n        \"enabled\": true,\n        \"opt_level\": \"O1\"\n    }\n}\n```", "```py\n{\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\"\n}\n```", "```py\n{\n    \"train_batch_size\": 12,\n    \"train_micro_batch_size_per_gpu\": 4\n}\n```", "```py\n{\n    \"gradient_accumulation_steps\": \"auto\"\n}\n```", "```py\n{\n    \"gradient_accumulation_steps\": 3\n}\n```", "```py\n{\n    \"gradient_clipping\": \"auto\"\n}\n```", "```py\n{\n    \"gradient_clipping\": 1.0\n}\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```", "```py\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = get_last_checkpoint(trainer.args.output_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```", "```py\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = os.path.join(trainer.args.output_dir, \"checkpoint-final\")\ntrainer.deepspeed.save_checkpoint(checkpoint_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```", "```py\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\nstate_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)  # already on cpu\nmodel = model.cpu()\nmodel.load_state_dict(state_dict)\n```", "```py\n$ ls -l output_dir/checkpoint-1/\n-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json\ndrwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/\n-rw-rw-r-- 1 stas stas   12 Mar 27 13:16 latest\n-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt\n-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch_model.bin\n-rw-rw-r-- 1 stas stas  623 Mar 27 20:42 scheduler.pt\n-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special_tokens_map.json\n-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model\n-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer_config.json\n-rw-rw-r-- 1 stas stas  339 Mar 27 20:42 trainer_state.json\n-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training_args.bin\n-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*\n```", "```py\npython zero_to_fp32.py . pytorch_model.bin\n```", "```py\nfrom transformers import T5ForConditionalGeneration, T5Config\nimport deepspeed\n\nwith deepspeed.zero.Init():\n    config = T5Config.from_pretrained(\"t5-small\")\n    model = T5ForConditionalGeneration(config)\n```", "```py\nfrom transformers import AutoModel, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(..., deepspeed=ds_config)\nmodel = AutoModel.from_pretrained(\"t5-small\")\ntrainer = Trainer(model=model, args=training_args, ...)\n```", "```py\ntensor([1.0], device=\"cuda:0\", dtype=torch.float16, requires_grad=True)\n```", "```py\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json\n```", "```py\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path t5-small --output_dir output_dir \\\n--do_eval --max_eval_samples 50 --warmup_steps 50  \\\n--max_source_length 128 --val_max_target_length 128 \\\n--overwrite_output_dir --per_device_eval_batch_size 4 \\\n--predict_with_generate --dataset_config \"ro-en\" --fp16 \\\n--source_lang en --target_lang ro --dataset_name wmt16 \\\n--source_prefix \"translate English to Romanian: \"\n```", "```py\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \\\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\"); \\\nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 1 GPU per node.\nSW: Model with 2783M total params, 65M largest layer params.\n  per CPU  |  per GPU |   Options\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n    0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1\n   15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0\n```", "```py\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \\\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\"); \\\nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 2 GPUs per node.\nSW: Model with 2783M total params, 65M largest layer params.\n  per CPU  |  per GPU |   Options\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n    0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1\n   31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0\n\n```", "```py\n    python -c 'import torch; print(f\"torch: {torch.__version__}\")'\n    python -c 'import transformers; print(f\"transformers: {transformers.__version__}\")'\n    python -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n    ```", "```py\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```", "```py\n0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144\n  1%|\u258c                                                                                                                    | 1/189 [00:00<01:26,  2.17it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0\n  1%|\u2588\u258f\n [...]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 14%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                   | 27/189 [00:14<01:13,  2.21it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                                                                  | 28/189 [00:14<01:13,  2.18it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 15%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                                                                                  | 29/189 [00:15<01:13,  2.18it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n[...]\n```", "```py\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel\nimport deepspeed\n\nds_config = {...}  # deepspeed config object or path to the file\n# must run before instantiating the model to detect zero 3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nmodel = AutoModel.from_pretrained(\"gpt2\")\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n```", "```py\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel, AutoConfig\nimport deepspeed\n\nds_config = {...}  # deepspeed config object or path to the file\n# must run before instantiating the model to detect zero 3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nconfig = AutoConfig.from_pretrained(\"gpt2\")\nmodel = AutoModel.from_config(config)\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n```", "```py\n( config_file_or_dict )\n```", "```py\n#!/usr/bin/env python\n\n# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model\n# into a single GPU\n#\n# 1\\. Use 1 GPU with CPU offload\n# 2\\. Or use multiple GPUs instead\n#\n# First you need to install deepspeed: pip install deepspeed\n#\n# Here we use a 3B \"bigscience/T0_3B\" model which needs about 15GB GPU RAM - so 1 largish or 2\n# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.\n#\n# To use a larger model like \"bigscience/T0\" which needs about 50GB, unless you have an 80GB GPU -\n# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to\n# process multiple inputs at once.\n#\n# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n# run faster if you don't want offload to CPU - so disable that section then.\n#\n# To deploy on 1 gpu:\n#\n# deepspeed --num_gpus 1 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=1 t0.py\n#\n# To deploy on 2 gpus:\n#\n# deepspeed --num_gpus 2 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=2 t0.py\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\nfrom transformers.integrations import HfDeepSpeedConfig\nimport deepspeed\nimport os\nimport torch\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n\n# distributed setup\nlocal_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\ntorch.cuda.set_device(local_rank)\ndeepspeed.init_distributed()\n\nmodel_name = \"bigscience/T0_3B\"\n\nconfig = AutoConfig.from_pretrained(model_name)\nmodel_hidden_size = config.d_model\n\n# batch size has to be divisible by world_size, but can be bigger than world_size\ntrain_batch_size = 1 * world_size\n\n# ds_config notes\n#\n# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n# faster.\n#\n# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.\n# all official t5 models are bf16-pretrained\n#\n# - set offload_param.device to \"none\" or completely remove the `offload_param` section if you don't\n# - want CPU offload\n#\n# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control\n# - which params should remain on gpus - the larger the value the smaller the offload size\n#\n# For indepth info on Deepspeed config see\n# https://huggingface.co/docs/transformers/main/main_classes/deepspeed\n\n# keeping the same format as json for consistency, except it uses lower case for true/false\n# fmt: off\nds_config = {\n    \"fp16\": {\n        \"enabled\": False\n    },\n    \"bf16\": {\n        \"enabled\": False\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": model_hidden_size * model_hidden_size,\n        \"stage3_prefetch_bucket_size\": 0.9 * model_hidden_size * model_hidden_size,\n        \"stage3_param_persistence_threshold\": 10 * model_hidden_size\n    },\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": train_batch_size,\n    \"train_micro_batch_size_per_gpu\": 1,\n    \"wall_clock_breakdown\": False\n}\n# fmt: on\n\n# next line instructs transformers to partition the model directly over multiple gpus using\n# deepspeed.zero.Init when model's `from_pretrained` method is called.\n#\n# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**\n#\n# otherwise the model will first be loaded normally and only partitioned at forward time which is\n# less efficient and when there is little CPU RAM may fail\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n\n# now a model can be loaded.\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# initialise Deepspeed ZeRO and store only the engine object\nds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\nds_engine.module.eval()  # inference\n\n# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.\n# If you use more GPUs adjust for more.\n# And of course if you have just one input to process you then need to pass the same string to both gpus\n# If you use only one GPU, then you will have only rank 0.\nrank = torch.distributed.get_rank()\nif rank == 0:\n    text_in = \"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\"\nelif rank == 1:\n    text_in = \"Is this review positive or negative? Review: this is the worst restaurant ever\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer.encode(text_in, return_tensors=\"pt\").to(device=local_rank)\nwith torch.no_grad():\n    outputs = ds_engine.module.generate(inputs, synced_gpus=True)\ntext_out = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"rank{rank}:\\n   in={text_in}\\n  out={text_out}\")\n```", "```py\n$ deepspeed --num_gpus 2 t0.py\nrank0:\n   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\n  out=Positive\nrank1:\n   in=Is this review positive or negative? Review: this is the worst restaurant ever\n  out=negative\n```", "```py\nRUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py\n```", "```py\nRUN_SLOW=1 pytest tests/deepspeed\n```"]