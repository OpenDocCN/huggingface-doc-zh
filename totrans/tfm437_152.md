# CPM

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cpm)

## 概述

CPM模型是由张政彦、韩旭、周浩、柯培、顾宇贤、叶德明、秦宇佳、苏玉生、季浩哲、关健、齐凡超、王晓智、郑亚楠、曾国阳、曹焕琦、陈胜奇、李代轩、孙振波、刘知远、黄民烈、韩文涛、唐杰、李娟姿、朱小燕、孙茂松在[CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413)中提出的。

论文摘要如下：

*预训练语言模型（PLMs）已被证明对各种下游NLP任务有益。最近，拥有1750亿参数和570GB训练数据的GPT-3因其少样本（甚至零样本）学习能力而引起了很多关注。然而，将GPT-3应用于解决中文NLP任务仍然具有挑战性，因为GPT-3的训练语料主要是英文，参数也不是公开的。在这份技术报告中，我们发布了在大规模中文训练数据上进行生成式预训练的中文预训练语言模型（CPM）。据我们所知，CPM拥有26亿参数和100GB中文训练数据，是目前最大的中文预训练语言模型，可以促进多个下游中文NLP任务，如对话、文章生成、填空测试和语言理解。大量实验证明，CPM在少样本（甚至零样本）学习的情况下在许多NLP任务上取得了强大的性能。*

该模型由[canwenxu](https://huggingface.co/canwenxu)贡献。原始实现可在此处找到：[https://github.com/TsinghuaAI/CPM-Generate](https://github.com/TsinghuaAI/CPM-Generate)

CPM的架构与GPT-2相同，除了分词方法。有关API参考信息，请参阅[GPT-2文档](gpt2)。

## CpmTokenizer

### `class transformers.CpmTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L38)

```py
( vocab_file do_lower_case = False remove_space = True keep_accents = False bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' sep_token = '<sep>' pad_token = '<pad>' cls_token = '<cls>' mask_token = '<mask>' additional_special_tokens = ['<eop>', '<eod>'] sp_model_kwargs: Optional = None **kwargs )
```

使用结巴分词工具进行预分词。用于CPM模型。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L245)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对应的ID列表（可选）。

返回

`List[int]`

具有适当特殊标记的[input IDs](../glossary#input-ids)列表。

通过连接和添加特殊标记，为序列分类任务构建模型输入，输入为单个序列或序列对。一个XLNet序列的格式如下：

+   单个序列：`X <sep> <cls>`

+   序列对：`A <sep> B <sep> <cls>`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L239)

```py
( tokens )
```

将一系列标记（子词的字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L300)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对应的ID列表（可选）。

返回

`List[int]`

根据给定序列的[token类型ID](../glossary#token-type-ids)列表。

从传递的两个序列创建一个掩码，用于在序列对分类任务中使用。一个XLNet

序列对掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm.py#L271)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

+   `already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经格式化为模型的特殊标记。

返回

`List[int]`

一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。当使用分词器`prepare_for_model`方法添加特殊标记时，将调用此方法。

## CpmTokenizerFast

### `class transformers.CpmTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L38)

```py
( vocab_file = None tokenizer_file = None do_lower_case = False remove_space = True keep_accents = False bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' sep_token = '<sep>' pad_token = '<pad>' cls_token = '<cls>' mask_token = '<mask>' additional_special_tokens = ['<eop>', '<eod>'] **kwargs )
```

使用结巴分词工具进行预分词。用于CPM模型。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L160)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

带有适当特殊标记的[input IDs](../glossary#input-ids)列表。

通过连接和添加特殊标记，从序列或序列对构建模型输入，用于序列分类任务。一个XLNet序列具有以下格式：

+   单个序列：`X <sep> <cls>`

+   序列对：`A <sep> B <sep> <cls>`

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cpm/tokenization_cpm_fast.py#L186)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

根据给定序列的[token type IDs](../glossary#token-type-ids)列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个XLNet

序列对掩码具有以下格式：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果 `token_ids_1` 是 `None`，则此方法仅返回掩码的第一部分（0s）。
