["```py\n( last_hidden_state: FloatTensor = None pooler_output: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( last_hidden_state: Array = None pooler_output: Array = None hidden_states: Optional = None attentions: Optional = None )\n```", "```py\n( vocab_size = 8192 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 16 num_channels = 3 use_mask_token = False use_absolute_position_embeddings = False use_relative_position_bias = False use_shared_relative_position_bias = False layer_scale_init_value = 0.1 drop_path_rate = 0.1 use_mean_pooling = True pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False semantic_loss_ignore_index = 255 out_features = None out_indices = None add_fpn = False reshape_hidden_states = True **kwargs )\n```", "```py\n>>> from transformers import BeitConfig, BeitModel\n\n>>> # Initializing a BEiT beit-base-patch16-224-pt22k style configuration\n>>> configuration = BeitConfig()\n\n>>> # Initializing a model (with random weights) from the beit-base-patch16-224-pt22k style configuration\n>>> model = BeitModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( *args **kwargs )\n```", "```py\n( images segmentation_maps = None **kwargs )\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';semantic_segmentation\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None rescale_factor: Union = 0.00392156862745098 do_rescale: bool = True do_normalize: bool = True image_mean: Union = None image_std: Union = None do_reduce_labels: bool = False **kwargs )\n```", "```py\n( images: Union segmentation_maps: Union = None do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( outputs target_sizes: List = None ) \u2192 export const metadata = 'undefined';semantic_segmentation\n```", "```py\n( config: BeitConfig add_pooling_layer: bool = True )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.beit.modeling_beit.BeitModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, BeitModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n>>> model = BeitModel.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 197, 768]\n```", "```py\n( config: BeitConfig )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n>>> model = BeitForMaskedImageModeling.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n\n>>> num_patches = (model.config.image_size // model.config.patch_size) ** 2\n>>> pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values\n>>> # create random boolean mask of shape (batch_size, num_patches)\n>>> bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()\n\n>>> outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n>>> loss, logits = outputs.loss, outputs.logits\n>>> list(logits.shape)\n[1, 196, 8192]\n```", "```py\n( config: BeitConfig )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, BeitForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224\")\n>>> model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```", "```py\n( config: BeitConfig )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, BeitForSemanticSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-finetuned-ade-640-640\")\n>>> model = BeitForSemanticSegmentation.from_pretrained(\"microsoft/beit-base-finetuned-ade-640-640\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> # logits are of shape (batch_size, num_labels, height, width)\n>>> logits = outputs.logits\n```", "```py\n( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, FlaxBeitModel\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n>>> model = FlaxBeitModel.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"np\")\n>>> outputs = model(**inputs)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, BeitForMaskedImageModeling\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n>>> model = BeitForMaskedImageModeling.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"np\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( config: BeitConfig input_shape = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( pixel_values bool_masked_pos = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, FlaxBeitForImageClassification\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224\")\n>>> model = FlaxBeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"np\")\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_class_idx = logits.argmax(-1).item()\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```"]