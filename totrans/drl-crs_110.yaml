- en: Offline vs. Online Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online](https://huggingface.co/learn/deep-rl-course/unitbonus3/offline-online)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (RL) is a framework **to build decision-making agents**.
    These agents aim to learn optimal behavior (policy) by interacting with the environment
    through **trial and error and receiving rewards as unique feedback**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent’s goal **is to maximize its cumulative reward**, called return. Because
    RL is based on the *reward hypothesis*: all goals can be described as the **maximization
    of the expected cumulative reward**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Reinforcement Learning agents **learn with batches of experience**. The
    question is, how do they collect it?:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unit bonus 3 thumbnail](../Images/88ecc42f5b64e1f4b4d04c4ba4639ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: A comparison between Reinforcement Learning in an Online and Offline setting,
    figure taken from [this post](https://offline-rl.github.io/)
  prefs: []
  type: TYPE_NORMAL
- en: 'In *online reinforcement learning*, which is what we’ve learned during this
    course, the agent **gathers data directly**: it collects a batch of experience
    by **interacting with the environment**. Then, it uses this experience immediately
    (or via some replay buffer) to learn from it (update its policy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But this implies that either you **train your agent directly in the real world
    or have a simulator**. If you don’t have one, you need to build it, which can
    be very complex (how to reflect the complex reality of the real world in an environment?),
    expensive, and insecure (if the simulator has flaws that may provide a competitive
    advantage, the agent will exploit them).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, in *offline reinforcement learning*, the agent only **uses
    data collected from other agents or human demonstrations**. It does **not interact
    with the environment**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a dataset** using one or more policies and/or human interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run **offline RL on this dataset** to learn a policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This method has one drawback: the *counterfactual queries problem*. What do
    we do if our agent **decides to do something for which we don’t have the data?**
    For instance, turning right on an intersection but we don’t have this trajectory.'
  prefs: []
  type: TYPE_NORMAL
- en: There exist some solutions on this topic, but if you want to know more about
    offline reinforcement learning, you can [watch this video](https://www.youtube.com/watch?v=k08N5a0gG0A)
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, we recommend you check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Offline Reinforcement Learning, Talk by Sergei Levine](https://www.youtube.com/watch?v=qgZPZREor5I)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open
    Problems](https://arxiv.org/abs/2005.01643)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section was written by [Thomas Simonini](https://twitter.com/ThomasSimonini)
  prefs: []
  type: TYPE_NORMAL
