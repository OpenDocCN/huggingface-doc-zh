# è¯„ä¼°æ‰©æ•£æ¨¡å‹

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/conceptual/evaluation`](https://huggingface.co/docs/diffusers/conceptual/evaluation)

![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb)

å¯¹äºåƒ[Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)è¿™æ ·çš„ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æ˜¯ä¸»è§‚çš„ã€‚ä½†ä½œä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜ï¼Œæˆ‘ä»¬ç»å¸¸ä¸å¾—ä¸åœ¨è®¸å¤šä¸åŒçš„å¯èƒ½æ€§ä¹‹é—´åšå‡ºè°¨æ…çš„é€‰æ‹©ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ GANã€Diffusion ç­‰ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©å…¶ä¸­ä¸€ä¸ªè€Œä¸æ˜¯å¦ä¸€ä¸ªå‘¢ï¼Ÿ

å¯¹è¿™äº›æ¨¡å‹çš„å®šæ€§è¯„ä¼°å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œå¹¶å¯èƒ½é”™è¯¯åœ°å½±å“å†³ç­–ã€‚ç„¶è€Œï¼Œå®šé‡æŒ‡æ ‡ä¸ä¸€å®šå¯¹åº”å›¾åƒè´¨é‡ã€‚å› æ­¤ï¼Œé€šå¸¸åœ¨é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è€Œä¸æ˜¯å¦ä¸€ä¸ªæ—¶ï¼Œå®šæ€§å’Œå®šé‡è¯„ä¼°çš„ç»“åˆæä¾›äº†æ›´å¼ºçš„ä¿¡å·ã€‚

åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…³äºå®šæ€§å’Œå®šé‡æ–¹æ³•è¯„ä¼°æ‰©æ•£æ¨¡å‹çš„éè¯¦å°½æ¦‚è¿°ã€‚å¯¹äºå®šé‡æ–¹æ³•ï¼Œæˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¦‚ä½•å°†å®ƒä»¬ä¸`diffusers`ä¸€èµ·å®æ–½ã€‚

æœ¬æ–‡æ¡£ä¸­æ˜¾ç¤ºçš„æ–¹æ³•ä¹Ÿå¯ç”¨äºè¯„ä¼°ä¸åŒçš„[å™ªå£°è°ƒåº¦å™¨](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç”Ÿæˆæ¨¡å‹ä¸å˜ã€‚

## åœºæ™¯

æˆ‘ä»¬æ¶µç›–ä»¥ä¸‹ç®¡é“çš„æ‰©æ•£æ¨¡å‹ï¼š

+   æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼ˆå¦‚[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)ï¼‰ã€‚

+   æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼Œå¦å¤–è¿˜ä¾èµ–äºè¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚[`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img)å’Œ[`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)ï¼‰ã€‚

+   ç±»åˆ«æ¡ä»¶çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚[`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)ï¼‰ã€‚

## å®šæ€§è¯„ä¼°

å®šæ€§è¯„ä¼°é€šå¸¸æ¶‰åŠå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œäººç±»è¯„ä¼°ã€‚è´¨é‡é€šè¿‡è¯¸å¦‚ç»„æˆæ€§ã€å›¾åƒæ–‡æœ¬å¯¹é½å’Œç©ºé—´å…³ç³»ç­‰æ–¹é¢æ¥è¡¡é‡ã€‚å¸¸è§æç¤ºä¸ºä¸»è§‚æŒ‡æ ‡æä¾›äº†ä¸€å®šç¨‹åº¦çš„ç»Ÿä¸€æ€§ã€‚DrawBench å’Œ PartiPrompts æ˜¯ç”¨äºå®šæ€§åŸºå‡†æµ‹è¯•çš„æç¤ºæ•°æ®é›†ã€‚DrawBench å’Œ PartiPrompts åˆ†åˆ«ç”±[Imagen](https://imagen.research.google/)å’Œ[Parti](https://parti.research.google/)å¼•å…¥ã€‚

æ¥è‡ª[å®˜æ–¹ Parti ç½‘ç«™](https://parti.research.google/)ï¼š

> PartiPromptsï¼ˆP2ï¼‰æ˜¯æˆ‘ä»¬ä½œä¸ºè¿™é¡¹å·¥ä½œçš„ä¸€éƒ¨åˆ†å‘å¸ƒçš„ä¸€ç»„è¶…è¿‡ 1600 ä¸ªè‹±æ–‡æç¤ºã€‚P2 å¯ç”¨äºè¡¡é‡æ¨¡å‹åœ¨å„ç§ç±»åˆ«å’ŒæŒ‘æˆ˜æ–¹é¢çš„èƒ½åŠ›ã€‚

![parti-prompts](img/a33baa9f457f5cb38b2e24aaacf3265c.png)

PartiPrompts å…·æœ‰ä»¥ä¸‹åˆ—ï¼š

+   æç¤º

+   æç¤ºçš„ç±»åˆ«ï¼ˆå¦‚â€œæŠ½è±¡â€ï¼Œâ€œä¸–ç•ŒçŸ¥è¯†â€ç­‰ï¼‰

+   åæ˜ éš¾åº¦çš„æŒ‘æˆ˜ï¼ˆå¦‚â€œåŸºæœ¬â€ï¼Œâ€œå¤æ‚â€ï¼Œâ€œå†™ä½œä¸ç¬¦å·â€ç­‰ï¼‰

è¿™äº›åŸºå‡†å…è®¸å¯¹ä¸åŒå›¾åƒç”Ÿæˆæ¨¡å‹è¿›è¡Œå¹¶æ’äººç±»è¯„ä¼°ã€‚

ä¸ºæ­¤ï¼ŒğŸ§¨ Diffusers å›¢é˜Ÿæ„å»ºäº†**Open Parti Prompts**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Parti Prompts çš„ç¤¾åŒºé©±åŠ¨çš„å®šæ€§åŸºå‡†ï¼Œç”¨äºæ¯”è¾ƒæœ€å…ˆè¿›çš„å¼€æºæ‰©æ•£æ¨¡å‹ï¼š

+   [æ‰“å¼€ Parti Prompts æ¸¸æˆ](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts)ï¼šå±•ç¤º 10 ä¸ª parti æç¤ºï¼Œç”¨æˆ·é€‰æ‹©æœ€é€‚åˆæç¤ºçš„å›¾åƒã€‚

+   [æ‰“å¼€ Parti Prompts æ’è¡Œæ¦œ](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)ï¼šå°†å½“å‰æœ€ä½³çš„å¼€æºæ‰©æ•£æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

ä¸ºäº†æ‰‹åŠ¨æ¯”è¾ƒå›¾åƒï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å‡ ä¸ª PartiPrompts ä¸Šä½¿ç”¨`diffusers`ã€‚

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸åŒæŒ‘æˆ˜ä¸­æŠ½æ ·çš„ä¸€äº›æç¤ºï¼šåŸºæœ¬ã€å¤æ‚ã€è¯­è¨€ç»“æ„ã€æƒ³è±¡åŠ›å’Œä¹¦å†™ä¸ç¬¦å·ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ PartiPrompts ä½œä¸ºä¸€ä¸ª[æ•°æ®é›†](https://huggingface.co/datasets/nateraw/parti-prompts)ã€‚

```py
from datasets import load_dataset

# prompts = load_dataset("nateraw/parti-prompts", split="train")
# prompts = prompts.shuffle()
# sample_prompts = [prompts[i]["Prompt"] for i in range(5)]

# Fixing these sample prompts in the interest of reproducibility.
sample_prompts = [
    "a corgi",
    "a hot air balloon with a yin-yang symbol, with the moon visible in the daytime sky",
    "a car with no windows",
    "a cube made of porcupine",
    'The saying "BE EXCELLENT TO EACH OTHER" written on a red brick wall with a graffiti image of a green alien wearing a tuxedo. A yellow fire hydrant is on a sidewalk in the foreground.',
]
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æç¤ºæ¥ä½¿ç”¨ Stable Diffusionï¼ˆ[v1-4 æ£€æŸ¥ç‚¹](https://huggingface.co/CompVis/stable-diffusion-v1-4)ï¼‰ç”Ÿæˆä¸€äº›å›¾åƒï¼š

```py
import torch

seed = 0
generator = torch.manual_seed(seed)

images = sd_pipeline(sample_prompts, num_images_per_prompt=1, generator=generator).images
```

![parti-prompts-14](img/bee6f1ffad12dea358f2f7f0b461f9a4.png)

æˆ‘ä»¬è¿˜å¯ä»¥ç›¸åº”åœ°è®¾ç½®`num_images_per_prompt`ä»¥æ¯”è¾ƒç›¸åŒæç¤ºçš„ä¸åŒå›¾åƒã€‚ä½¿ç”¨ä¸åŒæ£€æŸ¥ç‚¹ï¼ˆ[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ï¼‰è¿è¡Œç›¸åŒçš„ç®¡é“ï¼Œäº§ç”Ÿï¼š

![parti-prompts-15](img/8809682b839cee78ffe96b7e472b1f04.png)

ä¸€æ—¦ä½¿ç”¨å¤šä¸ªæ¨¡å‹ï¼ˆæ­£åœ¨è¯„ä¼°ä¸­ï¼‰ä»æ‰€æœ‰æç¤ºç”Ÿæˆäº†å‡ å¹…å›¾åƒï¼Œè¿™äº›ç»“æœå°†å‘ˆç°ç»™äººç±»è¯„ä¼°è€…è¿›è¡Œè¯„åˆ†ã€‚æœ‰å…³ DrawBench å’Œ PartiPrompts åŸºå‡†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒå®ƒä»¬å„è‡ªçš„è®ºæ–‡ã€‚

åœ¨æ¨¡å‹è®­ç»ƒæ—¶æŸ¥çœ‹ä¸€äº›æ¨ç†æ ·æœ¬æ˜¯æœ‰ç”¨çš„ï¼Œä»¥è¡¡é‡è®­ç»ƒè¿›åº¦ã€‚åœ¨æˆ‘ä»¬çš„[è®­ç»ƒè„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/)ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒè¿™ç§å®ç”¨ç¨‹åºï¼Œå¹¶é¢å¤–æ”¯æŒè®°å½•åˆ° TensorBoard å’Œ Weights & Biasesã€‚

## å®šé‡è¯„ä¼°

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŒ‡å¯¼æ‚¨å¦‚ä½•ä½¿ç”¨ä»¥ä¸‹å†…å®¹è¯„ä¼°ä¸‰ç§ä¸åŒçš„æ‰©æ•£ç®¡é“ï¼š

+   CLIP åˆ†æ•°

+   CLIP æ–¹å‘ç›¸ä¼¼æ€§

+   FID

### æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆ

[CLIP åˆ†æ•°](https://arxiv.org/abs/2104.08718)è¡¡é‡äº†å›¾åƒ-æ ‡é¢˜å¯¹çš„å…¼å®¹æ€§ã€‚æ›´é«˜çš„ CLIP åˆ†æ•°æ„å‘³ç€æ›´é«˜çš„å…¼å®¹æ€§ğŸ”¼ã€‚CLIP åˆ†æ•°æ˜¯å¯¹å®šæ€§æ¦‚å¿µâ€œå…¼å®¹æ€§â€çš„å®šé‡æµ‹é‡ã€‚å›¾åƒ-æ ‡é¢˜å¯¹çš„å…¼å®¹æ€§ä¹Ÿå¯ä»¥è¢«è§†ä¸ºå›¾åƒå’Œæ ‡é¢˜ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å‘ç° CLIP åˆ†æ•°ä¸äººç±»åˆ¤æ–­å…·æœ‰å¾ˆé«˜çš„ç›¸å…³æ€§ã€‚

è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸€ä¸ª StableDiffusionPipelineï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_ckpt = "CompVis/stable-diffusion-v1-4"
sd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to("cuda")
```

ä½¿ç”¨å¤šä¸ªæç¤ºç”Ÿæˆä¸€äº›å›¾åƒï¼š

```py
prompts = [
    "a photo of an astronaut riding a horse on mars",
    "A high tech solarpunk utopia in the Amazon rainforest",
    "A pikachu fine dining with a view to the Eiffel Tower",
    "A mecha robot in a favela in expressionist style",
    "an insect robot preparing a delicious meal",
    "A small cabin on top of a snowy mountain in the style of Disney, artstation",
]

images = sd_pipeline(prompts, num_images_per_prompt=1, output_type="np").images

print(images.shape)
# (6, 512, 512, 3)
```

ç„¶åï¼Œæˆ‘ä»¬è®¡ç®— CLIP åˆ†æ•°ã€‚

```py
from torchmetrics.functional.multimodal import clip_score
from functools import partial

clip_score_fn = partial(clip_score, model_name_or_path="openai/clip-vit-base-patch16")

def calculate_clip_score(images, prompts):
    images_int = (images * 255).astype("uint8")
    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()
    return round(float(clip_score), 4)

sd_clip_score = calculate_clip_score(images, prompts)
print(f"CLIP score: {sd_clip_score}")
# CLIP score: 35.7038
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆäº†ä¸€å¹…å›¾åƒã€‚å¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆå¤šå¹…å›¾åƒï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒä¸­å–å¹³å‡åˆ†æ•°ã€‚

ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ¯”è¾ƒä¸¤ä¸ªä¸ StableDiffusionPipeline å…¼å®¹çš„æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨è°ƒç”¨ç®¡é“æ—¶ä¼ é€’ä¸€ä¸ªç”Ÿæˆå™¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨[v1-4 Stable Diffusion æ£€æŸ¥ç‚¹](https://huggingface.co/CompVis/stable-diffusion-v1-4)ç”Ÿæˆå›¾åƒï¼š

```py
seed = 0
generator = torch.manual_seed(seed)

images = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type="np").images
```

ç„¶åæˆ‘ä»¬åŠ è½½[v1-5 æ£€æŸ¥ç‚¹](https://huggingface.co/runwayml/stable-diffusion-v1-5)æ¥ç”Ÿæˆå›¾åƒï¼š

```py
model_ckpt_1_5 = "runwayml/stable-diffusion-v1-5"
sd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=weight_dtype).to(device)

images_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type="np").images
```

æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒå®ƒä»¬çš„ CLIP åˆ†æ•°ï¼š

```py
sd_clip_score_1_4 = calculate_clip_score(images, prompts)
print(f"CLIP Score with v-1-4: {sd_clip_score_1_4}")
# CLIP Score with v-1-4: 34.9102

sd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)
print(f"CLIP Score with v-1-5: {sd_clip_score_1_5}")
# CLIP Score with v-1-5: 36.2137
```

ä¼¼ä¹[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)æ£€æŸ¥ç‚¹çš„æ€§èƒ½ä¼˜äºå…¶å‰èº«ã€‚ç„¶è€Œï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç”¨äºè®¡ç®— CLIP åˆ†æ•°çš„æç¤ºæ•°é‡ç›¸å½“ä½ã€‚ä¸ºäº†è¿›è¡Œæ›´å®é™…çš„è¯„ä¼°ï¼Œè¿™ä¸ªæ•°å­—åº”è¯¥æ›´é«˜ï¼Œå¹¶ä¸”æç¤ºåº”è¯¥æ›´åŠ å¤šæ ·åŒ–ã€‚

ä»æ„é€ ä¸Šçœ‹ï¼Œè¿™ä¸ªåˆ†æ•°å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ‡é¢˜æ˜¯ä»ç½‘ç»œä¸Šçˆ¬å–çš„ï¼Œå¹¶ä»äº’è”ç½‘ä¸Šä¸å›¾åƒç›¸å…³è”çš„`alt`å’Œç±»ä¼¼æ ‡ç­¾ä¸­æå–çš„ã€‚å®ƒä»¬ä¸ä¸€å®šä»£è¡¨äººç±»ç”¨æ¥æè¿°å›¾åƒçš„æ–¹å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œâ€œè®¾è®¡â€ä¸€äº›æç¤ºã€‚

### å›¾åƒæ¡ä»¶çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆç®¡é“ä¸è¾“å…¥å›¾åƒä»¥åŠæ–‡æœ¬æç¤ºä¸€èµ·è¿›è¡Œæ¡ä»¶åŒ–ã€‚è®©æˆ‘ä»¬ä»¥ StableDiffusionInstructPix2PixPipeline ä¸ºä¾‹ã€‚å®ƒå°†ç¼–è¾‘æŒ‡ä»¤ä½œä¸ºè¾“å…¥æç¤ºï¼Œå¹¶è¾“å…¥è¦ç¼–è¾‘çš„å›¾åƒã€‚

è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š

![edit-instruction](img/dd654c9963d1e235d32cfd4a6a4ce36b.png)

è¯„ä¼°è¿™æ ·ä¸€ä¸ªæ¨¡å‹çš„ä¸€ç§ç­–ç•¥æ˜¯æµ‹é‡ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å˜åŒ–çš„ä¸€è‡´æ€§ï¼ˆåœ¨[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)ç©ºé—´ä¸­ï¼‰ä¸ä¸¤ä¸ªå›¾åƒæ ‡é¢˜ä¹‹é—´çš„å˜åŒ–ä¹‹é—´çš„ä¸€è‡´æ€§ï¼ˆå¦‚[CLIP å¼•å¯¼çš„å›¾åƒç”Ÿæˆé¢†åŸŸè‡ªé€‚åº”](https://arxiv.org/abs/2108.00946)æ‰€ç¤ºï¼‰ã€‚è¿™è¢«ç§°ä¸ºâ€œ**CLIP æ–¹å‘ç›¸ä¼¼æ€§**â€ã€‚

+   æ ‡é¢˜ 1 å¯¹åº”äºè¦ç¼–è¾‘çš„è¾“å…¥å›¾åƒï¼ˆå›¾åƒ 1ï¼‰ã€‚

+   æ ‡é¢˜ 2 å¯¹åº”äºç¼–è¾‘åçš„å›¾åƒï¼ˆå›¾åƒ 2ï¼‰ã€‚å®ƒåº”è¯¥åæ˜ ç¼–è¾‘æŒ‡ä»¤ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå›¾è§£æ¦‚è¿°ï¼š

![edit-consistency](img/4246d7a1ea8f4bbc62b049bedecedc25.png)

æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªå°å‹æ•°æ®é›†æ¥å®ç°è¿™ä¸ªæŒ‡æ ‡ã€‚è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®é›†ã€‚

```py
from datasets import load_dataset

dataset = load_dataset("sayakpaul/instructpix2pix-demo", split="train")
dataset.features
```

```py
{'input': Value(dtype='string', id=None),
 'edit': Value(dtype='string', id=None),
 'output': Value(dtype='string', id=None),
 'image': Image(decode=True, id=None)}
```

è¿™é‡Œæœ‰ï¼š

+   `input` æ˜¯ä¸ `image` å¯¹åº”çš„æ ‡é¢˜ã€‚

+   `edit` è¡¨ç¤ºç¼–è¾‘æŒ‡ä»¤ã€‚

+   `output` è¡¨ç¤ºåæ˜  `edit` æŒ‡ä»¤çš„ä¿®æ”¹åæ ‡é¢˜ã€‚

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç¤ºä¾‹ã€‚

```py
idx = 0
print(f"Original caption: {dataset[idx]['input']}")
print(f"Edit instruction: {dataset[idx]['edit']}")
print(f"Modified caption: {dataset[idx]['output']}")
```

```py
Original caption: 2\. FAROE ISLANDS: An archipelago of 18 mountainous isles in the North Atlantic Ocean between Norway and Iceland, the Faroe Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'
Edit instruction: make the isles all white marble
Modified caption: 2\. WHITE MARBLE ISLANDS: An archipelago of 18 mountainous white marble isles in the North Atlantic Ocean between Norway and Iceland, the White Marble Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'
```

è¿™é‡Œæ˜¯å›¾ç‰‡ï¼š

```py
dataset[idx]["image"]
```

![edit-dataset](img/d57605cd3c5d841cf20ff0af193b5321.png)

æˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ç¼–è¾‘æŒ‡ä»¤ç¼–è¾‘æ•°æ®é›†çš„å›¾åƒï¼Œå¹¶è®¡ç®—æ–¹å‘ç›¸ä¼¼æ€§ã€‚

è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½ StableDiffusionInstructPix2PixPipelineï¼š

```py
from diffusers import StableDiffusionInstructPix2PixPipeline

instruct_pix2pix_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
    "timbrooks/instruct-pix2pix", torch_dtype=torch.float16
).to(device)
```

ç°åœ¨ï¼Œæˆ‘ä»¬è¿›è¡Œç¼–è¾‘ï¼š

```py
import numpy as np

def edit_image(input_image, instruction):
    image = instruct_pix2pix_pipeline(
        instruction,
        image=input_image,
        output_type="np",
        generator=generator,
    ).images[0]
    return image

input_images = []
original_captions = []
modified_captions = []
edited_images = []

for idx in range(len(dataset)):
    input_image = dataset[idx]["image"]
    edit_instruction = dataset[idx]["edit"]
    edited_image = edit_image(input_image, edit_instruction)

    input_images.append(np.array(input_image))
    original_captions.append(dataset[idx]["input"])
    modified_captions.append(dataset[idx]["output"])
    edited_images.append(edited_image)
```

ä¸ºäº†æµ‹é‡æ–¹å‘ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ CLIP çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼š

```py
from transformers import (
    CLIPTokenizer,
    CLIPTextModelWithProjection,
    CLIPVisionModelWithProjection,
    CLIPImageProcessor,
)

clip_id = "openai/clip-vit-large-patch14"
tokenizer = CLIPTokenizer.from_pretrained(clip_id)
text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to(device)
image_processor = CLIPImageProcessor.from_pretrained(clip_id)
image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to(device)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ç‰¹å®šçš„ CLIP æ£€æŸ¥ç‚¹ï¼Œå³ `openai/clip-vit-large-patch14`ã€‚è¿™æ˜¯å› ä¸º Stable Diffusion çš„é¢„è®­ç»ƒæ˜¯ä½¿ç”¨è¿™ä¸ª CLIP å˜ä½“è¿›è¡Œçš„ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/clip)ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å‡†å¤‡ä¸€ä¸ª PyTorch `nn.Module` æ¥è®¡ç®—æ–¹å‘ç›¸ä¼¼æ€§ï¼š

```py
import torch.nn as nn
import torch.nn.functional as F

class DirectionalSimilarity(nn.Module):
    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):
        super().__init__()
        self.tokenizer = tokenizer
        self.text_encoder = text_encoder
        self.image_processor = image_processor
        self.image_encoder = image_encoder

    def preprocess_image(self, image):
        image = self.image_processor(image, return_tensors="pt")["pixel_values"]
        return {"pixel_values": image.to(device)}

    def tokenize_text(self, text):
        inputs = self.tokenizer(
            text,
            max_length=self.tokenizer.model_max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {"input_ids": inputs.input_ids.to(device)}

    def encode_image(self, image):
        preprocessed_image = self.preprocess_image(image)
        image_features = self.image_encoder(**preprocessed_image).image_embeds
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        return image_features

    def encode_text(self, text):
        tokenized_text = self.tokenize_text(text)
        text_features = self.text_encoder(**tokenized_text).text_embeds
        text_features = text_features / text_features.norm(dim=1, keepdim=True)
        return text_features

    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):
        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)
        return sim_direction

    def forward(self, image_one, image_two, caption_one, caption_two):
        img_feat_one = self.encode_image(image_one)
        img_feat_two = self.encode_image(image_two)
        text_feat_one = self.encode_text(caption_one)
        text_feat_two = self.encode_text(caption_two)
        directional_similarity = self.compute_directional_similarity(
            img_feat_one, img_feat_two, text_feat_one, text_feat_two
        )
        return directional_similarity
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ `DirectionalSimilarity`ã€‚

```py
dir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)
scores = []

for i in range(len(input_images)):
    original_image = input_images[i]
    original_caption = original_captions[i]
    edited_image = edited_images[i]
    modified_caption = modified_captions[i]

    similarity_score = dir_similarity(original_image, edited_image, original_caption, modified_caption)
    scores.append(float(similarity_score.detach().cpu()))

print(f"CLIP directional similarity: {np.mean(scores)}")
# CLIP directional similarity: 0.0797976553440094
```

ä¸ CLIP åˆ†æ•°ç±»ä¼¼ï¼ŒCLIP æ–¹å‘ç›¸ä¼¼æ€§è¶Šé«˜ï¼Œæ•ˆæœè¶Šå¥½ã€‚

åº”è¯¥æ³¨æ„çš„æ˜¯ `StableDiffusionInstructPix2PixPipeline` å…¬å¼€äº†ä¸¤ä¸ªå‚æ•°ï¼Œå³ `image_guidance_scale` å’Œ `guidance_scale`ï¼Œè®©æ‚¨æ§åˆ¶æœ€ç»ˆç¼–è¾‘å›¾åƒçš„è´¨é‡ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•è¿™ä¸¤ä¸ªå‚æ•°ï¼Œå¹¶æŸ¥çœ‹å¯¹æ–¹å‘ç›¸ä¼¼æ€§çš„å½±å“ã€‚

æˆ‘ä»¬å¯ä»¥æ‰©å±•è¿™ä¸ªæŒ‡æ ‡çš„æ€æƒ³ï¼Œæ¥è¡¡é‡åŸå§‹å›¾åƒå’Œç¼–è¾‘ç‰ˆæœ¬æœ‰å¤šç›¸ä¼¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€æ‰§è¡Œ `F.cosine_similarity(img_feat_two, img_feat_one)`ã€‚å¯¹äºè¿™ç§ç±»å‹çš„ç¼–è¾‘ï¼Œæˆ‘ä»¬ä»ç„¶å¸Œæœ›å°½å¯èƒ½ä¿ç•™å›¾åƒçš„ä¸»è¦è¯­ä¹‰ï¼Œå³é«˜ç›¸ä¼¼æ€§åˆ†æ•°ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡æ ‡æ¥è¯„ä¼°ç±»ä¼¼çš„æµæ°´çº¿ï¼Œæ¯”å¦‚[`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline)ã€‚

CLIP åˆ†æ•°å’Œ CLIP æ–¹å‘ç›¸ä¼¼æ€§éƒ½ä¾èµ–äº CLIP æ¨¡å‹ï¼Œè¿™å¯èƒ½ä¼šä½¿è¯„ä¼°äº§ç”Ÿåè§ã€‚

***å½“è¯„ä¼°çš„æ¨¡å‹åœ¨å¤§å‹å›¾åƒå­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ˆä¾‹å¦‚[LAION-5B æ•°æ®é›†](https://laion.ai/blog/laion-5b/)ï¼‰æ—¶ï¼Œæ‰©å±•è¯¸å¦‚ ISã€FIDï¼ˆç¨åè®¨è®ºï¼‰æˆ– KID ç­‰æŒ‡æ ‡å¯èƒ½ä¼šå¾ˆå›°éš¾ã€‚***è¿™æ˜¯å› ä¸ºè¿™äº›æŒ‡æ ‡çš„åŸºç¡€æ˜¯ InceptionNetï¼ˆåœ¨ ImageNet-1k æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼‰ç”¨äºæå–ä¸­é—´å›¾åƒç‰¹å¾ã€‚Stable Diffusion çš„é¢„è®­ç»ƒæ•°æ®é›†å¯èƒ½ä¸ InceptionNet çš„é¢„è®­ç»ƒæ•°æ®é›†æœ‰é™çš„é‡å ï¼Œå› æ­¤åœ¨è¿™é‡Œä¸æ˜¯è¿›è¡Œç‰¹å¾æå–çš„å¥½é€‰æ‹©ã€‚

***ä½¿ç”¨ä¸Šè¿°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°ç±»åˆ«æ¡ä»¶çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œ[DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)ã€‚å®ƒæ˜¯åœ¨ ImageNet-1k ç±»åˆ«ä¸Šè¿›è¡Œæ¡ä»¶é¢„è®­ç»ƒçš„ã€‚***

### ç±»åˆ«æ¡ä»¶çš„å›¾åƒç”Ÿæˆ

ç±»åˆ«æ¡ä»¶ç”Ÿæˆæ¨¡å‹é€šå¸¸æ˜¯åœ¨ç±»åˆ«æ ‡è®°çš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ï¼Œä¾‹å¦‚[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ã€‚ç”¨äºè¯„ä¼°è¿™äº›æ¨¡å‹çš„æµè¡ŒæŒ‡æ ‡åŒ…æ‹¬ FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ã€Kernel Inception Distanceï¼ˆKIDï¼‰å’Œ Inception Scoreï¼ˆISï¼‰ã€‚åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äº FIDï¼ˆ[Heusel ç­‰äºº](https://arxiv.org/abs/1706.08500)ï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)æ¥è®¡ç®—å®ƒï¼Œè¯¥å·¥å…·åœ¨åº•å±‚ä½¿ç”¨[DiT æ¨¡å‹](https://arxiv.org/abs/2212.09748)ã€‚

FID æ—¨åœ¨è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æ ¹æ®[è¿™ä¸ªèµ„æº](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid)ï¼š

> FrÃ©chet Inception Distance æ˜¯è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†ä¹‹é—´ç›¸ä¼¼æ€§çš„æŒ‡æ ‡ã€‚å®ƒè¢«è¯æ˜ä¸äººç±»å¯¹è§†è§‰è´¨é‡çš„åˆ¤æ–­ç›¸å…³ï¼Œå¹¶ä¸”æœ€å¸¸ç”¨äºè¯„ä¼°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ ·æœ¬çš„è´¨é‡ã€‚FID é€šè¿‡è®¡ç®—é€‚åº”äº Inception ç½‘ç»œç‰¹å¾è¡¨ç¤ºçš„ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„ FrÃ©chet è·ç¦»æ¥è®¡ç®—ã€‚

è¿™ä¸¤ä¸ªæ•°æ®é›†æœ¬è´¨ä¸Šæ˜¯çœŸå®å›¾åƒæ•°æ®é›†å’Œå‡å›¾åƒæ•°æ®é›†ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ç”Ÿæˆçš„å›¾åƒï¼‰ã€‚FID é€šå¸¸æ˜¯ä½¿ç”¨ä¸¤ä¸ªå¤§å‹æ•°æ®é›†æ¥è®¡ç®—çš„ã€‚ä½†æ˜¯ï¼Œåœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªå°å‹æ•°æ®é›†ã€‚

è®©æˆ‘ä»¬é¦–å…ˆä» ImageNet-1k è®­ç»ƒé›†ä¸­ä¸‹è½½ä¸€äº›å›¾åƒï¼š

```py
from zipfile import ZipFile
import requests

def download(url, local_filepath):
    r = requests.get(url)
    with open(local_filepath, "wb") as f:
        f.write(r.content)
    return local_filepath

dummy_dataset_url = "https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip"
local_filepath = download(dummy_dataset_url, dummy_dataset_url.split("/")[-1])

with ZipFile(local_filepath, "r") as zipper:
    zipper.extractall(".")
```

```py
from PIL import Image
import os

dataset_path = "sample-imagenet-images"
image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])

real_images = [np.array(Image.open(path).convert("RGB")) for path in image_paths]
```

è¿™äº›æ˜¯æ¥è‡ªä»¥ä¸‹ ImageNet-1k ç±»åˆ«çš„ 10 å¼ å›¾åƒï¼šâ€œå¡å¸¦æ’­æ”¾å™¨â€ã€â€œé“¾é”¯â€ï¼ˆx2ï¼‰ã€â€œæ•™å ‚â€ã€â€œåŠ æ²¹æ³µâ€ï¼ˆx3ï¼‰ã€â€œé™è½ä¼â€ï¼ˆx2ï¼‰å’Œâ€œé²‘é±¼â€ã€‚

![çœŸå®å›¾åƒ](img/f9f0dfbeb754ed8d4f547460644fd7bd.png)

*çœŸå®å›¾åƒã€‚*

ç°åœ¨å›¾åƒå·²åŠ è½½ï¼Œè®©æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œä¸€äº›è½»é‡çº§é¢„å¤„ç†ï¼Œä»¥ä¾¿ç”¨äº FID è®¡ç®—ã€‚

```py
from torchvision.transforms import functional as F

def preprocess_image(image):
    image = torch.tensor(image).unsqueeze(0)
    image = image.permute(0, 3, 1, 2) / 255.0
    return F.center_crop(image, (256, 256))

real_images = torch.cat([preprocess_image(image) for image in real_images])
print(real_images.shape)
# torch.Size([10, 3, 256, 256])
```

ç°åœ¨æˆ‘ä»¬åŠ è½½[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)æ¥ç”ŸæˆåŸºäºä¸Šè¿°ç±»åˆ«çš„å›¾åƒã€‚

```py
from diffusers import DiTPipeline, DPMSolverMultistepScheduler

dit_pipeline = DiTPipeline.from_pretrained("facebook/DiT-XL-2-256", torch_dtype=torch.float16)
dit_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(dit_pipeline.scheduler.config)
dit_pipeline = dit_pipeline.to("cuda")

words = [
    "cassette player",
    "chainsaw",
    "chainsaw",
    "church",
    "gas pump",
    "gas pump",
    "gas pump",
    "parachute",
    "parachute",
    "tench",
]

class_ids = dit_pipeline.get_label_ids(words)
output = dit_pipeline(class_labels=class_ids, generator=generator, output_type="np")

fake_images = output.images
fake_images = torch.tensor(fake_images)
fake_images = fake_images.permute(0, 3, 1, 2)
print(fake_images.shape)
# torch.Size([10, 3, 256, 256])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[`torchmetrics`](https://torchmetrics.readthedocs.io/)æ¥è®¡ç®— FIDã€‚

```py
from torchmetrics.image.fid import FrechetInceptionDistance

fid = FrechetInceptionDistance(normalize=True)
fid.update(real_images, real=True)
fid.update(fake_images, real=False)

print(f"FID: {float(fid.compute())}")
# FID: 177.7147216796875
```

FID å€¼è¶Šä½ï¼Œæ•ˆæœè¶Šå¥½ã€‚è¿™é‡Œæœ‰å‡ ä¸ªå› ç´ å¯èƒ½ä¼šå½±å“ FIDï¼š

+   å›¾åƒæ•°é‡ï¼ˆçœŸå®å’Œå‡çš„ï¼‰

+   æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥çš„éšæœºæ€§

+   æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ¨ç†æ­¥éª¤æ•°é‡

+   æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è°ƒåº¦ç¨‹åº

å¯¹äºæœ€åä¸¤ç‚¹ï¼Œå› æ­¤æœ€å¥½çš„åšæ³•æ˜¯åœ¨ä¸åŒçš„ç§å­å’Œæ¨ç†æ­¥éª¤ä¸Šè¿è¡Œè¯„ä¼°ï¼Œç„¶åæŠ¥å‘Šå¹³å‡ç»“æœã€‚

FID çš„ç»“æœå¾€å¾€å¾ˆè„†å¼±ï¼Œå› ä¸ºå®ƒå–å†³äºè®¸å¤šå› ç´ ï¼š

+   åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ç‰¹å®š Inception æ¨¡å‹ã€‚

+   è®¡ç®—çš„å®ç°å‡†ç¡®æ€§ã€‚

+   å›¾åƒæ ¼å¼ï¼ˆå¦‚æœæˆ‘ä»¬ä» PNG å¼€å§‹ä¸ä» JPG å¼€å§‹ä¸åŒï¼‰ã€‚

ç‰¢è®°è¿™ä¸€ç‚¹ï¼ŒFID åœ¨æ¯”è¾ƒç›¸ä¼¼çš„è¿è¡Œæ—¶é€šå¸¸æœ€æœ‰ç”¨ï¼Œä½†è¦å¤ç°è®ºæ–‡ç»“æœå¾ˆéš¾ï¼Œé™¤éä½œè€…ä»”ç»†æŠ«éœ² FID æµ‹é‡ä»£ç ã€‚

è¿™äº›è¦ç‚¹ä¹Ÿé€‚ç”¨äºå…¶ä»–ç›¸å…³æŒ‡æ ‡ï¼Œå¦‚ KID å’Œ ISã€‚

ä½œä¸ºæœ€åä¸€æ­¥ï¼Œè®©æˆ‘ä»¬ç›´è§‚åœ°æ£€æŸ¥ä¸€ä¸‹`fake_images`ã€‚

![å‡å›¾åƒ](img/d9d2a296bc3c224017b8ff324b7d9c84.png)

*å‡å›¾åƒã€‚*
