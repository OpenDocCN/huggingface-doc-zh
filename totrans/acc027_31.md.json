["```py\nimport torch\n\nmy_model = ModelClass(...)\nstate_dict = torch.load(checkpoint_file)\nmy_model.load_state_dict(state_dict)\n```", "```py\nfrom accelerate import init_empty_weights\n\nwith init_empty_weights():\n    my_model = ModelClass(...)\n```", "```py\nwith init_empty_weights():\n    model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n```", "```py\nfirst_state_dict.bin\nindex.json\nsecond_state_dict.bin\n```", "```py\n{\n  \"linear1.weight\": \"first_state_dict.bin\",\n  \"linear1.bias\": \"first_state_dict.bin\",\n  \"linear2.weight\": \"second_state_dict.bin\",\n  \"linear2.bias\": \"second_state_dict.bin\"\n}\n```", "```py\npip install huggingface_hub\n```", "```py\nfrom huggingface_hub import snapshot_download\ncheckpoint = \"marcsun13/gpt2-xl-linear-sharded\"\nweights_location = snapshot_download(repo_id=checkpoint)\n```", "```py\ngit clone https://github.com/karpathy/minGPT.git\npip install minGPT/\n```", "```py\nfrom accelerate import init_empty_weights\nfrom mingpt.model import GPT\n\nmodel_config = GPT.get_default_config()\nmodel_config.model_type = 'gpt2-xl'\nmodel_config.vocab_size = 50257\nmodel_config.block_size = 1024\n\nwith init_empty_weights():\n    model = GPT(model_config)\n```", "```py\nfrom accelerate import load_checkpoint_and_dispatch\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=weights_location, device_map=\"auto\", no_split_module_classes=['Block']\n)\n```", "```py\nmodel.hf_device_map\n```", "```py\n{'transformer.wte': 0,\n 'transformer.wpe': 0,\n 'transformer.drop': 0,\n 'transformer.h.0': 0,\n ...\n 'transformer.h.21': 0, \n 'transformer.h.22': 1, \n 'transformer.h.23': 1, \n 'transformer.h.24': 1,\n ...\n 'transformer.h.47': 1, \n 'transformer.ln_f': 1, \n 'lm_head': 1}\n```", "```py\ndevice_map = {\n    \"transformer.wte\": \"cpu\",\n    \"transformer.wpe\": 0,\n    \"transformer.drop\": \"cpu\",\n    \"transformer.h.0\": \"disk\"\n}\n\nmodel = load_checkpoint_and_dispatch(\n    model, checkpoint=weights_location, device_map=device_map\n)\n\n```", "```py\nfrom mingpt.bpe import BPETokenizer\ntokenizer = BPETokenizer()\ninputs = tokenizer(\"Hello, my name is\").to(0)\n\noutputs = model.generate(x1, max_new_tokens=10, do_sample=False)[0]\ntokenizer.decode(outputs.cpu().squeeze())\n```", "```py\nfrom accelerate import infer_auto_device_map\n\ndevice_map = infer_auto_device_map(my_model, max_memory={0: \"10GiB\", 1: \"10GiB\", \"cpu\": \"30GiB\"})\n```", "```py\nmax_memory = {0: \"30GIB\", 1: \"46GIB\", 2: \"46GIB\", 3: \"46GIB\", 4: \"46GIB\", 5: \"46GIB\", 6: \"46GIB\", 7: \"46GIB\"}\n```", "```py\ndevice_map = {\"block1\": 0, \"block2\": 1}\n```", "```py\ndevice_map = {\"block1\": 0, \"block2.linear1\": 0, \"block2.linear2\": 1, \"block2.linear3\": 1}\n```", "```py\ndevice_map = {\"block1\": 0, \"block2.linear1\": 1, \"block2.linear2\": 1}\n```", "```py\ncpu_offload(model, execution_device)\n```", "```py\nmodel_1, hook_1 = cpu_offload_with_hook(model_1, execution_device)\nmodel_2, hook_2 = cpu_offload_with_hook(model_2, execution_device, prev_module_hook=hook_1)\nmodel_3, hook_3 = cpu_offload_with_hook(model_3, execution_device, prev_module_hook=hook_2)\n\nhid_1 = model_1(input)\nfor i in range(50):\n    # model1 is offloaded on the CPU at the first iteration, model 2 stays on the GPU for this whole loop.\n    hid_2 = model_2(hid_1)\n# model2 is offloaded to the CPU just before this forward.\nhid_3 = model_3(hid_3)\n\n# For model3, you need to manually call the hook offload method.\nhook_3.offload()\n```", "```py\ndisk_offload(model, offload_dir, execution_device)\n```"]