# 故障排除

> 原文链接：[`huggingface.co/docs/transformers/v4.37.2/en/troubleshooting`](https://huggingface.co/docs/transformers/v4.37.2/en/troubleshooting)

有时会出现错误，但我们在这里帮助您！本指南涵盖了我们见过的一些最常见问题以及您可以如何解决它们。但是，本指南并不旨在成为每个🤗 Transformers 问题的全面集合。如需更多有关故障排除的帮助，请尝试：

[`www.youtube-nocookie.com/embed/S2EEG3JIt2A`](https://www.youtube-nocookie.com/embed/S2EEG3JIt2A)

1.  在[论坛](https://discuss.huggingface.co/)上寻求帮助。您可以将问题发布到特定类别，如[初学者](https://discuss.huggingface.co/c/beginners/5)或[🤗 Transformers](https://discuss.huggingface.co/c/transformers/9)。请确保编写一个具有一些可重现代码的良好描述性论坛帖子，以最大程度地提高解决问题的可能性！

[`www.youtube-nocookie.com/embed/_PAli-V4wj0`](https://www.youtube-nocookie.com/embed/_PAli-V4wj0)

1.  如果是与库相关的错误，请在🤗 Transformers 存储库上创建一个[Issue](https://github.com/huggingface/transformers/issues/new/choose)。尽量包含尽可能多描述错误的信息，以帮助我们更好地找出问题所在以及如何修复它。

1.  如果您使用较旧版本的🤗 Transformers，请查看迁移指南，因为在版本之间引入了一些重要更改。

有关故障排除和获取帮助的更多详细信息，请查看 Hugging Face 课程的[第八章](https://huggingface.co/course/chapter8/1?fw=pt)。

## 防火墙环境

一些云端和内部设置的 GPU 实例被防火墙阻止对外部连接，导致连接错误。当您的脚本尝试下载模型权重或数据集时，下载将挂起，然后超时并显示以下消息：

```py
ValueError: Connection error, and we cannot find the requested files in the cached path.
Please try again or make sure your Internet connection is on.
```

在这种情况下，您应该尝试在离线模式下运行🤗 Transformers 以避免连接错误。

## CUDA 内存不足

在没有适当硬件的情况下训练拥有数百万参数的大型模型可能会很具挑战性。当 GPU 内存不足时，您可能会遇到的常见错误是：

```py
CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 9.70 GiB already allocated; 179.81 MiB free; 9.85 GiB reserved in total by PyTorch)
```

以下是一些潜在的解决方案，您可以尝试减少内存使用：

+   减少 TrainingArguments 中的`per_device_train_batch_size`值。

+   尝试在 TrainingArguments 中使用`gradient_accumulation_steps`来有效增加总体批量大小。

有关节省内存技术的更多详细信息，请参考性能指南。

## 无法加载保存的 TensorFlow 模型

TensorFlow 的[model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)方法将整个模型（架构、权重、训练配置）保存在单个文件中。然而，当您再次加载模型文件时，可能会遇到错误，因为🤗 Transformers 可能不会加载模型文件中的所有与 TensorFlow 相关的对象。为避免保存和加载 TensorFlow 模型时出现问题，我们建议您：

+   使用[`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model)将模型权重保存为`h5`文件扩展名，然后使用 from_pretrained()重新加载模型：

```py
>>> from transformers import TFPreTrainedModel
>>> from tensorflow import keras

>>> model.save_weights("some_folder/tf_model.h5")
>>> model = TFPreTrainedModel.from_pretrained("some_folder")
```

+   使用`~TFPretrainedModel.save_pretrained`保存模型，然后使用 from_pretrained()再次加载它：

```py
>>> from transformers import TFPreTrainedModel

>>> model.save_pretrained("path_to/model")
>>> model = TFPreTrainedModel.from_pretrained("path_to/model")
```

## ImportError

您可能会遇到另一种常见错误，特别是对于新发布的模型，即`ImportError`：

```py
ImportError: cannot import name 'ImageGPTImageProcessor' from 'transformers' (unknown location)
```

对于这些错误类型，请确保您已安装了最新版本的🤗 Transformers 以访问最新的模型：

```py
pip install transformers --upgrade
```

## CUDA error: device-side assert triggered

有时您可能会遇到有关设备代码错误的通用 CUDA 错误。

```py
RuntimeError: CUDA error: device-side assert triggered
```

您应该首先尝试在 CPU 上运行代码，以获得更详细的错误消息。将以下环境变量添加到您的代码开头以切换到 CPU：

```py
>>> import os

>>> os.environ["CUDA_VISIBLE_DEVICES"] = ""
```

另一个选项是从 GPU 获取更好的回溯。将以下环境变量添加到您的代码开头，以使回溯指向错误源：

```py
>>> import os

>>> os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
```

## 当填充标记未被屏蔽时输出不正确

在某些情况下，如果`input_ids`包含填充标记，则输出的`hidden_state`可能是不正确的。为了演示，加载一个模型和分词器。您可以访问模型的`pad_token_id`以查看其值。对于一些模型，`pad_token_id`可能为`None`，但您总是可以手动设置它。

```py
>>> from transformers import AutoModelForSequenceClassification
>>> import torch

>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
>>> model.config.pad_token_id
0
```

以下示例显示了不屏蔽填充标记时的输出：

```py
>>> input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [ 0.1317, -0.1683]], grad_fn=<AddmmBackward0>)
```

以下是第二个序列的实际输出：

```py
>>> input_ids = torch.tensor([[7592]])
>>> output = model(input_ids)
>>> print(output.logits)
tensor([[-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

大多数情况下，您应该为您的模型提供一个`attention_mask`来忽略填充标记，以避免这种潜在错误。现在第二个序列的输出与其实际输出匹配：

默认情况下，分词器根据特定分词器的默认值为您创建`attention_mask`。

```py
>>> attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
>>> output = model(input_ids, attention_mask=attention_mask)
>>> print(output.logits)
tensor([[ 0.0082, -0.2307],
        [-0.1008, -0.4061]], grad_fn=<AddmmBackward0>)
```

🤗 Transformers 不会自动创建`attention_mask`来屏蔽填充标记，如果提供了填充标记，因为：

+   一些模型没有填充标记。

+   对于某些用例，用户希望模型关注填充标记。

## ValueError: Unrecognized configuration class XYZ for this kind of AutoModel

通常，我们建议使用 AutoModel 类来加载预训练模型的实例。这个类可以根据配置自动推断和加载给定检查点中的正确架构。如果在从检查点加载模型时看到`ValueError`，这意味着 Auto 类无法从给定检查点中的配置找到到您尝试加载的模型类型的映射。最常见的情况是，当检查点不支持给定任务时会发生这种情况。例如，在以下示例中，您将看到此错误，因为没有用于问答的 GPT2：

```py
>>> from transformers import AutoProcessor, AutoModelForQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("gpt2-medium")
>>> model = AutoModelForQuestionAnswering.from_pretrained("gpt2-medium")
ValueError: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForQuestionAnswering.
Model type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...
```
