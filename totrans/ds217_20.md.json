["```py\n>>> from datasets import Dataset\n>>> data = [[1, 2],[3, 4]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': tensor([1, 2])}\n>>> ds[:2]\n{'data': tensor([[1, 2],\n         [3, 4]])}\n```", "```py\n>>> import torch\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n>>> ds = ds.with_format(\"torch\", device=device)\n>>> ds[0]\n{'data': tensor([1, 2], device='cuda:0')}\n```", "```py\n>>> from datasets import Dataset\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> ds = Dataset.from_dict({\"data\": data})\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': [tensor([1, 2]), tensor([3, 4])]}\n```", "```py\n>>> from datasets import Dataset, Features, Array2D\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\n>>> features = Features({\"data\": Array2D(shape=(2, 2), dtype='int32')})\n>>> ds = Dataset.from_dict({\"data\": data}, features=features)\n>>> ds = ds.with_format(\"torch\")\n>>> ds[0]\n{'data': tensor([[1, 2],\n         [3, 4]])}\n>>> ds[:2]\n{'data': tensor([[[1, 2],\n          [3, 4]],\n\n         [[5, 6],\n          [7, 8]]])}\n```", "```py\n>>> from datasets import Dataset, Features, ClassLabel\n>>> labels = [0, 0, 1]\n>>> features = Features({\"label\": ClassLabel(names=[\"negative\", \"positive\"])})\n>>> ds = Dataset.from_dict({\"label\": labels}, features=features) \n>>> ds = ds.with_format(\"torch\")  \n>>> ds[:3]\n{'label': tensor([0, 0, 1])}\n```", "```py\n>>> from datasets import Dataset, Features, Audio, Image\n>>> images = [\"path/to/image.png\"] * 10\n>>> features = Features({\"image\": Image()})\n>>> ds = Dataset.from_dict({\"image\": images}, features=features) \n>>> ds = ds.with_format(\"torch\")\n>>> ds[0][\"image\"].shape\ntorch.Size([512, 512, 4])\n>>> ds[0]\n{'image': tensor([[[255, 215, 106, 255],\n         [255, 215, 106, 255],\n         ...,\n         [255, 255, 255, 255],\n         [255, 255, 255, 255]]], dtype=torch.uint8)}\n>>> ds[:2][\"image\"].shape\ntorch.Size([2, 512, 512, 4])\n>>> ds[:2]\n{'image': tensor([[[[255, 215, 106, 255],\n          [255, 215, 106, 255],\n          ...,\n          [255, 255, 255, 255],\n          [255, 255, 255, 255]]]], dtype=torch.uint8)}\n```", "```py\n>>> from datasets import Dataset, Features, Audio, Image\n>>> audio = [\"path/to/audio.wav\"] * 10\n>>> features = Features({\"audio\": Audio()})\n>>> ds = Dataset.from_dict({\"audio\": audio}, features=features) \n>>> ds = ds.with_format(\"torch\")  \n>>> ds[0][\"audio\"][\"array\"]\ntensor([ 6.1035e-05,  1.5259e-05,  1.6785e-04,  ..., -1.5259e-05,\n        -1.5259e-05,  1.5259e-05])\n>>> ds[0][\"audio\"][\"sampling_rate\"]\ntensor(44100)\n```", "```py\n>>> import numpy as np\n>>> from datasets import Dataset \n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(16)\n>>> label = np.random.randint(0, 2, size=16)\n>>> ds = Dataset.from_dict({\"data\": data, \"label\": label}).with_format(\"torch\")\n>>> dataloader = DataLoader(ds, batch_size=4)\n>>> for batch in dataloader:\n...     print(batch)                                                                                            \n{'data': tensor([0.0047, 0.4979, 0.6726, 0.8105]), 'label': tensor([0, 1, 0, 1])}\n{'data': tensor([0.4832, 0.2723, 0.4259, 0.2224]), 'label': tensor([0, 0, 0, 0])}\n{'data': tensor([0.5837, 0.3444, 0.4658, 0.6417]), 'label': tensor([0, 1, 0, 0])}\n{'data': tensor([0.7022, 0.1225, 0.7228, 0.8259]), 'label': tensor([1, 1, 1, 1])}\n```", "```py\n>>> import numpy as np\n>>> from datasets import Dataset, load_from_disk\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10_000)\n>>> Dataset.from_dict({\"data\": data}).save_to_disk(\"my_dataset\")\n>>> ds = load_from_disk(\"my_dataset\").with_format(\"torch\")\n>>> dataloader = DataLoader(ds, batch_size=32, num_workers=4)\n```", "```py\n>>> import numpy as np\n>>> from datasets import Dataset, load_dataset\n>>> from torch.utils.data import DataLoader\n>>> data = np.random.rand(10_000)\n>>> Dataset.from_dict({\"data\": data}).push_to_hub(\"<username>/my_dataset\")  # Upload to the Hugging Face Hub\n>>> my_iterable_dataset = load_dataset(\"<username>/my_dataset\", streaming=True, split=\"train\")\n>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32)\n```", "```py\n>>> my_iterable_dataset = load_dataset(\"deepmind/code_contests\", streaming=True, split=\"train\")\n>>> my_iterable_dataset.n_shards\n39\n>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32, num_workers=4)\n```", "```py\nimport os\nfrom datasets.distributed import split_dataset_by_node\n\nds = split_dataset_by_node(ds, rank=int(os.environ[\"RANK\"]), world_size=int(os.environ[\"WORLD_SIZE\"]))\n```"]