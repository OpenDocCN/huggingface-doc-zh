["```py\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize gptq\n```", "```py\ntext-generation-server quantize tiiuae/falcon-40b /data/falcon-40b-gptq\n# Add --upload-to-model-id MYUSERNAME/falcon-40b to push the created model to the hub directly\n```", "```py\ntext-generation-launcher --model-id /data/falcon-40b-gptq/ --sharded true --num-shard 2 --quantize gptq\n```", "```py\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize bitsandbytes\n```", "```py\ndocker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest --model-id $model --quantize bitsandbytes-nf4\n```"]