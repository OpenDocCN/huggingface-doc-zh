# 有用的工具

> 原文：[`huggingface.co/docs/accelerate/package_reference/utilities`](https://huggingface.co/docs/accelerate/package_reference/utilities)

以下是🤗 Accelerate 提供的各种实用函数，按用例分类。

## 常量

用于参考的🤗 Accelerate 中使用的常量

以下是在使用 Accelerator.save_state()时使用的常量

`utils.MODEL_NAME`: `"pytorch_model"` `utils.OPTIMIZER_NAME`: `"optimizer"` `utils.RNG_STATE_NAME`: `"random_states"` `utils.SCALER_NAME`: `"scaler.pt` `utils.SCHEDULER_NAME`: `"scheduler`

以下是在使用 Accelerator.save_model()时使用的常量

`utils.WEIGHTS_NAME`: `"pytorch_model.bin"` `utils.SAFE_WEIGHTS_NAME`: `"model.safetensors"` `utils.WEIGHTS_INDEX_NAME`: `"pytorch_model.bin.index.json"` `utils.SAFE_WEIGHTS_INDEX_NAME`: `"model.safetensors.index.json"`

## 数据类

这些是在🤗 Accelerate 中使用的基本数据类，可以作为参数传递。

### 独立的

这些是用于检查的独立数据类，例如正在使用的分布式系统类型

### `class accelerate.utils.ComputeEnvironment`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L329)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

代表计算环境的一种类型。

值：

+   `LOCAL_MACHINE` — 私有/自定义集群硬件。

+   `AMAZON_SAGEMAKER` — Amazon SageMaker 作为计算环境。

### `class accelerate.DistributedType`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L285)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

代表一种分布式环境类型。

值：

+   `NO` — 不是分布式环境，只是单个进程。

+   `MULTI_CPU` — 在多个 CPU 节点上分布。

+   `MULTI_GPU` — 在多个 GPU 上分布。

+   `MULTI_NPU` — 在多个 NPU 上分布。

+   `MULTI_XPU` — 在多个 XPU 上分布。

+   `DEEPSPEED` — 使用 DeepSpeed。

+   `TPU` — 在 TPU 上分布。

### `class accelerate.utils.DynamoBackend`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L344)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

代表一个 dynamo 后端（参见[`github.com/pytorch/torchdynamo`](https://github.com/pytorch/torchdynamo)）。

值：

+   `NO` — 不使用 torch dynamo。

+   `EAGER` — 使用 PyTorch 来运行提取的 GraphModule。在调试 TorchDynamo 问题时非常有用。

+   `AOT_EAGER` — 使用 AotAutograd 而不使用编译器，即仅使用 PyTorch eager 进行 AotAutograd 的提取的前向和后向图。这对调试很有用，不太可能提供加速。

+   `INDUCTOR` — 使用 TorchInductor 后端与 AotAutograd 和 cudagraphs，通过利用 codegened Triton 内核。[阅读更多](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)

+   `AOT_TS_NVFUSER` — 使用 AotAutograd/TorchScript 的 nvFuser。[阅读更多](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)

+   `NVPRIMS_NVFUSER` — 使用 PrimTorch 的 nvFuser。[阅读更多](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)

+   `CUDAGRAPHS` — 使用 AotAutograd 的 cudagraphs。[阅读更多](https://github.com/pytorch/torchdynamo/pull/757)

+   `OFI` — 使用 Torchscript optimize_for_inference。仅推理。[阅读更多](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)

+   `FX2TRT` — 使用 Nvidia TensorRT 进行推理优化。仅推理。[阅读更多](https://github.com/pytorch/TensorRT/blob/master/docsrc/tutorials/getting_started_with_fx_path.rst)

+   `ONNXRT` — 使用 ONNXRT 在 CPU/GPU 上进行推理。仅推理。[阅读更多](https://onnxruntime.ai/)

+   `TENSORRT` — 使用 ONNXRT 来运行 TensorRT 进行推理优化。[阅读更多](https://github.com/onnx/onnx-tensorrt)

+   `IPEX` — 在 CPU 上使用 IPEX 进行推断。仅推断。[阅读更多](https://github.com/intel/intel-extension-for-pytorch)。

+   `TVM` — 使用 Apach TVM 进行推断优化。[阅读更多](https://tvm.apache.org/)

### `class accelerate.utils.LoggerType`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L392)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

表示支持的实验跟踪器类型

值：

+   `ALL` — 环境中所有可用的受支持跟踪器

+   `TENSORBOARD` — TensorBoard 作为实验跟踪器

+   `WANDB` — wandb 作为实验跟踪器

+   `COMETML` — comet_ml 作为实验跟踪器

+   `DVCLIVE` — dvclive 作为实验跟踪器

### `class accelerate.utils.PrecisionType`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L414)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

表示浮点值上使用的精度类型

值：

+   `NO` — 使用全精度（FP32）

+   `FP16` — 使用半精度

+   `BF16` — 使用脑浮点精度

### `class accelerate.utils.RNGType`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L430)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

一个枚举。

### `class accelerate.utils.SageMakerDistributedType`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L312)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

表示分布式环境类型。

值：

+   `NO` — 不是分布式环境，只是一个单一进程。

+   `DATA_PARALLEL` — 使用 sagemaker 分布式数据并行。

+   `MODEL_PARALLEL` — 使用 sagemaker 分布式模型并行。

### Kwargs

这些是加速在幕后处理的 PyTorch 生态系统中特定交互的可配置参数。

### `class accelerate.AutocastKwargs`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L60)

```py
( enabled: bool = True cache_enabled: bool = None )
```

在您的 Accelerator 中使用此对象，以自定义`torch.autocast`的行为。请参考此[上下文管理器](https://pytorch.org/docs/stable/amp.html#torch.autocast)的文档，以获取有关每个参数的更多信息。

示例：

```py
from accelerate import Accelerator
from accelerate.utils import AutocastKwargs

kwargs = AutocastKwargs(cache_enabled=True)
accelerator = Accelerator(kwargs_handlers=[kwargs])
```

### `class accelerate.DistributedDataParallelKwargs`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L82)

```py
( dim: int = 0 broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False check_reduction: bool = False gradient_as_bucket_view: bool = False static_graph: bool = False )
```

在您的 Accelerator 中使用此对象，以自定义如何将您的模型包装在`torch.nn.parallel.DistributedDataParallel`中。请参考此[包装器](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)的文档，以获取有关每个参数的更多信息。

`gradient_as_bucket_view` 仅在 PyTorch 1.7.0 及更高版本中可用。

`static_graph` 仅在 PyTorch 1.11.0 及更高版本中可用。

示例：

```py
from accelerate import Accelerator
from accelerate.utils import DistributedDataParallelKwargs

kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
accelerator = Accelerator(kwargs_handlers=[kwargs])
```

### `class accelerate.utils.FP8RecipeKwargs`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L179)

```py
( backend: Literal = 'MSAMP' opt_level: Literal = 'O2' margin: int = 0 interval: int = 1 fp8_format: Literal = 'E4M3' amax_history_len: int = 1 amax_compute_algo: Literal = 'most_recent' override_linear_precision: Tuple = (False, False, False) )
```

参数

+   `backend` (`str`, *可选*, 默认为`"msamp"`) — 使用的 FP8 引擎。必须是`"msamp"`（MS-AMP）或`"te"`（TransformerEngine）之一。

+   `margin` (`int`, *可选*, 默认为 0) — 用于梯度缩放的边距。

+   `interval` (`int`, *可选*, 默认为 1) — 用于重新计算缩放因子的间隔。

+   `fp8_format` (`str`, *可选*, 默认为`E4M3`) — 用于 FP8 配方的格式。必须是`E4M3`或`HYBRID`之一。

+   `amax_history_len` (`int`, *可选*, 默认为 1024) — 用于缩放因子计算的历史长度

+   `amax_compute_algo` (`str`, *可选*, 默认为`most_recent`) — 用于缩放因子计算的算法。必须是`max`或`most_recent`之一。

+   `override_linear_precision`（三个`bool`的`tuple`，*可选*，默认为`(False, False, False)`） — 是否在更高精度中执行`fprop`、`dgrad`和`wgrad` GEMMS。

+   `optimization_level`（`str`），其中之一为`O1`、`O2`。 （默认为`O2`） — 使用 MS-AMP 时应使用的 8 位集体通信级别。一般来说：

    +   O1: 权重梯度和`all_reduce`通信在 fp8 中完成，减少了 GPU 内存使用和通信带宽

    +   O2: 一阶优化器状态为 8 位，二阶状态为 FP16。 仅在使用 Adam 或 AdamW 时可用。这可以保持准确性，并可能节省最高的内存。

    +   03: 专门为 DeepSpeed 实现功能，使模型的权重和主权重存储在 FP8 中。如果选择了`fp8`并启用了 deepspeed，则将默认使用（目前不可用）。

在您的 Accelerator 中使用此对象，以自定义使用`transformer-engine`或`ms-amp`进行 FP8 混合精度训练的配方的初始化。

有关`transformer-engine`参数的更多信息，请参考 API[文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)。

有关`ms-amp`参数的更多信息，请参考优化级别[文档](https://azure.github.io/MS-AMP/docs/user-tutorial/optimization-level)。

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs

kwargs = FP8RecipeKwargs(backend="te", fp8_format="HYBRID")
accelerator = Accelerator(mixed_precision="fp8", kwargs_handlers=[kwargs])
```

要将 MS-AMP 作为引擎使用，请传递`backend="msamp"`和`optimization_level`：

```py
kwargs = FP8RecipeKwargs(backend="msamp", optimization_level="02")
```

### `class accelerate.GradScalerKwargs`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L118)

```py
( init_scale: float = 65536.0 growth_factor: float = 2.0 backoff_factor: float = 0.5 growth_interval: int = 2000 enabled: bool = True )
```

在您的 Accelerator 中使用此对象，以自定义混合精度的行为，特别是如何创建所使用的`torch.cuda.amp.GradScaler`。请参考此[scaler](https://pytorch.org/docs/stable/amp.html?highlight=gradscaler)的文档，以获取有关每个参数的更多信息。

`GradScaler`仅在 PyTorch 1.5.0 及更高版本中可用。

示例：

```py
from accelerate import Accelerator
from accelerate.utils import GradScalerKwargs

kwargs = GradScalerKwargs(backoff_filter=0.25)
accelerator = Accelerator(kwargs_handlers=[kwargs])
```

### `class accelerate.InitProcessGroupKwargs`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L149)

```py
( backend: Optional = 'nccl' init_method: Optional = None timeout: timedelta = datetime.timedelta(seconds=1800) )
```

在您的 Accelerator 中使用此对象，以自定义分布式进程的初始化。请参考此[method](https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group)的文档，以获取有关每个参数的更多信息。

```py
from datetime import timedelta
from accelerate import Accelerator
from accelerate.utils import InitProcessGroupKwargs

kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=800))
accelerator = Accelerator(kwargs_handlers=[kwargs])
```

## 插件

这些是可以传递给 Accelerator 对象的插件。虽然它们在文档的其他地方定义，但为了方便，所有插件都可以在此处查看：

### `class accelerate.DeepSpeedPlugin`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L562)

```py
( hf_ds_config: Any = None gradient_accumulation_steps: int = None gradient_clipping: float = None zero_stage: int = None is_train_batch_min: str = True offload_optimizer_device: bool = None offload_param_device: bool = None offload_optimizer_nvme_path: str = None offload_param_nvme_path: str = None zero3_init_flag: bool = None zero3_save_16bit_model: bool = None )
```

此插件用于集成 DeepSpeed。

#### `deepspeed_config_process`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L756)

```py
( prefix = '' mismatches = None config = None must_match = True **kwargs )
```

使用 kwargs 中的值处理 DeepSpeed 配置。

### `class accelerate.FullyShardedDataParallelPlugin`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L872)

```py
( sharding_strategy: typing.Any = None backward_prefetch: typing.Any = None mixed_precision_policy: typing.Any = None auto_wrap_policy: Optional = None cpu_offload: typing.Any = None ignored_modules: Optional = None state_dict_type: typing.Any = None state_dict_config: typing.Any = None optim_state_dict_config: typing.Any = None limit_all_gathers: bool = True use_orig_params: bool = True param_init_fn: Optional = None sync_module_states: bool = True forward_prefetch: bool = False activation_checkpointing: bool = False )
```

此插件用于启用完全分片的数据并行。

#### `get_module_class_from_name`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1025)

```py
( module name )
```

参数

+   `module`（`torch.nn.Module`） — 要从中获取类的模块。

+   `name`（`str`） — 类的名称。

通过名称从模块中获取一个类。

### `class accelerate.utils.GradientAccumulationPlugin`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L505)

```py
( num_steps: int = None adjust_scheduler: bool = True sync_with_dataloader: bool = True )
```

一个插件，用于配置梯度累积行为。

### `class accelerate.utils.MegatronLMPlugin`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1105)

```py
( tp_degree: int = None pp_degree: int = None num_micro_batches: int = None gradient_clipping: float = None sequence_parallelism: bool = None recompute_activations: bool = None use_distributed_optimizer: bool = None pipeline_model_parallel_split_rank: int = None num_layers_per_virtual_pipeline_stage: int = None is_train_batch_min: str = True train_iters: int = None train_samples: int = None weight_decay_incr_style: str = 'constant' start_weight_decay: float = None end_weight_decay: float = None lr_decay_style: str = 'linear' lr_decay_iters: int = None lr_decay_samples: int = None lr_warmup_iters: int = None lr_warmup_samples: int = None lr_warmup_fraction: float = None min_lr: float = 0 consumed_samples: List = None no_wd_decay_cond: Optional = None scale_lr_cond: Optional = None lr_mult: float = 1.0 megatron_dataset_flag: bool = False seq_length: int = None encoder_seq_length: int = None decoder_seq_length: int = None tensorboard_dir: str = None set_all_logging_options: bool = False eval_iters: int = 100 eval_interval: int = 1000 return_logits: bool = False custom_train_step_class: Optional = None custom_train_step_kwargs: Optional = None custom_model_provider_function: Optional = None custom_prepare_model_function: Optional = None other_megatron_args: Optional = None )
```

用于 Megatron-LM 的插件，以启用张量、管道、序列和数据并行。还可启用选择性激活重计算和优化融合内核。

### `class accelerate.utils.TorchDynamoPlugin`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L526)

```py
( backend: DynamoBackend = None mode: str = None fullgraph: bool = None dynamic: bool = None options: Any = None disable: bool = False )
```

此插件用于使用 PyTorch 2.0 编译模型

## 配置

这些是可以配置并传递给适当集成的类

### `class accelerate.utils.BnbQuantizationConfig`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L1480)

```py
( load_in_8bit: bool = False llm_int8_threshold: float = 6.0 load_in_4bit: bool = False bnb_4bit_quant_type: str = 'fp4' bnb_4bit_use_double_quant: bool = False bnb_4bit_compute_dtype: bool = 'fp16' torch_dtype: dtype = None skip_modules: List = None keep_in_fp32_modules: List = None )
```

一个插件，用于启用 BitsAndBytes 4 位和 8 位量化

### `class accelerate.utils.ProjectConfiguration`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L457)

```py
( project_dir: str = None logging_dir: str = None automatic_checkpoint_naming: bool = False total_limit: int = None iteration: int = 0 save_on_each_node: bool = False )
```

基于内部项目需求的加速器对象的配置。

#### `set_directories`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/dataclasses.py#L495)

```py
( project_dir: str = None )
```

将`self.project_dir`和`self.logging_dir`设置为适当的值。

## 环境变量

这些是可以为不同用例启用的环境变量

+   `ACCELERATE_DEBUG_MODE`（`str`）：是否在调试模式下运行加速。更多信息请参阅此处。

## 数据操作和操作

这些包括模拟相同`torch`操作的数据操作，但可以在分布式进程上使用。

#### `accelerate.utils.broadcast`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L542)

```py
( tensor from_process: int = 0 )
```

参数

+   `tensor`（嵌套列表/元组/张量字典的`torch.Tensor`）-要收集的数据。

+   `from_process`（`int`，*可选*，默认为 0）-要发送数据的进程

递归广播嵌套列表/元组/张量字典中的张量到所有设备。

#### `accelerate.utils.broadcast_object_list`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L564)

```py
( object_list from_process: int = 0 )
```

参数

+   `object_list`（可拾取对象列表）-要广播的对象列表。此列表将被就地修改。

+   `from_process`（`int`，*可选*，默认为 0）-要发送数据的进程。

从一个进程向其他进程广播一个可拾取对象列表。

#### `accelerate.utils.concatenate`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L605)

```py
( data dim = 0 )
```

参数

+   `data`（嵌套列表/元组/张量字典的张量列表`torch.Tensor`）-要连接的数据。

+   `dim`（`int`，*可选*，默认为 0）-要连接的维度。

递归连接具有相同形状的张量列表/元组/字典中的张量。

#### `accelerate.utils.convert_outputs_to_fp32`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L813)

```py
( model_forward )
```

#### `accelerate.utils.convert_to_fp32`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L766)

```py
( tensor )
```

参数

+   `tensor`（嵌套列表/元组/张量字典的`torch.Tensor`）-要从 FP16/BF16 转换为 FP32 的数据。

递归将 FP16/BF16 精度中的嵌套列表/元组/张量字典中的元素转换为 FP32。

#### `accelerate.utils.gather`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L422)

```py
( tensor )
```

参数

+   `tensor`（嵌套列表/元组/张量字典的`torch.Tensor`）-要收集的数据。

从所有设备中递归收集嵌套列表/元组/张量字典中的张量。

#### `accelerate.utils.gather_object`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L449)

```py
( object: Any )
```

参数

+   `对象`（嵌套的列表/元组/字典，包含可 pickle 对象）- 要收集的数据。

递归地从所有设备中的嵌套列表/元组/字典对象中收集对象。

#### `accelerate.utils.listify`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L283)

```py
( data )
```

参数

+   `数据`（嵌套的列表/元组/字典，包含`torch.Tensor`）- 要转换为常规数字的数据。

递归地查找嵌套列表/元组/字典中的张量，并将它们转换为数字列表。

#### `accelerate.utils.pad_across_processes`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L631)

```py
( tensor dim = 0 pad_index = 0 pad_first = False )
```

参数

+   `张量`（嵌套的列表/元组/字典，包含`torch.Tensor`）- 要收集的数据。

+   `dim`（`int`，*可选*，默认为 0）- 要填充的维度。

+   `pad_index`（`int`，*可选*，默认为 0）- 用于填充的值。

+   `pad_first`（`bool`，*可选*，默认为`False`）- 是否在开头或结尾填充。

递归地填充来自所有设备的嵌套列表/元组/字典中的张量，使它们达到相同的大小，以便安全地收集。

#### `accelerate.utils.recursively_apply`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L93)

```py
( func data *args test_type = <function is_torch_tensor at 0x7f2b4f4c7d00> error_on_other_type = False **kwargs )
```

参数

+   `func`（`callable`）- 要递归应用的函数。

+   `数据`（嵌套的列表/元组/字典，包含`main_type`）- 要应用`func`的数据*args- 当应用于解包数据时将传递给`func`的位置参数。

+   `main_type`（`type`，*可选*，默认为`torch.Tensor`）- 要应用`func`的对象的基本类型。

+   `error_on_other_type`（`bool`，*可选*，默认为`False`）- 如果在解包`数据`后得到一个不是`main_type`类型的对象，是否返回错误。如果为`False`，函数将保持与`main_type`不同类型的对象不变。**kwargs- 当应用于解包数据时将传递给`func`的关键字参数。

递归地在给定基本类型的嵌套列表/元组/字典上应用函数。

#### `accelerate.utils.reduce`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L724)

```py
( tensor reduction = 'mean' scale = 1.0 )
```

参数

+   `张量`（嵌套的列表/元组/字典，包含`torch.Tensor`）- 要减少的数据。

+   `reduction`（`str`，*可选*，默认为`"mean"`）- 一种减少方法。可以是`"mean"`，“sum”或“none”

+   `scale`（`float`，*可选*）- 在减少后应用的默认缩放值，仅在 XLA 上有效。

通过给定操作的平均值，递归地减少所有进程中的张量列表/元组/字典中的张量。

#### `accelerate.utils.send_to_device`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L144)

```py
( tensor device non_blocking = False skip_keys = None )
```

参数

+   `张量`（嵌套的列表/元组/字典，包含`torch.Tensor`）- 要发送到指定设备的数据。

+   `device`（`torch.device`）- 要发送数据的设备。

递归地将嵌套的列表/元组/字典中的元素发送到指定设备。

#### `accelerate.utils.slice_tensors`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/operations.py#L585)

```py
( data tensor_slice process_index = None num_processes = None )
```

参数

+   `数据`（嵌套的列表/元组/字典，包含`torch.Tensor`）- 要切片的数据。

+   `tensor_slice`（`slice`）- 要取的切片。

递归地在嵌套的列表/元组/字典中的张量中取一个切片。

## 环境检查

这些功能检查当前工作环境的状态，包括有关操作系统本身的信息，它可以支持的内容，以及特定依赖项是否已安装。

#### `accelerate.utils.is_bf16_available`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L130)

```py
( ignore_tpu = False )
```

检查是否支持 bf16，可选择忽略 TPU

#### `accelerate.utils.is_ipex_available`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L255)

```py
( )
```

#### `accelerate.utils.is_mps_available`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/imports.py#L251)

```py
( )
```

#### `accelerate.utils.is_npu_available`

```py
( check_device = False )
```

检查是否安装了`torch_npu`，并且可能环境中是否有 NPU

#### `accelerate.utils.is_torch_version`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/versions.py#L46)

```py
( operation: str version: str )
```

参数

+   `operation`（`str`）— 操作符的字符串表示，例如`">"`或`"<="`

+   `version`（`str`）— PyTorch 的字符串版本

将当前的 PyTorch 版本与给定的参考版本进行比较。

#### `accelerate.utils.is_tpu_available`

```py
( check_device = True )
```

检查是否安装了`torch_xla`，并且可能环境中是否有 TPU

#### `accelerate.utils.is_xpu_available`

```py
( check_device = False )
```

检查用户是否明确禁用它

## 环境操作

#### `accelerate.utils.patch_environment`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L219)

```py
( **kwargs )
```

一个上下文管理器，将每个关键字参数添加到`os.environ`中，并在退出时将它们删除。

将`kwargs`中的值转换为字符串，并将所有键大写。

示例：

```py
>>> import os
>>> from accelerate.utils import patch_environment

>>> with patch_environment(FOO="bar"):
...     print(os.environ["FOO"])  # prints "bar"
>>> print(os.environ["FOO"])  # raises KeyError
```

#### `accelerate.utils.clear_environment`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L186)

```py
( )
```

一个上下文管理器，将缓存的原始`os.environ`替换为一个空字典在这个上下文中。

当此上下文退出时，缓存的`os.environ`将恢复。

示例：

```py
>>> import os
>>> from accelerate.utils import clear_environment

>>> os.environ["FOO"] = "bar"
>>> with clear_environment():
...     print(os.environ)
...     os.environ["FOO"] = "new_bar"
...     print(os.environ["FOO"])
{}
new_bar

>>> print(os.environ["FOO"])
bar
```

#### `accelerate.commands.config.default.write_basic_config`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/commands/config/default.py#L29)

```py
( mixed_precision = 'no' save_location: str = '/github/home/.cache/huggingface/accelerate/default_config.yaml' use_xpu: bool = False )
```

参数

+   `mixed_precision`（`str`，*可选*，默认为“no”）— 要使用的混合精度。应该是“no”，“fp16”或“bf16”中的一个。

+   `save_location`（`str`，*可选*，默认为`default_json_config_file`）— 可选的自定义保存位置。在使用`accelerate launch`时应传递给`--config_file`。默认位置位于 huggingface 缓存文件夹内（`~/.cache/huggingface`），但可以通过设置`HF_HOME`环境变量，然后跟随`accelerate/default_config.yaml`来覆盖。

+   `use_xpu`（`bool`，*可选*，默认为`False`）— 是否在可用时使用 XPU。

创建并保存一个基本的集群配置，用于在本地机器上使用可能有多个 GPU。如果是仅 CPU 的机器，还将设置 CPU。

首次设置🤗 Accelerate 时，可以使用`accelerate config`而不是运行[~utils.write_basic_config]作为快速配置的替代方法。

## 内存

#### `accelerate.find_executable_batch_size`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/memory.py#L83)

```py
( function: callable = None starting_batch_size: int = 128 )
```

参数

+   `function`（`callable`，*可选*）— 要包装的函数

+   `starting_batch_size`（`int`，*可选*）— 尝试适应内存的批量大小

一个基本的装饰器，将尝试执行`function`。如果由于内存不足或 CUDNN 相关的异常而失败，则将批量大小减半并传递给`function`

`function`必须将`batch_size`参数作为其第一个参数。

示例：

```py
>>> from accelerate.utils import find_executable_batch_size

>>> @find_executable_batch_size(starting_batch_size=128)
... def train(batch_size, model, optimizer):
...     ...

>>> train(model, optimizer)
```

## 建模

这些实用程序与与 PyTorch 模型交互有关

#### `accelerate.utils.calculate_maximum_sizes`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1004)

```py
( model: Module )
```

计算模型及其最大层的总大小

#### `accelerate.utils.compute_module_sizes`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L686)

```py
( model: Module dtype: Union = None special_dtypes: Optional = None )
```

计算给定模型的每个子模块的大小。

#### `accelerate.utils.extract_model_from_parallel`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)

```py
( model keep_fp32_wrapper: bool = True ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 要提取的模型。

+   `keep_fp32_wrapper` (`bool`, *可选*) — 是否从模型中删除混合精度钩子。

返回

`torch.nn.Module`

提取的模型。

从其分布式容器中提取模型。

#### `accelerate.utils.get_balanced_memory`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L872)

```py
( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None low_zero: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 要分析的模型。

+   `max_memory` (`Dict`, *可选*) — 设备标识符到最大内存的字典。如果未设置，将默认为可用的最大内存。示例：`max_memory={0: "1GB"}`。

+   `no_split_module_classes` (`List[str]`, *可选*) — 不应跨设备拆分的层类名列表（例如具有残差连接的任何层）。

+   `dtype` (`str` 或 `torch.dtype`, *可选*) — 如果提供，加载时将将权重转换为该类型。

+   `special_dtypes` (`Dict[str, Union[str, torch.device]]`, *可选*) — 如果提供，用于某些特定权重的特殊数据类型（将覆盖默认用于所有权重的 dtype）。

+   `low_zero` (`bool`, *可选*) — 最小化 GPU 0 上的权重数量，在其他操作（如 Transformers 生成函数）中使用时很方便。

为 infer_auto_device_map() 计算一个 `max_memory` 字典，以平衡每个可用 GPU 的使用。

所有计算都是通过分析模型参数的大小和数据类型来完成的。因此，模型可以位于元设备上（就像在 `init_empty_weights` 上下文管理器中初始化时一样）。

#### `accelerate.utils.get_max_layer_size`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L719)

```py
( modules: List module_sizes: Dict no_split_module_classes: List ) → export const metadata = 'undefined';Tuple[int, List[str]]
```

参数

+   `modules` (`List[Tuple[str, torch.nn.Module]]`) — 我们要确定最大层尺寸的命名模块列表。

+   `module_sizes` (`Dict[str, int]`) — 将每个层名称映射到其大小的字典（由 `compute_module_sizes` 生成）。

+   `no_split_module_classes` (`List[str]`) — 不希望拆分的层类名列表。

返回

`Tuple[int, List[str]]`

具有实现最大尺寸的层名称列表的最大层尺寸。

将扫描命名模块列表并返回一个完整层使用的最大尺寸的实用函数。层的定义为：

+   没有直接子级（只有参数和缓冲区）的模块

+   类名在列表 `no_split_module_classes` 中的模块

#### `accelerate.infer_auto_device_map`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1022)

```py
( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None verbose: bool = False clean_result: bool = True )
```

参数

+   `model` (`torch.nn.Module`) — 要分析的模型。

+   `max_memory` (`Dict`, *可选*) — 设备标识符到最大内存的字典。如果未设置，将默认为可用的最大内存。示例：`max_memory={0: "1GB"}`。

+   `no_split_module_classes` (`List[str]`, *可选*) — 不应跨设备拆分的层类名列表（例如具有残差连接的任何层）。

+   `dtype` (`str` 或 `torch.dtype`, *可选*) — 如果提供，加载时将将权重转换为该类型。

+   `special_dtypes` (`Dict[str, Union[str, torch.device]]`, *可选*) — 如果提供，用于某些特定权重的特殊数据类型（将覆盖默认用于所有权重的 dtype）。

+   `verbose` (`bool`, *可选*, 默认为 `False`) — 是否在函数构建 device_map 时提供调试语句。

+   `clean_result` (`bool`, *可选*, 默认为 `True`) — 通过将所有放在同一设备上的子模块分组来清理结果的 device_map。

为给定模型计算设备映射，优先考虑 GPU，然后转移到 CPU，最后转移到磁盘，使得：

+   我们不会超出任何 GPU 可用的内存。

+   如果需要转移到 CPU，总是在 GPU 0 上留有空间，以将在 CPU 上转移的具有最大尺寸的层放回。

+   如果需要转移到 CPU，我们不会超出 CPU 可用的 RAM。

+   如果需要转移到磁盘，总是在 CPU 上留有空间，以将在磁盘上转移的具有最大尺寸的层放回。

所有计算都是通过分析模型参数的大小和数据类型来完成的。因此，模型可以在元设备上（就像在`init_empty_weights`上下文管理器中初始化时一样）。

#### `accelerate.load_checkpoint_in_model`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)

```py
( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 我们要加载检查点的模型。

+   `checkpoint` (`str`或`os.PathLike`) — 要加载的文件夹检查点。它可以是：

    +   包含整个模型状态字典的文件路径。

    +   包含分片检查点索引的`.json`文件路径。

    +   一个包含唯一的`.index.json`文件和检查点分片的文件夹路径。

    +   一个包含唯一的 pytorch_model.bin 或 model.safetensors 文件的文件夹路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`, *可选*) — 一个指定每个子模块应该去哪里的映射。它不需要细化到每个参数/缓冲区名称，一旦给定模块名称在内，它的每个子模块都将被发送到相同的设备。

+   `offload_folder` (`str`或`os.PathLike`, *可选*) — 如果`device_map`包含任何值为`"disk"`，则我们将转移权重的文件夹。

+   `dtype` (`str`或`torch.dtype`, *可选*) — 如果提供，加载时权重将转换为该类型。

+   `offload_state_dict` (`bool`, *可选*, 默认为`False`) — 如果为`True`，将临时将 CPU 状态字典转移到硬盘上，以避免如果 CPU 状态字典的权重+最大分片的权重不适合时超出 CPU RAM。

+   `offload_buffers` (`bool`, *可选*, 默认为`False`) — 是否将缓冲区包含在转移到磁盘的权重中。

+   `keep_in_fp32_modules(List[str],` *可选*) — 我们保留在`torch.float32`数据类型中的模块列表。

+   `offload_8bit_bnb` (`bool`, *可选*) — 是否启用在 cpu/disk 上转移 8 位模块。

在模型中加载（可能是分片的）检查点，可能在加载时将权重发送到指定的设备。

一旦跨设备加载，您仍然需要在模型上调用 dispatch_model()使其能够运行。要将检查点加载和分发组合在一个单一调用中，请使用 load_checkpoint_and_dispatch()。

#### `accelerate.utils.load_offloaded_weights`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L842)

```py
( model index offload_folder )
```

参数

+   `model` (`torch.nn.Module`) — 要加载权重的模型。

+   `index` (`dict`) — 包含从模型转移的每个参数的参数名称及其元数据的字典。

+   `offload_folder` (`str`) — 存储转移权重的文件夹。

将权重从转移文件夹加载到模型中。

#### `accelerate.utils.load_state_dict`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1301)

```py
( checkpoint_file device_map = None )
```

参数

+   `checkpoint_file` (`str`) — 要加载的检查点路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`, *可选*) — 指定每个子模块应该去的位置的映射。不需要对每个参数/缓冲区名称进行细化，一旦给定模块名称在内，它的每个子模块都将被发送到相同的设备。

从给定文件加载检查点。如果检查点是在 safetensors 格式中并且传递了设备映射，则权重可以直接快速加载到 GPU 上。

#### `accelerate.utils.offload_state_dict`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/offload.py#L85)

```py
( save_dir: Union state_dict: Dict )
```

参数

+   `save_dir` (`str` 或 `os.PathLike`) — 要卸载状态字典的目录。

+   `state_dict` (`Dict[str, torch.Tensor]`) — 要卸载的张量字典。

在给定文件夹中卸载状态字典。

#### `accelerate.utils.retie_parameters`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L644)

```py
( model tied_params )
```

参数

+   `model` (`torch.nn.Module`) — 要重新绑定参数的模型。

+   `tied_params` (`List[List[str]]`) — 通过`find_tied_parameters`获得的参数名称到绑定参数名称的映射。

在给定模型中重新绑定绑定参数，如果链接被断开（例如添加钩子时）。

#### `accelerate.utils.set_module_tensor_to_device`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L275)

```py
( module: Module tensor_name: str device: Union value: Optional = None dtype: Union = None fp16_statistics: Optional = None tied_params_map: Optional = None )
```

参数

+   `module` (`torch.nn.Module`) — 我们想要移动的张量所在的模块。

+   `tensor_name` (`str`) — 参数/缓冲区的完整名称。

+   `device` (`int`, `str` 或 `torch.device`) — 要设置张量的设备。

+   `value` (`torch.Tensor`, *可选*) — 张量的值（在从元设备转到任何其他设备时有用）。

+   `dtype` (`torch.dtype`, *可选*) — 如果传递了参数，参数的值将被转换为这个`dtype`。否则，`value`将被转换为模型中现有参数的`dtype`。

+   `fp16_statistics` (`torch.HalfTensor`, *可选*) — 要设置在模块上的 fp16 统计信息列表，用于 8 位模型序列化。

+   `tied_params_map` (Dict[int, Dict[torch.device, torch.Tensor]], *可选*, 默认为`None`) — 当前数据指针到已分派的绑定权重设备字典的映射。对于给定的执行设备，此参数对于重用设备上共享权重的第一个可用指针对于所有其他设备而言是有用的，而不是复制内存。

一个辅助函数，用于将模块的给定张量（参数或缓冲区）设置在特定设备上（请注意，执行`param.to(device)`会创建一个与参数不相关联的新张量，这就是为什么我们需要这个函数）。

#### `accelerate.utils.shard_checkpoint`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L193)

```py
( state_dict: Dict max_shard_size: Union = '10GB' weights_name: str = 'pytorch_model.bin' )
```

参数

+   `state_dict` (`Dict[str, torch.Tensor]`) — 要保存的模型的状态字典。

+   `max_shard_size` (`int` 或 `str`, *可选*, 默认为`"10GB"`) — 每个子检查点的最大大小。如果以字符串形式表示，需要是数字后跟一个单位（如`"5MB"`）。

+   `weights_name` (`str`, *可选*, 默认为`"pytorch_model.bin"`) — 模型保存文件的名称。

将模型状态字典拆分为子检查点，以便每个子检查点的最终大小不超过给定大小。

子检查点是通过按照其键的顺序迭代`state_dict`来确定的，因此没有优化使每个子检查点尽可能接近传递的最大大小。例如，如果限制为 10GB，我们有大小为[6GB, 6GB, 2GB, 6GB, 2GB, 2GB]的权重，它们将被分割为[6GB]，[6+2GB]，[6+2+2GB]，而不是[6+2+2GB]，[6+2GB]，[6GB]。

如果模型的权重之一大于`max_shard_size`，它将最终位于自己的子检查点中，其大小大于`max_shard_size`。

## 并行

这些包括应该在并行工作时使用的通用实用程序。

#### `accelerate.utils.extract_model_from_parallel`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L56)

```py
( model keep_fp32_wrapper: bool = True ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 要提取的模型。

+   `keep_fp32_wrapper` (`bool`, *可选*) — 是否从模型中删除混合精度钩子。

返回

`torch.nn.Module`

提取的模型。

从其分布式容器中提取模型。

#### `accelerate.utils.save`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L156)

```py
( obj f save_on_each_node: bool = False safe_serialization: bool = False )
```

参数

+   `save_on_each_node` (`bool`, *可选*, 默认为 `False`) — 是否仅在全局主进程上保存

+   `safe_serialization` (`bool`, *可选*, 默认为 `False`) — 是否使用 `safetensors` 或传统的 PyTorch 方式（使用 `pickle`）保存 `obj`。

将数据保存到磁盘。用于替代 `torch.save()`。

#### `accelerate.utils.wait_for_everyone`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/other.py#L108)

```py
( )
```

在脚本中引入一个阻塞点，确保所有进程在继续之前都已到达此点。

确保所有进程将到达此指令，否则其中一个进程将永远挂起。

## 随机

这些实用程序涉及设置和同步所有随机状态。

#### `accelerate.utils.set_seed`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L31)

```py
( seed: int device_specific: bool = False )
```

参数

+   `seed` (`int`) — 要设置的种子。

+   `device_specific` (`bool`, *可选*, 默认为 `False`) — 是否稍微在每个设备上使用 `self.process_index` 不同的种子。

用于在 `random`、`numpy`、`torch` 中设置种子以获得可重现行为的辅助函数。

#### `accelerate.utils.synchronize_rng_state`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L57)

```py
( rng_type: Optional = None generator: Optional = None )
```

#### `accelerate.synchronize_rng_states`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/random.py#L109)

```py
( rng_types: List generator: Optional = None )
```

## PyTorch XLA

这些包括在使用 PyTorch 与 XLA 时有用的实用程序。

#### `accelerate.utils.install_xla`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/torch_xla.py#L20)

```py
( upgrade: bool = False )
```

参数

+   `upgrade` (`bool`, *可选*, 默认为 `False`) — 是否升级 `torch` 并安装最新的 `torch_xla` 轮子。

在 Google Colaboratory 中根据 `torch` 版本安装适当的 xla 轮子的辅助函数。

示例：

```py
>>> from accelerate.utils import install_xla

>>> install_xla(upgrade=True)
```

## 加载模型权重

这些包括有用于加载检查点的实用程序。

#### `accelerate.load_checkpoint_in_model`

[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/modeling.py#L1442)

```py
( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )
```

参数

+   `model` (`torch.nn.Module`) — 我们要加载检查点的模型。

+   `checkpoint` (`str` 或 `os.PathLike`) — 要加载的文件夹检查点。可以是：

    +   包含整个模型状态字典的文件路径

    +   包含指向分片检查点索引的`.json`文件的路径

    +   包含唯一的 `.index.json` 文件和检查点分片的文件夹路径。

    +   包含唯一的 pytorch_model.bin 或 model.safetensors 文件的文件夹路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`, *可选*) — 指定每个子模块应放置在何处的映射。它不需要被细化到每个参数/缓冲区名称，一旦给定模块名称在内部，它的每个子模块都将被发送到相同的设备。

+   `offload_folder` (`str` 或 `os.PathLike`, *可选*) — 如果 `device_map` 包含任何值 `"disk"`，则我们将卸载权重的文件夹。

+   `dtype` (`str` 或 `torch.dtype`, *可选*) — 如果提供，加载时将将权重转换为该类型。

+   `offload_state_dict` (`bool`, *可选*, 默认为 `False`) — 如果为 `True`，将临时将 CPU 状态字典转移到硬盘上，以避免如果 CPU 状态字典的重量 + 最大分片的重量不适合 CPU RAM。

+   `offload_buffers` (`bool`, *可选*, 默认为 `False`) — 是否将缓冲区包含在卸载到磁盘的权重中。

+   `keep_in_fp32_modules(List[str],` *可选*) — 我们保留在 `torch.float32` 数据类型中的模块列表。

+   `offload_8bit_bnb` (`bool`, *可选*) — 是否启用在 cpu/磁盘上卸载 8 位模块。

加载（可能是分片的）检查点到模型中，可能在加载时将权重发送到给定设备。

一旦在设备间加载完成，您仍然需要调用 dispatch_model()来使模型能够运行。为了在一个单一调用中组合检查点加载和分发，可以使用 load_checkpoint_and_dispatch()。

## 量化

这些包括对量化模型有用的实用程序。

#### `accelerate.utils.load_and_quantize_model`

[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/utils/bnb.py#L44)

```py
( model: Module bnb_quantization_config: BnbQuantizationConfig weights_location: Union = None device_map: Optional = None no_split_module_classes: Optional = None max_memory: Optional = None offload_folder: Union = None offload_state_dict: bool = False ) → export const metadata = 'undefined';torch.nn.Module
```

参数

+   `model` (`torch.nn.Module`) — 输入模型。该模型可以已经加载或在元设备上

+   `bnb_quantization_config` (`BnbQuantizationConfig`) — 位和字节量化参数

+   `weights_location` (`str` or `os.PathLike`) — 要加载的权重文件夹。可以是：

    +   一个包含整个模型状态字典的文件路径

    +   一个包含分片检查点索引的 `.json` 文件路径

    +   一个包含唯一的 `.index.json` 文件和检查点分片的文件夹路径。

    +   一个包含唯一的 pytorch_model.bin 文件的文件夹路径。

+   `device_map` (`Dict[str, Union[int, str, torch.device]]`, *可选*) — 指定每个子模块应该去哪里的映射。它不需要被细化到每个参数/缓冲区名称，一旦给定模块名称在内，它的每个子模块都将被发送到相同的设备。

+   `no_split_module_classes` (`List[str]`, *可选*) — 不应该跨设备分割的层类名称列表（例如具有残差连接的任何层）。

+   `max_memory` (`Dict`, *可选*) — 设备标识符到最大内存的字典。如果未设置，将默认为可用的最大内存。

+   `offload_folder` (`str` or `os.PathLike`, *可选*) — 如果 `device_map` 包含任何值为 `"disk"`，则我们将卸载权重的文件夹。

+   `offload_state_dict` (`bool`, *可选*, 默认为 `False`) — 如果为 `True`，将临时将 CPU 状态字典卸载到硬盘上，以避免如果 CPU 状态字典的重量 + 最大分片的重量不适合时会超出 CPU RAM。

返回

`torch.nn.Module`

量化后的模型

此函数将使用传递给 `bnb_quantization_config` 的相关配置对输入模型进行量化。如果模型在元设备上，我们将根据传递的 `device_map` 加载和分发权重。如果模型已经加载，我们将量化模型并将模型放在 GPU 上，
