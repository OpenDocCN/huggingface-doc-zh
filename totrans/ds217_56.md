# 构建和加载

> 原文链接：[`huggingface.co/docs/datasets/about_dataset_load`](https://huggingface.co/docs/datasets/about_dataset_load)

几乎每个深度学习工作流都始于加载数据集，这使得它成为最重要的步骤之一。使用🤗 数据集，有超过 900 个数据集可供您开始进行 NLP 任务。您只需调用：load_dataset() 来迈出第一步。这个函数在各个方面都是一个真正的工作马，因为它会构建和加载您使用的每个数据集。

## ELI5：load_dataset

让我们从一个基本的五岁解释开始。

数据集是一个包含以下内容的目录：

+   一些以通用格式（JSON、CSV、Parquet、文本等）存储的数据文件

+   一个名为`README.md`的数据集卡片，包含有关数据集的文档以及一个 YAML 头部来定义数据集的标签和配置

+   如果需要一些代码来读取数据文件，则包含一个可选的数据集脚本。有时用于加载特定格式和结构的文件。

load_dataset()函数会在本地或从 Hugging Face Hub 获取请求的数据集。Hub 是一个中央存储库，存储了所有 Hugging Face 数据集和模型。

如果数据集只包含数据文件，则 load_dataset()会自动推断如何从其扩展名（json、csv、parquet、txt 等）加载数据文件。在幕后，🤗 数据集将根据数据文件格式使用适当的 DatasetBuilder。🤗 数据集中存在一个构建器对应于每种数据文件格式：

+   datasets.packaged_modules.text.Text 用于文本

+   datasets.packaged_modules.csv.Csv 用于 CSV 和 TSV

+   datasets.packaged_modules.json.Json 用于 JSON 和 JSONL

+   datasets.packaged_modules.parquet.Parquet 用于 Parquet

+   datasets.packaged_modules.arrow.Arrow 用于 Arrow（流文件格式）

+   datasets.packaged_modules.sql.Sql 用于 SQL 数据库

+   datasets.packaged_modules.imagefolder.ImageFolder 用于图像文件夹

+   datasets.packaged_modules.audiofolder.AudioFolder 用于音频文件夹

如果数据集有一个数据集脚本，那么它会从 Hugging Face Hub 下载并导入。数据集脚本中的代码定义了一个自定义 DatasetBuilder 来描述数据集信息（描述、特征、原始文件的 URL 等），并告诉🤗 数据集如何生成和显示示例。

阅读分享部分，了解如何分享数据集。该部分还提供了关于如何编写自己的数据集加载脚本的逐步指南！

🤗 数据集会从原始 URL 下载数据集文件，生成数据集并将其缓存在您的驱动器上的 Arrow 表中。如果您之前已经下载过数据集，则🤗 数据集将从缓存中重新加载，以免再次下载。

现在您已经对数据集是如何构建有了高层次的理解，让我们更仔细地看看所有这些是如何运作的。

## 构建数据集

当您第一次加载数据集时，🤗 数据集会将原始数据文件构建成一张行和类型列的表格。负责构建数据集的两个主要类是：BuilderConfig 和 DatasetBuilder。

![](img/bd493a651fa07ec8829a2c0ef2818f43.png)

### BuilderConfig

BuilderConfig 是 DatasetBuilder 的配置类。BuilderConfig 包含有关数据集的以下基本属性：

| 属性 | 描述 |
| --- | --- |
| `name` | 数据集的简称。 |
| `version` | 数据集版本标识符。 |
| `data_dir` | 存储包含数据文件的本地文件夹的路径。 |
| `data_files` | 存储本地数据文件的路径。 |
| `description` | 数据集描述。 |

如果您想向数据集添加额外的属性，例如类标签，可以对基础 BuilderConfig 类进行子类化。有两种方法可以填充 BuilderConfig 类或子类的属性：

+   在数据集的 `DatasetBuilder.BUILDER_CONFIGS()` 属性中提供预定义的 BuilderConfig 类（或子类）实例的列表。

+   当您调用 load_dataset() 时，任何不特定于该方法的关键字参数将用于设置 BuilderConfig 类的相关属性。如果选择了特定配置，这将覆盖预定义的属性。

您还可以将 DatasetBuilder.BUILDER_CONFIG_CLASS 设置为 BuilderConfig 的任何自定义子类。

### DatasetBuilder

DatasetBuilder 访问 BuilderConfig 中的所有属性来构建实际数据集。

![](img/bb7161ccc3fa095689cfa137b5590ded.png)

DatasetBuilder 中有三个主要方法：

1.  `DatasetBuilder._info()` 负责定义数据集属性。当您调用 `dataset.info` 时，🤗 数据集会返回存储在此处的信息。同样，Features 也在这里指定。请记住，Features 就像数据集的骨架。它提供了每列的名称和类型。

1.  `DatasetBuilder._split_generator` 下载或检索请求的数据文件，将其组织成拆分，并为生成过程定义特定参数。该方法具有一个 DownloadManager，用于下载文件或从本地文件系统获取文件。在 DownloadManager 中，有一个 DownloadManager.download_and_extract() 方法，接受一个包含原始数据文件的 URL 字典，并下载请求的文件。接受的输入包括：单个 URL 或路径，或 URL 或路径的列表/字典。任何压缩文件类型，如 TAR、GZIP 和 ZIP 存档，都将被自动提取。

    一旦文件下载完成，SplitGenerator 将它们组织成拆分。SplitGenerator 包含拆分的名称，以及提供给 `DatasetBuilder._generate_examples` 方法的任何关键字参数。关键字参数可以针对每个拆分具体设置，通常至少包括每个拆分的数据文件的本地路径。

1.  `DatasetBuilder._generate_examples` 读取并解析拆分的数据文件。然后根据 `DatasetBuilder._info()` 中指定的 `features` 格式生成数据集示例。`DatasetBuilder._generate_examples` 的输入实际上是上一个方法的关键字参数中提供的 `filepath`。

    数据集是使用 Python 生成器生成的，不会将所有数据加载到内存中。因此，生成器可以处理大型数据集。然而，在生成的样本刷新到磁盘上的数据集文件之前，它们会存储在 `ArrowWriter` 缓冲区中。这意味着生成的样本是按批写入的。如果您的数据集样本消耗大量内存（图像或视频），请确保在 DatasetBuilder 中指定 `DEFAULT_WRITER_BATCH_SIZE` 属性的低值。我们建议不要超过 200 MB 的大小。

## 保持完整性

为确保数据集完整，load_dataset() 将对下载的文件执行一系列测试，以确保一切就绪。这样，当您请求的数据集未按预期生成时，您就不会遇到任何意外。load_dataset() 验证：

+   生成的 `DatasetDict` 中的拆分数量。

+   生成的 `DatasetDict` 中每个拆分中的样本数量。

+   下载文件列表。

+   下载文件的 SHA256 校验和（默认情况下已禁用）。

如果数据集未通过验证，很可能是数据集的原始主机对数据文件进行了一些更改。

如果是您自己的数据集，您需要重新计算上述信息，并更新数据集存储库中的 `README.md` 文件。查看这个 section 了解如何生成和更新这些元数据。

在这种情况下，会引发错误以警示数据集已更改。要忽略错误，需要在 load_dataset() 中指定 `verification_mode="no_checks"`。每当看到验证错误时，请随时在相应数据集的“社区”选项卡中开启讨论或拉取请求，以便更新该数据集的完整性检查。

## 安全性

Hub 上的数据集存储库会进行恶意软件扫描，更多信息请参见[这里](https://huggingface.co/docs/hub/security#malware-scanning)。

此外，没有命名空间的数据集（最初在我们的 GitHub 存储库上贡献）已经由我们的维护人员进行了审核。这些数据集的代码被认为是**安全**的。这涉及到没有命名空间的数据集，例如“squad”或“glue”，不同于其他被命名为“username/dataset_name”或“org/dataset_name”的数据集。
