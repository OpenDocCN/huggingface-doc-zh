- en: Advanced Setup (Instance Types, Auto Scaling, Versioning)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜çº§è®¾ç½®ï¼ˆå®ä¾‹ç±»å‹ã€è‡ªåŠ¨ç¼©æ”¾ã€ç‰ˆæœ¬æ§åˆ¶ï¼‰
- en: 'Original text: [https://huggingface.co/docs/inference-endpoints/guides/advanced](https://huggingface.co/docs/inference-endpoints/guides/advanced)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/inference-endpoints/guides/advanced](https://huggingface.co/docs/inference-endpoints/guides/advanced)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how fast and easy it is to deploy an Endpoint in [Create your first
    Endpoint](/docs/inference-endpoints/guides/create_endpoint), but thatâ€™s not all
    you can manage. During the creation process and after selecting your Cloud Provider
    and Region, click on the [Advanced configuration] button to reveal further configuration
    options for your Endpoint.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°åœ¨[åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªç«¯ç‚¹](/docs/inference-endpoints/guides/create_endpoint)ä¸­éƒ¨ç½²ç«¯ç‚¹æ˜¯å¤šä¹ˆå¿«é€Ÿç®€ä¾¿ï¼Œä½†è¿™å¹¶ä¸æ˜¯æ‚¨å¯ä»¥ç®¡ç†çš„å…¨éƒ¨ã€‚åœ¨åˆ›å»ºè¿‡ç¨‹ä¸­å¹¶åœ¨é€‰æ‹©äº‘æä¾›å•†å’ŒåŒºåŸŸåï¼Œå•å‡»[é«˜çº§é…ç½®]æŒ‰é’®ä»¥æ˜¾ç¤ºæœ‰å…³ç«¯ç‚¹çš„è¿›ä¸€æ­¥é…ç½®é€‰é¡¹ã€‚
- en: '**Instance type**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**å®ä¾‹ç±»å‹**'
- en: ğŸ¤— Inference Endpoints offers a selection of curated CPU and GPU instances.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— æ¨ç†ç«¯ç‚¹æä¾›äº†ä¸€ç³»åˆ—ç»è¿‡ç­›é€‰çš„CPUå’ŒGPUå®ä¾‹ã€‚
- en: '*Note: Your Hugging Face account comes with a capacity quota for CPU and GPU
    instances. To increase your quota or request new instance types, please check
    with us.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šæ‚¨çš„Hugging Faceå¸æˆ·é…å¤‡äº†ç”¨äºCPUå’ŒGPUå®ä¾‹çš„å®¹é‡é…é¢ã€‚è¦å¢åŠ é…é¢æˆ–è¯·æ±‚æ–°çš„å®ä¾‹ç±»å‹ï¼Œè¯·ä¸æˆ‘ä»¬è”ç³»ã€‚*'
- en: '*Default: CPU-medium*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»˜è®¤ï¼šCPU-ä¸­ç­‰*'
- en: '![copy curl](../Images/698d77f5e435ec43634b873963aae15b.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![å¤åˆ¶curl](../Images/698d77f5e435ec43634b873963aae15b.png)'
- en: '**Replica autoscaling**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‰¯æœ¬è‡ªåŠ¨ç¼©æ”¾**'
- en: Set the range (minimum (>=1) and maximum ) of replicas you want your Endpoint
    to automatically scale within based on utilization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æ‚¨å¸Œæœ›æ‚¨çš„ç«¯ç‚¹æ ¹æ®åˆ©ç”¨ç‡è‡ªåŠ¨ç¼©æ”¾çš„å‰¯æœ¬èŒƒå›´ï¼ˆæœ€å°ï¼ˆ>=1ï¼‰å’Œæœ€å¤§ï¼‰ã€‚
- en: '*Default: min 1; max 2*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»˜è®¤ï¼šæœ€å°1ï¼›æœ€å¤§2*'
- en: '**Task**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡**'
- en: Select a [supported Machine Learning Task](/docs/inference-endpoints/supported_tasks),
    or set to [Custom](/docs/inference-endpoints/guides/custom_handler). [Custom](/docs/inference-endpoints/guides/custom_handler)
    can/should be used when you are not using a Transformers-based model or when you
    want to customize the inference pipeline, see [Create your own Inference handler](/docs/inference-endpoints/guides/custom_handler).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ª[æ”¯æŒçš„æœºå™¨å­¦ä¹ ä»»åŠ¡](/docs/inference-endpoints/supported_tasks)ï¼Œæˆ–è®¾ç½®ä¸º[è‡ªå®šä¹‰](/docs/inference-endpoints/guides/custom_handler)ã€‚å½“æ‚¨ä¸ä½¿ç”¨åŸºäºTransformersçš„æ¨¡å‹æˆ–æƒ³è¦è‡ªå®šä¹‰æ¨ç†æµç¨‹æ—¶ï¼Œåº”è¯¥ä½¿ç”¨[è‡ªå®šä¹‰](/docs/inference-endpoints/guides/custom_handler)ï¼Œè¯·å‚é˜…[åˆ›å»ºè‡ªå·±çš„æ¨ç†å¤„ç†ç¨‹åº](/docs/inference-endpoints/guides/custom_handler)ã€‚
- en: '*Default: derived from the model repository.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»˜è®¤ï¼šæ´¾ç”Ÿè‡ªæ¨¡å‹å­˜å‚¨åº“ã€‚*'
- en: '**Framework**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¡†æ¶**'
- en: For Transformers models, if both PyTorch and TensorFlow weights are both available,
    you can select which model weights to use. This will help reduce the image artifact
    size and accelerate startups/scaling of your endpoints.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºTransformersæ¨¡å‹ï¼Œå¦‚æœPyTorchå’ŒTensorFlowæƒé‡éƒ½å¯ç”¨ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ¨¡å‹æƒé‡ã€‚è¿™å°†æœ‰åŠ©äºå‡å°‘å›¾åƒä¼ªå½±å¤§å°ï¼Œå¹¶åŠ å¿«å¯åŠ¨/æ‰©å±•æ‚¨çš„ç«¯ç‚¹ã€‚
- en: '*Default: PyTorch if available.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»˜è®¤ï¼šå¦‚æœå¯ç”¨ï¼Œåˆ™ä¸ºPyTorchã€‚*'
- en: '**Revision**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¿®è®¢**'
- en: Create your Endpoint targeting a specific revision commit for its source Hugging
    Face Model Repository. This allows you to version your endpoint and make sure
    you are always using the same weights even if you are updating the Model Repository.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ‚¨çš„ç«¯ç‚¹åˆ›å»ºä¸€ä¸ªé’ˆå¯¹å…¶æºHugging Faceæ¨¡å‹å­˜å‚¨åº“çš„ç‰¹å®šä¿®è®¢æäº¤ã€‚è¿™å…è®¸æ‚¨å¯¹ç«¯ç‚¹è¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼Œå¹¶ç¡®ä¿å³ä½¿æ›´æ–°æ¨¡å‹å­˜å‚¨åº“ï¼Œæ‚¨å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„æƒé‡ã€‚
- en: '*Default: The most recent commit.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*é»˜è®¤ï¼šæœ€è¿‘çš„æäº¤ã€‚*'
- en: '**Image**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾åƒ**'
- en: Allows you to provide a custom container image you want to deploy into an Endpoint.
    Those can be public images, e.g *tensorflow/serving:2.7.3,* or private Images
    hosted on [Docker hub](https://hub.docker.com/), [AWS ECR](https://aws.amazon.com/ecr/?nc1=h_ls),
    [Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/), or
    [Google GCR](https://cloud.google.com/container-registry?hl=de).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å…è®¸æ‚¨æä¾›è¦éƒ¨ç½²åˆ°ç«¯ç‚¹çš„è‡ªå®šä¹‰å®¹å™¨æ˜ åƒã€‚è¿™äº›å¯ä»¥æ˜¯å…¬å…±æ˜ åƒï¼Œä¾‹å¦‚*tensorflow/serving:2.7.3*ï¼Œæˆ–æ‰˜ç®¡åœ¨[Docker hub](https://hub.docker.com/)ã€[AWS
    ECR](https://aws.amazon.com/ecr/?nc1=h_ls)ã€[Azure ACR](https://azure.microsoft.com/de-de/services/container-registry/)æˆ–[Google
    GCR](https://cloud.google.com/container-registry?hl=de)ä¸Šçš„ç§æœ‰æ˜ åƒã€‚
- en: More on how to [â€œUse your own custom containerâ€](/docs/inference-endpoints/guides/custom_handler)
    below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å¦‚ä½•åœ¨ä¸‹é¢[â€œä½¿ç”¨æ‚¨è‡ªå·±çš„è‡ªå®šä¹‰å®¹å™¨â€](/docs/inference-endpoints/guides/custom_handler)ã€‚
