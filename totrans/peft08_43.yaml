- en: OFT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/oft](https://huggingface.co/docs/peft/package_reference/oft)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/26.0e3fb233.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/ExampleCodeBlock.a22db1c3.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Orthogonal Finetuning (OFT)](https://hf.co/papers/2306.07280) is a method
    developed for adapting text-to-image diffusion models. It works by reparameterizing
    the pretrained weight matrices with it’s orthogonal matrix to preserve information
    in the pretrained model. To reduce the number of parameters, OFT introduces a
    block-diagonal structure in the orthogonal matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large text-to-image diffusion models have impressive capabilities in generating
    photorealistic images from text prompts. How to effectively guide or control these
    powerful models to perform different downstream tasks becomes an important open
    problem. To tackle this challenge, we introduce a principled finetuning method
    — Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to
    downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical
    energy which characterizes the pairwise neuron relationship on the unit hypersphere.
    We find that this property is crucial for preserving the semantic generation ability
    of text-to-image diffusion models. To improve finetuning stability, we further
    propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius
    constraint to the hypersphere. Specifically, we consider two important finetuning
    text-to-image tasks: subject-driven generation where the goal is to generate subject-specific
    images given a few images of a subject and a text prompt, and controllable generation
    where the goal is to enable the model to take in additional control signals. We
    empirically show that our OFT framework outperforms existing methods in generation
    quality and convergence speed*.'
  prefs: []
  type: TYPE_NORMAL
- en: OFTConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.OFTConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/config.py#L22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`r` (`int`) — OFT rank.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`module_dropout` (`int`) — The dropout probability for disabling OFT modules
    during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear modules are chosen, excluding the
    output layer. If this is not specified, modules will be chosen according to the
    model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_weights` (`bool`) — Whether to perform initialization of OFT weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modules_to_save` (`List[str]`) — List of modules apart from adapter layers
    to be set as trainable and saved in the final checkpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coft` (`bool`) — Whether to use the constrained variant of OFT or not, off
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps` (`float`) — The control strength of COFT. The freedom of rotation. Only
    has an effect if `coft` is set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`block_share` (`bool`) — Whether to share the OFT parameters between blocks
    or not. This is `False` by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [OFTModel](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTModel).
  prefs: []
  type: TYPE_NORMAL
- en: OFTModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.OFTModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/model.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig))
    — The configuration of the OFT model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.nn.Module`'
  prefs: []
  type: TYPE_NORMAL
- en: The OFT model.
  prefs: []
  type: TYPE_NORMAL
- en: Creates Orthogonal Finetuning model from a pretrained model. The method is described
    in [https://arxiv.org/abs/2306.07280](https://arxiv.org/abs/2306.07280)
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`~torch.nn.Module`) — The model to be adapted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`peft_config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig)):
    The configuration of the OFT model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
