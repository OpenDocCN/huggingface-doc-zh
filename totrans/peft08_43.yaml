- en: OFT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OFT
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/oft](https://huggingface.co/docs/peft/package_reference/oft)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/peft/package_reference/oft](https://huggingface.co/docs/peft/package_reference/oft)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Orthogonal Finetuning (OFT)](https://hf.co/papers/2306.07280) is a method
    developed for adapting text-to-image diffusion models. It works by reparameterizing
    the pretrained weight matrices with it’s orthogonal matrix to preserve information
    in the pretrained model. To reduce the number of parameters, OFT introduces a
    block-diagonal structure in the orthogonal matrix.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[正交微调（OFT）](https://hf.co/papers/2306.07280)是一种用于调整文本到图像扩散模型的方法。它通过使用正交矩阵重新参数化预训练的权重矩阵，以保留预训练模型中的信息。为了减少参数数量，OFT在正交矩阵中引入了一个块对角结构。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的摘要是：
- en: '*Large text-to-image diffusion models have impressive capabilities in generating
    photorealistic images from text prompts. How to effectively guide or control these
    powerful models to perform different downstream tasks becomes an important open
    problem. To tackle this challenge, we introduce a principled finetuning method
    — Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to
    downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical
    energy which characterizes the pairwise neuron relationship on the unit hypersphere.
    We find that this property is crucial for preserving the semantic generation ability
    of text-to-image diffusion models. To improve finetuning stability, we further
    propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius
    constraint to the hypersphere. Specifically, we consider two important finetuning
    text-to-image tasks: subject-driven generation where the goal is to generate subject-specific
    images given a few images of a subject and a text prompt, and controllable generation
    where the goal is to enable the model to take in additional control signals. We
    empirically show that our OFT framework outperforms existing methods in generation
    quality and convergence speed*.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*大型文本到图像扩散模型在从文本提示生成逼真图像方面具有令人印象深刻的能力。如何有效地引导或控制这些强大模型执行不同的下游任务成为一个重要的开放问题。为了解决这一挑战，我们引入了一种原则性的微调方法
    — 正交微调（OFT），用于将文本到图像扩散模型适应到下游任务。与现有方法不同，OFT可以明确保留超球面能量，这表征了单位超球面上的神经元之间的成对关系。我们发现这一属性对于保留文本到图像扩散模型的语义生成能力至关重要。为了提高微调的稳定性，我们进一步提出了约束正交微调（COFT），它对超球面施加了额外的半径约束。具体来说，我们考虑了两个重要的微调文本到图像任务：以主题驱动的生成为目标，即在给定主题的几张图像和文本提示的情况下生成特定主题的图像，以及可控生成为目标，即使模型能够接收额外的控制信号。我们在实证上展示了我们的OFT框架在生成质量和收敛速度方面优于现有方法*。'
- en: OFTConfig
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OFTConfig
- en: '### `class peft.OFTConfig`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.OFTConfig`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/config.py#L22)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/config.py#L22)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`r` (`int`) — OFT rank.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r` (`int`) — OFT秩。'
- en: '`module_dropout` (`int`) — The dropout probability for disabling OFT modules
    during training.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module_dropout` (`int`) — 训练期间禁用OFT模块的丢弃概率。'
- en: '`target_modules` (`Optional[Union[List[str], str]]`) — The names of the modules
    to apply the adapter to. If this is specified, only the modules with the specified
    names will be replaced. When passing a string, a regex match will be performed.
    When passing a list of strings, either an exact match will be performed or it
    is checked if the name of the module ends with any of the passed strings. If this
    is specified as ‘all-linear’, then all linear modules are chosen, excluding the
    output layer. If this is not specified, modules will be chosen according to the
    model architecture. If the architecture is not known, an error will be raised
    — in this case, you should specify the target modules manually.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_modules` (`Optional[Union[List[str], str]]`) — 要应用适配器的模块的名称。如果指定了这个，只有指定名称的模块将被替换。当传递一个字符串时，将执行正则匹配。当传递一个字符串列表时，将执行精确匹配或检查模块名称是否以任何传递的字符串结尾。如果指定为''all-linear''，则选择所有线性模块，不包括输出层。如果未指定，则根据模型架构选择模块。如果架构未知，将引发错误
    — 在这种情况下，您应该手动指定目标模块。'
- en: '`init_weights` (`bool`) — Whether to perform initialization of OFT weights.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_weights` (`bool`) — 是否执行OFT权重的初始化。'
- en: '`layers_to_transform` (`Union[List[int], int]`) — The layer indices to transform.
    If a list of ints is passed, it will apply the adapter to the layer indices that
    are specified in this list. If a single integer is passed, it will apply the transformations
    on the layer at this index.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_to_transform` (`Union[List[int], int]`) — 要转换的层索引。如果传递了一个整数列表，它将应用适配器到此列表中指定的层索引。如果传递了一个单个整数，它将在此索引处的层上应用转换。'
- en: '`layers_pattern` (`str`) — The layer pattern name, used only if `layers_to_transform`
    is different from `None`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_pattern` (`str`) — 层模式名称，仅在`layers_to_transform`与`None`不同的情况下使用。'
- en: '`rank_pattern` (`dict`) — The mapping from layer names or regexp expression
    to ranks which are different from the default rank specified by `r`.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank_pattern` (`dict`) — 层名称或正则表达式到与`r`指定的默认秩不同的秩的映射。'
- en: '`modules_to_save` (`List[str]`) — List of modules apart from adapter layers
    to be set as trainable and saved in the final checkpoint.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`modules_to_save` (`List[str]`) — 除了适配器层之外要设置为可训练并保存在最终检查点中的模块列表。'
- en: '`coft` (`bool`) — Whether to use the constrained variant of OFT or not, off
    by default.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`coft` (`bool`) — 是否使用OFT的约束变体，默认为关闭。'
- en: '`eps` (`float`) — The control strength of COFT. The freedom of rotation. Only
    has an effect if `coft` is set to True.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` (`float`) — COFT的控制强度。旋转的自由度。仅当`coft`设置为True时才有效。'
- en: '`block_share` (`bool`) — Whether to share the OFT parameters between blocks
    or not. This is `False` by default.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_share` (`bool`) — 是否在块之间共享OFT参数。默认为`False`。'
- en: This is the configuration class to store the configuration of a [OFTModel](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTModel).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[OFTModel](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTModel)配置的配置类。
- en: OFTModel
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OFTModel
- en: '### `class peft.OFTModel`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class peft.OFTModel`'
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/model.py#L26)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/oft/model.py#L26)'
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — The model to which the adapter tuner layers will
    be attached.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`torch.nn.Module`) — 适配器调谐层将附加到的模型。'
- en: '`config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig))
    — The configuration of the OFT model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig))
    — OFT模型的配置。'
- en: '`adapter_name` (`str`) — The name of the adapter, defaults to `"default"`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_name` (`str`) — 适配器的名称，默认为`"default"`。'
- en: Returns
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.nn.Module`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn.Module`'
- en: The OFT model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: OFT模型。
- en: Creates Orthogonal Finetuning model from a pretrained model. The method is described
    in [https://arxiv.org/abs/2306.07280](https://arxiv.org/abs/2306.07280)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型创建正交微调模型。该方法在[https://arxiv.org/abs/2306.07280](https://arxiv.org/abs/2306.07280)中有描述。
- en: 'Example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Attributes**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**属性**:'
- en: '`model` (`~torch.nn.Module`) — The model to be adapted.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model` (`~torch.nn.Module`) — 要适配的模型。'
- en: '`peft_config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig)):
    The configuration of the OFT model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`peft_config` ([OFTConfig](/docs/peft/v0.8.2/en/package_reference/oft#peft.OFTConfig)):
    OFT模型的配置。'
