# 训练定制

> 原始文本：[`huggingface.co/docs/trl/customization`](https://huggingface.co/docs/trl/customization)

TRL 旨在具有模块化的设计，以便用户能够有效地为其需求定制训练循环。以下是一些关于如何应用和测试不同技术的示例。

## 在多个 GPU/节点上训练

TRL 中的训练器使用🤗 Accelerate 来实现跨多个 GPU 或节点的分布式训练。要做到这一点，首先通过运行创建一个🤗 Accelerate 配置文件

```py
accelerate config
```

并根据您的多 GPU/多节点设置回答问题。然后通过运行启动分布式训练：

```py
accelerate launch your_script.py
```

我们还在[示例文件夹](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs)中提供配置文件，可用作模板。要使用这些模板，只需在启动作业时传递配置文件的路径，例如：

```py
accelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script
```

有关更多详细信息，请参考[示例页面](https://github.com/huggingface/trl/tree/main/examples)。

### 使用 DeepSpeed 进行分布式训练

TRL 中的所有训练器都可以与 DeepSpeed ZeRO-{1,2,3}一起在多个 GPU 上运行，以实现优化器状态、梯度和模型权重的有效分片。要这样做，请运行：

```py
accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_your_script.py --all_arguments_of_the_script
```

请注意，对于 ZeRO-3，需要通过`zero3_init_context_manager()`上下文管理器在正确的设备上初始化您的奖励模型进行一些小调整。特别是，这是为了避免 DeepSpeed 在固定数量的训练步骤后挂起。以下是从[`sentiment_tuning`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py)示例中涉及的部分内容：

```py
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", device=device)
else:
    sentiment_pipe = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", device=device)
```

有关 DeepSpeed 插件的更多信息，请参考🤗 Accelerate[文档](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)。

## 使用不同的优化器

默认情况下，`PPOTrainer`会创建一个`torch.optim.Adam`优化器。您可以创建并定义一个不同的优化器，并将其传递给`PPOTrainer`：

```py
import torch
from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

为了更节省内存的微调，您还可以从`bitsandbytes`传递`Adam8bit`优化器：

```py
import torch
import bitsandbytes as bnb

from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

### 使用 LION 优化器

您还可以使用来自 Google 的新[LION 优化器](https://arxiv.org/abs/2302.06675)，首先获取优化器定义的源代码[此处](https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py)，并复制它以便可以导入优化器。确保通过仅考虑可训练参数来初始化优化器，以实现更节省内存的训练：

```py
optimizer = Lion(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.config.learning_rate)

...
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

我们建议您使用`Adam`的学习率除以 3，如[此处](https://github.com/lucidrains/lion-pytorch#lion---pytorch)所指出。与经典的 Adam 相比，我们观察到使用此优化器时有所改善（查看完整日志[此处](https://wandb.ai/distill-bloom/trl/runs/lj4bheke?workspace=user-younesbelkada)）：

![](img/df158f76e736e28cb11b4474cc879ebe.png)

## 添加学习率调度器

您还可以通过添加学习率调度器来调整训练！

```py
import torch
from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer, lr_scheduler=lr_scheduler)
```

## 通过共享层进行内存高效微调

另一个可以用于更节省内存的微调的工具是在参考模型和您想要训练的模型之间共享层。

```py
import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')
model_ref = create_reference_model(model, num_shared_layers=6)
tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')

# 2\. initialize trainer
ppo_config = {'batch_size': 1}
config = PPOConfig(**ppo_config)
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)
```

## 传递 8 位参考模型

由于`trl`在使用`from_pretrained`从`transformers`加载模型时支持所有关键字参数，您还可以利用`transformers`中的`load_in_8bit`进行更节省内存的微调。

在`transformers`中阅读有关 8 位模型加载的更多信息[此处](https://huggingface.co/docs/transformers/perf_infer_gpu_one#bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition)。

```py
# 0\. imports
# pip install bitsandbytes
import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m', device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')

# 2\. initialize trainer
ppo_config = {'batch_size': 1}
config = PPOConfig(**ppo_config)
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)
```

## 使用 CUDA 缓存优化器

在训练大型模型时，最好通过迭代清除 CUDA 缓存来处理 CUDA 缓存。要这样做，只需将`optimize_cuda_cache=True`传递给`PPOConfig`：

```py
config = PPOConfig(..., optimize_cuda_cache=True)
```

## 使用分数缩放/归一化/裁剪

正如[大型语言模型 RLHF 的秘密第一部分：PPO](https://arxiv.org/abs/2307.04964)所建议的，我们支持通过`PPOConfig`进行分数（又名奖励）缩放/归一化/剪切，以改善训练稳定性。

```py
from trl import PPOConfig

ppo_config = {
    use_score_scaling=True,
    use_score_norm=True,
    score_clip=0.5,
}
config = PPOConfig(**ppo_config)
```

要运行`ppo.py`，您可以使用以下命令：

```py
python examples/scripts/ppo.py --log_with wandb --use_score_scaling --use_score_norm --score_clip 0.5
```
