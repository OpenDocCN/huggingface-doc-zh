# è®­ç»ƒå®šåˆ¶

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/trl/customization`](https://huggingface.co/docs/trl/customization)

TRL æ—¨åœ¨å…·æœ‰æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸ºå…¶éœ€æ±‚å®šåˆ¶è®­ç»ƒå¾ªç¯ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºå¦‚ä½•åº”ç”¨å’Œæµ‹è¯•ä¸åŒæŠ€æœ¯çš„ç¤ºä¾‹ã€‚

## åœ¨å¤šä¸ª GPU/èŠ‚ç‚¹ä¸Šè®­ç»ƒ

TRL ä¸­çš„è®­ç»ƒå™¨ä½¿ç”¨ğŸ¤— Accelerate æ¥å®ç°è·¨å¤šä¸ª GPU æˆ–èŠ‚ç‚¹çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œé¦–å…ˆé€šè¿‡è¿è¡Œåˆ›å»ºä¸€ä¸ªğŸ¤— Accelerate é…ç½®æ–‡ä»¶

```py
accelerate config
```

å¹¶æ ¹æ®æ‚¨çš„å¤š GPU/å¤šèŠ‚ç‚¹è®¾ç½®å›ç­”é—®é¢˜ã€‚ç„¶åé€šè¿‡è¿è¡Œå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼š

```py
accelerate launch your_script.py
```

æˆ‘ä»¬è¿˜åœ¨[ç¤ºä¾‹æ–‡ä»¶å¤¹](https://github.com/huggingface/trl/tree/main/examples/accelerate_configs)ä¸­æä¾›é…ç½®æ–‡ä»¶ï¼Œå¯ç”¨ä½œæ¨¡æ¿ã€‚è¦ä½¿ç”¨è¿™äº›æ¨¡æ¿ï¼Œåªéœ€åœ¨å¯åŠ¨ä½œä¸šæ—¶ä¼ é€’é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œä¾‹å¦‚ï¼š

```py
accelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ[ç¤ºä¾‹é¡µé¢](https://github.com/huggingface/trl/tree/main/examples)ã€‚

### ä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

TRL ä¸­çš„æ‰€æœ‰è®­ç»ƒå™¨éƒ½å¯ä»¥ä¸ DeepSpeed ZeRO-{1,2,3}ä¸€èµ·åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œï¼Œä»¥å®ç°ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œæ¨¡å‹æƒé‡çš„æœ‰æ•ˆåˆ†ç‰‡ã€‚è¦è¿™æ ·åšï¼Œè¯·è¿è¡Œï¼š

```py
accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_your_script.py --all_arguments_of_the_script
```

è¯·æ³¨æ„ï¼Œå¯¹äº ZeRO-3ï¼Œéœ€è¦é€šè¿‡`zero3_init_context_manager()`ä¸Šä¸‹æ–‡ç®¡ç†å™¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šåˆå§‹åŒ–æ‚¨çš„å¥–åŠ±æ¨¡å‹è¿›è¡Œä¸€äº›å°è°ƒæ•´ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™æ˜¯ä¸ºäº†é¿å… DeepSpeed åœ¨å›ºå®šæ•°é‡çš„è®­ç»ƒæ­¥éª¤åæŒ‚èµ·ã€‚ä»¥ä¸‹æ˜¯ä»[`sentiment_tuning`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py)ç¤ºä¾‹ä¸­æ¶‰åŠçš„éƒ¨åˆ†å†…å®¹ï¼š

```py
ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin
if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():
    with ds_plugin.zero3_init_context_manager(enable=False):
        sentiment_pipe = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", device=device)
else:
    sentiment_pipe = pipeline("sentiment-analysis", model="lvwerra/distilbert-imdb", device=device)
```

æœ‰å…³ DeepSpeed æ’ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒğŸ¤— Accelerate[æ–‡æ¡£](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)ã€‚

## ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`PPOTrainer`ä¼šåˆ›å»ºä¸€ä¸ª`torch.optim.Adam`ä¼˜åŒ–å™¨ã€‚æ‚¨å¯ä»¥åˆ›å»ºå¹¶å®šä¹‰ä¸€ä¸ªä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™`PPOTrainer`ï¼š

```py
import torch
from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

ä¸ºäº†æ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒï¼Œæ‚¨è¿˜å¯ä»¥ä»`bitsandbytes`ä¼ é€’`Adam8bit`ä¼˜åŒ–å™¨ï¼š

```py
import torch
import bitsandbytes as bnb

from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

### ä½¿ç”¨ LION ä¼˜åŒ–å™¨

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨æ¥è‡ª Google çš„æ–°[LION ä¼˜åŒ–å™¨](https://arxiv.org/abs/2302.06675)ï¼Œé¦–å…ˆè·å–ä¼˜åŒ–å™¨å®šä¹‰çš„æºä»£ç [æ­¤å¤„](https://github.com/lucidrains/lion-pytorch/blob/main/lion_pytorch/lion_pytorch.py)ï¼Œå¹¶å¤åˆ¶å®ƒä»¥ä¾¿å¯ä»¥å¯¼å…¥ä¼˜åŒ–å™¨ã€‚ç¡®ä¿é€šè¿‡ä»…è€ƒè™‘å¯è®­ç»ƒå‚æ•°æ¥åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼Œä»¥å®ç°æ›´èŠ‚çœå†…å­˜çš„è®­ç»ƒï¼š

```py
optimizer = Lion(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.config.learning_rate)

...
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)
```

æˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨`Adam`çš„å­¦ä¹ ç‡é™¤ä»¥ 3ï¼Œå¦‚[æ­¤å¤„](https://github.com/lucidrains/lion-pytorch#lion---pytorch)æ‰€æŒ‡å‡ºã€‚ä¸ç»å…¸çš„ Adam ç›¸æ¯”ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä½¿ç”¨æ­¤ä¼˜åŒ–å™¨æ—¶æœ‰æ‰€æ”¹å–„ï¼ˆæŸ¥çœ‹å®Œæ•´æ—¥å¿—[æ­¤å¤„](https://wandb.ai/distill-bloom/trl/runs/lj4bheke?workspace=user-younesbelkada)ï¼‰ï¼š

![](img/df158f76e736e28cb11b4474cc879ebe.png)

## æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨

æ‚¨è¿˜å¯ä»¥é€šè¿‡æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨æ¥è°ƒæ•´è®­ç»ƒï¼

```py
import torch
from transformers import GPT2Tokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 2\. define config
ppo_config = {'batch_size': 1, 'learning_rate':1e-5}
config = PPOConfig(**ppo_config)

# 2\. Create optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

# 3\. initialize trainer
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer, lr_scheduler=lr_scheduler)
```

## é€šè¿‡å…±äº«å±‚è¿›è¡Œå†…å­˜é«˜æ•ˆå¾®è°ƒ

å¦ä¸€ä¸ªå¯ä»¥ç”¨äºæ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒçš„å·¥å…·æ˜¯åœ¨å‚è€ƒæ¨¡å‹å’Œæ‚¨æƒ³è¦è®­ç»ƒçš„æ¨¡å‹ä¹‹é—´å…±äº«å±‚ã€‚

```py
import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')
model_ref = create_reference_model(model, num_shared_layers=6)
tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')

# 2\. initialize trainer
ppo_config = {'batch_size': 1}
config = PPOConfig(**ppo_config)
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)
```

## ä¼ é€’ 8 ä½å‚è€ƒæ¨¡å‹

ç”±äº`trl`åœ¨ä½¿ç”¨`from_pretrained`ä»`transformers`åŠ è½½æ¨¡å‹æ—¶æ”¯æŒæ‰€æœ‰å…³é”®å­—å‚æ•°ï¼Œæ‚¨è¿˜å¯ä»¥åˆ©ç”¨`transformers`ä¸­çš„`load_in_8bit`è¿›è¡Œæ›´èŠ‚çœå†…å­˜çš„å¾®è°ƒã€‚

åœ¨`transformers`ä¸­é˜…è¯»æœ‰å…³ 8 ä½æ¨¡å‹åŠ è½½çš„æ›´å¤šä¿¡æ¯[æ­¤å¤„](https://huggingface.co/docs/transformers/perf_infer_gpu_one#bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition)ã€‚

```py
# 0\. imports
# pip install bitsandbytes
import torch
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead

# 1\. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m', device_map="auto", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')

# 2\. initialize trainer
ppo_config = {'batch_size': 1}
config = PPOConfig(**ppo_config)
ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)
```

## ä½¿ç”¨ CUDA ç¼“å­˜ä¼˜åŒ–å™¨

åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œæœ€å¥½é€šè¿‡è¿­ä»£æ¸…é™¤ CUDA ç¼“å­˜æ¥å¤„ç† CUDA ç¼“å­˜ã€‚è¦è¿™æ ·åšï¼Œåªéœ€å°†`optimize_cuda_cache=True`ä¼ é€’ç»™`PPOConfig`ï¼š

```py
config = PPOConfig(..., optimize_cuda_cache=True)
```

## ä½¿ç”¨åˆ†æ•°ç¼©æ”¾/å½’ä¸€åŒ–/è£å‰ª

æ­£å¦‚[å¤§å‹è¯­è¨€æ¨¡å‹ RLHF çš„ç§˜å¯†ç¬¬ä¸€éƒ¨åˆ†ï¼šPPO](https://arxiv.org/abs/2307.04964)æ‰€å»ºè®®çš„ï¼Œæˆ‘ä»¬æ”¯æŒé€šè¿‡`PPOConfig`è¿›è¡Œåˆ†æ•°ï¼ˆåˆåå¥–åŠ±ï¼‰ç¼©æ”¾/å½’ä¸€åŒ–/å‰ªåˆ‡ï¼Œä»¥æ”¹å–„è®­ç»ƒç¨³å®šæ€§ã€‚

```py
from trl import PPOConfig

ppo_config = {
    use_score_scaling=True,
    use_score_norm=True,
    score_clip=0.5,
}
config = PPOConfig(**ppo_config)
```

è¦è¿è¡Œ`ppo.py`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```py
python examples/scripts/ppo.py --log_with wandb --use_score_scaling --use_score_norm --score_clip 0.5
```
