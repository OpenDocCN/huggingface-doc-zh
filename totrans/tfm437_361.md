# OneFormer

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer)

## æ¦‚è¿°

OneFormeræ¨¡å‹æ˜¯ç”±Jitesh Jainã€Jiachen Liã€MangTik Chiuã€Ali Hassaniã€Nikita Orlovã€Humphrey Shiåœ¨[OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)ä¸­æå‡ºçš„ã€‚OneFormeræ˜¯ä¸€ä¸ªé€šç”¨çš„å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå¯ä»¥åœ¨å•ä¸ªå…¨æ™¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ‰§è¡Œè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ã€‚OneFormerä½¿ç”¨ä»»åŠ¡æ ‡è®°æ¥ä½¿æ¨¡å‹åœ¨å…³æ³¨çš„ä»»åŠ¡ä¸Šè¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿æ¶æ„åœ¨è®­ç»ƒæ—¶å—ä»»åŠ¡å¼•å¯¼ï¼Œåœ¨æ¨æ–­æ—¶åŠ¨æ€é€‚åº”ä»»åŠ¡ã€‚

![](../Images/8cd6226185941da2d5d5f4ca249f7bd2.png)

è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*é€šç”¨å›¾åƒåˆ†å‰²å¹¶ä¸æ˜¯ä¸€ä¸ªæ–°æ¦‚å¿µã€‚è¿‡å»å‡ åå¹´æ¥ç»Ÿä¸€å›¾åƒåˆ†å‰²çš„å°è¯•åŒ…æ‹¬åœºæ™¯è§£æã€å…¨æ™¯åˆ†å‰²ï¼Œä»¥åŠæœ€è¿‘çš„æ–°å…¨æ™¯æ¶æ„ã€‚ç„¶è€Œï¼Œè¿™äº›å…¨æ™¯æ¶æ„å¹¶ä¸èƒ½çœŸæ­£ç»Ÿä¸€å›¾åƒåˆ†å‰²ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦åˆ†åˆ«åœ¨è¯­ä¹‰ã€å®ä¾‹æˆ–å…¨æ™¯åˆ†å‰²ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä¸€ä¸ªçœŸæ­£é€šç”¨çš„æ¡†æ¶åº”è¯¥åªéœ€è¦è®­ç»ƒä¸€æ¬¡ï¼Œå¹¶åœ¨æ‰€æœ‰ä¸‰ä¸ªå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå®ç°SOTAæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†OneFormerï¼Œä¸€ä¸ªé€šç”¨å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¤šä»»åŠ¡ä¸€æ¬¡è®­ç»ƒçš„è®¾è®¡ç»Ÿä¸€äº†åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªä»»åŠ¡æ¡ä»¶è”åˆè®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—å¯ä»¥åœ¨å•ä¸ªå¤šä»»åŠ¡è®­ç»ƒè¿‡ç¨‹ä¸­è®­ç»ƒæ¯ä¸ªé¢†åŸŸï¼ˆè¯­ä¹‰ã€å®ä¾‹å’Œå…¨æ™¯åˆ†å‰²ï¼‰çš„åœ°é¢çœŸç›¸ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä»»åŠ¡æ ‡è®°ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰‹å¤´çš„ä»»åŠ¡ä¸Šè¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹æ”¯æŒå¤šä»»åŠ¡è®­ç»ƒå’Œæ¨æ–­ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºåœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨æŸ¥è¯¢æ–‡æœ¬å¯¹æ¯”æŸå¤±ï¼Œä»¥å»ºç«‹æ›´å¥½çš„ä»»åŠ¡é—´å’Œç±»é—´åŒºåˆ«ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å•ä¸ªOneFormeræ¨¡å‹åœ¨ADE20kã€CityScapeså’ŒCOCOä¸Šçš„æ‰€æœ‰ä¸‰ä¸ªåˆ†å‰²ä»»åŠ¡ä¸­å‡ä¼˜äºä¸“é—¨çš„Mask2Formeræ¨¡å‹ï¼Œå°½ç®¡åè€…åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šåˆ†åˆ«ä½¿ç”¨äº†ä¸‰å€çš„èµ„æºè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡æ–°çš„ConvNeXtå’ŒDiNATéª¨å¹²ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ›´å¤šçš„æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬ç›¸ä¿¡OneFormeræ˜¯ä½¿å›¾åƒåˆ†å‰²æ›´åŠ é€šç”¨å’Œå¯è®¿é—®çš„é‡è¦ä¸€æ­¥ã€‚*

ä¸‹å›¾å±•ç¤ºäº†OneFormerçš„æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2211.06220)ã€‚

![](../Images/8617928e627dd67ce070d58bb21d63e1.png)

è¿™ä¸ªæ¨¡å‹æ˜¯ç”±[Jitesh Jain](https://huggingface.co/praeclarumjj3)è´¡çŒ®çš„ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/SHI-Labs/OneFormer)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   åœ¨æ¨æ–­æœŸé—´ï¼ŒOneFormeréœ€è¦ä¸¤ä¸ªè¾“å…¥ï¼š*å›¾åƒ*å’Œ*ä»»åŠ¡æ ‡è®°*ã€‚

+   åœ¨è®­ç»ƒæœŸé—´ï¼ŒOneFormeråªä½¿ç”¨å…¨æ™¯æ³¨é‡Šã€‚

+   å¦‚æœè¦åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šçš„åˆ†å¸ƒå¼ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œåˆ™åº”è¯¥åœ¨`modeling_oneformer.py`çš„`OneFormerLoss`ç±»ä¸­æ›´æ–°`get_num_masks`å‡½æ•°ã€‚åœ¨å¤šèŠ‚ç‚¹è®­ç»ƒæ—¶ï¼Œè¿™åº”è¯¥è®¾ç½®ä¸ºæ‰€æœ‰èŠ‚ç‚¹ä¸Šç›®æ ‡æ©ç çš„å¹³å‡æ•°é‡ï¼Œå¯ä»¥åœ¨åŸå§‹å®ç°ä¸­çœ‹åˆ°[è¿™é‡Œ](https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/criterion.py#L287)ã€‚

+   å¯ä»¥ä½¿ç”¨[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)æ¥å‡†å¤‡è¾“å…¥å›¾åƒå’Œæ¨¡å‹çš„ä»»åŠ¡è¾“å…¥ï¼Œä»¥åŠå¯é€‰çš„æ¨¡å‹ç›®æ ‡ã€‚ `OneformerProcessor`å°†[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)å’Œ[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)å°è£…æˆä¸€ä¸ªå•ä¸€å®ä¾‹ï¼Œæ—¢å¯ä»¥å‡†å¤‡å›¾åƒåˆå¯ä»¥ç¼–ç ä»»åŠ¡è¾“å…¥ã€‚

+   è¦è·å¾—æœ€ç»ˆçš„åˆ†å‰²ç»“æœï¼Œå¯ä»¥è°ƒç”¨[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)æˆ–[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ã€‚è¿™ä¸‰ä¸ªä»»åŠ¡éƒ½å¯ä»¥ä½¿ç”¨[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)çš„è¾“å‡ºæ¥è§£å†³ï¼Œå…¨æ™¯åˆ†å‰²æ¥å—ä¸€ä¸ªå¯é€‰çš„`label_ids_to_fuse`å‚æ•°ï¼Œç”¨äºå°†ç›®æ ‡å¯¹è±¡ï¼ˆä¾‹å¦‚å¤©ç©ºï¼‰çš„å®ä¾‹èåˆåœ¨ä¸€èµ·ã€‚

## èµ„æº

ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨OneFormerã€‚

+   å…³äºæ¨æ–­+åœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer)æ‰¾åˆ°ã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ã€‚èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## OneFormerç‰¹å®šè¾“å‡º

### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)

```py
( encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )
```

å‚æ•°

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” `torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_object_queries` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚

+   `transformer_decoder_contrastive_queries` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`) â€” æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚

+   `transformer_decoder_mask_predictions` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚

+   `transformer_decoder_class_predictions` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`) â€” æ¥è‡ªtransformerè§£ç å™¨æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚

+   `transformer_decoder_auxiliary_predictions`ï¼ˆ`str, torch.FloatTensor`å­—å…¸çš„å…ƒç»„ï¼Œ*å¯é€‰*ï¼‰ â€” æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„å…ƒç»„ã€‚

+   `text_queries` (`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`) â€” ä»ç”¨äºè®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚

+   `task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚

+   `attentions`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚transformerè§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

ç”¨äº[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)è¾“å‡ºçš„ç±»ã€‚æ­¤ç±»è¿”å›è®¡ç®—logitsæ‰€éœ€çš„æ‰€æœ‰éšè—çŠ¶æ€ã€‚

### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)

```py
( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_predictions: List = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )
```

å‚æ•°

+   `loss`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”å½“å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›è®¡ç®—çš„æŸå¤±ã€‚

+   `class_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„å»ºè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œéœ€è¦`+ 1`ï¼Œå› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»ã€‚

+   `masks_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„å»ºè®®æ©ç ã€‚

+   `auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_object_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚

+   `transformer_decoder_contrastive_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚

+   `transformer_decoder_mask_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚

+   `transformer_decoder_class_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`çš„`torch.FloatTensor`ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚

+   `transformer_decoder_auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚

+   `task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ã€‚æ¥è‡ªtransformerè§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

ç”¨äº`OneFormerForUniversalSegmentationOutput`çš„è¾“å‡ºç±»ã€‚

æ­¤è¾“å‡ºå¯ä»¥ç›´æ¥ä¼ é€’ç»™[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)æˆ–[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)æˆ–[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ã€‚è¯·å‚é˜…[`~OneFormerImageProcessor]ä»¥è·å–æœ‰å…³ç”¨æ³•çš„è¯¦ç»†ä¿¡æ¯ã€‚

## OneFormerConfig

### `class transformers.OneFormerConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)

```py
( backbone_config: Optional = None ignore_value: int = 255 num_queries: int = 150 no_object_weight: int = 0.1 class_weight: float = 2.0 mask_weight: float = 5.0 dice_weight: float = 5.0 contrastive_weight: float = 0.5 contrastive_temperature: float = 0.07 train_num_points: int = 12544 oversample_ratio: float = 3.0 importance_sample_ratio: float = 0.75 init_std: float = 0.02 init_xavier_std: float = 1.0 layer_norm_eps: float = 1e-05 is_training: bool = False use_auxiliary_loss: bool = True output_auxiliary_logits: bool = True strides: Optional = [4, 8, 16, 32] task_seq_len: int = 77 text_encoder_width: int = 256 text_encoder_context_length: int = 77 text_encoder_num_layers: int = 6 text_encoder_vocab_size: int = 49408 text_encoder_proj_layers: int = 2 text_encoder_n_ctx: int = 16 conv_dim: int = 256 mask_dim: int = 256 hidden_dim: int = 256 encoder_feedforward_dim: int = 1024 norm: str = 'GN' encoder_layers: int = 6 decoder_layers: int = 10 use_task_norm: bool = True num_attention_heads: int = 8 dropout: float = 0.1 dim_feedforward: int = 2048 pre_norm: bool = False enforce_input_proj: bool = False query_dec_layers: int = 2 common_stride: int = 4 **kwargs )
```

å‚æ•°

+   `backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`) â€” ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚

+   `ignore_value` (`int`, *optional*, defaults to 255) â€” åœ¨è®¡ç®—æŸå¤±æ—¶è¦å¿½ç•¥çš„GTæ ‡ç­¾ä¸­çš„å€¼ã€‚

+   `num_queries` (`int`, *optional*, defaults to 150) â€” å¯¹è±¡æŸ¥è¯¢çš„æ•°é‡ã€‚

+   `no_object_weight` (`float`, *optional*, defaults to 0.1) â€” æ— å¯¹è±¡ç±»é¢„æµ‹çš„æƒé‡ã€‚

+   `class_weight` (`float`, *optional*, defaults to 2.0) â€” åˆ†ç±»CEæŸå¤±çš„æƒé‡ã€‚

+   `mask_weight` (`float`, *optional*, defaults to 5.0) â€” äºŒå…ƒCEæŸå¤±çš„æƒé‡ã€‚

+   `dice_weight` (`float`, *optional*, defaults to 5.0) â€” DiceæŸå¤±çš„æƒé‡ã€‚

+   `contrastive_weight` (`float`, *optional*, defaults to 0.5) â€” å¯¹æ¯”æŸå¤±çš„æƒé‡ã€‚

+   `contrastive_temperature` (`float`, *optional*, defaults to 0.07) â€” ç”¨äºç¼©æ”¾å¯¹æ¯”å¯¹æ•°çš„åˆå§‹å€¼ã€‚

+   `train_num_points` (`int`, *optional*, defaults to 12544) â€” åœ¨è®¡ç®—æ©ç é¢„æµ‹æŸå¤±æ—¶è¦é‡‡æ ·çš„ç‚¹æ•°ã€‚

+   `oversample_ratio` (`float`, *optional*, defaults to 3.0) â€” å†³å®šè¿‡é‡‡æ ·å¤šå°‘ç‚¹çš„æ¯”ç‡ã€‚

+   `importance_sample_ratio` (`float`, *optional*, defaults to 0.75) â€” é€šè¿‡é‡è¦æ€§é‡‡æ ·æŠ½æ ·çš„ç‚¹çš„æ¯”ç‡ã€‚

+   `init_std` (`float`, *optional*, defaults to 0.02) â€” æ­£æ€åˆå§‹åŒ–çš„æ ‡å‡†å·®ã€‚

+   `init_xavier_std` (`float`, *optional*, defaults to 1.0) â€” ç”¨äºxavierå‡åŒ€åˆå§‹åŒ–çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” å±‚å½’ä¸€åŒ–çš„epsilonã€‚

+   `is_training` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è®­ç»ƒæˆ–æ¨ç†æ¨¡å¼ä¸‹è¿è¡Œã€‚

+   `use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä½¿ç”¨transformerè§£ç å™¨çš„ä¸­é—´é¢„æµ‹è®¡ç®—æŸå¤±ã€‚

+   `output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä»transformerè§£ç å™¨è¿”å›ä¸­é—´é¢„æµ‹ã€‚

+   `strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) â€” åŒ…å«ç¼–ç å™¨ä¸­ç‰¹å¾å›¾çš„æ­¥å¹…çš„åˆ—è¡¨ã€‚

+   `task_seq_len` (`int`, *optional*, defaults to 77) â€” ç”¨äºå¯¹æ–‡æœ¬åˆ—è¡¨è¾“å…¥è¿›è¡Œåˆ†è¯çš„åºåˆ—é•¿åº¦ã€‚

+   `text_encoder_width` (`int`, *optional*, defaults to 256) â€” æ–‡æœ¬ç¼–ç å™¨çš„éšè—å¤§å°ã€‚

+   `text_encoder_context_length` (`int`, *optional*, defaults to 77) â€” æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥åºåˆ—é•¿åº¦ã€‚

+   `text_encoder_num_layers` (`int`, *optional*, defaults to 6) â€” æ–‡æœ¬ç¼–ç å™¨ä¸­transformerçš„å±‚æ•°ã€‚

+   `text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) â€” åˆ†è¯å™¨çš„è¯æ±‡é‡ã€‚

+   `text_encoder_proj_layers` (`int`, *optional*, defaults to 2) â€” ç”¨äºé¡¹ç›®æ–‡æœ¬æŸ¥è¯¢çš„MLPä¸­çš„å±‚æ•°ã€‚

+   `text_encoder_n_ctx` (`int`, *optional*, é»˜è®¤ä¸º 16) â€” å¯å­¦ä¹ æ–‡æœ¬ä¸Šä¸‹æ–‡æŸ¥è¯¢çš„æ•°é‡ã€‚

+   `conv_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” ä»éª¨å¹²ç½‘ç»œæ˜ å°„è¾“å‡ºçš„ç‰¹å¾å›¾ç»´åº¦ã€‚

+   `mask_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” åƒç´ è§£ç å™¨ä¸­ç‰¹å¾å›¾çš„ç»´åº¦ã€‚

+   `hidden_dim` (`int`, *optional*, é»˜è®¤ä¸º 256) â€” å˜å‹å™¨è§£ç å™¨ä¸­éšè—çŠ¶æ€çš„ç»´åº¦ã€‚

+   `encoder_feedforward_dim` (`int`, *optional*, é»˜è®¤ä¸º 1024) â€” åƒç´ è§£ç å™¨ä¸­FFNå±‚çš„ç»´åº¦ã€‚

+   `norm` (`str`, *optional*, é»˜è®¤ä¸º `"GN"`) â€” å½’ä¸€åŒ–ç±»å‹ã€‚

+   `encoder_layers` (`int`, *optional*, é»˜è®¤ä¸º 6) â€” åƒç´ è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `decoder_layers` (`int`, *optional*, é»˜è®¤ä¸º 10) â€” å˜å‹å™¨è§£ç å™¨ä¸­çš„å±‚æ•°ã€‚

+   `use_task_norm` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹ä»»åŠ¡ä»¤ç‰Œè¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º 8) â€” åƒç´ å’Œå˜å‹å™¨è§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `dropout` (`float`, *optional*, é»˜è®¤ä¸º 0.1) â€” åƒç´ å’Œå˜å‹å™¨è§£ç å™¨çš„ä¸¢å¤±æ¦‚ç‡ã€‚

+   `dim_feedforward` (`int`, *optional*, é»˜è®¤ä¸º 2048) â€” å˜å‹å™¨è§£ç å™¨ä¸­FFNå±‚çš„ç»´åº¦ã€‚

+   `pre_norm` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å˜å‹å™¨è§£ç å™¨ä¸­çš„æ³¨æ„åŠ›å±‚ä¹‹å‰å¯¹éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `enforce_input_proj` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨å˜å‹å™¨è§£ç å™¨ä¸­æŠ•å½±éšè—çŠ¶æ€ã€‚

+   `query_dec_layers` (`int`, *optional*, é»˜è®¤ä¸º 2) â€” æŸ¥è¯¢å˜å‹å™¨ä¸­çš„å±‚æ•°ã€‚

+   `common_stride` (`int`, *optional*, é»˜è®¤ä¸º 4) â€” ç”¨äºåƒç´ è§£ç å™¨ä¸­ç‰¹å¾çš„å¸¸ç”¨æ­¥å¹…ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel) é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª OneFormer æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºåœ¨ [ADE20k-150](https://huggingface.co/datasets/scene_parse_150) ä¸Šè®­ç»ƒçš„ OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) å¹¶å¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import OneFormerConfig, OneFormerModel

>>> # Initializing a OneFormer shi-labs/oneformer_ade20k_swin_tiny configuration
>>> configuration = OneFormerConfig()
>>> # Initializing a model (with random weights) from the shi-labs/oneformer_ade20k_swin_tiny style configuration
>>> model = OneFormerModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## OneFormerImageProcessor

### `class transformers.OneFormerImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: bool = False repo_path: Optional = 'shi-labs/oneformer_demo' class_info_file: str = None num_text: Optional = None **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥è°ƒæ•´å¤§å°åˆ°ç‰¹å®šçš„ `size`ã€‚

+   `size` (`int`, *optional*, é»˜è®¤ä¸º 800) â€” å°†è¾“å…¥è°ƒæ•´å¤§å°åˆ°ç»™å®šå¤§å°ã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªç±»ä¼¼ `(width, height)` çš„åºåˆ—ï¼Œåˆ™è¾“å‡ºå¤§å°å°†åŒ¹é…åˆ°è¿™ä¸ªã€‚å¦‚æœ size æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œå›¾åƒçš„è¾ƒå°è¾¹å°†åŒ¹é…åˆ°è¿™ä¸ªæ•°å­—ã€‚å³ï¼Œå¦‚æœ `height > width`ï¼Œåˆ™å›¾åƒå°†é‡æ–°ç¼©æ”¾ä¸º `(size * height / width, size)`ã€‚

+   `resample` (`int`, *optional*, é»˜è®¤ä¸º `Resampling.BILINEAR`) â€” å¯é€‰çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚å¯ä»¥æ˜¯ `PIL.Image.Resampling.NEAREST`, `PIL.Image.Resampling.BOX`, `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`, `PIL.Image.Resampling.BICUBIC` æˆ– `PIL.Image.Resampling.LANCZOS` ä¸­çš„ä¸€ä¸ªã€‚ä»…åœ¨ `do_resize` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥é‡æ–°ç¼©æ”¾åˆ°ç‰¹å®šçš„ `scale`ã€‚

+   `rescale_factor` (`float`, *optional*, é»˜è®¤ä¸º `1/ 255`) â€” é€šè¿‡ç»™å®šå› å­é‡æ–°ç¼©æ”¾è¾“å…¥ã€‚ä»…åœ¨ `do_rescale` è®¾ç½®ä¸º `True` æ—¶æœ‰æ•ˆã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œå‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–ã€‚

+   `image_mean`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.485, 0.456, 0.406]`ï¼‰â€” æ¯ä¸ªé€šé“çš„å‡å€¼åºåˆ—ï¼Œåœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºImageNetå‡å€¼ã€‚

+   `image_std`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.229, 0.224, 0.225]`ï¼‰â€” æ¯ä¸ªé€šé“çš„æ ‡å‡†å·®åºåˆ—ï¼Œåœ¨è§„èŒƒåŒ–å›¾åƒæ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºImageNetæ ‡å‡†å·®ã€‚

+   `ignore_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” åˆ†å‰²åœ°å›¾ä¸­è¦åˆ†é…ç»™èƒŒæ™¯åƒç´ çš„æ ‡ç­¾ã€‚å¦‚æœæä¾›ï¼Œç”¨0ï¼ˆèƒŒæ™¯ï¼‰è¡¨ç¤ºçš„åˆ†å‰²åœ°å›¾åƒç´ å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚

+   `do_reduce_labels`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†æ‰€æœ‰åˆ†å‰²åœ°å›¾çš„æ ‡ç­¾å€¼å‡1ã€‚é€šå¸¸ç”¨äºæ•°æ®é›†ä¸­ä½¿ç”¨0è¡¨ç¤ºèƒŒæ™¯ï¼Œå¹¶ä¸”èƒŒæ™¯æœ¬èº«ä¸åŒ…å«åœ¨æ•°æ®é›†çš„æ‰€æœ‰ç±»ä¸­çš„æƒ…å†µï¼ˆä¾‹å¦‚ADE20kï¼‰ã€‚èƒŒæ™¯æ ‡ç­¾å°†è¢«æ›¿æ¢ä¸º`ignore_index`ã€‚

+   `repo_path`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"shi-labs/oneformer_demo"`ï¼‰â€” åŒ…å«æ•°æ®é›†ç±»ä¿¡æ¯çš„JSONæ–‡ä»¶çš„hubå­˜å‚¨åº“æˆ–æœ¬åœ°ç›®å½•çš„è·¯å¾„ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†åœ¨å½“å‰å·¥ä½œç›®å½•ä¸­æŸ¥æ‰¾`class_info_file`ã€‚

+   `class_info_file`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” åŒ…å«æ•°æ®é›†ç±»ä¿¡æ¯çš„JSONæ–‡ä»¶ã€‚æŸ¥çœ‹`shi-labs/oneformer_demo/cityscapes_panoptic.json`ä»¥è·å–ç¤ºä¾‹ã€‚

+   `num_text`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ–‡æœ¬è¾“å…¥åˆ—è¡¨ä¸­çš„æ–‡æœ¬æ¡ç›®æ•°ã€‚

æ„å»ºä¸€ä¸ªOneFormerå›¾åƒå¤„ç†å™¨ã€‚è¯¥å›¾åƒå¤„ç†å™¨å¯ç”¨äºä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒã€ä»»åŠ¡è¾“å…¥ä»¥åŠå¯é€‰çš„æ–‡æœ¬è¾“å…¥å’Œç›®æ ‡ã€‚

è¿™ä¸ªå›¾åƒå¤„ç†å™¨ç»§æ‰¿è‡ª`BaseImageProcessor`ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¿™ä¸ªè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)

```py
( images: Union task_inputs: Optional = None segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

#### `encode_inputs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)

```py
( pixel_values_list: List task_inputs: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) â†’ export const metadata = 'undefined';BatchFeature
```

å‚æ•°

+   `pixel_values_list`ï¼ˆ`List[ImageInput]`ï¼‰â€” è¦å¡«å……çš„å›¾åƒï¼ˆåƒç´ å€¼ï¼‰åˆ—è¡¨ã€‚æ¯ä¸ªå›¾åƒåº”è¯¥æ˜¯å½¢çŠ¶ä¸º`(channels, height, width)`çš„å¼ é‡ã€‚

+   `task_inputs`ï¼ˆ`List[str]`ï¼‰â€” ä»»åŠ¡å€¼åˆ—è¡¨ã€‚

+   `segmentation_maps`ï¼ˆ`ImageInput`ï¼Œ*å¯é€‰*ï¼‰â€” å…·æœ‰åƒç´ çº§æ³¨é‡Šçš„ç›¸åº”è¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚

    ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ï¼šæ˜¯å¦å°†å›¾åƒå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒå¹¶åˆ›å»ºåƒç´ æ©ç ã€‚

    å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†è¿”å›ä¸€ä¸ªåƒç´ æ©ç ï¼Œå³ï¼š

    +   å¯¹äºçœŸå®åƒç´ ï¼ˆå³`æœªæ©ç `ï¼‰ä¸º1ï¼Œ

    +   å¯¹äºå¡«å……åƒç´ ï¼ˆå³`æ©ç `ï¼‰ä¸º0ã€‚

+   `instance_id_to_semantic_id`ï¼ˆ`List[Dict[int, int]]`æˆ–`Dict[int, int]`ï¼Œ*å¯é€‰*ï¼‰â€” å¯¹è±¡å®ä¾‹IDå’Œç±»IDä¹‹é—´çš„æ˜ å°„ã€‚å¦‚æœä¼ é€’ï¼Œ`segmentation_maps`å°†è¢«è§†ä¸ºå®ä¾‹åˆ†å‰²åœ°å›¾ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸€ä¸ªå®ä¾‹IDã€‚å¯ä»¥æä¾›ä¸ºå•ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«å…¨å±€/æ•°æ®é›†çº§åˆ«çš„æ˜ å°„ï¼Œæˆ–ä½œä¸ºå­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªå›¾åƒä¸€ä¸ªï¼‰ï¼Œä»¥åˆ†åˆ«æ˜ å°„æ¯ä¸ªå›¾åƒä¸­çš„å®ä¾‹IDã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯NumPyæ•°ç»„ã€‚å¦‚æœè®¾ç½®ä¸º`'pt'`ï¼Œåˆ™è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚

+   `input_data_format`ï¼ˆ`str`æˆ–`ChannelDimension`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­ã€‚

è¿”å›

[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)

ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)ï¼š

+   `pixel_values` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ å€¼ã€‚

+   `pixel_mask` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„åƒç´ æ©æ¨¡ï¼ˆå½“ `=True` æˆ– `pixel_mask` åœ¨ `self.model_input_names` ä¸­æ—¶ï¼‰ã€‚

+   `mask_labels` â€” å½¢çŠ¶ä¸º `(labels, height, width)` çš„å¯é€‰æ©æ¨¡æ ‡ç­¾åˆ—è¡¨ï¼Œè¦é¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾› `annotations` æ—¶ï¼‰ã€‚

+   `class_labels` â€” å½¢çŠ¶ä¸º `(labels)` çš„å¯é€‰ç±»æ ‡ç­¾åˆ—è¡¨ï¼Œè¦é¦ˆé€ç»™æ¨¡å‹ï¼ˆå½“æä¾› `annotations` æ—¶ï¼‰ã€‚å®ƒä»¬æ ‡è¯† `mask_labels` çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ `class_labels[i][j]` çš„æ ‡ç­¾ä¸º `mask_labels[i][j]`ã€‚

+   `text_inputs` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„å¯é€‰æ–‡æœ¬å­—ç¬¦ä¸²æ¡ç›®åˆ—è¡¨ï¼ˆå½“æä¾› `annotations` æ—¶ï¼‰ã€‚å®ƒä»¬æ ‡è¯†å›¾åƒä¸­å­˜åœ¨çš„äºŒè¿›åˆ¶æ©æ¨¡ã€‚

å°†å›¾åƒå¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºç›¸åº”çš„ `pixel_mask`ã€‚

OneFormer ä½¿ç”¨æ©æ¨¡åˆ†ç±»èŒƒå¼æ¥å¤„ç†è¯­ä¹‰åˆ†å‰²ï¼Œå› æ­¤è¾“å…¥åˆ†å‰²åœ°å›¾å°†è¢«è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ©æ¨¡åˆ—è¡¨åŠå…¶ç›¸åº”çš„æ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå‡è®¾ `segmentation_maps = [[2,6,7,9]]`ï¼Œè¾“å‡ºå°†åŒ…å« `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`ï¼ˆå››ä¸ªäºŒè¿›åˆ¶æ©æ¨¡ï¼‰å’Œ `class_labels = [2,6,7,9]`ï¼Œæ¯ä¸ªæ©æ¨¡çš„æ ‡ç­¾ã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)

```py
( outputs target_sizes: Optional = None ) â†’ export const metadata = 'undefined';List[torch.Tensor]
```

å‚æ•°

+   `outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)) â€” æ¨¡å‹çš„åŸå§‹è¾“å‡ºã€‚

+   `target_sizes` (`List[Tuple[int, int]]`, *å¯é€‰*) â€” é•¿åº¦ä¸º (batch_size) çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ (`Tuple[int, int]]`) å¯¹åº”äºæ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚

è¿”å›

`List[torch.Tensor]`

ä¸€ä¸ªé•¿åº¦ä¸º `batch_size` çš„åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (height, width) çš„è¯­ä¹‰åˆ†å‰²åœ°å›¾ï¼Œå¯¹åº”äºç›®æ ‡å¤§å°æ¡ç›®ï¼ˆå¦‚æœæŒ‡å®šäº† `target_sizes`ï¼‰ã€‚æ¯ä¸ª `torch.Tensor` çš„æ¯ä¸ªæ¡ç›®å¯¹åº”äºä¸€ä¸ªè¯­ä¹‰ç±»åˆ« idã€‚

å°† [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) çš„è¾“å‡ºè½¬æ¢ä¸ºè¯­ä¹‰åˆ†å‰²åœ°å›¾ã€‚ä»…æ”¯æŒ PyTorchã€‚

#### `post_process_instance_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)

```py
( outputs task_type: str = 'instance' is_demo: bool = True threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs` (`OneFormerForUniversalSegmentationOutput`) â€” ä» `OneFormerForUniversalSegmentationOutput` å¾—åˆ°çš„è¾“å‡ºã€‚

+   `task_type` (`str`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸ºâ€œinstanceâ€) â€” åå¤„ç†å–å†³äºä»»åŠ¡ä»¤ç‰Œè¾“å…¥ã€‚å¦‚æœ `task_type` æ˜¯â€œpanopticâ€ï¼Œæˆ‘ä»¬éœ€è¦å¿½ç•¥æ‚é¡¹é¢„æµ‹ã€‚

+   `is_demo` (`bool`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦å¤„äºæ¼”ç¤ºæ¨¡å¼ã€‚å¦‚æœä¸ºçœŸï¼Œåˆ™ä½¿ç”¨é˜ˆå€¼é¢„æµ‹æœ€ç»ˆæ©æ¨¡ã€‚

+   `threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚

+   `mask_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.5) â€” åœ¨å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚

+   `overlap_mask_area_threshold` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.8) â€” åˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©æ¨¡åŒºåŸŸé˜ˆå€¼ã€‚

+   `target_sizes` (`List[Tuple]`, *å¯é€‰*) â€” é•¿åº¦ä¸º (batch_size) çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ (`Tuple[int, int]]`) å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸º Noneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚

+   `return_coco_annotation` (`bool`, *å¯é€‰*)ï¼Œé»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»¥ COCO æ ¼å¼è¿”å›é¢„æµ‹ã€‚

è¿”å›

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š

+   `segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª`segment_id`ï¼Œå¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©æ¨¡ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚

+   `segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„å…¶ä»–ä¿¡æ¯çš„å­—å…¸ã€‚

    +   `id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚

    +   `label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«IDçš„æ•´æ•°ã€‚

    +   `was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„`segment_id`ã€‚

    +   `score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚

å°†`OneFormerForUniversalSegmentationOutput`çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå®ä¾‹åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚

#### `post_process_panoptic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) â†’ export const metadata = 'undefined';List[Dict]
```

å‚æ•°

+   `outputs`ï¼ˆ`MaskFormerForInstanceSegmentationOutput`ï¼‰â€” æ¥è‡ª[MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)çš„è¾“å‡ºã€‚

+   `threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰â€” ä¿ç•™é¢„æµ‹å®ä¾‹æ©æ¨¡çš„æ¦‚ç‡åˆ†æ•°é˜ˆå€¼ã€‚

+   `mask_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.5ï¼‰â€” åœ¨å°†é¢„æµ‹çš„æ©æ¨¡è½¬æ¢ä¸ºäºŒè¿›åˆ¶å€¼æ—¶ä½¿ç”¨çš„é˜ˆå€¼ã€‚

+   `overlap_mask_area_threshold`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8ï¼‰â€” ç”¨äºåˆå¹¶æˆ–ä¸¢å¼ƒæ¯ä¸ªäºŒè¿›åˆ¶å®ä¾‹æ©æ¨¡ä¸­çš„å°æ–­å¼€éƒ¨åˆ†çš„é‡å æ©æ¨¡åŒºåŸŸé˜ˆå€¼ã€‚

+   `label_ids_to_fuse`ï¼ˆ`Set[int]`ï¼Œ*å¯é€‰*ï¼‰â€” æ­¤çŠ¶æ€ä¸­çš„æ ‡ç­¾å°†ä½¿å…¶æ‰€æœ‰å®ä¾‹è¢«èåˆåœ¨ä¸€èµ·ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å›¾åƒä¸­åªèƒ½æœ‰ä¸€ä¸ªå¤©ç©ºï¼Œä½†å¯ä»¥æœ‰å‡ ä¸ªäººï¼Œå› æ­¤å¤©ç©ºçš„æ ‡ç­¾IDå°†åœ¨è¯¥é›†åˆä¸­ï¼Œä½†äººçš„æ ‡ç­¾IDä¸åœ¨å…¶ä¸­ã€‚

+   `target_sizes`ï¼ˆ`List[Tuple]`ï¼Œ*å¯é€‰*ï¼‰â€” é•¿åº¦ä¸ºï¼ˆbatch_sizeï¼‰çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªåˆ—è¡¨é¡¹ï¼ˆ`Tuple[int, int]`ï¼‰å¯¹åº”äºæ‰¹å¤„ç†ä¸­æ¯ä¸ªé¢„æµ‹çš„è¯·æ±‚çš„æœ€ç»ˆå¤§å°ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœä¿æŒä¸ºNoneï¼Œåˆ™ä¸ä¼šè°ƒæ•´é¢„æµ‹å¤§å°ã€‚

è¿”å›å€¼

`List[Dict]`

ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾åƒä¸€ä¸ªï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä¸¤ä¸ªé”®ï¼š

+   `segmentation` â€” å½¢çŠ¶ä¸º`(height, width)`çš„å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ª`segment_id`ï¼Œå¦‚æœæœªæ‰¾åˆ°é«˜äº`threshold`çš„æ©æ¨¡ï¼Œåˆ™è®¾ç½®ä¸º`None`ã€‚å¦‚æœæŒ‡å®šäº†`target_sizes`ï¼Œåˆ™å°†åˆ†å‰²è°ƒæ•´ä¸ºç›¸åº”çš„`target_sizes`æ¡ç›®ã€‚

+   `segments_info` â€” åŒ…å«æ¯ä¸ªæ®µçš„å…¶ä»–ä¿¡æ¯çš„å­—å…¸ã€‚

    +   `id` â€” ä»£è¡¨`segment_id`çš„æ•´æ•°ã€‚

    +   `label_id` â€” ä»£è¡¨ä¸`segment_id`å¯¹åº”çš„æ ‡ç­¾/è¯­ä¹‰ç±»åˆ«IDçš„æ•´æ•°ã€‚

    +   `was_fused` â€” ä¸€ä¸ªå¸ƒå°”å€¼ï¼Œå¦‚æœ`label_id`åœ¨`label_ids_to_fuse`ä¸­ï¼Œåˆ™ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚ç›¸åŒç±»åˆ«/æ ‡ç­¾çš„å¤šä¸ªå®ä¾‹è¢«èåˆå¹¶åˆ†é…ä¸€ä¸ªå•ç‹¬çš„`segment_id`ã€‚

    +   `score` â€” å…·æœ‰`segment_id`çš„æ®µçš„é¢„æµ‹åˆ†æ•°ã€‚

å°†`MaskFormerForInstanceSegmentationOutput`çš„è¾“å‡ºè½¬æ¢ä¸ºå›¾åƒå…¨æ™¯åˆ†å‰²é¢„æµ‹ã€‚ä»…æ”¯æŒPyTorchã€‚

## OneFormerProcessor

### `class transformers.OneFormerProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)

```py
( image_processor = None tokenizer = None max_seq_length: int = 77 task_seq_length: int = 77 **kwargs )
```

å‚æ•°

+   `image_processor`ï¼ˆ[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)ï¼‰â€” å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ[`CLIPTokenizer`ï¼Œ`CLIPTokenizerFast`]ï¼‰â€” åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `max_seq_len`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º77ï¼‰â€” è¾“å…¥æ–‡æœ¬åˆ—è¡¨çš„åºåˆ—é•¿åº¦ã€‚

+   `task_seq_len`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º77ï¼‰â€” è¾“å…¥ä»»åŠ¡ä»¤ç‰Œçš„åºåˆ—é•¿åº¦ã€‚

æ„å»ºä¸€ä¸ªOneFormerå¤„ç†å™¨ï¼Œå°†[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)å’Œ[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ï¼Œç»§æ‰¿äº†å›¾åƒå¤„ç†å™¨å’Œæ ‡è®°åŒ–å™¨çš„åŠŸèƒ½ã€‚

#### `encode_inputs`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)

```py
( images = None task_inputs = None segmentation_maps = None **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)ï¼Œç„¶åå¯¹ä»»åŠ¡è¾“å…¥è¿›è¡Œæ ‡è®°åŒ–ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `post_process_instance_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `post_process_panoptic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `post_process_semantic_segmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°[OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

## OneFormerModel

### `class transformers.OneFormerModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)

```py
( config: OneFormerConfig )
```

å‚æ•°

+   `config`ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„OneFormeræ¨¡å‹ï¼Œåœ¨é¡¶éƒ¨æ²¡æœ‰ä»»ä½•ç‰¹å®šçš„å¤´éƒ¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)

```py
( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚

+   `task_inputs`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- ä»»åŠ¡è¾“å…¥ã€‚ä»»åŠ¡è¾“å…¥å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚

+   `pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚åœ¨`[0, 1]`ä¸­é€‰æ‹©çš„æ©ç å€¼ï¼š

    +   å¯¹äºçœŸå®åƒç´ ä¸º1ï¼ˆå³`not masked`ï¼‰ï¼Œ

    +   å¯¹äºå¡«å……çš„åƒç´ ä¸º0ï¼ˆå³`masked`ï¼‰ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›Detrè§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›`~OneFormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)æˆ–`tuple(torch.FloatTensor)`

[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)æˆ–`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º+ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_object_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰- transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚

+   `transformer_decoder_contrastive_queries`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`çš„`torch.FloatTensor`ï¼‰- æ¥è‡ªtransformerè§£ç å™¨çš„å¯¹æ¯”æŸ¥è¯¢ã€‚

+   `transformer_decoder_mask_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„`torch.FloatTensor`ï¼‰- transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚

+   `transformer_decoder_class_predictions`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`çš„`torch.FloatTensor`ï¼‰- transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚

+   `transformer_decoder_auxiliary_predictions`ï¼ˆ`strï¼Œtorch.FloatTensor`å­—å…¸çš„å…ƒç»„ï¼Œ*å¯é€‰*ï¼‰- æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„å…ƒç»„ã€‚

+   `text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰- ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚

+   `task_token`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`çš„`torch.FloatTensor`ï¼‰- ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚

+   `attentions`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ã€‚æ¥è‡ªå˜å‹å™¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

`OneFormerModelOutput`

[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import OneFormerProcessor, OneFormerModel

>>> # download texting image
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # load processor for preprocessing the inputs
>>> processor = OneFormerProcessor.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> model = OneFormerModel.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> inputs = processor(image, ["semantic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> mask_predictions = outputs.transformer_decoder_mask_predictions
>>> class_predictions = outputs.transformer_decoder_class_predictions

>>> f"ğŸ‘‰ Mask Predictions Shape: {list(mask_predictions.shape)}, Class Predictions Shape: {list(class_predictions.shape)}"
'ğŸ‘‰ Mask Predictions Shape: [1, 150, 128, 171], Class Predictions Shape: [1, 150, 151]'
```

## OneFormerForUniversalSegmentation

### `class transformers.OneFormerForUniversalSegmentation`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)

```py
( config: OneFormerConfig )
```

å‚æ•°

+   `config`ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

OneFormeræ¨¡å‹ä¾‹å¦‚ï¼Œè¯­ä¹‰å’Œå…¨æ™¯å›¾åƒåˆ†å‰²ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)

```py
( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_auxiliary_logits: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚

+   `task_inputs`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” ä»»åŠ¡è¾“å…¥ã€‚ä»»åŠ¡è¾“å…¥å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`OneFormerProcessor.__call__()`ã€‚

+   `pixel_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……åƒç´ å€¼ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºçœŸå®åƒç´ ï¼ˆå³`not masked`ï¼‰çš„åƒç´ ä¸º1ï¼Œ

    +   å¯¹äºå¡«å……åƒç´ ï¼ˆå³`masked`ï¼‰çš„åƒç´ ä¸º0ã€‚

    æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›Detrè§£ç å™¨æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›`~OneFormerModelOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `text_inputs`ï¼ˆ`List[torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_queries, sequence_length)`çš„å¼ é‡ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹

+   `mask_labels`ï¼ˆ`List[torch.Tensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„æ©ç æ ‡ç­¾åˆ—è¡¨ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹

+   `class_labels`ï¼ˆ`List[torch.LongTensor]`ï¼Œ*å¯é€‰*ï¼‰â€” å½¢çŠ¶ä¸º`(num_labels, height, width)`çš„ç›®æ ‡ç±»æ ‡ç­¾åˆ—è¡¨ï¼Œå°†è¢«é¦ˆé€åˆ°æ¨¡å‹ã€‚å®ƒä»¬æ ‡è¯†`mask_labels`çš„æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ`class_labels[i][j]`çš„æ ‡ç­¾æ˜¯`mask_labels[i][j]`ã€‚

è¿”å›

[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)æˆ–`tuple(torch.FloatTensor)`

[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”è®¡ç®—å¾—åˆ°çš„æŸå¤±ï¼Œåœ¨å­˜åœ¨æ ‡ç­¾æ—¶è¿”å›ã€‚

+   `class_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, num_labels + 1)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®ç±»åˆ«ã€‚è¯·æ³¨æ„ï¼Œéœ€è¦`+ 1`ï¼Œå› ä¸ºæˆ‘ä»¬åŒ…å«äº†ç©ºç±»ã€‚

+   `masks_queries_logits`ï¼ˆ`torch.FloatTensor`ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„æè®®æ©ç ã€‚

+   `auxiliary_predictions`ï¼ˆ`str`ï¼Œ`torch.FloatTensor`å­—å…¸çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `encoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚ç¼–ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `pixel_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚åƒç´ è§£ç å™¨æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºï¼‰ã€‚transformerè§£ç å™¨åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `transformer_decoder_object_queries`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è±¡æŸ¥è¯¢ã€‚

+   `transformer_decoder_contrastive_queries`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”transformerè§£ç å™¨ä¸­çš„å¯¹æ¯”æŸ¥è¯¢ã€‚

+   `transformer_decoder_mask_predictions`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, height, width)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„æ©ç é¢„æµ‹ã€‚

+   `transformer_decoder_class_predictions`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, num_classes+1)`ï¼‰â€”transformerè§£ç å™¨ä¸­æœ€åä¸€å±‚çš„ç±»åˆ«é¢„æµ‹ã€‚

+   `transformer_decoder_auxiliary_predictions`ï¼ˆ`str`ï¼Œ`torch.FloatTensor`å­—å…¸çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€”æ¥è‡ªtransformerè§£ç å™¨æ¯ä¸€å±‚çš„ç±»åˆ«å’Œæ©ç é¢„æµ‹çš„åˆ—è¡¨ã€‚

+   `text_queries`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_queries, hidden_dim)`ï¼‰â€”ä»ç”¨äºè®­ç»ƒæœŸé—´è®¡ç®—å¯¹æ¯”æŸå¤±çš„è¾“å…¥æ–‡æœ¬åˆ—è¡¨æ´¾ç”Ÿçš„æ–‡æœ¬æŸ¥è¯¢ã€‚

+   `task_token`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_dim)`ï¼‰â€”ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸€ç»´ä»»åŠ¡ä»¤ç‰Œã€‚

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚æ¥è‡ªå˜å‹å™¨è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æƒé‡ã€‚

`OneFormerUniversalSegmentationOutput`

[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

é€šç”¨åˆ†å‰²ç¤ºä¾‹ï¼š

```py
>>> from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation
>>> from PIL import Image
>>> import requests
>>> import torch

>>> # load OneFormer fine-tuned on ADE20k for universal segmentation
>>> processor = OneFormerProcessor.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> model = OneFormerForUniversalSegmentation.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")

>>> url = (
...     "https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg"
... )
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # Semantic Segmentation
>>> inputs = processor(image, ["semantic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for semantic postprocessing
>>> predicted_semantic_map = processor.post_process_semantic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]
>>> f"ğŸ‘‰ Semantic Predictions Shape: {list(predicted_semantic_map.shape)}"
'ğŸ‘‰ Semantic Predictions Shape: [512, 683]'

>>> # Instance Segmentation
>>> inputs = processor(image, ["instance"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for instance postprocessing
>>> predicted_instance_map = processor.post_process_instance_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]["segmentation"]
>>> f"ğŸ‘‰ Instance Predictions Shape: {list(predicted_instance_map.shape)}"
'ğŸ‘‰ Instance Predictions Shape: [512, 683]'

>>> # Panoptic Segmentation
>>> inputs = processor(image, ["panoptic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for panoptic postprocessing
>>> predicted_panoptic_map = processor.post_process_panoptic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]["segmentation"]
>>> f"ğŸ‘‰ Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}"
'ğŸ‘‰ Panoptic Predictions Shape: [512, 683]'
```
