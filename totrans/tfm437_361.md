# OneFormer

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/oneformer)

## 概述

OneFormer模型是由Jitesh Jain、Jiachen Li、MangTik Chiu、Ali Hassani、Nikita Orlov、Humphrey Shi在[OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220)中提出的。OneFormer是一个通用的图像分割框架，可以在单个全景数据集上进行训练，执行语义、实例和全景分割任务。OneFormer使用任务标记来使模型在关注的任务上进行条件化，使架构在训练时受任务引导，在推断时动态适应任务。

![](../Images/8cd6226185941da2d5d5f4ca249f7bd2.png)

该论文的摘要如下：

*通用图像分割并不是一个新概念。过去几十年来统一图像分割的尝试包括场景解析、全景分割，以及最近的新全景架构。然而，这些全景架构并不能真正统一图像分割，因为它们需要分别在语义、实例或全景分割上进行训练，才能达到最佳性能。理想情况下，一个真正通用的框架应该只需要训练一次，并在所有三个图像分割任务上实现SOTA性能。为此，我们提出了OneFormer，一个通用图像分割框架，通过多任务一次训练的设计统一了分割。我们首先提出了一个任务条件联合训练策略，使得可以在单个多任务训练过程中训练每个领域（语义、实例和全景分割）的地面真相。其次，我们引入了一个任务标记，使我们的模型在手头的任务上进行条件化，使我们的模型支持多任务训练和推断。第三，我们提出在训练期间使用查询文本对比损失，以建立更好的任务间和类间区别。值得注意的是，我们的单个OneFormer模型在ADE20k、CityScapes和COCO上的所有三个分割任务中均优于专门的Mask2Former模型，尽管后者在每个任务上分别使用了三倍的资源进行训练。通过新的ConvNeXt和DiNAT骨干，我们观察到更多的性能改进。我们相信OneFormer是使图像分割更加通用和可访问的重要一步。*

下图展示了OneFormer的架构。摘自[原始论文](https://arxiv.org/abs/2211.06220)。

![](../Images/8617928e627dd67ce070d58bb21d63e1.png)

这个模型是由[Jitesh Jain](https://huggingface.co/praeclarumjj3)贡献的。原始代码可以在[这里](https://github.com/SHI-Labs/OneFormer)找到。

## 使用提示

+   在推断期间，OneFormer需要两个输入：*图像*和*任务标记*。

+   在训练期间，OneFormer只使用全景注释。

+   如果要在多个节点上的分布式环境中训练模型，则应该在`modeling_oneformer.py`的`OneFormerLoss`类中更新`get_num_masks`函数。在多节点训练时，这应该设置为所有节点上目标掩码的平均数量，可以在原始实现中看到[这里](https://github.com/SHI-Labs/OneFormer/blob/33ebb56ed34f970a30ae103e786c0cb64c653d9a/oneformer/modeling/criterion.py#L287)。

+   可以使用[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)来准备输入图像和模型的任务输入，以及可选的模型目标。 `OneformerProcessor`将[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)和[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)封装成一个单一实例，既可以准备图像又可以编码任务输入。

+   要获得最终的分割结果，可以调用[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)或[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)。这三个任务都可以使用[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)的输出来解决，全景分割接受一个可选的`label_ids_to_fuse`参数，用于将目标对象（例如天空）的实例融合在一起。

## 资源

一份官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用OneFormer。

+   关于推断+在自定义数据上进行微调的演示笔记本可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/OneFormer)找到。

如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将对其进行审查。资源应该展示一些新的东西，而不是重复现有资源。

## OneFormer特定输出

### `class transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L803)

```py
( encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )
```

参数

+   `encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出），形状为`(batch_size, num_channels, height, width)`。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `pixel_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出），形状为`(batch_size, num_channels, height, width)`。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — `torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出），形状为`(batch_size, sequence_length, hidden_size)`。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_object_queries` (`torch.FloatTensor`，形状为`(batch_size, num_queries, hidden_dim)`) — 来自transformer解码器最后一层的输出对象查询。

+   `transformer_decoder_contrastive_queries` (`torch.FloatTensor`，形状为`(batch_size, num_queries, hidden_dim)`) — 来自transformer解码器的对比查询。

+   `transformer_decoder_mask_predictions` (`torch.FloatTensor`，形状为`(batch_size, num_queries, height, width)`) — 来自transformer解码器最后一层的掩码预测。

+   `transformer_decoder_class_predictions` (`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes+1)`) — 来自transformer解码器最后一层的类别预测。

+   `transformer_decoder_auxiliary_predictions`（`str, torch.FloatTensor`字典的元组，*可选*） — 来自transformer解码器每一层的类别和掩码预测的元组。

+   `text_queries` (`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`) — 从用于计算对比损失的输入文本列表派生的文本查询。

+   `task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）—用于条件查询的一维任务令牌。

+   `attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组（每层一个）。transformer解码器的自注意力和交叉注意力权重。

用于[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)输出的类。此类返回计算logits所需的所有隐藏状态。

### `class transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L853)

```py
( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_predictions: List = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )
```

参数

+   `loss`（`torch.Tensor`，*可选*）—当存在标签时返回计算的损失。

+   `class_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, num_labels + 1)`的张量，表示每个查询的建议类别。请注意，需要`+ 1`，因为我们包含了空类。

+   `masks_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, height, width)`的张量，表示每个查询的建议掩码。

+   `auxiliary_predictions`（`str，torch.FloatTensor`字典列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_object_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的输出对象查询。

+   `transformer_decoder_contrastive_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）—来自transformer解码器的对比查询。

+   `transformer_decoder_mask_predictions`（形状为`(batch_size, num_queries, height, width)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的掩码预测。

+   `transformer_decoder_class_predictions`（形状为`(batch_size, num_queries, num_classes+1)`的`torch.FloatTensor`）—来自transformer解码器中最后一层的类别预测。

+   `transformer_decoder_auxiliary_predictions`（`str，torch.FloatTensor`字典列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。

+   `text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）—从用于训练期间计算对比损失的输入文本列表派生的文本查询。

+   `task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）—用于条件查询的一维任务令牌。

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组。来自transformer解码器的自注意力和交叉注意力权重。

用于`OneFormerForUniversalSegmentationOutput`的输出类。

此输出可以直接传递给[post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)或[post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)或[post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)，具体取决于任务。请参阅[`~OneFormerImageProcessor]以获取有关用法的详细信息。

## OneFormerConfig

### `class transformers.OneFormerConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/configuration_oneformer.py#L33)

```py
( backbone_config: Optional = None ignore_value: int = 255 num_queries: int = 150 no_object_weight: int = 0.1 class_weight: float = 2.0 mask_weight: float = 5.0 dice_weight: float = 5.0 contrastive_weight: float = 0.5 contrastive_temperature: float = 0.07 train_num_points: int = 12544 oversample_ratio: float = 3.0 importance_sample_ratio: float = 0.75 init_std: float = 0.02 init_xavier_std: float = 1.0 layer_norm_eps: float = 1e-05 is_training: bool = False use_auxiliary_loss: bool = True output_auxiliary_logits: bool = True strides: Optional = [4, 8, 16, 32] task_seq_len: int = 77 text_encoder_width: int = 256 text_encoder_context_length: int = 77 text_encoder_num_layers: int = 6 text_encoder_vocab_size: int = 49408 text_encoder_proj_layers: int = 2 text_encoder_n_ctx: int = 16 conv_dim: int = 256 mask_dim: int = 256 hidden_dim: int = 256 encoder_feedforward_dim: int = 1024 norm: str = 'GN' encoder_layers: int = 6 decoder_layers: int = 10 use_task_norm: bool = True num_attention_heads: int = 8 dropout: float = 0.1 dim_feedforward: int = 2048 pre_norm: bool = False enforce_input_proj: bool = False query_dec_layers: int = 2 common_stride: int = 4 **kwargs )
```

参数

+   `backbone_config` (`PretrainedConfig`, *optional*, defaults to `SwinConfig`) — 主干模型的配置。

+   `ignore_value` (`int`, *optional*, defaults to 255) — 在计算损失时要忽略的GT标签中的值。

+   `num_queries` (`int`, *optional*, defaults to 150) — 对象查询的数量。

+   `no_object_weight` (`float`, *optional*, defaults to 0.1) — 无对象类预测的权重。

+   `class_weight` (`float`, *optional*, defaults to 2.0) — 分类CE损失的权重。

+   `mask_weight` (`float`, *optional*, defaults to 5.0) — 二元CE损失的权重。

+   `dice_weight` (`float`, *optional*, defaults to 5.0) — Dice损失的权重。

+   `contrastive_weight` (`float`, *optional*, defaults to 0.5) — 对比损失的权重。

+   `contrastive_temperature` (`float`, *optional*, defaults to 0.07) — 用于缩放对比对数的初始值。

+   `train_num_points` (`int`, *optional*, defaults to 12544) — 在计算掩码预测损失时要采样的点数。

+   `oversample_ratio` (`float`, *optional*, defaults to 3.0) — 决定过采样多少点的比率。

+   `importance_sample_ratio` (`float`, *optional*, defaults to 0.75) — 通过重要性采样抽样的点的比率。

+   `init_std` (`float`, *optional*, defaults to 0.02) — 正态初始化的标准差。

+   `init_xavier_std` (`float`, *optional*, defaults to 1.0) — 用于xavier均匀初始化的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化的epsilon。

+   `is_training` (`bool`, *optional*, defaults to `False`) — 是否在训练或推理模式下运行。

+   `use_auxiliary_loss` (`bool`, *optional*, defaults to `True`) — 是否使用transformer解码器的中间预测计算损失。

+   `output_auxiliary_logits` (`bool`, *optional*, defaults to `True`) — 是否从transformer解码器返回中间预测。

+   `strides` (`list`, *optional*, defaults to `[4, 8, 16, 32]`) — 包含编码器中特征图的步幅的列表。

+   `task_seq_len` (`int`, *optional*, defaults to 77) — 用于对文本列表输入进行分词的序列长度。

+   `text_encoder_width` (`int`, *optional*, defaults to 256) — 文本编码器的隐藏大小。

+   `text_encoder_context_length` (`int`, *optional*, defaults to 77) — 文本编码器的输入序列长度。

+   `text_encoder_num_layers` (`int`, *optional*, defaults to 6) — 文本编码器中transformer的层数。

+   `text_encoder_vocab_size` (`int`, *optional*, defaults to 49408) — 分词器的词汇量。

+   `text_encoder_proj_layers` (`int`, *optional*, defaults to 2) — 用于项目文本查询的MLP中的层数。

+   `text_encoder_n_ctx` (`int`, *optional*, 默认为 16) — 可学习文本上下文查询的数量。

+   `conv_dim` (`int`, *optional*, 默认为 256) — 从骨干网络映射输出的特征图维度。

+   `mask_dim` (`int`, *optional*, 默认为 256) — 像素解码器中特征图的维度。

+   `hidden_dim` (`int`, *optional*, 默认为 256) — 变压器解码器中隐藏状态的维度。

+   `encoder_feedforward_dim` (`int`, *optional*, 默认为 1024) — 像素解码器中FFN层的维度。

+   `norm` (`str`, *optional*, 默认为 `"GN"`) — 归一化类型。

+   `encoder_layers` (`int`, *optional*, 默认为 6) — 像素解码器中的层数。

+   `decoder_layers` (`int`, *optional*, 默认为 10) — 变压器解码器中的层数。

+   `use_task_norm` (`bool`, *optional*, 默认为 `True`) — 是否对任务令牌进行归一化。

+   `num_attention_heads` (`int`, *optional*, 默认为 8) — 像素和变压器解码器中的注意力头数。

+   `dropout` (`float`, *optional*, 默认为 0.1) — 像素和变压器解码器的丢失概率。

+   `dim_feedforward` (`int`, *optional*, 默认为 2048) — 变压器解码器中FFN层的维度。

+   `pre_norm` (`bool`, *optional*, 默认为 `False`) — 是否在变压器解码器中的注意力层之前对隐藏状态进行归一化。

+   `enforce_input_proj` (`bool`, *optional*, 默认为 `False`) — 是否在变压器解码器中投影隐藏状态。

+   `query_dec_layers` (`int`, *optional*, 默认为 2) — 查询变压器中的层数。

+   `common_stride` (`int`, *optional*, 默认为 4) — 用于像素解码器中特征的常用步幅。

这是用于存储 [OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel) 配置的配置类。它用于根据指定的参数实例化一个 OneFormer 模型，定义模型架构。使用默认值实例化配置将产生类似于在 [ADE20k-150](https://huggingface.co/datasets/scene_parse_150) 上训练的 OneFormer [shi-labs/oneformer_ade20k_swin_tiny](https://huggingface.co/shi-labs/oneformer_ade20k_swin_tiny) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 并可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例:

```py
>>> from transformers import OneFormerConfig, OneFormerModel

>>> # Initializing a OneFormer shi-labs/oneformer_ade20k_swin_tiny configuration
>>> configuration = OneFormerConfig()
>>> # Initializing a model (with random weights) from the shi-labs/oneformer_ade20k_swin_tiny style configuration
>>> model = OneFormerModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## OneFormerImageProcessor

### `class transformers.OneFormerImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L368)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: bool = False repo_path: Optional = 'shi-labs/oneformer_demo' class_info_file: str = None num_text: Optional = None **kwargs )
```

参数

+   `do_resize` (`bool`, *optional*, 默认为 `True`) — 是否将输入调整大小到特定的 `size`。

+   `size` (`int`, *optional*, 默认为 800) — 将输入调整大小到给定大小。仅在 `do_resize` 设置为 `True` 时有效。如果 size 是一个类似 `(width, height)` 的序列，则输出大小将匹配到这个。如果 size 是一个整数，图像的较小边将匹配到这个数字。即，如果 `height > width`，则图像将重新缩放为 `(size * height / width, size)`。

+   `resample` (`int`, *optional*, 默认为 `Resampling.BILINEAR`) — 可选的重采样滤波器。可以是 `PIL.Image.Resampling.NEAREST`, `PIL.Image.Resampling.BOX`, `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`, `PIL.Image.Resampling.BICUBIC` 或 `PIL.Image.Resampling.LANCZOS` 中的一个。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否将输入重新缩放到特定的 `scale`。

+   `rescale_factor` (`float`, *optional*, 默认为 `1/ 255`) — 通过给定因子重新缩放输入。仅在 `do_rescale` 设置为 `True` 时有效。

+   `do_normalize`（`bool`，*可选*，默认为`True`）— 是否对输入进行均值和标准差归一化。

+   `image_mean`（`int`，*可选*，默认为`[0.485, 0.456, 0.406]`）— 每个通道的均值序列，在规范化图像时使用。默认为ImageNet均值。

+   `image_std`（`int`，*可选*，默认为`[0.229, 0.224, 0.225]`）— 每个通道的标准差序列，在规范化图像时使用。默认为ImageNet标准差。

+   `ignore_index`（`int`，*可选*）— 分割地图中要分配给背景像素的标签。如果提供，用0（背景）表示的分割地图像素将被替换为`ignore_index`。

+   `do_reduce_labels`（`bool`，*可选*，默认为`False`）— 是否将所有分割地图的标签值减1。通常用于数据集中使用0表示背景，并且背景本身不包含在数据集的所有类中的情况（例如ADE20k）。背景标签将被替换为`ignore_index`。

+   `repo_path`（`str`，*可选*，默认为`"shi-labs/oneformer_demo"`）— 包含数据集类信息的JSON文件的hub存储库或本地目录的路径。如果未设置，将在当前工作目录中查找`class_info_file`。

+   `class_info_file`（`str`，*可选*）— 包含数据集类信息的JSON文件。查看`shi-labs/oneformer_demo/cityscapes_panoptic.json`以获取示例。

+   `num_text`（`int`，*可选*）— 文本输入列表中的文本条目数。

构建一个OneFormer图像处理器。该图像处理器可用于为模型准备图像、任务输入以及可选的文本输入和目标。

这个图像处理器继承自`BaseImageProcessor`，其中包含大部分主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L656)

```py
( images: Union task_inputs: Optional = None segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

#### `encode_inputs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L954)

```py
( pixel_values_list: List task_inputs: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) → export const metadata = 'undefined';BatchFeature
```

参数

+   `pixel_values_list`（`List[ImageInput]`）— 要填充的图像（像素值）列表。每个图像应该是形状为`(channels, height, width)`的张量。

+   `task_inputs`（`List[str]`）— 任务值列表。

+   `segmentation_maps`（`ImageInput`，*可选*）— 具有像素级注释的相应语义分割地图。

    （`bool`，*可选*，默认为`True`）：是否将图像填充到批次中最大的图像并创建像素掩码。

    如果保持默认设置，将返回一个像素掩码，即：

    +   对于真实像素（即`未掩码`）为1，

    +   对于填充像素（即`掩码`）为0。

+   `instance_id_to_semantic_id`（`List[Dict[int, int]]`或`Dict[int, int]`，*可选*）— 对象实例ID和类ID之间的映射。如果传递，`segmentation_maps`将被视为实例分割地图，其中每个像素表示一个实例ID。可以提供为单个字典，其中包含全局/数据集级别的映射，或作为字典列表（每个图像一个），以分别映射每个图像中的实例ID。

+   `return_tensors`（`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)，*可选*）— 如果设置，将返回张量而不是NumPy数组。如果设置为`'pt'`，则返回PyTorch `torch.Tensor`对象。

+   `input_data_format`（`str`或`ChannelDimension`，*可选*）— 输入图像的通道维度格式。如果未提供，将从输入图像中推断。

返回

[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)

一个具有以下字段的[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)：

+   `pixel_values` — 要馈送给模型的像素值。

+   `pixel_mask` — 要馈送给模型的像素掩模（当 `=True` 或 `pixel_mask` 在 `self.model_input_names` 中时）。

+   `mask_labels` — 形状为 `(labels, height, width)` 的可选掩模标签列表，要馈送给模型（当提供 `annotations` 时）。

+   `class_labels` — 形状为 `(labels)` 的可选类标签列表，要馈送给模型（当提供 `annotations` 时）。它们标识 `mask_labels` 的标签，例如，如果 `class_labels[i][j]` 的标签为 `mask_labels[i][j]`。

+   `text_inputs` — 要馈送给模型的可选文本字符串条目列表（当提供 `annotations` 时）。它们标识图像中存在的二进制掩模。

将图像填充到批处理中最大的图像，并创建相应的 `pixel_mask`。

OneFormer 使用掩模分类范式来处理语义分割，因此输入分割地图将被转换为二进制掩模列表及其相应的标签。让我们看一个例子，假设 `segmentation_maps = [[2,6,7,9]]`，输出将包含 `mask_labels = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]`（四个二进制掩模）和 `class_labels = [2,6,7,9]`，每个掩模的标签。

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1089)

```py
( outputs target_sizes: Optional = None ) → export const metadata = 'undefined';List[torch.Tensor]
```

参数

+   `outputs` ([MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)) — 模型的原始输出。

+   `target_sizes` (`List[Tuple[int, int]]`, *可选*) — 长度为 (batch_size) 的列表，其中每个列表项 (`Tuple[int, int]]`) 对应于每个预测的请求最终大小（高度、宽度）。如果保持为 None，则不会调整预测大小。

返回

`List[torch.Tensor]`

一个长度为 `batch_size` 的列表，每个项是一个形状为 (height, width) 的语义分割地图，对应于目标大小条目（如果指定了 `target_sizes`）。每个 `torch.Tensor` 的每个条目对应于一个语义类别 id。

将 [MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation) 的输出转换为语义分割地图。仅支持 PyTorch。

#### `post_process_instance_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1139)

```py
( outputs task_type: str = 'instance' is_demo: bool = True threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs` (`OneFormerForUniversalSegmentationOutput`) — 从 `OneFormerForUniversalSegmentationOutput` 得到的输出。

+   `task_type` (`str`, *可选*)，默认为“instance”) — 后处理取决于任务令牌输入。如果 `task_type` 是“panoptic”，我们需要忽略杂项预测。

+   `is_demo` (`bool`, *可选*)，默认为 `True`) — 模型是否处于演示模式。如果为真，则使用阈值预测最终掩模。

+   `threshold` (`float`, *可选*, 默认为 0.5) — 保留预测实例掩模的概率分数阈值。

+   `mask_threshold` (`float`, *可选*, 默认为 0.5) — 在将预测的掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold` (`float`, *可选*, 默认为 0.8) — 合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模区域阈值。

+   `target_sizes` (`List[Tuple]`, *可选*) — 长度为 (batch_size) 的列表，其中每个列表项 (`Tuple[int, int]]`) 对应于批处理中每个预测的请求最终大小（高度、宽度）。如果保持为 None，则不会调整预测大小。

+   `return_coco_annotation` (`bool`, *可选*)，默认为 `False`) — 是否以 COCO 格式返回预测。

返回

`List[Dict]`

一个字典列表，每个图像一个字典，每个字典包含两个键：

+   `segmentation` — 形状为`(height, width)`的张量，其中每个像素代表一个`segment_id`，如果未找到高于`threshold`的掩模，则设置为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 包含每个段的其他信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别ID的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单独的`segment_id`。

    +   `score` — 具有`segment_id`的段的预测分数。

将`OneFormerForUniversalSegmentationOutput`的输出转换为图像实例分割预测。仅支持PyTorch。

#### `post_process_panoptic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/image_processing_oneformer.py#L1259)

```py
( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) → export const metadata = 'undefined';List[Dict]
```

参数

+   `outputs`（`MaskFormerForInstanceSegmentationOutput`）— 来自[MaskFormerForInstanceSegmentation](/docs/transformers/v4.37.2/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation)的输出。

+   `threshold`（`float`，*可选*，默认为0.5）— 保留预测实例掩模的概率分数阈值。

+   `mask_threshold`（`float`，*可选*，默认为0.5）— 在将预测的掩模转换为二进制值时使用的阈值。

+   `overlap_mask_area_threshold`（`float`，*可选*，默认为0.8）— 用于合并或丢弃每个二进制实例掩模中的小断开部分的重叠掩模区域阈值。

+   `label_ids_to_fuse`（`Set[int]`，*可选*）— 此状态中的标签将使其所有实例被融合在一起。例如，我们可以说图像中只能有一个天空，但可以有几个人，因此天空的标签ID将在该集合中，但人的标签ID不在其中。

+   `target_sizes`（`List[Tuple]`，*可选*）— 长度为（batch_size）的列表，其中每个列表项（`Tuple[int, int]`）对应于批处理中每个预测的请求的最终大小（高度，宽度）。如果保持为None，则不会调整预测大小。

返回值

`List[Dict]`

一个字典列表，每个图像一个，每个字典包含两个键：

+   `segmentation` — 形状为`(height, width)`的张量，其中每个像素代表一个`segment_id`，如果未找到高于`threshold`的掩模，则设置为`None`。如果指定了`target_sizes`，则将分割调整为相应的`target_sizes`条目。

+   `segments_info` — 包含每个段的其他信息的字典。

    +   `id` — 代表`segment_id`的整数。

    +   `label_id` — 代表与`segment_id`对应的标签/语义类别ID的整数。

    +   `was_fused` — 一个布尔值，如果`label_id`在`label_ids_to_fuse`中，则为`True`，否则为`False`。相同类别/标签的多个实例被融合并分配一个单独的`segment_id`。

    +   `score` — 具有`segment_id`的段的预测分数。

将`MaskFormerForInstanceSegmentationOutput`的输出转换为图像全景分割预测。仅支持PyTorch。

## OneFormerProcessor

### `class transformers.OneFormerProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L29)

```py
( image_processor = None tokenizer = None max_seq_length: int = 77 task_seq_length: int = 77 **kwargs )
```

参数

+   `image_processor`（[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)）— 图像处理器是必需的输入。

+   `tokenizer`（[`CLIPTokenizer`，`CLIPTokenizerFast`]）— 分词器是必需的输入。

+   `max_seq_len`（`int`，*可选*，默认为77）— 输入文本列表的序列长度。

+   `task_seq_len`（`int`，*可选*，默认为77）— 输入任务令牌的序列长度。

构建一个OneFormer处理器，将[OneFormerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor)和[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)/[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)包装成一个单一处理器，继承了图像处理器和标记化器的功能。

#### `encode_inputs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L146)

```py
( images = None task_inputs = None segmentation_maps = None **kwargs )
```

此方法将其所有参数转发到[OneFormerImageProcessor.encode_inputs()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.encode_inputs)，然后对任务输入进行标记化。有关更多信息，请参阅此方法的文档字符串。

#### `post_process_instance_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L193)

```py
( *args **kwargs )
```

此方法将其所有参数转发到[OneFormerImageProcessor.post_process_instance_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_instance_segmentation)。有关更多信息，请参阅此方法的文档字符串。

#### `post_process_panoptic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L200)

```py
( *args **kwargs )
```

此方法将其所有参数转发到[OneFormerImageProcessor.post_process_panoptic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_panoptic_segmentation)。有关更多信息，请参阅此方法的文档字符串。

#### `post_process_semantic_segmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/processing_oneformer.py#L186)

```py
( *args **kwargs )
```

此方法将其所有参数转发到[OneFormerImageProcessor.post_process_semantic_segmentation()](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerImageProcessor.post_process_semantic_segmentation)。有关更多信息，请参阅此方法的文档字符串。

## OneFormerModel

### `class transformers.OneFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2898)

```py
( config: OneFormerConfig )
```

参数

+   `config`（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的OneFormer模型，在顶部没有任何特定的头部输出原始隐藏状态。此模型是PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L2919)

```py
( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。像素值可以使用[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)获得。有关详细信息，请参阅`OneFormerProcessor.__call__()`。

+   `task_inputs`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 任务输入。任务输入可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获得。有关详细信息，请参阅`OneFormerProcessor.__call__()`。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）- 用于避免在填充像素值上执行注意力的掩码。在`[0, 1]`中选择的掩码值：

    +   对于真实像素为1（即`not masked`），

    +   对于填充的像素为0（即`masked`）。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `output_attentions`（`bool`，*可选*）- 是否返回Detr解码器注意力层的注意力张量。

+   `return_dict`（`bool`，*可选*）- 是否返回`~OneFormerModelOutput`而不是普通元组。

返回

[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)或`tuple(torch.FloatTensor)`

[transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput)或`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）和输入的各种元素。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出+一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_object_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）- transformer解码器中最后一层的输出对象查询。

+   `transformer_decoder_contrastive_queries`（形状为`(batch_size, num_queries, hidden_dim)`的`torch.FloatTensor`）- 来自transformer解码器的对比查询。

+   `transformer_decoder_mask_predictions`（形状为`(batch_size, num_queries, height, width)`的`torch.FloatTensor`）- transformer解码器中最后一层的掩码预测。

+   `transformer_decoder_class_predictions`（形状为`(batch_size, num_queries, num_classes+1)`的`torch.FloatTensor`）- transformer解码器中最后一层的类别预测。

+   `transformer_decoder_auxiliary_predictions`（`str，torch.FloatTensor`字典的元组，*可选*）- 来自transformer解码器每一层的类别和掩码预测的元组。

+   `text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）- 从用于训练期间计算对比损失的输入文本列表派生的文本查询。

+   `task_token`（形状为`(batch_size, hidden_dim)`的`torch.FloatTensor`）- 用于条件查询的一维任务令牌。

+   `attentions`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组。来自变压器解码器的自注意力和交叉注意力权重。

`OneFormerModelOutput`

[OneFormerModel](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import OneFormerProcessor, OneFormerModel

>>> # download texting image
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # load processor for preprocessing the inputs
>>> processor = OneFormerProcessor.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> model = OneFormerModel.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> inputs = processor(image, ["semantic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> mask_predictions = outputs.transformer_decoder_mask_predictions
>>> class_predictions = outputs.transformer_decoder_class_predictions

>>> f"👉 Mask Predictions Shape: {list(mask_predictions.shape)}, Class Predictions Shape: {list(class_predictions.shape)}"
'👉 Mask Predictions Shape: [1, 150, 128, 171], Class Predictions Shape: [1, 150, 151]'
```

## OneFormerForUniversalSegmentation

### `class transformers.OneFormerForUniversalSegmentation`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3027)

```py
( config: OneFormerConfig )
```

参数

+   `config`（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

OneFormer模型例如，语义和全景图像分割。此模型是PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/oneformer/modeling_oneformer.py#L3098)

```py
( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_auxiliary_logits: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[OneFormerProcessor](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerProcessor)获取。有关详细信息，请参阅`OneFormerProcessor.__call__()`。

+   `task_inputs`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 任务输入。任务输入可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅`OneFormerProcessor.__call__()`。

+   `pixel_mask`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）— 用于避免在填充像素值上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于真实像素（即`not masked`）的像素为1，

    +   对于填充像素（即`masked`）的像素为0。

    注意力掩码是什么？

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `output_attentions`（`bool`，*可选*）— 是否返回Detr解码器注意力层的注意力张量。

+   `return_dict`（`bool`，*可选*）— 是否返回`~OneFormerModelOutput`而不是普通元组。

+   `text_inputs`（`List[torch.Tensor]`，*可选*）— 形状为`(num_queries, sequence_length)`的张量，将被馈送到模型

+   `mask_labels`（`List[torch.Tensor]`，*可选*）— 形状为`(num_labels, height, width)`的掩码标签列表，将被馈送到模型

+   `class_labels`（`List[torch.LongTensor]`，*可选*）— 形状为`(num_labels, height, width)`的目标类标签列表，将被馈送到模型。它们标识`mask_labels`的标签，例如，如果`class_labels[i][j]`的标签是`mask_labels[i][j]`。

返回

[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)或`tuple(torch.FloatTensor)`

[transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[OneFormerConfig](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerConfig)）和输入的各种元素。

+   `loss`（`torch.Tensor`，*可选*）—计算得到的损失，在存在标签时返回。

+   `class_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, num_labels + 1)`的张量，表示每个查询的提议类别。请注意，需要`+ 1`，因为我们包含了空类。

+   `masks_queries_logits`（`torch.FloatTensor`）—形状为`(batch_size, num_queries, height, width)`的张量，表示每个查询的提议掩码。

+   `auxiliary_predictions`（`str`，`torch.FloatTensor`字典的列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。编码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `pixel_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。像素解码器模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个阶段的输出）。transformer解码器在每个阶段输出的隐藏状态（也称为特征图）。

+   `transformer_decoder_object_queries`（`torch.FloatTensor`，形状为`(batch_size, num_queries, hidden_dim)`）—transformer解码器中最后一层的输出对象查询。

+   `transformer_decoder_contrastive_queries`（`torch.FloatTensor`，形状为`(batch_size, num_queries, hidden_dim)`）—transformer解码器中的对比查询。

+   `transformer_decoder_mask_predictions`（`torch.FloatTensor`，形状为`(batch_size, num_queries, height, width)`）—transformer解码器中最后一层的掩码预测。

+   `transformer_decoder_class_predictions`（`torch.FloatTensor`，形状为`(batch_size, num_queries, num_classes+1)`）—transformer解码器中最后一层的类别预测。

+   `transformer_decoder_auxiliary_predictions`（`str`，`torch.FloatTensor`字典的列表，*可选*）—来自transformer解码器每一层的类别和掩码预测的列表。

+   `text_queries`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_queries, hidden_dim)`）—从用于训练期间计算对比损失的输入文本列表派生的文本查询。

+   `task_token`（`torch.FloatTensor`，形状为`(batch_size, hidden_dim)`）—用于条件查询的一维任务令牌。

+   `attentions` (`tuple(tuple(torch.FloatTensor))`, *可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tuple(torch.FloatTensor)`元组（每层一个）。来自变压器解码器的自注意力和交叉注意力权重。

`OneFormerUniversalSegmentationOutput`

[OneFormerForUniversalSegmentation](/docs/transformers/v4.37.2/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

通用分割示例：

```py
>>> from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation
>>> from PIL import Image
>>> import requests
>>> import torch

>>> # load OneFormer fine-tuned on ADE20k for universal segmentation
>>> processor = OneFormerProcessor.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")
>>> model = OneFormerForUniversalSegmentation.from_pretrained("shi-labs/oneformer_ade20k_swin_tiny")

>>> url = (
...     "https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg"
... )
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # Semantic Segmentation
>>> inputs = processor(image, ["semantic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for semantic postprocessing
>>> predicted_semantic_map = processor.post_process_semantic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]
>>> f"👉 Semantic Predictions Shape: {list(predicted_semantic_map.shape)}"
'👉 Semantic Predictions Shape: [512, 683]'

>>> # Instance Segmentation
>>> inputs = processor(image, ["instance"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for instance postprocessing
>>> predicted_instance_map = processor.post_process_instance_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]["segmentation"]
>>> f"👉 Instance Predictions Shape: {list(predicted_instance_map.shape)}"
'👉 Instance Predictions Shape: [512, 683]'

>>> # Panoptic Segmentation
>>> inputs = processor(image, ["panoptic"], return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)
>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`
>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`
>>> class_queries_logits = outputs.class_queries_logits
>>> masks_queries_logits = outputs.masks_queries_logits

>>> # you can pass them to processor for panoptic postprocessing
>>> predicted_panoptic_map = processor.post_process_panoptic_segmentation(
...     outputs, target_sizes=[image.size[::-1]]
... )[0]["segmentation"]
>>> f"👉 Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}"
'👉 Panoptic Predictions Shape: [512, 683]'
```
