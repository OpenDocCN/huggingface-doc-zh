["```py\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n\n```", "```py\nimport imageio\nimport requests\nimport torch\nfrom diffusers import AnimateDiffVideoToVideoPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\nfrom io import BytesIO\nfrom PIL import Image\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffVideoToVideoPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16).to(\"cuda\")\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\n# helper function to load videos\ndef load_video(file_path: str):\n    images = []\n\n    if file_path.startswith(('http://', 'https://')):\n        # If the file_path is a URL\n        response = requests.get(file_path)\n        response.raise_for_status()\n        content = BytesIO(response.content)\n        vid = imageio.get_reader(content)\n    else:\n        # Assuming it's a local file path\n        vid = imageio.get_reader(file_path)\n\n    for frame in vid:\n        pil_image = Image.fromarray(frame)\n        images.append(pil_image)\n\n    return images\n\nvideo = load_video(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-vid2vid-input-1.gif\")\n\noutput = pipe(\n    video = video,\n    prompt=\"panda playing a guitar, on a boat, in the ocean, high quality\",\n    negative_prompt=\"bad quality, worse quality\",\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    strength=0.5,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```", "```py\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\npipe.load_lora_weights(\n    \"guoyww/animatediff-motion-lora-zoom-out\", adapter_name=\"zoom-out\"\n)\n\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    beta_schedule=\"linear\",\n    timestep_spacing=\"linspace\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n\n```", "```py\npip install peft\n```", "```py\nimport torch\nfrom diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter\nfrom diffusers.utils import export_to_gif\n\n# Load the motion adapter\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=torch.float16)\n# load SD 1.5 based finetuned model\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16)\n\npipe.load_lora_weights(\n    \"diffusers/animatediff-motion-lora-zoom-out\", adapter_name=\"zoom-out\",\n)\npipe.load_lora_weights(\n    \"diffusers/animatediff-motion-lora-pan-left\", adapter_name=\"pan-left\",\n)\npipe.set_adapters([\"zoom-out\", \"pan-left\"], adapter_weights=[1.0, 1.0])\n\nscheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    beta_schedule=\"linear\",\n    steps_offset=1,\n)\npipe.scheduler = scheduler\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_model_cpu_offload()\n\noutput = pipe(\n    prompt=(\n        \"masterpiece, bestquality, highlydetailed, ultradetailed, sunset, \"\n        \"orange sky, warm lighting, fishing boats, ocean waves seagulls, \"\n        \"rippling water, wharf, silhouette, serene atmosphere, dusk, evening glow, \"\n        \"golden hour, coastal landscape, seaside scenery\"\n    ),\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=25,\n    generator=torch.Generator(\"cpu\").manual_seed(42),\n)\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n\n```", "```py\nimport torch\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\nfrom diffusers.utils import export_to_gif\n\nadapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\nmodel_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\npipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DDIMScheduler.from_pretrained(\n    model_id,\n    subfolder=\"scheduler\",\n    beta_schedule=\"linear\",\n    clip_sample=False,\n    timestep_spacing=\"linspace\",\n    steps_offset=1\n)\n\n# enable memory savings\npipe.enable_vae_slicing()\npipe.enable_vae_tiling()\n\n# enable FreeInit\n# Refer to the enable_free_init documentation for a full list of configurable parameters\npipe.enable_free_init(method=\"butterworth\", use_fast_sampling=True)\n\n# run inference\noutput = pipe(\n    prompt=\"a panda playing a guitar, on a boat, in the ocean, high quality\",\n    negative_prompt=\"bad quality, worse quality\",\n    num_frames=16,\n    guidance_scale=7.5,\n    num_inference_steps=20,\n    generator=torch.Generator(\"cpu\").manual_seed(666),\n)\n\n# disable FreeInit\npipe.disable_free_init()\n\nframes = output.frames[0]\nexport_to_gif(frames, \"animation.gif\")\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel motion_adapter: MotionAdapter scheduler: Union feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )\n```", "```py\n( prompt: Union = None num_frames: Optional = 16 height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';TextToVideoSDPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler\n>>> from diffusers.utils import export_to_gif\n\n>>> adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\")\n>>> pipe = AnimateDiffPipeline.from_pretrained(\"frankjoshua/toonyou_beta6\", motion_adapter=adapter)\n>>> pipe.scheduler = DDIMScheduler(beta_schedule=\"linear\", steps_offset=1, clip_sample=False)\n>>> output = pipe(prompt=\"A corgi walking in the park\")\n>>> frames = output.frames[0]\n>>> export_to_gif(frames, \"animation.gif\")\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( num_iters: int = 3 use_fast_sampling: bool = False method: str = 'butterworth' order: int = 4 spatial_stop_frequency: float = 0.25 temporal_stop_frequency: float = 0.25 generator: Generator = None )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel motion_adapter: MotionAdapter scheduler: Union feature_extractor: CLIPImageProcessor = None image_encoder: CLIPVisionModelWithProjection = None )\n```", "```py\n( video: List = None prompt: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 50 timesteps: Optional = None guidance_scale: float = 7.5 strength: float = 0.8 negative_prompt: Union = None num_videos_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] ) \u2192 export const metadata = 'undefined';AnimateDiffPipelineOutput or tuple\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( frames: Union )\n```"]