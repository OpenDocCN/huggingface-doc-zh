- en: UVit2DModel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/api/models/uvit2d](https://huggingface.co/docs/diffusers/api/models/uvit2d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/28.37a5e9af.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: The [U-ViT](https://hf.co/papers/2301.11093) model is a vision transformer (ViT)
    based UNet. This model incorporates elements from ViT (considers all inputs such
    as time, conditions and noisy image patches as tokens) and a UNet (long skip connections
    between the shallow and deep layers). The skip connection is important for predicting
    pixel-level features. An additional 3x3 convolutional block is applied prior to
    the final output to improve image quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Currently, applying diffusion models in pixel space of high resolution images
    is difficult. Instead, existing approaches focus on diffusion in lower dimensional
    spaces (latent diffusion), or have multiple super-resolution levels of generation
    referred to as cascades. The downside is that these approaches add additional
    complexity to the diffusion framework. This paper aims to improve denoising diffusion
    for high resolution images while keeping the model as simple as possible. The
    paper is centered around the research question: How can one train a standard denoising
    diffusion models on high resolution images, and still obtain performance comparable
    to these alternate approaches? The four main findings are: 1) the noise schedule
    should be adjusted for high resolution images, 2) It is sufficient to scale only
    a particular part of the architecture, 3) dropout should be added at specific
    locations in the architecture, and 4) downsampling is an effective strategy to
    avoid high resolution feature maps. Combining these simple yet effective techniques,
    we achieve state-of-the-art on image generation among diffusion models without
    sampling modifiers on ImageNet.*'
  prefs: []
  type: TYPE_NORMAL
- en: UVit2DModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.UVit2DModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L39)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### `set_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L241)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) —
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sets the attention processor to use to compute attention.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_default_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L276)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Disables custom attention processors and sets the default attention implementation.
  prefs: []
  type: TYPE_NORMAL
- en: UVit2DConvEmbed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.unets.uvit_2d.UVit2DConvEmbed`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L292)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: UVitBlock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.unets.uvit_2d.UVitBlock`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L307)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ConvNextBlock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.unets.uvit_2d.ConvNextBlock`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L406)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ConvMlmLayer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.unets.uvit_2d.ConvMlmLayer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L451)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
