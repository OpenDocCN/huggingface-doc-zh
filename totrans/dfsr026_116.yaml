- en: UVit2DModel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UVit2DModel
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/uvit2d](https://huggingface.co/docs/diffusers/api/models/uvit2d)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/models/uvit2d](https://huggingface.co/docs/diffusers/api/models/uvit2d)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [U-ViT](https://hf.co/papers/2301.11093) model is a vision transformer (ViT)
    based UNet. This model incorporates elements from ViT (considers all inputs such
    as time, conditions and noisy image patches as tokens) and a UNet (long skip connections
    between the shallow and deep layers). The skip connection is important for predicting
    pixel-level features. An additional 3x3 convolutional block is applied prior to
    the final output to improve image quality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[U-ViT](https://hf.co/papers/2301.11093) 模型是基于视觉变换器（ViT）的UNet。该模型结合了ViT（将所有输入，如时间、条件和嘈杂的图像补丁视为标记）和UNet（浅层和深层之间的长跳连接）的元素。跳连接对于预测像素级特征很重要。在最终输出之前应用了额外的3x3卷积块以提高图像质量。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Currently, applying diffusion models in pixel space of high resolution images
    is difficult. Instead, existing approaches focus on diffusion in lower dimensional
    spaces (latent diffusion), or have multiple super-resolution levels of generation
    referred to as cascades. The downside is that these approaches add additional
    complexity to the diffusion framework. This paper aims to improve denoising diffusion
    for high resolution images while keeping the model as simple as possible. The
    paper is centered around the research question: How can one train a standard denoising
    diffusion models on high resolution images, and still obtain performance comparable
    to these alternate approaches? The four main findings are: 1) the noise schedule
    should be adjusted for high resolution images, 2) It is sufficient to scale only
    a particular part of the architecture, 3) dropout should be added at specific
    locations in the architecture, and 4) downsampling is an effective strategy to
    avoid high resolution feature maps. Combining these simple yet effective techniques,
    we achieve state-of-the-art on image generation among diffusion models without
    sampling modifiers on ImageNet.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*目前，在高分辨率图像的像素空间中应用扩散模型是困难的。相反，现有方法侧重于在较低维度空间（潜在扩散）中进行扩散，或者具有多个被称为级联的超分辨率生成级别。缺点是这些方法会给扩散框架增加额外的复杂性。本文旨在改进高分辨率图像的去噪扩散，同时尽可能保持模型简单。本文围绕研究问题展开：如何在高分辨率图像上训练标准的去噪扩散模型，并且仍然获得与这些替代方法可比的性能？四个主要发现是：1）噪声计划应针对高分辨率图像进行调整，2）只需对架构的特定部分进行缩放即可，3）应在架构的特定位置添加辍学，4）下采样是避免高分辨率特征图的有效策略。结合这些简单而有效的技术，我们在ImageNet上实现了扩散模型中的图像生成的最新技术。*'
- en: UVit2DModel
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UVit2DModel
- en: '### `class diffusers.UVit2DModel`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UVit2DModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L39)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L39)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#### `set_attn_processor`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L241)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L241)'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) —
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`processor`（`AttentionProcessor`的字典或仅`AttentionProcessor`）— 实例化的处理器类或将设置为**所有**`Attention`层的处理器的处理器类字典。'
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果`processor`是一个字典，则键需要定义到相应交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。
- en: Sets the attention processor to use to compute attention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于计算注意力的注意力处理器。
- en: '#### `set_default_attn_processor`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_default_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L276)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L276)'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Disables custom attention processors and sets the default attention implementation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用自定义注意力处理器并设置默认的注意力实现。
- en: UVit2DConvEmbed
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UVit2DConvEmbed
- en: '### `class diffusers.models.unets.uvit_2d.UVit2DConvEmbed`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.unets.uvit_2d.UVit2DConvEmbed`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L292)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L292)'
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: UVitBlock
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UVitBlock
- en: '### `class diffusers.models.unets.uvit_2d.UVitBlock`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.unets.uvit_2d.UVitBlock`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L307)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L307)'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ConvNextBlock
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvNextBlock
- en: '### `class diffusers.models.unets.uvit_2d.ConvNextBlock`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.unets.uvit_2d.ConvNextBlock`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L406)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L406)'
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ConvMlmLayer
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ConvMlmLayer
- en: '### `class diffusers.models.unets.uvit_2d.ConvMlmLayer`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.unets.uvit_2d.ConvMlmLayer`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L451)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/uvit_2d.py#L451)'
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
