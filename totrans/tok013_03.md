# 快速入门

> 原始文本：[https://huggingface.co/docs/tokenizers/quicktour](https://huggingface.co/docs/tokenizers/quicktour)

让我们快速看一下🤗 Tokenizers库的特点。该库提供了今天最常用的标记器的实现，既易于使用又速度快。

## 从头开始构建一个标记器

为了说明🤗 Tokenizers库有多快，让我们在几秒钟内在[wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)（516M文本）上训练一个新的标记器。首先，您需要下载这个数据集并解压缩：

```py
wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip
```

### 训练标记器

在本次快速入门中，我们将构建和训练一个字节对编码（BPE）标记器。有关不同类型的标记器的更多信息，请查看🤗 Transformers文档中的[指南](https://huggingface.co/transformers/tokenizer_summary.html)。在这里，训练标记器意味着它将通过以下方式学习合并规则：

+   从训练语料库中的所有字符开始作为标记。

+   识别最常见的一对标记并将其合并为一个标记。

+   重复直到词汇表（例如，标记数）达到我们想要的大小。

该库的主要API是`class` `Tokenizer`，以下是如何使用BPE模型实例化一个：

PythonRustNode

```py
from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
```

要在wikitext文件上训练我们的标记器，我们将需要实例化一个[trainer]{.title-ref}，在这种情况下是一个`BpeTrainer`

PythonRustNode

```py
from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
```

我们可以设置训练参数，如`vocab_size`或`min_frequency`（这里保持默认值为30,000和0），但最重要的部分是提供我们以后打算使用的`special_tokens`（在训练过程中根本不使用），以便它们被插入到词汇表中。

编写特殊标记列表的顺序很重要：这里`"[UNK]"`将获得ID 0，`"[CLS]"`将获得ID 1，依此类推。

我们现在可以训练我们的标记器，但这并不是最佳选择。没有一个预标记器将我们的输入拆分为单词，我们可能会得到重叠几个单词的标记：例如，我们可能会得到一个`"it is"`标记，因为这两个单词经常相邻出现。使用预标记器将确保没有一个标记比预标记器返回的单词更大。在这里，我们想训练一个子词BPE标记器，并且我们将使用最简单的预标记器，即按空格拆分。

PythonRustNode

```py
from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()
```

现在，我们只需调用`Tokenizer.train`方法，并使用任何我们想要使用的文件列表：

PythonRustNode

```py
files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)
```

在完整的维基文本数据集上训练我们的标记器应该只需要几秒钟！要将标记器保存在包含所有配置和词汇表的文件中，只需使用`Tokenizer.save`方法：

PythonRustNode

```py
tokenizer.save("data/tokenizer-wiki.json")
```

您可以使用`Tokenizer.from_file`的`classmethod`从该文件重新加载您的标记器：

PythonRustNode

```py
tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")
```

### 使用标记器

现在我们已经训练了一个标记器，我们可以使用`Tokenizer.encode`方法对任何文本进行标记：

PythonRustNode

```py
output = tokenizer.encode("Hello, y'all! How are you 😁 ?")
```

这将在文本上应用标记器的完整流程，返回一个`Encoding`对象。要了解有关此流程的更多信息，以及如何应用（或自定义）其中的部分，请查看[此页面](pipeline)。 

这个`Encoding`对象具有您深度学习模型（或其他模型）所需的所有属性。`tokens`属性包含了文本在标记中的分割：

PythonRustNode

```py
print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]
```

同样，`ids`属性将包含这些标记在标记器词汇表中的索引：

PythonRustNode

```py
print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]
```

🤗 Tokenizers库的一个重要特性是它具有完整的对齐跟踪，这意味着您始终可以获得原始句子中与给定标记对应的部分。这些存储在我们的`Encoding`对象的`offsets`属性中。例如，假设我们想找回导致`"[UNK]"`标记出现的原因，该标记在列表中的索引为9，我们只需请求该索引处的偏移量：

PythonRustNode

```py
print(output.offsets[9])
# (26, 27)
```

这些是原始句子中对应表情符号的索引：

PythonRustNode

```py
sentence = "Hello, y'all! How are you 😁 ?"
sentence[26:27]
# "😁"
```

### 后处理

我们可能希望我们的分词器自动添加特殊标记，如`"[CLS]"`或`"[SEP]"`。为此，我们使用后处理器。`TemplateProcessing`是最常用的，您只需为单个句子和句子对的处理指定一个模板，以及特殊标记及其ID。

当构建分词器时，我们将`"[CLS]"`和`"[SEP]"`设置为特殊标记列表中的位置1和2，因此这应该是它们的ID。为了双重检查，我们可以使用`Tokenizer.token_to_id`方法：

PythonRustNode

```py
tokenizer.token_to_id("[SEP]")
# 2
```

这是我们如何设置后处理以获得传统的BERT输入：

PythonRustNode

```py
from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)
```

让我们更详细地查看这段代码。首先，我们指定了单个句子的模板：这些应该是`"[CLS] $A [SEP]"`的形式，其中`$A`代表我们的句子。

然后，我们指定了句子对的模板，应该是`"[CLS] $A [SEP] $B [SEP]"`的形式，其中`$A`代表第一个句子，`$B`代表第二个句子。模板中添加的`:1`代表我们希望为输入的每个部分设置的`类型ID`：默认为0（这就是为什么我们没有`$A:0`），这里我们将其设置为1，用于第二个句子的标记和最后的`"[SEP]"`标记。

最后，我们在分词器的词汇表中指定了我们使用的特殊标记及其ID。

为了检查这是否正常工作，让我们尝试对之前的相同句子进行编码：

PythonRustNode

```py
output = tokenizer.encode("Hello, y'all! How are you 😁 ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]
```

要检查一对句子的结果，我们只需将这两个句子传递给`Tokenizer.encode`：

PythonRustNode

```py
output = tokenizer.encode("Hello, y'all!", "How are you 😁 ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]
```

然后您可以使用以下方法检查每个标记分配的类型ID是否正确

PythonRustNode

```py
print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
```

如果您使用`Tokenizer.save`保存了您的分词器，后处理器也会被保存。

### 在批量中编码多个句子

为了获得🤗 Tokenizers库的完整速度，最好通过使用`Tokenizer.encode_batch`方法批量处理文本：

PythonRustNode

```py
output = tokenizer.encode_batch(["Hello, y'all!", "How are you 😁 ?"])
```

然后输出是一个`Encoding`对象的列表，就像我们之前看到的那样。只要内存足够，您可以一起处理尽可能多的文本。

要处理一批句子对，将两个列表传递给`Tokenizer.encode_batch`方法：句子A的列表和句子B的列表：

PythonRustNode

```py
output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you 😁 ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)
```

在编码多个句子时，您可以通过使用`Tokenizer.enable_padding`自动将输出填充到存在的最长句子，使用`pad_token`及其ID（我们可以像之前一样使用`Tokenizer.token_to_id`来双重检查填充标记的ID）：

PythonRustNode

```py
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
```

我们可以设置填充的`方向`（默认为右侧）或给定的`长度`，如果我们想要将每个样本填充到特定数量（这里我们将其保持未设置，以便填充到最长文本的大小）。

PythonRustNode

```py
output = tokenizer.encode_batch(["Hello, y'all!", "How are you 😁 ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]
```

在这种情况下，分词器生成的`注意力掩码`考虑了填充：

PythonRustNode

```py
print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]
```

## 预训练

PythonRustNode

### 使用预训练的分词器

只要存储库中有一个`tokenizer.json`文件可用，您就可以从Hugging Face Hub加载任何分词器。

```py
from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
```

### 从传统词汇文件导入预训练的分词器

只要您有其词汇文件，您也可以直接导入一个预训练的分词器。例如，这是如何导入经典的预训练BERT分词器的方法：

```py
from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
```

只要您已经下载了文件`bert-base-uncased-vocab.txt`，就可以使用预训练的分词器

```py
wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt
```
