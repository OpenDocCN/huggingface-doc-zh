- en: BLIP-2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLIP-2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: 'The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training
    with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
    by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen
    pre-trained image encoders and large language models (LLMs) by training a lightweight,
    12-layer Transformer encoder in between them, achieving state-of-the-art performance
    on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198),
    an 80 billion parameter model, by 8.7% on zero-shot VQAv2 with 54x fewer trainable
    parameters.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'BLIP-2 æ¨¡å‹ç”± Junnan Liã€Dongxu Liã€Silvio Savareseã€Steven Hoi åœ¨[BLIP-2: Bootstrapping
    Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)ä¸­æå‡ºã€‚BLIP-2
    åˆ©ç”¨å†»ç»“çš„é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé€šè¿‡åœ¨å®ƒä»¬ä¹‹é—´è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ 12 å±‚ Transformer ç¼–ç å™¨ï¼Œå®ç°äº†å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒBLIP-2
    åœ¨é›¶æ ·æœ¬ VQAv2 ä¸Šæ¯”[Flamingo](https://arxiv.org/abs/2204.14198)ï¼ˆä¸€ä¸ª 80 äº¿å‚æ•°æ¨¡å‹ï¼‰æé«˜äº† 8.7%ï¼Œå¹¶ä¸”å¯è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº†
    54 å€ã€‚'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*The cost of vision-and-language pre-training has become increasingly prohibitive
    due to end-to-end training of large-scale models. This paper proposes BLIP-2,
    a generic and efficient pre-training strategy that bootstraps vision-language
    pre-training from off-the-shelf frozen pre-trained image encoders and frozen large
    language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer,
    which is pre-trained in two stages. The first stage bootstraps vision-language
    representation learning from a frozen image encoder. The second stage bootstraps
    vision-to-language generative learning from a frozen language model. BLIP-2 achieves
    state-of-the-art performance on various vision-language tasks, despite having
    significantly fewer trainable parameters than existing methods. For example, our
    model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable
    parameters. We also demonstrate the modelâ€™s emerging capabilities of zero-shot
    image-to-text generation that can follow natural language instructions.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç”±äºå¤§è§„æ¨¡æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„æˆæœ¬å˜å¾—è¶Šæ¥è¶Šé«˜ã€‚æœ¬æ–‡æå‡ºäº† BLIP-2ï¼Œä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»ç°æˆçš„å†»ç»“é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œå†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å¯¼è§†è§‰-è¯­è¨€é¢„è®­ç»ƒã€‚BLIP-2
    é€šè¿‡è½»é‡çº§çš„ Querying Transformer æ¶ˆé™¤äº†æ¨¡æ€å·®å¼‚ï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸¤ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µä»å†»ç»“å›¾åƒç¼–ç å™¨å¼•å¯¼è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µä»å†»ç»“è¯­è¨€æ¨¡å‹å¼•å¯¼è§†è§‰-è¯­è¨€ç”Ÿæˆå­¦ä¹ ã€‚å°½ç®¡å¯è®­ç»ƒå‚æ•°æ•°é‡æ˜æ˜¾å°‘äºç°æœ‰æ–¹æ³•ï¼Œä½†
    BLIP-2 åœ¨å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬ VQAv2 ä¸Šæ¯” Flamingo80B æé«˜äº† 8.7%ï¼Œå¹¶ä¸”å¯è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº†
    54 å€ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ¨¡å‹çš„æ–°å…´èƒ½åŠ›ï¼Œå³é›¶æ ·æœ¬å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆï¼Œå¯ä»¥éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚*'
- en: '![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2301.12597)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2301.12597)'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: BLIP-2 can be used for conditional text generation given an image and an optional
    text prompt. At inference time, itâ€™s recommended to use the `generate` method.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLIP-2 å¯ç”¨äºåœ¨ç»™å®šå›¾åƒå’Œå¯é€‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œæ¡ä»¶æ–‡æœ¬ç”Ÿæˆã€‚åœ¨æ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨ `generate` æ–¹æ³•ã€‚
- en: One can use [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)
    to prepare images for the model, and decode the predicted tokens IDâ€™s back to
    text.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒï¼Œå¹¶å°†é¢„æµ‹çš„æ ‡è®°IDè§£ç å›æ–‡æœ¬ã€‚
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with BLIP-2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ BLIP-2ã€‚
- en: Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA)
    and chat-like conversations can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLIP-2 çš„æ¼”ç¤ºç¬”è®°æœ¬ç”¨äºå›¾åƒå­—å¹•ã€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œç±»ä¼¼å¯¹è¯çš„ä¼šè¯ï¼Œå¯åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2)æ‰¾åˆ°ã€‚
- en: If youâ€™re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: Blip2Config
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Config
- en: '### `class transformers.Blip2Config`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vision_config` (`dict`, *optional*) â€” Dictionary of configuration options
    used to initialize [Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–[Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚'
- en: '`qformer_config` (`dict`, *optional*) â€” Dictionary of configuration options
    used to initialize [Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–[Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚'
- en: '`text_config` (`dict`, *optional*) â€” Dictionary of configuration options used
    to initialize any [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–ä»»ä½•[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚'
- en: '`num_query_tokens` (`int`, *optional*, defaults to 32) â€” The number of query
    tokens passed through the Transformer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_query_tokens` (`int`, *optional*, defaults to 32) â€” é€šè¿‡Transformerä¼ é€’çš„æŸ¥è¯¢ä»¤ç‰Œæ•°é‡ã€‚'
- en: '`kwargs` (*optional*) â€” Dictionary of keyword arguments.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) â€” å…³é”®å­—å‚æ•°çš„å­—å…¸ã€‚'
- en: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)
    is the configuration class to store the configuration of a [Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration).
    It is used to instantiate a BLIP-2 model according to the specified arguments,
    defining the vision model, Q-Former model and language model configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)æ˜¯ç”¨äºå­˜å‚¨[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªBLIP-2æ¨¡å‹ï¼Œå®šä¹‰è§†è§‰æ¨¡å‹ã€Q-Formeræ¨¡å‹å’Œè¯­è¨€æ¨¡å‹é…ç½®ã€‚ä½¿ç”¨é»˜è®¤é…ç½®å®ä¾‹åŒ–å°†äº§ç”Ÿç±»ä¼¼äºBLIP-2
    [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    æ¶æ„çš„é…ç½®ã€‚'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_vision_qformer_text_configs`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_vision_qformer_text_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)'
- en: An instance of a configuration object
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡çš„å®ä¾‹
- en: Instantiate a [Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)
    (or a derived class) from a BLIP-2 vision model, Q-Former and language model configurations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»BLIP-2è§†è§‰æ¨¡å‹ã€Q-Formerå’Œè¯­è¨€æ¨¡å‹é…ç½®ä¸­å®ä¾‹åŒ–ä¸€ä¸ª[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚
- en: Blip2VisionConfig
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2VisionConfig
- en: '### `class transformers.Blip2VisionConfig`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2VisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`hidden_size` (`int`, *optional*, defaults to 1408) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 1408) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 6144) â€” Dimensionality
    of the â€œintermediateâ€ (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 6144) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 39) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 39) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 16) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚'
- en: '`image_size` (`int`, *optional*, defaults to 224) â€” The size (resolution) of
    each image.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`patch_size` (`int`, *optional*, defaults to 14) â€” The size (resolution) of
    each patch.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 14) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"gelu"` are supported.
    layer_norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon used by the
    layer normalization layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`
    `"gelu"`ã€‚layer_norm_eps (`float`, *optional*, defaults to 1e-5): å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” Whether to add a bias
    to the queries and values in the self-attention layers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä¸ºæŸ¥è¯¢å’Œå€¼æ·»åŠ åç½®ã€‚'
- en: This is the configuration class to store the configuration of a [Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel).
    It is used to instantiate a BLIP-2 vision encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration defaults will yield
    a similar configuration to that of the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªBLIP-2è§†è§‰ç¼–ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚å®ä¾‹åŒ–é»˜è®¤é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºBLIP-2
    [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Blip2QFormerConfig
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2QFormerConfig
- en: '### `class transformers.Blip2QFormerConfig`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2QFormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) â€” Vocabulary size of the
    Q-Former model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling the model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 30522) â€” Q-Former æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’çš„
    `inputs_ids` å¯è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) â€” Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Number of hidden
    layers in the Transformer encoder.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Dimensionality
    of the â€œintermediateâ€ (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ
    `"gelu"`ã€`"relu"`ã€`"silu"` å’Œ `"gelu_new"`ã€‚'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„
    dropout æ¦‚ç‡ã€‚'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” The
    dropout ratio for the attention probabilities.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„
    dropout æ¯”ç‡ã€‚'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) â€” The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” The epsilon used
    by the layer normalization layers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) â€” Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹©
    `"absolute"`ã€`"relative_key"`ã€`"relative_key_query"` ä¸­çš„ä¸€ä¸ªã€‚å¯¹äºä½ç½®åµŒå…¥ï¼Œè¯·ä½¿ç”¨ `"absolute"`ã€‚æœ‰å…³
    `"relative_key"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Self-Attention with Relative Position Representations
    (Shaw et al.)](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³ `"relative_key_query"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Improve
    Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)
    ä¸­çš„ *Method 4*ã€‚'
- en: '`cross_attention_frequency` (`int`, *optional*, defaults to 2) â€” The frequency
    of adding cross-attention to the Transformer layers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_frequency` (`int`, *optional*, defaults to 2) â€” å‘ Transformer
    å±‚æ·»åŠ äº¤å‰æ³¨æ„åŠ›çš„é¢‘ç‡ã€‚'
- en: '`encoder_hidden_size` (`int`, *optional*, defaults to 1408) â€” The hidden size
    of the hidden states for cross-attention.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_size` (`int`, *optional*, defaults to 1408) â€” äº¤å‰æ³¨æ„åŠ›ä¸­éšè—çŠ¶æ€çš„éšè—å¤§å°ã€‚'
- en: This is the configuration class to store the configuration of a [Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel).
    It is used to instantiate a BLIP-2 Querying Transformer (Q-Former) model according
    to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª
    BLIP-2 Querying Transformer (Q-Former) æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    æ¶æ„çš„é…ç½®ã€‚é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: Note that [Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)
    is very similar to [BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)
    with interleaved cross-attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œ[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)ä¸[BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)éå¸¸ç›¸ä¼¼ï¼Œå…·æœ‰äº¤é”™çš„äº¤å‰æ³¨æ„åŠ›ã€‚
- en: 'Examples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Blip2Processor
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Processor
- en: '### `class transformers.Blip2Processor`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)'
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`image_processor` (`BlipImageProcessor`) â€” An instance of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor).
    The image processor is a required input.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`ï¼ˆ`BlipImageProcessor`ï¼‰â€” [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)çš„ä¸€ä¸ªå®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` (`AutoTokenizer`) â€” An instance of [â€˜PreTrainedTokenizer`]. The
    tokenizer is a required input.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`AutoTokenizer`ï¼‰â€” [â€˜PreTrainedTokenizerâ€™]çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5
    tokenizer into a single processor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªBLIP-2å¤„ç†å™¨ï¼Œå°†BLIPå›¾åƒå¤„ç†å™¨å’ŒOPT/T5åˆ†è¯å™¨å°è£…åˆ°ä¸€ä¸ªå¤„ç†å™¨ä¸­ã€‚
- en: '[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)
    offers all the functionalities of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See the docstring of `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)
    for more information.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)æä¾›äº†[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)å’Œ[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…`__call__()`å’Œ[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚'
- en: '#### `batch_decode`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)'
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method forwards all its arguments to PreTrainedTokenizerâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `decode`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç 
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)'
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This method forwards all its arguments to PreTrainedTokenizerâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: Blip2VisionModel
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2VisionModel
- en: '### `class transformers.Blip2VisionModel`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2VisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `forward`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å‰è¿›
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class
    'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼‰-
    æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    â€” Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`ï¼‰- ç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼Œåºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äºBERTç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œtanhæ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: Blip2QFormerModel
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2QFormerModel
- en: '### `class transformers.Blip2QFormerModel`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2QFormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Querying Transformer (Q-Former), used in BLIP-2.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢å˜å‹å™¨ï¼ˆQ-Formerï¼‰ï¼Œç”¨äºBLIP-2ã€‚
- en: '#### `forward`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, `optional`): Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder. encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size,
    sequence_length)`, `optional`): Mask to avoid performing attention on the padding
    token indices of the encoder input. This mask is used in the cross-attention if
    the model is configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: encoder_hidden_statesï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ`å¯é€‰`ï¼‰ï¼šç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚encoder_attention_maskï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`ï¼Œ`å¯é€‰`ï¼‰ï¼šé¿å…å¯¹ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•æ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0,
    1]`ä¸­ï¼š
- en: 1 for tokens that are `not masked`,
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ä¸º1ï¼Œ
- en: '0 for tokens that are `masked`. past_key_values (`tuple(tuple(torch.FloatTensor))`
    of length `config.n_layers` with each tuple having 4 tensors of: shape `(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key
    and value hidden states of the attention blocks. Can be used to speed up decoding.
    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
    use_cache (`bool`, `optional`): If set to `True`, `past_key_values` key value
    states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºè¢«â€œæ©ç›–â€çš„æ ‡è®°ä¸º0ã€‚past_key_valuesï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰4ä¸ªå¼ é‡ï¼šå½¢çŠ¶ä¸º`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`ï¼‰ï¼šåŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚use_cacheï¼ˆ`bool`ï¼Œ`å¯é€‰`ï¼‰ï¼šå¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: Blip2Model
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Model
- en: '### `class transformers.Blip2Model`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)'
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)ï¼‰â€”
    å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: BLIP-2 Model for generating text and image features. The model consists of a
    vision encoder, Querying Transformer (Q-Former) and a language model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒç‰¹å¾çš„BLIP-2æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±è§†è§‰ç¼–ç å™¨ã€æŸ¥è¯¢å˜æ¢å™¨ï¼ˆQ-Formerï¼‰å’Œè¯­è¨€æ¨¡å‹ç»„æˆã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)'
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€”
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚è¾“å…¥æ ‡è®°å¯ä»¥é€‰æ‹©æ€§åœ°æä¾›ä½œä¸ºæ–‡æœ¬æç¤ºï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ã€‚'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 ç”¨äºâ€œæœªæ©ç â€æ ‡è®°çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 ç”¨äºâ€œæ©ç â€æ ‡è®°çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    æˆ– `tuple(torch.FloatTensor)`'
- en: A `transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    æˆ– `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class ''transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig''>`ï¼‰å’Œè¾“å…¥ã€‚'
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `labels` is provided,
    `torch.FloatTensor` of shape `(1,)`) â€” Language modeling loss from the language
    model.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾› `labels` æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º `(1,)` çš„ `torch.FloatTensor`ï¼‰â€”
    è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head of the language model.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)` çš„ `torch.FloatTensor`ï¼‰â€”
    è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ã€‚'
- en: '`vision_outputs` (`BaseModelOutputWithPooling`) â€” Outputs of the vision encoder.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_outputs`ï¼ˆ`BaseModelOutputWithPooling`ï¼‰â€” è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºã€‚'
- en: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) â€” Outputs
    of the Q-Former (Querying Transformer).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_outputs`ï¼ˆ`BaseModelOutputWithPoolingAndCrossAttentions`ï¼‰â€” Q-Formerï¼ˆQuerying
    Transformerï¼‰çš„è¾“å‡ºã€‚'
- en: '`language_model_outputs` (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`) â€”
    Outputs of the language model.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model_outputs`ï¼ˆ`CausalLMOutputWithPast` æˆ– `Seq2SeqLMOutput`ï¼‰â€” è¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚'
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#### `get_text_features`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_text_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)'
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥
    IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº `æœªè¢«æ©ç›–` çš„æ ‡è®°ä¸º 1ï¼Œ
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº `è¢«æ©ç›–` çš„æ ‡è®°ä¸º 0ã€‚[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, target_sequence_length)` çš„ `torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ](../glossary#decoder-input-ids)'
- en: T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: T5 ä½¿ç”¨ `pad_token_id` ä½œä¸º `decoder_input_ids` ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™å¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„
    `decoder_input_ids`ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [T5 Training](./t5#training).
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡ `decoder_input_ids` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [T5 Training](./t5#training)ã€‚
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, target_sequence_length)` çš„ `torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥ `decoder_input_ids` ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: text_outputs (`CausalLMOutputWithPast`, or `tuple(torch.FloatTensor)` if `return_dict=False`)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: text_outputs (`CausalLMOutputWithPast`ï¼Œæˆ–è€…å¦‚æœ`return_dict=False`åˆ™ä¸º`tuple(torch.FloatTensor)`)
- en: The language model outputs. If `return_dict=True`, the output is a `CausalLMOutputWithPast`
    that contains the language model logits, the past key values and the hidden states
    if `output_hidden_states=True`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­è¨€æ¨¡å‹è¾“å‡ºã€‚å¦‚æœ`return_dict=True`ï¼Œåˆ™è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«è¯­è¨€æ¨¡å‹logitsã€è¿‡å»çš„é”®å€¼å’Œéšè—çŠ¶æ€ï¼ˆå¦‚æœ`output_hidden_states=True`ï¼‰çš„`CausalLMOutputWithPast`ã€‚
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `get_image_features`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_image_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)'
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: vision_outputs (`BaseModelOutputWithPooling`æˆ–`torch.FloatTensor`çš„å…ƒç»„ï¼‰
- en: The vision model outputs. If `return_dict=True`, the output is a `BaseModelOutputWithPooling`
    that contains the image features, the pooled image features and the hidden states
    if `output_hidden_states=True`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰æ¨¡å‹è¾“å‡ºã€‚å¦‚æœ`return_dict=True`ï¼Œåˆ™è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«å›¾åƒç‰¹å¾ã€æ± åŒ–å›¾åƒç‰¹å¾å’Œéšè—çŠ¶æ€ï¼ˆå¦‚æœ`output_hidden_states=True`ï¼‰çš„`BaseModelOutputWithPooling`ã€‚
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `get_qformer_features`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_qformer_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)'
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height,
    width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” è¯­è¨€æ¨¡å‹è¯æ±‡ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥æä¾›è¾“å…¥æ ‡è®°ä½œä¸ºæ–‡æœ¬æç¤ºï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ç”Ÿæˆã€‚'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*)
    â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`not masked`çš„æ ‡è®°ä¸º1ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº`masked`çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The vision model outputs. If `return_dict=True`, the output is a `BaseModelOutputWithPooling`
    that contains the image features, the pooled image features and the hidden states
    if `output_hidden_states=True`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Blip2ForConditionalGeneration
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Blip2ForConditionalGeneration`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1524)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLIP-2 Model for generating text given an image and an optional text prompt.
    The model consists of a vision encoder, Querying Transformer (Q-Former) and a
    language model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: One can optionally pass `input_ids` to the model, which serve as a text prompt,
    to make the language model continue the prompt. Otherwise, the language model
    starts generating text from the [BOS] (beginning-of-sequence) token.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Note that Flan-T5 checkpoints cannot be cast to float16\. They are pre-trained
    using bfloat16.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)'
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-
    åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`Blip2Processor.__call__()`ã€‚'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚è¾“å…¥æ ‡è®°å¯ä»¥é€‰æ‹©ä½œä¸ºæ–‡æœ¬æç¤ºæä¾›ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ã€‚'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`Blip2Processor.__call__()`ã€‚
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0,
    1]`ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºâ€œå±è”½â€çš„æ ‡è®°ä¸º0ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è§£ç å™¨è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰-
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`æˆ–`tuple(torch.FloatTensor)`'
- en: A `transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class
    'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `labels` is provided,
    `torch.FloatTensor` of shape `(1,)`) â€” Language modeling loss from the language
    model.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰-
    è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head of the language model.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œlogitsâ€ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰-
    è¯­è¨€æ¨¡å‹å¤´çš„é¢„æµ‹åˆ†æ•°ã€‚
- en: '`vision_outputs` (`BaseModelOutputWithPooling`) â€” Outputs of the vision encoder.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_outputs`ï¼ˆ`BaseModelOutputWithPooling`ï¼‰- è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºã€‚'
- en: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) â€” Outputs
    of the Q-Former (Querying Transformer).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_outputs`ï¼ˆ`BaseModelOutputWithPoolingAndCrossAttentions`ï¼‰- Q-Formerï¼ˆQuerying
    Transformerï¼‰çš„è¾“å‡ºã€‚'
- en: '`language_model_outputs` (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`) â€”
    Outputs of the language model.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model_outputs`ï¼ˆ`CausalLMOutputWithPast`æˆ–`Seq2SeqLMOutput`ï¼‰- è¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚'
- en: The [Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Examples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: Prepare processor, model and image input
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡å¤„ç†å™¨ã€æ¨¡å‹å’Œå›¾åƒè¾“å…¥
- en: '[PRE25]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Image captioning (without providing a text prompt):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒå­—å¹•ï¼ˆä¸æä¾›æ–‡æœ¬æç¤ºï¼‰ï¼š
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Visual question answering (prompt = question):'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰é—®ç­”ï¼ˆæç¤º=é—®é¢˜ï¼‰ï¼š
- en: '[PRE27]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that int8 inference is also supported through [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
    This greatly reduces the amount of memory used by the model while maintaining
    the same performance.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä¹Ÿæ”¯æŒé€šè¿‡[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)è¿›è¡Œint8æ¨ç†ã€‚è¿™å¤§å¤§å‡å°‘äº†æ¨¡å‹ä½¿ç”¨çš„å†…å­˜é‡ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„æ€§èƒ½ã€‚
- en: '[PRE28]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#### `generate`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)'
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pixel_values` (`torch.FloatTensor` of shape (batch_size, num_channels, height,
    width)) â€” Input images to be processed.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`ï¼ˆå½¢çŠ¶ä¸º(batch_size, num_channels, height, width)çš„`torch.FloatTensor`ï¼‰â€”è¦å¤„ç†çš„è¾“å…¥å›¾åƒã€‚'
- en: '`input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*)
    â€” The sequence used as a prompt for the generation.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`ï¼ˆå½¢çŠ¶ä¸º(batch_size, sequence_length)çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨ä½œç”Ÿæˆæç¤ºçš„åºåˆ—ã€‚'
- en: '`attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) â€” Mask to avoid performing attention on padding token indices'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º(batch_size, sequence_length)çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç '
- en: Returns
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: captions (list)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: å­—å¹•ï¼ˆåˆ—è¡¨ï¼‰
- en: A list of strings of length batch_size * num_captions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé•¿åº¦ä¸ºbatch_size * num_captionsçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: Overrides `generate` function to be able to use the model as a conditional generator.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: è¦†ç›–`generate`å‡½æ•°ä»¥èƒ½å¤Ÿå°†æ¨¡å‹ç”¨ä½œæ¡ä»¶ç”Ÿæˆå™¨ã€‚
