- en: BLIP-2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BLIP-2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training
    with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)
    by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen
    pre-trained image encoders and large language models (LLMs) by training a lightweight,
    12-layer Transformer encoder in between them, achieving state-of-the-art performance
    on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198),
    an 80 billion parameter model, by 8.7% on zero-shot VQAv2 with 54x fewer trainable
    parameters.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'BLIP-2 模型由 Junnan Li、Dongxu Li、Silvio Savarese、Steven Hoi 在[BLIP-2: Bootstrapping
    Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)中提出。BLIP-2
    利用冻结的预训练图像编码器和大型语言模型（LLMs），通过在它们之间训练一个轻量级的 12 层 Transformer 编码器，实现了各种视觉-语言任务的最先进性能。值得注意的是，BLIP-2
    在零样本 VQAv2 上比[Flamingo](https://arxiv.org/abs/2204.14198)（一个 80 亿参数模型）提高了 8.7%，并且可训练参数数量减少了
    54 倍。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*The cost of vision-and-language pre-training has become increasingly prohibitive
    due to end-to-end training of large-scale models. This paper proposes BLIP-2,
    a generic and efficient pre-training strategy that bootstraps vision-language
    pre-training from off-the-shelf frozen pre-trained image encoders and frozen large
    language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer,
    which is pre-trained in two stages. The first stage bootstraps vision-language
    representation learning from a frozen image encoder. The second stage bootstraps
    vision-to-language generative learning from a frozen language model. BLIP-2 achieves
    state-of-the-art performance on various vision-language tasks, despite having
    significantly fewer trainable parameters than existing methods. For example, our
    model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable
    parameters. We also demonstrate the model’s emerging capabilities of zero-shot
    image-to-text generation that can follow natural language instructions.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*由于大规模模型的端到端训练，视觉-语言预训练的成本变得越来越高。本文提出了 BLIP-2，一种通用且高效的预训练策略，从现成的冻结预训练图像编码器和冻结大型语言模型中引导视觉-语言预训练。BLIP-2
    通过轻量级的 Querying Transformer 消除了模态差异，该模型经过两个阶段的预训练。第一阶段从冻结图像编码器引导视觉-语言表示学习。第二阶段从冻结语言模型引导视觉-语言生成学习。尽管可训练参数数量明显少于现有方法，但
    BLIP-2 在各种视觉-语言任务上实现了最先进的性能。例如，我们的模型在零样本 VQAv2 上比 Flamingo80B 提高了 8.7%，并且可训练参数数量减少了
    54 倍。我们还展示了模型的新兴能力，即零样本图像到文本生成，可以遵循自然语言指令。*'
- en: '![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2301.12597)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 架构。摘自[原始论文。](https://arxiv.org/abs/2301.12597)'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The original
    code can be found [here](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207)找到。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: BLIP-2 can be used for conditional text generation given an image and an optional
    text prompt. At inference time, it’s recommended to use the `generate` method.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLIP-2 可用于在给定图像和可选文本提示的情况下进行条件文本生成。在推理时，建议使用 `generate` 方法。
- en: One can use [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)
    to prepare images for the model, and decode the predicted tokens ID’s back to
    text.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)来为模型准备图像，并将预测的标记ID解码回文本。
- en: Resources
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with BLIP-2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 官方 Hugging Face 和社区（由🌎表示）资源列表，帮助您开始使用 BLIP-2。
- en: Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA)
    and chat-like conversations can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLIP-2 的演示笔记本用于图像字幕、视觉问答（VQA）和类似对话的会话，可在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2)找到。
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we’ll review it! The resource should ideally demonstrate
    something new instead of duplicating an existing resource.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。
- en: Blip2Config
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Config
- en: '### `class transformers.Blip2Config`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Config`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vision_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_config` (`dict`, *可选*) — 用于初始化[Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig)的配置选项字典。'
- en: '`qformer_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_config` (`dict`, *可选*) — 用于初始化[Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig)的配置选项字典。'
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize any [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`dict`, *可选*) — 用于初始化任何[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的配置选项字典。'
- en: '`num_query_tokens` (`int`, *optional*, defaults to 32) — The number of query
    tokens passed through the Transformer.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_query_tokens` (`int`, *optional*, defaults to 32) — 通过Transformer传递的查询令牌数量。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 关键字参数的字典。'
- en: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)
    is the configuration class to store the configuration of a [Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration).
    It is used to instantiate a BLIP-2 model according to the specified arguments,
    defining the vision model, Q-Former model and language model configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)是用于存储[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)配置的配置类。根据指定的参数实例化一个BLIP-2模型，定义视觉模型、Q-Former模型和语言模型配置。使用默认配置实例化将产生类似于BLIP-2
    [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    架构的配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#### `from_vision_qformer_text_configs`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_vision_qformer_text_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)'
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Returns
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)'
- en: An instance of a configuration object
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的实例
- en: Instantiate a [Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)
    (or a derived class) from a BLIP-2 vision model, Q-Former and language model configurations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从BLIP-2视觉模型、Q-Former和语言模型配置中实例化一个[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)（或派生类）。
- en: Blip2VisionConfig
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2VisionConfig
- en: '### `class transformers.Blip2VisionConfig`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2VisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)'
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 1408) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 1408) — 编码器层和池化层的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 6144) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 6144) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 39) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 39) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数量。'
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 14) — The size (resolution) of
    each patch.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, defaults to 14) — 每个补丁的大小（分辨率）。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"gelu"` are supported.
    layer_norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon used by the
    layer normalization layers.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`
    `"gelu"`。layer_norm_eps (`float`, *optional*, defaults to 1e-5): 层归一化层使用的epsilon。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries and values in the self-attention layers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否在自注意力层中为查询和值添加偏置。'
- en: This is the configuration class to store the configuration of a [Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel).
    It is used to instantiate a BLIP-2 vision encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration defaults will yield
    a similar configuration to that of the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)配置的配置类。根据指定的参数实例化一个BLIP-2视觉编码器，定义模型架构。实例化默认配置将产生类似于BLIP-2
    [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Blip2QFormerConfig
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2QFormerConfig
- en: '### `class transformers.Blip2QFormerConfig`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2QFormerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    Q-Former model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling the model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 30522) — Q-Former 模型的词汇量。定义在调用模型时传递的
    `inputs_ids` 可表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — 每个注意力层中的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持
    `"gelu"`、`"relu"`、`"silu"` 和 `"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的
    dropout 概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的
    dropout 比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如，512、1024或2048）。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — 位置嵌入的类型。选择
    `"absolute"`、`"relative_key"`、`"relative_key_query"` 中的一个。对于位置嵌入，请使用 `"absolute"`。有关
    `"relative_key"` 的更多信息，请参考[Self-Attention with Relative Position Representations
    (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关 `"relative_key_query"` 的更多信息，请参考[Improve
    Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)
    中的 *Method 4*。'
- en: '`cross_attention_frequency` (`int`, *optional*, defaults to 2) — The frequency
    of adding cross-attention to the Transformer layers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_frequency` (`int`, *optional*, defaults to 2) — 向 Transformer
    层添加交叉注意力的频率。'
- en: '`encoder_hidden_size` (`int`, *optional*, defaults to 1408) — The hidden size
    of the hidden states for cross-attention.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_size` (`int`, *optional*, defaults to 1408) — 交叉注意力中隐藏状态的隐藏大小。'
- en: This is the configuration class to store the configuration of a [Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel).
    It is used to instantiate a BLIP-2 Querying Transformer (Q-Former) model according
    to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    architecture. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)的配置。根据指定的参数实例化一个
    BLIP-2 Querying Transformer (Q-Former) 模型，定义模型架构。使用默认值实例化配置将产生类似于 BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)
    架构的配置。配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: Note that [Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)
    is very similar to [BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)
    with interleaved cross-attention.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)与[BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)非常相似，具有交错的交叉注意力。
- en: 'Examples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Blip2Processor
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Processor
- en: '### `class transformers.Blip2Processor`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Processor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)'
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`BlipImageProcessor`) — An instance of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor).
    The image processor is a required input.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor`（`BlipImageProcessor`）— [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)的一个实例。图像处理器是必需的输入。'
- en: '`tokenizer` (`AutoTokenizer`) — An instance of [‘PreTrainedTokenizer`]. The
    tokenizer is a required input.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`（`AutoTokenizer`）— [‘PreTrainedTokenizer’]的一个实例。分词器是必需的输入。'
- en: Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5
    tokenizer into a single processor.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个BLIP-2处理器，将BLIP图像处理器和OPT/T5分词器封装到一个处理器中。
- en: '[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)
    offers all the functionalities of [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)
    and [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See the docstring of `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)
    for more information.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)提供了[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)的文档字符串。'
- en: '#### `batch_decode`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)'
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 解码
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)'
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This method forwards all its arguments to PreTrainedTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发到PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: Blip2VisionModel
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2VisionModel
- en: '### `class transformers.Blip2VisionModel`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2VisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)'
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#### `forward`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前进
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。像素值可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获得。有关详细信息，请参阅`Blip2Processor.__call__()`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class
    'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）-
    模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（`torch.FloatTensor`，形状为`(batch_size, hidden_size)`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: Blip2QFormerModel
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2QFormerModel
- en: '### `class transformers.Blip2QFormerModel`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2QFormerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Querying Transformer (Q-Former), used in BLIP-2.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 查询变压器（Q-Former），用于BLIP-2。
- en: '#### `forward`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, `optional`): Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder. encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size,
    sequence_length)`, `optional`): Mask to avoid performing attention on the padding
    token indices of the encoder input. This mask is used in the cross-attention if
    the model is configured as a decoder. Mask values selected in `[0, 1]`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: encoder_hidden_states（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，`可选`）：编码器最后一层的输出的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。encoder_attention_mask（`torch.FloatTensor`，形状为`(batch_size,
    sequence_length)`，`可选`）：避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。选择的掩码值在`[0,
    1]`中：
- en: 1 for tokens that are `not masked`,
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被“掩盖”的标记为1，
- en: '0 for tokens that are `masked`. past_key_values (`tuple(tuple(torch.FloatTensor))`
    of length `config.n_layers` with each tuple having 4 tensors of: shape `(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key
    and value hidden states of the attention blocks. Can be used to speed up decoding.
    If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
    use_cache (`bool`, `optional`): If set to `True`, `past_key_values` key value
    states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩盖”的标记为0。past_key_values（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个张量：形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size,
    1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。use_cache（`bool`，`可选`）：如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。
- en: Blip2Model
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Blip2Model
- en: '### `class transformers.Blip2Model`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.Blip2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)'
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: BLIP-2 Model for generating text and image features. The model consists of a
    vision encoder, Querying Transformer (Q-Former) and a language model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成文本和图像特征的BLIP-2模型。该模型由视觉编码器、查询变换器（Q-Former）和语言模型组成。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)'
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 语言模型词汇表中输入序列标记的索引。输入标记可以选择性地提供作为文本提示，语言模型可以继续。'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参阅`Blip2Processor.__call__()`。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 用于“未掩码”标记的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 用于“掩码”标记的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    语言模型词汇表中解码器输入序列标记的索引。仅在使用编码器-解码器语言模型（如T5）时相关。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是解码器输入ID？](../glossary#decoder-input-ids)
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅在使用编码器-解码器语言模型（如T5）时相关。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含各种元素，取决于配置（`<class ''transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig''>`）和输入。'
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `labels` is provided,
    `torch.FloatTensor` of shape `(1,)`) — Language modeling loss from the language
    model.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.FloatTensor`，*可选*，在提供 `labels` 时返回，形状为 `(1,)` 的 `torch.FloatTensor`）—
    语言模型的语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head of the language model.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为 `(batch_size, sequence_length, config.vocab_size)` 的 `torch.FloatTensor`）—
    语言模型的语言建模头的预测分数。'
- en: '`vision_outputs` (`BaseModelOutputWithPooling`) — Outputs of the vision encoder.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_outputs`（`BaseModelOutputWithPooling`）— 视觉编码器的输出。'
- en: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) — Outputs
    of the Q-Former (Querying Transformer).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_outputs`（`BaseModelOutputWithPoolingAndCrossAttentions`）— Q-Former（Querying
    Transformer）的输出。'
- en: '`language_model_outputs` (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`) —
    Outputs of the language model.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model_outputs`（`CausalLMOutputWithPast` 或 `Seq2SeqLMOutput`）— 语言模型的输出。'
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#### `get_text_features`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_text_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)'
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。可以使用
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入
    ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.Tensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `未被掩盖` 的标记为 1，
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `被掩盖` 的标记为 0。[什么是注意力掩码？](../glossary#attention-mask)
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为 `(batch_size, target_sequence_length)` 的 `torch.LongTensor`，*可选*）—
    词汇表中解码器输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是解码器输入 ID？](../glossary#decoder-input-ids)'
- en: T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`).
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: T5 使用 `pad_token_id` 作为 `decoder_input_ids` 生成的起始标记。如果使用了 `past_key_values`，则可选择仅输入最后的
    `decoder_input_ids`（参见 `past_key_values`）。
- en: To know more on how to prepare `decoder_input_ids` for pretraining take a look
    at [T5 Training](./t5#training).
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解有关如何为预训练准备 `decoder_input_ids` 的更多信息，请查看 [T5 Training](./t5#training)。
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为 `(batch_size, target_sequence_length)` 的 `torch.BoolTensor`，*可选*）—
    默认行为：生成一个忽略 `decoder_input_ids` 中填充标记的张量。因果掩码也将默认使用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: text_outputs (`CausalLMOutputWithPast`, or `tuple(torch.FloatTensor)` if `return_dict=False`)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: text_outputs (`CausalLMOutputWithPast`，或者如果`return_dict=False`则为`tuple(torch.FloatTensor)`)
- en: The language model outputs. If `return_dict=True`, the output is a `CausalLMOutputWithPast`
    that contains the language model logits, the past key values and the hidden states
    if `output_hidden_states=True`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型输出。如果`return_dict=True`，则输出是一个包含语言模型logits、过去的键值和隐藏状态（如果`output_hidden_states=True`）的`CausalLMOutputWithPast`。
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `get_image_features`'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_image_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)'
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: vision_outputs (`BaseModelOutputWithPooling`或`torch.FloatTensor`的元组）
- en: The vision model outputs. If `return_dict=True`, the output is a `BaseModelOutputWithPooling`
    that contains the image features, the pooled image features and the hidden states
    if `output_hidden_states=True`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉模型输出。如果`return_dict=True`，则输出是一个包含图像特征、池化图像特征和隐藏状态（如果`output_hidden_states=True`）的`BaseModelOutputWithPooling`。
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `get_qformer_features`'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_qformer_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)'
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 语言模型词汇中输入序列标记的索引。可以提供输入标记作为文本提示，语言模型可以继续生成。'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参阅`Blip2Processor.__call__()`。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`not masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: vision_outputs (`BaseModelOutputWithPooling` or tuple of `torch.FloatTensor`)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The vision model outputs. If `return_dict=True`, the output is a `BaseModelOutputWithPooling`
    that contains the image features, the pooled image features and the hidden states
    if `output_hidden_states=True`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The [Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)
    forward method, overrides the `__call__` special method.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Blip2ForConditionalGeneration
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Blip2ForConditionalGeneration`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1524)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLIP-2 Model for generating text given an image and an optional text prompt.
    The model consists of a vision encoder, Querying Transformer (Q-Former) and a
    language model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: One can optionally pass `input_ids` to the model, which serve as a text prompt,
    to make the language model continue the prompt. Otherwise, the language model
    starts generating text from the [BOS] (beginning-of-sequence) token.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Note that Flan-T5 checkpoints cannot be cast to float16\. They are pre-trained
    using bfloat16.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)'
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-
    像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参见`Blip2Processor.__call__()`。'
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of input sequence tokens in the vocabulary of the language model. Input
    tokens can optionally be provided to serve as text prompt, which the language
    model can continue.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 语言模型词汇表中输入序列标记的索引。输入标记可以选择作为文本提示提供，语言模型可以继续。'
- en: Indices can be obtained using [Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor).
    See `Blip2Processor.__call__()` for details.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参见`Blip2Processor.__call__()`。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入ID是什么？
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“未屏蔽”的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“屏蔽”的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary of the
    language model. Only relevant in case an encoder-decoder language model (like
    T5) is used.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    解码器输入序列标记在语言模型词汇表中的索引。仅在使用编码器-解码器语言模型（如T5）时相关。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids)
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。解码器输入ID是什么？
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）-
    默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: Only relevant in case an encoder-decoder language model (like T5) is used.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅在使用编码器-解码器语言模型（如T5）时相关。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`)
    and inputs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class
    'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `labels` is provided,
    `torch.FloatTensor` of shape `(1,)`) — Language modeling loss from the language
    model.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（`torch.FloatTensor`，*可选*，在提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`）-
    语言模型的语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head of the language model.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “logits”（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）-
    语言模型头的预测分数。
- en: '`vision_outputs` (`BaseModelOutputWithPooling`) — Outputs of the vision encoder.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_outputs`（`BaseModelOutputWithPooling`）- 视觉编码器的输出。'
- en: '`qformer_outputs` (`BaseModelOutputWithPoolingAndCrossAttentions`) — Outputs
    of the Q-Former (Querying Transformer).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qformer_outputs`（`BaseModelOutputWithPoolingAndCrossAttentions`）- Q-Former（Querying
    Transformer）的输出。'
- en: '`language_model_outputs` (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`) —
    Outputs of the language model.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language_model_outputs`（`CausalLMOutputWithPast`或`Seq2SeqLMOutput`）- 语言模型的输出。'
- en: The [Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: Prepare processor, model and image input
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 准备处理器、模型和图像输入
- en: '[PRE25]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Image captioning (without providing a text prompt):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字幕（不提供文本提示）：
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Visual question answering (prompt = question):'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉问答（提示=问题）：
- en: '[PRE27]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that int8 inference is also supported through [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
    This greatly reduces the amount of memory used by the model while maintaining
    the same performance.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，也支持通过[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)进行int8推理。这大大减少了模型使用的内存量，同时保持相同的性能。
- en: '[PRE28]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#### `generate`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)'
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape (batch_size, num_channels, height,
    width)) — Input images to be processed.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为(batch_size, num_channels, height, width)的`torch.FloatTensor`）—要处理的输入图像。'
- en: '`input_ids` (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*)
    — The sequence used as a prompt for the generation.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为(batch_size, sequence_length)的`torch.LongTensor`，*可选*）—用作生成提示的序列。'
- en: '`attention_mask` (`torch.LongTensor` of shape (batch_size, sequence_length),
    *optional*) — Mask to avoid performing attention on padding token indices'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为(batch_size, sequence_length)的`torch.LongTensor`，*可选*）—避免在填充标记索引上执行注意力的掩码'
- en: Returns
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: captions (list)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 字幕（列表）
- en: A list of strings of length batch_size * num_captions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长度为batch_size * num_captions的字符串列表。
- en: Overrides `generate` function to be able to use the model as a conditional generator.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`generate`函数以能够将模型用作条件生成器。
