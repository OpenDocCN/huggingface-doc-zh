# ä»HubåŠ è½½æ•°æ®é›†

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/datasets/load_hub](https://huggingface.co/docs/datasets/load_hub)

æŸ¥æ‰¾å¯é‡ç°ä¸”å¯è®¿é—®çš„é«˜è´¨é‡æ•°æ®é›†å¯èƒ½å¾ˆå›°éš¾ã€‚ğŸ¤— æ•°æ®é›†çš„ä¸»è¦ç›®æ ‡ä¹‹ä¸€æ˜¯æä¾›ä¸€ç§ç®€å•çš„æ–¹å¼æ¥åŠ è½½ä»»ä½•æ ¼å¼æˆ–ç±»å‹çš„æ•°æ®é›†ã€‚å¼€å§‹çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[Hugging Face Hub](https://huggingface.co/datasets)ä¸Šå‘ç°ç°æœ‰æ•°æ®é›† - è¿™æ˜¯ä¸€ä¸ªç¤¾åŒºé©±åŠ¨çš„æ•°æ®é›†é›†åˆï¼Œç”¨äºNLPã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘ä»»åŠ¡ - å¹¶ä½¿ç”¨ğŸ¤— æ•°æ®é›†ä¸‹è½½å’Œç”Ÿæˆæ•°æ®é›†ã€‚

æœ¬æ•™ç¨‹ä½¿ç”¨[rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes)å’Œ[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼Œä½†è¯·éšæ„åŠ è½½æ‚¨æƒ³è¦çš„ä»»ä½•æ•°æ®é›†å¹¶è·Ÿéšæ“ä½œã€‚ç«‹å³å‰å¾€Hubå¹¶æ‰¾åˆ°é€‚åˆæ‚¨ä»»åŠ¡çš„æ•°æ®é›†ï¼

## åŠ è½½æ•°æ®é›†

åœ¨èŠ±æ—¶é—´ä¸‹è½½æ•°æ®é›†ä¹‹å‰ï¼Œå¿«é€Ÿè·å–æœ‰å…³æ•°æ®é›†çš„ä¸€èˆ¬ä¿¡æ¯é€šå¸¸æ˜¯æœ‰å¸®åŠ©çš„ã€‚æ•°æ®é›†çš„ä¿¡æ¯å­˜å‚¨åœ¨[DatasetInfo](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetInfo)ä¸­ï¼Œå¯ä»¥åŒ…æ‹¬æ•°æ®é›†æè¿°ã€ç‰¹å¾å’Œæ•°æ®é›†å¤§å°ç­‰ä¿¡æ¯ã€‚

ä½¿ç”¨[load_dataset_builder()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset_builder)å‡½æ•°åŠ è½½æ•°æ®é›†æ„å»ºå™¨å¹¶æ£€æŸ¥æ•°æ®é›†çš„å±æ€§ï¼Œè€Œä¸å¿…ä¸‹è½½å®ƒï¼š

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder("rotten_tomatoes")

# Inspect dataset description
>>> ds_builder.info.description
Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.

# Inspect dataset features
>>> ds_builder.info.features
{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),
 'text': Value(dtype='string', id=None)}
```

å¦‚æœæ‚¨å¯¹æ•°æ®é›†æ»¡æ„ï¼Œè¯·ä½¿ç”¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)åŠ è½½å®ƒï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("rotten_tomatoes", split="train")
```

## æ‹†åˆ†

æ‹†åˆ†æ˜¯æ•°æ®é›†çš„ç‰¹å®šå­é›†ï¼Œå¦‚`train`å’Œ`test`ã€‚ä½¿ç”¨[get_dataset_split_names()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.get_dataset_split_names)å‡½æ•°åˆ—å‡ºæ•°æ®é›†çš„æ‹†åˆ†åç§°ï¼š

```py
>>> from datasets import get_dataset_split_names

>>> get_dataset_split_names("rotten_tomatoes")
['train', 'validation', 'test']
```

ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨`split`å‚æ•°åŠ è½½ç‰¹å®šçš„æ‹†åˆ†ã€‚åŠ è½½æ•°æ®é›†`split`å°†è¿”å›ä¸€ä¸ª[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("rotten_tomatoes", split="train")
>>> dataset
Dataset({
    features: ['text', 'label'],
    num_rows: 8530
})
```

å¦‚æœæ‚¨æ²¡æœ‰æŒ‡å®š`split`ï¼ŒğŸ¤— æ•°æ®é›†å°†è¿”å›ä¸€ä¸ª[DatasetDict](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict)å¯¹è±¡ï¼š

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("rotten_tomatoes")
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 8530
    })
    validation: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 1066
    })
})
```

## é…ç½®

ä¸€äº›æ•°æ®é›†åŒ…å«å¤šä¸ªå­æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œ[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†æœ‰å¤šä¸ªå­æ•°æ®é›†ï¼Œæ¯ä¸ªå­æ•°æ®é›†åŒ…å«ä¸åŒè¯­è¨€çš„éŸ³é¢‘æ•°æ®ã€‚è¿™äº›å­æ•°æ®é›†ç§°ä¸º*é…ç½®*ï¼Œåœ¨åŠ è½½æ•°æ®é›†æ—¶å¿…é¡»æ˜ç¡®é€‰æ‹©ä¸€ä¸ªã€‚å¦‚æœæ‚¨æ²¡æœ‰æä¾›é…ç½®åç§°ï¼ŒğŸ¤— æ•°æ®é›†å°†å¼•å‘`ValueError`å¹¶æé†’æ‚¨é€‰æ‹©ä¸€ä¸ªé…ç½®ã€‚

ä½¿ç”¨[get_dataset_config_names()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.get_dataset_config_names)å‡½æ•°æ£€ç´¢æ•°æ®é›†å¯ç”¨çš„æ‰€æœ‰å¯èƒ½é…ç½®çš„åˆ—è¡¨ï¼š

```py
>>> from datasets import get_dataset_config_names

>>> configs = get_dataset_config_names("PolyAI/minds14")
>>> print(configs)
['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', 'all']
```

ç„¶ååŠ è½½æ‚¨æƒ³è¦çš„é…ç½®ï¼š

```py
>>> from datasets import load_dataset

>>> mindsFR = load_dataset("PolyAI/minds14", "fr-FR", split="train")
```

## è¿œç¨‹ä»£ç 

æŸäº›æ•°æ®é›†å­˜å‚¨åº“åŒ…å«ç”¨äºç”Ÿæˆæ•°æ®é›†çš„Pythonä»£ç çš„åŠ è½½è„šæœ¬ã€‚è¿™äº›æ•°æ®é›†é€šå¸¸ç”±Hugging Faceå¯¼å‡ºä¸ºParquetï¼Œä»¥ä¾¿ğŸ¤— æ•°æ®é›†å¯ä»¥å¿«é€ŸåŠ è½½æ•°æ®é›†è€Œæ— éœ€è¿è¡ŒåŠ è½½è„šæœ¬ã€‚

å³ä½¿Parquetå¯¼å‡ºä¸å¯ç”¨ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨`load_dataset`ä¸­çš„Pythonä»£ç åœ¨å…¶å­˜å‚¨åº“ä¸­ä½¿ç”¨ä»»ä½•æ•°æ®é›†ã€‚æ‰€æœ‰ä¸Šä¼ åˆ°Hubçš„æ–‡ä»¶å’Œä»£ç éƒ½ä¼šè¢«æ‰«æä»¥æ£€æµ‹æ¶æ„è½¯ä»¶ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…Hubå®‰å…¨æ–‡æ¡£ï¼‰ï¼Œä½†æ‚¨ä»åº”æŸ¥çœ‹æ•°æ®é›†åŠ è½½è„šæœ¬å’Œä½œè€…ï¼Œä»¥é¿å…åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šæ‰§è¡Œæ¶æ„ä»£ç ã€‚æ‚¨åº”è¯¥è®¾ç½®`trust_remote_code=True`ä»¥ä½¿ç”¨å¸¦æœ‰åŠ è½½è„šæœ¬çš„æ•°æ®é›†ï¼Œå¦åˆ™æ‚¨å°†æ”¶åˆ°è­¦å‘Šï¼š

```py
>>> from datasets import get_dataset_config_names, get_dataset_split_names, load_dataset

>>> c4 = load_dataset("c4", "en", split="train", trust_remote_code=True)
>>> get_dataset_config_names("c4", trust_remote_code=True)
['en', 'realnewslike', 'en.noblocklist', 'en.noclean']
>>> get_dataset_split_names("c4", "en", trust_remote_code=True)
['train', 'validation']
```

åœ¨ä¸‹ä¸€ä¸ªä¸»è¦ç‰ˆæœ¬ä¸­ï¼ŒğŸ¤— æ•°æ®é›†çš„æ–°å®‰å…¨åŠŸèƒ½å°†é»˜è®¤ç¦ç”¨è¿è¡Œæ•°æ®é›†åŠ è½½è„šæœ¬ï¼Œå¹¶ä¸”æ‚¨å¿…é¡»ä¼ é€’`trust_remote_code=True`ä»¥åŠ è½½éœ€è¦è¿è¡Œæ•°æ®é›†è„šæœ¬çš„æ•°æ®é›†ã€‚
