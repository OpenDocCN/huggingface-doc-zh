- en: XLM-V
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLM-V
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'XLM-V is multilingual language model with a one million token vocabulary trained
    on 2.5TB of data from Common Crawl (same as XLM-R). It was introduced in the [XLM-V:
    Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)
    paper by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad,
    Luke Zettlemoyer and Madian Khabsa.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'XLM-V是一个多语言语言模型，具有一个由Common Crawl的2.5TB数据训练的一百万标记词汇（与XLM-R相同）。它在Davis Liang、Hila
    Gonen、Yuning Mao、Rui Hou、Naman Goyal、Marjan Ghazvininejad、Luke Zettlemoyer和Madian
    Khabsa的论文[XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language
    Models](https://arxiv.org/abs/2301.10472)中介绍。'
- en: 'From the abstract of the XLM-V paper:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从XLM-V论文的摘要中：
- en: '*Large multilingual language models typically rely on a single vocabulary shared
    across 100+ languages. As these models have increased in parameter count and depth,
    vocabulary size has remained largely unchanged. This vocabulary bottleneck limits
    the representational capabilities of multilingual models like XLM-R. In this paper,
    we introduce a new approach for scaling to very large multilingual vocabularies
    by de-emphasizing token sharing between languages with little lexical overlap
    and assigning vocabulary capacity to achieve sufficient coverage for each individual
    language. Tokenizations using our vocabulary are typically more semantically meaningful
    and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V,
    a multilingual language model with a one million token vocabulary. XLM-V outperforms
    XLM-R on every task we tested on ranging from natural language inference (XNLI),
    question answering (MLQA, XQuAD, TyDiQA), and named entity recognition (WikiAnn)
    to low-resource tasks (Americas NLI, MasakhaNER).*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*大型多语言语言模型通常依赖于跨100多种语言共享的单一词汇表。随着这些模型的参数数量和深度增加，词汇表大小基本保持不变。这种词汇瓶颈限制了诸如XLM-R之类的多语言模型的表征能力。在本文中，我们介绍了一种通过减少语言之间的标记共享并分配词汇容量来实现对非常大型多语言词汇的扩展的新方法，以实现对每种单独语言的足够覆盖。使用我们的词汇表进行标记化通常比XLM-R更具语义意义且更短。利用这个改进的词汇表，我们训练了XLM-V，一个具有一百万标记词汇的多语言语言模型。XLM-V在我们测试的每个任务上都优于XLM-R，包括自然语言推理（XNLI）、问答（MLQA、XQuAD、TyDiQA）和命名实体识别（WikiAnn）等低资源任务（Americas
    NLI、MasakhaNER）。*'
- en: This model was contributed by [stefan-it](https://huggingface.co/stefan-it),
    including detailed experiments with XLM-V on downstream tasks. The experiments
    repository can be found [here](https://github.com/stefan-it/xlm-v-experiments).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[stefan-it](https://huggingface.co/stefan-it)贡献，包括对XLM-V在下游任务上的详细实验。实验存储库可以在[这里](https://github.com/stefan-it/xlm-v-experiments)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights
    from [`fairseq`](https://github.com/facebookresearch/fairseq) library had to be
    converted.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLM-V与XLM-RoBERTa模型架构兼容，只需将模型权重从[`fairseq`](https://github.com/facebookresearch/fairseq)库转换即可。
- en: The `XLMTokenizer` implementation is used to load the vocab and performs tokenization.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XLMTokenizer`实现用于加载词汇表并执行标记化。'
- en: A XLM-V (base size) model is available under the [`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base)
    identifier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-V（基础大小）模型可在[`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base)标识符下找到。
- en: XLM-V architecture is the same as XLM-RoBERTa, refer to [XLM-RoBERTa documentation](xlm-roberta)
    for API reference, and examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: XLM-V架构与XLM-RoBERTa相同，请参考[XLM-RoBERTa文档](xlm-roberta)以获取API参考和示例。
