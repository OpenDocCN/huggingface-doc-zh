- en: XLM-V
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xlm-v)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'XLM-V is multilingual language model with a one million token vocabulary trained
    on 2.5TB of data from Common Crawl (same as XLM-R). It was introduced in the [XLM-V:
    Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)
    paper by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad,
    Luke Zettlemoyer and Madian Khabsa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the abstract of the XLM-V paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large multilingual language models typically rely on a single vocabulary shared
    across 100+ languages. As these models have increased in parameter count and depth,
    vocabulary size has remained largely unchanged. This vocabulary bottleneck limits
    the representational capabilities of multilingual models like XLM-R. In this paper,
    we introduce a new approach for scaling to very large multilingual vocabularies
    by de-emphasizing token sharing between languages with little lexical overlap
    and assigning vocabulary capacity to achieve sufficient coverage for each individual
    language. Tokenizations using our vocabulary are typically more semantically meaningful
    and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V,
    a multilingual language model with a one million token vocabulary. XLM-V outperforms
    XLM-R on every task we tested on ranging from natural language inference (XNLI),
    question answering (MLQA, XQuAD, TyDiQA), and named entity recognition (WikiAnn)
    to low-resource tasks (Americas NLI, MasakhaNER).*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [stefan-it](https://huggingface.co/stefan-it),
    including detailed experiments with XLM-V on downstream tasks. The experiments
    repository can be found [here](https://github.com/stefan-it/xlm-v-experiments).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights
    from [`fairseq`](https://github.com/facebookresearch/fairseq) library had to be
    converted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `XLMTokenizer` implementation is used to load the vocab and performs tokenization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A XLM-V (base size) model is available under the [`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base)
    identifier.
  prefs: []
  type: TYPE_NORMAL
- en: XLM-V architecture is the same as XLM-RoBERTa, refer to [XLM-RoBERTa documentation](xlm-roberta)
    for API reference, and examples.
  prefs: []
  type: TYPE_NORMAL
