["```py\n>>> from transformers import ElectraConfig, ElectraModel\n\n>>> # Initializing a ELECTRA electra-base-uncased style configuration\n>>> configuration = ElectraConfig()\n\n>>> # Initializing a model (with random weights) from the electra-base-uncased style configuration\n>>> model = ElectraModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import ElectraForPreTraining, AutoTokenizer\n>>> import torch\n\n>>> discriminator = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-discriminator\")\n\n>>> sentence = \"The quick brown fox jumps over the lazy dog\"\n>>> fake_sentence = \"The quick brown fox fake over the lazy dog\"\n\n>>> fake_tokens = tokenizer.tokenize(fake_sentence, add_special_tokens=True)\n>>> fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\n>>> discriminator_outputs = discriminator(fake_inputs)\n>>> predictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n\n>>> fake_tokens\n['[CLS]', 'the', 'quick', 'brown', 'fox', 'fake', 'over', 'the', 'lazy', 'dog', '[SEP]']\n\n>>> predictions.squeeze().tolist()\n[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraForCausalLM, ElectraConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-base-generator\")\n>>> config = ElectraConfig.from_pretrained(\"google/electra-base-generator\")\n>>> config.is_decoder = True\n>>> model = ElectraForCausalLM.from_pretrained(\"google/electra-base-generator\", config=config)\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraForMaskedLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n>>> model = ElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # retrieve index of [MASK]\n>>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n\n>>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n>>> tokenizer.decode(predicted_token_id)\n'paris'\n\n>>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n>>> # mask labels of non-[MASK] tokens\n>>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n\n>>> outputs = model(**inputs, labels=labels)\n>>> round(outputs.loss.item(), 2)\n1.22\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ElectraForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n>>> model = ElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'joy'\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = ElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.06\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, ElectraForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n>>> model = ElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = ElectraForSequenceClassification.from_pretrained(\n...     \"bhadresh-savani/electra-base-emotion\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = ElectraForMultipleChoice.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\")\n>>> model = ElectraForTokenClassification.from_pretrained(\"bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes\n['B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC']\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.11\n```", "```py\n>>> from transformers import AutoTokenizer, ElectraForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-squad2\")\n>>> model = ElectraForQuestionAnswering.from_pretrained(\"bhadresh-savani/electra-base-squad2\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n'a nice puppet'\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([11])\n>>> target_end_index = torch.tensor([12])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n2.64\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = TFElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, TFElectraForPreTraining\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = TFElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")\n>>> input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\"))[None, :]  # Batch size 1\n>>> outputs = model(input_ids)\n>>> scores = outputs[0]\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraForMaskedLM\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n>>> model = TFElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"tf\")\n>>> logits = model(**inputs).logits\n\n>>> # retrieve index of [MASK]\n>>> mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])\n>>> selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n\n>>> predicted_token_id = tf.math.argmax(selected_logits, axis=-1)\n>>> tokenizer.decode(predicted_token_id)\n'paris'\n```", "```py\n>>> labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"tf\")[\"input_ids\"]\n>>> # mask labels of non-[MASK] tokens\n>>> labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n\n>>> outputs = model(**inputs, labels=labels)\n>>> round(float(outputs.loss), 2)\n1.22\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraForSequenceClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n>>> model = TFElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n\n>>> logits = model(**inputs).logits\n\n>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n>>> model.config.id2label[predicted_class_id]\n'joy'\n```", "```py\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = TFElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\", num_labels=num_labels)\n\n>>> labels = tf.constant(1)\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(float(loss), 2)\n0.06\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraForMultipleChoice\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = TFElectraForMultipleChoice.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"tf\", padding=True)\n>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}\n>>> outputs = model(inputs)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraForTokenClassification\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\")\n>>> model = TFElectraForTokenClassification.from_pretrained(\"bhadresh-savani/electra-base-discriminator-finetuned-conll03-english\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"tf\"\n... )\n\n>>> logits = model(**inputs).logits\n>>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n>>> predicted_tokens_classes\n['B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC']\n```", "```py\n>>> labels = predicted_token_class_ids\n>>> loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)\n>>> round(float(loss), 2)\n0.11\n```", "```py\n>>> from transformers import AutoTokenizer, TFElectraForQuestionAnswering\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bhadresh-savani/electra-base-squad2\")\n>>> model = TFElectraForQuestionAnswering.from_pretrained(\"bhadresh-savani/electra-base-squad2\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n>>> outputs = model(**inputs)\n\n>>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n>>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens)\n'a nice puppet'\n```", "```py\n>>> # target is \"nice puppet\"\n>>> target_start_index = tf.constant([11])\n>>> target_end_index = tf.constant([12])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = tf.math.reduce_mean(outputs.loss)\n>>> round(float(loss), 2)\n2.64\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForPreTraining\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForCausalLM.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForMaskedLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForMaskedLM.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForMultipleChoice\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForMultipleChoice.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"jax\", padding=True)\n>>> outputs = model(**{k: v[None, :] for k, v in encoding.items()})\n\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForTokenClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForTokenClassification.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxElectraForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n>>> model = FlaxElectraForQuestionAnswering.from_pretrained(\"google/electra-small-discriminator\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n>>> inputs = tokenizer(question, text, return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n```"]