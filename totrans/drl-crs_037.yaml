- en: Q-Learning Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/32.0387b092.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: '*Q-Learning* **is the RL algorithm that** :'
  prefs: []
  type: TYPE_NORMAL
- en: Trains a *Q-function*, an **action-value function** encoded, in internal memory,
    by a *Q-table* **containing all the state-action pair values.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a state and action, our Q-function **will search its Q-table for the corresponding
    value.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Q function](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
  prefs: []
  type: TYPE_IMG
- en: When the training is done, **we have an optimal Q-function, or, equivalently,
    an optimal Q-table.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if we **have an optimal Q-function**, we have an optimal policy, since we
    **know, for each state, the best action to take.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  prefs: []
  type: TYPE_IMG
- en: But, in the beginning, our **Q-table is useless since it gives arbitrary values
    for each state-action pair (most of the time we initialize the Q-table to 0 values)**.
    But, as we explore the environment and update our Q-table it will give us a better
    and better approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the Q-Learning pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  prefs: []
  type: TYPE_IMG
