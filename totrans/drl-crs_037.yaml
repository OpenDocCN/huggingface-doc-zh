- en: Q-Learning Recap
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-Learning回顾
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-recap)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '*Q-Learning* **is the RL algorithm that** :'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q-Learning* **是强化学习算法**：'
- en: Trains a *Q-function*, an **action-value function** encoded, in internal memory,
    by a *Q-table* **containing all the state-action pair values.**
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个*Q函数*，一个**动作值函数**，在内部内存中，由一个包含所有状态-动作对值的*Q表*编码。
- en: Given a state and action, our Q-function **will search its Q-table for the corresponding
    value.**
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个状态和动作，我们的Q函数**将在其Q表中搜索相应的值。**
- en: '![Q function](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![Q函数](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
- en: When the training is done, **we have an optimal Q-function, or, equivalently,
    an optimal Q-table.**
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练完成时，**我们有一个最优的Q函数，或者等效地，一个最优的Q表。**
- en: And if we **have an optimal Q-function**, we have an optimal policy, since we
    **know, for each state, the best action to take.**
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们**有一个最优的Q函数**，我们就有一个最优的策略，因为我们**知道，对于每个状态，要采取的最佳动作是什么。**
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![链接值策略](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
- en: But, in the beginning, our **Q-table is useless since it gives arbitrary values
    for each state-action pair (most of the time we initialize the Q-table to 0 values)**.
    But, as we explore the environment and update our Q-table it will give us a better
    and better approximation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在开始时，我们的**Q表是无用的，因为它为每个状态-动作对给出任意值（大多数情况下，我们将Q表初始化为0值）**。但是，当我们探索环境并更新我们的Q表时，它将给我们一个越来越好的近似值。
- en: '![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)'
- en: 'This is the Q-Learning pseudocode:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Q-Learning的伪代码：
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
