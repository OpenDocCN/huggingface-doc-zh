["```py\nmy_dataset/\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 my_dataset.py\n```", "```py\n>>> from datasets import load_dataset\n>>> load_dataset(\"path/to/my_dataset\")\n```", "```py\ndatasets.Features(\n    {\n        \"id\": datasets.Value(\"string\"),\n        \"title\": datasets.Value(\"string\"),\n        \"context\": datasets.Value(\"string\"),\n        \"question\": datasets.Value(\"string\"),\n        \"answers\": datasets.Sequence(\n            {\n                \"text\": datasets.Value(\"string\"),\n                \"answer_start\": datasets.Value(\"int32\"),\n            }\n        ),\n    }\n)\n```", "```py\ndef _info(self):\n    return datasets.DatasetInfo(\n        description=_DESCRIPTION,\n        features=datasets.Features(\n            {\n                \"id\": datasets.Value(\"string\"),\n                \"title\": datasets.Value(\"string\"),\n                \"context\": datasets.Value(\"string\"),\n                \"question\": datasets.Value(\"string\"),\n                \"answers\": datasets.features.Sequence(\n                    {\"text\": datasets.Value(\"string\"), \"answer_start\": datasets.Value(\"int32\"),}\n                ),\n            }\n        ),\n        # No default supervised_keys (as we have to pass both question\n        # and context as input).\n        supervised_keys=None,\n        homepage=\"https://rajpurkar.github.io/SQuAD-explorer/\",\n        citation=_CITATION,\n    )\n```", "```py\nclass SuperGlueConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for SuperGLUE.\"\"\"\n\n    def __init__(self, features, data_url, citation, url, label_classes=(\"False\", \"True\"), **kwargs):\n        \"\"\"BuilderConfig for SuperGLUE.\n\n        Args:\n        features: *list[string]*, list of the features that will appear in the\n            feature dict. Should not include \"label\".\n        data_url: *string*, url to download the zip file from.\n        citation: *string*, citation for the data set.\n        url: *string*, url for information about the data set.\n        label_classes: *list[string]*, the list of classes for the label if the\n            label is present as a string. Non-string labels will be cast to either\n            'False' or 'True'.\n        **kwargs: keyword arguments forwarded to super.\n        \"\"\"\n        # Version history:\n        # 1.0.2: Fixed non-nondeterminism in ReCoRD.\n        # 1.0.1: Change from the pre-release trial version of SuperGLUE (v1.9) to\n        #        the full release (v2.0).\n        # 1.0.0: S3 (new shuffling, sharding and slicing mechanism).\n        # 0.0.2: Initial version.\n        super().__init__(version=datasets.Version(\"1.0.2\"), **kwargs)\n        self.features = features\n        self.label_classes = label_classes\n        self.data_url = data_url\n        self.citation = citation\n        self.url = url\n```", "```py\nclass SuperGlue(datasets.GeneratorBasedBuilder):\n    \"\"\"The SuperGLUE benchmark.\"\"\"\n\n    BUILDER_CONFIG_CLASS = SuperGlueConfig\n\n    BUILDER_CONFIGS = [\n        SuperGlueConfig(\n            name=\"boolq\",\n            description=_BOOLQ_DESCRIPTION,\n            features=[\"question\", \"passage\"],\n            data_url=\"https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\",\n            citation=_BOOLQ_CITATION,\n            url=\"https://github.com/google-research-datasets/boolean-questions\",\n        ),\n        ...\n        ...\n        SuperGlueConfig(\n            name=\"axg\",\n            description=_AXG_DESCRIPTION,\n            features=[\"premise\", \"hypothesis\"],\n            label_classes=[\"entailment\", \"not_entailment\"],\n            data_url=\"https://dl.fbaipublicfiles.com/glue/superglue/data/v2/AX-g.zip\",\n            citation=_AXG_CITATION,\n            url=\"https://github.com/rudinger/winogender-schemas\",\n        ),\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('super_glue', 'boolq')\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('super_glue', data_url=\"https://custom_url\")\n```", "```py\nclass NewDataset(datasets.GeneratorBasedBuilder):\n\nVERSION = datasets.Version(\"1.1.0\")\n\nBUILDER_CONFIGS = [\n    datasets.BuilderConfig(name=\"first_domain\", version=VERSION, description=\"This part of my dataset covers a first domain\"),\n    datasets.BuilderConfig(name=\"second_domain\", version=VERSION, description=\"This part of my dataset covers a second domain\"),\n]\n\nDEFAULT_CONFIG_NAME = \"first_domain\"\n```", "```py\n_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n_URLS = {\n    \"train\": _URL + \"train-v1.1.json\",\n    \"dev\": _URL + \"dev-v1.1.json\",\n}\n```", "```py\ndef _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:\n    urls_to_download = self._URLS\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\n\n    return [\n        datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": downloaded_files[\"train\"]}),\n        datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": downloaded_files[\"dev\"]}),\n    ]\n```", "```py\ndef _generate_examples(self, filepath):\n    \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n    logger.info(\"generating examples from = %s\", filepath)\n    with open(filepath) as f:\n        squad = json.load(f)\n        for article in squad[\"data\"]:\n            title = article.get(\"title\", \"\").strip()\n            for paragraph in article[\"paragraphs\"]:\n                context = paragraph[\"context\"].strip()\n                for qa in paragraph[\"qas\"]:\n                    question = qa[\"question\"].strip()\n                    id_ = qa[\"id\"]\n\n                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n\n                    # Features currently used are \"context\", \"question\", and \"answers\".\n                    # Others are extracted here for the ease of future expansions.\n                    yield id_, {\n                        \"title\": title,\n                        \"context\": context,\n                        \"question\": question,\n                        \"id\": id_,\n                        \"answers\": {\"answer_start\": answer_starts, \"text\": answers,},\n                    }\n```", "```py\ndatasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs\n```", "```py\n>>> from datasets import load_dataset\n>>> load_dataset(\"<username>/my_dataset\")\n```", "```py\n\nclass MyShardedDataset(datasets.GeneratorBasedBuilder):\n\n    def _split_generators(self, dl_manager: datasets.DownloadManager) -> List[datasets.SplitGenerator]:\n        downloaded_files = dl_manager.download([f\"data/shard_{i}.jsonl\" for i in range(1024)])\n        return [\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepaths\": downloaded_files}),\n        ]\n\n    def _generate_examples(self, filepaths):\n        # Each worker can be given a slice of the original `filepaths` list defined in the `gen_kwargs`\n        # so that this code can run in parallel on several shards at the same time\n        for filepath in filepaths:\n            ...\n```", "```py\nimport pyarrow as pa\npa_table = pa.Table.from_pandas(df)\n```", "```py\nclass MySuperFastDataset(datasets.ArrowBasedBuilder):\n\n    def _generate_tables(self, filepaths):\n        idx = 0\n        for filepath in filepaths:\n            ...\n            yield idx, pa_table\n            idx += 1\n```"]