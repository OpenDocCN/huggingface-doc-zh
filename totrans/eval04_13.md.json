["```py\npip install datasets transformers torch evaluate nltk rouge_score\n```", "```py\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\n\n# Prepare and tokenize dataset\ndataset = load_dataset(\"yelp_review_full\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))\n\n# Setup evaluation \nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Load pretrained model and evaluate model after each epoch\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```", "```py\nimport nltk\nfrom datasets import load_dataset\nimport evaluate\nimport numpy as np\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n\n# Prepare and tokenize dataset\nbillsum = load_dataset(\"billsum\", split=\"ca_test\").shuffle(seed=42).select(range(200))\nbillsum = billsum.train_test_split(test_size=0.2)\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\nprefix = \"summarize: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"text\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n\n    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_billsum = billsum.map(preprocess_function, batched=True)\n\n# Setup evaluation\nnltk.download(\"punkt\", quiet=True)\nmetric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # rougeLSum expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return result\n\n# Load pretrained model and evaluate model after each epoch\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=2,\n    fp16=True,\n    predict_with_generate=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_billsum[\"train\"],\n    eval_dataset=tokenized_billsum[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n```"]