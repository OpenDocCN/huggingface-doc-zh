["```py\n( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )\n```", "```py\n( split: Optional = None run_post_process = True verification_mode: Union = None ignore_verifications = 'deprecated' in_memory = False )\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> builder = load_dataset_builder('rotten_tomatoes')\n>>> builder.download_and_prepare()\n>>> ds = builder.as_dataset(split='train')\n>>> ds\nDataset({\n    features: ['text', 'label'],\n    num_rows: 8530\n})\n```", "```py\n( output_dir: Optional = None download_config: Optional = None download_mode: Union = None verification_mode: Union = None ignore_verifications = 'deprecated' try_from_hf_gcs: bool = True dl_manager: Optional = None base_path: Optional = None use_auth_token = 'deprecated' file_format: str = 'arrow' max_shard_size: Union = None num_proc: Optional = None storage_options: Optional = None **download_and_prepare_kwargs )\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> builder = load_dataset_builder(\"rotten_tomatoes\")\n>>> builder.download_and_prepare()\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> builder = load_dataset_builder(\"rotten_tomatoes\")\n>>> builder.download_and_prepare(\"./output_dir\", file_format=\"parquet\")\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> storage_options = {\"key\": aws_access_key_id, \"secret\": aws_secret_access_key}\n>>> builder = load_dataset_builder(\"rotten_tomatoes\")\n>>> builder.download_and_prepare(\"s3://my-bucket/my_rotten_tomatoes\", storage_options=storage_options, file_format=\"parquet\")\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder('rotten_tomatoes')\n>>> ds_builder.get_all_exported_dataset_infos()\n{'default': DatasetInfo(description=\"Movie Review Dataset.\na dataset of containing 5,331 positive and 5,331 negative processed\ns from Rotten Tomatoes movie reviews. This data was first used in Bo\n Lillian Lee, ``Seeing stars: Exploiting class relationships for\nt categorization with respect to rating scales.'', Proceedings of the\n5.\nion='@InProceedings{Pang+Lee:05a,\n =       {Bo Pang and Lillian Lee},\n=        {Seeing stars: Exploiting class relationships for sentiment\n          categorization with respect to rating scales},\ntle =    {Proceedings of the ACL},\n         2005\n\nage='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}\n```", "```py\n( )\n```", "```py\n>>> from datasets import load_dataset_builder\n>>> ds_builder = load_dataset_builder('rotten_tomatoes')\n>>> ds_builder.get_exported_dataset_info()\nDatasetInfo(description=\"Movie Review Dataset.\na dataset of containing 5,331 positive and 5,331 negative processed\ns from Rotten Tomatoes movie reviews. This data was first used in Bo\n Lillian Lee, ``Seeing stars: Exploiting class relationships for\nt categorization with respect to rating scales.'', Proceedings of the\n5.\nion='@InProceedings{Pang+Lee:05a,\n =       {Bo Pang and Lillian Lee},\n=        {Seeing stars: Exploiting class relationships for sentiment\n          categorization with respect to rating scales},\ntle =    {Proceedings of the ACL},\n         2005\n\nage='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)\n```", "```py\n( )\n```", "```py\n( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )\n```", "```py\n( *args beam_runner = None beam_options = None **kwargs )\n```", "```py\n( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )\n```", "```py\n( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None )\n```", "```py\n( config_kwargs: dict custom_features: Optional = None )\n```", "```py\n( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None record_checksums = True )\n```", "```py\n( url_or_urls ) \u2192 export const metadata = 'undefined';str or list or dict\n```", "```py\n>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n```", "```py\n( url_or_urls ) \u2192 export const metadata = 'undefined';extracted_path(s)\n```", "```py\nextracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))\n```", "```py\n( url_or_urls custom_download ) \u2192 export const metadata = 'undefined';downloaded_path(s)\n```", "```py\n>>> downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)\n```", "```py\n( path_or_paths num_proc = 'deprecated' ) \u2192 export const metadata = 'undefined';extracted_path(s)\n```", "```py\n>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n>>> extracted_files = dl_manager.extract(downloaded_files)\n```", "```py\n( path_or_buf: Union ) \u2192 export const metadata = 'undefined';tuple[str, io.BufferedReader]\n```", "```py\n>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n>>> files = dl_manager.iter_archive(archive)\n```", "```py\n( paths: Union ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')\n>>> files = dl_manager.iter_files(files)\n```", "```py\n( downloaded_path_or_paths pipeline )\n```", "```py\n( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None )\n```", "```py\n( url_or_urls ) \u2192 export const metadata = 'undefined';url(s)\n```", "```py\n>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n```", "```py\n( url_or_urls ) \u2192 export const metadata = 'undefined';url(s)\n```", "```py\nurls = dl_manager.extract(dl_manager.download(url_or_urls))\n```", "```py\n( url_or_urls ) \u2192 export const metadata = 'undefined';url(s)\n```", "```py\n>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n>>> extracted_files = dl_manager.extract(downloaded_files)\n```", "```py\n( urlpath_or_buf: Union ) \u2192 export const metadata = 'undefined';tuple[str, io.BufferedReader]\n```", "```py\n>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')\n>>> files = dl_manager.iter_archive(archive)\n```", "```py\n( urlpaths: Union ) \u2192 export const metadata = 'undefined';str\n```", "```py\n>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')\n>>> files = dl_manager.iter_files(files)\n```", "```py\n( cache_dir: Union = None force_download: bool = False resume_download: bool = False local_files_only: bool = False proxies: Optional = None user_agent: Optional = None extract_compressed_file: bool = False force_extract: bool = False delete_extracted: bool = False use_etag: bool = True num_proc: Optional = None max_retries: int = 1 token: Union = None use_auth_token: dataclasses.InitVar[typing.Union[str, bool, NoneType]] = 'deprecated' ignore_url_params: bool = False storage_options: Dict = <factory> download_desc: Optional = None )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( name: str gen_kwargs: Dict = <factory> )\n```", "```py\n>>> datasets.SplitGenerator(\n...     name=datasets.Split.TRAIN,\n...     gen_kwargs={\"split_key\": \"train\", \"files\": dl_manager.download_and_extract(url)},\n... )\n```", "```py\n( name )\n```", "```py\n>>> datasets.SplitGenerator(\n...     name=datasets.Split.TRAIN,\n...     gen_kwargs={\"split_key\": \"train\", \"files\": dl_manager.download_and extract(url)},\n... ),\n... datasets.SplitGenerator(\n...     name=datasets.Split.VALIDATION,\n...     gen_kwargs={\"split_key\": \"validation\", \"files\": dl_manager.download_and extract(url)},\n... ),\n... datasets.SplitGenerator(\n...     name=datasets.Split.TEST,\n...     gen_kwargs={\"split_key\": \"test\", \"files\": dl_manager.download_and extract(url)},\n... )\n```", "```py\n( name )\n```", "```py\nsplit = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST\n```", "```py\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TRAIN.subsplit(datasets.percent[75:])\n)  # Error\nsplit = datasets.Split.TEST + datasets.Split.ALL  # Error\n```", "```py\nsplit = (\n        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +\n        datasets.Split.TEST.subsplit(datasets.percent[:50])\n)\nsplit = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])\n```", "```py\ntrain = datasets.Split.TRAIN\ntest = datasets.Split.TEST\nsplit = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])\nsplit = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])\n```", "```py\n( )\n```", "```py\n( split_name rounding = None from_ = None to = None unit = None )\n```", "```py\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%'))\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%]+train[1:-1]'))\nds = datasets.load_dataset('mnist', split=(\ndatasets.ReadInstruction('test', to=33, unit='%') +\ndatasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))\n\n# The following lines are equivalent:\nds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(\n'test[:33%](pct1_dropremainder)'))\nds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(\n'test', from_=0, to=33, unit='%', rounding=\"pct1_dropremainder\"))\n\n# 10-fold validation:\ntests = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')\nfor k in range(0, 100, 10)])\ntrains = datasets.load_dataset(\n'mnist',\n[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')\nfor k in range(0, 100, 10)])\n```", "```py\n( spec )\n```", "```py\ntest: test split.\ntest + validation: test split + validation split.\ntest[10:]: test split, minus its first 10 records.\ntest[:10%]: first 10% records of test split.\ntest[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.\ntest[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.\n```", "```py\n( name2len )\n```", "```py\n( version_str: str description: Optional = None major: Union = None minor: Union = None patch: Union = None )\n```", "```py\n>>> VERSION = datasets.Version(\"1.0.0\")\n```"]