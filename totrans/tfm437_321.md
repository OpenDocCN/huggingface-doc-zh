# UnivNet

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/univnet](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/univnet)

## 概述

UnivNet模型是由Won Jang、Dan Lim、Jaesam Yoon、Bongwan Kin和Juntae Kim在[UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://arxiv.org/abs/2106.07889)中提出的。UnivNet模型是一个生成对抗网络（GAN），用于合成高保真度语音波形。在`transformers`中共享的UnivNet模型是*生成器*，它将一个条件化的对数梅尔频谱图和可选的噪声序列映射到语音波形（例如声码器）。推理只需要生成器。用于训练`生成器`的*鉴别器*没有实现。

论文摘要如下：

*大多数神经声码器使用带限制的梅尔频谱图来生成波形。如果将全频段谱特征用作输入，则声码器可以提供尽可能多的声学信息。然而，在一些使用全频段梅尔频谱图的模型中，会出现过度平滑的问题，其中生成了非锐利的频谱图。为了解决这个问题，我们提出了UnivNet，一个能够实时合成高保真波形的神经声码器。受到语音活动检测领域的研究启发，我们添加了一个多分辨率谱图鉴别器，该鉴别器使用了使用不同参数集计算的多个线性谱图幅度。通过使用全频段梅尔频谱图作为输入，我们希望通过添加一个使用多个分辨率谱图作为输入的鉴别器来生成高分辨率信号。在包含数百位说话者信息的数据集上进行评估时，UnivNet在看到和未看到的说话者方面获得了最佳客观和主观结果。这些结果，包括文本转语音的最佳主观评分，展示了快速适应新说话者的潜力，而无需从头开始训练。*

提示：

+   [UnivNetModel.forward()](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetModel.forward)的`noise_sequence`参数应为标准的高斯噪声（例如来自`torch.randn`），形状为`([batch_size], noise_length, model.config.model_in_channels)`，其中`noise_length`应与`input_features`参数的长度维度（维度1）匹配。如果未提供，将随机生成；可以向`generator`参数提供`torch.Generator`，以便可以重现前向传递。（请注意，[UnivNetFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor)默认会返回生成的噪声，因此通常不需要手动生成`noise_sequence`。）

+   通过`UnivNetFeatureExtractor.batch_decode()`方法可以从[UnivNetModel](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetModel)输出中移除[UnivNetFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor)添加的填充，如下面的用法示例所示。

+   在每个波形的末尾填充静音可以减少生成音频样本末尾的伪影。可以通过在[UnivNetFeatureExtractor.**call**()](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__)中提供`pad_end = True`来实现。更多细节请参见[此问题](https://github.com/seungwonpark/melgan/issues/8)。

用法示例：

```py
import torch
from scipy.io.wavfile import write
from datasets import Audio, load_dataset

from transformers import UnivNetFeatureExtractor, UnivNetModel

model_id_or_path = "dg845/univnet-dev"
model = UnivNetModel.from_pretrained(model_id_or_path)
feature_extractor = UnivNetFeatureExtractor.from_pretrained(model_id_or_path)

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
# Resample the audio to the model and feature extractor's sampling rate.
ds = ds.cast_column("audio", Audio(sampling_rate=feature_extractor.sampling_rate))
# Pad the end of the converted waveforms to reduce artifacts at the end of the output audio samples.
inputs = feature_extractor(
    ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], pad_end=True, return_tensors="pt"
)

with torch.no_grad():
    audio = model(**inputs)

# Remove the extra padding at the end of the output.
audio = feature_extractor.batch_decode(**audio)[0]
# Convert to wav file
write("sample_audio.wav", feature_extractor.sampling_rate, audio)
```

该模型由[dg845](https://huggingface.co/dg845)贡献。据我所知，没有官方代码发布，但可以在[maum-ai/univnet](https://github.com/maum-ai/univnet)找到非官方实现，预训练检查点在[这里](https://github.com/maum-ai/univnet#pre-trained-model)。

## UnivNetConfig

### `class transformers.UnivNetConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/univnet/configuration_univnet.py#L28)

```py
( model_in_channels = 64 model_hidden_channels = 32 num_mel_bins = 100 resblock_kernel_sizes = [3, 3, 3] resblock_stride_sizes = [8, 8, 4] resblock_dilation_sizes = [[1, 3, 9, 27], [1, 3, 9, 27], [1, 3, 9, 27]] kernel_predictor_num_blocks = 3 kernel_predictor_hidden_channels = 64 kernel_predictor_conv_size = 3 kernel_predictor_dropout = 0.0 initializer_range = 0.01 leaky_relu_slope = 0.2 **kwargs )
```

参数

+   `model_in_channels`（`int`，*可选*，默认为64）- UnivNet残差网络的输入通道数。这应该对应于`noise_sequence.shape[1]`和[UnivNetFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor)类中使用的值。

+   `model_hidden_channels`（`int`，*可选*，默认为32）- UnivNet残差网络中每个残差块的隐藏通道数。

+   `num_mel_bins`（`int`，*可选*，默认为100）- 条件对数梅尔频谱图中的频率箱数。这应该对应于[UnivNetFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor)类中使用的值。

+   `resblock_kernel_sizes`（`Tuple[int]`或`List[int]`，*可选*，默认为`[3, 3, 3]`）- 一个整数元组，定义了UnivNet残差网络中1D卷积层的内核大小。`resblock_kernel_sizes`的长度定义了resnet块的数量，并应与`resblock_stride_sizes`和`resblock_dilation_sizes`相匹配。

+   `resblock_stride_sizes`（`Tuple[int]`或`List[int]`，*可选*，默认为`[8, 8, 4]`）- 一个整数元组，定义了UnivNet残差网络中1D卷积层的步幅大小。`resblock_stride_sizes`的长度应与`resblock_kernel_sizes`和`resblock_dilation_sizes`相匹配。

+   `resblock_dilation_sizes`（`Tuple[Tuple[int]]`或`List[List[int]]`，*可选*，默认为`[[1, 3, 9, 27], [1, 3, 9, 27], [1, 3, 9, 27]]`）- 一个嵌套的整数元组，定义了UnivNet残差网络中扩张的1D卷积层的扩张率。`resblock_dilation_sizes`的长度应与`resblock_kernel_sizes`和`resblock_stride_sizes`相匹配。`resblock_dilation_sizes`中每个嵌套列表的长度定义了每个resnet块中的卷积层数量。

+   `kernel_predictor_num_blocks`（`int`，*可选*，默认为3）- 核预测网络中的残差块数量，用于计算UnivNet残差网络中每个位置变量卷积层的内核和偏置。

+   `kernel_predictor_hidden_channels`（`int`，*可选*，默认为64）- 核预测网络中每个残差块的隐藏通道数。

+   `kernel_predictor_conv_size`（`int`，*可选*，默认为3）- 核预测网络中每个1D卷积层的内核大小。

+   `kernel_predictor_dropout`（`float`，*可选*，默认为0.0）- 核预测网络中每个残差块的丢失概率。

+   `initializer_range`（`float`，*可选*，默认为0.01）- 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `leaky_relu_slope`（`float`，*可选*，默认为0.2）- leaky ReLU激活函数使用的负斜率的角度。

这是一个配置类，用于存储[UnivNetModel](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetModel)的配置。根据指定的参数实例化一个UnivNet声码器模型，定义模型架构。使用默认值实例化配置将产生类似于UnivNet [dg845/univnet-dev](https://huggingface.co/dg845/univnet-dev)架构的配置，该架构对应于[maum-ai/univnet](https://github.com/maum-ai/univnet/blob/master/config/default_c32.yaml)中的‘c32’架构。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import UnivNetModel, UnivNetConfig

>>> # Initializing a Tortoise TTS style configuration
>>> configuration = UnivNetConfig()

>>> # Initializing a model (with random weights) from the Tortoise TTS style configuration
>>> model = UnivNetModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## UnivNetFeatureExtractor

### `class transformers.UnivNetFeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/univnet/feature_extraction_univnet.py#L29)

```py
( feature_size: int = 1 sampling_rate: int = 24000 padding_value: float = 0.0 do_normalize: bool = False num_mel_bins: int = 100 hop_length: int = 256 win_length: int = 1024 win_function: str = 'hann_window' filter_length: Optional = 1024 max_length_s: int = 10 fmin: float = 0.0 fmax: Optional = None mel_floor: float = 1e-09 center: bool = False compression_factor: float = 1.0 compression_clip_val: float = 1e-05 normalize_min: float = -11.512925148010254 normalize_max: float = 2.3143386840820312 model_in_channels: int = 64 pad_end_length: int = 10 return_attention_mask = True **kwargs )
```

参数

+   `feature_size` (`int`, *optional*, 默认为 1) — 提取特征的特征维度。

+   `sampling_rate` (`int`, *optional*, 默认为 24000) — 音频文件应该以赫兹（Hz）表示的数字化采样率。

+   `padding_value` (`float`, *optional*, 默认为 0.0) — 在应用由 `padding` 参数定义的填充策略时要填充的值，应与音频静音对应。`__call__` 中的 `pad_end` 参数也将使用此填充值。

+   `do_normalize` (`bool`, *optional*, 默认为 `False`) — 是否对输入执行 Tacotron 2 标准化。标准化可以帮助一些模型显著提高性能。

+   `num_mel_bins` (`int`, *optional*, 默认为 100) — 提取的频谱图特征中的梅尔频率箱数。这应该与 `UnivNetModel.config.num_mel_bins` 匹配。

+   `hop_length` (`int`, *optional*, 默认为 256) — 滑动窗口之间的直接样本数。在许多论文中也称为“shift”。请注意，这与其他音频特征提取器（如 [SpeechT5FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor））中以毫秒为单位的 `hop_length` 不同。

+   `win_length` (`int`, *optional*, 默认为 1024) — 每个滑动窗口的直接样本数。请注意，这与其他音频特征提取器（如 [SpeechT5FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor））中以毫秒为单位的 `win_length` 不同。

+   `win_function` (`str`, *optional*, 默认为 `"hann_window"`) — 用于窗口化的窗口函数的名称，必须通过 `torch.{win_function}` 访问。

+   `filter_length` (`int`, *optional*, 默认为 1024) — 要使用的 FFT 组件数。如果为 `None`，则使用 `transformers.audio_utils.optimal_fft_length` 来确定。

+   `max_length_s` (`int`, *optional*, 默认为 10) — 模型的最大输入长度，以秒为单位。这用于填充音频。

+   `fmin` (`float`, *optional*, 默认为 0.0) — 最小的梅尔频率，以赫兹为单位。

+   `fmax` (`float`, *optional*) — 最大的梅尔频率，以赫兹为单位。如果未设置，默认为 `sampling_rate / 2`。

+   `mel_floor` (`float`, *optional*, 默认为 1e-09) — 梅尔频率银行的最小值。请注意，[UnivNetFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor) 使用 `mel_floor` 的方式与 [transformers.audio_utils.spectrogram()](/docs/transformers/v4.37.2/en/internal/audio_utils#transformers.audio_utils.spectrogram) 中的方式不同。

+   `center` (`bool`, *optional*, 默认为 `False`) — 是否填充波形，使帧 `t` 围绕时间 `t * hop_length` 居中。如果为 `False`，帧 `t` 将从时间 `t * hop_length` 开始。

+   `compression_factor` (`float`, *optional*, 默认为 1.0) — 动态范围压缩期间的乘法压缩因子。

+   `compression_clip_val` (`float`, *optional*, 默认为 1e-05) — 在应用动态范围压缩期间应用于波形的剪切值。

+   `normalize_min` (`float`, *可选*, 默认为-11.512925148010254) — 用于Tacotron 2风格线性归一化的最小值。默认值是Tacotron 2实现的原始值。

+   `normalize_max` (`float`, *可选*, 默认为2.3143386840820312) — 用于Tacotron 2风格线性归一化的最大值。默认值是Tacotron 2实现的原始值。

+   `model_in_channels` (`int`, *可选*, 默认为64) — [UnivNetModel](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetModel)模型的输入通道数。这应该与`UnivNetModel.config.model_in_channels`匹配。

+   `pad_end_length` (`int`, *可选*, 默认为10) — 如果在每个波形的末尾填充，要附加的样本的频谱图帧数。附加的样本数将为`pad_end_length * hop_length`。

+   `return_attention_mask` (`bool`, *可选*, 默认为`True`) — [`call`()](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetFeatureExtractor.__call__)是否应返回`attention_mask`。

构建一个UnivNet特征提取器。

该类使用短时傅里叶变换（STFT）从原始语音中提取对数梅尔滤波器组特征。STFT实现遵循TacoTron 2和Hifi-GAN的实现。

该特征提取器继承自[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/univnet/feature_extraction_univnet.py#L286)

```py
( raw_speech: Union sampling_rate: Optional = None padding: Union = True max_length: Optional = None truncation: bool = True pad_to_multiple_of: Optional = None return_noise: bool = True generator: Optional = None pad_end: bool = False pad_length: Optional = None do_normalize: Optional = None return_attention_mask: Optional = None return_tensors: Union = None )
```

参数

+   `raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`) — 要填充的序列或批次序列。每个序列可以是一个numpy数组，一个浮点值列表，一个numpy数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。

+   `sampling_rate` (`int`, *可选*) — 输入`raw_speech`采样的采样率。强烈建议在前向调用时传递`sampling_rate`，以防止静默错误并允许自动语音识别流水线。

+   `padding` (`bool`, `str`或[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *可选*, 默认为`True`) — 选择一种策略来填充输入`raw_speech`波形（根据模型的填充方向和填充索引）：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供一个序列，则不填充）。

    +   `'max_length'`：填充到指定的最大长度，该长度由参数`max_length`指定，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

    如果`pad_end = True`，则填充将在应用`padding`策略之前发生。

+   `max_length` (`int`, *可选*) — 返回列表的最大长度和可选填充长度（见上文）。

+   `truncation` (`bool`, *可选*, 默认为`True`) — 激活截断，将长于`max_length`的输入序列截断为`max_length`。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。

    这对于在NVIDIA硬件上启用Tensor Cores特别有用，其计算能力为`>= 7.5`（Volta），或者对于受益于序列长度为128的倍数的TPU。

+   `return_noise` (`bool`, *可选*, 默认为`True`) — 是否生成并返回用于[UnivNetModel.forward()](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetModel.forward)的噪声波形。

+   `generator` (`numpy.random.Generator`, *可选*, 默认为 `None`) — 生成噪音时要使用的可选 `numpy.random.Generator` 随机数生成器。

+   `pad_end` (`bool`, *可选*, 默认为 `False`) — 是否在每个波形的末尾填充静音。这可以帮助减少生成音频样本末尾的伪影。有关更多详细信息，请参阅 [https://github.com/seungwonpark/melgan/issues/8](https://github.com/seungwonpark/melgan/issues/8)。此填充将在执行 `padding` 中指定的填充策略之前完成。

+   `pad_length` (`int`, *可选*, 默认为 `None`) — 如果在每个波形的末尾填充，填充在频谱图帧中的长度。如果未设置，将默认为 `self.config.pad_end_length`。

+   `do_normalize` (`bool`, *可选*) — 是否对输入执行 Tacotron 2 标准化。标准化可以帮助显著提高某些模型的性能。如果未设置，将默认为 `self.config.do_normalize`。

+   `return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定的 feature_extractor 默认返回注意力掩码。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.np.array` 对象。

    +   `'np'`: 返回Numpy `np.ndarray` 对象。

对一个或多个序列进行特征化和准备模型的主要方法。

## UnivNetModel

### `class transformers.UnivNetModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/univnet/modeling_univnet.py#L464)

```py
( config: UnivNetConfig )
```

参数

+   `config` ([UnivNetConfig](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

UnivNet GAN 语音合成器。此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/univnet/modeling_univnet.py#L511)

```py
( input_features: FloatTensor noise_sequence: Optional = None padding_mask: Optional = None generator: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.univnet.modeling_univnet.UnivNetModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_features` (`torch.FloatTensor`) — 包含对数梅尔频谱图的张量。可以是批处理的，形状为 `(batch_size, sequence_length, config.num_mel_channels)`，也可以是非批处理的，形状为 `(sequence_length, config.num_mel_channels)`。

+   `noise_sequence` (`torch.FloatTensor`, *可选*) — 包含标准高斯噪音的噪音序列的张量。可以是批处理的，形状为 `(batch_size, sequence_length, config.model_in_channels)`，或者非批处理的，形状为 `(sequence_length, config.model_in_channels)`。如果未提供，则将随机生成。

+   `padding_mask` (`torch.BoolTensor`, *可选*) — 指示每个序列的哪些部分被填充的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 用于未被“masked”的标记

    +   对于被`masked`的标记

    掩码可以是批处理的，形状为 `(batch_size, sequence_length)`，也可以是非批处理的，形状为 `(sequence_length,)`。

+   `generator`（`torch.Generator`，*可选*）— 一个用于使生成过程确定性的[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)。return_dict — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)子类而不是一个普通元组。

返回

`transformers.models.univnet.modeling_univnet.UnivNetModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.univnet.modeling_univnet.UnivNetModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[UnivNetConfig](/docs/transformers/v4.37.2/en/model_doc/univnet#transformers.UnivNetConfig)）和输入的不同元素。

+   `waveforms`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 批量的1D（单声道）输出音频波形。

+   `waveform_lengths`（形状为`(batch_size,)`的`torch.FloatTensor`）— `waveforms`中每个未填充波形的批量长度（以样本为单位）。

`UnivNetModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

将噪声波形和一个条件谱图转换为语音波形。传递一批log-mel谱图返回一批语音波形。传递一个单独的、未批处理的log-mel谱图返回一个单独的、未批处理的语音波形。

示例：

```py
>>> from transformers import UnivNetFeatureExtractor, UnivNetModel
>>> from datasets import load_dataset, Audio

>>> model = UnivNetModel.from_pretrained("dg845/univnet-dev")
>>> feature_extractor = UnivNetFeatureExtractor.from_pretrained("dg845/univnet-dev")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> # Resample the audio to the feature extractor's sampling rate.
>>> ds = ds.cast_column("audio", Audio(sampling_rate=feature_extractor.sampling_rate))
>>> inputs = feature_extractor(
...     ds[0]["audio"]["array"], sampling_rate=ds[0]["audio"]["sampling_rate"], return_tensors="pt"
... )
>>> audio = model(**inputs).waveforms
>>> list(audio.shape)
[1, 140288]
```
