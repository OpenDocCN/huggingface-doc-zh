- en: Whisper
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Whisper
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/whisper)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: The Whisper model was proposed in [Robust Speech Recognition via Large-Scale
    Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford,
    Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Whisperæ¨¡å‹ç”±Alec Radfordã€Jong Wook Kimã€Tao Xuã€Greg Brockmanã€Christine McLeaveyã€Ilya
    Sutskeveråœ¨[é€šè¿‡å¤§è§„æ¨¡å¼±ç›‘ç£å®ç°ç¨³å¥è¯­éŸ³è¯†åˆ«](https://cdn.openai.com/papers/whisper.pdf)ä¸­æå‡ºã€‚
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*We study the capabilities of speech processing systems trained simply to predict
    large amounts of transcripts of audio on the internet. When scaled to 680,000
    hours of multilingual and multitask supervision, the resulting models generalize
    well to standard benchmarks and are often competitive with prior fully supervised
    results but in a zeroshot transfer setting without the need for any finetuning.
    When compared to humans, the models approach their accuracy and robustness. We
    are releasing models and inference code to serve as a foundation for further work
    on robust speech processing.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬ç ”ç©¶äº†ç®€å•è®­ç»ƒä»¥é¢„æµ‹äº’è”ç½‘ä¸Šå¤§é‡éŸ³é¢‘è½¬å½•çš„è¯­éŸ³å¤„ç†ç³»ç»Ÿçš„èƒ½åŠ›ã€‚å½“æ‰©å±•åˆ°680,000å°æ—¶çš„å¤šè¯­è¨€å’Œå¤šä»»åŠ¡ç›‘ç£æ—¶ï¼Œå¾—åˆ°çš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”é€šå¸¸ä¸å…ˆå‰çš„å®Œå…¨ç›‘ç£ç»“æœç«äº‰ï¼Œä½†åœ¨é›¶æ¬¡è¿ç§»è®¾ç½®ä¸­æ— éœ€ä»»ä½•å¾®è°ƒã€‚ä¸äººç±»ç›¸æ¯”ï¼Œæ¨¡å‹æ¥è¿‘å…¶å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹å’Œæ¨ç†ä»£ç ï¼Œä»¥ä½œä¸ºè¿›ä¸€æ­¥ç ”ç©¶ç¨³å¥è¯­éŸ³å¤„ç†çš„åŸºç¡€ã€‚*'
- en: This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ).
    The Tensorflow version of this model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).
    The original code can be found [here](https://github.com/openai/whisper).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç”±[Arthur Zucker](https://huggingface.co/ArthurZ)è´¡çŒ®ã€‚æ­¤æ¨¡å‹çš„Tensorflowç‰ˆæœ¬ç”±[amyeroberts](https://huggingface.co/amyeroberts)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/openai/whisper)æ‰¾åˆ°ã€‚
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æç¤º
- en: The model usually performs well without requiring any finetuning.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹é€šå¸¸æ— éœ€ä»»ä½•å¾®è°ƒå³å¯è¡¨ç°è‰¯å¥½ã€‚
- en: The architecture follows a classic encoder-decoder architecture, which means
    that it relies on the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    function for inference.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥æ¶æ„éµå¾ªç»å…¸çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¿™æ„å‘³ç€å®ƒä¾èµ–äº[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)å‡½æ•°è¿›è¡Œæ¨ç†ã€‚
- en: Inference is currently only implemented for short-form i.e. audio is pre-segmented
    into <=30s segments. Long-form (including timestamps) will be implemented in a
    future release.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›®å‰ä»…å®ç°äº†çŸ­å½¢å¼çš„æ¨ç†ï¼Œå³éŸ³é¢‘è¢«é¢„åˆ†æ®µä¸º<=30ç§’çš„ç‰‡æ®µã€‚é•¿å½¢å¼ï¼ˆåŒ…æ‹¬æ—¶é—´æˆ³ï¼‰å°†åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å®ç°ã€‚
- en: One can use [WhisperProcessor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor)
    to prepare audio for the model, and decode the predicted IDâ€™s back into text.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[WhisperProcessor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor)æ¥å‡†å¤‡éŸ³é¢‘ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ï¼Œå¹¶å°†é¢„æµ‹çš„IDè§£ç å›æ–‡æœ¬ã€‚
- en: 'To convert the model and the processor, we recommend using the following:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è½¬æ¢æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The script will automatically determine all necessary parameters from the OpenAI
    checkpoint. A `tiktoken` library needs to be installed to perform the conversion
    of the OpenAI tokenizer to the `tokenizers` version.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬å°†è‡ªåŠ¨ä»OpenAIæ£€æŸ¥ç‚¹ç¡®å®šæ‰€æœ‰å¿…è¦çš„å‚æ•°ã€‚éœ€è¦å®‰è£…`tiktoken`åº“ä»¥æ‰§è¡Œå°†OpenAIåˆ†è¯å™¨è½¬æ¢ä¸º`tokenizers`ç‰ˆæœ¬çš„è½¬æ¢ã€‚
- en: Inference
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: 'Here is a step-by-step guide to transcribing an audio sample using a pre-trained
    Whisper model:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„Whisperæ¨¡å‹è½¬å½•éŸ³é¢‘æ ·æœ¬çš„é€æ­¥æŒ‡å—ï¼š
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Resources
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èµ„æº
- en: A list of official Hugging Face and community (indicated by ğŸŒ) resources to
    help you get started with Whisper. If youâ€™re interested in submitting a resource
    to be included here, please feel free to open a Pull Request and weâ€™ll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨Whisperã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æ ¸ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚
- en: 'A fork with a script to [convert a Whisper model in Hugging Face format to
    OpenAI format](https://github.com/zuazo-forks/transformers/blob/convert_hf_to_openai/src/transformers/models/whisper/convert_hf_to_openai.py).
    ğŸŒ Usage example:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«è„šæœ¬çš„åˆ†æ”¯ï¼Œç”¨äº[å°†Hugging Faceæ ¼å¼çš„Whisperæ¨¡å‹è½¬æ¢ä¸ºOpenAIæ ¼å¼](https://github.com/zuazo-forks/transformers/blob/convert_hf_to_openai/src/transformers/models/whisper/convert_hf_to_openai.py)ã€‚ğŸŒ
    ä½¿ç”¨ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: WhisperConfig
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperConfig
- en: '### `class transformers.WhisperConfig`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/configuration_whisper.py#L62)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/configuration_whisper.py#L62)'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_size` (`int`, *optional*, defaults to 51865) â€” Vocabulary size of the
    Whisper model. Defines the number of different tokens that can be represented
    by the `decoder_input_ids` passed when calling [WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º51865ï¼‰â€” Whisperæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel)æ—¶å¯ä»¥ç”±`decoder_input_ids`è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚'
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) â€” Number of mel features
    used per input features. Should correspond to the value used in the `WhisperProcessor`
    class.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_mel_bins`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º80ï¼‰â€” æ¯ä¸ªè¾“å…¥ç‰¹å¾ä¸­ä½¿ç”¨çš„melç‰¹å¾æ•°é‡ã€‚åº”ä¸`WhisperProcessor`ç±»ä¸­ä½¿ç”¨çš„å€¼å¯¹åº”ã€‚'
- en: '`encoder_layers` (`int`, *optional*, defaults to 4) â€” Number of encoder layers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4ï¼‰â€” ç¼–ç å™¨å±‚æ•°ã€‚'
- en: '`decoder_layers` (`int`, *optional*, defaults to 4) â€” Number of decoder layers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4ï¼‰â€” è§£ç å™¨å±‚æ•°ã€‚'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 6) â€” Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º6ï¼‰â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 6) â€” Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º6) â€” Transformerè§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 1536) â€” Dimensionality of
    the â€œintermediateâ€ (often named feed-forward) layer in encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, é»˜è®¤ä¸º1536) â€” ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 1536) â€” Dimensionality of
    the â€œintermediateâ€ (often named feed-forward) layer in decoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, é»˜è®¤ä¸º1536) â€” è§£ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” The LayerDrop
    probability for the encoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” ç¼–ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDrop
    paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) â€” The LayerDrop
    probability for the decoder. See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” è§£ç å™¨çš„LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDrop
    paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚'
- en: '`decoder_start_token_id` (`int`, *optional*, defaults to 50257) â€” Corresponds
    to the â€<|startoftranscript|>â€ token, which is automatically used when no `decoder_input_ids`
    are provided to the `generate` function. It is used to guide the model`s generation
    process depending on the task.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_start_token_id`ï¼ˆ`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º50257ï¼‰--å¯¹åº”äº`â€œ<|startoftranscript|>â€`æ ‡è®°ï¼Œå½“æ²¡æœ‰å‘`generate`å‡½æ•°æä¾›`decoder_input_ids`æ—¶ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨è¯¥æ ‡è®°ã€‚å®ƒç”¨äºæ ¹æ®ä»»åŠ¡æŒ‡å¯¼æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ã€‚'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) â€” Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚'
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) â€” Whether the
    model is used as an encoder/decoder or not.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_encoder_decoder` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦ç”¨ä½œç¼–ç å™¨/è§£ç å™¨ã€‚'
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) â€” The non-linear
    activation function (function or string) in the encoder and pooler. If string,
    `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`, *optional*, é»˜è®¤ä¸º`"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"silu"`å’Œ`"gelu_new"`ã€‚'
- en: '`d_model` (`int`, *optional*, defaults to 384) â€” Dimensionality of the layers.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, é»˜è®¤ä¸º384) â€” å±‚çš„ç»´åº¦ã€‚'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) â€” The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, é»˜è®¤ä¸º0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å±‚ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ã€‚'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for the attention probabilities.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” å…¨è¿æ¥å±‚å†…éƒ¨æ¿€æ´»çš„ä¸¢å¼ƒæ¯”ç‡ã€‚'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) â€” The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚'
- en: '`scale_embedding` (`bool`, *optional*, defaults to False) â€” Scale embeddings
    by diving by sqrt(d_model).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_embedding` (`bool`, *optional*, é»˜è®¤ä¸ºFalse) â€” é€šè¿‡é™¤ä»¥sqrt(d_model)æ¥ç¼©æ”¾åµŒå…¥ã€‚'
- en: '`max_source_positions` (`int`, *optional*, defaults to 1500) â€” The maximum
    sequence length of log-mel filter-bank features that this model might ever be
    used with.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_source_positions` (`int`, *optional*, é»˜è®¤ä¸º1500) â€” è¯¥æ¨¡å‹å¯èƒ½ç”¨äºçš„å¯¹æ•°æ¢…å°”æ»¤æ³¢å™¨ç»„ç‰¹å¾çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚'
- en: '`max_target_positions` (`int`, *optional*, defaults to 448) â€” The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_target_positions` (`int`, *optional*, é»˜è®¤ä¸º448) â€” è¯¥æ¨¡å‹å¯èƒ½ç”¨äºçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸å°†å…¶è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚512ã€1024æˆ–2048ï¼‰ã€‚'
- en: '`pad_token_id` (`int`, *optional*, defaults to 50256) â€” Padding token id.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, é»˜è®¤ä¸º50256) â€” å¡«å……æ ‡è®°idã€‚'
- en: '`bos_token_id` (`int`, *optional*, defaults to 50256) â€” Begin of stream token
    id.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, é»˜è®¤ä¸º50256) â€” æµçš„å¼€å§‹æ ‡è®°idã€‚'
- en: '`eos_token_id` (`int`, *optional*, defaults to 50256) â€” End of stream token
    id.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, é»˜è®¤ä¸º50256) â€” æµçš„ç»“æŸæ ‡è®°idã€‚'
- en: '`suppress_tokens` (`List[int]`, *optional*) â€” A list containing the non-speech
    tokens that will be used by the logit processor in the `generate` function. NON_SPEECH_TOKENS
    and NON_SPEECH_TOKENS_MULTI each correspond to the `english-only` and the `multilingual`
    model.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suppress_tokens` (`List[int]`, *optional*) â€” åŒ…å«å°†åœ¨`generate`å‡½æ•°ä¸­ç”±å¯¹æ•°å¤„ç†å™¨ä½¿ç”¨çš„éè¯­éŸ³æ ‡è®°çš„åˆ—è¡¨ã€‚NON_SPEECH_TOKENSå’ŒNON_SPEECH_TOKENS_MULTIåˆ†åˆ«å¯¹åº”äº`english-only`å’Œ`multilingual`æ¨¡å‹ã€‚'
- en: '`begin_suppress_tokens` (`List[int]`, *optional*, defaults to `[220,50256]`)
    â€” A list containing tokens that will be supressed at the beginning of the sampling
    process. Initialized as the token for `" "` (`blank_token_id`) and the `eos_token_id`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`begin_suppress_tokens` (`List[int]`, *optional*, é»˜è®¤ä¸º`[220,50256]`) â€” åŒ…å«åœ¨é‡‡æ ·è¿‡ç¨‹å¼€å§‹æ—¶å°†è¢«æŠ‘åˆ¶çš„æ ‡è®°çš„åˆ—è¡¨ã€‚åˆå§‹åŒ–ä¸º`"
    "`ï¼ˆ`blank_token_id`ï¼‰å’Œ`eos_token_id`çš„æ ‡è®°ã€‚'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) â€” Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä½¿ç”¨å¸¦æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡å€¼ã€‚ä»…åœ¨ä½¿ç”¨[WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification)çš„å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) â€” Dimensionality
    of the projection before token mean-pooling for classification. Only relevant
    when using an instance of [WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *optional*, é»˜è®¤ä¸º256) â€” åˆ†ç±»å‰çš„æŠ•å½±ç»´åº¦ï¼Œç”¨äºæ ‡è®°å‡å€¼æ± åŒ–ã€‚ä»…åœ¨ä½¿ç”¨[WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification)çš„å®ä¾‹æ—¶ç›¸å…³ã€‚'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `False`) â€” Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å°†*SpecAugment*æ•°æ®å¢å¼ºåº”ç”¨äºç‰¹å¾ç¼–ç å™¨çš„è¾“å‡ºã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[SpecAugment:
    A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)ã€‚'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates `mask_time_prob*len(time_axis)/mask_time_length` independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked, *mask_time_prob* should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if `apply_spec_augment ==
    True`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰å°†è¢«æ©ç›–ã€‚æ©ç›–è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆ`mask_time_prob*len(time_axis)/mask_time_length`ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*mask_time_prob*åº”ä¸º`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚åªæœ‰åœ¨`apply_spec_augment
    == True`æ—¶æ‰ç›¸å…³ã€‚'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” Length of vector span
    along the time axis.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚åªæœ‰åœ¨â€mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masksâ€æ—¶æ‰ç›¸å…³ã€‚'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length`
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked, *mask_feature_prob*
    should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if `apply_spec_augment
    is True`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰å°†è¢«æ©ç›–ã€‚æ©ç›–è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆ`mask_feature_prob*len(feature_axis)/mask_time_length`ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºè¦æ©ç›–çš„å‘é‡è·¨åº¦çš„èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ*mask_feature_prob*åº”ä¸º`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½æ©ç›–å‘é‡çš„å®é™…ç™¾åˆ†æ¯”ã€‚åªæœ‰åœ¨`apply_spec_augment`ä¸ºTrueæ—¶æ‰ç›¸å…³ã€‚'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” Length of vector
    span along the feature axis.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” The minimum
    number of masks of length `mask_feature_length` generated along the feature axis,
    each time step, irrespectively of `mask_feature_prob`. Only relevant if `mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æ©ç çš„æœ€å°æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚åªæœ‰åœ¨`mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks`æ—¶æ‰ç›¸å…³ã€‚'
- en: '`median_filter_width` (`int`, *optional*, defaults to 7) â€” Width of the median
    filter used to smoothen to cross-attention outputs when computing token timestamps.
    Should be an odd number.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`median_filter_width` (`int`, *optional*, defaults to 7) â€” ç”¨äºåœ¨è®¡ç®—æ ‡è®°æ—¶é—´æˆ³æ—¶å¹³æ»‘äº¤å‰æ³¨æ„åŠ›è¾“å‡ºçš„ä¸­å€¼æ»¤æ³¢å™¨çš„å®½åº¦ã€‚åº”ä¸ºå¥‡æ•°ã€‚'
- en: This is the configuration class to store the configuration of a [WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel).
    It is used to instantiate a Whisper model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Whisper [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny)
    architecture.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç”¨äºå­˜å‚¨[WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–Whisperæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºWhisper
    [openai/whisper-tiny](https://huggingface.co/openai/whisper-tiny)æ¶æ„çš„é…ç½®ã€‚
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: 'Example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: WhisperTokenizer
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperTokenizer
- en: '### `class transformers.WhisperTokenizer`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L217)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L217)'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`vocab_file` (`str`) â€” Path to the vocabulary file.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`merges_file` (`str`) â€” Path to the merges file.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) â€” åˆå¹¶æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`normalizer_file` (`str`, *optional*) â€” Path to the normalizer_file file.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalizer_file` (`str`, *optional*) â€” æ­£åˆ™åŒ–å™¨æ–‡ä»¶çš„è·¯å¾„ã€‚'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) â€” Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œreplaceâ€`ï¼‰--å°†å­—èŠ‚è§£ç ä¸ºUTF-8æ—¶è¦éµå¾ªçš„ç¤ºä¾‹ã€‚è¯·å‚é˜…[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)äº†è§£æ›´å¤šä¿¡æ¯ã€‚'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--æœªçŸ¥ä»¤ç‰Œã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„ä»¤ç‰Œæ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºè¯¥ä»¤ç‰Œã€‚'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The beginning
    of sequence token. The `decoder_start_token_id` is used to set the first token
    as `"<|startoftranscript|>"` when generating.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚`decoder_start_token_id`ç”¨äºåœ¨ç”Ÿæˆæ—¶å°†ç¬¬ä¸€ä¸ªä»¤ç‰Œè®¾ç½®ä¸º`â€œ<|startoftranscript|>â€`ã€‚'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The end of
    sequence token.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: '`pad_token` (`str`, *optional*) â€” The token used for padding, for example when
    batching sequences of different lengths.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ã€‚'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨è¾“å…¥å‰æ·»åŠ ä¸€ä¸ªåˆå§‹ç©ºæ ¼ã€‚è¿™å…è®¸å°†å‰å¯¼å•è¯è§†ä¸ºä»»ä½•å…¶ä»–å•è¯ã€‚'
- en: '`language` (`str`, *optional*) â€” The language of the transcription text. The
    corresponding language id token is appended to the start of the sequence for multilingual
    speech recognition and speech translation tasks, e.g. for Spanish the token `"<|es|>"`
    is appended to the start of sequence. This should be used for multilingual fine-tuning
    only.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ï¼Œç›¸åº”çš„è¯­è¨€idæ ‡è®°è¢«é™„åŠ åˆ°åºåˆ—çš„å¼€å¤´ï¼Œä¾‹å¦‚å¯¹äºè¥¿ç­ç‰™è¯­ï¼Œæ ‡è®°`â€œ<|es|>â€`è¢«é™„åŠ åˆ°é¡ºåºçš„å¼€å¤´ã€‚è¿™åªèƒ½ç”¨äºå¤šè¯­è¨€å¾®è°ƒã€‚'
- en: '`task` (`str`, *optional*) â€” Task identifier to append at the start of sequence
    (if any). This should be used for mulitlingual fine-tuning, with `"transcribe"`
    for speech recognition and `"translate"` for speech translation.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è¦é™„åŠ åœ¨åºåˆ—å¼€å¤´çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚è¿™åº”ç”¨äºå¤šè¯­è¨€å¾®è°ƒï¼Œ`"transcribe"`ç”¨äºè¯­éŸ³è¯†åˆ«ï¼Œ`"translate"`ç”¨äºè¯­éŸ³ç¿»è¯‘ã€‚'
- en: '`predict_timestamps` (`bool`, *optional*, defaults to `False`) â€” Whether to
    omit the `<|notimestamps|>` token at the start of the sequence.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚
- en: Construct a Whisper tokenizer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªWhisper tokenizerã€‚
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `set_prefix_tokens`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_prefix_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L389)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L389)'
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`language` (`str`, *optional*, defaults to `None`) â€” The language of the transcription
    text.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `None`ï¼‰â€” è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚'
- en: '`task` (`str`, *optional*, defaults to `None`) â€” Task identifier to append
    at the start of sequence (if any).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `None`ï¼‰â€” ä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œé™„åŠ åœ¨åºåˆ—å¼€å¤´ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚'
- en: '`predict_timestamps` (`bool`, *optional*, defaults to `None`) â€” Whether to
    omit the `<|notimestamps|>` token at the start of the sequence.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`None`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚'
- en: Override the prefix tokens appended to the start of the label sequence. This
    method can be used standalone to
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: è¦†ç›–é™„åŠ åˆ°æ ‡ç­¾åºåˆ—å¼€å¤´çš„å‰ç¼€æ ‡è®°ã€‚æ­¤æ–¹æ³•å¯ä»¥ç‹¬ç«‹ä½¿ç”¨ä»¥
- en: 'update the prefix tokens as required when fine-tuning. Example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è°ƒæ—¶æ ¹æ®éœ€è¦æ›´æ–°å‰ç¼€æ ‡è®°ã€‚ç¤ºä¾‹ï¼š
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L448)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L448)'
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Build model inputs from a sequence by appending eos_token_id.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é™„åŠ  eos_token_id ä»åºåˆ—æ„å»ºæ¨¡å‹è¾“å…¥ã€‚
- en: '#### `get_special_tokens_mask`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L456)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L456)'
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” List of IDs.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ID åˆ—è¡¨ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” Optional second list of IDs for sequence
    pairs.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„å¯é€‰ç¬¬äºŒä¸ª ID åˆ—è¡¨ã€‚'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: Returns
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º [0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ— IDã€‚åœ¨ä½¿ç”¨åˆ†è¯å™¨çš„ `prepare_for_model` æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” The first tokenized sequence.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” The second tokenized sequence.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: Returns
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The token type ids.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°ç±»å‹ IDã€‚
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹ IDã€‚[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹ IDï¼Ÿ](../glossary#token-type-ids)
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹æœ‰ä¸€ç§ç‰¹æ®Šçš„æ„å»ºæ–¹å¼ï¼Œåˆ™åº”åœ¨å­ç±»ä¸­é‡å†™å®ƒã€‚
- en: '#### `save_vocabulary`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L763)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L763)'
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `batch_decode`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨ `__call__` æ–¹æ³•è·å–ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç æ—¶åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¸º `self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç çš„å¥å­åˆ—è¡¨ã€‚
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®° ID çš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '#### `decode`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L639)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper.py#L639)'
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥ ID çš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨ `__call__` æ–¹æ³•è·å–ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç æ—¶åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`
    (available in the `tokenizer_config`).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º `None`ï¼Œå°†é»˜è®¤ä¸º `self.clean_up_tokenization_spaces`ï¼ˆåœ¨
    `tokenizer_config` ä¸­å¯ç”¨ï¼‰ã€‚'
- en: '`output_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to output the offsets of the tokens. This should only be set if the model predicted
    timestamps.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_precision` (`float`, `optional`, defaults to 0.02) â€” The time ratio to
    convert from token to time.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode_with_timestamps` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to decode with timestamps included in the raw text.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize` (`bool`, *optional*, defaults to `False`) â€” Whether or not to apply
    the English text normalizer to the decoded text. Only applicable when the target
    text is in English. Otherwise, the basic text normalizer should be applied.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`basic_normalize` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to apply the Basic text normalizer to the decoded text. Applicable to multilingual
    target text.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`remove_diacritics` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove diacritics when applying the Basic text normalizer. Removing diacritics
    may destroy information in the decoded text, hence it should be used with caution.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '`str`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The decoded sentence.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: WhisperTokenizerFast
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.WhisperTokenizerFast`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L90)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`, *optional*) â€” Path to the vocabulary file.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merges_file` (`str`, *optional*) â€” Path to the merges file.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalizer_file` (`str`, *optional*) â€” Path to the normalizer_file file.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer_file` (`str`, *optional*) â€” Path to [tokenizers](https://github.com/huggingface/tokenizers)
    file (generally has a .json extension) that contains everything needed to load
    the tokenizer.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--æœªçŸ¥ä»¤ç‰Œã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„ä»¤ç‰Œæ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºè¯¥ä»¤ç‰Œã€‚'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The beginning
    of sequence token. The `decoder_start_token_id` is used to set the first token
    as `"<|startoftranscript|>"` when generating.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚`decoder_start_token_id`ç”¨äºåœ¨ç”Ÿæˆæ—¶å°†ç¬¬ä¸€ä¸ªä»¤ç‰Œè®¾ç½®ä¸º`â€œ<|startoftranscript|>â€`ã€‚'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) â€” The end of
    sequence token.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<|endoftext|>â€`ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚'
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to add an initial space to the input. This allows to treat the leading word just
    as any other word. (Whisper tokenizer detect beginning of words by the preceding
    space).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language` (`str`, *optional*) â€” The language of the transcription text. The
    corresponding language id token is appended to the start of the sequence for multilingual
    speech recognition and speech translation tasks, e.g. for Spanish the token `"<|es|>"`
    is appended to the start of sequence. This should be used for multilingual fine-tuning
    only.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è½¬å½•æ–‡æœ¬çš„è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³ç¿»è¯‘ä»»åŠ¡ï¼Œç›¸åº”çš„è¯­è¨€idæ ‡è®°è¢«é™„åŠ åˆ°åºåˆ—çš„å¼€å¤´ï¼Œä¾‹å¦‚å¯¹äºè¥¿ç­ç‰™è¯­ï¼Œæ ‡è®°`"<|es|>"`è¢«é™„åŠ åˆ°é¡ºåºçš„å¼€å¤´ã€‚è¿™åªèƒ½ç”¨äºå¤šè¯­è¨€å¾®è°ƒã€‚'
- en: '`task` (`str`, *optional*) â€” Task identifier to append at the start of sequence
    (if any). This should be used for mulitlingual fine-tuning, with `"transcribe"`
    for speech recognition and `"translate"` for speech translation.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰--è¦é™„åŠ åœ¨åºåˆ—å¼€å¤´çš„ä»»åŠ¡æ ‡è¯†ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚è¿™åº”ç”¨äºå¤šè¯­è¨€å¾®è°ƒï¼Œâ€œè½¬å½•â€ç”¨äºè¯­éŸ³è¯†åˆ«ï¼Œâ€œç¿»è¯‘â€ç”¨äºè¯­éŸ³ç¿»è¯‘ã€‚'
- en: '`predict_timestamps` (`bool`, *optional*, defaults to `False`) â€” Whether to
    omit the `<|notimestamps|>` token at the start of the sequence.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`False`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚'
- en: Construct a â€œfastâ€ Whisper tokenizer (backed by HuggingFaceâ€™s *tokenizers* library).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_prefix_tokens`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L462)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`language` (`str`, *optional*, defaults to `None`) â€” The language of the transcription
    text.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task` (`str`, *optional*, defaults to `None`) â€” Task identifier to append
    at the start of sequence (if any).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predict_timestamps` (`bool`, *optional*, defaults to `None`) â€” Whether to
    omit the `<|notimestamps|>` token at the start of the sequence.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_timestamps`ï¼ˆ`bool`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`None`ï¼‰--æ˜¯å¦çœç•¥åºåˆ—å¼€å¤´çš„`<|notimestamps|>`æ ‡è®°ã€‚'
- en: Override the prefix tokens appended to the start of the label sequence. This
    method can be used standalone to
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'update the prefix tokens as required when fine-tuning. Example:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L536)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Build model inputs from a sequence by appending eos_token_id.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_special_tokens_mask`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L544)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L544)'
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” List of IDs.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” IDåˆ—è¡¨ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” Optional second list of IDs for sequence
    pairs.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” åºåˆ—å¯¹çš„ç¬¬äºŒä¸ªIDåˆ—è¡¨ã€‚'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ ¼å¼åŒ–ä¸ºæ¨¡å‹ã€‚'
- en: Returns
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´æ•°åˆ—è¡¨åœ¨èŒƒå›´[0, 1]å†…ï¼š1è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—IDã€‚å½“ä½¿ç”¨tokenizerçš„`prepare_for_model`æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids_0` (`List[int]`) â€” The first tokenized sequence.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`ï¼ˆ`List[int]`ï¼‰â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: '`token_ids_1` (`List[int]`, *optional*) â€” The second tokenized sequence.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚'
- en: Returns
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: The token type ids.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°ç±»å‹IDã€‚
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹IDã€‚[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ¨¡å‹æœ‰ç‰¹æ®Šæ„å»ºæ–¹å¼ï¼Œåˆ™åº”åœ¨å­ç±»ä¸­é‡å†™ã€‚
- en: '#### `save_vocabulary`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L447)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L447)'
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#### `batch_decode`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE21]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥IDçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤æ ‡è®°ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šè§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å¥å­çš„åˆ—è¡¨ã€‚
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®°IDåˆ—è¡¨çš„åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '#### `decode`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L338)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/tokenization_whisper_fast.py#L338)'
- en: '[PRE22]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥IDçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`
    (available in the `tokenizer_config`).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤æ ‡è®°ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ï¼ˆåœ¨`tokenizer_config`ä¸­å¯ç”¨ï¼‰ã€‚'
- en: '`output_offsets` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to output the offsets of the tokens. This should only be set if the model predicted
    timestamps.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_offsets`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¾“å‡ºæ ‡è®°çš„åç§»é‡ã€‚åªæœ‰åœ¨æ¨¡å‹é¢„æµ‹æ—¶é—´æˆ³æ—¶æ‰åº”è®¾ç½®æ­¤é€‰é¡¹ã€‚'
- en: '`time_precision` (`float`, `optional`, defaults to 0.02) â€” The time ratio to
    convert from token to time.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_precision`ï¼ˆ`float`ï¼Œ`å¯é€‰`ï¼Œé»˜è®¤ä¸º0.02ï¼‰â€” ä»æ ‡è®°åˆ°æ—¶é—´çš„è½¬æ¢æ—¶é—´æ¯”ç‡ã€‚'
- en: '`decode_with_timestamps` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to decode with timestamps included in the raw text.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_with_timestamps`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åŸå§‹æ–‡æœ¬ä¸­åŒ…å«æ—¶é—´æˆ³è¿›è¡Œè§£ç ã€‚'
- en: '`normalize` (`bool`, *optional*, defaults to `False`) â€” Whether or not to apply
    the English text normalizer to the decoded text. Only applicable when the target
    text is in English. Otherwise, the basic text normalizer should be applied.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è§£ç æ–‡æœ¬åº”ç”¨è‹±æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€‚ä»…åœ¨ç›®æ ‡æ–‡æœ¬ä¸ºè‹±æ–‡æ—¶é€‚ç”¨ã€‚å¦åˆ™ï¼Œåº”åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚'
- en: '`basic_normalize` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to apply the Basic text normalizer to the decoded text. Applicable to multilingual
    target text.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`basic_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è§£ç æ–‡æœ¬åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–ã€‚é€‚ç”¨äºå¤šè¯­è¨€ç›®æ ‡æ–‡æœ¬ã€‚'
- en: '`remove_diacritics` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove diacritics when applying the Basic text normalizer. Removing diacritics
    may destroy information in the decoded text, hence it should be used with caution.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_diacritics`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨åº”ç”¨åŸºæœ¬æ–‡æœ¬è§„èŒƒåŒ–æ—¶åˆ é™¤å˜éŸ³ç¬¦å·ã€‚åˆ é™¤å˜éŸ³ç¬¦å·å¯èƒ½ä¼šç ´åè§£ç æ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œå› æ­¤åº”è°¨æ…ä½¿ç”¨ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰- å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The decoded sentence.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­ã€‚
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å­—ç¬¦ä¸²ä¸­çš„idsåºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨tokenizerå’Œè¯æ±‡è¡¨ï¼Œå¯é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚
- en: WhisperFeatureExtractor
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperFeatureExtractor
- en: '### `class transformers.WhisperFeatureExtractor`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L35)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L35)'
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_size` (`int`, defaults to 80) â€” The feature dimension of the extracted
    features.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_size`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º80ï¼‰- æå–ç‰¹å¾çš„ç‰¹å¾ç»´åº¦ã€‚'
- en: '`sampling_rate` (`int`, defaults to 16000) â€” The sampling rate at which the
    audio files should be digitalized expressed in hertz (Hz).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º16000ï¼‰- éŸ³é¢‘æ–‡ä»¶åº”æ•°å­—åŒ–çš„é‡‡æ ·ç‡ï¼Œä»¥èµ«å…¹ï¼ˆHzï¼‰è¡¨ç¤ºã€‚'
- en: '`hop_length` (`int`, defaults to 160) â€” Length of the overlaping windows for
    the STFT used to obtain the Mel Frequency coefficients.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hop_length`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º160ï¼‰- ç”¨äºè·å–æ¢…å°”é¢‘ç‡ç³»æ•°çš„STFTçš„é‡å çª—å£çš„é•¿åº¦ã€‚'
- en: '`chunk_length` (`int`, defaults to 30) â€” The maximum number of chuncks of `sampling_rate`
    samples used to trim and pad longer or shorter audio sequences.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_length`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º30ï¼‰- ç”¨äºä¿®å‰ªå’Œå¡«å……è¾ƒé•¿æˆ–è¾ƒçŸ­éŸ³é¢‘åºåˆ—çš„`sampling_rate`æ ·æœ¬çš„æœ€å¤§å—æ•°ã€‚'
- en: '`n_fft` (`int`, defaults to 400) â€” Size of the Fourier transform.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_fft`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º400ï¼‰- å‚…ç«‹å¶å˜æ¢çš„å¤§å°ã€‚'
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) â€” Padding value used
    to pad the audio. Should correspond to silences.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.0ï¼‰- ç”¨äºå¡«å……éŸ³é¢‘çš„å¡«å……å€¼ã€‚åº”å¯¹åº”äºé™éŸ³ã€‚'
- en: Constructs a Whisper feature extractor.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªWhisperç‰¹å¾æå–å™¨ã€‚
- en: This feature extractor inherits from [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç‰¹å¾æå–å™¨ç»§æ‰¿è‡ª[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚
- en: This class extracts mel-filter bank features from raw speech using a custom
    numpy implementation of the `Short Time Fourier Transform` which should match
    pytorchâ€™s `torch.stft` equivalent.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç±»ä½¿ç”¨è‡ªå®šä¹‰çš„numpyå®ç°ä»åŸå§‹è¯­éŸ³ä¸­æå–melæ»¤æ³¢å™¨ç»„ç‰¹å¾ï¼Œè¯¥å®ç°åº”ä¸pytorchçš„`torch.stft`ç­‰æ•ˆã€‚
- en: '#### `__call__`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L157)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/feature_extraction_whisper.py#L157)'
- en: '[PRE24]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    â€” The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_speech`ï¼ˆ`np.ndarray`ï¼Œ`List[float]`ï¼Œ`List[np.ndarray]`ï¼Œ`List[List[float]]`ï¼‰-
    è¦å¡«å……çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯numpyæ•°ç»„ï¼Œæµ®ç‚¹å€¼åˆ—è¡¨ï¼Œnumpyæ•°ç»„åˆ—è¡¨æˆ–æµ®ç‚¹å€¼åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¿…é¡»æ˜¯å•å£°é“éŸ³é¢‘ï¼Œä¸æ˜¯ç«‹ä½“å£°ï¼Œå³æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚'
- en: '`truncation` (`bool`, *optional*, default to `True`) â€” Activates truncation
    to cut input sequences longer than *max_length* to *max_length*.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰- æ¿€æ´»æˆªæ–­ä»¥å°†è¾“å…¥åºåˆ—æˆªæ–­ä¸º*max_length*ä»¥ä¸Šçš„é•¿åº¦ä¸º*max_length*ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*, defaults to None) â€” If set will pad
    the sequence to a multiple of the provided value.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸ºNoneï¼‰- å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚'
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºå¯ç”¨NVIDIAç¡¬ä»¶ä¸Šçš„Tensor Coresç‰¹åˆ«æœ‰ç”¨ï¼Œå…¶è®¡ç®—èƒ½åŠ›ä¸º`>= 7.5`ï¼ˆVoltaï¼‰ï¼Œæˆ–è€…å¯¹äºå—ç›Šäºåºåˆ—é•¿åº¦ä¸º128çš„TPUsã€‚
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractorâ€™s default.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šfeature_extractorçš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: For Whisper models, `attention_mask` should always be passed for batched inference,
    to avoid subtle bugs.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºWhisperæ¨¡å‹ï¼Œæ‰¹é‡æ¨ç†æ—¶åº”å§‹ç»ˆä¼ é€’`attention_mask`ï¼Œä»¥é¿å…ç»†å¾®çš„é”™è¯¯ã€‚
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰-
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`sampling_rate` (`int`, *optional*) â€” The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors and allow automatic speech recognition pipeline.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰- `raw_speech`è¾“å…¥é‡‡æ ·çš„é‡‡æ ·ç‡ã€‚å¼ºçƒˆå»ºè®®åœ¨å‰å‘è°ƒç”¨æ—¶ä¼ é€’`sampling_rate`ï¼Œä»¥é˜²æ­¢é™é»˜é”™è¯¯å¹¶å…è®¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµæ°´çº¿ã€‚'
- en: '`padding_value` (`float`, defaults to 0.0) â€” The value that is used to fill
    the padding values / vectors.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_value`ï¼ˆ`float`ï¼Œé»˜è®¤ä¸º0.0ï¼‰- ç”¨äºå¡«å……å¡«å……å€¼/å‘é‡çš„å€¼ã€‚'
- en: '`do_normalize` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    zero-mean unit-variance normalize the input. Normalizing can help to significantly
    improve the performance of the model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œé›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–ã€‚å½’ä¸€åŒ–å¯ä»¥å¸®åŠ©æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚'
- en: Main method to featurize and prepare for the model one or several sequence(s).
    Implementation uses PyTorch for the STFT computation if available, otherwise a
    slower NumPy based one.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—è¿›è¡Œç‰¹å¾åŒ–å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚å¦‚æœå¯ç”¨ï¼Œå®ç°ä½¿ç”¨PyTorchè¿›è¡ŒSTFTè®¡ç®—ï¼Œå¦åˆ™ä½¿ç”¨è¾ƒæ…¢çš„åŸºäºNumPyçš„æ–¹æ³•ã€‚
- en: WhisperProcessor
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperProcessor
- en: '### `class transformers.WhisperProcessor`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L23)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L23)'
- en: '[PRE25]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`feature_extractor` (`WhisperFeatureExtractor`) â€” An instance of [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor).
    The feature extractor is a required input.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor`ï¼ˆ`WhisperFeatureExtractor`ï¼‰â€” [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: '`tokenizer` (`WhisperTokenizer`) â€” An instance of [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    The tokenizer is a required input.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer`ï¼ˆ`WhisperTokenizer`ï¼‰â€” [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚'
- en: Constructs a Whisper processor which wraps a Whisper feature extractor and a
    Whisper tokenizer into a single processor.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªWhisperå¤„ç†å™¨ï¼Œå°†Whisperç‰¹å¾æå–å™¨å’ŒWhisperåˆ†è¯å™¨åŒ…è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚
- en: '[WhisperProcessor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor)
    offers all the functionalities of [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)
    and [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor.decode)
    for more information.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[WhisperProcessor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor)æä¾›äº†[WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)å’Œ[WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[**call**()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor.__call__)å’Œ[decode()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperProcessor.decode)ã€‚'
- en: '#### `__call__`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L49)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L49)'
- en: '[PRE26]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Forwards the `audio` argument to WhisperFeatureExtractorâ€™s [**call**()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)
    and the `text` argument to [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the doctsring of the above two methods for more information.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`audio`å‚æ•°è½¬å‘åˆ°WhisperFeatureExtractorçš„[**call**()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)ï¼Œå°†`text`å‚æ•°è½¬å‘åˆ°[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è¯·å‚é˜…ä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `from_pretrained`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)'
- en: '[PRE27]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” This can be either:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” è¿™å¯ä»¥æ˜¯ï¼š'
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„*æ¨¡å‹ID*ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“å†…ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…å‘½åç©ºé—´ä¸‹çš„ç”¨æˆ·æˆ–ç»„ç»‡åç§°ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)æ–¹æ³•ä¿å­˜çš„ç‰¹å¾æå–å™¨æ–‡ä»¶çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/`ã€‚
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
    **kwargs â€” Additional keyword arguments passed along to both [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)
    and `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`.
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿å­˜çš„ç‰¹å¾æå–å™¨JSON *æ–‡ä»¶*çš„è·¯å¾„æˆ–URLï¼Œä¾‹å¦‚ï¼Œ`./my_model_directory/preprocessor_config.json`ã€‚**kwargs
    â€” ä¼ é€’ç»™[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)å’Œ`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`çš„å…¶ä»–å…³é”®å­—å‚æ•°ã€‚
- en: Instantiate a processor associated with a pretrained model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: å®ä¾‹åŒ–ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³è”çš„å¤„ç†å™¨ã€‚
- en: This class method is simply calling the feature extractor [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained),
    image processor [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    and the tokenizer `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`
    methods. Please refer to the docstrings of the methods above for more information.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨ç‰¹å¾æå–å™¨çš„[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)ã€å›¾åƒå¤„ç†å™¨[ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)å’Œåˆ†è¯å™¨`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`æ–¹æ³•ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚
- en: '#### `save_pretrained`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_pretrained`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
- en: '[PRE28]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`save_directory` (`str` or `os.PathLike`) â€” Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€”è¦ä¿å­˜ç‰¹å¾æå–å™¨JSONæ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™å°†åˆ›å»ºè¯¥ç›®å½•ï¼‰ã€‚'
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) â€” Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`push_to_hub`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”ä¿å­˜æ¨¡å‹åæ˜¯å¦å°†å…¶æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¼ é€’ç»™[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Saves the attributes of this processor (feature extractor, tokenizerâ€¦) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)æ–¹æ³•é‡æ–°åŠ è½½å®ƒã€‚
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç±»æ–¹æ³•åªæ˜¯è°ƒç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)å’Œ[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)ã€‚è¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `batch_decode`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L83)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L83)'
- en: '[PRE29]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This method forwards all its arguments to WhisperTokenizerâ€™s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°WhisperTokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: '#### `decode`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L90)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/processing_whisper.py#L90)'
- en: '[PRE30]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This method forwards all its arguments to WhisperTokenizerâ€™s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°WhisperTokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚è¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚
- en: PytorchHide Pytorch content
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorchå†…å®¹
- en: WhisperModel
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperModel
- en: '### `class transformers.WhisperModel`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1488)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1488)'
- en: '[PRE31]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰â€”å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Whisper Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„Whisperæ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚
- en: '#### `forward`'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1563)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1563)'
- en: '[PRE32]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    sequence_length)`) â€” Float values mel features extracted from the raw speech waveform.
    Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio file
    into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile
    library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰-
    ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–melç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚é˜…[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing *SpecAugment* data augmentation on padding
    token indices. Mask values selected in `[0, 1]`:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œ*SpecAugment*æ•°æ®å¢å¼ºçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºæ ‡è®°æœªè¢«`æ©ç `ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºæ ‡è®°è¢«`æ©ç `ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[è§£ç å™¨è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#decoder-input-ids)'
- en: Whisper uses the `decoder_start_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Whisperä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: If you want to change padding behavior, you should read `modeling_whisper._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the BART paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æƒ³è¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[BARTè®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºå°†ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºå°†è§£ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºå°†äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`)
    `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨å¸Œæœ›æ›´å¤šåœ°æ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`) â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*optional*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel)
    forward method, overrides the `__call__` special method.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[WhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE33]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#### `_mask_input_features`'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `_mask_input_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1520)'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1520)'
- en: '[PRE34]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Masks extracted features along time axis and/or along feature axis according
    to [SpecAugment](https://arxiv.org/abs/1904.08779).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[SpecAugment](https://arxiv.org/abs/1904.08779)æ²¿æ—¶é—´è½´å’Œ/æˆ–ç‰¹å¾è½´æ©ç›–æå–çš„ç‰¹å¾ã€‚
- en: WhisperForConditionalGeneration
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperForConditionalGeneration
- en: '### `class transformers.WhisperForConditionalGeneration`'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1658)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1658)'
- en: '[PRE35]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The Whisper Model with a language modeling head. Can be used for automatic speech
    recognition. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„Whisperæ¨¡å‹ã€‚å¯ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1696)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1696)'
- en: '[PRE36]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    sequence_length)`) â€” Float values mel features extracted from the raw speech waveform.
    Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio file
    into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile
    library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰-
    ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º`List[float]`æˆ–`numpy.ndarray`çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–melç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚å‚è§[`call()`](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing *SpecAugment* data augmentation on padding
    token indices. Mask values selected in `[0, 1]`:'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œ*SpecAugment*æ•°æ®å¢å¼ºçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,
- en: 0 for tokens that are `masked`.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤º`è¢«æ©ç›–`çš„æ ‡è®°ã€‚
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨[WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)'
- en: Whisper uses the `decoder_start_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Whisperä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: If you want to change padding behavior, you should read `modeling_whisper._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the BART paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[BARTè®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,
- en: 0 indicates the head is `masked`.
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,
- en: 0 indicates the head is `masked`.
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç›–`,
- en: 0 indicates the head is `masked`.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç›–`ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`ï¼ˆ`tuple(tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼‰â€” å…ƒç»„åŒ…å«ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*å¯é€‰*æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€”
    å…ƒç»„ç”±é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`ç»„æˆï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼Œä»¥åŠ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„è¾“å…¥ï¼‰ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size,
    sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹å†…éƒ¨çš„åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the language modeling loss. Indices should either be in
    `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with
    indices set to `-100` are ignored (masked), the loss is only computed for the
    tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰`[0,
    ..., config.vocab_size]`æ ‡ç­¾çš„æ ‡è®°ã€‚'
- en: Returns
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€”
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºå’Œæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥è¾“å‡ºå’Œæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [WhisperForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '[WhisperForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ æ’­çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„é¢„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE37]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '#### `generate`'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/generation_whisper.py#L250)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/generation_whisper.py#L250)'
- en: '[PRE38]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.Tensor` of shape `(batch_size, feature_size, sequence_length)`,
    *optional*) â€” Float values of log-mel features extracted from the raw speech waveform.
    The raw speech waveform can be obtained by loading a `.flac` or `.wav` audio file
    into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile
    library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)
    for details.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆ`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰â€”
    æµ®ç‚¹å€¼çš„å¯¹æ•°æ¢…å°”ç‰¹å¾ï¼Œä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–æ¢…å°”ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)ã€‚'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) â€” The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)â€™s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`ï¼ˆ`~generation.GenerationConfig`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œç”Ÿæˆè°ƒç”¨çš„åŸºæœ¬å‚æ•°åŒ–çš„ç”Ÿæˆé…ç½®ã€‚ä¼ é€’ç»™ç”Ÿæˆçš„`**kwargs`ä¸`generation_config`çš„å±æ€§åŒ¹é…å°†è¦†ç›–å®ƒä»¬ã€‚å¦‚æœæœªæä¾›`generation_config`ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ï¼Œå…¶åŠ è½½ä¼˜å…ˆçº§å¦‚ä¸‹ï¼š1ï¼‰ä»`generation_config.json`æ¨¡å‹æ–‡ä»¶ä¸­ï¼Œå¦‚æœå­˜åœ¨ï¼›2ï¼‰ä»æ¨¡å‹é…ç½®ä¸­ã€‚è¯·æ³¨æ„ï¼ŒæœªæŒ‡å®šçš„å‚æ•°å°†ç»§æ‰¿[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)çš„é»˜è®¤å€¼ï¼Œåº”æ£€æŸ¥å…¶æ–‡æ¡£ä»¥å‚æ•°åŒ–ç”Ÿæˆã€‚'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) â€” Custom logits processors
    that complement the default logits processors built from arguments and generation
    config. If a logit processor is passed that is already created with the arguments
    or a generation config an error is thrown. This feature is intended for advanced
    users.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`ï¼ˆ`LogitsProcessorList`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰å¯¹æ•°å¤„ç†å™¨ï¼Œè¡¥å……ç”±å‚æ•°å’Œç”Ÿæˆé…ç½®æ„å»ºçš„é»˜è®¤å¯¹æ•°å¤„ç†å™¨ã€‚å¦‚æœä¼ é€’çš„å¯¹æ•°å¤„ç†å™¨å·²ç»ä½¿ç”¨å‚æ•°æˆ–ç”Ÿæˆé…ç½®åˆ›å»ºï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºé«˜çº§ç”¨æˆ·ã€‚'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) â€” Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a generation config. If a stopping criteria is passed that is already created
    with the arguments or a generation config an error is thrown. This feature is
    intended for advanced users.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria`ï¼ˆ`StoppingCriteriaList`ï¼Œ*å¯é€‰*ï¼‰â€” è‡ªå®šä¹‰åœæ­¢æ ‡å‡†ï¼Œè¡¥å……ç”±å‚æ•°å’Œç”Ÿæˆé…ç½®æ„å»ºçš„é»˜è®¤åœæ­¢æ ‡å‡†ã€‚å¦‚æœä¼ é€’çš„åœæ­¢æ ‡å‡†å·²ç»ä½¿ç”¨å‚æ•°æˆ–ç”Ÿæˆé…ç½®åˆ›å»ºï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºé«˜çº§ç”¨æˆ·ã€‚'
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    â€” If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments: the batch ID `batch_id` and `input_ids`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the batch ID `batch_id`
    and the previously generated tokens `inputs_ids`. This argument is useful for
    constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn`ï¼ˆ`Callable[[int, torch.Tensor], List[int]]`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœæä¾›ï¼Œæ­¤å‡½æ•°å°†åœ¨æ¯ä¸ªæ­¥éª¤å°†æŸæœç´¢é™åˆ¶ä¸ºä»…å…è®¸çš„æ ‡è®°ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä¸åº”ç”¨çº¦æŸã€‚æ­¤å‡½æ•°æ¥å—2ä¸ªå‚æ•°ï¼šæ‰¹æ¬¡ID`batch_id`å’Œ`input_ids`ã€‚å®ƒå¿…é¡»è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸‹ä¸€ä»£æ­¥éª¤çš„å…è®¸æ ‡è®°ï¼Œæ¡ä»¶æ˜¯æ‰¹æ¬¡ID`batch_id`å’Œå…ˆå‰ç”Ÿæˆçš„æ ‡è®°`inputs_ids`ã€‚æ­¤å‚æ•°å¯¹äºå—å‰ç¼€çº¦æŸçš„ç”Ÿæˆå¾ˆæœ‰ç”¨ï¼Œå¦‚[è‡ªå›å½’å®ä½“æ£€ç´¢](https://arxiv.org/abs/2010.00904)ä¸­æ‰€è¿°ã€‚'
- en: '`synced_gpus` (`bool`, *optional*, defaults to `False`) â€” Whether to continue
    running the while loop until max_length (needed for ZeRO stage 3)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`synced_gpus`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ç»§ç»­è¿è¡Œwhileå¾ªç¯ç›´åˆ°max_lengthï¼ˆå¯¹äºZeROé˜¶æ®µ3æ˜¯å¿…éœ€çš„ï¼‰'
- en: '`return_timestamps` (`bool`, *optional*) â€” Whether to return the timestamps
    with the text. This enables the `WhisperTimestampsLogitsProcessor`.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_timestamps`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ–‡æœ¬çš„æ—¶é—´æˆ³ã€‚è¿™å°†å¯ç”¨`WhisperTimestampsLogitsProcessor`ã€‚'
- en: '`task` (`str`, *optional*) â€” Task to use for generation, either â€œtranslateâ€
    or â€œtranscribeâ€. The `model.config.forced_decoder_ids` will be updated accordingly.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºç”Ÿæˆçš„ä»»åŠ¡ï¼Œå¯ä»¥æ˜¯â€œtranslateâ€æˆ–â€œtranscribeâ€ã€‚`model.config.forced_decoder_ids`å°†ç›¸åº”æ›´æ–°ã€‚'
- en: '`language` (`str`, *optional*) â€” Language token to use for generation, can
    be either in the form of `<|en|>`, `en` or `english`. You can find all the possible
    language tokens in the `model.generation_config.lang_to_id` dictionary.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`language`ï¼ˆ`str`ï¼Œ*optional*ï¼‰--ç”¨äºç”Ÿæˆçš„è¯­è¨€æ ‡è®°ï¼Œå¯ä»¥æ˜¯`<|en|>`ã€`en`æˆ–`english`å½¢å¼ã€‚æ‚¨å¯ä»¥åœ¨`model.generation_config.lang_to_id`å­—å…¸ä¸­æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„è¯­è¨€æ ‡è®°ã€‚'
- en: '`is_multilingual` (`bool`, *optional*) â€” Whether or not the model is multilingual.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_multilingual`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹æ˜¯å¦æ˜¯å¤šè¯­è¨€çš„ã€‚'
- en: '`prompt_ids` (`torch.Tensor`, *optional*) â€” Rank-1 tensor of token IDs created
    by passing text to `get_prompt_ids()` that is provided as a prompt to each chunk.
    This can be used to provide or â€œprompt-engineerâ€ a context for transcription,
    e.g. custom vocabularies or proper nouns to make it more likely to predict those
    words correctly. It cannot be used in conjunction with `decoder_start_token_id`
    as it overwrites this value.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_ids`ï¼ˆ`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é€šè¿‡å°†æ–‡æœ¬ä¼ é€’ç»™`get_prompt_ids()`åˆ›å»ºçš„ä»¤ç‰ŒIDçš„ç§©-1å¼ é‡ï¼Œä½œä¸ºæ¯ä¸ªå—çš„æç¤ºæä¾›ã€‚è¿™å¯ç”¨äºä¸ºè½¬å½•æä¾›æˆ–â€œæç¤ºå·¥ç¨‹â€ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚è‡ªå®šä¹‰è¯æ±‡æˆ–ä¸“æœ‰åè¯ï¼Œä»¥ä½¿å…¶æ›´æœ‰å¯èƒ½æ­£ç¡®é¢„æµ‹è¿™äº›å•è¯ã€‚å®ƒä¸èƒ½ä¸`decoder_start_token_id`ç»“åˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒä¼šè¦†ç›–æ­¤å€¼ã€‚'
- en: '`condition_on_prev_tokens` (`bool`, *optional*) â€” Only relevant for long-form
    transcription. Whether to condition each segment on the previous segment. As shown
    in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can
    help to improve performance.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`condition_on_prev_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚æ˜¯å¦å°†æ¯ä¸ªç‰‡æ®µçš„ç”Ÿæˆæ¡ä»¶è®¾ç½®ä¸ºå‰ä¸€ä¸ªç‰‡æ®µã€‚å¦‚[Whisperè®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚'
- en: '`temperature` (`float` or list of `float`, *optional*) â€” The temperature to
    be used for generation. Passing a single `float` value and `do_sample=True` activates
    generation using sampling. For long-form transcription, temperature fallback can
    be activated by passing a list of float values such as (0.0, 0.2, 0.4, 0.6, 0.8,
    1.0). As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf),
    this can help to improve performance.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float` æˆ– `float` åˆ—è¡¨ï¼Œ*å¯é€‰*) â€” ç”¨äºç”Ÿæˆçš„æ¸©åº¦ã€‚ä¼ é€’å•ä¸ª `float` å€¼å¹¶ä¸” `do_sample=True`
    ä¼šæ¿€æ´»ä½¿ç”¨é‡‡æ ·è¿›è¡Œç”Ÿæˆã€‚å¯¹äºé•¿ç¯‡è½¬å½•ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’ä¸€ç»„æµ®ç‚¹å€¼ï¼ˆä¾‹å¦‚ (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)ï¼‰æ¥æ¿€æ´»æ¸©åº¦å›é€€ã€‚æ­£å¦‚[Whisperè®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚'
- en: '`compression_ratio_threshold` (`float`, *optional*) â€” Only relevant for long-form
    transcription. If defined, the zlib compression rate of each segment will be computed.
    If the compression rate of a segment is higher than `compression_ratio_threshold`,
    temperature fallback is activated: the generated segment is discarded and the
    generation is repeated using a higher temperature. The intuition behind this feature
    is that segments with very high compression rates suffer from a lot of repetition.
    The unwanted repetition can be reduced by injecting more randomness by increasing
    the temperature. If `compression_ratio_threshold` is defined make sure that `temperature`
    is a list of values. A common value for `compression_ratio_threshold` is 1.35.
    As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf),
    this can help to improve performance.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compression_ratio_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œå°†è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„ zlib
    å‹ç¼©ç‡ã€‚å¦‚æœä¸€ä¸ªç‰‡æ®µçš„å‹ç¼©ç‡é«˜äº `compression_ratio_threshold`ï¼Œåˆ™æ¿€æ´»æ¸©åº¦å›é€€ï¼šç”Ÿæˆçš„ç‰‡æ®µè¢«ä¸¢å¼ƒï¼Œä½¿ç”¨æ›´é«˜çš„æ¸©åº¦é‡å¤ç”Ÿæˆã€‚è¿™ä¸ªç‰¹æ€§èƒŒåçš„ç›´è§‰æ˜¯ï¼Œå…·æœ‰éå¸¸é«˜å‹ç¼©ç‡çš„ç‰‡æ®µå­˜åœ¨å¤§é‡é‡å¤ã€‚é€šè¿‡å¢åŠ æ¸©åº¦æ³¨å…¥æ›´å¤šéšæœºæ€§å¯ä»¥å‡å°‘ä¸éœ€è¦çš„é‡å¤ã€‚å¦‚æœå®šä¹‰äº†
    `compression_ratio_threshold`ï¼Œè¯·ç¡®ä¿ `temperature` æ˜¯ä¸€ä¸ªå€¼åˆ—è¡¨ã€‚`compression_ratio_threshold`
    çš„å¸¸è§å€¼ä¸º 1.35ã€‚æ­£å¦‚[Whisperè®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚'
- en: '`logprob_threshold` (`float`, *optional*) â€” Only relevant for long-form transcription.
    If defined, the average log-probability of each segment will be computed. If the
    log-probability of a given segment is lower than `logprob_threshold`, temperature
    fallback is activated: the generated segment is discarded and the generation is
    repeated using a higher temperature. The intuition behind this feature is that
    segments of low log-probability can be improved by injecting more randomness by
    increasing the temperature. If `logprob_threshold` is defined make sure that `temperature`
    is a list of values. A common value for `logprob_threshold` is -1.0. As shown
    in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf), this can
    help to improve performance.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logprob_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œå°†è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„å¹³å‡å¯¹æ•°æ¦‚ç‡ã€‚å¦‚æœç»™å®šç‰‡æ®µçš„å¯¹æ•°æ¦‚ç‡ä½äº
    `logprob_threshold`ï¼Œåˆ™æ¿€æ´»æ¸©åº¦å›é€€ï¼šç”Ÿæˆçš„ç‰‡æ®µè¢«ä¸¢å¼ƒï¼Œä½¿ç”¨æ›´é«˜çš„æ¸©åº¦é‡å¤ç”Ÿæˆã€‚è¿™ä¸ªç‰¹æ€§èƒŒåçš„ç›´è§‰æ˜¯ï¼Œä½å¯¹æ•°æ¦‚ç‡çš„ç‰‡æ®µå¯ä»¥é€šè¿‡å¢åŠ æ¸©åº¦æ³¨å…¥æ›´å¤šéšæœºæ€§æ¥æ”¹å–„ã€‚å¦‚æœå®šä¹‰äº†
    `logprob_threshold`ï¼Œè¯·ç¡®ä¿ `temperature` æ˜¯ä¸€ä¸ªå€¼åˆ—è¡¨ã€‚`logprob_threshold` çš„å¸¸è§å€¼ä¸º -1.0ã€‚æ­£å¦‚[Whisperè®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚'
- en: '`no_speech_threshold` (`float`, *optional*) â€” Only relevant for long-form transcription.
    If defined, the â€œno-speechâ€ token combined with the `logprob_threshold` is used
    to determine whether a segment contains only silence. In this case, the transcription
    for this segment is skipped. As shown in the [the Whisper paper](https://cdn.openai.com/papers/whisper.pdf),
    this can help to improve performance.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_speech_threshold` (`float`, *å¯é€‰*) â€” ä»…é€‚ç”¨äºé•¿ç¯‡è½¬å½•ã€‚å¦‚æœå®šä¹‰äº†ï¼Œâ€œæ— è¯­éŸ³â€æ ‡è®°ä¸ `logprob_threshold`
    ç»“åˆä½¿ç”¨æ¥ç¡®å®šä¸€ä¸ªç‰‡æ®µæ˜¯å¦åªåŒ…å«é™éŸ³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†è·³è¿‡è¯¥ç‰‡æ®µçš„è½¬å½•ã€‚æ­£å¦‚[Whisperè®ºæ–‡](https://cdn.openai.com/papers/whisper.pdf)æ‰€ç¤ºï¼Œè¿™å¯ä»¥å¸®åŠ©æé«˜æ€§èƒ½ã€‚'
- en: '`num_segment_frames` (`int`, *optional*) â€” The number of frames a single segment
    is made of. If not defined, `num_segment_frames` defaults to the modelâ€™s stride
    times the maximum input length.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_segment_frames` (`int`, *å¯é€‰*) â€” å•ä¸ªç‰‡æ®µåŒ…å«çš„å¸§æ•°ã€‚å¦‚æœæœªå®šä¹‰ï¼Œ`num_segment_frames` é»˜è®¤ä¸ºæ¨¡å‹çš„æ­¥å¹…ä¹˜ä»¥æœ€å¤§è¾“å…¥é•¿åº¦ã€‚'
- en: '`attention_mask` (`torch.Tensor`, *optional*) â€” `attention_mask` needs to be
    passed when doing long-form transcription using a batch size > 1.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`, *å¯é€‰*) â€” åœ¨ä½¿ç”¨æ‰¹é‡å¤§å° > 1 è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶éœ€è¦ä¼ é€’ `attention_mask`ã€‚'
- en: '`time_precision` (`int`, *optional*, defaults to 0.02) â€” The duration of output
    token in seconds. *E.g.* 0.02 means that a generated token on average accounts
    for 20 ms.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_precision` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” è¾“å‡ºæ ‡è®°çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰ã€‚*ä¾‹å¦‚*ï¼Œ0.02 è¡¨ç¤ºç”Ÿæˆçš„æ ‡è®°å¹³å‡å æ®
    20 æ¯«ç§’ã€‚'
- en: '`return_token_timestamps` (`bool`, *optional*) â€” Whether to return token-level
    timestamps with the text. This can be used with or without the `return_timestamps`
    option. To get word-level timestamps, use the tokenizer to group the tokens into
    words.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_timestamps` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ–‡æœ¬çš„æ ‡è®°çº§æ—¶é—´æˆ³ã€‚å¯ä»¥ä¸ `return_timestamps`
    é€‰é¡¹ä¸€èµ·ä½¿ç”¨ã€‚è¦è·å¾—å•è¯çº§æ—¶é—´æˆ³ï¼Œè¯·ä½¿ç”¨åˆ†è¯å™¨å°†æ ‡è®°åˆ†ç»„æˆå•è¯ã€‚'
- en: '`return_segments` (`bool`, *optional*, defaults to `False`) â€” Whether to additionally
    return a list of all segments. Note that this option can only be enabled when
    doing long-form transcription.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_segments` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦é¢å¤–è¿”å›æ‰€æœ‰ç‰‡æ®µçš„åˆ—è¡¨ã€‚è¯·æ³¨æ„ï¼Œåªæœ‰åœ¨è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶æ‰èƒ½å¯ç”¨æ­¤é€‰é¡¹ã€‚'
- en: '`return_dict_in_generate` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of just returning the generated tokens. Note that when doing long-form
    transcription, `return_dict_in_generate` can only be enabled when `return_segments`
    is set True. In this case the generation outputs of each segment is added to each
    segment.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict_in_generate` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸ä»…ä»…è¿”å›ç”Ÿæˆçš„æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œåœ¨è¿›è¡Œé•¿ç¯‡è½¬å½•æ—¶ï¼Œåªæœ‰åœ¨è®¾ç½® `return_segments` ä¸º True æ—¶æ‰èƒ½å¯ç”¨ `return_dict_in_generate`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç‰‡æ®µçš„ç”Ÿæˆè¾“å‡ºå°†æ·»åŠ åˆ°æ¯ä¸ªç‰‡æ®µä¸­ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model. If the model is an encoder-decoder model, encoder specific
    kwargs should not be prefixed and decoder specific kwargs should be prefixed with
    *decoder_*.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰-`generate_config`çš„ç‰¹å®šäºç‰¹å®šæ¨¡å‹çš„å‚æ•°åŒ–å’Œ/æˆ–å…¶ä»–æ¨¡å‹ç‰¹å®škwargsï¼Œå°†è½¬å‘åˆ°æ¨¡å‹çš„`forward`å‡½æ•°ã€‚å¦‚æœæ¨¡å‹æ˜¯ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œåˆ™ä¸åº”ä»¥å‰ç¼€å½¢å¼æŒ‡å®šç¼–ç å™¨ç‰¹å®škwargsï¼Œè€Œåº”ä»¥*decoder_*ä¸ºå‰ç¼€æŒ‡å®šè§£ç å™¨ç‰¹å®škwargsã€‚'
- en: Returns
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor` or `Dict[str, Any]`'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)æˆ–`torch.LongTensor`æˆ–`Dict[str,
    Any]`'
- en: A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor` or a dict of segments when `return_segments=True`.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)ï¼ˆå¦‚æœ`return_dict_in_generate=True`æˆ–å½“`config.return_dict_in_generate=True`æ—¶ï¼‰æˆ–ä¸€ä¸ª`torch.FloatTensor`æˆ–ä¸€ä¸ªæ®µçš„å­—å…¸ï¼Œå½“`return_segments=True`æ—¶ã€‚
- en: If the passed input is > 30 seconds / > 3000 mel input features and `return_segments=True`
    then a dictionary of generated sequence ids, called `sequences` and a list of
    each generated segment is returned.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¼ å…¥çš„è¾“å…¥> 30ç§’/ > 3000 melè¾“å…¥ç‰¹å¾ï¼Œå¹¶ä¸”`return_segments=True`ï¼Œåˆ™è¿”å›ä¸€ä¸ªç”Ÿæˆçš„åºåˆ—idå­—å…¸ï¼Œç§°ä¸º`sequences`ï¼Œä»¥åŠæ¯ä¸ªç”Ÿæˆæ®µçš„åˆ—è¡¨ã€‚
- en: 'else if the passed input is <= 30 seconds / >= 3000 mel input features, the
    possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œå¦‚æœä¼ å…¥çš„è¾“å…¥<= 30ç§’/ >= 3000 melè¾“å…¥ç‰¹å¾ï¼Œåˆ™å¯èƒ½çš„[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)ç±»å‹ä¸ºï¼š
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput)ï¼Œ'
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
- en: else only the generated output sequence ids are returned.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™ï¼Œä»…è¿”å›ç”Ÿæˆçš„è¾“å‡ºåºåˆ—idã€‚
- en: Transcribes or translates log-mel input features to a sequence of auto-regressively
    generated token ids.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¯¹æ•°melè¾“å…¥ç‰¹å¾è½¬å½•æˆ–ç¿»è¯‘ä¸ºè‡ªå›å½’ç”Ÿæˆçš„ä»¤ç‰Œidåºåˆ—ã€‚
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the modelâ€™s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç”Ÿæˆæ§åˆ¶å‚æ•°éƒ½åœ¨`generation_config`ä¸­è®¾ç½®ï¼Œå¦‚æœæœªä¼ é€’ï¼Œåˆ™å°†è®¾ç½®ä¸ºæ¨¡å‹çš„é»˜è®¤ç”Ÿæˆé…ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†ç›¸åº”çš„å‚æ•°ä¼ é€’ç»™generate()æ¥è¦†ç›–ä»»ä½•`generation_config`ï¼Œä¾‹å¦‚`.generate(inputs,
    num_beams=4, do_sample=True)`ã€‚
- en: For an overview of generation strategies and code examples, check out the [following
    guide](./generation_strategies).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ç”Ÿæˆç­–ç•¥å’Œä»£ç ç¤ºä¾‹çš„æ¦‚è¿°ï¼Œè¯·æŸ¥çœ‹[ä»¥ä¸‹æŒ‡å—](./generation_strategies)ã€‚
- en: 'Example:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '*Longform transcription*: To transcribe or translate audios longer than 30
    seconds, process the audio files without truncation and pass all mel features
    at once to generate.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è¯¦ç»†è½¬å½•*ï¼šè¦è½¬å½•æˆ–ç¿»è¯‘è¶…è¿‡30ç§’çš„éŸ³é¢‘ï¼Œè¯·å¤„ç†éŸ³é¢‘æ–‡ä»¶è€Œä¸æˆªæ–­ï¼Œå¹¶ä¸€æ¬¡ä¼ é€’æ‰€æœ‰melç‰¹å¾ä»¥ç”Ÿæˆã€‚'
- en: '[PRE39]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '*Shortform transcription*: If passed mel input features are < 30 seconds, the
    whole audio will be transcribed with a single call to generate.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç®€åŒ–è½¬å½•*ï¼šå¦‚æœä¼ å…¥çš„melè¾“å…¥ç‰¹å¾< 30ç§’ï¼Œåˆ™æ•´ä¸ªéŸ³é¢‘å°†é€šè¿‡ä¸€æ¬¡è°ƒç”¨ç”Ÿæˆè¿›è¡Œè½¬å½•ã€‚'
- en: '[PRE40]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: WhisperForCausalLM
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperForCausalLM
- en: '### `class transformers.WhisperForCausalLM`'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperForCausalLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1865)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1865)'
- en: '[PRE41]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰-æ¨¡å‹é…ç½®ç±»ï¼Œå…·æœ‰æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: Whisper decoder with with a language modeling head on top (linear layer with
    weights tied to the input embeddings).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: Whisperè§£ç å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼Œå…¶æƒé‡ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼‰ã€‚
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥å¤§å°ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚
- en: '#### `forward`'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1903)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L1903)'
- en: '[PRE42]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€”
    è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥
    IDï¼Ÿ](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š'
- en: 1 for tokens that are `not masked`,
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºæ ‡è®°æ˜¯ `not masked`ï¼Œ
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºæ ‡è®°æ˜¯ `masked`ã€‚[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)
- en: '`encoder_outputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder. Used in the cross-attention if the model is configured as
    a decoder.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚'
- en: '`head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules. Mask values
    selected in `[0, 1]`:'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” ç”¨äºä½¿æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]` ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-516
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-517
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.
    The two additional tensors are only required when the model is used as a decoder
    in a Sequence to Sequence model. Contains pre-computed hidden-states (key and
    values in the self-attention blocks and in the cross-attention blocks) that can
    be used (see `past_key_values` input) to speed up sequential decoding. If `past_key_values`
    are used, the user can optionally input only the last `decoder_input_ids` (those
    that donâ€™t have their past key value states given to this model) of shape `(batch_size,
    1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’ `use_cache=True`
    æˆ– `config.use_cache=True` æ—¶è¿”å›) â€” é•¿åº¦ä¸º `config.n_layers` çš„ `tuple(torch.FloatTensor)`
    çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, embed_size_per_head)`
    çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`
    çš„å¼ é‡ã€‚å½“æ¨¡å‹ç”¨ä½œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„è§£ç å™¨æ—¶ï¼Œåªæœ‰è¿™ä¸¤ä¸ªé¢å¤–çš„å¼ é‡æ˜¯å¿…éœ€çš„ã€‚åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§
    `past_key_values` è¾“å…¥ï¼‰ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º `(batch_size, 1)`
    çš„ `decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™è¯¥æ¨¡å‹çš„æ ‡è®°ï¼‰è€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º `(batch_size, sequence_length)`
    çš„ `decoder_input_ids`ã€‚'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°† `input_ids`
    ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the masked language modeling loss. Indices should either
    be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨ `[0, ..., config.vocab_size]` æˆ– -100ï¼ˆå‚è§ `input_ids` æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º
    `-100` çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨ `[0, ..., config.vocab_size]` ä¸­çš„æ ‡è®°ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™ä¼šè¿”å› `past_key_values` é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§
    `past_key_values`ï¼‰ã€‚'
- en: 1 for tokens that are `not masked`,
  id: totrans-522
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 è¡¨ç¤ºæ ‡è®°æ˜¯ `not masked`ï¼Œ
- en: 0 for tokens that are `masked`.
  id: totrans-523
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 è¡¨ç¤ºæ ‡è®°æ˜¯ `masked`ã€‚
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„
    `attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or `tuple(torch.FloatTensor)`'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss (for next-token prediction).'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æŸå¤±` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚'
- en: Cross attentions weights after the attention softmax, used to compute the weighted
    average in the cross-attention heads.
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `torch.FloatTensor`
    tuples of length `config.n_layers`, with each tuple containing the cached key,
    value states of the self-attention and the cross-attention layers if model is
    used in encoder-decoder setting. Only relevant if `config.is_decoder = True`.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`torch.FloatTensor`å…ƒç»„çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›å±‚çš„ç¼“å­˜é”®ã€å€¼çŠ¶æ€ï¼Œå¦‚æœæ¨¡å‹ç”¨äºç¼–ç å™¨-è§£ç å™¨è®¾ç½®ï¼Œåˆ™ç›¸å…³ã€‚ä»…åœ¨`config.is_decoder
    = True`æ—¶ç›¸å…³ã€‚'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    that can be used (see `past_key_values` input) to speed up sequential decoding.
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆæŸ¥çœ‹`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: 'Example:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE43]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: WhisperForAudioClassification
  id: totrans-542
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WhisperForAudioClassification
- en: '### `class transformers.WhisperForAudioClassification`'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.WhisperForAudioClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2085)'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2085)'
- en: '[PRE44]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    sequence_length)`) â€” Float values mel features extracted from the raw speech waveform.
    Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio file
    into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile
    library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`)
    â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–melç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*optional*)
    â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-549
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*optional*:
    `hidden_states`ï¼Œ*optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Whisper Encoder Model with a sequence classification head on top (a linear layer
    over the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰é¡¶éƒ¨åºåˆ—åˆ†ç±»å¤´éƒ¨ï¼ˆåœ¨æ±‡èšè¾“å‡ºä¸Šçš„çº¿æ€§å±‚ï¼‰çš„Whisperç¼–ç å™¨æ¨¡å‹ï¼Œç”¨äºç±»ä¼¼SUPERBå…³é”®è¯è¯†åˆ«çš„ä»»åŠ¡ã€‚
- en: '#### `forward`'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2119)'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_whisper.py#L2119)'
- en: '[PRE45]'
  id: totrans-558
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, feature_size,
    sequence_length)`) â€” Float values mel features extracted from the raw speech waveform.
    Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio file
    into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile
    library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the mel features, padding and conversion into a
    tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`)
    â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–melç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`ï¼Œ*optional*)
    â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬(`last_hidden_state`ï¼Œ*optional*:
    `hidden_states`ï¼Œ*optional*: `attentions`) `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*optional*)æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨
    `[0, ..., config.num_labels - 1]` ä¸­ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ
    `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚'
- en: Returns
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ– `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False`
    æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Classification (or regression if config.num_labels==1) loss.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, å½“æä¾› `labels` æ—¶è¿”å›)
    â€” åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ config.num_labels==1ï¼‰æŸå¤±ã€‚'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) â€”
    åˆ†ç±»ï¼ˆæˆ–å›å½’ï¼Œå¦‚æœ config.num_labels==1ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True`
    æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length,
    hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True`
    æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length,
    sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› SoftMax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '[WhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperForAudioClassification)
    çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE46]'
  id: totrans-581
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: TensorFlowHide TensorFlow content
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowHide TensorFlow å†…å®¹
- en: TFWhisperModel
  id: totrans-583
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWhisperModel
- en: '### `class transformers.TFWhisperModel`'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWhisperModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1230)'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1230)'
- en: '[PRE47]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The bare Whisper Model outputting raw hidden-states without any specific head
    on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸çš„ Whisper æ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: '#### `call`'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1258)'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1258)'
- en: '[PRE48]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`)
    â€” Float values of fbank features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `tf.Tensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`tf.Tensor`ï¼‰-
    ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„fbankç‰¹å¾çš„æµ®ç‚¹å€¼ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–fbankç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`tf.Tensor`ç±»å‹çš„å¼ é‡ã€‚è¯·å‚è§[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using `SpeechToTextTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨`SpeechToTextTokenizer`è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯decoder input IDsï¼Ÿ](../glossary#decoder-input-ids)'
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SpeechToTextä½¿ç”¨`eos_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰æ‹©åªè¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚
- en: '`decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœè’™ç‰ˆä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: If you want to change padding behavior, you should read `modeling_whisper._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œè¯·é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: '`head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`ï¼ˆå½¢çŠ¶ä¸º`(encoder_layers, encoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºåœ¨ç¼–ç å™¨ä¸­ä½¿é€‰å®šæ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿é€‰å®šæ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-607
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`ï¼ˆå½¢çŠ¶ä¸º`(decoder_layers, decoder_attention_heads)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„è’™ç‰ˆã€‚è’™ç‰ˆå€¼é€‰å®šåœ¨`[0, 1]`ä¸­ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-609
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-610
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«é®è”½ã€‚
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) â€” Tuple consists of
    (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`
    of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence
    of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
    of the decoder.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`ï¼ˆ`tuple(tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼‰- å…ƒç»„åŒ…å«(`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`)
    `last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” Tuple of `tuple(tf.Tensor)` of length
    `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads,
    sequence_length, embed_size_per_head)`) and 2 additional tensors of shape `(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`tuple(tuple(tf.Tensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰-
    é•¿åº¦ä¸º`config.n_layers`çš„`tuple(tf.Tensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚
- en: '`decoder_inputs_embeds` (`tf.Tensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.modeling_tf_outputs.TFSeq2SeqModelOutput`æˆ–`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    â€” Sequence of hidden-states at the output of the last layer of the decoder of
    the model.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰â€”
    æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™ä»…è¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€”
    é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`ã€‚'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«è§£ç å™¨çš„é¢„è®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€”
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFWhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.TFWhisperModel)
    forward method, overrides the `__call__` special method.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWhisperModel](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.TFWhisperModel)çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE49]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: TFWhisperForConditionalGeneration
  id: totrans-642
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFWhisperForConditionalGeneration
- en: '### `class transformers.TFWhisperForConditionalGeneration`'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFWhisperForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1346)'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1346)'
- en: '[PRE50]'
  id: totrans-645
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Parameters
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰â€”
    åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: The Whisper Model with a language modeling head. Can be used for automatic speech
    recognition. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„Whisperæ¨¡å‹ã€‚å¯ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF
    2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚
- en: '#### `call`'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1381)'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_tf_whisper.py#L1381)'
- en: '[PRE51]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Parameters
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`tf.Tensor` of shape `(batch_size, feature_size, sequence_length)`)
    â€” Float values of fbank features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `tf.Tensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`tf.Tensor`ï¼‰â€”
    ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„fbankç‰¹å¾çš„æµ®ç‚¹å€¼ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨[AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)æ¥æå–fbankç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸º`tf.Tensor`ç±»å‹çš„å¼ é‡ã€‚å‚è§[`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”
    è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚'
- en: Indices can be obtained using `SpeechToTextTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨`SpeechToTextTokenizer`è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)'
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SpeechToTextä½¿ç”¨`eos_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚
- en: '`decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚'
- en: If you want to change padding behavior, you should read `modeling_whisper._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_whisper._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚
- en: '`head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) â€” ç”¨äºä½¿ç¼–ç å™¨ä¸­æ³¨æ„åŠ›æ¨¡å—çš„é€‰æ‹©å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-662
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-663
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚
- en: '`decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” ç”¨äºåœ¨è§£ç å™¨ä¸­ä½¿é€‰æ‹©çš„æ³¨æ„åŠ›æ¨¡å—çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-665
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-666
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚
- en: '`cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) â€” ç”¨äºä½¿äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„é€‰æ‹©å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`èŒƒå›´å†…ï¼š'
- en: 1 indicates the head is `not masked`,
  id: totrans-668
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1è¡¨ç¤ºå¤´éƒ¨æœªè¢«å±è”½ï¼Œ
- en: 0 indicates the head is `masked`.
  id: totrans-669
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0è¡¨ç¤ºå¤´éƒ¨è¢«å±è”½ã€‚
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) â€” Tuple consists of
    (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state`
    of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence
    of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
    of the decoder.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) â€” å…ƒç»„åŒ…æ‹¬ï¼ˆ`last_hidden_state`ï¼Œ*å¯é€‰*ï¼š`hidden_states`ï¼Œ*å¯é€‰*ï¼š`attentions`ï¼‰`last_hidden_state`çš„å½¢çŠ¶ä¸º`(batch_size,
    sequence_length, hidden_size)`ï¼Œ*å¯é€‰*ï¼‰æ˜¯ç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚ç”¨äºè§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ã€‚'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” Tuple of `tuple(tf.Tensor)` of length
    `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size, num_heads,
    sequence_length, embed_size_per_head)`) and 2 additional tensors of shape `(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(tf.Tensor))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(tf.Tensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-672
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that donâ€™t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-673
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æœªå°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size,
    1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚
- en: '`decoder_inputs_embeds` (`tf.Tensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`tf.Tensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`use_cache` (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Labels for computing the language modeling loss. Indices should either be in
    `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with
    indices set to `-100` are ignored (masked), the loss is only computed for the
    tokens with labels in `[0, ..., config.vocab_size]`.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºè®¡ç®—è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0,
    ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰`[0,
    ..., config.vocab_size]`æ ‡ç­¾çš„æ ‡è®°ã€‚'
- en: Returns
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)æˆ–`tuple(tf.Tensor)`'
- en: A [transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) â€” Language modeling loss.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼Œå…¶ä¸­næ˜¯æœªå±è”½æ ‡ç­¾çš„æ•°é‡ï¼‰- è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰-
    è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`ï¼ˆ`List[tf.Tensor]`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰-
    é•¿åº¦ä¸º`config.n_layers`çš„`tf.Tensor`åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡å½¢çŠ¶ä¸º`(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`ã€‚'
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-686
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«è§£ç å™¨çš„é¢„å…ˆè®¡ç®—éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-690
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡ã€‚
- en: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡ã€‚
- en: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰-
    æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰-
    å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-695
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨æ¯å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-697
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [TFWhisperForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.TFWhisperForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFWhisperForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.TFWhisperForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE52]'
  id: totrans-701
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: JAXHide JAX content
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: JAXHide JAX content
- en: FlaxWhisperModel
  id: totrans-703
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWhisperModel
- en: '### `class transformers.FlaxWhisperModel`'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWhisperModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1185)'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1185)'
- en: '[PRE53]'
  id: totrans-706
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Parameters
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs). This can be used to enable mixed-precision
    training or half-precision inference on GPUs or TPUs. If specified all the computation
    will be performed with the given `dtype`. **Note that this only specifies the
    dtype of the computation and does not influence the dtype of model parameters.**
    If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„dtypeï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„dtypeã€‚**å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„dtypeï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚'
- en: 'The bare Whisper Model transformer outputting raw hidden-states without any
    specific head on top. This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.) This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior. Finally, this model supports
    inherent JAX features such as:'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: è£¸Whisperæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯Flax
    Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå›ºæœ‰çš„JAXåŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)'
- en: '[PRE54]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`)
    â€” Float values mel features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)
    should be used for extracting the features, padding and conversion into a tensor
    of type `numpy.ndarray`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`çš„`numpy.ndarray`ï¼‰-
    ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼melç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º`List[float]`æˆ–`numpy.ndarray`çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_features`ï¼Œåº”ä½¿ç”¨[WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)æ¥æå–ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸ºç±»å‹ä¸º`numpy.ndarray`çš„å¼ é‡ã€‚å‚è§[`call()`](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Whisper does not support masking of the `input_features`, this argument
    is preserved for compatibility, but is not used. By default the silence in the
    input log mel spectrogram are ignored.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰-
    Whisperä¸æ”¯æŒå¯¹`input_features`è¿›è¡Œæ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°melé¢‘è°±å›¾ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚'
- en: '`decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary. Indices
    can be obtained using [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids) Whisper
    uses the `decoder_start_token_id` as the starting token for `decoder_input_ids`
    generation.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰-
    è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨[WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call()`](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)
    Whisperä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚'
- en: '`decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default. If you want to change padding behavior,
    you should modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰-
    é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚'
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Whisper does not use `position_ids` in the encoder as `input_features` is always
    the same size and doesnâ€™t use masking, but this argument is preserved for compatibility.
    By default the silence in the input log mel spectrogram are ignored.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- Whisperåœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨`position_ids`ï¼Œå› ä¸º`input_features`å§‹ç»ˆå…·æœ‰ç›¸åŒçš„å¤§å°ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†æ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°melé¢‘è°±å›¾ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚'
- en: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰-
    æ¯ä¸ªè§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings - 1]`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥ã€‚'
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) â€” Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`)
    â€” æ¨¡å‹è§£ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-732
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™åªè¾“å‡ºå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„åºåˆ—çš„æœ€åéšè—çŠ¶æ€ã€‚
- en: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” Tuple of `tuple(jnp.ndarray)` of
    length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *å¯é€‰çš„*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-734
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚
- en: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-736
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-738
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`,
    *å¯é€‰çš„*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-743
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºæ—¶çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰çš„*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-745
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The `FlaxWhisperPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWhisperPreTrainedModel`çš„å‰å‘æ–¹æ³•è¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Example:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¤ºä¾‹:'
- en: '[PRE55]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: FlaxWhisperForConditionalGeneration
  id: totrans-750
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWhisperForConditionalGeneration
- en: '### `class transformers.FlaxWhisperForConditionalGeneration`'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWhisperForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1267)'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1267)'
- en: '[PRE56]'
  id: totrans-753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Parameters
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs). This can be used to enable mixed-precision
    training or half-precision inference on GPUs or TPUs. If specified all the computation
    will be performed with the given `dtype`. **Note that this only specifies the
    dtype of the computation and does not influence the dtype of model parameters.**
    If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`, *å¯é€‰*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚**å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚'
- en: 'The Whisper Model with a language modeling head. This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.) This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior. Finally, this model supports
    inherent JAX features such as:'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„Whisperæ¨¡å‹ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯Flax
    Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥äº†è§£ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„JAXåŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1134)'
- en: '[PRE57]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Parameters
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`)
    â€” Float values mel features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)
    should be used for extracting the features, padding and conversion into a tensor
    of type `numpy.ndarray`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, feature_size, sequence_length)`)
    â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼æ¢…å°”ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip
    install soundfile`ï¼‰ã€‚è¦å‡†å¤‡æ•°ç»„ä¸º`input_features`ï¼Œåº”ä½¿ç”¨[WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)æ¥æå–ç‰¹å¾ã€å¡«å……å’Œè½¬æ¢ä¸º`numpy.ndarray`ç±»å‹çš„å¼ é‡ã€‚å‚è§[`call()`](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)ã€‚'
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Whisper does not support masking of the `input_features`, this argument
    is preserved for compatibility, but is not used. By default the silence in the
    input log mel spectrogram are ignored.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`numpy.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*)
    â€” Whisperä¸æ”¯æŒ`input_features`çš„æ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™ä»¥ç¡®ä¿å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°æ¢…å°”é¢‘è°±ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚'
- en: '`decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary. Indices
    can be obtained using [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids) Whisper
    uses the `decoder_start_token_id` as the starting token for `decoder_input_ids`
    generation.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨[WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚Whisperä½¿ç”¨`decoder_start_token_id`ä½œä¸º`decoder_input_ids`ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚'
- en: '`decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default. If you want to change padding behavior,
    you should modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚'
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Whisper does not use `position_ids` in the encoder as `input_features` is always
    the same size and doesnâ€™t use masking, but this argument is preserved for compatibility.
    By default the silence in the input log mel spectrogram are ignored.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Whisperåœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨`position_ids`ï¼Œå› ä¸º`input_features`å§‹ç»ˆå…·æœ‰ç›¸åŒçš„å¤§å°å¹¶ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™äº†è¿™ä¸ªå‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°æ¢…å°”é¢‘è°±ä¸­çš„é™éŸ³å°†è¢«å¿½ç•¥ã€‚'
- en: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º`[0, config.max_position_embeddings -
    1]`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥è€Œç»„æˆçš„å„ç§å…ƒç´ çš„[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰ã€‚
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚'
- en: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) â€” Tuple of `tuple(jnp.ndarray)` of
    length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, å½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›)
    â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(jnp.ndarray)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„å…·æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ2ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`çš„å¼ é‡ã€‚'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-780
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚
- en: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º +
    ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-782
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-784
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`cross_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-786
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—äº¤å‰æ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: '`encoder_last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length,
    hidden_size)`ï¼Œ*å¯é€‰*) â€” æ¨¡å‹ç¼–ç å™¨æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚'
- en: '`encoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  id: totrans-789
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`encoder_attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-791
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œåœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The `FlaxWhisperPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '`FlaxWhisperPreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Transcription example:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬å½•ç¤ºä¾‹ï¼š
- en: '[PRE58]'
  id: totrans-795
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: FlaxWhisperForAudioClassification
  id: totrans-796
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxWhisperForAudioClassification
- en: '### `class transformers.FlaxWhisperForAudioClassification`'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.FlaxWhisperForAudioClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1597)'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1597)'
- en: '[PRE59]'
  id: totrans-799
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`config` ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰
    â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚'
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) â€”
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs). This can be used to enable mixed-precision
    training or half-precision inference on GPUs or TPUs. If specified all the computation
    will be performed with the given `dtype`. **Note that this only specifies the
    dtype of the computation and does not influence the dtype of model parameters.**
    If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¸­çš„ä¸€ç§ã€‚è¿™å¯ä»¥ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚**è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚**å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚'
- en: 'The Whisper Model with an audio classification head on top. This model inherits
    from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.) This model is also a Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    subclass. Use it as a regular Flax Module and refer to the Flax documentation
    for all matter related to general usage and behavior. Finally, this model supports
    inherent JAX features such as:'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰é¡¶éƒ¨éŸ³é¢‘åˆ†ç±»å¤´çš„ Whisper æ¨¡å‹ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚æ­¤æ¨¡å‹è¿˜æ˜¯
    Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)
    å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒå†…åœ¨çš„ JAX åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ (JIT) ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: '#### `__call__`'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1625)'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/whisper/modeling_flax_whisper.py#L1625)'
- en: '[PRE60]'
  id: totrans-810
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Parameters
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`input_features` (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`)
    â€” Float values mel features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)
    should be used for extracting the features, padding and conversion into a tensor
    of type `numpy.ndarray`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_features` (`numpy.ndarray` of shape `(batch_size, feature_size, sequence_length)`)
    â€” ä»åŸå§‹è¯­éŸ³æ³¢å½¢ä¸­æå–çš„æµ®ç‚¹å€¼ mel ç‰¹å¾ã€‚åŸå§‹è¯­éŸ³æ³¢å½¢å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]`
    æˆ– `numpy.ndarray` çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (`pip install soundfile`)ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_features`ï¼Œåº”ä½¿ç”¨
    [WhisperFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor)
    æ¥æå–ç‰¹å¾ï¼Œå¡«å……å¹¶è½¬æ¢ä¸ºç±»å‹ä¸º `numpy.ndarray` çš„å¼ é‡ã€‚å‚è§ [`call`()](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperFeatureExtractor.__call__)'
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Whisper does not support masking of the `input_features`, this argument
    is preserved for compatibility, but is not used. By default the silence in the
    input log mel spectrogram are ignored.'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *å¯é€‰*) â€” Whisper ä¸æ”¯æŒå¯¹ `input_features` è¿›è¡Œæ©ç ï¼Œæ­¤å‚æ•°ä¿ç•™äº†å…¼å®¹æ€§ï¼Œä½†ä¸ä¼šä½¿ç”¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•° mel é¢‘è°±å›¾ä¸­çš„é™éŸ³ä¼šè¢«å¿½ç•¥ã€‚'
- en: '`decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary. Indices
    can be obtained using [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids) Whisper
    uses the `decoder_start_token_id` as the starting token for `decoder_input_ids`
    generation.'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *å¯é€‰*) â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥ä½¿ç”¨ [WhisperTokenizer](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperTokenizer)
    è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥
    IDï¼Ÿ](../glossary#decoder-input-ids) Whisper ä½¿ç”¨ `decoder_start_token_id` ä½œä¸º `decoder_input_ids`
    ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚'
- en: '`decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default. If you want to change padding behavior,
    you should modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`numpy.ndarray` of shape `(batch_size, target_sequence_length)`,
    *å¯é€‰*) â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥ `decoder_input_ids` ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚é»˜è®¤æƒ…å†µä¸‹è¿˜å°†ä½¿ç”¨å› æœæ©ç ã€‚å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§
    [è®ºæ–‡](https://arxiv.org/abs/1910.13461) ä¸­çš„å›¾è¡¨ 1ã€‚'
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    â€” Whisper does not use `position_ids` in the encoder as `input_features` is always
    the same size and doesnâ€™t use masking, but this argument is preserved for compatibility.
    By default the silence in the input log mel spectrogram are ignored.'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*)
    â€” Whisper åœ¨ç¼–ç å™¨ä¸­ä¸ä½¿ç”¨ `position_ids`ï¼Œå› ä¸º `input_features` æ€»æ˜¯ç›¸åŒå¤§å°ä¸”ä¸ä½¿ç”¨æ©ç ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™äº†æ­¤å‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å…¥å¯¹æ•°
    mel é¢‘è°±å›¾ä¸­çš„é™éŸ³ä¼šè¢«å¿½ç•¥ã€‚'
- en: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Indices of positions of each decoder input sequence tokens in the
    position embeddings. Selected in the range `[0, config.max_position_embeddings
    - 1]`.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *å¯é€‰*) â€” æ¯ä¸ªè§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´ä¸º `[0, config.max_position_embeddings - 1]`ã€‚'
- en: '`output_attentions` (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚'
- en: '`output_hidden_states` (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚'
- en: '`return_dict` (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚'
- en: Returns
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)æˆ–`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig))
    and inputs.
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[WhisperConfig](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.WhisperConfig)ï¼‰å’Œè¾“å…¥ã€‚
- en: '`logits` (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) â€” Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`) â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚'
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º +
    ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›)
    â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚
- en: The [FlaxWhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.FlaxWhisperForAudioClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '[FlaxWhisperForAudioClassification](/docs/transformers/v4.37.2/en/model_doc/whisper#transformers.FlaxWhisperForAudioClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚
- en: 'Transcription example:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬å½•ç¤ºä¾‹ï¼š
- en: '[PRE61]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
