- en: Understanding how big of a model can fit on your machine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator](https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: One very difficult aspect when exploring potential models to use on your machine
    is knowing just how big of a model will *fit* into memory with your current graphics
    card (such as loading the model onto CUDA).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: To help alleviate this, 🤗 Accelerate has a CLI interface through `accelerate
    estimate-memory`. This tutorial will help walk you through using it, what to expect,
    and at the end link to the interactive demo hosted on the 🤗 Hub which will even
    let you post those results directly on the model repo!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Currently we support searching for models that can be used in `timm` and `transformers`.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: This API will load the model into memory on the `meta` device, so we are not
    actually downloading and loading the full weights of the model into memory, nor
    do we need to. As a result it’s perfectly fine to measure 8 billion parameter
    models (or more), without having to worry about if your CPU can handle it!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Gradio Demos
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are a few gradio demos related to what was described above. The first
    is the official Hugging Face memory estimation space, utilizing Accelerate directly:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=light](https://hf-accelerate-model-memory-usage.hf.space?__theme=light)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=dark](https://hf-accelerate-model-memory-usage.hf.space?__theme=dark)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A community member has taken the idea and expended it further, allowing you
    to filter models directly and see if you can run a particular LLM given GPU constraints
    and LoRA configurations. To play with it, see [here](https://huggingface.co/spaces/Vokturz/can-it-run-llm)
    for more details.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The Command
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using `accelerate estimate-memory`, you need to pass in the name of the
    model you want to use, potentially the framework that model utilizing (if it can’t
    be found automatically), and the data types you want the model to be loaded in
    with.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is how we can calculate the memory footprint for `bert-base-cased`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will download the `config.json` for `bert-based-cased`, load the model
    on the `meta` device, and report back how much space it will use:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory Usage for loading `bert-base-cased`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 418.18 MB | 1.61 GB |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| int8 | 21.24 MB | 103.29 MB | 413.18 MB |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| int4 | 10.62 MB | 51.65 MB | 206.59 MB |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: By default it will return all the supported dtypes (`int4` through `float32`),
    but if you are interested in specific ones these can be filtered.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Specific libraries
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the source library cannot be determined automatically (like it could in the
    case of `bert-base-cased`), a library name can be passed in.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Memory Usage for loading `HuggingFaceM4/idefics-80b-instruct`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| float32 | 3.02 GB | 297.12 GB | 1.16 TB |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| float16 | 1.51 GB | 148.56 GB | 594.24 GB |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| int8 | 772.52 MB | 74.28 GB | 297.12 GB |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| int4 | 386.26 MB | 37.14 GB | 148.56 GB |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Memory Usage for loading `timm/resnet50.a1_in1k`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| float32 | 9.0 MB | 97.7 MB | 390.78 MB |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| float16 | 4.5 MB | 48.85 MB | 195.39 MB |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| int8 | 2.25 MB | 24.42 MB | 97.7 MB |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| int4 | 1.12 MB | 12.21 MB | 48.85 MB |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: Specific dtypes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, while we return `int4` through `float32` by default, any
    dtype can be used from `float32`, `float16`, `int8`, and `int4`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, pass them in after specifying `--dtypes`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Memory Usage for loading `bert-base-cased`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 413.18 MB | 1.61 GB |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
- en: Caveats with this calculator
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这个计算器的注意事项
- en: This calculator will tell you how much memory is needed to purely load the model
    in, *not* to perform inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算器将告诉您加载模型所需的内存量，*而不是*执行推断。
- en: This calculation is accurate within a few % of the actual value, so it is a
    very good view of just how much memory it will take. For instance loading `bert-base-cased`
    actually takes `413.68 MB` when loaded on CUDA in full precision, and the calculator
    estimates `413.18 MB`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算准确度在实际值的几个百分点内，因此它非常好地展示了需要多少内存。例如，完整精度下加载`bert-base-cased`实际上需要`413.68
    MB`，而计算器估计为`413.18 MB`。
- en: When performing inference you can expect to add up to an additional 20% as found
    by [EleutherAI](https://blog.eleuther.ai/transformer-math/). We’ll be conducting
    research into finding a more accurate estimate to these values, and will update
    this calculator once done.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行推断时，您可以期望额外增加高达20%，这是由[EleutherAI](https://blog.eleuther.ai/transformer-math/)发现的。我们将进行研究，以找到这些值更准确的估计，并在完成后更新这个计算器。
