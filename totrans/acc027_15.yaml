- en: Understanding how big of a model can fit on your machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator](https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/accelerate/v0.27.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/start.6e0fb178.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/scheduler.69131cc3.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/singletons.ac467c20.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/paths.b2f3aeca.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/entry/app.67e11fc0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/index.e1f30d73.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/0.bfeed9f0.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/nodes/42.790bb131.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Tip.22e79575.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/CodeBlock.30cef355.js">
    <link rel="modulepreload" href="/docs/accelerate/v0.27.2/en/_app/immutable/chunks/Heading.0aab6758.js">
  prefs: []
  type: TYPE_NORMAL
- en: One very difficult aspect when exploring potential models to use on your machine
    is knowing just how big of a model will *fit* into memory with your current graphics
    card (such as loading the model onto CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: To help alleviate this, ðŸ¤— Accelerate has a CLI interface through `accelerate
    estimate-memory`. This tutorial will help walk you through using it, what to expect,
    and at the end link to the interactive demo hosted on the ðŸ¤— Hub which will even
    let you post those results directly on the model repo!
  prefs: []
  type: TYPE_NORMAL
- en: Currently we support searching for models that can be used in `timm` and `transformers`.
  prefs: []
  type: TYPE_NORMAL
- en: This API will load the model into memory on the `meta` device, so we are not
    actually downloading and loading the full weights of the model into memory, nor
    do we need to. As a result itâ€™s perfectly fine to measure 8 billion parameter
    models (or more), without having to worry about if your CPU can handle it!
  prefs: []
  type: TYPE_NORMAL
- en: Gradio Demos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are a few gradio demos related to what was described above. The first
    is the official Hugging Face memory estimation space, utilizing Accelerate directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=light](https://hf-accelerate-model-memory-usage.hf.space?__theme=light)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=dark](https://hf-accelerate-model-memory-usage.hf.space?__theme=dark)'
  prefs: []
  type: TYPE_NORMAL
- en: A community member has taken the idea and expended it further, allowing you
    to filter models directly and see if you can run a particular LLM given GPU constraints
    and LoRA configurations. To play with it, see [here](https://huggingface.co/spaces/Vokturz/can-it-run-llm)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: The Command
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using `accelerate estimate-memory`, you need to pass in the name of the
    model you want to use, potentially the framework that model utilizing (if it canâ€™t
    be found automatically), and the data types you want the model to be loaded in
    with.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is how we can calculate the memory footprint for `bert-base-cased`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will download the `config.json` for `bert-based-cased`, load the model
    on the `meta` device, and report back how much space it will use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory Usage for loading `bert-base-cased`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 418.18 MB | 1.61 GB |'
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  prefs: []
  type: TYPE_TB
- en: '| int8 | 21.24 MB | 103.29 MB | 413.18 MB |'
  prefs: []
  type: TYPE_TB
- en: '| int4 | 10.62 MB | 51.65 MB | 206.59 MB |'
  prefs: []
  type: TYPE_TB
- en: By default it will return all the supported dtypes (`int4` through `float32`),
    but if you are interested in specific ones these can be filtered.
  prefs: []
  type: TYPE_NORMAL
- en: Specific libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the source library cannot be determined automatically (like it could in the
    case of `bert-base-cased`), a library name can be passed in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory Usage for loading `HuggingFaceM4/idefics-80b-instruct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | 3.02 GB | 297.12 GB | 1.16 TB |'
  prefs: []
  type: TYPE_TB
- en: '| float16 | 1.51 GB | 148.56 GB | 594.24 GB |'
  prefs: []
  type: TYPE_TB
- en: '| int8 | 772.52 MB | 74.28 GB | 297.12 GB |'
  prefs: []
  type: TYPE_TB
- en: '| int4 | 386.26 MB | 37.14 GB | 148.56 GB |'
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory Usage for loading `timm/resnet50.a1_in1k`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | 9.0 MB | 97.7 MB | 390.78 MB |'
  prefs: []
  type: TYPE_TB
- en: '| float16 | 4.5 MB | 48.85 MB | 195.39 MB |'
  prefs: []
  type: TYPE_TB
- en: '| int8 | 2.25 MB | 24.42 MB | 97.7 MB |'
  prefs: []
  type: TYPE_TB
- en: '| int4 | 1.12 MB | 12.21 MB | 48.85 MB |'
  prefs: []
  type: TYPE_TB
- en: Specific dtypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, while we return `int4` through `float32` by default, any
    dtype can be used from `float32`, `float16`, `int8`, and `int4`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, pass them in after specifying `--dtypes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory Usage for loading `bert-base-cased`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 413.18 MB | 1.61 GB |'
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  prefs: []
  type: TYPE_TB
- en: Caveats with this calculator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This calculator will tell you how much memory is needed to purely load the model
    in, *not* to perform inference.
  prefs: []
  type: TYPE_NORMAL
- en: This calculation is accurate within a few % of the actual value, so it is a
    very good view of just how much memory it will take. For instance loading `bert-base-cased`
    actually takes `413.68 MB` when loaded on CUDA in full precision, and the calculator
    estimates `413.18 MB`.
  prefs: []
  type: TYPE_NORMAL
- en: When performing inference you can expect to add up to an additional 20% as found
    by [EleutherAI](https://blog.eleuther.ai/transformer-math/). Weâ€™ll be conducting
    research into finding a more accurate estimate to these values, and will update
    this calculator once done.
  prefs: []
  type: TYPE_NORMAL
