- en: Understanding how big of a model can fit on your machine
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator](https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: One very difficult aspect when exploring potential models to use on your machine
    is knowing just how big of a model will *fit* into memory with your current graphics
    card (such as loading the model onto CUDA).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: To help alleviate this, ğŸ¤— Accelerate has a CLI interface through `accelerate
    estimate-memory`. This tutorial will help walk you through using it, what to expect,
    and at the end link to the interactive demo hosted on the ğŸ¤— Hub which will even
    let you post those results directly on the model repo!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Currently we support searching for models that can be used in `timm` and `transformers`.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: This API will load the model into memory on the `meta` device, so we are not
    actually downloading and loading the full weights of the model into memory, nor
    do we need to. As a result itâ€™s perfectly fine to measure 8 billion parameter
    models (or more), without having to worry about if your CPU can handle it!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Gradio Demos
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are a few gradio demos related to what was described above. The first
    is the official Hugging Face memory estimation space, utilizing Accelerate directly:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=light](https://hf-accelerate-model-memory-usage.hf.space?__theme=light)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hf-accelerate-model-memory-usage.hf.space?__theme=dark](https://hf-accelerate-model-memory-usage.hf.space?__theme=dark)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A community member has taken the idea and expended it further, allowing you
    to filter models directly and see if you can run a particular LLM given GPU constraints
    and LoRA configurations. To play with it, see [here](https://huggingface.co/spaces/Vokturz/can-it-run-llm)
    for more details.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The Command
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using `accelerate estimate-memory`, you need to pass in the name of the
    model you want to use, potentially the framework that model utilizing (if it canâ€™t
    be found automatically), and the data types you want the model to be loaded in
    with.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is how we can calculate the memory footprint for `bert-base-cased`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will download the `config.json` for `bert-based-cased`, load the model
    on the `meta` device, and report back how much space it will use:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory Usage for loading `bert-base-cased`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 418.18 MB | 1.61 GB |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| int8 | 21.24 MB | 103.29 MB | 413.18 MB |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| int4 | 10.62 MB | 51.65 MB | 206.59 MB |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: By default it will return all the supported dtypes (`int4` through `float32`),
    but if you are interested in specific ones these can be filtered.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Specific libraries
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the source library cannot be determined automatically (like it could in the
    case of `bert-base-cased`), a library name can be passed in.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Memory Usage for loading `HuggingFaceM4/idefics-80b-instruct`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| float32 | 3.02 GB | 297.12 GB | 1.16 TB |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| float16 | 1.51 GB | 148.56 GB | 594.24 GB |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| int8 | 772.52 MB | 74.28 GB | 297.12 GB |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| int4 | 386.26 MB | 37.14 GB | 148.56 GB |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Memory Usage for loading `timm/resnet50.a1_in1k`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| float32 | 9.0 MB | 97.7 MB | 390.78 MB |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| float16 | 4.5 MB | 48.85 MB | 195.39 MB |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| int8 | 2.25 MB | 24.42 MB | 97.7 MB |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| int4 | 1.12 MB | 12.21 MB | 48.85 MB |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: Specific dtypes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, while we return `int4` through `float32` by default, any
    dtype can be used from `float32`, `float16`, `int8`, and `int4`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, pass them in after specifying `--dtypes`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Memory Usage for loading `bert-base-cased`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '| dtype | Largest Layer | Total Size | Training using Adam |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| float32 | 84.95 MB | 413.18 MB | 1.61 GB |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| float16 | 42.47 MB | 206.59 MB | 826.36 MB |'
- en: Caveats with this calculator
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®¡ç®—å™¨çš„æ³¨æ„äº‹é¡¹
- en: This calculator will tell you how much memory is needed to purely load the model
    in, *not* to perform inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®¡ç®—å™¨å°†å‘Šè¯‰æ‚¨åŠ è½½æ¨¡å‹æ‰€éœ€çš„å†…å­˜é‡ï¼Œ*è€Œä¸æ˜¯*æ‰§è¡Œæ¨æ–­ã€‚
- en: This calculation is accurate within a few % of the actual value, so it is a
    very good view of just how much memory it will take. For instance loading `bert-base-cased`
    actually takes `413.68 MB` when loaded on CUDA in full precision, and the calculator
    estimates `413.18 MB`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè®¡ç®—å‡†ç¡®åº¦åœ¨å®é™…å€¼çš„å‡ ä¸ªç™¾åˆ†ç‚¹å†…ï¼Œå› æ­¤å®ƒéå¸¸å¥½åœ°å±•ç¤ºäº†éœ€è¦å¤šå°‘å†…å­˜ã€‚ä¾‹å¦‚ï¼Œå®Œæ•´ç²¾åº¦ä¸‹åŠ è½½`bert-base-cased`å®é™…ä¸Šéœ€è¦`413.68
    MB`ï¼Œè€Œè®¡ç®—å™¨ä¼°è®¡ä¸º`413.18 MB`ã€‚
- en: When performing inference you can expect to add up to an additional 20% as found
    by [EleutherAI](https://blog.eleuther.ai/transformer-math/). Weâ€™ll be conducting
    research into finding a more accurate estimate to these values, and will update
    this calculator once done.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œæ¨æ–­æ—¶ï¼Œæ‚¨å¯ä»¥æœŸæœ›é¢å¤–å¢åŠ é«˜è¾¾20%ï¼Œè¿™æ˜¯ç”±[EleutherAI](https://blog.eleuther.ai/transformer-math/)å‘ç°çš„ã€‚æˆ‘ä»¬å°†è¿›è¡Œç ”ç©¶ï¼Œä»¥æ‰¾åˆ°è¿™äº›å€¼æ›´å‡†ç¡®çš„ä¼°è®¡ï¼Œå¹¶åœ¨å®Œæˆåæ›´æ–°è¿™ä¸ªè®¡ç®—å™¨ã€‚
