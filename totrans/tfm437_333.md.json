["```py\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import AltCLIPModel, AltCLIPProcessor\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AltCLIPConfig, AltCLIPModel\n\n>>> # Initializing a AltCLIPConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPConfig()\n\n>>> # Initializing a AltCLIPModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a AltCLIPConfig from a AltCLIPTextConfig and a AltCLIPVisionConfig\n\n>>> # Initializing a AltCLIPText and AltCLIPVision configuration\n>>> config_text = AltCLIPTextConfig()\n>>> config_vision = AltCLIPVisionConfig()\n\n>>> config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import AltCLIPTextModel, AltCLIPTextConfig\n\n>>> # Initializing a AltCLIPTextConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPTextConfig()\n\n>>> # Initializing a AltCLIPTextModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AltCLIPVisionConfig, AltCLIPVisionModel\n\n>>> # Initializing a AltCLIPVisionConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPVisionConfig()\n\n>>> # Initializing a AltCLIPVisionModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoProcessor, AltCLIPTextModel\n\n>>> model = AltCLIPTextModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> texts = [\"it's a cat\", \"it's a dog\"]\n\n>>> inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPVisionModel\n\n>>> model = AltCLIPVisionModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]