["```py\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import AltCLIPModel, AltCLIPProcessor\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AltCLIPProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( text_config = None vision_config = None projection_dim = 768 logit_scale_init_value = 2.6592 **kwargs )\n```", "```py\n>>> from transformers import AltCLIPConfig, AltCLIPModel\n\n>>> # Initializing a AltCLIPConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPConfig()\n\n>>> # Initializing a AltCLIPModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a AltCLIPConfig from a AltCLIPTextConfig and a AltCLIPVisionConfig\n\n>>> # Initializing a AltCLIPText and AltCLIPVision configuration\n>>> config_text = AltCLIPTextConfig()\n>>> config_vision = AltCLIPVisionConfig()\n\n>>> config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: AltCLIPTextConfig vision_config: AltCLIPVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';AltCLIPConfig\n```", "```py\n( vocab_size = 250002 hidden_size = 1024 num_hidden_layers = 24 num_attention_heads = 16 intermediate_size = 4096 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 initializer_range = 0.02 initializer_factor = 0.02 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True project_dim = 768 **kwargs )\n```", "```py\n>>> from transformers import AltCLIPTextModel, AltCLIPTextConfig\n\n>>> # Initializing a AltCLIPTextConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPTextConfig()\n\n>>> # Initializing a AltCLIPTextModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import AltCLIPVisionConfig, AltCLIPVisionModel\n\n>>> # Initializing a AltCLIPVisionConfig with BAAI/AltCLIP style configuration\n>>> configuration = AltCLIPVisionConfig()\n\n>>> # Initializing a AltCLIPVisionModel (with random weights) from the BAAI/AltCLIP style configuration\n>>> model = AltCLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: AltCLIPConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None token_type_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.altclip.modeling_altclip.AltCLIPOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None token_type_ids = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPModel\n\n>>> model = AltCLIPModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None output_attentions: Optional = None return_dict: Optional = None output_hidden_states: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, AltCLIPTextModel\n\n>>> model = AltCLIPTextModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> texts = [\"it's a cat\", \"it's a dog\"]\n\n>>> inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n( config: AltCLIPVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, AltCLIPVisionModel\n\n>>> model = AltCLIPVisionModel.from_pretrained(\"BAAI/AltCLIP\")\n>>> processor = AutoProcessor.from_pretrained(\"BAAI/AltCLIP\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```"]