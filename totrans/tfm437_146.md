# ByT5

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5)

## 概述

ByT5 模型由 Linting Xue、Aditya Barua、Noah Constant、Rami Al-Rfou、Sharan Narang、Mihir Kale、Adam Roberts、Colin Raffel 在[ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)中提出。

论文摘要如下：

*大多数广泛使用的预训练语言模型操作的是与单词或子词单元对应的标记序列。将文本编码为标记序列需要一个分词器，通常作为模型的独立工件创建。直接在原始文本（字节或字符）上运行的无标记模型具有许多优点：它们可以直接处理任何语言的文本，对噪声更加稳健，并通过消除复杂和易出错的文本预处理流程来最小化技术债务。由于字节或字符序列比标记序列更长，过去关于无标记模型的工作通常引入了新的模型架构，旨在分摊直接在原始文本上运行的成本。在本文中，我们展示了标准 Transformer 架构可以在最小修改的情况下用于处理字节序列。我们仔细研究了参数数量、训练 FLOPs 和推理速度方面的权衡，并表明字节级模型与其标记级对应物具有竞争力。我们还证明了字节级模型对噪声更加稳健，并在对拼写和发音敏感的任务上表现更好。作为我们的贡献的一部分，我们发布了一组基于 T5 架构的新的预训练字节级 Transformer 模型，以及我们实验中使用的所有代码和数据。*

这个模型是由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。原始代码可以在[这里](https://github.com/google-research/byt5)找到。

ByT5 的架构基于 T5v1.1 模型，请参考 T5v1.1 的文档页面获取 API 参考。它们只在输入如何为模型准备方面有所不同，请参见下面的代码示例。

由于 ByT5 是无监督预训练的，单任务微调时使用任务前缀并没有真正的优势。如果进行多任务微调，则应使用前缀。

## 用法示例

ByT5 使用原始的 UTF-8 字节，因此可以在没有分词器的情况下使用：

```py
>>> from transformers import T5ForConditionalGeneration
>>> import torch

>>> model = T5ForConditionalGeneration.from_pretrained("google/byt5-small")

>>> num_special_tokens = 3
>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.
>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.

>>> input_ids = torch.tensor([list("Life is like a box of chocolates.".encode("utf-8"))]) + num_special_tokens

>>> labels = torch.tensor([list("La vie est comme une boîte de chocolat.".encode("utf-8"))]) + num_special_tokens

>>> loss = model(input_ids, labels=labels).loss
>>> loss.item()
2.66
```

然而，对于批量推理和训练，建议使用分词器：

```py
>>> from transformers import T5ForConditionalGeneration, AutoTokenizer

>>> model = T5ForConditionalGeneration.from_pretrained("google/byt5-small")
>>> tokenizer = AutoTokenizer.from_pretrained("google/byt5-small")

>>> model_inputs = tokenizer(
...     ["Life is like a box of chocolates.", "Today is Monday."], padding="longest", return_tensors="pt"
... )
>>> labels_dict = tokenizer(
...     ["La vie est comme une boîte de chocolat.", "Aujourd'hui c'est lundi."], padding="longest", return_tensors="pt"
... )
>>> labels = labels_dict.input_ids

>>> loss = model(**model_inputs, labels=labels).loss
>>> loss.item()
17.9
```

与 T5 类似，ByT5 是在 span-mask 去噪任务上进行训练的。然而，由于该模型直接在字符上工作，预训练任务有些不同。让我们破坏输入句子`"The dog chases a ball in the park."`的一些字符，并要求 ByT5 为我们预测它们。

```py
>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("google/byt5-base")
>>> model = AutoModelForSeq2SeqLM.from_pretrained("google/byt5-base")

>>> input_ids_prompt = "The dog chases a ball in the park."
>>> input_ids = tokenizer(input_ids_prompt).input_ids

>>> # Note that we cannot add "{extra_id_...}" to the string directly
>>> # as the Byte tokenizer would incorrectly merge the tokens
>>> # For ByT5, we need to work directly on the character level
>>> # Contrary to T5, ByT5 does not use sentinel tokens for masking, but instead
>>> # uses final utf character ids.
>>> # UTF-8 is represented by 8 bits and ByT5 has 3 special tokens.
>>> # => There are 2**8+2 = 259 input ids and mask tokens count down from index 258.
>>> # => mask to "The dog [258]a ball [257]park."

>>> input_ids = torch.tensor([input_ids[:8] + [258] + input_ids[14:21] + [257] + input_ids[28:]])
>>> input_ids
tensor([[ 87, 107, 104,  35, 103, 114, 106,  35, 258,  35, 100,  35, 101, 100, 111, 111, 257,  35, 115, 100, 117, 110,  49,   1]])

>>> # ByT5 produces only one char at a time so we need to produce many more output characters here -> set `max_length=100`.
>>> output_ids = model.generate(input_ids, max_length=100)[0].tolist()
>>> output_ids
[0, 258, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118, 257,  35, 108, 113,  35, 119, 107, 104,  35, 103, 108, 118, 102, 114, 256, 108, 113,  35, 119, 107, 104, 35, 115, 100, 117, 110,  49,  35,  87, 107, 104,  35, 103, 114, 106, 35, 108, 118,  35, 119, 107, 104,  35, 114, 113, 104,  35, 122, 107, 114,  35, 103, 114, 104, 118,  35, 100,  35, 101, 100, 111, 111,  35, 108, 113, 255,  35, 108, 113,  35, 119, 107, 104,  35, 115, 100, 117, 110,  49]

>>> # ^- Note how 258 descends to 257, 256, 255

>>> # Now we need to split on the sentinel tokens, let's write a short loop for this
>>> output_ids_list = []
>>> start_token = 0
>>> sentinel_token = 258
>>> while sentinel_token in output_ids:
...     split_idx = output_ids.index(sentinel_token)
...     output_ids_list.append(output_ids[start_token:split_idx])
...     start_token = split_idx
...     sentinel_token -= 1

>>> output_ids_list.append(output_ids[start_token:])
>>> output_string = tokenizer.batch_decode(output_ids_list)
>>> output_string
['<pad>', 'is the one who does', ' in the disco', 'in the park. The dog is the one who does a ball in', ' in the park.']
```

## ByT5Tokenizer

### `class transformers.ByT5Tokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L28)

```py
( eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 125 additional_special_tokens = None **kwargs )
```

参数

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。

    当构建使用特殊标记的序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `extra_ids` (`int`, *可选*, 默认为 125) — 添加一定数量的额外 ID，添加到词汇表末尾以用作哨兵。这些 token 可以作为“<extra>id{%d}>”访问，其中”{%d}”是 0 到 extra_ids-1 之间的数字。额外 token 从词汇表末尾向开始索引（“<extra_id_0>”是词汇表中的最后一个 token，就像 ByT5 预处理中看到的那样，请参见[此处](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)）。</extra_id_0></extra>

+   `additional_special_tokens` (`List[str]`, *可选*) — 分词器使用的额外特殊 token。

构建一个 ByT5 分词器。ByT5 简单地使用原始字节 utf-8 编码。

此分词器继承自 PreTrainedTokenizer，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L172)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊 token 的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

带有适当特殊 token 的 输入 ID 列表。

通过连接和添加特殊 token，从序列或序列对构建用于序列分类任务的模型输入。序列的格式如下：

+   单个序列：`X </s>`

+   序列对：`A </s> B </s>`

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L218)

```py
( tokens )
```

将 token 序列（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L150)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

返回

`List[int]`

零的列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。ByT5 不使用 token 类型 id，因此返回一个零的列表。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L111)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。

+   `already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经为模型格式化了特殊 token。

返回

`List[int]`

一个整数列表，范围为 [0, 1]：特殊 token 为 1，序列 token 为 0。

从没有添加特殊 token 的 token 列表中检索序列 id。在使用分词器的 `prepare_for_model` 方法添加特殊 token 时调用此方法。

有关所有详细信息，请参阅 ByT5Tokenizer。
