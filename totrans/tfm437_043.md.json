["```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\")\n>>> text = \"[clears throat] This is a test ... and I just took a long pause.\"\n>>> output = pipe(text)\n```", "```py\n>>> from IPython.display import Audio\n>>> Audio(output[\"audio\"], rate=output[\"sampling_rate\"])\n```", "```py\npip install datasets soundfile speechbrain accelerate\n```", "```py\npip install git+https://github.com/huggingface/transformers.git\n```", "```py\n!nvidia-smi\n```", "```py\n!rocm-smi\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\")\n>>> len(dataset)\n20968\n```", "```py\ndataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n```", "```py\n>>> from transformers import SpeechT5Processor\n\n>>> checkpoint = \"microsoft/speecht5_tts\"\n>>> processor = SpeechT5Processor.from_pretrained(checkpoint)\n```", "```py\n>>> tokenizer = processor.tokenizer\n```", "```py\n>>> def extract_all_chars(batch):\n...     all_text = \" \".join(batch[\"normalized_text\"])\n...     vocab = list(set(all_text))\n...     return {\"vocab\": [vocab], \"all_text\": [all_text]}\n\n>>> vocabs = dataset.map(\n...     extract_all_chars,\n...     batched=True,\n...     batch_size=-1,\n...     keep_in_memory=True,\n...     remove_columns=dataset.column_names,\n... )\n\n>>> dataset_vocab = set(vocabs[\"vocab\"][0])\n>>> tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\n```", "```py\n>>> dataset_vocab - tokenizer_vocab\n{' ', '\u00e0', '\u00e7', '\u00e8', '\u00eb', '\u00ed', '\u00ef', '\u00f6', '\u00fc'}\n```", "```py\n>>> replacements = [\n...     (\"\u00e0\", \"a\"),\n...     (\"\u00e7\", \"c\"),\n...     (\"\u00e8\", \"e\"),\n...     (\"\u00eb\", \"e\"),\n...     (\"\u00ed\", \"i\"),\n...     (\"\u00ef\", \"i\"),\n...     (\"\u00f6\", \"o\"),\n...     (\"\u00fc\", \"u\"),\n... ]\n\n>>> def cleanup_text(inputs):\n...     for src, dst in replacements:\n...         inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n...     return inputs\n\n>>> dataset = dataset.map(cleanup_text)\n```", "```py\n>>> from collections import defaultdict\n\n>>> speaker_counts = defaultdict(int)\n\n>>> for speaker_id in dataset[\"speaker_id\"]:\n...     speaker_counts[speaker_id] += 1\n```", "```py\n>>> import matplotlib.pyplot as plt\n\n>>> plt.figure()\n>>> plt.hist(speaker_counts.values(), bins=20)\n>>> plt.ylabel(\"Speakers\")\n>>> plt.xlabel(\"Examples\")\n>>> plt.show()\n```", "```py\n>>> def select_speaker(speaker_id):\n...     return 100 <= speaker_counts[speaker_id] <= 400\n\n>>> dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n```", "```py\n>>> len(set(dataset[\"speaker_id\"]))\n42\n```", "```py\n>>> len(dataset)\n9973\n```", "```py\n>>> import os\n>>> import torch\n>>> from speechbrain.pretrained import EncoderClassifier\n\n>>> spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> speaker_model = EncoderClassifier.from_hparams(\n...     source=spk_model_name,\n...     run_opts={\"device\": device},\n...     savedir=os.path.join(\"/tmp\", spk_model_name),\n... )\n\n>>> def create_speaker_embedding(waveform):\n...     with torch.no_grad():\n...         speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n...         speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n...         speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n...     return speaker_embeddings\n```", "```py\n>>> def prepare_dataset(example):\n...     audio = example[\"audio\"]\n\n...     example = processor(\n...         text=example[\"normalized_text\"],\n...         audio_target=audio[\"array\"],\n...         sampling_rate=audio[\"sampling_rate\"],\n...         return_attention_mask=False,\n...     )\n\n...     # strip off the batch dimension\n...     example[\"labels\"] = example[\"labels\"][0]\n\n...     # use SpeechBrain to obtain x-vector\n...     example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n\n...     return example\n```", "```py\n>>> processed_example = prepare_dataset(dataset[0])\n>>> list(processed_example.keys())\n['input_ids', 'labels', 'stop_labels', 'speaker_embeddings']\n```", "```py\n>>> processed_example[\"speaker_embeddings\"].shape\n(512,)\n```", "```py\n>>> import matplotlib.pyplot as plt\n\n>>> plt.figure()\n>>> plt.imshow(processed_example[\"labels\"].T)\n>>> plt.show()\n```", "```py\n>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n```", "```py\n>>> def is_not_too_long(input_ids):\n...     input_length = len(input_ids)\n...     return input_length < 200\n\n>>> dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n>>> len(dataset)\n8259\n```", "```py\n>>> dataset = dataset.train_test_split(test_size=0.1)\n```", "```py\n>>> from dataclasses import dataclass\n>>> from typing import Any, Dict, List, Union\n\n>>> @dataclass\n... class TTSDataCollatorWithPadding:\n...     processor: Any\n\n...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n...         input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n...         label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n...         speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n\n...         # collate the inputs and targets into a batch\n...         batch = processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n\n...         # replace padding with -100 to ignore loss correctly\n...         batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n\n...         # not used during fine-tuning\n...         del batch[\"decoder_attention_mask\"]\n\n...         # round down target lengths to multiple of reduction factor\n...         if model.config.reduction_factor > 1:\n...             target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n...             target_lengths = target_lengths.new(\n...                 [length - length % model.config.reduction_factor for length in target_lengths]\n...             )\n...             max_length = max(target_lengths)\n...             batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n\n...         # also add in the speaker embeddings\n...         batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n\n...         return batch\n```", "```py\n>>> data_collator = TTSDataCollatorWithPadding(processor=processor)\n```", "```py\n>>> from transformers import SpeechT5ForTextToSpeech\n\n>>> model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n```", "```py\n>>> model.config.use_cache = False\n```", "```py\n>>> from transformers import Seq2SeqTrainingArguments\n\n>>> training_args = Seq2SeqTrainingArguments(\n...     output_dir=\"speecht5_finetuned_voxpopuli_nl\",  # change to a repo name of your choice\n...     per_device_train_batch_size=4,\n...     gradient_accumulation_steps=8,\n...     learning_rate=1e-5,\n...     warmup_steps=500,\n...     max_steps=4000,\n...     gradient_checkpointing=True,\n...     fp16=True,\n...     evaluation_strategy=\"steps\",\n...     per_device_eval_batch_size=2,\n...     save_steps=1000,\n...     eval_steps=1000,\n...     logging_steps=25,\n...     report_to=[\"tensorboard\"],\n...     load_best_model_at_end=True,\n...     greater_is_better=False,\n...     label_names=[\"labels\"],\n...     push_to_hub=True,\n... )\n```", "```py\n>>> from transformers import Seq2SeqTrainer\n\n>>> trainer = Seq2SeqTrainer(\n...     args=training_args,\n...     model=model,\n...     train_dataset=dataset[\"train\"],\n...     eval_dataset=dataset[\"test\"],\n...     data_collator=data_collator,\n...     tokenizer=processor,\n... )\n```", "```py\n>>> trainer.train()\n```", "```py\n>>> processor.save_pretrained(\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> from transformers import pipeline\n\n>>> pipe = pipeline(\"text-to-speech\", model=\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")\n```", "```py\n>>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n```", "```py\n>>> example = dataset[\"test\"][304]\n>>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n```", "```py\n>>> forward_params = {\"speaker_embeddings\": speaker_embeddings}\n>>> output = pipe(text, forward_params=forward_params)\n>>> output\n{'audio': array([-6.82714235e-05, -4.26525949e-04,  1.06134125e-04, ...,\n        -1.22392643e-03, -7.76011671e-04,  3.29112721e-04], dtype=float32),\n 'sampling_rate': 16000}\n```", "```py\n>>> from IPython.display import Audio\n>>> Audio(output['audio'], rate=output['sampling_rate']) \n```", "```py\n>>> model = SpeechT5ForTextToSpeech.from_pretrained(\"YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl\")\n```", "```py\n>>> example = dataset[\"test\"][304]\n>>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n```", "```py\n>>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n>>> inputs = processor(text=text, return_tensors=\"pt\")\n```", "```py\n>>> spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n```", "```py\n>>> plt.figure()\n>>> plt.imshow(spectrogram.T)\n>>> plt.show()\n```", "```py\n>>> with torch.no_grad():\n...     speech = vocoder(spectrogram)\n\n>>> from IPython.display import Audio\n\n>>> Audio(speech.numpy(), rate=16000)\n```"]