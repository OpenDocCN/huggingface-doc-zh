["```py\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt>###Assistant:\n```", "```py\nA chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n<prompt1>###Assistant: <answer1>###Human: <prompt2>###Assistant:\n```", "```py\n( vision_config = None text_config = None ignore_index = -100 image_token_index = 32000 projector_hidden_act = 'gelu' projector_layernorm_eps = 1e-05 vision_feature_layers = [-2, -5, -8, -11, 6] vocab_size = 32000 **kwargs )\n```", "```py\n>>> from transformers import VipLlavaForConditionalGeneration, VipLlavaConfig, CLIPVisionConfig, LlamaConfig\n\n>>> # Initializing a CLIP-vision config\n>>> vision_config = CLIPVisionConfig()\n\n>>> # Initializing a Llama config\n>>> text_config = LlamaConfig()\n\n>>> # Initializing a VipLlava vipllava-7b style configuration\n>>> configuration = VipLlavaConfig(vision_config, text_config)\n\n>>> # Initializing a model from the vipllava-7b style configuration\n>>> model = VipLlavaForConditionalGeneration(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: VipLlavaConfig )\n```", "```py\n( input_ids: LongTensor = None pixel_values: FloatTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None vision_feature_layers: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.vipllava.modeling_vipllava.VipLlavaCausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, VipLlavaForConditionalGeneration\n\n>>> model = VipLlavaForConditionalGeneration.from_pretrained(\"llava-hf/vip-llava-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16)\n>>> processor = AutoProcessor.from_pretrained(\"llava-hf/vip-llava-7b-hf\")\n\n>>> prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n{}###Assistant:\"\n>>> question = \"Can you please describe this image?\"\n>>> prompt = prompt.format(question)\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=text, images=image, return_tensors=\"pt\").to(0, torch.float16)\n\n>>> # Generate\n>>> generate_ids = model.generate(**inputs, max_new_tokens=20)\n>>> processor.decode(generate_ids[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\nThe image features a brown and white cat sitting on a green surface, with a red ball in its\n```"]