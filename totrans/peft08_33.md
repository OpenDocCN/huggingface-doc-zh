# 调谐器

> 原文：[https://huggingface.co/docs/peft/package_reference/tuners](https://huggingface.co/docs/peft/package_reference/tuners)

调谐器（或适配器）是可以插入到`torch.nn.Module`中的模块。`BaseTuner`是其他调谐器的基类，并为准备适配器配置和用适配器模块替换目标模块提供共享方法和属性。`BaseTunerLayer`是适配器层的基类。它提供了管理适配器的方法和属性，如激活和禁用适配器。

## BaseTuner

### `class peft.tuners.tuners_utils.BaseTuner`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L91)

```py
( model peft_config: Union[PeftConfig, dict[str, PeftConfig]] adapter_name: str )
```

参数

+   `model`（`torch.nn.Module`）— 适配器调谐器层将附加到的模型。

+   `forward`（`Callable`）— 模型的前向方法。

+   `peft_config`（`Union[`PeftConfig`, dict[str, PeftConfig]]`）— 适配器配置对象，应该是一个`str`到`PeftConfig`对象的字典。也可以传递一个PeftConfig对象，将创建一个默认名称为`adapter`的新适配器，或创建一个具有键`adapter_name`和该peft配置值的新字典。

+   `config`（`dict[str, Any]`）— 模型配置对象，应该是一个`str`到`Any`对象的字典。

+   `targeted_module_names`（`list[str]`）— 实际上被调整的模块名称列表。如果要快速检查`config.target_modules`是否被正确指定，这可能很有用。

提供所有可注入到torch.nn.Module中的调谐器共有方法和属性的基本调谐器模型

要添加一个新的Tuner类，需要重写以下方法：

+   `_prepare_adapter_config`：一个私有方法，最终准备适配器配置，例如在缺少字段`target_modules`的情况下。 

+   `_create_and_replace`：一个私有方法，用于创建并替换目标模块为适配器模块。

+   `_check_target_module_exists`：一个私有辅助方法，用于检查传递的模块键名称是否与适配器配置中的任何目标模块匹配。

最简单的方法是查看`peft.tuners.lora.LoraModel`类中的操作。

#### `inject_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L245)

```py
( model: nn.Module adapter_name: str )
```

参数

+   `model`（`nn.Module`）— 要调整的模型。

+   `adapter_name`（`str`）— 适配器名称。

创建适配器层并用适配器层替换目标模块。如果传递了一个非提示调谐适配器类，则此方法将在幕后被`peft.mapping.get_peft_model`调用。

相应的PEFT配置直接从BaseTuner类的`peft_config`属性中检索。

#### `merge_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L323)

```py
( adapter_names: Optional[list[str]] = None )
```

参数

+   `safe_merge`（`bool`，*可选*）— 如果为`True`，将在原始权重的副本中执行合并操作，并在合并权重之前检查NaN。如果要检查合并操作是否会产生NaN，则这很有用。默认为`False`。

+   `adapter_names`（`list[str]`，*可选*）— 应该合并的适配器名称列表。如果为`None`，则将合并所有活动适配器。默认为`None`。

此方法将适配器层合并到基本模型中。

合并适配器可以加快前向传递的速度。适配器权重的副本仍然保存在内存中，这是解除合并适配器所必需的。为了在不保留内存中的情况下合并适配器权重，请调用`merge_and_unload`。

#### `unmerge_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L345)

```py
( )
```

此方法从基本模型中取消合并所有合并的适配器层。

## BaseTunerLayer

### `class peft.tuners.tuners_utils.BaseTunerLayer`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L363)

```py
( )
```

参数

+   `is_pluggable`（`bool`，*可选*) — 适配器层是否可以插入到任何pytorch模块中

+   `active_adapters`（Union[List`str`，`str`], *可选*) — 活动适配器的名称。

一个调谐器层mixin，为所有调谐器提供共同的方法和属性。

#### `delete_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L505)

```py
( adapter_name: str )
```

参数

+   `adapter_name`（`str`） — 要删除的适配器的名称

从层中删除一个适配器

这应该在所有适配器层上调用，否则我们将得到一个不一致的状态。

如果删除的适配器是活动适配器，此方法还将设置一个新的活动适配器。重要的是选择新的适配器是以确定性方式进行的，以便在所有层上选择相同的适配器。

#### `enable_adapters`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L445)

```py
( enabled: bool )
```

参数

+   `enabled`（bool） — True表示启用适配器，False表示禁用适配器

切换适配器的启用和禁用

负责设置适配器权重的requires_grad标志。

#### `get_base_layer`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L390)

```py
( )
```

（递归地）获取基础层。

这对于调谐器层包装另一个调谐器层的情况是必要的。

#### `set_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/tuners_utils.py#L463)

```py
( adapter_names: str | list[str] )
```

参数

+   `adapter_name`（`str`或`List[str]`） — 要激活的适配器的名称。

设置活动适配器。

此外，此函数将设置指定的适配器为可训练（即requires_grad=True）。如果不需要此功能，请使用以下代码。

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```
