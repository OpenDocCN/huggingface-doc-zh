# å› æžœè¯­è¨€å»ºæ¨¡

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/tasks/language_modeling`](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/language_modeling)

è¯­è¨€å»ºæ¨¡æœ‰ä¸¤ç§ç±»åž‹ï¼Œå› æžœå’ŒæŽ©ç ã€‚æœ¬æŒ‡å—è¯´æ˜Žäº†å› æžœè¯­è¨€å»ºæ¨¡ã€‚å› æžœè¯­è¨€æ¨¡åž‹ç»å¸¸ç”¨äºŽæ–‡æœ¬ç”Ÿæˆã€‚æ‚¨å¯ä»¥å°†è¿™äº›æ¨¡åž‹ç”¨äºŽåˆ›æ„åº”ç”¨ï¼Œå¦‚é€‰æ‹©è‡ªå·±çš„æ–‡æœ¬å†’é™©æˆ–æ™ºèƒ½ç¼–ç åŠ©æ‰‹ï¼Œå¦‚ Copilot æˆ– CodeParrotã€‚

[`www.youtube-nocookie.com/embed/Vpjb1lu0MDk`](https://www.youtube-nocookie.com/embed/Vpjb1lu0MDk)

å› æžœè¯­è¨€å»ºæ¨¡é¢„æµ‹ä»¤ç‰Œåºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œæ¨¡åž‹åªèƒ½å…³æ³¨å·¦ä¾§çš„ä»¤ç‰Œã€‚è¿™æ„å‘³ç€æ¨¡åž‹æ— æ³•çœ‹åˆ°æœªæ¥çš„ä»¤ç‰Œã€‚GPT-2 æ˜¯å› æžœè¯­è¨€æ¨¡åž‹çš„ä¸€ä¸ªä¾‹å­ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

1.  åœ¨[ELI5](https://huggingface.co/datasets/eli5)æ•°æ®é›†çš„[r/askscience](https://www.reddit.com/r/askscience/)å­é›†ä¸Šå¾®è°ƒ[DistilGPT2](https://huggingface.co/distilgpt2)ã€‚

1.  ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡åž‹è¿›è¡ŒæŽ¨ç†ã€‚

æ‚¨å¯ä»¥æŒ‰ç…§æœ¬æŒ‡å—ä¸­çš„ç›¸åŒæ­¥éª¤å¾®è°ƒå…¶ä»–æž¶æž„ä»¥è¿›è¡Œå› æžœè¯­è¨€å»ºæ¨¡ã€‚é€‰æ‹©ä»¥ä¸‹æž¶æž„ä¹‹ä¸€ï¼š

BART, BERT, Bert Generation, BigBird, BigBird-Pegasus, BioGpt, Blenderbot, BlenderbotSmall, BLOOM, CamemBERT, CodeLlama, CodeGen, CPM-Ant, CTRL, Data2VecText, ELECTRA, ERNIE, Falcon, Fuyu, GIT, GPT-Sw3, OpenAI GPT-2, GPTBigCode, GPT Neo, GPT NeoX, GPT NeoX Japanese, GPT-J, LLaMA, Marian, mBART, MEGA, Megatron-BERT, Mistral, Mixtral, MPT, MusicGen, MVP, OpenLlama, OpenAI GPT, OPT, Pegasus, Persimmon, Phi, PLBart, ProphetNet, QDQBert, Qwen2, Reformer, RemBERT, RoBERTa, RoBERTa-PreLayerNorm, RoCBert, RoFormer, RWKV, Speech2Text2, Transformer-XL, TrOCR, Whisper, XGLM, XLM, XLM-ProphetNet, XLM-RoBERTa, XLM-RoBERTa-XL, XLNet, X-MOD

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚

```py
pip install transformers datasets evaluate
```

æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face å¸æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œä¸Žç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡åž‹ã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## åŠ è½½ ELI5 æ•°æ®é›†

é¦–å…ˆåŠ è½½ðŸ¤—æ•°æ®é›†åº“ä¸­ r/askscience å­é›†çš„ ELI5 æ•°æ®é›†çš„è¾ƒå°å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®žéªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œç„¶åŽå†èŠ±æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚

```py
>>> from datasets import load_dataset

>>> eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

ä½¿ç”¨[train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)æ–¹æ³•å°†æ•°æ®é›†çš„`train_asks`æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```py
>>> eli5 = eli5.train_test_split(test_size=0.2)
```

ç„¶åŽçœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
>>> eli5["train"][0]
{'answers': {'a_id': ['c3d1aib', 'c3d4lya'],
  'score': [6, 3],
  'text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
   "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"]},
 'answers_urls': {'url': []},
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls': {'url': []}}
```

è™½ç„¶è¿™çœ‹èµ·æ¥å¾ˆå¤šï¼Œä½†æ‚¨å®žé™…ä¸Šåªå¯¹`text`å­—æ®µæ„Ÿå…´è¶£ã€‚è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æœ‰è¶£ä¹‹å¤„åœ¨äºŽæ‚¨ä¸éœ€è¦æ ‡ç­¾ï¼ˆä¹Ÿç§°ä¸ºæ— ç›‘ç£ä»»åŠ¡ï¼‰ï¼Œå› ä¸ºä¸‹ä¸€ä¸ªè¯*å°±æ˜¯*æ ‡ç­¾ã€‚

## é¢„å¤„ç†

[`www.youtube-nocookie.com/embed/ma1TrR7gE7I`](https://www.youtube-nocookie.com/embed/ma1TrR7gE7I)

ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ª DistilGPT2 åˆ†è¯å™¨æ¥å¤„ç†`text`å­å­—æ®µï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
```

ä»Žä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°`text`å­—æ®µå®žé™…ä¸Šæ˜¯åµŒå¥—åœ¨`answers`ä¸­çš„ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦ä½¿ç”¨[`flatten`](https://huggingface.co/docs/datasets/process#flatten)æ–¹æ³•ä»Žå…¶åµŒå¥—ç»“æž„ä¸­æå–`text`å­å­—æ®µï¼š

```py
>>> eli5 = eli5.flatten()
>>> eli5["train"][0]
{'answers.a_id': ['c3d1aib', 'c3d4lya'],
 'answers.score': [6, 3],
 'answers.text': ["The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
  "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"],
 'answers_urls.url': [],
 'document': '',
 'q_id': 'nyxfp',
 'selftext': '_URL_0_\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
 'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
 'subreddit': 'askscience',
 'title': 'Few questions about this space walk photograph.',
 'title_urls.url': []}
```

çŽ°åœ¨ï¼Œæ¯ä¸ªå­å­—æ®µéƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ—ï¼Œç”±`answers`å‰ç¼€æŒ‡ç¤ºï¼Œè€Œ`text`å­—æ®µçŽ°åœ¨æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚ä¸è¦å•ç‹¬å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ†è¯ï¼Œè€Œæ˜¯å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿å¯ä»¥è”åˆå¯¹å®ƒä»¬è¿›è¡Œåˆ†è¯ã€‚

è¿™æ˜¯ä¸€ä¸ªç”¨äºŽè¿žæŽ¥æ¯ä¸ªç¤ºä¾‹çš„å­—ç¬¦ä¸²åˆ—è¡¨å¹¶å¯¹ç»“æžœè¿›è¡Œåˆ†è¯çš„ç¬¬ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼š

```py
>>> def preprocess_function(examples):
...     return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

è¦åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨æ­¤é¢„å¤„ç†å‡½æ•°ï¼Œè¯·ä½¿ç”¨ðŸ¤—æ•°æ®é›†çš„[map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)æ–¹æ³•ã€‚é€šè¿‡è®¾ç½®`batched=True`ä»¥ä¸€æ¬¡å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œå¹¶ä½¿ç”¨`num_proc`å¢žåŠ è¿›ç¨‹æ•°é‡ï¼Œå¯ä»¥åŠ å¿«`map`å‡½æ•°çš„é€Ÿåº¦ã€‚åˆ é™¤æ‚¨ä¸éœ€è¦çš„ä»»ä½•åˆ—ï¼š

```py
>>> tokenized_eli5 = eli5.map(
...     preprocess_function,
...     batched=True,
...     num_proc=4,
...     remove_columns=eli5["train"].column_names,
... )
```

è¯¥æ•°æ®é›†åŒ…å«ä»¤ç‰Œåºåˆ—ï¼Œä½†å…¶ä¸­ä¸€äº›æ¯”æ¨¡åž‹çš„æœ€å¤§è¾“å…¥é•¿åº¦æ›´é•¿ã€‚

çŽ°åœ¨å¯ä»¥ä½¿ç”¨ç¬¬äºŒä¸ªé¢„å¤„ç†å‡½æ•°

+   è¿žæŽ¥æ‰€æœ‰åºåˆ—

+   å°†è¿žæŽ¥çš„åºåˆ—æ‹†åˆ†ä¸ºç”±`block_size`å®šä¹‰çš„è¾ƒçŸ­å—ï¼Œè¯¥å—åº”æ¯”æœ€å¤§è¾“å…¥é•¿åº¦çŸ­ä¸”è¶³å¤ŸçŸ­ä»¥é€‚åº”æ‚¨çš„ GPU RAMã€‚

```py
>>> block_size = 128

>>> def group_texts(examples):
...     # Concatenate all texts.
...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
...     total_length = len(concatenated_examples[list(examples.keys())[0]])
...     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
...     # customize this part to your needs.
...     if total_length >= block_size:
...         total_length = (total_length // block_size) * block_size
...     # Split by chunks of block_size.
...     result = {
...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
...         for k, t in concatenated_examples.items()
...     }
...     result["labels"] = result["input_ids"].copy()
...     return result
```

åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨`group_texts`å‡½æ•°ï¼š

```py
>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

çŽ°åœ¨ä½¿ç”¨ DataCollatorForLanguageModeling åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­çš„æœ€é•¿é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

Pytorch éšè— Pytorch å†…å®¹

ä½¿ç”¨ç»“æŸåºåˆ—æ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶è®¾ç½®`mlm=False`ã€‚è¿™å°†ä½¿ç”¨è¾“å…¥ä½œä¸ºæ ‡ç­¾ï¼Œå‘å³ç§»åŠ¨ä¸€ä¸ªå…ƒç´ ï¼š

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> tokenizer.pad_token = tokenizer.eos_token
>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

TensorFlow éšè— TensorFlow å†…å®¹

ä½¿ç”¨ç»“æŸåºåˆ—æ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶è®¾ç½®`mlm=False`ã€‚è¿™å°†ä½¿ç”¨è¾“å…¥ä½œä¸ºæ ‡ç­¾ï¼Œå‘å³ç§»åŠ¨ä¸€ä¸ªå…ƒç´ ï¼š

```py
>>> from transformers import DataCollatorForLanguageModeling

>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
```

## è®­ç»ƒ

Pytorch éšè— Pytorch å†…å®¹

å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ Trainer å¾®è°ƒæ¨¡åž‹ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹!

æ‚¨çŽ°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡åž‹äº†ï¼ä½¿ç”¨ AutoModelForCausalLM åŠ è½½ DistilGPT2ï¼š

```py
>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

æ­¤æ—¶ï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š

1.  åœ¨ TrainingArguments ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€å¿…éœ€çš„å‚æ•°æ˜¯`output_dir`ï¼ŒæŒ‡å®šä¿å­˜æ¨¡åž‹çš„ä½ç½®ã€‚é€šè¿‡è®¾ç½®`push_to_hub=True`å°†æ­¤æ¨¡åž‹æŽ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡åž‹ï¼‰ã€‚

1.  å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ Trainerï¼Œä»¥åŠæ¨¡åž‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ã€‚

1.  è°ƒç”¨ train()æ¥å¾®è°ƒæ‚¨çš„æ¨¡åž‹ã€‚

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_eli5_clm-model",
...     evaluation_strategy="epoch",
...     learning_rate=2e-5,
...     weight_decay=0.01,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=lm_dataset["train"],
...     eval_dataset=lm_dataset["test"],
...     data_collator=data_collator,
... )

>>> trainer.train()
```

è®­ç»ƒå®ŒæˆåŽï¼Œä½¿ç”¨ evaluate()æ–¹æ³•è¯„ä¼°æ‚¨çš„æ¨¡åž‹å¹¶èŽ·å–å…¶å›°æƒ‘åº¦ï¼š

```py
>>> import math

>>> eval_results = trainer.evaluate()
>>> print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
Perplexity: 49.61
```

ç„¶åŽä½¿ç”¨ push_to_hub()æ–¹æ³•å°†æ‚¨çš„æ¨¡åž‹åˆ†äº«åˆ° Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡åž‹ï¼š

```py
>>> trainer.push_to_hub()
```

TensorFlow éšè— TensorFlow å†…å®¹

å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰å¦‚ä½•ä½¿ç”¨ Keras å¾®è°ƒæ¨¡åž‹ï¼Œè¯·æŸ¥çœ‹åŸºç¡€æ•™ç¨‹ï¼

è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡åž‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ çŽ‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š

```py
>>> from transformers import create_optimizer, AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ç„¶åŽï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ TFAutoModelForCausalLM åŠ è½½ DistilGPT2ï¼š

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")
```

ä½¿ç”¨ prepare_tf_dataset()å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     lm_dataset["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     lm_dataset["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ä¸ºè®­ç»ƒé…ç½®æ¨¡åž‹ã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡åž‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸Žä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éžæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦ï¼š

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

è¿™å¯ä»¥é€šè¿‡åœ¨ PushToHubCallback ä¸­æŒ‡å®šå°†æ¨¡åž‹å’Œæ ‡è®°å™¨æŽ¨é€åˆ°ä½•å¤„æ¥å®Œæˆï¼š

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> callback = PushToHubCallback(
...     output_dir="my_awesome_eli5_clm-model",
...     tokenizer=tokenizer,
... )
```

æœ€åŽï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡åž‹äº†ï¼ä½¿ç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)è°ƒç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œæ—¶ä»£æ•°é‡ä»¥åŠå¾®è°ƒæ¨¡åž‹çš„å›žè°ƒï¼š

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])
```

è®­ç»ƒå®ŒæˆåŽï¼Œæ‚¨çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼

æœ‰å…³å¦‚ä½•ä¸ºå› æžœè¯­è¨€å»ºæ¨¡å¾®è°ƒæ¨¡åž‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)æˆ–[TensorFlow ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚

## æŽ¨ç†

å¾ˆå¥½ï¼ŒçŽ°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡åž‹ï¼Œå¯ä»¥ç”¨äºŽæŽ¨ç†ï¼

æƒ³å‡ºä¸€ä¸ªæ‚¨æƒ³è¦ä»Žä¸­ç”Ÿæˆæ–‡æœ¬çš„æç¤ºï¼š

```py
>>> prompt = "Somatic hypermutation allows the immune system to"
```

å°è¯•ä½¿ç”¨ pipeline()æ¥è¿›è¡ŒæŽ¨ç†æ˜¯å°è¯•å¾®è°ƒæ¨¡åž‹çš„æœ€ç®€å•æ–¹æ³•ã€‚å®žä¾‹åŒ–ä¸€ä¸ªç”¨äºŽæ–‡æœ¬ç”Ÿæˆçš„`pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š

```py
>>> from transformers import pipeline

>>> generator = pipeline("text-generation", model="my_awesome_eli5_clm-model")
>>> generator(prompt)
[{'generated_text': "Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\n\n\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks."}]
```

Pytorch éšè— Pytorch å†…å®¹

å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°†`input_ids`è¿”å›žä¸º PyTorch å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

ä½¿ç”¨ generate()æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’ŒæŽ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ–‡æœ¬ç”Ÿæˆç­–ç•¥é¡µé¢ã€‚

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›žæ–‡æœ¬ï¼š

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
["Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system"]
```

TensorFlow éšè— TensorFlow å†…å®¹

å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¹¶å°†`input_ids`è¿”å›žä¸º TensorFlow å¼ é‡ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
>>> inputs = tokenizer(prompt, return_tensors="tf").input_ids
```

ä½¿ç”¨ generate()æ–¹æ³•åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’ŒæŽ§åˆ¶ç”Ÿæˆçš„å‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ–‡æœ¬ç”Ÿæˆç­–ç•¥é¡µé¢ã€‚

```py
>>> from transformers import TFAutoModelForCausalLM

>>> model = TFAutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›žæ–‡æœ¬ï¼š

```py
>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']
```
