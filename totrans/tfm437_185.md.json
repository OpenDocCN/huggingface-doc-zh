["```py\n( vqvae_config = None prior_config_list = None nb_priors = 3 sampling_rate = 44100 timing_dims = 64 min_duration = 0 max_duration = 600.0 max_nb_genres = 5 metadata_conditioning = True **kwargs )\n```", "```py\n>>> from transformers import JukeboxModel, JukeboxConfig\n\n>>> # Initializing a Jukebox configuration\n>>> configuration = JukeboxConfig()\n\n>>> # Initializing a model from the configuration\n>>> model = JukeboxModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( prior_configs: List vqvae_config: JukeboxVQVAEConfig **kwargs ) \u2192 export const metadata = 'undefined';JukeboxConfig\n```", "```py\n( act_fn = 'quick_gelu' level = 0 alignment_head = 2 alignment_layer = 68 attention_multiplier = 0.25 attention_pattern = 'enc_dec_with_lyrics' attn_dropout = 0 attn_res_scale = False blocks = 64 conv_res_scale = None num_layers = 72 emb_dropout = 0 encoder_config = None encoder_loss_fraction = 0.4 hidden_size = 2048 init_scale = 0.2 is_encoder_decoder = True lyric_vocab_size = 80 mask = False max_duration = 600 max_nb_genres = 1 merged_decoder = True metadata_conditioning = True metadata_dims = [604, 7898] min_duration = 0 mlp_multiplier = 1.0 music_vocab_size = 2048 n_ctx = 6144 n_heads = 2 nb_relevant_lyric_tokens = 384 res_conv_depth = 3 res_conv_width = 128 res_convolution_multiplier = 1 res_dilation_cycle = None res_dilation_growth_rate = 1 res_downs_t = [3, 2, 2] res_strides_t = [2, 2, 2] resid_dropout = 0 sampling_rate = 44100 spread = None timing_dims = 64 zero_out = False **kwargs )\n```", "```py\n( act_fn = 'relu' nb_discrete_codes = 2048 commit = 0.02 conv_input_shape = 1 conv_res_scale = False embed_dim = 64 hop_fraction = [0.125, 0.5, 0.5] levels = 3 lmu = 0.99 multipliers = [2, 1, 1] res_conv_depth = 4 res_conv_width = 32 res_convolution_multiplier = 1 res_dilation_cycle = None res_dilation_growth_rate = 3 res_downs_t = [3, 2, 2] res_strides_t = [2, 2, 2] sample_length = 1058304 init_scale = 0.2 zero_out = False **kwargs )\n```", "```py\n( artists_file genres_file lyrics_file version = ['v3', 'v2', 'v2'] max_n_lyric_tokens = 512 n_genres = 5 unk_token = '<|endoftext|>' **kwargs )\n```", "```py\n>>> from transformers import JukeboxTokenizer\n\n>>> tokenizer = JukeboxTokenizer.from_pretrained(\"openai/jukebox-1b-lyrics\")\n>>> tokenizer(\"Alan Jackson\", \"Country Rock\", \"old town road\")[\"input_ids\"]\n[tensor([[   0,    0,    0, 6785,  546,   41,   38,   30,   76,   46,   41,   49,\n           40,   76,   44,   41,   27,   30]]), tensor([[  0,   0,   0, 145,   0]]), tensor([[  0,   0,   0, 145,   0]])]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config )\n```", "```py\n( labels n_samples = 1 **sampling_kwargs )\n```", "```py\n>>> from transformers import AutoTokenizer, JukeboxModel, set_seed\n\n>>> model = JukeboxModel.from_pretrained(\"openai/jukebox-1b-lyrics\", min_duration=0).eval()\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/jukebox-1b-lyrics\")\n\n>>> lyrics = \"Hey, are you awake? Can you talk to me?\"\n>>> artist = \"Zac Brown Band\"\n>>> genre = \"Country\"\n>>> metas = tokenizer(artist=artist, genres=genre, lyrics=lyrics)\n>>> set_seed(0)\n>>> music_tokens = model.ancestral_sample(metas.input_ids, sample_length=400)\n\n>>> with torch.no_grad():\n...     model.decode(music_tokens)[:, :10].squeeze(-1)\ntensor([[-0.0219, -0.0679, -0.1050, -0.1203, -0.1271, -0.0936, -0.0396, -0.0405,\n    -0.0818, -0.0697]])\n```", "```py\n( raw_audio labels **sampling_kwargs )\n```", "```py\n( music_tokens labels **sampling_kwargs )\n```", "```py\n( music_tokens labels **sampling_kwargs )\n```", "```py\n( music_tokens labels sample_levels metas = None chunk_size = 32 sampling_temperature = 0.98 lower_batch_size = 16 max_batch_size = 16 sample_length_in_seconds = 24 compute_alignments = False sample_tokens = None offset = 0 save_results = True sample_length = None )\n```", "```py\n>>> from transformers import AutoTokenizer, JukeboxModel, set_seed\n>>> import torch\n\n>>> metas = dict(artist=\"Zac Brown Band\", genres=\"Country\", lyrics=\"I met a traveller from an antique land\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/jukebox-1b-lyrics\")\n>>> model = JukeboxModel.from_pretrained(\"openai/jukebox-1b-lyrics\", min_duration=0).eval()\n\n>>> labels = tokenizer(**metas)[\"input_ids\"]\n>>> set_seed(0)\n>>> zs = [torch.zeros(1, 0, dtype=torch.long) for _ in range(3)]\n>>> zs = model._sample(zs, labels, [0], sample_length=40 * model.priors[0].raw_to_tokens, save_results=False)\n>>> zs[0]\ntensor([[1853, 1369, 1150, 1869, 1379, 1789,  519,  710, 1306, 1100, 1229,  519,\n      353, 1306, 1379, 1053,  519,  653, 1631, 1467, 1229, 1229,   10, 1647,\n     1254, 1229, 1306, 1528, 1789,  216, 1631, 1434,  653,  475, 1150, 1528,\n     1804,  541, 1804, 1434]])\n```", "```py\n( config: JukeboxPriorConfig level = None nb_priors = 3 vqvae_encoder = None vqvae_decoder = None )\n```", "```py\n( n_samples music_tokens = None music_tokens_conds = None metadata = None temp = 1.0 top_k = 0 top_p = 0.0 chunk_size = None sample_tokens = None )\n```", "```py\n( hidden_states: Tensor metadata: Optional decode: Optional = False get_preds: Optional = False )\n```", "```py\n( config: JukeboxVQVAEConfig )\n```", "```py\n( raw_audio: FloatTensor )\n```", "```py\n>>> from transformers import JukeboxVQVAE, set_seed\n>>> import torch\n\n>>> model = JukeboxVQVAE.from_pretrained(\"openai/jukebox-1b-lyrics\").eval()\n>>> set_seed(0)\n>>> zs = [torch.randint(100, (4, 1))]\n>>> model.decode(zs).shape\ntorch.Size([4, 8, 1])\n```", "```py\n( input_audio start_level = 0 end_level = None bs_chunks = 1 )\n```", "```py\n( music_tokens start_level = 0 end_level = None bs_chunks = 1 )\n```"]