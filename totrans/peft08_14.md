# LoRA

> 原文链接：[https://huggingface.co/docs/peft/developer_guides/lora](https://huggingface.co/docs/peft/developer_guides/lora)

LoRA是一种低秩分解方法，可以减少可训练参数的数量，加快微调大型模型的速度并减少内存使用。在PEFT中，使用LoRA就像设置[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)并将其与[get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)结合使用，以创建可训练的[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)一样简单。

本指南更详细地探讨了使用LoRA的其他选项和功能。

## 初始化

LoRA权重的初始化由[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)中的参数`init_lora_weights`控制。默认情况下，PEFT使用Kaiming-uniform初始化LoRA权重A，并使用零初始化权重B，从而产生恒等变换（与参考[实现](https://github.com/microsoft/LoRA)相同）。

也可以传递`init_lora_weights="gaussian"`。正如名称所示，这将使用高斯分布初始化权重A，并将权重B初始化为零（这是[Diffusers](https://huggingface.co/docs/diffusers/index)初始化LoRA权重的方式）。

```py
from peft import LoraConfig

config = LoraConfig(init_lora_weights="gaussian", ...)
```

还有一个选项可以设置`init_lora_weights=False`，这对调试和测试很有用。这应该是您使用此选项的唯一时机。选择此选项时，LoRA权重将被初始化，使其*不*导致恒等变换。

```py
from peft import LoraConfig

config = LoraConfig(init_lora_weights=False, ...)
```

### LoftQ

在为QLoRA训练量化基础模型时，考虑使用[LoftQ初始化](https://arxiv.org/abs/2310.08659)，已经证明在训练量化模型时可以提高性能。其思想是初始化LoRA权重，使量化误差最小化。如果使用LoftQ，*不要*量化基础模型。您应该设置`LoftQConfig`：

```py
from peft import LoftQConfig, LoraConfig, get_peft_model

base_model = AutoModelForCausalLM.from_pretrained(...)  # don't quantize here
loftq_config = LoftQConfig(loftq_bits=4, ...)           # set 4bit quantization
lora_config = LoraConfig(..., init_lora_weights="loftq", loftq_config=loftq_config)
peft_model = get_peft_model(base_model, lora_config)
```

了解有关PEFT如何与量化一起工作的更多信息，请参阅[量化](quantization)指南。

### 秩稳定的LoRA

另一种初始化[LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig)的方法是使用[rank-stabilized LoRA (rsLoRA)](https://huggingface.co/papers/2312.03732)方法。LoRA架构在每次前向传递期间通过固定标量按比例缩放每个适配器，该标量在初始化时设置，并取决于秩`r`。标量由原始实现中的`lora_alpha/r`给出，但rsLoRA使用`lora_alpha/math.sqrt(r)`，这稳定了适配器并增加了使用更高`r`的性能潜力。

```py
from peft import LoraConfig

config = LoraConfig(use_rslora=True, ...)
```

### QLoRA风格的训练

PEFT中的默认LoRA设置会向每个注意力块的查询和值层添加可训练权重。但是[QLoRA](https://hf.co/papers/2305.14314)会向变压器模型的所有线性层添加可训练权重，可以提供与完全微调模型相等的性能。要将LoRA应用于所有线性层，就像在QLoRA中一样，请设置`target_modules="all-linear"`（比根据架构的不同而变化的单独指定模块名称更容易）。

```py
config = LoraConfig(target_modules="all-linear", ...)
```

## 合并适配器

虽然LoRA明显更小更快，但在推理过程中可能会遇到延迟问题，因为需要分别加载基础模型和LoRA适配器。为了消除延迟，使用[merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)函数将适配器权重与基础模型合并。这样可以将新合并的模型用作独立模型。[merge_and_unload()](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraModel.merge_and_unload)函数不会在内存中保留适配器权重。

```py
from transformers import AutoModelForCausalLM
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
model = PeftModel.from_pretrained(base_model, peft_model_id)
model.merge_and_unload()
```

如果您需要保留权重的副本，以便稍后取消合并适配器或删除并加载不同的适配器，应该使用merge_adapter()函数。现在您可以选择使用unmerge_adapter()来返回基础模型。

```py
from transformers import AutoModelForCausalLM
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
model = PeftModel.from_pretrained(base_model, peft_model_id)
model.merge_adapter()

# unmerge the LoRA layers from the base model
model.unmerge_adapter()
```

add_weighted_adapter()函数对于根据用户提供的权重方案将多个LoRA合并成一个新的适配器非常有用，权重方案在`weights`参数中指定。下面是一个端到端的示例。

首先加载基础模型：

```py
from transformers import AutoModelForCausalLM
from peft import PeftModel
import torch

base_model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1", torch_dtype=torch.float16, device_map="auto"
)
```

然后加载第一个适配器：

```py
peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
model = PeftModel.from_pretrained(base_model, peft_model_id, adapter_name="sft")
```

然后加载一个不同的适配器并将其与第一个合并：

```py
weighted_adapter_name = "sft-dpo"
model.load_adapter("alignment-handbook/zephyr-7b-dpo-lora", adapter_name="dpo")
model.add_weighted_adapter(
    adapters=["sft", "dpo"],
    weights=[0.7, 0.3],
    adapter_name=weighted_adapter_name,
    combination_type="linear"
)
model.set_adapter(weighted_adapter_name)
```

有几种支持的`combination_type`方法。有关更多详细信息，请参阅文档。请注意，在使用`torch.float16`或`torch.bfloat16`作为数据类型时，“svd”作为`combination_type`是不受支持的。

现在，执行推理：

```py
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

prompt = "Hey, are you conscious? Can you talk to me?"
inputs = tokenizer(prompt, return_tensors="pt")
inputs = {k: v.to("cuda") for k, v in inputs.items()}

with torch.no_grad():
    generate_ids = model.generate(**inputs, max_length=30)
outputs = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
print(outputs)
```

## 加载适配器

适配器可以使用load_adapter()加载到预训练模型中，这对于尝试不合并权重的不同适配器非常有用。使用set_adapter()函数设置活动适配器的权重。

```py
from transformers import AutoModelForCausalLM
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1")
peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
model = PeftModel.from_pretrained(base_model, peft_model_id)

# load different adapter
model.load_adapter("alignment-handbook/zephyr-7b-dpo-lora", adapter_name="dpo")

# set adapter as active
model.set_adapter("dpo")
```

要返回基础模型，您可以使用unload()来卸载所有的LoRA模块，或者使用delete_adapter()来完全删除适配器。

```py
# unload adapter
model.unload()

# delete adapter
model.delete_adapter("dpo")
```
