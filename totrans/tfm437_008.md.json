["```py\npip install datasets\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n```", "```py\n>>> encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n>>> print(encoded_input)\n{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```", "```py\n>>> tokenizer.decode(encoded_input[\"input_ids\"])\n'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'\n```", "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_inputs = tokenizer(batch_sentences)\n>>> print(encoded_inputs)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1]]}\n```", "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True)\n>>> print(encoded_input)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n```", "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n>>> print(encoded_input)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n```", "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n>>> print(encoded_input)\n{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n```", "```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n>>> print(encoded_input)\n{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>,\n 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n```", "```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n```", "```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 8000}\n```", "```py\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n```", "```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```", "```py\n>>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n```", "```py\n>>> audio_input = [dataset[0][\"audio\"][\"array\"]]\n>>> feature_extractor(audio_input, sampling_rate=16000)\n{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,\n        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}\n```", "```py\n>>> dataset[0][\"audio\"][\"array\"].shape\n(173398,)\n\n>>> dataset[1][\"audio\"][\"array\"].shape\n(106496,)\n```", "```py\n>>> def preprocess_function(examples):\n...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays,\n...         sampling_rate=16000,\n...         padding=True,\n...         max_length=100000,\n...         truncation=True,\n...     )\n...     return inputs\n```", "```py\n>>> processed_dataset = preprocess_function(dataset[:5])\n```", "```py\n>>> processed_dataset[\"input_values\"][0].shape\n(100000,)\n\n>>> processed_dataset[\"input_values\"][1].shape\n(100000,)\n```", "```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"food101\", split=\"train[:100]\")\n```", "```py\n>>> dataset[0][\"image\"]\n```", "```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```", "```py\n>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n\n>>> size = (\n...     image_processor.size[\"shortest_edge\"]\n...     if \"shortest_edge\" in image_processor.size\n...     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n... )\n\n>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n```", "```py\n>>> def transforms(examples):\n...     images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n...     examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n...     return examples\n```", "```py\n>>> dataset.set_transform(transforms)\n```", "```py\n>>> dataset[0].keys()\n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n\n>>> img = dataset[0][\"pixel_values\"]\n>>> plt.imshow(img.permute(1, 2, 0))\n```", "```py\n>>> def collate_fn(batch):\n...     pixel_values = [item[\"pixel_values\"] for item in batch]\n...     encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n...     labels = [item[\"labels\"] for item in batch]\n...     batch = {}\n...     batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n...     batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n...     batch[\"labels\"] = labels\n...     return batch\n```", "```py\n>>> from datasets import load_dataset\n\n>>> lj_speech = load_dataset(\"lj_speech\", split=\"train\")\n```", "```py\n>>> lj_speech = lj_speech.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])\n```", "```py\n>>> lj_speech[0][\"audio\"]\n{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,\n         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',\n 'sampling_rate': 22050}\n\n>>> lj_speech[0][\"text\"]\n'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'\n```", "```py\n>>> lj_speech = lj_speech.cast_column(\"audio\", Audio(sampling_rate=16_000))\n```", "```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n```", "```py\n>>> def prepare_dataset(example):\n...     audio = example[\"audio\"]\n\n...     example.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\n\n...     return example\n```", "```py\n>>> prepare_dataset(lj_speech[0])\n```"]