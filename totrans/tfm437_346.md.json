["```py\n( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 16 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 **kwargs )\n```", "```py\n>>> from transformers import GitVisionConfig, GitVisionModel\n\n>>> # Initializing a GitVisionConfig with microsoft/git-base style configuration\n>>> configuration = GitVisionConfig()\n\n>>> # Initializing a GitVisionModel (with random weights) from the microsoft/git-base style configuration\n>>> model = GitVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: GitVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, GitVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n>>> model = GitVisionModel.from_pretrained(\"microsoft/git-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( vision_config = None vocab_size = 30522 hidden_size = 768 num_hidden_layers = 6 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 1024 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' use_cache = True tie_word_embeddings = False bos_token_id = 101 eos_token_id = 102 num_image_with_embedding = None **kwargs )\n```", "```py\n>>> from transformers import GitConfig, GitModel\n\n>>> # Initializing a GIT microsoft/git-base style configuration\n>>> configuration = GitConfig()\n\n>>> # Initializing a model (with random weights) from the microsoft/git-base style configuration\n>>> model = GitModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( text = None images = None return_tensors = None **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None pixel_values: Optional = None head_mask: Optional = None inputs_embeds: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, AutoModel\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n>>> model = AutoModel.from_pretrained(\"microsoft/git-base\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> text = \"this is an image of two cats\"\n\n>>> inputs = processor(text, images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None pixel_values: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, AutoModelForCausalLM\n>>> import requests\n>>> from PIL import Image\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n>>> generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n>>> generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n>>> print(generated_caption)\ntwo cats sleeping on a pink blanket next to remotes.\n```", "```py\n>>> from transformers import AutoProcessor, AutoModelForCausalLM\n>>> from huggingface_hub import hf_hub_download\n>>> from PIL import Image\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textvqa\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textvqa\")\n\n>>> file_path = hf_hub_download(repo_id=\"nielsr/textvqa-sample\", filename=\"bus.png\", repo_type=\"dataset\")\n>>> image = Image.open(file_path).convert(\"RGB\")\n\n>>> pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n>>> question = \"what does the front of the bus say at the top?\"\n\n>>> input_ids = processor(text=question, add_special_tokens=False).input_ids\n>>> input_ids = [processor.tokenizer.cls_token_id] + input_ids\n>>> input_ids = torch.tensor(input_ids).unsqueeze(0)\n\n>>> generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n>>> print(processor.batch_decode(generated_ids, skip_special_tokens=True))\n['what does the front of the bus say at the top? special']\n```", "```py\n>>> import av\n>>> import numpy as np\n>>> from PIL import Image\n>>> from huggingface_hub import hf_hub_download\n>>> from transformers import AutoProcessor, AutoModelForCausalLM\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n\n>>> # set seed for reproducability\n>>> np.random.seed(45)\n\n>>> def read_video_pyav(container, indices):\n...     '''\n...     Decode the video with PyAV decoder.\n...     Args:\n...         container (`av.container.input.InputContainer`): PyAV container.\n...         indices (`List[int]`): List of frame indices to decode.\n...     Returns:\n...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n...     '''\n...     frames = []\n...     container.seek(0)\n...     start_index = indices[0]\n...     end_index = indices[-1]\n...     for i, frame in enumerate(container.decode(video=0)):\n...         if i > end_index:\n...             break\n...         if i >= start_index and i in indices:\n...             frames.append(frame)\n...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n>>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n...     '''\n...     Sample a given number of frame indices from the video.\n...     Args:\n...         clip_len (`int`): Total number of frames to sample.\n...         frame_sample_rate (`int`): Sample every n-th frame.\n...         seg_len (`int`): Maximum allowed index of sample's last frame.\n...     Returns:\n...         indices (`List[int]`): List of sampled frame indices\n...     '''\n...     converted_len = int(clip_len * frame_sample_rate)\n...     end_idx = np.random.randint(converted_len, seg_len)\n...     start_idx = end_idx - converted_len\n...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n...     return indices\n\n>>> # load video\n>>> file_path = hf_hub_download(\n...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n... )\n>>> container = av.open(file_path)\n\n>>> # sample frames\n>>> num_frames = model.config.num_image_with_embedding\n>>> indices = sample_frame_indices(\n...     clip_len=num_frames, frame_sample_rate=4, seg_len=container.streams.video[0].frames\n... )\n>>> frames = read_video_pyav(container, indices)\n\n>>> pixel_values = processor(images=list(frames), return_tensors=\"pt\").pixel_values\n\n>>> generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n\n>>> print(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\nGenerated caption: ['a woman is sitting at a table and she is talking about the food she is holding.']\n```"]