- en: Effective and efficient diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆå’Œé«˜æ•ˆçš„æ‰©æ•£
- en: 'Original text: [https://huggingface.co/docs/diffusers/stable_diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/stable_diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Getting the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    to generate images in a certain style or include what you want can be tricky.
    Often times, you have to run the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    several times before you end up with an image youâ€™re happy with. But generating
    something out of nothing is a computationally intensive process, especially if
    youâ€™re running inference over and over again.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è®©[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ç”Ÿæˆç‰¹å®šé£æ ¼çš„å›¾åƒæˆ–åŒ…å«æ‚¨æƒ³è¦çš„å†…å®¹å¯èƒ½æœ‰äº›æ£˜æ‰‹ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæ‚¨å¿…é¡»å¤šæ¬¡è¿è¡Œ[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)æ‰èƒ½å¾—åˆ°æ»¡æ„çš„å›¾åƒã€‚ä½†æ˜¯ä»æ— åˆ°æœ‰åœ°ç”Ÿæˆä¸œè¥¿æ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹çš„è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæ‚¨ä¸€éåˆä¸€éåœ°è¿è¡Œæ¨ç†ã€‚
- en: This is why itâ€™s important to get the most *computational* (speed) and *memory*
    (GPU vRAM) efficiency from the pipeline to reduce the time between inference cycles
    so you can iterate faster.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä»ç®¡é“ä¸­è·å¾—æœ€é«˜çš„*è®¡ç®—*ï¼ˆé€Ÿåº¦ï¼‰å’Œ*å†…å­˜*ï¼ˆGPU vRAMï¼‰æ•ˆç‡éå¸¸é‡è¦ï¼Œä»¥å‡å°‘æ¨ç†å‘¨æœŸä¹‹é—´çš„æ—¶é—´ï¼Œä½¿æ‚¨å¯ä»¥æ›´å¿«åœ°è¿­ä»£ã€‚
- en: This tutorial walks you through how to generate faster and better with the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨å¦‚ä½•ä½¿ç”¨[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)æ›´å¿«æ›´å¥½åœ°ç”Ÿæˆã€‚
- en: 'Begin by loading the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    model:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆåŠ è½½[`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)æ¨¡å‹ï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The example prompt youâ€™ll use is a portrait of an old warrior chief, but feel
    free to use your own prompt:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†ä½¿ç”¨çš„ç¤ºä¾‹æç¤ºæ˜¯ä¸€ä½è€æˆ˜å£«é¦–é¢†çš„è‚–åƒï¼Œä½†è¯·éšæ„ä½¿ç”¨æ‚¨è‡ªå·±çš„æç¤ºï¼š
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Speed
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€Ÿåº¦
- en: ğŸ’¡ If you donâ€™t have access to a GPU, you can use one for free from a GPU provider
    like [Colab](https://colab.research.google.com/)!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å¦‚æœæ‚¨æ²¡æœ‰GPUè®¿é—®æƒé™ï¼Œæ‚¨å¯ä»¥å…è´¹ä½¿ç”¨GPUæä¾›å•†ï¼ˆå¦‚[Colab](https://colab.research.google.com/)ï¼‰çš„GPUï¼
- en: 'One of the simplest ways to speed up inference is to place the pipeline on
    a GPU the same way you would with any PyTorch module:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å¿«æ¨ç†çš„æœ€ç®€å•æ–¹æ³•ä¹‹ä¸€æ˜¯å°†ç®¡é“æ”¾åœ¨GPUä¸Šï¼Œå°±åƒæ‚¨å¯¹ä»»ä½•PyTorchæ¨¡å—æ‰€åšçš„é‚£æ ·ï¼š
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To make sure you can use the same image and improve on it, use a [`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    and set a seed for [reproducibility](./using-diffusers/reproducibility):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿æ‚¨å¯ä»¥ä½¿ç”¨ç›¸åŒçš„å›¾åƒå¹¶å¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œè¯·ä½¿ç”¨[`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)å¹¶ä¸º[å¯é‡ç°æ€§](./using-diffusers/reproducibility)è®¾ç½®ä¸€ä¸ªç§å­ï¼š
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now you can generate an image:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨å¯ä»¥ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/97b4b76d7c052f96d96b7a8b402a25ce.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97b4b76d7c052f96d96b7a8b402a25ce.png)'
- en: This process took ~30 seconds on a T4 GPU (it might be faster if your allocated
    GPU is better than a T4). By default, the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    runs inference with full `float32` precision for 50 inference steps. You can speed
    this up by switching to a lower precision like `float16` or running fewer inference
    steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹åœ¨T4 GPUä¸ŠèŠ±è´¹äº†~30ç§’ï¼ˆå¦‚æœæ‚¨åˆ†é…çš„GPUæ¯”T4æ›´å¥½ï¼Œåˆ™å¯èƒ½ä¼šæ›´å¿«ï¼‰ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ä»¥å®Œæ•´çš„`float32`ç²¾åº¦è¿è¡Œ50ä¸ªæ¨ç†æ­¥éª¤ã€‚æ‚¨å¯ä»¥é€šè¿‡åˆ‡æ¢åˆ°è¾ƒä½ç²¾åº¦ï¼ˆå¦‚`float16`ï¼‰æˆ–å‡å°‘æ¨ç†æ­¥éª¤æ¥åŠ å¿«é€Ÿåº¦ã€‚
- en: 'Letâ€™s start by loading the model in `float16` and generate an image:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»åœ¨`float16`ä¸­åŠ è½½æ¨¡å‹å¹¶ç”Ÿæˆå›¾åƒå¼€å§‹ï¼š
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/66880d2cc1c9bb78b8e03483b16d1ca3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66880d2cc1c9bb78b8e03483b16d1ca3.png)'
- en: This time, it only took ~11 seconds to generate the image, which is almost 3x
    faster than before!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ï¼Œç”Ÿæˆå›¾åƒä»…èŠ±è´¹äº†~11ç§’ï¼Œæ¯”ä»¥å‰å¿«äº†è¿‘3å€ï¼
- en: ğŸ’¡ We strongly suggest always running your pipelines in `float16`, and so far,
    weâ€™ve rarely seen any degradation in output quality.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘ä»¬å¼ºçƒˆå»ºè®®å§‹ç»ˆåœ¨`float16`ä¸­è¿è¡Œæ‚¨çš„ç®¡é“ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¾ˆå°‘çœ‹åˆ°è¾“å‡ºè´¨é‡ä¸‹é™ã€‚
- en: 'Another option is to reduce the number of inference steps. Choosing a more
    efficient scheduler could help decrease the number of steps without sacrificing
    output quality. You can find which schedulers are compatible with the current
    model in the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    by calling the `compatibles` method:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé€‰é¡¹æ˜¯å‡å°‘æ¨ç†æ­¥éª¤çš„æ•°é‡ã€‚é€‰æ‹©æ›´é«˜æ•ˆçš„è°ƒåº¦å™¨å¯ä»¥å¸®åŠ©å‡å°‘æ­¥éª¤æ•°é‡ï¼Œè€Œä¸ä¼šç‰ºç‰²è¾“å‡ºè´¨é‡ã€‚æ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨`compatibles`æ–¹æ³•åœ¨[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ä¸­æ‰¾åˆ°ä¸å½“å‰æ¨¡å‹å…¼å®¹çš„è°ƒåº¦å™¨ï¼š
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Stable Diffusion model uses the [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    by default which usually requires ~50 inference steps, but more performant schedulers
    like [DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler),
    require only ~20 or 25 inference steps. Use the [from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)
    method to load a new scheduler:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šçš„æ‰©æ•£æ¨¡å‹é»˜è®¤ä½¿ç”¨[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)ï¼Œé€šå¸¸éœ€è¦~50ä¸ªæ¨ç†æ­¥éª¤ï¼Œä½†æ€§èƒ½æ›´å¥½çš„è°ƒåº¦å™¨å¦‚[DPMSolverMultistepScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler)ï¼Œåªéœ€è¦~20æˆ–25ä¸ªæ¨ç†æ­¥éª¤ã€‚ä½¿ç”¨[from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)æ–¹æ³•åŠ è½½æ–°çš„è°ƒåº¦å™¨ï¼š
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now set the `num_inference_steps` to 20:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°†`num_inference_steps`è®¾ç½®ä¸º20ï¼š
- en: '[PRE8]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/a0e3931ebf91a12e3caef2d780586fec.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0e3931ebf91a12e3caef2d780586fec.png)'
- en: Great, youâ€™ve managed to cut the inference time to just 4 seconds! âš¡ï¸
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæ‚¨å·²ç»æˆåŠŸå°†æ¨ç†æ—¶é—´ç¼©çŸ­åˆ°ä»…ä¸º4ç§’ï¼âš¡ï¸
- en: Memory
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å†…å­˜
- en: The other key to improving pipeline performance is consuming less memory, which
    indirectly implies more speed, since youâ€™re often trying to maximize the number
    of images generated per second. The easiest way to see how many images you can
    generate at once is to try out different batch sizes until you get an `OutOfMemoryError`
    (OOM).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¹è¿›ç®¡é“æ€§èƒ½çš„å¦ä¸€ä¸ªå…³é”®æ˜¯æ¶ˆè€—æ›´å°‘çš„å†…å­˜ï¼Œè¿™é—´æ¥æ„å‘³ç€æ›´å¿«çš„é€Ÿåº¦ï¼Œå› ä¸ºæ‚¨é€šå¸¸è¯•å›¾æœ€å¤§åŒ–æ¯ç§’ç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚æŸ¥çœ‹æ‚¨å¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šå°‘å›¾åƒçš„æœ€ç®€å•æ–¹æ³•æ˜¯å°è¯•ä¸åŒçš„æ‰¹å¤„ç†å¤§å°ï¼Œç›´åˆ°å‡ºç°`OutOfMemoryError`ï¼ˆOOMï¼‰ã€‚
- en: Create a function thatâ€™ll generate a batch of images from a list of prompts
    and `Generators`. Make sure to assign each `Generator` a seed so you can reuse
    it if it produces a good result.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œä»æç¤ºå’Œ`Generators`åˆ—è¡¨ä¸­ç”Ÿæˆä¸€æ‰¹å›¾åƒã€‚ç¡®ä¿ä¸ºæ¯ä¸ª`Generator`åˆ†é…ä¸€ä¸ªç§å­ï¼Œè¿™æ ·å¦‚æœå®ƒäº§ç”Ÿäº†å¥½çš„ç»“æœï¼Œä½ å°±å¯ä»¥é‡å¤ä½¿ç”¨å®ƒã€‚
- en: '[PRE9]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Start with `batch_size=4` and see how much memory youâ€™ve consumed:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä»`batch_size=4`å¼€å§‹ï¼Œçœ‹çœ‹ä½ æ¶ˆè€—äº†å¤šå°‘å†…å­˜ï¼š
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Unless you have a GPU with more vRAM, the code above probably returned an `OOM`
    error! Most of the memory is taken up by the cross-attention layers. Instead of
    running this operation in a batch, you can run it sequentially to save a significant
    amount of memory. All you have to do is configure the pipeline to use the [enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)
    function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éä½ æœ‰æ›´å¤švRAMçš„GPUï¼Œä¸Šé¢çš„ä»£ç å¯èƒ½è¿”å›äº†ä¸€ä¸ª`OOM`é”™è¯¯ï¼å¤§éƒ¨åˆ†å†…å­˜è¢«äº¤å‰æ³¨æ„åŠ›å±‚å ç”¨ã€‚ä½ å¯ä»¥å°†è¿™ä¸ªæ“ä½œé¡ºåºè¿è¡Œè€Œä¸æ˜¯æ‰¹é‡è¿è¡Œï¼Œä»¥èŠ‚çœå¤§é‡å†…å­˜ã€‚ä½ åªéœ€è¦é…ç½®ç®¡é“ä½¿ç”¨[enable_attention_slicing()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_attention_slicing)å‡½æ•°ï¼š
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now try increasing the `batch_size` to 8!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å°è¯•å°†`batch_size`å¢åŠ åˆ°8ï¼
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/f242684980096d86bea7fa383ccd606a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f242684980096d86bea7fa383ccd606a.png)'
- en: Whereas before you couldnâ€™t even generate a batch of 4 images, now you can generate
    a batch of 8 images at ~3.5 seconds per image! This is probably the fastest you
    can go on a T4 GPU without sacrificing quality.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ä½ ç”šè‡³ä¸èƒ½ç”Ÿæˆä¸€æ‰¹4å¼ å›¾åƒï¼Œç°åœ¨ä½ å¯ä»¥ä»¥æ¯å¼ å›¾åƒçº¦3.5ç§’çš„é€Ÿåº¦ç”Ÿæˆä¸€æ‰¹8å¼ å›¾åƒï¼è¿™å¯èƒ½æ˜¯åœ¨T4 GPUä¸Šä¿æŒè´¨é‡çš„æœ€å¿«é€Ÿåº¦äº†ã€‚
- en: Quality
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è´¨é‡
- en: In the last two sections, you learned how to optimize the speed of your pipeline
    by using `fp16`, reducing the number of inference steps by using a more performant
    scheduler, and enabling attention slicing to reduce memory consumption. Now youâ€™re
    going to focus on how to improve the quality of generated images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸¤èŠ‚ä¸­ï¼Œä½ å­¦ä¼šäº†å¦‚ä½•é€šè¿‡ä½¿ç”¨`fp16`æ¥ä¼˜åŒ–ç®¡é“çš„é€Ÿåº¦ï¼Œé€šè¿‡ä½¿ç”¨æ›´é«˜æ€§èƒ½çš„è°ƒåº¦ç¨‹åºå‡å°‘æ¨ç†æ­¥éª¤çš„æ•°é‡ï¼Œå¹¶å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡æ¥å‡å°‘å†…å­˜æ¶ˆè€—ã€‚ç°åœ¨ä½ è¦ä¸“æ³¨äºå¦‚ä½•æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚
- en: Better checkpoints
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„æ£€æŸ¥ç‚¹
- en: The most obvious step is to use better checkpoints. The Stable Diffusion model
    is a good starting point, and since its official launch, several improved versions
    have also been released. However, using a newer version doesnâ€™t automatically
    mean youâ€™ll get better results. Youâ€™ll still have to experiment with different
    checkpoints yourself, and do a little research (such as using [negative prompts](https://minimaxir.com/2022/11/stable-diffusion-negative-prompt/))
    to get the best results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æ˜æ˜¾çš„ä¸€æ­¥æ˜¯ä½¿ç”¨æ›´å¥½çš„æ£€æŸ¥ç‚¹ã€‚ç¨³å®šæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œè‡ªä»å®ƒæ­£å¼æ¨å‡ºä»¥æ¥ï¼Œä¹Ÿå‘å¸ƒäº†å‡ ä¸ªæ”¹è¿›ç‰ˆæœ¬ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ›´æ–°ç‰ˆæœ¬å¹¶ä¸æ„å‘³ç€ä½ ä¼šè‡ªåŠ¨è·å¾—æ›´å¥½çš„ç»“æœã€‚ä½ ä»ç„¶éœ€è¦è‡ªå·±å°è¯•ä¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œå¹¶è¿›è¡Œä¸€äº›ç ”ç©¶ï¼ˆæ¯”å¦‚ä½¿ç”¨[è´Ÿé¢æç¤º](https://minimaxir.com/2022/11/stable-diffusion-negative-prompt/)ï¼‰æ¥è·å¾—æœ€ä½³ç»“æœã€‚
- en: As the field grows, there are more and more high-quality checkpoints finetuned
    to produce certain styles. Try exploring the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads)
    and [Diffusers Gallery](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)
    to find one youâ€™re interested in!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€é¢†åŸŸçš„å‘å±•ï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„ç»è¿‡å¾®è°ƒçš„é«˜è´¨é‡æ£€æŸ¥ç‚¹ï¼Œç”¨äºäº§ç”Ÿç‰¹å®šé£æ ¼ã€‚å°è¯•æ¢ç´¢[Hub](https://huggingface.co/models?library=diffusers&sort=downloads)å’Œ[Diffusers
    Gallery](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery)ï¼Œæ‰¾åˆ°ä½ æ„Ÿå…´è¶£çš„ä¸€ä¸ªï¼
- en: Better pipeline components
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„ç®¡é“ç»„ä»¶
- en: 'You can also try replacing the current pipeline components with a newer version.
    Letâ€™s try loading the latest [autoencoder](https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main/vae)
    from Stability AI into the pipeline, and generate some images:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥å°è¯•ç”¨æ›´æ–°ç‰ˆæœ¬æ›¿æ¢å½“å‰çš„ç®¡é“ç»„ä»¶ã€‚è®©æˆ‘ä»¬å°è¯•å°†Stability AIçš„æœ€æ–°[è‡ªåŠ¨ç¼–ç å™¨](https://huggingface.co/stabilityai/stable-diffusion-2-1/tree/main/vae)åŠ è½½åˆ°ç®¡é“ä¸­ï¼Œå¹¶ç”Ÿæˆä¸€äº›å›¾åƒï¼š
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/f35b7f9a028460ebed6e9b753a1241d3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f35b7f9a028460ebed6e9b753a1241d3.png)'
- en: Better prompt engineering
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„æç¤ºå·¥ç¨‹
- en: 'The text prompt you use to generate an image is super important, so much so
    that it is called *prompt engineering*. Some considerations to keep during prompt
    engineering are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ç”¨æ¥ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æç¤ºéå¸¸é‡è¦ï¼Œä»¥è‡³äºè¢«ç§°ä¸º*æç¤ºå·¥ç¨‹*ã€‚åœ¨è¿›è¡Œæç¤ºå·¥ç¨‹æ—¶è¦è€ƒè™‘çš„ä¸€äº›å› ç´ æ˜¯ï¼š
- en: How is the image or similar images of the one I want to generate stored on the
    internet?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒæˆ–ç±»ä¼¼æˆ‘æƒ³ç”Ÿæˆçš„å›¾åƒåœ¨äº’è”ç½‘ä¸Šæ˜¯å¦‚ä½•å­˜å‚¨çš„ï¼Ÿ
- en: What additional detail can I give that steers the model towards the style I
    want?
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥æä¾›ä»€ä¹ˆé¢å¤–çš„ç»†èŠ‚æ¥å¼•å¯¼æ¨¡å‹æœç€æˆ‘æƒ³è¦çš„é£æ ¼å‘å±•ï¼Ÿ
- en: 'With this in mind, letâ€™s improve the prompt to include color and higher quality
    details:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ”¹è¿›æç¤ºï¼ŒåŒ…æ‹¬é¢œè‰²å’Œæ›´é«˜è´¨é‡çš„ç»†èŠ‚ï¼š
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Generate a batch of images with the new prompt:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨æ–°æç¤ºç”Ÿæˆä¸€æ‰¹å›¾åƒï¼š
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/52c687d1d7c73bcf4a482dc45feea714.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52c687d1d7c73bcf4a482dc45feea714.png)'
- en: 'Pretty impressive! Letâ€™s tweak the second image - corresponding to the `Generator`
    with a seed of `1` - a bit more by adding some text about the age of the subject:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å½“ä»¤äººå°è±¡æ·±åˆ»ï¼è®©æˆ‘ä»¬å¾®è°ƒç¬¬äºŒå¼ å›¾åƒ - å¯¹åº”äºç§å­ä¸º`1`çš„`Generator` - æ·»åŠ ä¸€äº›å…³äºä¸»é¢˜å¹´é¾„çš„æ–‡å­—ï¼š
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/181d5d3b3b595630cddf8064c033ea4f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/181d5d3b3b595630cddf8064c033ea4f.png)'
- en: Next steps
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'In this tutorial, you learned how to optimize a [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    for computational and memory efficiency as well as improving the quality of generated
    outputs. If youâ€™re interested in making your pipeline even faster, take a look
    at the following resources:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å­¦ä¼šäº†å¦‚ä½•ä¸ºè®¡ç®—å’Œå†…å­˜æ•ˆç‡ä»¥åŠæ”¹å–„ç”Ÿæˆè¾“å‡ºè´¨é‡ä¼˜åŒ–[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚å¦‚æœä½ æœ‰å…´è¶£è®©ä½ çš„ç®¡é“è¿è¡Œå¾—æ›´å¿«ï¼Œå¯ä»¥æŸ¥çœ‹ä»¥ä¸‹èµ„æºï¼š
- en: Learn how [PyTorch 2.0](./optimization/torch2.0) and [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)
    can yield 5 - 300% faster inference speed. On an A100 GPU, inference can be up
    to 50% faster!
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£å¦‚ä½•ä½¿ç”¨[PyTorch 2.0](./optimization/torch2.0)å’Œ[`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)å¯ä»¥æé«˜5-300%çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨A100
    GPUä¸Šï¼Œæ¨ç†é€Ÿåº¦å¯ä»¥æé«˜50%ï¼
- en: If you canâ€™t use PyTorch 2, we recommend you install [xFormers](./optimization/xformers).
    Its memory-efficient attention mechanism works great with PyTorch 1.13.1 for faster
    speed and reduced memory consumption.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä¸èƒ½ä½¿ç”¨PyTorch 2ï¼Œæˆ‘ä»¬å»ºè®®ä½ å®‰è£…[xFormers](./optimization/xformers)ã€‚å®ƒçš„å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ä¸PyTorch
    1.13.1éå¸¸æ­é…ï¼Œå¯ä»¥å®ç°æ›´å¿«çš„é€Ÿåº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚
- en: Other optimization techniques, such as model offloading, are covered in [this
    guide](./optimization/fp16).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¯”å¦‚æ¨¡å‹å¸è½½ï¼Œåœ¨[è¿™ä¸ªæŒ‡å—](./optimization/fp16)ä¸­æœ‰ä»‹ç»ã€‚
