- en: Language models in RL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的语言模型
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models](https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models](https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: LMs encode useful knowledge for agents
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型为代理编码了有用的知识
- en: '**Language models** (LMs) can exhibit impressive abilities when manipulating
    text such as question-answering or even step-by-step reasoning. Additionally,
    their training on massive text corpora allowed them to **encode various types
    of knowledge including abstract ones about the physical rules of our world** (for
    instance what is possible to do with an object, what happens when one rotates
    an object…).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型**（LMs）在处理文本时可以展示出令人印象深刻的能力，例如问答或逐步推理。此外，它们在大规模文本语料库上的训练使它们能够**编码各种类型的知识，包括关于我们世界的物理规则的抽象知识**（例如使用物体时可能发生的情况，旋转物体时会发生什么...）。'
- en: A natural question recently studied was whether such knowledge could benefit
    agents such as robots when trying to solve everyday tasks. And while these works
    showed interesting results, the proposed agents lacked any learning method. **This
    limitation prevents these agent from adapting to the environment (e.g. fixing
    wrong knowledge) or learning new skills.**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最近研究的一个自然问题是，这种知识是否能够使代理（如机器人）在尝试解决日常任务时受益。虽然这些工作显示了有趣的结果，但所提出的代理缺乏任何学习方法。**这种限制阻碍了这些代理适应环境（例如修正错误知识）或学习新技能。**
- en: '![Language](../Images/d3604ad1da0cba5f90b5a666458c0220.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![语言](../Images/d3604ad1da0cba5f90b5a666458c0220.png)'
- en: 'Source: [Towards Helpful Robots: Grounding Language in Robotic Affordances](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[朝着有用的机器人：将语言基于机器人的可供性](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)
- en: LMs and RL
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型和强化学习
- en: 'There is therefore a potential synergy between LMs which can bring knowledge
    about the world, and RL which can align and correct this knowledge by interacting
    with an environment. It is especially interesting from a RL point-of-view as the
    RL field mostly relies on the **Tabula-rasa** setup where everything is learned
    from scratch by the agent leading to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语言模型和强化学习之间存在潜在的协同作用，语言模型可以为世界带来知识，而强化学习可以通过与环境的交互来对齐和纠正这些知识。从强化学习的角度来看，这是特别有趣的，因为强化学习领域主要依赖于**Tabula-rasa**设置，其中一切都是由代理从头学习的，导致：
- en: 1) Sample inefficiency
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 1）样本效率
- en: 2) Unexpected behaviors from humans’ eyes
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2）从人类眼中看到的意外行为
- en: As a first attempt, the paper [“Grounding Large Language Models with Online
    Reinforcement Learning”](https://arxiv.org/abs/2302.02662v1) tackled the problem
    of **adapting or aligning a LM to a textual environment using PPO**. They showed
    that the knowledge encoded in the LM lead to a fast adaptation to the environment
    (opening avenues for sample efficient RL agents) but also that such knowledge
    allowed the LM to better generalize to new tasks once aligned.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一次尝试，论文[“使用在线强化学习对大型语言模型进行基础”](https://arxiv.org/abs/2302.02662v1)解决了**使用PPO将语言模型适应或对齐到文本环境的问题**。他们表明，语言模型中编码的知识导致对环境的快速适应（为样本有效的RL代理开辟了途径），但也表明这种知识使得一旦对齐，语言模型能够更好地推广到新任务。
- en: <https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/papier_v4.mp4>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/papier_v4.mp4>
- en: Another direction studied in [“Guiding Pretraining in Reinforcement Learning
    with Large Language Models”](https://arxiv.org/abs/2302.06692) was to keep the
    LM frozen but leverage its knowledge to **guide an RL agent’s exploration**. Such
    a method allows the RL agent to be guided towards human-meaningful and plausibly
    useful behaviors without requiring a human in the loop during training.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个研究方向在[“使用大型语言模型引导强化学习的预训练”](https://arxiv.org/abs/2302.06692)中探讨了保持语言模型冻结但利用其知识来**引导RL代理的探索**。这种方法允许RL代理被引导朝着人类有意义且可能有用的行为方向发展，而无需在训练过程中需要人类参与。
- en: '![Language](../Images/c2a676d4648b2bca76ad77fd0935c33c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![语言](../Images/c2a676d4648b2bca76ad77fd0935c33c.png)'
- en: 'Source: [Towards Helpful Robots: Grounding Language in Robotic Affordances](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[朝着有用的机器人：将语言基于机器人的可供性](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)
- en: Several limitations make these works still very preliminary such as the need
    to convert the agent’s observation to text before giving it to a LM as well as
    the compute cost of interacting with very large LMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些限制使这些工作仍然非常初步，例如需要将代理的观察转换为文本然后再将其提供给语言模型，以及与非常大的语言模型交互的计算成本。
- en: Further reading
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information we recommend you check out the following resources:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，我们建议您查看以下资源：
- en: '[Google Research, 2022 & beyond: Robotics](https://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Google Research，2022年及以后：机器人技术](https://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html)'
- en: '[Pre-Trained Language Models for Interactive Decision-Making](https://arxiv.org/abs/2202.01771)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于交互式决策制定的预训练语言模型](https://arxiv.org/abs/2202.01771)'
- en: '[Grounding Large Language Models with Online Reinforcement Learning](https://arxiv.org/abs/2302.02662v1)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用在线强化学习对大型语言模型进行基础](https://arxiv.org/abs/2302.02662v1)'
- en: '[Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型语言模型引导强化学习的预训练](https://arxiv.org/abs/2302.06692)'
- en: Author
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Clément Romac](https://twitter.com/ClementRomac)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节由[Clément Romac](https://twitter.com/ClementRomac)撰写
