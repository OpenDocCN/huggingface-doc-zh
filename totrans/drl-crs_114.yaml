- en: Language models in RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models](https://huggingface.co/learn/deep-rl-course/unitbonus3/language-models)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/103.85c29ef1.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">
  prefs: []
  type: TYPE_NORMAL
- en: LMs encode useful knowledge for agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Language models** (LMs) can exhibit impressive abilities when manipulating
    text such as question-answering or even step-by-step reasoning. Additionally,
    their training on massive text corpora allowed them to **encode various types
    of knowledge including abstract ones about the physical rules of our world** (for
    instance what is possible to do with an object, what happens when one rotates
    an object…).'
  prefs: []
  type: TYPE_NORMAL
- en: A natural question recently studied was whether such knowledge could benefit
    agents such as robots when trying to solve everyday tasks. And while these works
    showed interesting results, the proposed agents lacked any learning method. **This
    limitation prevents these agent from adapting to the environment (e.g. fixing
    wrong knowledge) or learning new skills.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Language](../Images/d3604ad1da0cba5f90b5a666458c0220.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Towards Helpful Robots: Grounding Language in Robotic Affordances](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)'
  prefs: []
  type: TYPE_NORMAL
- en: LMs and RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is therefore a potential synergy between LMs which can bring knowledge
    about the world, and RL which can align and correct this knowledge by interacting
    with an environment. It is especially interesting from a RL point-of-view as the
    RL field mostly relies on the **Tabula-rasa** setup where everything is learned
    from scratch by the agent leading to:'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Sample inefficiency
  prefs: []
  type: TYPE_NORMAL
- en: 2) Unexpected behaviors from humans’ eyes
  prefs: []
  type: TYPE_NORMAL
- en: As a first attempt, the paper [“Grounding Large Language Models with Online
    Reinforcement Learning”](https://arxiv.org/abs/2302.02662v1) tackled the problem
    of **adapting or aligning a LM to a textual environment using PPO**. They showed
    that the knowledge encoded in the LM lead to a fast adaptation to the environment
    (opening avenues for sample efficient RL agents) but also that such knowledge
    allowed the LM to better generalize to new tasks once aligned.
  prefs: []
  type: TYPE_NORMAL
- en: <https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/papier_v4.mp4>
  prefs: []
  type: TYPE_NORMAL
- en: Another direction studied in [“Guiding Pretraining in Reinforcement Learning
    with Large Language Models”](https://arxiv.org/abs/2302.06692) was to keep the
    LM frozen but leverage its knowledge to **guide an RL agent’s exploration**. Such
    a method allows the RL agent to be guided towards human-meaningful and plausibly
    useful behaviors without requiring a human in the loop during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Language](../Images/c2a676d4648b2bca76ad77fd0935c33c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Towards Helpful Robots: Grounding Language in Robotic Affordances](https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Several limitations make these works still very preliminary such as the need
    to convert the agent’s observation to text before giving it to a LM as well as
    the compute cost of interacting with very large LMs.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information we recommend you check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Google Research, 2022 & beyond: Robotics](https://ai.googleblog.com/2023/02/google-research-2022-beyond-robotics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pre-Trained Language Models for Interactive Decision-Making](https://arxiv.org/abs/2202.01771)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Grounding Large Language Models with Online Reinforcement Learning](https://arxiv.org/abs/2302.02662v1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Guiding Pretraining in Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2302.06692)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section was written by [Clément Romac](https://twitter.com/ClementRomac)
  prefs: []
  type: TYPE_NORMAL
