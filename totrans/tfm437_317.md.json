["```py\n>>> import torch\n>>> from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel\n>>> from datasets import load_dataset\n>>> import soundfile as sf\n\n>>> model = SpeechEncoderDecoderModel.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n>>> processor = Speech2Text2Processor.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n\n>>> def map_to_array(batch):\n...     speech, _ = sf.read(batch[\"file\"])\n...     batch[\"speech\"] = speech\n...     return batch\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> ds = ds.map(map_to_array)\n\n>>> inputs = processor(ds[\"speech\"][0], sampling_rate=16_000, return_tensors=\"pt\")\n>>> generated_ids = model.generate(inputs=inputs[\"input_values\"], attention_mask=inputs[\"attention_mask\"])\n\n>>> transcription = processor.batch_decode(generated_ids)\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import pipeline\n\n>>> librispeech_en = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> asr = pipeline(\n...     \"automatic-speech-recognition\",\n...     model=\"facebook/s2t-wav2vec2-large-en-de\",\n...     feature_extractor=\"facebook/s2t-wav2vec2-large-en-de\",\n... )\n\n>>> translation_de = asr(librispeech_en[0][\"file\"])\n```", "```py\n( vocab_size = 10000 decoder_layers = 6 decoder_ffn_dim = 2048 decoder_attention_heads = 4 decoder_layerdrop = 0.0 use_cache = True activation_function = 'relu' d_model = 256 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 2 scale_embedding = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 max_target_positions = 1024 **kwargs )\n```", "```py\n>>> from transformers import Speech2Text2Config, Speech2Text2ForCausalLM\n\n>>> # Initializing a Speech2Text2 s2t_transformer_s style configuration\n>>> configuration = Speech2Text2Config()\n\n>>> # Initializing a model (with random weights) from the s2t_transformer_s style configuration\n>>> model = Speech2Text2ForCausalLM(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' pad_token = '<pad>' eos_token = '</s>' unk_token = '<unk>' do_lower_case = False merges_file = None **kwargs )\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( feature_extractor tokenizer )\n```", "```py\n( *args **kwargs )\n```", "```py\n( pretrained_model_name_or_path: Union cache_dir: Union = None force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n( save_directory push_to_hub: bool = False **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import (\n...     SpeechEncoderDecoderModel,\n...     Speech2Text2ForCausalLM,\n...     Wav2Vec2Model,\n...     Speech2Text2Config,\n...     Wav2Vec2Config,\n...     Wav2Vec2FeatureExtractor,\n...     Speech2Text2Tokenizer,\n... )\n>>> from datasets import load_dataset\n\n>>> feature_extractor = Wav2Vec2FeatureExtractor()\n>>> tokenizer = Speech2Text2Tokenizer.from_pretrained(\"facebook/s2t-wav2vec2-large-en-de\")\n\n>>> encoder = Wav2Vec2Model(Wav2Vec2Config())\n>>> decoder = Speech2Text2ForCausalLM(Speech2Text2Config())\n>>> # init random speech2text model\n\n>>> model = SpeechEncoderDecoderModel(encoder=encoder, decoder=decoder)\n>>> model.config.pad_token_id = tokenizer.pad_token_id\n>>> model.config.decoder_start_token_id = tokenizer.bos_token_id\n>>> # pre-process inputs and labels\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> inputs = feature_extractor(\n...     ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\"\n... )\n>>> input_values = inputs.input_values\n>>> decoder_input_ids = tokenizer(ds[0][\"text\"], return_tensors=\"pt\").input_ids\n>>> # compute loss\n\n>>> loss = model(inputs=input_values, labels=decoder_input_ids).loss\n>>> # backprop loss\n\n>>> loss.backward()\n```"]