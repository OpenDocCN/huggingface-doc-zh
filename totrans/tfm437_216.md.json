["```py\n( vocab_size = 96103 max_position_embeddings = 16384 encoder_layers = 16 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 16 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 0 scale_embedding = True pad_token_id = 0 eos_token_id = 1 forced_eos_token_id = 1 num_global_tokens = 32 block_size = 512 stagger_local_blocks = True **kwargs )\n```", "```py\n>>> from transformers import PegasusXConfig, PegasusXModel\n\n>>> # Initializing a PEGASUS google/pegasus-x-large style configuration\n>>> configuration = PegasusXConfig()\n\n>>> # Initializing a model (with random weights) from the google/pegasus-x-large style configuration\n>>> model = PegasusXModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: PegasusXConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PegasusModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n>>> model = PegasusModel.from_pretrained(\"google/pegasus-x-large\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_inputs = tokenizer(\"Studies show that\", return_tensors=\"pt\")\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 4, 1024]\n```", "```py\n( config: PegasusXConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PegasusXForConditionalGeneration\n\n>>> model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-x-large\")\n\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"])\n>>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n```"]