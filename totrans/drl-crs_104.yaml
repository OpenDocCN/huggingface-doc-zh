- en: Introduction to PPO with Sample-Factory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit8/introduction-sf](https://huggingface.co/learn/deep-rl-course/unit8/introduction-sf)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '![thumbnail](../Images/1636e0752d93a1e41c70f4a1147a2563.png)'
  prefs: []
  type: TYPE_IMG
- en: In this second part of Unit 8, weâ€™ll get deeper into PPO optimization by using
    [Sample-Factory](https://samplefactory.dev/), an **asynchronous implementation
    of the PPO algorithm**, to train our agent to play [vizdoom](https://vizdoom.cs.put.edu.pl/)
    (an open source version of Doom).
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook, **youâ€™ll train your agent to play the Health Gathering level**,
    where the agent must collect health packs to avoid dying. After that, you can
    **train your agent to play more complex levels, such as Deathmatch**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Environment](../Images/3244d92e568ba653445e92e451579990.png)'
  prefs: []
  type: TYPE_IMG
- en: Sound exciting? Letâ€™s get started! ðŸš€
  prefs: []
  type: TYPE_NORMAL
- en: The hands-on is made by [Edward Beeching](https://twitter.com/edwardbeeching),
    a Machine Learning Research Scientist at Hugging Face. He worked on Godot Reinforcement
    Learning Agents, an open-source interface for developing environments and agents
    in the Godot Game Engine.
  prefs: []
  type: TYPE_NORMAL
