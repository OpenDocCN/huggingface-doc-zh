# 图像到图像

> 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/img2img](https://huggingface.co/docs/diffusers/using-diffusers/img2img)

图像到图像类似于 [文本到图像](conditional_image_generation)，但除了一个提示外，您还可以传递一个初始图像作为扩散过程的起点。初始图像被编码到潜在空间中，并向其添加噪声。然后，潜在扩散模型接收一个提示和嘈杂的潜在图像，预测添加的噪声，并从初始潜在图像中去除预测的噪声以获得新的潜在图像。最后，解码器将新的潜在图像解码回图像。

使用 🤗 Diffusers，这就像 1-2-3 一样简单：

1.  将检查点加载到 [AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image) 类中；此管道会根据检查点自动处理加载正确的管道类：

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()
```

在整个指南中，您会注意到我们使用 [enable_model_cpu_offload()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/gligen#diffusers.StableDiffusionGLIGENTextImagePipeline.enable_model_cpu_offload) 和 [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)，以节省内存并增加推理速度。如果您使用的是 PyTorch 2.0，则不需要在管道上调用 [enable_xformers_memory_efficient_attention()](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.StableDiffusionUpscalePipeline.enable_xformers_memory_efficient_attention)，因为它已经在使用 PyTorch 2.0 的本机 [scaled-dot product attention](../optimization/torch2.0#scaled-dot-product-attention)。

1.  加载一个图像传递给管道：

```py
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png")
```

1.  传递一个提示和图像给管道以生成一个图像：

```py
prompt = "cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k"
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/b7dea39428bc1d2cd88a6400783552cb.png)

初始图像

![](../Images/54142d2106911436dd03998048a92fad.png)

生成的图像

## 流行的模型

最受欢迎的图像到图像模型是 [稳定扩散 v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)、[稳定扩散 XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) 和 [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)。由于它们的架构差异和训练过程的不同，稳定扩散和 Kandinsky 模型的结果会有所不同；通常可以预期 SDXL 会产生比稳定扩散 v1.5 更高质量的图像。让我们快速看看如何使用这些模型并比较它们的结果。

### 稳定扩散 v1.5

稳定扩散 v1.5 是一个从早期检查点初始化的潜在扩散模型，并在 512x512 图像上进一步微调了 595K 步。要将此管道用于图像到图像，您需要准备一个初始图像传递给管道。然后，您可以传递一个提示和图像给管道以生成一个新图像：

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/36dc16e92be7b4f357b7201ad98da118.png)

初始图像

![](../Images/0145794767dd4ef55a2071169ec77aa2.png)

生成的图像

### 稳定扩散 XL (SDXL)

SDXL 是稳定扩散模型的更强大版本。它使用一个更大的基础模型，以及一个额外的细化模型来提高基础模型输出的质量。阅读 [SDXL](sdxl) 指南，了解如何使用此模型以及它用于生成高质量图像的其他技术。

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, strength=0.5).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/2fe2edb85787ba08991e059ff912c98c.png)

初始图像

![](../Images/7ec0479f09a14b51ed59ccd01d6f93c9.png)

生成的图像

### Kandinsky 2.2

Kandinsky 模型与稳定扩散模型不同，因为它使用图像先验模型来创建图像嵌入。这些嵌入有助于在文本和图像之间创建更好的对齐，使潜在扩散模型能够生成更好的图像。

使用 Kandinsky 2.2 的最简单方法是：

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/36dc16e92be7b4f357b7201ad98da118.png)

初始图像

![](../Images/97dcd8328035f392ab7d774997117ed0.png)

生成的图像

## 配置管道参数

您可以配置管道中的几个重要参数，这些参数将影响图像生成过程和图像质量。让我们更仔细地看看这些参数的作用以及如何更改它们会影响输出。

### 强度

`strength`是要考虑的最重要参数之一，它将对生成的图像产生巨大影响。它决定了生成的图像与初始图像的相似程度。换句话说：

+   📈较高的`strength`值使模型更具“创造力”，可以生成与初始图像不同的图像；`strength`值为1.0意味着初始图像或多或少被忽略

+   📉较低的`strength`值意味着生成的图像更类似于初始图像

`strength`和`num_inference_steps`参数相关，因为`strength`确定要添加的噪声步数。例如，如果`num_inference_steps`为50，`strength`为0.8，则这意味着向初始图像添加40（50 * 0.8）步噪声，然后去噪40步以获得新生成的图像。

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, strength=0.8).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/b18d4d9ef1fda116b32a37eb344dd61d.png)

strength = 0.4

![](../Images/caabae187343ac728e132d95979f20d9.png)

strength = 0.6

![](../Images/6dd3ccf15afa5473f942682270943955.png)

strength = 1.0

### 指导尺度

`guidance_scale`参数用于控制生成的图像和文本提示之间的对齐程度。较高的`guidance_scale`值意味着您生成的图像与提示更加对齐，而较低的`guidance_scale`值意味着您生成的图像有更多的空间偏离提示。

您可以将`guidance_scale`与`strength`结合使用，以更精确地控制模型的表现力。例如，结合高`strength + guidance_scale`以获得最大创造力，或者使用低`strength`和低`guidance_scale`的组合生成类似于初始图像但不严格受限于提示的图像。

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, guidance_scale=8.0).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/8f312f9d19115eba39cce061ca99ee11.png)

guidance_scale = 0.1

![](../Images/ee6aee8a1bb3c419a6a94dd5b4a870b8.png)

guidance_scale = 5.0

![](../Images/94e53acaee818fe2a3dddf5671819e4e.png)

guidance_scale = 10.0

### 负提示

负提示条件模型*不*在图像中包含某些内容，可用于改善图像质量或修改图像。例如，您可以通过包含“细节差”或“模糊”等负提示来改善图像质量，以鼓励模型生成更高质量的图像。或者您可以通过指定要从图像中排除的内容来修改图像。

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=negative_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/b0371dfac3fc42c122a50da2e48f809a.png)

negative_prompt = "丑陋，畸形，毁容，细节差，解剖不良"

![](../Images/420cf48fd86187c0fb09679e8d6a92bd.png)

negative_prompt = "丛林"

## 链式图像到图像管道

除了生成图像之外，您还可以以其他有趣的方式使用图像到图像管道（尽管生成图像也很酷）。您可以进一步将其与其他管道链接起来。

### 文本到图像到图像

将文本到图像和图像到图像管道链接在一起，可以从文本生成图像，并将生成的图像用作图像到图像管道的初始图像。如果您想完全从头开始生成图像，这将非常有用。例如，让我们链接一个稳定扩散和一个康定斯基模型。

首先使用文本到图像管道生成一个图像：

```py
from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
import torch
from diffusers.utils import make_image_grid

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

text2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k").images[0]
text2image
```

现在您可以将生成的图像传递给图像到图像管道：

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k", image=text2image).images[0]
make_image_grid([text2image, image2image], rows=1, cols=2)
```

### 图像到图像到图像

您还可以将多个图像到图像管道链接在一起，以创建更有趣的图像。这对于在图像上迭代执行风格转移、生成短GIF、恢复图像的颜色或恢复图像中缺失的区域非常有用。

首先生成一个图像：

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, output_type="latent").images[0]
```

在管道中指定`output_type="latent"`是很重要的，以保持所有输出在潜在空间中，避免不必要的解码-编码步骤。只有当链接的管道使用相同的VAE时才有效。

将此管道的潜在输出传递给下一个管道，以生成[漫画风格的图像](https://huggingface.co/ogkalu/Comic-Diffusion)：

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "ogkalu/Comic-Diffusion", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# need to include the token "charliebo artstyle" in the prompt to use this checkpoint
image = pipeline("Astronaut in a jungle, charliebo artstyle", image=image, output_type="latent").images[0]
```

再重复一次，以生成最终图像，采用[像素艺术风格](https://huggingface.co/kohbanye/pixel-art-style)：

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kohbanye/pixel-art-style", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# need to include the token "pixelartstyle" in the prompt to use this checkpoint
image = pipeline("Astronaut in a jungle, pixelartstyle", image=image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

### 图像到上采样器到超分辨率

您可以将图像到图像管道与上采样器和超分辨率管道链接，以真正增加图像中的细节级别。

从图像到图像管道开始：

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image_1 = pipeline(prompt, image=init_image, output_type="latent").images[0]
```

在管道中指定`output_type="latent"`是很重要的，以保持所有输出在*潜在*空间中，避免不必要的解码-编码步骤。只有当链接的管道使用相同的VAE时才有效。

将其链接到一个上采样器管道，以增加图像分辨率：

```py
from diffusers import StableDiffusionLatentUpscalePipeline

upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(
    "stabilityai/sd-x2-latent-upscaler", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
upscaler.enable_model_cpu_offload()
upscaler.enable_xformers_memory_efficient_attention()

image_2 = upscaler(prompt, image=image_1, output_type="latent").images[0]
```

最后，将其链接到一个超分辨率管道，进一步增强分辨率：

```py
from diffusers import StableDiffusionUpscalePipeline

super_res = StableDiffusionUpscalePipeline.from_pretrained(
    "stabilityai/stable-diffusion-x4-upscaler", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
super_res.enable_model_cpu_offload()
super_res.enable_xformers_memory_efficient_attention()

image_3 = super_res(prompt, image=image_2).images[0]
make_image_grid([init_image, image_3.resize((512, 512))], rows=1, cols=2)
```

## 控制图像生成

尝试生成完全符合您要求的图像可能很困难，这就是受控生成技术和模型如此有用的原因。虽然您可以使用`negative_prompt`部分控制图像生成，但还有更健壮的方法，如提示加权和ControlNets。

### 提示加权

提示加权允许您调整提示中每个概念的表示。例如，在一个提示中，“太空人在丛林中，冷色调，柔和的颜色，详细，8k”，您可以选择增加或减少“太空人”和“丛林”的嵌入。[Compel](https://github.com/damian0815/compel)库提供了一个简单的语法，用于调整提示权重和生成嵌入。您可以学习如何在[提示加权](weighted_prompts)指南中创建嵌入。

[AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)有一个`prompt_embeds`（如果您使用负向提示，则还有`negative_prompt_embeds`参数），您可以传递嵌入，替换`prompt`参数。

```py
from diffusers import AutoPipelineForImage2Image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt_embeds=prompt_embeds, # generated from Compel
    negative_prompt_embeds=negative_prompt_embeds, # generated from Compel
    image=init_image,
).images[0]
```

### ControlNet

ControlNets提供了一种更灵活和准确的控制图像生成的方式，因为您可以使用额外的条件图像。条件图像可以是canny图像、深度图、图像分割，甚至是涂鸦！无论您选择哪种类型的条件图像，ControlNet都会生成保留其中信息的图像。

例如，让我们使用深度图对图像进行条件处理，以保留图像中的空间信息。

```py
from diffusers.utils import load_image, make_image_grid

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)
init_image = init_image.resize((958, 960)) # resize to depth image dimensions
depth_image = load_image("https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/images/control.png")
make_image_grid([init_image, depth_image], rows=1, cols=2)
```

加载一个在深度图上进行条件处理的ControlNet模型和[AutoPipelineForImage2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForImage2Image)：

```py
from diffusers import ControlNetModel, AutoPipelineForImage2Image
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth", torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()
```

现在生成一个新的图像，以深度图、初始图像和提示为条件：

```py
prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
image_control_net = pipeline(prompt, image=init_image, control_image=depth_image).images[0]
make_image_grid([init_image, depth_image, image_control_net], rows=1, cols=3)
```

![](../Images/36dc16e92be7b4f357b7201ad98da118.png)

初始图像

![](../Images/795c4016502683bcda2f00fa33a84c2d.png)

深度图像

![](../Images/e425aa0e6403b0304599567d11b91a4d.png)

ControlNet图像

让我们对从ControlNet生成的图像应用一个新的[风格](https://huggingface.co/nitrosocke/elden-ring-diffusion)，通过将其与图像到图像管道链接：

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "nitrosocke/elden-ring-diffusion", torch_dtype=torch.float16,
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

prompt = "elden ring style astronaut in a jungle" # include the token "elden ring style" in the prompt
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

image_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image_control_net, strength=0.45, guidance_scale=10.5).images[0]
make_image_grid([init_image, depth_image, image_control_net, image_elden_ring], rows=2, cols=2)
```

![](../Images/3d19fb4449a5e38b2448796e3bf6cf11.png)

## 优化

运行扩散模型在计算上是昂贵且密集的，但通过一些优化技巧，完全可以在消费者和免费级别的GPU上运行它们。例如，您可以使用更节省内存的注意力形式，如PyTorch 2.0的[缩放点积注意力](../optimization/torch2.0#scaled-dot-product-attention)或[xFormers](../optimization/xformers)（您可以使用其中一个，但没有必要同时使用两者）。您还可以将模型卸载到GPU，而其他管道组件在CPU上等待。

```py
+ pipeline.enable_model_cpu_offload()
+ pipeline.enable_xformers_memory_efficient_attention()
```

使用[`torch.compile`](../optimization/torch2.0#torchcompile)，您可以通过将UNet与其包装来进一步提高推理速度：

```py
pipeline.unet = torch.compile(pipeline.unet, mode="reduce-overhead", fullgraph=True)
```

要了解更多信息，请查看[减少内存使用](../optimization/memory)和[Torch 2.0](../optimization/torch2.0)指南。
