- en: Hands-on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/hands-on](https://huggingface.co/learn/deep-rl-course/unit2/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we studied the Q-Learning algorithm, let‚Äôs implement it from scratch
    and train our Q-Learning agent in two environments:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Frozen-Lake-v1 (non-slippery and slippery version)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
    ‚òÉÔ∏è : where our agent will need to¬†**go from the starting state (S) to the goal
    state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[An autonomous taxi](https://gymnasium.farama.org/environments/toy_text/taxi/)
    üöñ will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from
    point A to point B.**'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Environments](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Thanks to a [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    you‚Äôll be able to compare your results with other classmates and exchange the
    best practices to improve your agent‚Äôs scores. Who will win the challenge for
    Unit 2?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained Taxi model to the Hub and **get a result of >= 4.5**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here üëâ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on the Open In Colab button** üëá :'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 2: Q-Learning with FrozenLake-v1 ‚õÑ and Taxi-v3 üöï'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 2 Thumbnail](../Images/0b9bbdbce6b297349ae6de9075b71340.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: In this notebook, **you‚Äôll code your first Reinforcement Learning agent from
    scratch** to play FrozenLake ‚ùÑÔ∏è using Q-Learning, share it with the community,
    and experiment with different configurations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: ‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.**
    ‚¨áÔ∏è
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'üéÆ Environments:'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'üìö RL-Library:'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python and NumPy
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gymnasium](https://gymnasium.farama.org/)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We‚Äôre constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook üèÜ
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to code a Q-Learning agent from scratch.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score üî•.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This notebook is from the Deep Reinforcement Learning Course
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'In this free course, you will:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: üìñ Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü§ñ Train **agents in unique environments**
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more check üìö the syllabus üëâ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able to¬†**send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep in touch is to join our discord server to exchange with
    the community and with us üëâüèª [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites üèóÔ∏è
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: üî≤ üìö **Study [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)**
    ü§ó
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A small recap of Q-Learning
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Q-Learning* **is the RL algorithm that**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Trains *Q-Function*, an **action-value function** that is encoded, in internal
    memory, by a *Q-table* **that contains all the state-action pair values.**
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a state and action, our Q-Function **will search the Q-table for the corresponding
    value.**
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Q function](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if we **have an optimal Q-function**, we have an optimal policy, since we
    **know for each state, the best action to take.**
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: But, in the beginning,¬†our **Q-Table is useless since it gives arbitrary value
    for each state-action pair¬†(most of the time we initialize the Q-Table to 0 values)**.
    But, as we‚Äôll¬†explore the environment and update our Q-Table it will give us better
    and better approximations
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'This is the Q-Learning pseudocode:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs code our first Reinforcement Learning algorithm üöÄ
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained Taxi model to the Hub and **get a result of >= 4.5**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section üëâ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Install dependencies and create a virtual display üîΩ
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the notebook, we‚Äôll need to generate a replay video. To do so, with Colab,
    **we need to have a virtual screen to render the environment** (and thus record
    the frames).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install the libraries and create and run a virtual
    screen üñ•
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll install multiple ones:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium`: Contains the FrozenLake-v1 ‚õÑ and Taxi-v3 üöï environments.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pygame`: Used for the FrozenLake-v1 and Taxi-v3 UI.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`: Used for handling our Q-table.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hugging Face Hub ü§ó works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: You can see here all the Deep RL models available (if they use Q Learning) here
    üëâ [https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To make sure the new installed libraries are used, **sometimes it‚Äôs required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so you‚Äôll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ‰∏∫‰∫ÜÁ°Æ‰øùÊñ∞ÂÆâË£ÖÁöÑÂ∫ìË¢´‰ΩøÁî®Ôºå**ÊúâÊó∂ÈúÄË¶ÅÈáçÊñ∞ÂêØÂä®Á¨îËÆ∞Êú¨ËøêË°åÊó∂**„ÄÇ‰∏ã‰∏Ä‰∏™ÂçïÂÖÉÊ†ºÂ∞ÜÂº∫Âà∂**ËøêË°åÊó∂Â¥©Ê∫ÉÔºåÂõ†Ê≠§ÊÇ®ÈúÄË¶ÅÈáçÊñ∞ËøûÊé•Âπ∂‰ªéËøôÈáåÂºÄÂßãËøêË°å‰ª£Á†Å**„ÄÇÈÄöËøáËøô‰∏™ÊäÄÂ∑ßÔºå**Êàë‰ª¨Â∞ÜËÉΩÂ§üËøêË°åÊàë‰ª¨ÁöÑËôöÊãüÂ±èÂπï**„ÄÇ
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Import the packages üì¶
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂØºÂÖ•ÂåÖüì¶
- en: 'In addition to the installed libraries, we also use:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Èô§‰∫ÜÂÆâË£ÖÁöÑÂ∫ìÔºåÊàë‰ª¨Ëøò‰ΩøÁî®Ôºö
- en: '`random`: To generate random numbers (that will be useful for epsilon-greedy
    policy).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random`ÔºöÁîüÊàêÈöèÊú∫Êï∞ÔºàÂØπepsilon-Ë¥™Â©™Á≠ñÁï•ÊúâÁî®Ôºâ„ÄÇ'
- en: '`imageio`: To generate a replay video.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imageio`ÔºöÁîüÊàêÈáçÊí≠ËßÜÈ¢ë„ÄÇ'
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We‚Äôre now ready to code our Q-Learning algorithm üî•
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Êàë‰ª¨Áé∞Âú®ÂáÜÂ§áÁºñÂÜôÊàë‰ª¨ÁöÑQÂ≠¶‰π†ÁÆóÊ≥ïüî•
- en: 'Part 1: Frozen Lake ‚õÑ (non slippery version)'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Á¨¨1ÈÉ®ÂàÜÔºöÂÜ∞ÂÜªÊπñ‚õÑÔºàÈùûÊªëÂä®ÁâàÊú¨Ôºâ
- en: Create and understand FrozenLake environment ‚õÑ (( https://gymnasium.farama.org/environments/toy_text/frozen_lake/
    )
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂàõÂª∫ÂíåÁêÜËß£ÂÜ∞ÂÜªÊπñÁéØÂ¢É‚õÑÔºàhttps://gymnasium.farama.org/environments/toy_text/frozen_lake/Ôºâ
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: üí° A good habit when you start to use an environment is to check its documentation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: üí° ÂΩìÊÇ®ÂºÄÂßã‰ΩøÁî®ÁéØÂ¢ÉÊó∂ÔºåÂÖªÊàê‰∏Ä‰∏™Â•Ω‰π†ÊÉØÊòØÊ£ÄÊü•ÂÖ∂ÊñáÊ°£
- en: üëâ [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: üëâ [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- en: '* * *'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: We‚Äôre going to train our Q-Learning agent **to navigate from the starting state
    (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes
    (H)**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Êàë‰ª¨Â∞ÜËÆ≠ÁªÉÊàë‰ª¨ÁöÑQÂ≠¶‰π†‰ª£ÁêÜ**‰ªéËµ∑ÂßãÁä∂ÊÄÅÔºàSÔºâÂØºËà™Âà∞ÁõÆÊ†áÁä∂ÊÄÅÔºàGÔºâÔºåÂè™ËÉΩÂú®ÂÜ∞ÂÜªÁì∑Á†ñÔºàFÔºâ‰∏äË°åËµ∞ÔºåÈÅøÂÖçÊéâÂÖ•Ê¥û‰∏≠ÔºàHÔºâ**„ÄÇ
- en: 'We can have two sizes of environment:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Êàë‰ª¨ÂèØ‰ª•Êúâ‰∏§ÁßçÁéØÂ¢ÉÂ§ßÂ∞èÔºö
- en: '`map_name="4x4"`: a 4x4 grid version'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_name="4x4"`Ôºö‰∏Ä‰∏™4x4ÁΩëÊ†ºÁâàÊú¨'
- en: '`map_name="8x8"`: a 8x8 grid version'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_name="8x8"`Ôºö‰∏Ä‰∏™8x8ÁΩëÊ†ºÁâàÊú¨'
- en: 'The environment has two modes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ÁéØÂ¢ÉÊúâ‰∏§ÁßçÊ®°ÂºèÔºö
- en: '`is_slippery=False`: The agent always moves **in the intended direction** due
    to the non-slippery nature of the frozen lake (deterministic).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_slippery=False`ÔºöÁî±‰∫éÂÜ∞ÂÜªÊπñÁöÑÈùûÊªëÂä®ÊÄßË¥®Ôºå‰ª£ÁêÜÂßãÁªà**ÊúùÁùÄÈ¢ÑÊúüÊñπÂêëÁßªÂä®**„ÄÇ'
- en: '`is_slippery=True`: The agent **may not always move in the intended direction**
    due to the slippery nature of the frozen lake (stochastic).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_slippery=True`ÔºöÁî±‰∫éÂÜ∞ÂÜªÊπñÁöÑÊªëÂä®ÊÄßË¥®Ôºå‰ª£ÁêÜ**ÂèØËÉΩ‰∏çÊÄªÊòØÊúùÁùÄÈ¢ÑÊúüÊñπÂêëÁßªÂä®**ÔºàÈöèÊú∫ÁöÑÔºâ„ÄÇ'
- en: For now let‚Äôs keep it simple with the 4x4 map and non-slippery. We add a parameter
    called `render_mode` that specifies how the environment should be visualised.
    In our case because we **want to record a video of the environment at the end,
    we need to set render_mode to rgb_array**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Áé∞Âú®ËÆ©Êàë‰ª¨‰øùÊåÅÁÆÄÂçïÔºå‰ΩøÁî®4x4Âú∞ÂõæÂíåÈùûÊªëÂä®„ÄÇÊàë‰ª¨Ê∑ªÂä†‰∏Ä‰∏™Âêç‰∏∫`render_mode`ÁöÑÂèÇÊï∞ÔºåÊåáÂÆöÁéØÂ¢ÉÂ∫îÂ¶Ç‰ΩïÂèØËßÜÂåñ„ÄÇÂú®Êàë‰ª¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂõ†‰∏∫Êàë‰ª¨**Â∏åÊúõÂú®ÊúÄÂêéËÆ∞ÂΩïÁéØÂ¢ÉÁöÑËßÜÈ¢ëÔºåÊàë‰ª¨ÈúÄË¶ÅÂ∞Ürender_modeËÆæÁΩÆ‰∏∫rgb_array**„ÄÇ
- en: 'As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)
    ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment.
    A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y
    pixel image.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Â¶Ç[ÊñáÊ°£‰∏≠ÊâÄËø∞](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)Ôºå‚Äúrgb_array‚ÄùÔºöËøîÂõû‰ª£Ë°®ÁéØÂ¢ÉÂΩìÂâçÁä∂ÊÄÅÁöÑÂçï‰∏™Â∏ß„ÄÇ‰∏Ä‰∏™Â∏ßÊòØ‰∏Ä‰∏™ÂΩ¢Áä∂‰∏∫ÔºàxÔºåyÔºå3ÔºâÁöÑnp.ndarrayÔºåË°®Á§∫x‰πòyÂÉèÁ¥†ÂõæÂÉèÁöÑRGBÂÄº„ÄÇ
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Solution
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ëß£ÂÜ≥ÊñπÊ°à
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can create your own custom grid like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•ÂàõÂª∫Ëá™Â∑±ÁöÑËá™ÂÆö‰πâÁΩëÊ†ºÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: but we‚Äôll use the default environment for now.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ΩÜÊàë‰ª¨Áé∞Âú®Â∞Ü‰ΩøÁî®ÈªòËÆ§ÁéØÂ¢É„ÄÇ
- en: 'Let‚Äôs see what the Environment looks like:'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ËÆ©Êàë‰ª¨ÁúãÁúãÁéØÂ¢ÉÊòØ‰ªÄ‰πàÊ†∑Â≠êÁöÑÔºö
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We see with `Observation Space Shape Discrete(16)` that the observation is an
    integer representing the **agent‚Äôs current position as current_row * ncols + current_col
    (where both the row and col start at 0)**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Êàë‰ª¨ÁúãÂà∞`Observation Space Shape Discrete(16)`ÔºåËßÇÂØüÊòØ‰∏Ä‰∏™Êï¥Êï∞ÔºåË°®Á§∫**‰ª£ÁêÜÁöÑÂΩìÂâç‰ΩçÁΩÆ‰∏∫current_row *
    ncols + current_colÔºàÂÖ∂‰∏≠Ë°åÂíåÂàóÈÉΩ‰ªé0ÂºÄÂßãÔºâ**„ÄÇ
- en: 'For example, the goal position in the 4x4 map can be calculated as follows:
    3 * 4 + 3 = 15\. The number of possible observations is dependent on the size
    of the map. **For example, the 4x4 map has 16 possible observations.**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ‰æãÂ¶ÇÔºå4x4Âú∞Âõæ‰∏≠ÁöÑÁõÆÊ†á‰ΩçÁΩÆÂèØ‰ª•ËÆ°ÁÆóÂ¶Ç‰∏ãÔºö3 * 4 + 3 = 15„ÄÇÂèØËÉΩÁöÑËßÇÂØüÊ¨°Êï∞ÂèñÂÜ≥‰∫éÂú∞ÂõæÁöÑÂ§ßÂ∞è„ÄÇ**‰æãÂ¶ÇÔºå4x4Âú∞ÂõæÊúâ16ÁßçÂèØËÉΩÁöÑËßÇÂØü„ÄÇ**
- en: 'For instance, this is what state = 0 looks like:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ‰æãÂ¶ÇÔºåËøôÂ∞±ÊòØÁä∂ÊÄÅ=0ÁöÑÊ†∑Â≠êÔºö
- en: '![FrozenLake](../Images/5d72cc95a82a64293bb4212444acec50.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![FrozenLake](../Images/5d72cc95a82a64293bb4212444acec50.png)'
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available üéÆ:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Âä®‰ΩúÁ©∫Èó¥Ôºà‰ª£ÁêÜÂèØ‰ª•ÈááÂèñÁöÑÂèØËÉΩÂä®‰ΩúÈõÜÔºâÊòØÁ¶ªÊï£ÁöÑÔºåÊúâ4‰∏™ÂèØÁî®Âä®‰ΩúüéÆÔºö
- en: '0: GO LEFT'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0ÔºöÂêëÂ∑¶ÁßªÂä®
- en: '1: GO DOWN'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ÔºöÂêë‰∏ãÁßªÂä®
- en: '2: GO RIGHT'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ÔºöÂêëÂè≥ÁßªÂä®
- en: '3: GO UP'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ÔºöÂêë‰∏äÁßªÂä®
- en: 'Reward function üí∞:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Â•ñÂä±ÂáΩÊï∞üí∞Ôºö
- en: 'Reach goal: +1'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ËææÂà∞ÁõÆÊ†áÔºö+1
- en: 'Reach hole: 0'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Âà∞ËææÊ¥ûÔºö0
- en: 'Reach frozen: 0'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Âà∞ËææÂÜ∞ÂÜªÔºö0
- en: Create and Initialize the Q-table üóÑÔ∏è
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂàõÂª∫ÂíåÂàùÂßãÂåñQË°®üóÑÔ∏è
- en: (üëÄ Step 1 of the pseudocode)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ÔºàüëÄ ‰º™‰ª£Á†ÅÁöÑÁ¨¨1Ê≠•Ôºâ
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
- en: 'It‚Äôs time to initialize our Q-table! To know how many rows (states) and columns
    (actions) to use, we need to know the action and observation space. We already
    know their values from before, but we‚Äôll want to obtain them programmatically
    so that our algorithm generalizes for different environments. Gym provides us
    a way to do that: `env.action_space.n` and `env.observation_space.n`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Áé∞Âú®ÊòØÂàùÂßãÂåñÊàë‰ª¨ÁöÑQË°®ÁöÑÊó∂ÂÄô‰∫ÜÔºÅË¶ÅÁü•ÈÅìË¶Å‰ΩøÁî®Â§öÂ∞ëË°åÔºàÁä∂ÊÄÅÔºâÂíåÂàóÔºàÂä®‰ΩúÔºâÔºåÊàë‰ª¨ÈúÄË¶ÅÁü•ÈÅìÂä®‰ΩúÂíåËßÇÂØüÁ©∫Èó¥„ÄÇÊàë‰ª¨Â∑≤ÁªèÁü•ÈÅìÂÆÉ‰ª¨ÁöÑÂÄº‰∫ÜÔºå‰ΩÜÊàë‰ª¨Â∏åÊúõ‰ª•ÁºñÁ®ãÊñπÂºèËé∑ÂèñÂÆÉ‰ª¨Ôºå‰ª•‰æøÊàë‰ª¨ÁöÑÁÆóÊ≥ïÂèØ‰ª•Êé®ÂπøÂà∞‰∏çÂêåÁöÑÁéØÂ¢É„ÄÇGym‰∏∫Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºö`env.action_space.n`Âíå`env.observation_space.n`
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Solution
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ëß£ÂÜ≥ÊñπÊ°à
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Define the greedy policy ü§ñ
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂÆö‰πâË¥™Â©™Á≠ñÁï•ü§ñ
- en: Remember we have two policies since Q-Learning is an **off-policy** algorithm.
    This means we‚Äôre using a **different policy for acting and updating the value
    function**.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ËØ∑ËÆ∞‰ΩèÔºåÁî±‰∫éQÂ≠¶‰π†ÊòØ‰∏ÄÁßç**Á¶ªÁ≠ñÁï•**ÁÆóÊ≥ïÔºåÊàë‰ª¨Êúâ‰∏§ÁßçÁ≠ñÁï•„ÄÇËøôÊÑèÂë≥ÁùÄÊàë‰ª¨Âú®**Ë°åÂä®ÂíåÊõ¥Êñ∞ÂÄºÂáΩÊï∞Êó∂‰ΩøÁî®‰∏çÂêåÁöÑÁ≠ñÁï•**„ÄÇ
- en: Epsilon-greedy policy (acting policy)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon-Ë¥™Â©™Á≠ñÁï•ÔºàË°åÂä®Á≠ñÁï•Ôºâ
- en: Greedy-policy (updating policy)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ë¥™Â©™Á≠ñÁï•ÔºàÊõ¥Êñ∞Á≠ñÁï•Ôºâ
- en: The greedy policy will also be the final policy we‚Äôll have when the Q-learning
    agent completes training. The greedy policy is used to select an action using
    the Q-table.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Ë¥™Â©™Á≠ñÁï•‰πüÂ∞ÜÊòØQÂ≠¶‰π†‰ª£ÁêÜÂÆåÊàêËÆ≠ÁªÉÂêéÁöÑÊúÄÁªàÁ≠ñÁï•„ÄÇË¥™Â©™Á≠ñÁï•Áî®‰∫é‰ΩøÁî®QË°®ÈÄâÊã©Âä®‰Ωú„ÄÇ
- en: '![Q-Learning](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Solution
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Ëß£ÂÜ≥ÊñπÊ°à
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Define the epsilon-greedy policy ü§ñ
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂÆö‰πâepsilon-greedyÁ≠ñÁï• ü§ñ
- en: Epsilon-greedy is the training policy that handles the exploration/exploitation
    trade-off.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon-greedyÊòØÂ§ÑÁêÜÊé¢Á¥¢/Âà©Áî®ÊùÉË°°ÁöÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇ
- en: 'The idea with epsilon-greedy:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ΩøÁî®epsilon-greedyÁöÑÊÉ≥Ê≥ïÔºö
- en: 'With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the
    action with the highest state-action pair value).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰ª•*Ê¶ÇÁéá1-…õ*Ôºö**Êàë‰ª¨ËøõË°åÂà©Áî®**ÔºàÂç≥Êàë‰ª¨ÁöÑ‰ª£ÁêÜÈÄâÊã©ÂÖ∑ÊúâÊúÄÈ´òÁä∂ÊÄÅ-Âä®‰ΩúÂØπÂÄºÁöÑÂä®‰ΩúÔºâ„ÄÇ
- en: 'With *probability …õ*: we do **exploration** (trying a random action).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‰ª•*Ê¶ÇÁéá…õ*ÔºöÊàë‰ª¨ËøõË°å**Êé¢Á¥¢**ÔºàÂ∞ùËØïÈöèÊú∫Âä®‰ΩúÔºâ„ÄÇ
- en: As the training continues, we progressively **reduce the epsilon value since
    we will need less and less exploration and more exploitation.**
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ÈöèÁùÄËÆ≠ÁªÉÁöÑËøõË°åÔºåÊàë‰ª¨ÈÄêÊ∏ê**ÂáèÂ∞ëepsilonÂÄºÔºåÂõ†‰∏∫Êàë‰ª¨Â∞ÜÈúÄË¶ÅË∂äÊù•Ë∂äÂ∞ëÁöÑÊé¢Á¥¢ÂíåÊõ¥Â§öÁöÑÂà©Áî®**„ÄÇ
- en: '![Q-Learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Ëß£ÂÜ≥ÊñπÊ°à
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Define the hyperparameters ‚öôÔ∏è
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂÆö‰πâË∂ÖÂèÇÊï∞ ‚öôÔ∏è
- en: The exploration related hyperparamters are some of the most important ones.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ‰∏éÊé¢Á¥¢Áõ∏ÂÖ≥ÁöÑË∂ÖÂèÇÊï∞ÊòØÊúÄÈáçË¶ÅÁöÑ‰πã‰∏Ä„ÄÇ
- en: We need to make sure that our agent **explores enough of the state space** to
    learn a good value approximation. To do that, we need to have progressive decay
    of the epsilon.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êàë‰ª¨ÈúÄË¶ÅÁ°Æ‰øùÊàë‰ª¨ÁöÑ‰ª£ÁêÜ**Êé¢Á¥¢Ë∂≥Â§üÁöÑÁä∂ÊÄÅÁ©∫Èó¥**‰ª•Â≠¶‰π†‰∏Ä‰∏™ËâØÂ•ΩÁöÑÂÄºËøë‰ºº„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÈúÄË¶ÅÈÄêÊ∏êÂáèÂ∞ëepsilonÁöÑË°∞Âáè„ÄÇ
- en: If you decrease epsilon too fast (too high decay_rate), **you take the risk
    that your agent will be stuck**, since your agent didn‚Äôt explore enough of the
    state space and hence can‚Äôt solve the problem.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â¶ÇÊûúÊÇ®Â∞ÜepsilonÂáèÂ∞ëÂæóÂ§™Âø´ÔºàË°∞ÂáèÁéáÂ§™È´òÔºâÔºå**ÊÇ®‰ºöÂÜíÁùÄ‰ª£ÁêÜË¢´Âç°‰ΩèÁöÑÈ£éÈô©**ÔºåÂõ†‰∏∫ÊÇ®ÁöÑ‰ª£ÁêÜÊ≤°ÊúâÊé¢Á¥¢Ë∂≥Â§üÁöÑÁä∂ÊÄÅÁ©∫Èó¥ÔºåÂõ†Ê≠§Êó†Ê≥ïËß£ÂÜ≥ÈóÆÈ¢ò„ÄÇ
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Create the training loop method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂàõÂª∫ËÆ≠ÁªÉÂæ™ÁéØÊñπÊ≥ï
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
- en: 'The training loop goes like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ËÆ≠ÁªÉÂæ™ÁéØÂ¶Ç‰∏ãÔºö
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Solution
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Ëß£ÂÜ≥ÊñπÊ°à
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Train the Q-Learning agent üèÉ
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ËÆ≠ÁªÉQ-Learning‰ª£ÁêÜ üèÉ
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let‚Äôs see what our Q-Learning table looks like now üëÄ
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ËÆ©Êàë‰ª¨ÁúãÁúãÊàë‰ª¨ÁöÑQ-LearningË°®Áé∞Â¶Ç‰Ωï üëÄ
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The evaluation method üìù
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ËØÑ‰º∞ÊñπÊ≥ï üìù
- en: We defined the evaluation method that we‚Äôre going to use to test our Q-Learning
    agent.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êàë‰ª¨ÂÆö‰πâ‰∫ÜË¶Å‰ΩøÁî®ÁöÑËØÑ‰º∞ÊñπÊ≥ïÊù•ÊµãËØïÊàë‰ª¨ÁöÑQ-Learning‰ª£ÁêÜ„ÄÇ
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Evaluate our Q-Learning agent üìà
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ËØÑ‰º∞Êàë‰ª¨ÁöÑQ-Learning‰ª£ÁêÜ üìà
- en: Usually, you should have a mean reward of 1.0
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÄöÂ∏∏ÔºåÊÇ®Â∫îËØ•Êúâ‰∏Ä‰∏™Âπ≥ÂùáÂ•ñÂä±‰∏∫1.0
- en: The **environment is relatively easy** since the state space is really small
    (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/),
    which introduces stochasticity, making the environment more complex.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Áî±‰∫éÁä∂ÊÄÅÁ©∫Èó¥ÈùûÂ∏∏Â∞èÔºà16ÔºâÔºå**ÁéØÂ¢ÉÁõ∏ÂØπÂÆπÊòì**„ÄÇÊÇ®ÂèØ‰ª•Â∞ùËØï[Áî®ÊªëÂä®ÁâàÊú¨ÊõøÊç¢ÂÆÉ](https://www.gymlibrary.dev/environments/toy_text/frozen_lake)ÔºåËøô‰ºöÂºïÂÖ•ÈöèÊú∫ÊÄßÔºå‰ΩøÁéØÂ¢ÉÊõ¥Âä†Â§çÊùÇ„ÄÇ
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Publish our trained model to the Hub üî•
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Â∞ÜÊàë‰ª¨ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂèëÂ∏ÉÂà∞Hub üî•
- en: Now that we saw good results after the training, **we can publish our trained
    model to the Hub ü§ó with one line of code**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Áé∞Âú®Êàë‰ª¨Âú®ËÆ≠ÁªÉÂêéÁúãÂà∞‰∫ÜËâØÂ•ΩÁöÑÁªìÊûúÔºå**Êàë‰ª¨ÂèØ‰ª•Áî®‰∏ÄË°å‰ª£Á†ÅÂ∞ÜËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂèëÂ∏ÉÂà∞Hub ü§ó**„ÄÇ
- en: 'Here‚Äôs an example of a Model Card:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÈáåÊòØ‰∏Ä‰∏™Ê®°ÂûãÂç°ÁöÑÁ§∫‰æãÔºö
- en: '![Model card](../Images/9832a45306ed2f863e30acb21be3060d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Ê®°ÂûãÂç°](../Images/9832a45306ed2f863e30acb21be3060d.png)'
- en: Under the hood, the Hub uses git-based repositories (don‚Äôt worry if you don‚Äôt
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ÂπïÂêéÔºåHub‰ΩøÁî®Âü∫‰∫égitÁöÑÂ≠òÂÇ®Â∫ìÔºàÂ¶ÇÊûúÊÇ®‰∏çÁü•ÈÅìgitÊòØ‰ªÄ‰πàÔºå‰∏çÁî®ÊãÖÂøÉÔºâÔºåËøôÊÑèÂë≥ÁùÄÊÇ®ÂèØ‰ª•ÈöèÁùÄÂÆûÈ™åÂíåÊîπËøõ‰ª£ÁêÜÊõ¥Êñ∞Ê®°ÂûãÁöÑÊñ∞ÁâàÊú¨„ÄÇ
- en: Do not modify this code
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ‰∏çË¶Å‰øÆÊîπËøôÊÆµ‰ª£Á†Å
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: .
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: „ÄÇ
- en: By using `push_to_hub` **you evaluate, record a replay, generate a model card
    of your agent and push it to the Hub**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ÈÄöËøá‰ΩøÁî®`push_to_hub` **ÊÇ®ÂèØ‰ª•ËØÑ‰º∞„ÄÅËÆ∞ÂΩïÈáçÊîæ„ÄÅÁîüÊàêÊÇ®ÁöÑ‰ª£ÁêÜÁöÑÊ®°ÂûãÂç°Âπ∂Â∞ÜÂÖ∂Êé®ÈÄÅÂà∞Hub**„ÄÇ
- en: 'This way:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÊ†∑Ôºö
- en: You can **showcase our work** üî•
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•**Â±ïÁ§∫Êàë‰ª¨ÁöÑÂ∑•‰Ωú** üî•
- en: You can **visualize your agent playing** üëÄ
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•**ÂèØËßÜÂåñÊÇ®ÁöÑ‰ª£ÁêÜËøõË°åÊ∏∏Êàè** üëÄ
- en: You can **share an agent with the community that others can use** üíæ
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•**‰∏éÁ§æÂå∫ÂàÜ‰∫´‰∏Ä‰∏™ÂÖ∂‰ªñ‰∫∫ÂèØ‰ª•‰ΩøÁî®ÁöÑ‰ª£ÁêÜ** üíæ
- en: You can **access a leaderboard üèÜ to see how well your agent is performing compared
    to your classmates** üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊÇ®ÂèØ‰ª•**ËÆøÈóÆÊéíË°åÊ¶ú üèÜ Êü•ÁúãÊÇ®ÁöÑ‰ª£ÁêÜ‰∏éÂêåÂ≠¶Áõ∏ÊØîÁöÑË°®Áé∞Â¶Ç‰Ωï** üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ‰∏∫‰∫ÜËÉΩÂ§ü‰∏éÁ§æÂå∫ÂàÜ‰∫´ÊÇ®ÁöÑÊ®°ÂûãÔºåËøòÊúâ‰∏â‰∏™Ê≠•È™§Ë¶ÅÈÅµÂæ™Ôºö
- en: 1Ô∏è‚É£ (If it‚Äôs not already done) create an account to HF ‚û° [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 1Ô∏è‚É£ÔºàÂ¶ÇÊûúÂ∞öÊú™ÂÆåÊàêÔºâÂàõÂª∫‰∏Ä‰∏™HFË¥¶Êà∑ ‚û° [https://huggingface.co/join](https://huggingface.co/join)
- en: 2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 2Ô∏è‚É£ ÁôªÂΩïÔºåÁÑ∂ÂêéÔºåÊÇ®ÈúÄË¶Å‰ªéHugging FaceÁΩëÁ´ôÂ≠òÂÇ®ÊÇ®ÁöÑË∫´‰ªΩÈ™åËØÅ‰ª§Áâå„ÄÇ
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑ‰ª§ÁâåÔºà[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)Ôºâ**ÂÖ∑ÊúâÂÜôÂÖ•ÊùÉÈôê**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![ÂàõÂª∫HF‰ª§Áâå](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you don‚Äôt want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login` (or `login`)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Â¶ÇÊûúÊÇ®‰∏çÊÉ≥‰ΩøÁî®Google ColabÊàñJupyter NotebookÔºåÊÇ®ÈúÄË¶Å‰ΩøÁî®Ëøô‰∏™ÂëΩ‰ª§Ôºö`huggingface-cli login`ÔºàÊàñ`login`Ôºâ
- en: 3Ô∏è‚É£ We‚Äôre now ready to push our trained agent to the ü§ó Hub üî• using `push_to_hub()`
    function
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 3Ô∏è‚É£ Áé∞Âú®Êàë‰ª¨ÂáÜÂ§á‰ΩøÁî®`push_to_hub()`ÂáΩÊï∞Â∞ÜËÆ≠ÁªÉÂ•ΩÁöÑ‰ª£ÁêÜÊé®ÈÄÅÂà∞ü§ó Hub üî•
- en: Let‚Äôs create **the model dictionary that contains the hyperparameters and the
    Q_table**.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ËÆ©Êàë‰ª¨ÂàõÂª∫**ÂåÖÂê´Ë∂ÖÂèÇÊï∞ÂíåQË°®ÁöÑÊ®°ÂûãÂ≠óÂÖ∏**„ÄÇ
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let‚Äôs fill the `push_to_hub` function:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ËÆ©Êàë‰ª¨Â°´ÂÜô`push_to_hub`ÂáΩÊï∞Ôºö
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})` üí° A good `repo_id` is `{username}/q-{env_id}`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`ÔºöÂ∞ÜË¶ÅÂàõÂª∫/Êõ¥Êñ∞ÁöÑHugging Face HubÂ≠òÂÇ®Â∫ìÁöÑÂêçÁß∞Ôºà`repo_id = {username}/{repo_name}`Ôºâüí°
    ‰∏Ä‰∏™Â•ΩÁöÑ`repo_id`ÊòØ`{username}/q-{env_id}`'
- en: '`model`: our model dictionary containing the hyperparameters and the Qtable.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ÔºöÂåÖÂê´Ë∂ÖÂèÇÊï∞ÂíåQË°®ÁöÑÊ®°ÂûãÂ≠óÂÖ∏„ÄÇ'
- en: '`env`: the environment.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`ÔºöÁéØÂ¢É„ÄÇ'
- en: '`commit_message`: message of the commit'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`ÔºöÊèê‰∫§ÁöÑÊ∂àÊÅØ'
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congrats ü•≥ you‚Äôve just implemented from scratch, trained, and uploaded your
    first Reinforcement Learning agent. FrozenLake-v1 no_slippery is very simple environment,
    let‚Äôs try a harder one üî•.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÅ≠Âñúü•≥Ôºå‰Ω†ÂàöÂàö‰ªéÂ§¥ÂºÄÂßãÂÆûÁé∞„ÄÅËÆ≠ÁªÉÂíå‰∏ä‰º†‰∫Ü‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜ„ÄÇFrozenLake-v1 no_slipperyÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÁÆÄÂçïÁöÑÁéØÂ¢ÉÔºåËÆ©Êàë‰ª¨Â∞ùËØï‰∏Ä‰∏™Êõ¥ÈöæÁöÑüî•„ÄÇ
- en: 'Part 2: Taxi-v3 üöñ'
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Á¨¨2ÈÉ®ÂàÜÔºöTaxi-v3 üöñ
- en: Create and understand Taxi-v3 üöï
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂàõÂª∫Âπ∂ÁêÜËß£Taxi-v3 üöï
- en: '* * *'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: üí° A good habit when you start to use an environment is to check its documentation
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: üí° ÂΩì‰Ω†ÂºÄÂßã‰ΩøÁî®‰∏Ä‰∏™ÁéØÂ¢ÉÊó∂ÔºåÂÖªÊàê‰∏Ä‰∏™Â•Ω‰π†ÊÉØÊòØÊü•ÁúãÂÖ∂ÊñáÊ°£
- en: üëâ [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: üëâ [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)
- en: '* * *'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In `Taxi-v3` üöï, there are four designated locations in the grid world indicated
    by R(ed), G(reen), Y(ellow), and B(lue).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®`Taxi-v3` üöï‰∏≠ÔºåÁΩëÊ†º‰∏ñÁïå‰∏≠ÊúâÂõõ‰∏™ÊåáÂÆö‰ΩçÁΩÆÔºåÂàÜÂà´Áî®RÔºàÁ∫¢Ëâ≤Ôºâ„ÄÅGÔºàÁªøËâ≤Ôºâ„ÄÅYÔºàÈªÑËâ≤ÔºâÂíåBÔºàËìùËâ≤ÔºâË°®Á§∫„ÄÇ
- en: When the episode starts, **the taxi starts off at a random square** and the
    passenger is at a random location. The taxi drives to the passenger‚Äôs location,
    **picks up the passenger**, drives to the passenger‚Äôs destination (another one
    of the four specified locations), and then **drops off the passenger**. Once the
    passenger is dropped off, the episode ends.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ÂΩìÂâßÈõÜÂºÄÂßãÊó∂Ôºå**Âá∫ÁßüËΩ¶‰ªéÈöèÊú∫ÊñπÊ†ºÂºÄÂßã**Ôºå‰πòÂÆ¢Âú®ÈöèÊú∫‰ΩçÁΩÆ„ÄÇÂá∫ÁßüËΩ¶È©∂Âêë‰πòÂÆ¢ÁöÑ‰ΩçÁΩÆÔºå**Êé•ËΩΩ‰πòÂÆ¢**ÔºåÈ©∂Âêë‰πòÂÆ¢ÁöÑÁõÆÁöÑÂú∞ÔºàÂè¶Â§ñÂõõ‰∏™ÊåáÂÆö‰ΩçÁΩÆ‰πã‰∏ÄÔºâÔºåÁÑ∂Âêé**Êîæ‰∏ã‰πòÂÆ¢**„ÄÇ‰∏ÄÊó¶‰πòÂÆ¢‰∏ãËΩ¶ÔºåÂâßÈõÜÁªìÊùü„ÄÇ
- en: '![Taxi](../Images/dcffe584c3d93a6b2668209fe5b78373.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![Âá∫ÁßüËΩ¶](../Images/dcffe584c3d93a6b2668209fe5b78373.png)'
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There are **500 discrete states since there are 25 taxi positions, 5 possible
    locations of the passenger** (including the case when the passenger is in the
    taxi), and **4 destination locations.**
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Êúâ**500‰∏™Á¶ªÊï£Áä∂ÊÄÅÔºåÂõ†‰∏∫Êúâ25‰∏™Âá∫ÁßüËΩ¶‰ΩçÁΩÆÔºå5‰∏™‰πòÂÆ¢ÂèØËÉΩÁöÑ‰ΩçÁΩÆ**ÔºàÂåÖÊã¨‰πòÂÆ¢Âú®Âá∫ÁßüËΩ¶‰∏äÁöÑÊÉÖÂÜµÔºâÔºå‰ª•Âèä**4‰∏™ÁõÆÁöÑÂú∞‰ΩçÁΩÆ**„ÄÇ
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with **6 actions available üéÆ**:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Âä®‰ΩúÁ©∫Èó¥Ôºà‰ª£ÁêÜÂèØ‰ª•ÈááÂèñÁöÑÂèØËÉΩÂä®‰ΩúÈõÜÔºâÊòØÁ¶ªÊï£ÁöÑÔºåÊúâ**6‰∏™ÂèØÁî®Âä®‰ΩúüéÆ**Ôºö
- en: '0: move south'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0ÔºöÂêëÂçóÁßªÂä®
- en: '1: move north'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ÔºöÂêëÂåóÁßªÂä®
- en: '2: move east'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ÔºöÂêë‰∏úÁßªÂä®
- en: '3: move west'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ÔºöÂêëË•øÁßªÂä®
- en: '4: pickup passenger'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4ÔºöÊé•ËΩΩ‰πòÂÆ¢
- en: '5: drop off passenger'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5ÔºöÊîæ‰∏ã‰πòÂÆ¢
- en: 'Reward function üí∞:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Â•ñÂä±ÂáΩÊï∞üí∞Ôºö
- en: -1 per step unless other reward is triggered.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÊØèÊ≠•-1ÔºåÈô§ÈùûËß¶ÂèëÂÖ∂‰ªñÂ•ñÂä±„ÄÇ
- en: +20 delivering passenger.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +20 ÈÄÅËææ‰πòÂÆ¢„ÄÇ
- en: -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -10ÊâßË°å‚ÄúÊé•ËΩΩ‚ÄùÂíå‚ÄúÊîæ‰∏ã‚ÄùÂä®‰ΩúÈùûÊ≥ï„ÄÇ
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Define the hyperparameters ‚öôÔ∏è
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂÆö‰πâË∂ÖÂèÇÊï∞‚öôÔ∏è
- en: '‚ö† DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your
    agent with the same taxi starting positions for every classmate**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ‚ö† ‰∏çË¶Å‰øÆÊîπEVAL_SEEDÔºöeval_seedÊï∞ÁªÑ**ÂÖÅËÆ∏Êàë‰ª¨‰∏∫ÊØè‰∏™ÂêåÂ≠¶ËØÑ‰º∞ÊÇ®ÁöÑ‰ª£ÁêÜÔºå‰ΩøÂá∫ÁßüËΩ¶ÁöÑËµ∑Âßã‰ΩçÁΩÆÁõ∏Âêå**
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Train our Q-Learning agent üèÉ
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ËÆ≠ÁªÉÊàë‰ª¨ÁöÑQÂ≠¶‰π†‰ª£ÁêÜüèÉ
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Create a model dictionary üíæ and publish our trained model to the Hub üî•
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÂàõÂª∫‰∏Ä‰∏™Ê®°ÂûãÂ≠óÂÖ∏üíæÂπ∂Â∞ÜÊàë‰ª¨ËÆ≠ÁªÉÂ•ΩÁöÑÊ®°ÂûãÂèëÂ∏ÉÂà∞Hubüî•
- en: We create a model dictionary that will contain all the training hyperparameters
    for reproducibility and the Q-Table.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êàë‰ª¨ÂàõÂª∫‰∏Ä‰∏™Ê®°ÂûãÂ≠óÂÖ∏ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÊâÄÊúâËÆ≠ÁªÉË∂ÖÂèÇÊï∞‰ª•ÂÆûÁé∞ÂèØÈáçÁé∞ÊÄßÂíåQË°®„ÄÇ
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now that it‚Äôs on the Hub, you can compare the results of your Taxi-v3 with your
    classmates using the leaderboard üèÜ üëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Áé∞Âú®ÂÆÉÂú®Hub‰∏äÔºå‰Ω†ÂèØ‰ª•ÈÄöËøáÊéíË°åÊ¶ú‰∏éÂêåÂ≠¶‰ª¨ÊØîËæÉ‰Ω†ÁöÑTaxi-v3ÁöÑÁªìÊûúüèÜüëâ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: '![Taxi Leaderboard](../Images/0d0ce61b026fca34f96441a33217667d.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![Âá∫ÁßüËΩ¶ÊéíË°åÊ¶ú](../Images/0d0ce61b026fca34f96441a33217667d.png)'
- en: 'Part 3: Load from Hub üîΩ'
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Á¨¨3ÈÉ®ÂàÜÔºö‰ªéHubÂä†ËΩΩüîΩ
- en: What‚Äôs amazing with Hugging Face Hub ü§ó is that you can easily load powerful
    models from the community.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub ü§ó ‰ª§‰∫∫ÊÉäÂèπÁöÑÂú∞ÊñπÂú®‰∫é‰Ω†ÂèØ‰ª•ËΩªÊùæÂú∞Âä†ËΩΩÁ§æÂå∫‰∏≠Âº∫Â§ßÁöÑÊ®°Âûã„ÄÇ
- en: 'Loading a saved model from the Hub is really easy:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ªéHubÂä†ËΩΩÂ∑≤‰øùÂ≠òÁöÑÊ®°ÂûãÈùûÂ∏∏ÂÆπÊòìÔºö
- en: You go [https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)
    to see the list of all the q-learning saved models.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ‰Ω†ÂèØ‰ª•ÂâçÂæÄ[https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)Êü•ÁúãÊâÄÊúâ‰øùÂ≠òÁöÑq-learningÊ®°ÂûãÂàóË°®„ÄÇ
- en: You select one and copy its repo_id
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ‰Ω†ÈÄâÊã©‰∏Ä‰∏™Âπ∂Â§çÂà∂ÂÖ∂repo_id
- en: '![Copy id](../Images/38b2f545a7fd317518e1c20d339f44d7.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![Â§çÂà∂id](../Images/38b2f545a7fd317518e1c20d339f44d7.png)'
- en: 'Then we just need to use `load_from_hub` with:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ÁÑ∂ÂêéÊàë‰ª¨Âè™ÈúÄË¶Å‰ΩøÁî®`load_from_hub`Ôºö
- en: The repo_id
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Êñá‰ª∂ÂêçÔºöÂ≠òÂÇ®Âú®Â≠òÂÇ®Â∫ì‰∏≠ÁöÑÂ∑≤‰øùÂ≠òÊ®°Âûã„ÄÇ
- en: Do not modify this code
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ‰∏çË¶Å‰øÆÊîπËøôÊÆµ‰ª£Á†Å
- en: '[PRE43]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: .
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: „ÄÇ
- en: '[PRE44]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Some additional challenges üèÜ
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÊåëÊàòüèÜ
- en: The best way to learn **is to try things on your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Â≠¶‰π†ÁöÑÊúÄ‰Ω≥ÊñπÂºèÊòØ**Â∞ùËØïËá™Â∑±Âä®Êâã**ÔºÅÊ≠£Â¶Ç‰Ω†ÊâÄÁúãÂà∞ÁöÑÔºåÂΩìÂâçÁöÑ‰ª£ÁêÜË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰Ωú‰∏∫Á¨¨‰∏Ä‰∏™Âª∫ËÆÆÔºå‰Ω†ÂèØ‰ª•ËÆ≠ÁªÉÊõ¥Â§öÊ≠•È™§„ÄÇËøõË°å100‰∏áÊ≠•ËÆ≠ÁªÉÂêéÔºåÊàë‰ª¨ÁúãÂà∞‰∫Ü‰∏Ä‰∫õÂæàÂ•ΩÁöÑÁªìÊûúÔºÅ
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®[ÊéíË°åÊ¶ú](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)‰∏≠Ôºå‰Ω†‰ºöÊâæÂà∞‰Ω†ÁöÑ‰ª£ÁêÜ„ÄÇ‰Ω†ËÉΩÁôªÈ°∂ÂêóÔºü
- en: 'Here are some ideas to climb up the leaderboard:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÊèêÂçáÊéíË°åÊ¶úÁöÑÊÉ≥Ê≥ïÔºö
- en: Train more steps
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ËÆ≠ÁªÉÊõ¥Â§öÊ≠•È™§
- en: Try different hyperparameters by looking at what your classmates have done.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ÈÄöËøáÊü•ÁúãÂêåÂ≠¶‰ª¨ÁöÑÂ∑•‰ΩúÔºåÂ∞ùËØï‰∏çÂêåÁöÑË∂ÖÂèÇÊï∞„ÄÇ
- en: '**Push your new trained model** on the Hub üî•'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Â∞Ü‰Ω†ÁöÑÊñ∞ËÆ≠ÁªÉÊ®°ÂûãÊé®ÈÄÅ**Âà∞Hubüî•'
- en: Are walking on ice and driving taxis too boring to you? Try to **change the
    environment**, why not use FrozenLake-v1 slippery version? Check how they work
    [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun
    üéâ.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®ÂÜ∞‰∏äË°åËµ∞ÂíåÂºÄÂá∫ÁßüËΩ¶ÂØπ‰Ω†Êù•ËØ¥Â§™Êó†ËÅä‰∫ÜÂêóÔºüÂ∞ùËØï**ÊîπÂèòÁéØÂ¢É**Ôºå‰∏∫‰ªÄ‰πà‰∏ç‰ΩøÁî®FrozenLake-v1ÊªëÂä®ÁâàÊú¨ÔºüÊü•ÁúãÂÆÉ‰ª¨ÊòØÂ¶Ç‰ΩïÂ∑•‰ΩúÁöÑ[‰ΩøÁî®gymnasiumÊñáÊ°£](https://gymnasium.farama.org/)Âπ∂Áé©ÂæóÂºÄÂøÉüéâ„ÄÇ
- en: '* * *'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats ü•≥, you‚Äôve just implemented, trained, and uploaded your first Reinforcement
    Learning agent.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ÊÅ≠Âñúü•≥Ôºå‰Ω†ÂàöÂàöÂÆûÁé∞„ÄÅËÆ≠ÁªÉÂíå‰∏ä‰º†‰∫Ü‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Âº∫ÂåñÂ≠¶‰π†‰ª£ÁêÜ„ÄÇ
- en: Understanding Q-Learning is an **important step to understanding value-based
    methods.**
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ÁêÜËß£QÂ≠¶‰π†ÊòØÁêÜËß£Âü∫‰∫é‰ª∑ÂÄºÁöÑÊñπÊ≥ïÁöÑÈáçË¶Å‰∏ÄÊ≠•„ÄÇ
- en: In the next Unit with Deep Q-Learning, we‚Äôll see that while creating and updating
    a Q-table was a good strategy ‚Äî **however, it is not scalable.**
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®‰∏ã‰∏Ä‰∏™‰ΩøÁî®Ê∑±Â∫¶ Q Â≠¶‰π†ÁöÑÂçïÂÖÉ‰∏≠ÔºåÊàë‰ª¨Â∞ÜÁúãÂà∞ÂàõÂª∫ÂíåÊõ¥Êñ∞ Q Ë°®ÊòØ‰∏Ä‰∏™‰∏çÈîôÁöÑÁ≠ñÁï•Ôºå**‰ΩÜÊòØÔºåËøôÂπ∂‰∏çÂÖ∑ÊúâÂèØÊâ©Â±ïÊÄß„ÄÇ**
- en: For instance, imagine you create an agent that learns to play Doom.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ‰æãÂ¶ÇÔºåÊÉ≥Ë±°‰∏Ä‰∏ã‰Ω†ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â≠¶‰π†Áé©ÊØÅÁÅ≠ÁöÑ‰ª£ÁêÜÁ®ãÂ∫è„ÄÇ
- en: '![Doom](../Images/5692612f8572824879d3b76eb2ec21b4.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![ÊØÅÁÅ≠](../Images/5692612f8572824879d3b76eb2ec21b4.png)'
- en: Doom is a large environment with a huge state space (millions of different states).
    Creating and updating a Q-table for that environment would not be efficient.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ÊØÅÁÅ≠ÊòØ‰∏Ä‰∏™Â∫ûÂ§ßÁöÑÁéØÂ¢ÉÔºåÂÖ∑ÊúâÂ∑®Â§ßÁöÑÁä∂ÊÄÅÁ©∫Èó¥ÔºàÊï∞Áôæ‰∏á‰∏™‰∏çÂêåÁöÑÁä∂ÊÄÅÔºâ„ÄÇ‰∏∫ËØ•ÁéØÂ¢ÉÂàõÂª∫ÂíåÊõ¥Êñ∞ Q Ë°®Â∞Ü‰∏çÈ´òÊïà„ÄÇ
- en: That‚Äôs why we‚Äôll study Deep Q-Learning in the next unit, an algorithm **where
    we use a neural network that approximates, given a state, the different Q-values
    for each action.**
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ËøôÂ∞±ÊòØ‰∏∫‰ªÄ‰πàÊàë‰ª¨Â∞ÜÂú®‰∏ã‰∏Ä‰∏™ÂçïÂÖÉ‰∏≠Â≠¶‰π†Ê∑±Â∫¶ Q Â≠¶‰π†ÔºåËøôÊòØ‰∏ÄÁßçÁÆóÊ≥ïÔºå**Êàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™Á•ûÁªèÁΩëÁªúÊù•Ëøë‰ººÔºåÁªôÂÆö‰∏Ä‰∏™Áä∂ÊÄÅÔºåÊØè‰∏™Âä®‰ΩúÁöÑ‰∏çÂêå Q ÂÄº„ÄÇ**
- en: '![Environments](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![ÁéØÂ¢É](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
- en: See you in Unit 3! üî•
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Âú®Á¨¨‰∏âÂçïÂÖÉËßÅÔºÅüî•
- en: Keep learning, stay awesome ü§ó
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ÁªßÁª≠Â≠¶‰π†Ôºå‰øùÊåÅÊ£íÊ£íÁöÑ ü§ó
