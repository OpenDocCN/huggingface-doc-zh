- en: Hands-on
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/hands-on](https://huggingface.co/learn/deep-rl-course/unit2/hands-on)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we studied the Q-Learning algorithm, letâ€™s implement it from scratch
    and train our Q-Learning agent in two environments:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Frozen-Lake-v1 (non-slippery and slippery version)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
    â˜ƒï¸ : where our agent will need toÂ **go from the starting state (S) to the goal
    state (G)**Â by walking only on frozen tiles (F) and avoiding holes (H).'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[An autonomous taxi](https://gymnasium.farama.org/environments/toy_text/taxi/)
    ğŸš– will needÂ **to learn to navigate**Â a city toÂ **transport its passengers from
    point A to point B.**'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Environments](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Thanks to a [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard),
    youâ€™ll be able to compare your results with other classmates and exchange the
    best practices to improve your agentâ€™s scores. Who will win the challenge for
    Unit 2?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained Taxi model to the Hub and **get a result of >= 4.5**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: And you can check your progress here ğŸ‘‰ [https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course](https://huggingface.co/spaces/ThomasSimonini/Check-my-progress-Deep-RL-Course)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on click on the Open In Colab button** ğŸ‘‡ :'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 2: Q-Learning with FrozenLake-v1 â›„ and Taxi-v3 ğŸš•'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Unit 2 Thumbnail](../Images/0b9bbdbce6b297349ae6de9075b71340.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: In this notebook, **youâ€™ll code your first Reinforcement Learning agent from
    scratch** to play FrozenLake â„ï¸ using Q-Learning, share it with the community,
    and experiment with different configurations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: â¬‡ï¸ Here is an example of what **you will achieve in just a couple of minutes.**
    â¬‡ï¸
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/10f0816329e6557bfe5cfedcbbc9c8e0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'ğŸ® Environments:'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ğŸ“š RL-Library:'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python and NumPy
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gymnasium](https://gymnasium.farama.org/)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weâ€™re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook ğŸ†
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use **Gymnasium**, the environment library.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to code a Q-Learning agent from scratch.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **push your trained agent and the code to the Hub** with a nice video
    replay and an evaluation score ğŸ”¥.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This notebook is from the Deep Reinforcement Learning Course
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Deep RL Course illustration](../Images/1ffbb6aa2076af9a6f9eb9b4e21ecf34.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'In this free course, you will:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“– Study Deep Reinforcement Learning in **theory and practice**.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ§‘â€ğŸ’» Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL
    Baselines3 Zoo, CleanRL and Sample Factory 2.0.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ğŸ¤– Train **agents in unique environments**
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more check ğŸ“š the syllabus ğŸ‘‰ [https://simoninithomas.github.io/deep-rl-course](https://simoninithomas.github.io/deep-rl-course)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Donâ€™t forget to **[sign up to the course](http://eepurl.com/ic5ZUD)** (we are
    collecting your email to be able toÂ **send you the links when each Unit is published
    and give you information about the challenges and updates).**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The best way to keep in touch is to join our discord server to exchange with
    the community and with us ğŸ‘‰ğŸ» [https://discord.gg/ydHrjt3WP5](https://discord.gg/ydHrjt3WP5)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites ğŸ—ï¸
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ”² ğŸ“š **Study [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)**
    ğŸ¤—
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: A small recap of Q-Learning
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Q-Learning* **is the RL algorithm that**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Trains *Q-Function*, an **action-value function** that is encoded, in internal
    memory, by a *Q-table* **that contains all the state-action pair values.**
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a state and action, our Q-Function **will search the Q-table for the corresponding
    value.**
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Q function](../Images/c6f51357ba01781edc9f3041b33e5be4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if we **have an optimal Q-function**, we have an optimal policy, since we
    **know for each state, the best action to take.**
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Link value policy](../Images/06e7785cc764e6109bfc6c89005a4d92.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: But, in the beginning,Â our **Q-Table is useless since it gives arbitrary value
    for each state-action pairÂ (most of the time we initialize the Q-Table to 0 values)**.
    But, as weâ€™llÂ explore the environment and update our Q-Table it will give us better
    and better approximations
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![q-learning.jpeg](../Images/ef3754e1d95bf97371e1a41ca61d6d72.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: 'This is the Q-Learning pseudocode:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Letâ€™s code our first Reinforcement Learning algorithm ğŸš€
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),
    you need to push your trained Taxi model to the Hub and **get a result of >= 4.5**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    and find your model, **the result = mean_reward - std of reward**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section ğŸ‘‰ [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Install dependencies and create a virtual display ğŸ”½
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the notebook, weâ€™ll need to generate a replay video. To do so, with Colab,
    **we need to have a virtual screen to render the environment** (and thus record
    the frames).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Hence the following cell will install the libraries and create and run a virtual
    screen ğŸ–¥
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Weâ€™ll install multiple ones:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '`gymnasium`: Contains the FrozenLake-v1 â›„ and Taxi-v3 ğŸš• environments.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pygame`: Used for the FrozenLake-v1 and Taxi-v3 UI.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`: Used for handling our Q-table.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hugging Face Hub ğŸ¤— works as a central place where anyone can share and explore
    models and datasets. It has versioning, metrics, visualizations and other features
    that will allow you to easily collaborate with others.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: You can see here all the Deep RL models available (if they use Q Learning) here
    ğŸ‘‰ [https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To make sure the new installed libraries are used, **sometimes itâ€™s required
    to restart the notebook runtime**. The next cell will force the **runtime to crash,
    so youâ€™ll need to connect again and run the code starting from here**. Thanks
    to this trick, **we will be able to run our virtual screen.**
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¡®ä¿æ–°å®‰è£…çš„åº“è¢«ä½¿ç”¨ï¼Œ**æœ‰æ—¶éœ€è¦é‡æ–°å¯åŠ¨ç¬”è®°æœ¬è¿è¡Œæ—¶**ã€‚ä¸‹ä¸€ä¸ªå•å…ƒæ ¼å°†å¼ºåˆ¶**è¿è¡Œæ—¶å´©æºƒï¼Œå› æ­¤æ‚¨éœ€è¦é‡æ–°è¿æ¥å¹¶ä»è¿™é‡Œå¼€å§‹è¿è¡Œä»£ç **ã€‚é€šè¿‡è¿™ä¸ªæŠ€å·§ï¼Œ**æˆ‘ä»¬å°†èƒ½å¤Ÿè¿è¡Œæˆ‘ä»¬çš„è™šæ‹Ÿå±å¹•**ã€‚
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Import the packages ğŸ“¦
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼å…¥åŒ…ğŸ“¦
- en: 'In addition to the installed libraries, we also use:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å®‰è£…çš„åº“ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ï¼š
- en: '`random`: To generate random numbers (that will be useful for epsilon-greedy
    policy).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random`ï¼šç”Ÿæˆéšæœºæ•°ï¼ˆå¯¹epsilon-è´ªå©ªç­–ç•¥æœ‰ç”¨ï¼‰ã€‚'
- en: '`imageio`: To generate a replay video.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imageio`ï¼šç”Ÿæˆé‡æ’­è§†é¢‘ã€‚'
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Weâ€™re now ready to code our Q-Learning algorithm ğŸ”¥
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™æˆ‘ä»¬çš„Qå­¦ä¹ ç®—æ³•ğŸ”¥
- en: 'Part 1: Frozen Lake â›„ (non slippery version)'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬1éƒ¨åˆ†ï¼šå†°å†»æ¹–â›„ï¼ˆéæ»‘åŠ¨ç‰ˆæœ¬ï¼‰
- en: Create and understand FrozenLake environment â›„ (( https://gymnasium.farama.org/environments/toy_text/frozen_lake/
    )
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºå’Œç†è§£å†°å†»æ¹–ç¯å¢ƒâ›„ï¼ˆhttps://gymnasium.farama.org/environments/toy_text/frozen_lake/ï¼‰
- en: '* * *'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ğŸ’¡ A good habit when you start to use an environment is to check its documentation
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å½“æ‚¨å¼€å§‹ä½¿ç”¨ç¯å¢ƒæ—¶ï¼Œå…»æˆä¸€ä¸ªå¥½ä¹ æƒ¯æ˜¯æ£€æŸ¥å…¶æ–‡æ¡£
- en: ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- en: '* * *'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Weâ€™re going to train our Q-Learning agent **to navigate from the starting state
    (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes
    (H)**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è®­ç»ƒæˆ‘ä»¬çš„Qå­¦ä¹ ä»£ç†**ä»èµ·å§‹çŠ¶æ€ï¼ˆSï¼‰å¯¼èˆªåˆ°ç›®æ ‡çŠ¶æ€ï¼ˆGï¼‰ï¼Œåªèƒ½åœ¨å†°å†»ç“·ç –ï¼ˆFï¼‰ä¸Šè¡Œèµ°ï¼Œé¿å…æ‰å…¥æ´ä¸­ï¼ˆHï¼‰**ã€‚
- en: 'We can have two sizes of environment:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æœ‰ä¸¤ç§ç¯å¢ƒå¤§å°ï¼š
- en: '`map_name="4x4"`: a 4x4 grid version'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_name="4x4"`ï¼šä¸€ä¸ª4x4ç½‘æ ¼ç‰ˆæœ¬'
- en: '`map_name="8x8"`: a 8x8 grid version'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map_name="8x8"`ï¼šä¸€ä¸ª8x8ç½‘æ ¼ç‰ˆæœ¬'
- en: 'The environment has two modes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç¯å¢ƒæœ‰ä¸¤ç§æ¨¡å¼ï¼š
- en: '`is_slippery=False`: The agent always moves **in the intended direction** due
    to the non-slippery nature of the frozen lake (deterministic).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_slippery=False`ï¼šç”±äºå†°å†»æ¹–çš„éæ»‘åŠ¨æ€§è´¨ï¼Œä»£ç†å§‹ç»ˆ**æœç€é¢„æœŸæ–¹å‘ç§»åŠ¨**ã€‚'
- en: '`is_slippery=True`: The agent **may not always move in the intended direction**
    due to the slippery nature of the frozen lake (stochastic).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_slippery=True`ï¼šç”±äºå†°å†»æ¹–çš„æ»‘åŠ¨æ€§è´¨ï¼Œä»£ç†**å¯èƒ½ä¸æ€»æ˜¯æœç€é¢„æœŸæ–¹å‘ç§»åŠ¨**ï¼ˆéšæœºçš„ï¼‰ã€‚'
- en: For now letâ€™s keep it simple with the 4x4 map and non-slippery. We add a parameter
    called `render_mode` that specifies how the environment should be visualised.
    In our case because we **want to record a video of the environment at the end,
    we need to set render_mode to rgb_array**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¿æŒç®€å•ï¼Œä½¿ç”¨4x4åœ°å›¾å’Œéæ»‘åŠ¨ã€‚æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªåä¸º`render_mode`çš„å‚æ•°ï¼ŒæŒ‡å®šç¯å¢ƒåº”å¦‚ä½•å¯è§†åŒ–ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå› ä¸ºæˆ‘ä»¬**å¸Œæœ›åœ¨æœ€åè®°å½•ç¯å¢ƒçš„è§†é¢‘ï¼Œæˆ‘ä»¬éœ€è¦å°†render_modeè®¾ç½®ä¸ºrgb_array**ã€‚
- en: 'As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)
    â€œrgb_arrayâ€: Return a single frame representing the current state of the environment.
    A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y
    pixel image.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[æ–‡æ¡£ä¸­æ‰€è¿°](https://gymnasium.farama.org/api/env/#gymnasium.Env.render)ï¼Œâ€œrgb_arrayâ€ï¼šè¿”å›ä»£è¡¨ç¯å¢ƒå½“å‰çŠ¶æ€çš„å•ä¸ªå¸§ã€‚ä¸€ä¸ªå¸§æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸ºï¼ˆxï¼Œyï¼Œ3ï¼‰çš„np.ndarrayï¼Œè¡¨ç¤ºxä¹˜yåƒç´ å›¾åƒçš„RGBå€¼ã€‚
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Solution
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can create your own custom grid like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åˆ›å»ºè‡ªå·±çš„è‡ªå®šä¹‰ç½‘æ ¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: but weâ€™ll use the default environment for now.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨é»˜è®¤ç¯å¢ƒã€‚
- en: 'Letâ€™s see what the Environment looks like:'
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ç¯å¢ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ï¼š
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We see with `Observation Space Shape Discrete(16)` that the observation is an
    integer representing the **agentâ€™s current position as current_row * ncols + current_col
    (where both the row and col start at 0)**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°`Observation Space Shape Discrete(16)`ï¼Œè§‚å¯Ÿæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤º**ä»£ç†çš„å½“å‰ä½ç½®ä¸ºcurrent_row *
    ncols + current_colï¼ˆå…¶ä¸­è¡Œå’Œåˆ—éƒ½ä»0å¼€å§‹ï¼‰**ã€‚
- en: 'For example, the goal position in the 4x4 map can be calculated as follows:
    3 * 4 + 3 = 15\. The number of possible observations is dependent on the size
    of the map. **For example, the 4x4 map has 16 possible observations.**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œ4x4åœ°å›¾ä¸­çš„ç›®æ ‡ä½ç½®å¯ä»¥è®¡ç®—å¦‚ä¸‹ï¼š3 * 4 + 3 = 15ã€‚å¯èƒ½çš„è§‚å¯Ÿæ¬¡æ•°å–å†³äºåœ°å›¾çš„å¤§å°ã€‚**ä¾‹å¦‚ï¼Œ4x4åœ°å›¾æœ‰16ç§å¯èƒ½çš„è§‚å¯Ÿã€‚**
- en: 'For instance, this is what state = 0 looks like:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™å°±æ˜¯çŠ¶æ€=0çš„æ ·å­ï¼š
- en: '![FrozenLake](../Images/5d72cc95a82a64293bb4212444acec50.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![FrozenLake](../Images/5d72cc95a82a64293bb4212444acec50.png)'
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with 4 actions available ğŸ®:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´ï¼ˆä»£ç†å¯ä»¥é‡‡å–çš„å¯èƒ½åŠ¨ä½œé›†ï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰4ä¸ªå¯ç”¨åŠ¨ä½œğŸ®ï¼š
- en: '0: GO LEFT'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0ï¼šå‘å·¦ç§»åŠ¨
- en: '1: GO DOWN'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ï¼šå‘ä¸‹ç§»åŠ¨
- en: '2: GO RIGHT'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ï¼šå‘å³ç§»åŠ¨
- en: '3: GO UP'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ï¼šå‘ä¸Šç§»åŠ¨
- en: 'Reward function ğŸ’°:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å‡½æ•°ğŸ’°ï¼š
- en: 'Reach goal: +1'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾¾åˆ°ç›®æ ‡ï¼š+1
- en: 'Reach hole: 0'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ°è¾¾æ´ï¼š0
- en: 'Reach frozen: 0'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ°è¾¾å†°å†»ï¼š0
- en: Create and Initialize the Q-table ğŸ—„ï¸
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºå’Œåˆå§‹åŒ–Qè¡¨ğŸ—„ï¸
- en: (ğŸ‘€ Step 1 of the pseudocode)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆğŸ‘€ ä¼ªä»£ç çš„ç¬¬1æ­¥ï¼‰
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
- en: 'Itâ€™s time to initialize our Q-table! To know how many rows (states) and columns
    (actions) to use, we need to know the action and observation space. We already
    know their values from before, but weâ€™ll want to obtain them programmatically
    so that our algorithm generalizes for different environments. Gym provides us
    a way to do that: `env.action_space.n` and `env.observation_space.n`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯åˆå§‹åŒ–æˆ‘ä»¬çš„Qè¡¨çš„æ—¶å€™äº†ï¼è¦çŸ¥é“è¦ä½¿ç”¨å¤šå°‘è¡Œï¼ˆçŠ¶æ€ï¼‰å’Œåˆ—ï¼ˆåŠ¨ä½œï¼‰ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´ã€‚æˆ‘ä»¬å·²ç»çŸ¥é“å®ƒä»¬çš„å€¼äº†ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä»¥ç¼–ç¨‹æ–¹å¼è·å–å®ƒä»¬ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æ¨å¹¿åˆ°ä¸åŒçš„ç¯å¢ƒã€‚Gymä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§æ–¹æ³•ï¼š`env.action_space.n`å’Œ`env.observation_space.n`
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Solution
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Define the greedy policy ğŸ¤–
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰è´ªå©ªç­–ç•¥ğŸ¤–
- en: Remember we have two policies since Q-Learning is an **off-policy** algorithm.
    This means weâ€™re using a **different policy for acting and updating the value
    function**.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œç”±äºQå­¦ä¹ æ˜¯ä¸€ç§**ç¦»ç­–ç•¥**ç®—æ³•ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§ç­–ç•¥ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨**è¡ŒåŠ¨å’Œæ›´æ–°å€¼å‡½æ•°æ—¶ä½¿ç”¨ä¸åŒçš„ç­–ç•¥**ã€‚
- en: Epsilon-greedy policy (acting policy)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon-è´ªå©ªç­–ç•¥ï¼ˆè¡ŒåŠ¨ç­–ç•¥ï¼‰
- en: Greedy-policy (updating policy)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è´ªå©ªç­–ç•¥ï¼ˆæ›´æ–°ç­–ç•¥ï¼‰
- en: The greedy policy will also be the final policy weâ€™ll have when the Q-learning
    agent completes training. The greedy policy is used to select an action using
    the Q-table.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è´ªå©ªç­–ç•¥ä¹Ÿå°†æ˜¯Qå­¦ä¹ ä»£ç†å®Œæˆè®­ç»ƒåçš„æœ€ç»ˆç­–ç•¥ã€‚è´ªå©ªç­–ç•¥ç”¨äºä½¿ç”¨Qè¡¨é€‰æ‹©åŠ¨ä½œã€‚
- en: '![Q-Learning](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/ce691ce98ae89b58669eb975be3f446c.png)'
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Solution
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Define the epsilon-greedy policy ğŸ¤–
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰epsilon-greedyç­–ç•¥ ğŸ¤–
- en: Epsilon-greedy is the training policy that handles the exploration/exploitation
    trade-off.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon-greedyæ˜¯å¤„ç†æ¢ç´¢/åˆ©ç”¨æƒè¡¡çš„è®­ç»ƒç­–ç•¥ã€‚
- en: 'The idea with epsilon-greedy:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨epsilon-greedyçš„æƒ³æ³•ï¼š
- en: 'With *probability 1â€Š-â€ŠÉ›* : **we do exploitation** (i.e. our agent selects the
    action with the highest state-action pair value).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥*æ¦‚ç‡1-É›*ï¼š**æˆ‘ä»¬è¿›è¡Œåˆ©ç”¨**ï¼ˆå³æˆ‘ä»¬çš„ä»£ç†é€‰æ‹©å…·æœ‰æœ€é«˜çŠ¶æ€-åŠ¨ä½œå¯¹å€¼çš„åŠ¨ä½œï¼‰ã€‚
- en: 'With *probability É›*: we do **exploration** (trying a random action).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥*æ¦‚ç‡É›*ï¼šæˆ‘ä»¬è¿›è¡Œ**æ¢ç´¢**ï¼ˆå°è¯•éšæœºåŠ¨ä½œï¼‰ã€‚
- en: As the training continues, we progressively **reduce the epsilon value since
    we will need less and less exploration and more exploitation.**
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬é€æ¸**å‡å°‘epsilonå€¼ï¼Œå› ä¸ºæˆ‘ä»¬å°†éœ€è¦è¶Šæ¥è¶Šå°‘çš„æ¢ç´¢å’Œæ›´å¤šçš„åˆ©ç”¨**ã€‚
- en: '![Q-Learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/30b0aba4490af7f85f0594dc198e9c03.png)'
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Solution
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Define the hyperparameters âš™ï¸
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰è¶…å‚æ•° âš™ï¸
- en: The exploration related hyperparamters are some of the most important ones.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¢ç´¢ç›¸å…³çš„è¶…å‚æ•°æ˜¯æœ€é‡è¦çš„ä¹‹ä¸€ã€‚
- en: We need to make sure that our agent **explores enough of the state space** to
    learn a good value approximation. To do that, we need to have progressive decay
    of the epsilon.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„ä»£ç†**æ¢ç´¢è¶³å¤Ÿçš„çŠ¶æ€ç©ºé—´**ä»¥å­¦ä¹ ä¸€ä¸ªè‰¯å¥½çš„å€¼è¿‘ä¼¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é€æ¸å‡å°‘epsilonçš„è¡°å‡ã€‚
- en: If you decrease epsilon too fast (too high decay_rate), **you take the risk
    that your agent will be stuck**, since your agent didnâ€™t explore enough of the
    state space and hence canâ€™t solve the problem.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å°†epsilonå‡å°‘å¾—å¤ªå¿«ï¼ˆè¡°å‡ç‡å¤ªé«˜ï¼‰ï¼Œ**æ‚¨ä¼šå†’ç€ä»£ç†è¢«å¡ä½çš„é£é™©**ï¼Œå› ä¸ºæ‚¨çš„ä»£ç†æ²¡æœ‰æ¢ç´¢è¶³å¤Ÿçš„çŠ¶æ€ç©ºé—´ï¼Œå› æ­¤æ— æ³•è§£å†³é—®é¢˜ã€‚
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Create the training loop method
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºè®­ç»ƒå¾ªç¯æ–¹æ³•
- en: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](../Images/e98aadd735672374a66857c170d3b2ce.png)'
- en: 'The training loop goes like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯å¦‚ä¸‹ï¼š
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Solution
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: '[PRE23]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Train the Q-Learning agent ğŸƒ
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒQ-Learningä»£ç† ğŸƒ
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Letâ€™s see what our Q-Learning table looks like now ğŸ‘€
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„Q-Learningè¡¨ç°å¦‚ä½• ğŸ‘€
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The evaluation method ğŸ“
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ–¹æ³• ğŸ“
- en: We defined the evaluation method that weâ€™re going to use to test our Q-Learning
    agent.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰äº†è¦ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•æ¥æµ‹è¯•æˆ‘ä»¬çš„Q-Learningä»£ç†ã€‚
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Evaluate our Q-Learning agent ğŸ“ˆ
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æˆ‘ä»¬çš„Q-Learningä»£ç† ğŸ“ˆ
- en: Usually, you should have a mean reward of 1.0
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œæ‚¨åº”è¯¥æœ‰ä¸€ä¸ªå¹³å‡å¥–åŠ±ä¸º1.0
- en: The **environment is relatively easy** since the state space is really small
    (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/),
    which introduces stochasticity, making the environment more complex.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”±äºçŠ¶æ€ç©ºé—´éå¸¸å°ï¼ˆ16ï¼‰ï¼Œ**ç¯å¢ƒç›¸å¯¹å®¹æ˜“**ã€‚æ‚¨å¯ä»¥å°è¯•[ç”¨æ»‘åŠ¨ç‰ˆæœ¬æ›¿æ¢å®ƒ](https://www.gymlibrary.dev/environments/toy_text/frozen_lake)ï¼Œè¿™ä¼šå¼•å…¥éšæœºæ€§ï¼Œä½¿ç¯å¢ƒæ›´åŠ å¤æ‚ã€‚
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Publish our trained model to the Hub ğŸ”¥
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ”¥
- en: Now that we saw good results after the training, **we can publish our trained
    model to the Hub ğŸ¤— with one line of code**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åœ¨è®­ç»ƒåçœ‹åˆ°äº†è‰¯å¥½çš„ç»“æœï¼Œ**æˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°Hub ğŸ¤—**ã€‚
- en: 'Hereâ€™s an example of a Model Card:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªæ¨¡å‹å¡çš„ç¤ºä¾‹ï¼š
- en: '![Model card](../Images/9832a45306ed2f863e30acb21be3060d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![æ¨¡å‹å¡](../Images/9832a45306ed2f863e30acb21be3060d.png)'
- en: Under the hood, the Hub uses git-based repositories (donâ€™t worry if you donâ€™t
    know what git is), which means you can update the model with new versions as you
    experiment and improve your agent.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹•åï¼ŒHubä½¿ç”¨åŸºäºgitçš„å­˜å‚¨åº“ï¼ˆå¦‚æœæ‚¨ä¸çŸ¥é“gitæ˜¯ä»€ä¹ˆï¼Œä¸ç”¨æ‹…å¿ƒï¼‰ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥éšç€å®éªŒå’Œæ”¹è¿›ä»£ç†æ›´æ–°æ¨¡å‹çš„æ–°ç‰ˆæœ¬ã€‚
- en: Do not modify this code
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: .
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ã€‚
- en: By using `push_to_hub` **you evaluate, record a replay, generate a model card
    of your agent and push it to the Hub**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨`push_to_hub` **æ‚¨å¯ä»¥è¯„ä¼°ã€è®°å½•é‡æ”¾ã€ç”Ÿæˆæ‚¨çš„ä»£ç†çš„æ¨¡å‹å¡å¹¶å°†å…¶æ¨é€åˆ°Hub**ã€‚
- en: 'This way:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼š
- en: You can **showcase our work** ğŸ”¥
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œ** ğŸ”¥
- en: You can **visualize your agent playing** ğŸ‘€
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**å¯è§†åŒ–æ‚¨çš„ä»£ç†è¿›è¡Œæ¸¸æˆ** ğŸ‘€
- en: You can **share an agent with the community that others can use** ğŸ’¾
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**ä¸ç¤¾åŒºåˆ†äº«ä¸€ä¸ªå…¶ä»–äººå¯ä»¥ä½¿ç”¨çš„ä»£ç†** ğŸ’¾
- en: You can **access a leaderboard ğŸ† to see how well your agent is performing compared
    to your classmates** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥**è®¿é—®æ’è¡Œæ¦œ ğŸ† æŸ¥çœ‹æ‚¨çš„ä»£ç†ä¸åŒå­¦ç›¸æ¯”çš„è¡¨ç°å¦‚ä½•** ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: 'To be able to share your model with the community there are three more steps
    to follow:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†èƒ½å¤Ÿä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ï¼Œè¿˜æœ‰ä¸‰ä¸ªæ­¥éª¤è¦éµå¾ªï¼š
- en: 1ï¸âƒ£ (If itâ€™s not already done) create an account to HF â¡ [https://huggingface.co/join](https://huggingface.co/join)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ï¼ˆå¦‚æœå°šæœªå®Œæˆï¼‰åˆ›å»ºä¸€ä¸ªHFè´¦æˆ· â¡ [https://huggingface.co/join](https://huggingface.co/join)
- en: 2ï¸âƒ£ Sign in and then, you need to store your authentication token from the Hugging
    Face website.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç™»å½•ï¼Œç„¶åï¼Œæ‚¨éœ€è¦ä»Hugging Faceç½‘ç«™å­˜å‚¨æ‚¨çš„èº«ä»½éªŒè¯ä»¤ç‰Œã€‚
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ–°çš„ä»¤ç‰Œï¼ˆ[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)ï¼‰**å…·æœ‰å†™å…¥æƒé™**
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![åˆ›å»ºHFä»¤ç‰Œ](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you donâ€™t want to use a Google Colab or a Jupyter Notebook, you need to
    use this command instead: `huggingface-cli login` (or `login`)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸æƒ³ä½¿ç”¨Google Colabæˆ–Jupyter Notebookï¼Œæ‚¨éœ€è¦ä½¿ç”¨è¿™ä¸ªå‘½ä»¤ï¼š`huggingface-cli login`ï¼ˆæˆ–`login`ï¼‰
- en: 3ï¸âƒ£ Weâ€™re now ready to push our trained agent to the ğŸ¤— Hub ğŸ”¥ using `push_to_hub()`
    function
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ ç°åœ¨æˆ‘ä»¬å‡†å¤‡ä½¿ç”¨`push_to_hub()`å‡½æ•°å°†è®­ç»ƒå¥½çš„ä»£ç†æ¨é€åˆ°ğŸ¤— Hub ğŸ”¥
- en: Letâ€™s create **the model dictionary that contains the hyperparameters and the
    Q_table**.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»º**åŒ…å«è¶…å‚æ•°å’ŒQè¡¨çš„æ¨¡å‹å­—å…¸**ã€‚
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Letâ€™s fill the `push_to_hub` function:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¡«å†™`push_to_hub`å‡½æ•°ï¼š
- en: '`repo_id`: the name of the Hugging Face Hub Repository that will be created/updated
    `(repo_id = {username}/{repo_name})` ğŸ’¡ A good `repo_id` is `{username}/q-{env_id}`'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`ï¼šå°†è¦åˆ›å»º/æ›´æ–°çš„Hugging Face Hubå­˜å‚¨åº“çš„åç§°ï¼ˆ`repo_id = {username}/{repo_name}`ï¼‰ğŸ’¡
    ä¸€ä¸ªå¥½çš„`repo_id`æ˜¯`{username}/q-{env_id}`'
- en: '`model`: our model dictionary containing the hyperparameters and the Qtable.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`ï¼šåŒ…å«è¶…å‚æ•°å’ŒQè¡¨çš„æ¨¡å‹å­—å…¸ã€‚'
- en: '`env`: the environment.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env`ï¼šç¯å¢ƒã€‚'
- en: '`commit_message`: message of the commit'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`ï¼šæäº¤çš„æ¶ˆæ¯'
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congrats ğŸ¥³ youâ€™ve just implemented from scratch, trained, and uploaded your
    first Reinforcement Learning agent. FrozenLake-v1 no_slippery is very simple environment,
    letâ€™s try a harder one ğŸ”¥.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œğŸ¥³ï¼Œä½ åˆšåˆšä»å¤´å¼€å§‹å®ç°ã€è®­ç»ƒå’Œä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚FrozenLake-v1 no_slipperyæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¯å¢ƒï¼Œè®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªæ›´éš¾çš„ğŸ”¥ã€‚
- en: 'Part 2: Taxi-v3 ğŸš–'
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬2éƒ¨åˆ†ï¼šTaxi-v3 ğŸš–
- en: Create and understand Taxi-v3 ğŸš•
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºå¹¶ç†è§£Taxi-v3 ğŸš•
- en: '* * *'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ğŸ’¡ A good habit when you start to use an environment is to check its documentation
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å½“ä½ å¼€å§‹ä½¿ç”¨ä¸€ä¸ªç¯å¢ƒæ—¶ï¼Œå…»æˆä¸€ä¸ªå¥½ä¹ æƒ¯æ˜¯æŸ¥çœ‹å…¶æ–‡æ¡£
- en: ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‰ [https://gymnasium.farama.org/environments/toy_text/taxi/](https://gymnasium.farama.org/environments/toy_text/taxi/)
- en: '* * *'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In `Taxi-v3` ğŸš•, there are four designated locations in the grid world indicated
    by R(ed), G(reen), Y(ellow), and B(lue).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`Taxi-v3` ğŸš•ä¸­ï¼Œç½‘æ ¼ä¸–ç•Œä¸­æœ‰å››ä¸ªæŒ‡å®šä½ç½®ï¼Œåˆ†åˆ«ç”¨Rï¼ˆçº¢è‰²ï¼‰ã€Gï¼ˆç»¿è‰²ï¼‰ã€Yï¼ˆé»„è‰²ï¼‰å’ŒBï¼ˆè“è‰²ï¼‰è¡¨ç¤ºã€‚
- en: When the episode starts, **the taxi starts off at a random square** and the
    passenger is at a random location. The taxi drives to the passengerâ€™s location,
    **picks up the passenger**, drives to the passengerâ€™s destination (another one
    of the four specified locations), and then **drops off the passenger**. Once the
    passenger is dropped off, the episode ends.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰§é›†å¼€å§‹æ—¶ï¼Œ**å‡ºç§Ÿè½¦ä»éšæœºæ–¹æ ¼å¼€å§‹**ï¼Œä¹˜å®¢åœ¨éšæœºä½ç½®ã€‚å‡ºç§Ÿè½¦é©¶å‘ä¹˜å®¢çš„ä½ç½®ï¼Œ**æ¥è½½ä¹˜å®¢**ï¼Œé©¶å‘ä¹˜å®¢çš„ç›®çš„åœ°ï¼ˆå¦å¤–å››ä¸ªæŒ‡å®šä½ç½®ä¹‹ä¸€ï¼‰ï¼Œç„¶å**æ”¾ä¸‹ä¹˜å®¢**ã€‚ä¸€æ—¦ä¹˜å®¢ä¸‹è½¦ï¼Œå‰§é›†ç»“æŸã€‚
- en: '![Taxi](../Images/dcffe584c3d93a6b2668209fe5b78373.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![å‡ºç§Ÿè½¦](../Images/dcffe584c3d93a6b2668209fe5b78373.png)'
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There are **500 discrete states since there are 25 taxi positions, 5 possible
    locations of the passenger** (including the case when the passenger is in the
    taxi), and **4 destination locations.**
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰**500ä¸ªç¦»æ•£çŠ¶æ€ï¼Œå› ä¸ºæœ‰25ä¸ªå‡ºç§Ÿè½¦ä½ç½®ï¼Œ5ä¸ªä¹˜å®¢å¯èƒ½çš„ä½ç½®**ï¼ˆåŒ…æ‹¬ä¹˜å®¢åœ¨å‡ºç§Ÿè½¦ä¸Šçš„æƒ…å†µï¼‰ï¼Œä»¥åŠ**4ä¸ªç›®çš„åœ°ä½ç½®**ã€‚
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The action space (the set of possible actions the agent can take) is discrete
    with **6 actions available ğŸ®**:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´ï¼ˆä»£ç†å¯ä»¥é‡‡å–çš„å¯èƒ½åŠ¨ä½œé›†ï¼‰æ˜¯ç¦»æ•£çš„ï¼Œæœ‰**6ä¸ªå¯ç”¨åŠ¨ä½œğŸ®**ï¼š
- en: '0: move south'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0ï¼šå‘å—ç§»åŠ¨
- en: '1: move north'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1ï¼šå‘åŒ—ç§»åŠ¨
- en: '2: move east'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2ï¼šå‘ä¸œç§»åŠ¨
- en: '3: move west'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3ï¼šå‘è¥¿ç§»åŠ¨
- en: '4: pickup passenger'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4ï¼šæ¥è½½ä¹˜å®¢
- en: '5: drop off passenger'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5ï¼šæ”¾ä¸‹ä¹˜å®¢
- en: 'Reward function ğŸ’°:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å‡½æ•°ğŸ’°ï¼š
- en: -1 per step unless other reward is triggered.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯æ­¥-1ï¼Œé™¤éè§¦å‘å…¶ä»–å¥–åŠ±ã€‚
- en: +20 delivering passenger.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +20 é€è¾¾ä¹˜å®¢ã€‚
- en: -10 executing â€œpickupâ€ and â€œdrop-offâ€ actions illegally.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -10æ‰§è¡Œâ€œæ¥è½½â€å’Œâ€œæ”¾ä¸‹â€åŠ¨ä½œéæ³•ã€‚
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Define the hyperparameters âš™ï¸
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰è¶…å‚æ•°âš™ï¸
- en: 'âš  DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your
    agent with the same taxi starting positions for every classmate**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: âš  ä¸è¦ä¿®æ”¹EVAL_SEEDï¼ševal_seedæ•°ç»„**å…è®¸æˆ‘ä»¬ä¸ºæ¯ä¸ªåŒå­¦è¯„ä¼°æ‚¨çš„ä»£ç†ï¼Œä½¿å‡ºç§Ÿè½¦çš„èµ·å§‹ä½ç½®ç›¸åŒ**
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Train our Q-Learning agent ğŸƒ
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒæˆ‘ä»¬çš„Qå­¦ä¹ ä»£ç†ğŸƒ
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Create a model dictionary ğŸ’¾ and publish our trained model to the Hub ğŸ”¥
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ğŸ’¾å¹¶å°†æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹å‘å¸ƒåˆ°HubğŸ”¥
- en: We create a model dictionary that will contain all the training hyperparameters
    for reproducibility and the Q-Table.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡å‹å­—å…¸ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰è®­ç»ƒè¶…å‚æ•°ä»¥å®ç°å¯é‡ç°æ€§å’ŒQè¡¨ã€‚
- en: '[PRE41]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now that itâ€™s on the Hub, you can compare the results of your Taxi-v3 with your
    classmates using the leaderboard ğŸ† ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒåœ¨Hubä¸Šï¼Œä½ å¯ä»¥é€šè¿‡æ’è¡Œæ¦œä¸åŒå­¦ä»¬æ¯”è¾ƒä½ çš„Taxi-v3çš„ç»“æœğŸ†ğŸ‘‰ [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
- en: '![Taxi Leaderboard](../Images/0d0ce61b026fca34f96441a33217667d.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![å‡ºç§Ÿè½¦æ’è¡Œæ¦œ](../Images/0d0ce61b026fca34f96441a33217667d.png)'
- en: 'Part 3: Load from Hub ğŸ”½'
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬3éƒ¨åˆ†ï¼šä»HubåŠ è½½ğŸ”½
- en: Whatâ€™s amazing with Hugging Face Hub ğŸ¤— is that you can easily load powerful
    models from the community.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face Hub ğŸ¤— ä»¤äººæƒŠå¹çš„åœ°æ–¹åœ¨äºä½ å¯ä»¥è½»æ¾åœ°åŠ è½½ç¤¾åŒºä¸­å¼ºå¤§çš„æ¨¡å‹ã€‚
- en: 'Loading a saved model from the Hub is really easy:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ä»HubåŠ è½½å·²ä¿å­˜çš„æ¨¡å‹éå¸¸å®¹æ˜“ï¼š
- en: You go [https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)
    to see the list of all the q-learning saved models.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å‰å¾€[https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)æŸ¥çœ‹æ‰€æœ‰ä¿å­˜çš„q-learningæ¨¡å‹åˆ—è¡¨ã€‚
- en: You select one and copy its repo_id
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ é€‰æ‹©ä¸€ä¸ªå¹¶å¤åˆ¶å…¶repo_id
- en: '![Copy id](../Images/38b2f545a7fd317518e1c20d339f44d7.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![å¤åˆ¶id](../Images/38b2f545a7fd317518e1c20d339f44d7.png)'
- en: 'Then we just need to use `load_from_hub` with:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨`load_from_hub`ï¼š
- en: The repo_id
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: repo_id
- en: 'The filename: the saved model inside the repo.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡ä»¶åï¼šå­˜å‚¨åœ¨å­˜å‚¨åº“ä¸­çš„å·²ä¿å­˜æ¨¡å‹ã€‚
- en: Do not modify this code
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ä¸è¦ä¿®æ”¹è¿™æ®µä»£ç 
- en: '[PRE43]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: .
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ã€‚
- en: '[PRE44]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Some additional challenges ğŸ†
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€äº›é¢å¤–çš„æŒ‘æˆ˜ğŸ†
- en: The best way to learn **is to try things on your own**! As you saw, the current
    agent is not doing great. As a first suggestion, you can train for more steps.
    With 1,000,000 steps, we saw some great results!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å­¦ä¹ çš„æœ€ä½³æ–¹å¼æ˜¯**å°è¯•è‡ªå·±åŠ¨æ‰‹**ï¼æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå½“å‰çš„ä»£ç†è¡¨ç°ä¸ä½³ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªå»ºè®®ï¼Œä½ å¯ä»¥è®­ç»ƒæ›´å¤šæ­¥éª¤ã€‚è¿›è¡Œ100ä¸‡æ­¥è®­ç»ƒåï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›å¾ˆå¥½çš„ç»“æœï¼
- en: In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)
    you will find your agents. Can you get to the top?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ä¸­ï¼Œä½ ä¼šæ‰¾åˆ°ä½ çš„ä»£ç†ã€‚ä½ èƒ½ç™»é¡¶å—ï¼Ÿ
- en: 'Here are some ideas to climb up the leaderboard:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€äº›æå‡æ’è¡Œæ¦œçš„æƒ³æ³•ï¼š
- en: Train more steps
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ›´å¤šæ­¥éª¤
- en: Try different hyperparameters by looking at what your classmates have done.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æŸ¥çœ‹åŒå­¦ä»¬çš„å·¥ä½œï¼Œå°è¯•ä¸åŒçš„è¶…å‚æ•°ã€‚
- en: '**Push your new trained model** on the Hub ğŸ”¥'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°†ä½ çš„æ–°è®­ç»ƒæ¨¡å‹æ¨é€**åˆ°HubğŸ”¥'
- en: Are walking on ice and driving taxis too boring to you? Try to **change the
    environment**, why not use FrozenLake-v1 slippery version? Check how they work
    [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun
    ğŸ‰.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†°ä¸Šè¡Œèµ°å’Œå¼€å‡ºç§Ÿè½¦å¯¹ä½ æ¥è¯´å¤ªæ— èŠäº†å—ï¼Ÿå°è¯•**æ”¹å˜ç¯å¢ƒ**ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨FrozenLake-v1æ»‘åŠ¨ç‰ˆæœ¬ï¼ŸæŸ¥çœ‹å®ƒä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„[ä½¿ç”¨gymnasiumæ–‡æ¡£](https://gymnasium.farama.org/)å¹¶ç©å¾—å¼€å¿ƒğŸ‰ã€‚
- en: '* * *'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Congrats ğŸ¥³, youâ€™ve just implemented, trained, and uploaded your first Reinforcement
    Learning agent.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œğŸ¥³ï¼Œä½ åˆšåˆšå®ç°ã€è®­ç»ƒå’Œä¸Šä¼ äº†ä½ çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ä»£ç†ã€‚
- en: Understanding Q-Learning is an **important step to understanding value-based
    methods.**
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£Qå­¦ä¹ æ˜¯ç†è§£åŸºäºä»·å€¼çš„æ–¹æ³•çš„é‡è¦ä¸€æ­¥ã€‚
- en: In the next Unit with Deep Q-Learning, weâ€™ll see that while creating and updating
    a Q-table was a good strategy â€” **however, it is not scalable.**
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ä¸ªä½¿ç”¨æ·±åº¦ Q å­¦ä¹ çš„å•å…ƒä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°åˆ›å»ºå’Œæ›´æ–° Q è¡¨æ˜¯ä¸€ä¸ªä¸é”™çš„ç­–ç•¥ï¼Œ**ä½†æ˜¯ï¼Œè¿™å¹¶ä¸å…·æœ‰å¯æ‰©å±•æ€§ã€‚**
- en: For instance, imagine you create an agent that learns to play Doom.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸‹ä½ åˆ›å»ºäº†ä¸€ä¸ªå­¦ä¹ ç©æ¯ç­çš„ä»£ç†ç¨‹åºã€‚
- en: '![Doom](../Images/5692612f8572824879d3b76eb2ec21b4.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![æ¯ç­](../Images/5692612f8572824879d3b76eb2ec21b4.png)'
- en: Doom is a large environment with a huge state space (millions of different states).
    Creating and updating a Q-table for that environment would not be efficient.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ç­æ˜¯ä¸€ä¸ªåºå¤§çš„ç¯å¢ƒï¼Œå…·æœ‰å·¨å¤§çš„çŠ¶æ€ç©ºé—´ï¼ˆæ•°ç™¾ä¸‡ä¸ªä¸åŒçš„çŠ¶æ€ï¼‰ã€‚ä¸ºè¯¥ç¯å¢ƒåˆ›å»ºå’Œæ›´æ–° Q è¡¨å°†ä¸é«˜æ•ˆã€‚
- en: Thatâ€™s why weâ€™ll study Deep Q-Learning in the next unit, an algorithm **where
    we use a neural network that approximates, given a state, the different Q-values
    for each action.**
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªå•å…ƒä¸­å­¦ä¹ æ·±åº¦ Q å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§ç®—æ³•ï¼Œ**æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ï¼Œç»™å®šä¸€ä¸ªçŠ¶æ€ï¼Œæ¯ä¸ªåŠ¨ä½œçš„ä¸åŒ Q å€¼ã€‚**
- en: '![Environments](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![ç¯å¢ƒ](../Images/bf441b005cda192d0dc86eb42475aeb3.png)'
- en: See you in Unit 3! ğŸ”¥
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸‰å•å…ƒè§ï¼ğŸ”¥
- en: Keep learning, stay awesome ğŸ¤—
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»§ç»­å­¦ä¹ ï¼Œä¿æŒæ£’æ£’çš„ ğŸ¤—
