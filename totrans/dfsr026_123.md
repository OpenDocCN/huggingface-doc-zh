# Transformer Temporal

> 原始文本：[https://huggingface.co/docs/diffusers/api/models/transformer_temporal](https://huggingface.co/docs/diffusers/api/models/transformer_temporal)

用于类似视频数据的Transformer模型。

## TransformerTemporalModel

### `class diffusers.models.TransformerTemporalModel`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_temporal.py#L41)

```py
( num_attention_heads: int = 16 attention_head_dim: int = 88 in_channels: Optional = None out_channels: Optional = None num_layers: int = 1 dropout: float = 0.0 norm_num_groups: int = 32 cross_attention_dim: Optional = None attention_bias: bool = False sample_size: Optional = None activation_fn: str = 'geglu' norm_elementwise_affine: bool = True double_self_attention: bool = True positional_embeddings: Optional = None num_positional_embeddings: Optional = None )
```

参数

+   `num_attention_heads`（`int`，*可选*，默认为16）— 用于多头注意力的头数。

+   `attention_head_dim`（`int`，*可选*，默认为88）— 每个头中的通道数。

+   `in_channels`（`int`，*可选*）— 输入和输出中的通道数（如果输入是**连续**，请指定）。

+   `num_layers`（`int`，*可选*，默认为1）— 要使用的Transformer块层数。

+   `dropout`（`float`，*可选*，默认为0.0）— 要使用的丢弃概率。

+   `cross_attention_dim`（`int`，*可选*）— 要使用的`encoder_hidden_states`维度数。

+   `attention_bias`（`bool`，*可选*）— 配置`TransformerBlock`注意力是否应包含偏置参数。

+   `sample_size` (`int`, *optional*) — 潜在图像的宽度（如果输入是**离散**，请指定）。在训练期间固定，因为它用于学习一些位置嵌入。

+   `activation_fn`（`str`，*可选*，默认为`"geglu"`）— 在前馈中使用的激活函数。查看`diffusers.models.activations.get_activation`以获取支持的激活函数。

+   `norm_elementwise_affine`（`bool`，*可选*）— 配置`TransformerBlock`是否应该使用可学习的逐元素仿射参数进行归一化。

+   `double_self_attention`（`bool`，*可选*）— 配置每个`TransformerBlock`是否应包含两个自注意力层。位置嵌入 —（`str`，*可选*）：在传递之前应用于序列输入的位置嵌入的类型。num_positional_embeddings —（`int`，*可选*）：要应用位置嵌入的序列的最大长度。

用于类似视频数据的Transformer模型。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_temporal.py#L121)

```py
( hidden_states: FloatTensor encoder_hidden_states: Optional = None timestep: Optional = None class_labels: LongTensor = None num_frames: int = 1 cross_attention_kwargs: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';~models.transformer_temporal.TransformerTemporalModelOutput or tuple
```

参数

+   `hidden_states`（如果是离散的，则形状为`(batch size, num latent pixels)`的`torch.LongTensor`，如果是连续的，则形状为`(batch size, channel, height, width)`的`torch.FloatTensor`）— 输入的隐藏状态。

+   `encoder_hidden_states`（形状为`(batch size, encoder_hidden_states dim)`的`torch.LongTensor`，*可选*）— 用于交叉注意力层的条件嵌入。如果未给出，则交叉注意力默认为自注意力。

+   `timestep`（`torch.LongTensor`，*可选*）— 用于指示去噪步骤。可选的时间步骤，将作为嵌入应用于`AdaLayerNorm`中。

+   `class_labels`（形状为`(batch size, num classes)`的`torch.LongTensor`，*可选*）— 用于指示类标签条件的类标签。可选的类标签，将作为嵌入应用于`AdaLayerZeroNorm`。

+   `num_frames`（`int`，*可选*，默认为1）— 要在每批次中处理的帧数。这用于重塑隐藏状态。

+   `cross_attention_kwargs`（`dict`，*可选*）— 如果指定，将传递给`AttentionProcessor`的kwargs字典，如在[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中的`self.processor`下定义的。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[UNet2DConditionOutput](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.models.unets.unet_2d_condition.UNet2DConditionOutput)而不是普通元组。

返回

`~models.transformer_temporal.TransformerTemporalModelOutput`或`tuple`

如果`return_dict`为True，则返回一个`~models.transformer_temporal.TransformerTemporalModelOutput`，否则返回一个元组，其中第一个元素是样本张量。

`TransformerTemporal`的前向方法。

## TransformerTemporalModelOutput

### `class diffusers.models.transformers.transformer_temporal.TransformerTemporalModelOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/transformer_temporal.py#L28)

```py
( sample: FloatTensor )
```

参数

+   `sample`（形状为`(batch_size x num_frames, num_channels, height, width)`的`torch.FloatTensor`）- 在`encoder_hidden_states`输入条件下的隐藏状态输出。

`TransformerTemporalModel`的输出。
