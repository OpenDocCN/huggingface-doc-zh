["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel low_res_scheduler: DDPMScheduler scheduler: KarrasDiffusionSchedulers safety_checker: Optional = None feature_extractor: Optional = None watermarker: Optional = None max_noise_level: int = 350 )\n```", "```py\n( prompt: Union = None image: Union = None num_inference_steps: int = 75 guidance_scale: float = 9.0 noise_level: int = 20 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None clip_skip: int = None ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> from io import BytesIO\n>>> from diffusers import StableDiffusionUpscalePipeline\n>>> import torch\n\n>>> # load model and scheduler\n>>> model_id = \"stabilityai/stable-diffusion-x4-upscaler\"\n>>> pipeline = StableDiffusionUpscalePipeline.from_pretrained(\n...     model_id, revision=\"fp16\", torch_dtype=torch.float16\n... )\n>>> pipeline = pipeline.to(\"cuda\")\n\n>>> # let's download an  image\n>>> url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\n>>> response = requests.get(url)\n>>> low_res_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n>>> low_res_img = low_res_img.resize((128, 128))\n>>> prompt = \"a white cat\"\n\n>>> upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n>>> upscaled_image.save(\"upsampled_cat.png\")\n```", "```py\n( slice_size: Union = 'auto' )\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n( )\n```", "```py\n( attention_op: Optional = None )\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]