# 文本分类

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/sequence_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/sequence_classification)

[https://www.youtube-nocookie.com/embed/leNG9fN9FQU](https://www.youtube-nocookie.com/embed/leNG9fN9FQU)

文本分类是一项常见的NLP任务，它为文本分配标签或类别。一些最大的公司在生产中运行文本分类，用于各种实际应用。文本分类中最流行的形式之一是情感分析，它为一系列文本分配标签如🙂积极，🙁消极或😐中性。

本指南将向您展示如何：

1.  在[IMDb](https://huggingface.co/datasets/imdb)数据集上对[DistilBERT](https://huggingface.co/distilbert-base-uncased)进行微调，以确定电影评论是积极的还是消极的。

1.  使用您的微调模型进行推理。

本教程中演示的任务由以下模型架构支持：

[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [BigBird-Pegasus](../model_doc/bigbird_pegasus), [BioGpt](../model_doc/biogpt), [BLOOM](../model_doc/bloom), [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [CodeLlama](../model_doc/code_llama), [ConvBERT](../model_doc/convbert), [CTRL](../model_doc/ctrl), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ErnieM](../model_doc/ernie_m), [ESM](../model_doc/esm), [Falcon](../model_doc/falcon), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode), [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [GPT-J](../model_doc/gptj), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3), [LED](../model_doc/led), [LiLT](../model_doc/lilt), [LLaMA](../model_doc/llama), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [MarkupLM](../model_doc/markuplm), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [Mistral](../model_doc/mistral), [Mixtral](../model_doc/mixtral), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt), [MRA](../model_doc/mra), [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha), [Nyströmformer](../model_doc/nystromformer), [OpenLlama](../model_doc/open-llama), [OpenAI GPT](../model_doc/openai-gpt), [OPT](../model_doc/opt), [Perceiver](../model_doc/perceiver), [Persimmon](../model_doc/persimmon), [Phi](../model_doc/phi), [PLBart](../model_doc/plbart), [QDQBert](../model_doc/qdqbert), [Qwen2](../model_doc/qwen2), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [T5](../model_doc/t5), [TAPAS](../model_doc/tapas), [Transformer-XL](../model_doc/transfo-xl), [UMT5](../model_doc/umt5), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)

在开始之前，请确保您已安装所有必要的库：

```py
pip install transformers datasets evaluate accelerate
```

我们鼓励您登录您的Hugging Face账户，这样您就可以上传和与社区分享您的模型。在提示时，输入您的令牌以登录：

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## 加载IMDb数据集

首先从🤗数据集库中加载IMDb数据集：

```py
>>> from datasets import load_dataset

>>> imdb = load_dataset("imdb")
```

然后看一个例子：

```py
>>> imdb["test"][0]
{
    "label": 0,
    "text": "I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.",
}
```

这个数据集中有两个字段：

+   `text`: 电影评论文本。

+   `label`: 一个值，要么是 `0` 表示负面评价，要么是 `1` 表示正面评价。

## 预处理

下一步是加载 DistilBERT 标记器来预处理 `text` 字段：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```

创建一个预处理函数来对 `text` 进行标记化，并截断序列，使其不超过 DistilBERT 的最大输入长度：

```py
>>> def preprocess_function(examples):
...     return tokenizer(examples["text"], truncation=True)
```

应用预处理函数到整个数据集，使用🤗 Datasets [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map) 函数。您可以通过设置 `batched=True` 来加速 `map`，以一次处理数据集的多个元素：

```py
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```

现在使用 [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding) 创建一个示例批次。在整理过程中，动态填充句子到批次中的最长长度比整个数据集填充到最大长度更有效。

Pytorch隐藏 Pytorch 内容

```py
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

TensorFlow隐藏 TensorFlow 内容

```py
>>> from transformers import DataCollatorWithPadding

>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

## 评估

在训练过程中包含一个度量通常有助于评估模型的性能。您可以使用 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) 库快速加载一个评估方法。对于这个任务，加载 [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) 度量（查看 🤗 Evaluate [快速导览](https://huggingface.co/docs/evaluate/a_quick_tour) 以了解如何加载和计算度量）：

```py
>>> import evaluate

>>> accuracy = evaluate.load("accuracy")
```

然后创建一个函数，将您的预测和标签传递给 `compute` 来计算准确率：

```py
>>> import numpy as np

>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

您的 `compute_metrics` 函数现在已经准备就绪，当您设置训练时会返回到它。

## 训练

在开始训练模型之前，使用 `id2label` 和 `label2id` 创建预期 id 到标签的映射：

```py
>>> id2label = {0: "NEGATIVE", 1: "POSITIVE"}
>>> label2id = {"NEGATIVE": 0, "POSITIVE": 1}
```

Pytorch隐藏 Pytorch 内容

如果您不熟悉如何使用 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 对模型进行微调，请查看这里的基本教程！

现在您已经准备好开始训练您的模型了！加载 DistilBERT 与 [AutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification) 以及预期标签的数量和标签映射：

```py
>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

>>> model = AutoModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

此时，只剩下三个步骤：

1.  在 [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments) 中定义您的训练超参数。唯一必需的参数是 `output_dir`，指定保存模型的位置。通过设置 `push_to_hub=True` 将此模型推送到 Hub（您需要登录 Hugging Face 以上传您的模型）。在每个 epoch 结束时，[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 将评估准确率并保存训练检查点。

1.  将训练参数传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)，以及模型、数据集、标记器、数据整理器和 `compute_metrics` 函数。

1.  调用 [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train) 来微调您的模型。

```py
>>> training_args = TrainingArguments(
...     output_dir="my_awesome_model",
...     learning_rate=2e-5,
...     per_device_train_batch_size=16,
...     per_device_eval_batch_size=16,
...     num_train_epochs=2,
...     weight_decay=0.01,
...     evaluation_strategy="epoch",
...     save_strategy="epoch",
...     load_best_model_at_end=True,
...     push_to_hub=True,
... )

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     train_dataset=tokenized_imdb["train"],
...     eval_dataset=tokenized_imdb["test"],
...     tokenizer=tokenizer,
...     data_collator=data_collator,
...     compute_metrics=compute_metrics,
... )

>>> trainer.train()
```

当您将 `tokenizer` 传递给 [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer) 时，默认情况下会应用动态填充。在这种情况下，您不需要显式指定数据整理器。

训练完成后，使用 [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub) 方法将您的模型分享到 Hub，这样每个人都可以使用您的模型：

```py
>>> trainer.push_to_hub()
```

TensorFlow隐藏 TensorFlow 内容

如果您不熟悉如何使用 Keras 对模型进行微调，请查看这里的基本教程！

要在TensorFlow中微调模型，请首先设置优化器函数、学习率调度和一些训练超参数：

```py
>>> from transformers import create_optimizer
>>> import tensorflow as tf

>>> batch_size = 16
>>> num_epochs = 5
>>> batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
>>> total_train_steps = int(batches_per_epoch * num_epochs)
>>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)
```

然后，您可以加载DistilBERT与[TFAutoModelForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification)，以及预期标签的数量和标签映射：

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id
... )
```

将您的数据集转换为`tf.data.Dataset`格式，使用[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)：

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_imdb["train"],
...     shuffle=True,
...     batch_size=16,
...     collate_fn=data_collator,
... )

>>> tf_validation_set = model.prepare_tf_dataset(
...     tokenized_imdb["test"],
...     shuffle=False,
...     batch_size=16,
...     collate_fn=data_collator,
... )
```

使用[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)为训练配置模型。请注意，Transformers模型都有一个默认的与任务相关的损失函数，因此除非您想要，否则不需要指定一个：

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer)  # No loss argument!
```

在开始训练之前设置的最后两件事是从预测中计算准确率，并提供一种将模型推送到Hub的方法。这两个都可以通过使用[Keras callbacks](../main_classes/keras_callbacks)来完成。

将您的`compute_metrics`函数传递给[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)：

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

指定在[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)中推送模型和分词器的位置：

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_model",
...     tokenizer=tokenizer,
... )
```

然后将您的回调捆绑在一起：

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

最后，您已经准备好开始训练您的模型了！使用您的训练和验证数据集、时代数和回调来微调模型调用[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)：

```py
>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)
```

训练完成后，您的模型会自动上传到Hub，以便每个人都可以使用它！

有关如何为文本分类微调模型的更深入示例，请查看相应的[PyTorch笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)或[TensorFlow笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)。

## 推理

很好，现在您已经对模型进行了微调，可以用它进行推理！

获取一些您想要进行推理的文本：

```py
>>> text = "This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."
```

尝试使用您微调的模型进行推理的最简单方法是在[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)中使用它。用您的模型实例化一个情感分析的`pipeline`，并将文本传递给它：

```py
>>> from transformers import pipeline

>>> classifier = pipeline("sentiment-analysis", model="stevhliu/my_awesome_model")
>>> classifier(text)
[{'label': 'POSITIVE', 'score': 0.9994940757751465}]
```

如果您愿意，也可以手动复制`pipeline`的结果：

Pytorch隐藏Pytorch内容

对文本进行标记化并返回PyTorch张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="pt")
```

将您的输入传递给模型并返回`logits`：

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

获取具有最高概率的类，并使用模型的`id2label`映射将其转换为文本标签：

```py
>>> predicted_class_id = logits.argmax().item()
>>> model.config.id2label[predicted_class_id]
'POSITIVE'
```

TensorFlow隐藏TensorFlow内容

对文本进行标记化并返回TensorFlow张量：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("stevhliu/my_awesome_model")
>>> inputs = tokenizer(text, return_tensors="tf")
```

将您的输入传递给模型并返回`logits`：

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
>>> logits = model(**inputs).logits
```

获取具有最高概率的类，并使用模型的`id2label`映射将其转换为文本标签：

```py
>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
>>> model.config.id2label[predicted_class_id]
'POSITIVE'
```
