# PEFT 配置和模型

> 原始文本：[`huggingface.co/docs/peft/tutorial/peft_model_config`](https://huggingface.co/docs/peft/tutorial/peft_model_config)

如今的大型预训练模型之所以庞大 - 通常具有数十亿个参数 - 是因为它们需要更多的存储空间和更多的计算能力来处理所有这些计算。您需要访问强大的 GPU 或 TPU 来训练这些大型预训练模型，这是昂贵的，不是每个人都能轻易获得的，不环保，也不太实用。PEFT 方法解决了许多这些挑战。有几种类型的 PEFT 方法（软提示、矩阵分解、适配器），但它们都专注于同一点，减少可训练参数的数量。这使得在消费者硬件上训练和存储大型模型更容易。

PEFT 库旨在帮助您快速在免费或低成本的 GPU 上训练大型模型，在本教程中，您将学习如何设置一个配置来应用 PEFT 方法到预训练基础模型进行训练。一旦 PEFT 配置设置完成，您可以使用任何您喜欢的训练框架（Transformer 的[Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，[Accelerate](https://hf.co/docs/accelerate)，自定义的 PyTorch 训练循环）。

## PEFT 配置

在各自的 API 参考页面中了解您可以为每种 PEFT 方法配置的参数。

配置存储了指定如何应用特定 PEFT 方法的重要参数。

例如，查看以下用于应用 LoRA 的[`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json)和用于应用 p-tuning 的[`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json)（这些配置文件已经是 JSON 序列化的）。每当加载 PEFT 适配器时，最好检查是否有一个必需的关联的 adapter_config.json 文件。

LoraConfigPromptEncoderConfig

```py
{
  "base_model_name_or_path": "facebook/opt-350m", #base model to apply LoRA to
  "bias": "none",
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "layers_pattern": null,
  "layers_to_transform": null,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "modules_to_save": null,
  "peft_type": "LORA", #PEFT method type
  "r": 16,
  "revision": null,
  "target_modules": [
    "q_proj", #model modules to apply LoRA to (query and value projection layers)
    "v_proj"
  ],
  "task_type": "CAUSAL_LM" #type of task to train model on
}
```

您可以通过初始化 LoraConfig 来为训练创建自己的配置。

```py
from peft import LoraConfig, TaskType

lora_config = LoraConfig(
    r=16,
    target_modules=["q_proj", "v_proj"],
    task_type=TaskType.CAUSAL_LM,
    lora_alpha=32,
    lora_dropout=0.05
)
```

## PEFT 模型

有了 PEFT 配置，您现在可以将其应用于任何预训练模型以创建 PeftModel。可以选择来自[Transformers](https://hf.co/docs/transformers)库的任何最先进的模型，自定义模型，甚至新的和不受支持的变压器架构。

在本教程中，加载一个基础的[facebook/opt-350m](https://huggingface.co/facebook/opt-350m)模型进行微调。

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
```

使用 get_peft_model()函数从基础 facebook/opt-350m 模型和您之前创建的`lora_config`创建 PeftModel。

```py
from peft import get_peft_model

lora_model = get_peft_model(model, lora_config)
lora_model.print_trainable_parameters()
"trainable params: 1,572,864 || all params: 332,769,280 || trainable%: 0.472659014678278"
```

现在您可以使用您喜欢的训练框架训练 PeftModel！训练完成后，您可以使用 save_pretrained()将模型保存到本地，或使用[push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)方法上传到 Hub。

```py
# save locally
lora_model.save_pretrained("your-name/opt-350m-lora")

# push to Hub
lora_model.push_to_hub("your-name/opt-350m-lora")
```

要加载用于推理的 PeftModel，您需要提供用于创建它的 PeftConfig 和它训练的基础模型。

```py
from peft import PeftModel, PeftConfig

config = PeftConfig.from_pretrained("ybelkada/opt-350m-lora")
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)
lora_model = PeftModel.from_pretrained(model, "ybelkada/opt-350m-lora")
```

默认情况下，PeftModel 设置为推理，但如果您想要进一步训练适配器，可以设置`is_trainable=True`。

```py
lora_model = PeftModel.from_pretrained(model, "ybelkada/opt-350m-lora", is_trainable=True)
```

PeftModel.from_pretrained()方法是加载 PeftModel 最灵活的方式，因为不管使用了什么模型框架（Transformers、timm、通用的 PyTorch 模型）都可以。其他类，如 AutoPeftModel，只是基本 PeftModel 的一个方便包装器，使得可以更容易地直接从 Hub 或本地加载 PEFT 模型，其中存储了 PEFT 权重。

```py
from peft import AutoPeftModelForCausalLM

lora_model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
```

查看 AutoPeftModel API 参考，了解更多关于 AutoPeftModel 类的信息。

## 下一步

通过适当的 PeftConfig，您可以将其应用于任何预训练模型，创建一个 PeftModel，并在免费提供的 GPU 上更快地训练大型强大模型！要了解更多关于 PEFT 配置和模型的信息，以下指南可能会有所帮助：

+   学习如何为不来自 Transformers 的模型配置 PEFT 方法，可以查看使用自定义模型指南。
