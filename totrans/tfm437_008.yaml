- en: Preprocess
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing](https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing](https://huggingface.co/docs/transformers/v4.37.2/en/preprocessing)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can train a model on a dataset, it needs to be preprocessed into
    the expected model input format. Whether your data is text, images, or audio,
    they need to be converted and assembled into batches of tensors. ğŸ¤— Transformers
    provides a set of preprocessing classes to help prepare your data for the model.
    In this tutorial, youâ€™ll learn that for:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦å°†å…¶é¢„å¤„ç†ä¸ºé¢„æœŸçš„æ¨¡å‹è¾“å…¥æ ¼å¼ã€‚æ— è®ºæ‚¨çš„æ•°æ®æ˜¯æ–‡æœ¬ã€å›¾åƒè¿˜æ˜¯éŸ³é¢‘ï¼Œéƒ½éœ€è¦å°†å…¶è½¬æ¢å¹¶ç»„è£…æˆå¼ é‡æ‰¹æ¬¡ã€‚ğŸ¤— Transformersæä¾›äº†ä¸€ç»„é¢„å¤„ç†ç±»æ¥å¸®åŠ©å‡†å¤‡æ•°æ®ä¾›æ¨¡å‹ä½¿ç”¨ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†äº†è§£åˆ°ï¼š
- en: Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence
    of tokens, create a numerical representation of the tokens, and assemble them
    into tensors.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬ï¼Œä½¿ç”¨[Tokenizer](./main_classes/tokenizer)å°†æ–‡æœ¬è½¬æ¢ä¸ºä¸€ç³»åˆ—æ ‡è®°ï¼Œåˆ›å»ºæ ‡è®°çš„æ•°å€¼è¡¨ç¤ºï¼Œå¹¶å°†å®ƒä»¬ç»„è£…æˆå¼ é‡ã€‚
- en: Speech and audio, use a [Feature extractor](./main_classes/feature_extractor)
    to extract sequential features from audio waveforms and convert them into tensors.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯­éŸ³å’ŒéŸ³é¢‘ï¼Œä½¿ç”¨[Feature extractor](./main_classes/feature_extractor)ä»éŸ³é¢‘æ³¢å½¢ä¸­æå–åºåˆ—ç‰¹å¾å¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: Image inputs use a [ImageProcessor](./main_classes/image) to convert images
    into tensors.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒè¾“å…¥ä½¿ç”¨[ImageProcessor](./main_classes/image)å°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
- en: Multimodal inputs, use a [Processor](./main_classes/processors) to combine a
    tokenizer and a feature extractor or image processor.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨[Processor](./main_classes/processors)æ¥ç»“åˆä¸€ä¸ªåˆ†è¯å™¨å’Œä¸€ä¸ªç‰¹å¾æå–å™¨æˆ–å›¾åƒå¤„ç†å™¨ã€‚
- en: '`AutoProcessor` **always** works and automatically chooses the correct class
    for the model youâ€™re using, whether youâ€™re using a tokenizer, image processor,
    feature extractor or processor.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`AutoProcessor` **æ€»æ˜¯**æœ‰æ•ˆï¼Œå¹¶è‡ªåŠ¨é€‰æ‹©æ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹çš„æ­£ç¡®ç±»åˆ«ï¼Œæ— è®ºæ‚¨æ˜¯ä½¿ç”¨åˆ†è¯å™¨ã€å›¾åƒå¤„ç†å™¨ã€ç‰¹å¾æå–å™¨è¿˜æ˜¯å¤„ç†å™¨ã€‚'
- en: 'Before you begin, install ğŸ¤— Datasets so you can load some datasets to experiment
    with:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·å®‰è£…ğŸ¤—æ•°æ®é›†ï¼Œä»¥ä¾¿åŠ è½½ä¸€äº›æ•°æ®é›†è¿›è¡Œå®éªŒï¼š
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Natural Language Processing
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å¤„ç†
- en: '[https://www.youtube-nocookie.com/embed/Yffk5aydLzg](https://www.youtube-nocookie.com/embed/Yffk5aydLzg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/Yffk5aydLzg](https://www.youtube-nocookie.com/embed/Yffk5aydLzg)'
- en: The main tool for preprocessing textual data is a [tokenizer](main_classes/tokenizer).
    A tokenizer splits text into *tokens* according to a set of rules. The tokens
    are converted into numbers and then tensors, which become the model inputs. Any
    additional inputs required by the model are added by the tokenizer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†æ–‡æœ¬æ•°æ®çš„ä¸»è¦å·¥å…·æ˜¯[tokenizer](main_classes/tokenizer)ã€‚åˆ†è¯å™¨æ ¹æ®ä¸€ç»„è§„åˆ™å°†æ–‡æœ¬åˆ†å‰²ä¸º*æ ‡è®°*ã€‚è¿™äº›æ ‡è®°è¢«è½¬æ¢ä¸ºæ•°å­—ï¼Œç„¶åæˆä¸ºæ¨¡å‹è¾“å…¥çš„å¼ é‡ã€‚åˆ†è¯å™¨ä¼šæ·»åŠ æ¨¡å‹æ‰€éœ€çš„ä»»ä½•é¢å¤–è¾“å…¥ã€‚
- en: If you plan on using a pretrained model, itâ€™s important to use the associated
    pretrained tokenizer. This ensures the text is split the same way as the pretraining
    corpus, and uses the same corresponding tokens-to-index (usually referred to as
    the *vocab*) during pretraining.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ‰“ç®—ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡è¦çš„æ˜¯ä½¿ç”¨ç›¸å…³çš„é¢„è®­ç»ƒåˆ†è¯å™¨ã€‚è¿™ç¡®ä¿æ–‡æœ¬è¢«åˆ†å‰²çš„æ–¹å¼ä¸é¢„è®­ç»ƒè¯­æ–™åº“ç›¸åŒï¼Œå¹¶ä¸”åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ç›¸åŒçš„å¯¹åº”æ ‡è®°ç´¢å¼•ï¼ˆé€šå¸¸ç§°ä¸º*è¯æ±‡è¡¨*ï¼‰ã€‚
- en: 'Get started by loading a pretrained tokenizer with the [AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)
    method. This downloads the *vocab* a model was pretrained with:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡[AutoTokenizer.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)æ–¹æ³•åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨æ¥å¼€å§‹ã€‚è¿™ä¼šä¸‹è½½æ¨¡å‹é¢„è®­ç»ƒæ—¶ä½¿ç”¨çš„*è¯æ±‡è¡¨*ï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then pass your text to the tokenizer:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™åˆ†è¯å™¨ï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The tokenizer returns a dictionary with three important items:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è¿”å›ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªé‡è¦é¡¹ç›®çš„å­—å…¸ï¼š
- en: '[input_ids](glossary#input-ids) are the indices corresponding to each token
    in the sentence.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[input_ids](glossary#input-ids)æ˜¯å¥å­ä¸­æ¯ä¸ªæ ‡è®°å¯¹åº”çš„ç´¢å¼•ã€‚'
- en: '[attention_mask](glossary#attention-mask) indicates whether a token should
    be attended to or not.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[attention_mask](glossary#attention-mask)æŒ‡ç¤ºä¸€ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥è¢«å…³æ³¨ã€‚'
- en: '[token_type_ids](glossary#token-type-ids) identifies which sequence a token
    belongs to when there is more than one sequence.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[token_type_ids](glossary#token-type-ids)æ ‡è¯†ä¸€ä¸ªæ ‡è®°å±äºå“ªä¸ªåºåˆ—ï¼Œå½“æœ‰å¤šä¸ªåºåˆ—æ—¶ã€‚'
- en: 'Return your input by decoding the `input_ids`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è§£ç `input_ids`è¿”å›æ‚¨çš„è¾“å…¥ï¼š
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier
    and separator) - to the sentence. Not all models need special tokens, but if they
    do, the tokenizer automatically adds them for you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œåˆ†è¯å™¨æ·»åŠ äº†ä¸¤ä¸ªç‰¹æ®Šæ ‡è®° - `CLS`å’Œ`SEP`ï¼ˆåˆ†ç±»å™¨å’Œåˆ†éš”ç¬¦ï¼‰- åˆ°å¥å­ä¸­ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Šæ ‡è®°ï¼Œä½†å¦‚æœéœ€è¦ï¼Œåˆ†è¯å™¨ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ·»åŠ å®ƒä»¬ã€‚
- en: 'If there are several sentences you want to preprocess, pass them as a list
    to the tokenizer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰å‡ ä¸ªå¥å­éœ€è¦é¢„å¤„ç†ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™åˆ†è¯å™¨ï¼š
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pad
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¡«å……
- en: Sentences arenâ€™t always the same length which can be an issue because tensors,
    the model inputs, need to have a uniform shape. Padding is a strategy for ensuring
    tensors are rectangular by adding a special *padding token* to shorter sentences.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¥å­é•¿åº¦ä¸æ€»æ˜¯ç›¸åŒï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºå¼ é‡ï¼Œå³æ¨¡å‹è¾“å…¥ï¼Œéœ€è¦å…·æœ‰ç»Ÿä¸€çš„å½¢çŠ¶ã€‚å¡«å……æ˜¯ä¸€ç§ç¡®ä¿å¼ é‡æ˜¯çŸ©å½¢çš„ç­–ç•¥ï¼Œé€šè¿‡å‘è¾ƒçŸ­çš„å¥å­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„*å¡«å……æ ‡è®°*ã€‚
- en: 'Set the `padding` parameter to `True` to pad the shorter sequences in the batch
    to match the longest sequence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`padding`å‚æ•°è®¾ç½®ä¸º`True`ï¼Œä»¥å°†æ‰¹æ¬¡ä¸­è¾ƒçŸ­çš„åºåˆ—å¡«å……åˆ°ä¸æœ€é•¿åºåˆ—ç›¸åŒ¹é…çš„é•¿åº¦ï¼š
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first and third sentences are now padded with `0`â€™s because they are shorter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥ç°åœ¨ç”¨`0`å¡«å……ï¼Œå› ä¸ºå®ƒä»¬è¾ƒçŸ­ã€‚
- en: Truncation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆªæ–­
- en: On the other end of the spectrum, sometimes a sequence may be too long for a
    model to handle. In this case, youâ€™ll need to truncate the sequence to a shorter
    length.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œæœ‰æ—¶ä¸€ä¸ªåºåˆ—å¯èƒ½å¤ªé•¿ï¼Œæ¨¡å‹æ— æ³•å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å°†åºåˆ—æˆªæ–­ä¸ºè¾ƒçŸ­çš„é•¿åº¦ã€‚
- en: 'Set the `truncation` parameter to `True` to truncate a sequence to the maximum
    length accepted by the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`truncation`å‚æ•°è®¾ç½®ä¸º`True`ï¼Œå°†åºåˆ—æˆªæ–­ä¸ºæ¨¡å‹æ¥å—çš„æœ€å¤§é•¿åº¦ï¼š
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Check out the [Padding and truncation](./pad_truncation) concept guide to learn
    more different padding and truncation arguments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[å¡«å……å’Œæˆªæ–­](./pad_truncation)æ¦‚å¿µæŒ‡å—ï¼Œäº†è§£æ›´å¤šä¸åŒçš„å¡«å……å’Œæˆªæ–­å‚æ•°ã€‚
- en: Build tensors
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ„å»ºå¼ é‡
- en: Finally, you want the tokenizer to return the actual tensors that get fed to
    the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å¸Œæœ›åˆ†è¯å™¨è¿”å›å®é™…é¦ˆé€åˆ°æ¨¡å‹çš„å¼ é‡ã€‚
- en: 'Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for
    TensorFlow:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`return_tensors`å‚æ•°è®¾ç½®ä¸º`pt`ä»¥ä¾›PyTorchä½¿ç”¨ï¼Œæˆ–è®¾ç½®ä¸º`tf`ä»¥ä¾›TensorFlowä½¿ç”¨ï¼š
- en: PytorchHide Pytorch content
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorchéšè—Pytorchå†…å®¹
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: TensorFlowHide TensorFlow content
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowéšè—TensorFlowå†…å®¹
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Different pipelines support tokenizer arguments in their `__call__()` differently.
    `text-2-text-generation` pipelines support (i.e. pass on) only `truncation`. `text-generation`
    pipelines support `max_length`, `truncation`, `padding` and `add_special_tokens`.
    In `fill-mask` pipelines, tokenizer arguments can be passed in the `tokenizer_kwargs`
    argument (dictionary).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„ç®¡é“ä»¥ä¸åŒçš„æ–¹å¼åœ¨å…¶`__call__()`ä¸­æ”¯æŒåˆ†è¯å™¨å‚æ•°ã€‚`text-2-text-generation`ç®¡é“ä»…æ”¯æŒï¼ˆå³ä¼ é€’ï¼‰`truncation`ã€‚`text-generation`ç®¡é“æ”¯æŒ`max_length`ã€`truncation`ã€`padding`å’Œ`add_special_tokens`ã€‚åœ¨`fill-mask`ç®¡é“ä¸­ï¼Œåˆ†è¯å™¨å‚æ•°å¯ä»¥åœ¨`tokenizer_kwargs`å‚æ•°ï¼ˆå­—å…¸ï¼‰ä¸­ä¼ é€’ã€‚
- en: Audio
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éŸ³é¢‘
- en: For audio tasks, youâ€™ll need a [feature extractor](main_classes/feature_extractor)
    to prepare your dataset for the model. The feature extractor is designed to extract
    features from raw audio data, and convert them into tensors.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œæ‚¨å°†éœ€è¦ä¸€ä¸ª[ç‰¹å¾æå–å™¨](main_classes/feature_extractor)æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚ç‰¹å¾æå–å™¨æ—¨åœ¨ä»åŸå§‹éŸ³é¢‘æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: 'Load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset
    (see the ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub)
    for more details on how to load a dataset) to see how you can use a feature extractor
    with audio datasets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)æ•°æ®é›†ï¼ˆæŸ¥çœ‹ğŸ¤—[Datasetsæ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰ä»¥æŸ¥çœ‹å¦‚ä½•åœ¨éŸ³é¢‘æ•°æ®é›†ä¸­ä½¿ç”¨ç‰¹å¾æå–å™¨ï¼š
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Access the first element of the `audio` column to take a look at the input.
    Calling the `audio` column automatically loads and resamples the audio file:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—®`audio`åˆ—çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä»¥æŸ¥çœ‹è¾“å…¥ã€‚è°ƒç”¨`audio`åˆ—ä¼šè‡ªåŠ¨åŠ è½½å’Œé‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This returns three items:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¿”å›ä¸‰ä¸ªé¡¹ç›®ï¼š
- en: '`array` is the speech signal loaded - and potentially resampled - as a 1D array.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`array`æ˜¯åŠ è½½çš„è¯­éŸ³ä¿¡å· - å¯èƒ½å·²é‡æ–°é‡‡æ · - ä½œä¸º1Dæ•°ç»„ã€‚'
- en: '`path` points to the location of the audio file.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`æŒ‡å‘éŸ³é¢‘æ–‡ä»¶çš„ä½ç½®ã€‚'
- en: '`sampling_rate` refers to how many data points in the speech signal are measured
    per second.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_rate`æŒ‡çš„æ˜¯æ¯ç§’æµ‹é‡çš„è¯­éŸ³ä¿¡å·ä¸­æœ‰å¤šå°‘æ•°æ®ç‚¹ã€‚'
- en: For this tutorial, youâ€™ll use the [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)
    model. Take a look at the model card, and youâ€™ll learn Wav2Vec2 is pretrained
    on 16kHz sampled speech audio. It is important your audio dataâ€™s sampling rate
    matches the sampling rate of the dataset used to pretrain the model. If your dataâ€™s
    sampling rate isnâ€™t the same, then you need to resample your data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†ä½¿ç”¨[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)æ¨¡å‹ã€‚æŸ¥çœ‹æ¨¡å‹å¡ç‰‡ï¼Œæ‚¨å°†äº†è§£åˆ°Wav2Vec2æ˜¯åœ¨16kHzé‡‡æ ·çš„è¯­éŸ³éŸ³é¢‘ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚é‡è¦çš„æ˜¯ï¼Œæ‚¨çš„éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡è¦ä¸ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡åŒ¹é…ã€‚å¦‚æœæ‚¨çš„æ•°æ®é‡‡æ ·ç‡ä¸åŒï¼Œåˆ™éœ€è¦å¯¹æ•°æ®è¿›è¡Œé‡æ–°é‡‡æ ·ã€‚
- en: 'Use ğŸ¤— Datasetsâ€™ [cast_column](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.cast_column)
    method to upsample the sampling rate to 16kHz:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Datasetsçš„[cast_column](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.cast_column)æ–¹æ³•å°†é‡‡æ ·ç‡ä¸Šé‡‡æ ·è‡³16kHzï¼š
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Call the `audio` column again to resample the audio file:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å†æ¬¡è°ƒç”¨`audio`åˆ—ä»¥é‡æ–°é‡‡æ ·éŸ³é¢‘æ–‡ä»¶ï¼š
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, load a feature extractor to normalize and pad the input. When padding
    textual data, a `0` is added for shorter sequences. The same idea applies to audio
    data. The feature extractor adds a `0` - interpreted as silence - to `array`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ªç‰¹å¾æå–å™¨æ¥å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–å’Œå¡«å……ã€‚åœ¨å¡«å……æ–‡æœ¬æ•°æ®æ—¶ï¼Œä¼šä¸ºè¾ƒçŸ­çš„åºåˆ—æ·»åŠ `0`ã€‚ç›¸åŒçš„æ€æƒ³ä¹Ÿé€‚ç”¨äºéŸ³é¢‘æ•°æ®ã€‚ç‰¹å¾æå–å™¨ä¼šå‘`array`ä¸­æ·»åŠ ä¸€ä¸ª`0`
    - è¢«è§£é‡Šä¸ºé™éŸ³ã€‚
- en: 'Load the feature extractor with [AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoFeatureExtractor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained)åŠ è½½ç‰¹å¾æå–å™¨ï¼š
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Pass the audio `array` to the feature extractor. We also recommend adding the
    `sampling_rate` argument in the feature extractor in order to better debug any
    silent errors that may occur.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å°†éŸ³é¢‘`array`ä¼ é€’ç»™ç‰¹å¾æå–å™¨ã€‚æˆ‘ä»¬è¿˜å»ºè®®åœ¨ç‰¹å¾æå–å™¨ä¸­æ·»åŠ `sampling_rate`å‚æ•°ï¼Œä»¥æ›´å¥½åœ°è°ƒè¯•å¯èƒ½å‘ç”Ÿçš„ä»»ä½•é™é»˜é”™è¯¯ã€‚
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Just like the tokenizer, you can apply padding or truncation to handle variable
    sequences in a batch. Take a look at the sequence length of these two audio samples:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åˆ†è¯å™¨ä¸€æ ·ï¼Œæ‚¨å¯ä»¥åº”ç”¨å¡«å……æˆ–æˆªæ–­æ¥å¤„ç†æ‰¹å¤„ç†ä¸­çš„å¯å˜åºåˆ—ã€‚æŸ¥çœ‹è¿™ä¸¤ä¸ªéŸ³é¢‘æ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼š
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create a function to preprocess the dataset so the audio samples are the same
    lengths. Specify a maximum sample length, and the feature extractor will either
    pad or truncate the sequences to match it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥é¢„å¤„ç†æ•°æ®é›†ï¼Œä½¿éŸ³é¢‘æ ·æœ¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚æŒ‡å®šæœ€å¤§æ ·æœ¬é•¿åº¦ï¼Œç‰¹å¾æå–å™¨å°†å¡«å……æˆ–æˆªæ–­åºåˆ—ä»¥åŒ¹é…å®ƒï¼š
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Apply the `preprocess_function` to the first few examples in the dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•°æ®é›†ä¸­çš„å‰å‡ ä¸ªç¤ºä¾‹åº”ç”¨`preprocess_function`ï¼š
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The sample lengths are now the same and match the specified maximum length.
    You can pass your processed dataset to the model now!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ ·æœ¬é•¿åº¦ç›¸åŒå¹¶ä¸æŒ‡å®šçš„æœ€å¤§é•¿åº¦åŒ¹é…ã€‚ç°åœ¨å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Computer vision
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¡ç®—æœºè§†è§‰
- en: For computer vision tasks, youâ€™ll need an [image processor](main_classes/image_processor)
    to prepare your dataset for the model. Image preprocessing consists of several
    steps that convert images into the input expected by the model. These steps include
    but are not limited to resizing, normalizing, color channel correction, and converting
    images to tensors.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ‚¨å°†éœ€è¦ä¸€ä¸ª[å›¾åƒå¤„ç†å™¨](main_classes/image_processor)æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†ä»¥ä¾›æ¨¡å‹ä½¿ç”¨ã€‚å›¾åƒé¢„å¤„ç†åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„è¾“å…¥ã€‚è¿™äº›æ­¥éª¤åŒ…æ‹¬ä½†ä¸é™äºè°ƒæ•´å¤§å°ã€å½’ä¸€åŒ–ã€é¢œè‰²é€šé“æ ¡æ­£ä»¥åŠå°†å›¾åƒè½¬æ¢ä¸ºå¼ é‡ã€‚
- en: 'Image preprocessing often follows some form of image augmentation. Both image
    preprocessing and image augmentation transform image data, but they serve different
    purposes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒé¢„å¤„ç†é€šå¸¸éµå¾ªæŸç§å½¢å¼çš„å›¾åƒå¢å¼ºã€‚å›¾åƒé¢„å¤„ç†å’Œå›¾åƒå¢å¼ºéƒ½ä¼šè½¬æ¢å›¾åƒæ•°æ®ï¼Œä½†å®ƒä»¬æœ‰ä¸åŒçš„ç›®çš„ï¼š
- en: Image augmentation alters images in a way that can help prevent overfitting
    and increase the robustness of the model. You can get creative in how you augment
    your data - adjust brightness and colors, crop, rotate, resize, zoom, etc. However,
    be mindful not to change the meaning of the images with your augmentations.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒå¢å¼ºä»¥ä¸€ç§å¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶å¢åŠ æ¨¡å‹é²æ£’æ€§çš„æ–¹å¼æ”¹å˜å›¾åƒã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®å¢å¼ºä¸­å‘æŒ¥åˆ›é€ åŠ› - è°ƒæ•´äº®åº¦å’Œé¢œè‰²ï¼Œè£å‰ªï¼Œæ—‹è½¬ï¼Œè°ƒæ•´å¤§å°ï¼Œç¼©æ”¾ç­‰ã€‚ä½†æ˜¯ï¼Œè¯·æ³¨æ„ä¸è¦é€šè¿‡å¢å¼ºæ”¹å˜å›¾åƒçš„å«ä¹‰ã€‚
- en: Image preprocessing guarantees that the images match the modelâ€™s expected input
    format. When fine-tuning a computer vision model, images must be preprocessed
    exactly as when the model was initially trained.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒé¢„å¤„ç†ç¡®ä¿å›¾åƒä¸æ¨¡å‹æœŸæœ›çš„è¾“å…¥æ ¼å¼åŒ¹é…ã€‚åœ¨å¾®è°ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹æ—¶ï¼Œå›¾åƒå¿…é¡»ä¸æ¨¡å‹æœ€åˆè®­ç»ƒæ—¶çš„é¢„å¤„ç†æ–¹å¼å®Œå…¨ç›¸åŒã€‚
- en: You can use any library you like for image augmentation. For image preprocessing,
    use the `ImageProcessor` associated with the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„åº“è¿›è¡Œå›¾åƒå¢å¼ºã€‚å¯¹äºå›¾åƒé¢„å¤„ç†ï¼Œè¯·ä½¿ç”¨ä¸æ¨¡å‹å…³è”çš„`ImageProcessor`ã€‚
- en: 'Load the [food101](https://huggingface.co/datasets/food101) dataset (see the
    ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more
    details on how to load a dataset) to see how you can use an image processor with
    computer vision datasets:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½[food101](https://huggingface.co/datasets/food101)æ•°æ®é›†ï¼ˆè¯·å‚é˜…ğŸ¤—[æ•°æ®é›†æ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰ï¼Œä»¥æŸ¥çœ‹å¦‚ä½•åœ¨è®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸­ä½¿ç”¨å›¾åƒå¤„ç†å™¨ï¼š
- en: Use ğŸ¤— Datasets `split` parameter to only load a small sample from the training
    split since the dataset is quite large!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤—æ•°æ®é›†`split`å‚æ•°ä»…åŠ è½½è®­ç»ƒé›†ä¸­çš„ä¸€å°éƒ¨åˆ†æ ·æœ¬ï¼Œå› ä¸ºæ•°æ®é›†éå¸¸å¤§ï¼
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, take a look at the image with ğŸ¤— Datasets [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)
    feature:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œçœ‹ä¸€ä¸‹å¸¦æœ‰ğŸ¤—æ•°æ®é›†[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)ç‰¹å¾çš„å›¾åƒï¼š
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/63a998c1115de762652ef3c59b938a3f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a998c1115de762652ef3c59b938a3f.png)'
- en: 'Load the image processor with [AutoImageProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoImageProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained)åŠ è½½å›¾åƒå¤„ç†å™¨ï¼š
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: First, letâ€™s add some image augmentation. You can use any library you prefer,
    but in this tutorial, weâ€™ll use torchvisionâ€™s [`transforms`](https://pytorch.org/vision/stable/transforms.html)
    module. If youâ€™re interested in using another data augmentation library, learn
    how in the [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)
    or [Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ·»åŠ ä¸€äº›å›¾åƒå¢å¼ºã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„åº“ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨torchvisionçš„[`transforms`](https://pytorch.org/vision/stable/transforms.html)æ¨¡å—ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å…¶ä»–æ•°æ®å¢å¼ºåº“ï¼Œè¯·åœ¨[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)æˆ–[Kornia
    notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)ä¸­å­¦ä¹ å¦‚ä½•ä½¿ç”¨ã€‚
- en: Here we use [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)
    to chain together a couple of transforms - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)
    and [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html).
    Note that for resizing, we can get the image size requirements from the `image_processor`.
    For some models, an exact height and width are expected, for others only the `shortest_edge`
    is defined.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)æ¥é“¾æ¥ä¸€äº›è½¬æ¢
    - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)å’Œ[`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)ã€‚è¯·æ³¨æ„ï¼Œå¯¹äºè°ƒæ•´å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥ä»`image_processor`è·å–å›¾åƒå¤§å°è¦æ±‚ã€‚å¯¹äºæŸäº›æ¨¡å‹ï¼ŒæœŸæœ›ç²¾ç¡®çš„é«˜åº¦å’Œå®½åº¦ï¼Œå¯¹äºå…¶ä»–æ¨¡å‹åªå®šä¹‰äº†`shortest_edge`ã€‚
- en: '[PRE22]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The model accepts [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)
    as its input. `ImageProcessor` can take care of normalizing the images, and generating
    appropriate tensors. Create a function that combines image augmentation and image
    preprocessing for a batch of images and generates `pixel_values`:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¥å—[`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values)ä½œä¸ºå…¶è¾“å…¥ã€‚`ImageProcessor`å¯ä»¥è´Ÿè´£å½’ä¸€åŒ–å›¾åƒï¼Œå¹¶ç”Ÿæˆé€‚å½“çš„å¼ é‡ã€‚åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å›¾åƒå¢å¼ºå’Œå›¾åƒé¢„å¤„ç†ç»„åˆä¸ºä¸€æ‰¹å›¾åƒï¼Œå¹¶ç”Ÿæˆ`pixel_values`ï¼š
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the example above we set `do_resize=False` because we have already resized
    the images in the image augmentation transformation, and leveraged the `size`
    attribute from the appropriate `image_processor`. If you do not resize images
    during image augmentation, leave this parameter out. By default, `ImageProcessor`
    will handle the resizing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®äº†`do_resize=False`ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨å›¾åƒå¢å¼ºè½¬æ¢ä¸­è°ƒæ•´äº†å›¾åƒçš„å¤§å°ï¼Œå¹¶åˆ©ç”¨äº†é€‚å½“çš„`image_processor`çš„`size`å±æ€§ã€‚å¦‚æœæ‚¨åœ¨å›¾åƒå¢å¼ºæœŸé—´ä¸è°ƒæ•´å›¾åƒå¤§å°ï¼Œè¯·çœç•¥æ­¤å‚æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`ImageProcessor`å°†å¤„ç†è°ƒæ•´å¤§å°ã€‚
- en: If you wish to normalize images as a part of the augmentation transformation,
    use the `image_processor.image_mean`, and `image_processor.image_std` values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¸Œæœ›å°†å›¾åƒå½’ä¸€åŒ–ä½œä¸ºå¢å¼ºè½¬æ¢çš„ä¸€éƒ¨åˆ†ï¼Œè¯·ä½¿ç”¨`image_processor.image_mean`å’Œ`image_processor.image_std`å€¼ã€‚
- en: 'Then use ğŸ¤— Datasets[set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)
    to apply the transforms on the fly:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åä½¿ç”¨ğŸ¤—æ•°æ®é›†[set_transform](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.set_transform)æ¥åŠ¨æ€åº”ç”¨è½¬æ¢ï¼š
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now when you access the image, youâ€™ll notice the image processor has added `pixel_values`.
    You can pass your processed dataset to the model now!
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“æ‚¨è®¿é—®å›¾åƒæ—¶ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°å›¾åƒå¤„ç†å™¨å·²æ·»åŠ äº†`pixel_values`ã€‚ç°åœ¨æ‚¨å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here is what the image looks like after the transforms are applied. The image
    has been randomly cropped and itâ€™s color properties are different.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åº”ç”¨è½¬æ¢åçš„å›¾åƒæ ·å­ã€‚å›¾åƒå·²è¢«éšæœºè£å‰ªï¼Œå…¶é¢œè‰²å±æ€§ä¸åŒã€‚
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/eee368b92fc814b0667081ba147249f4.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eee368b92fc814b0667081ba147249f4.png)'
- en: For tasks like object detection, semantic segmentation, instance segmentation,
    and panoptic segmentation, `ImageProcessor` offers post processing methods. These
    methods convert modelâ€™s raw outputs into meaningful predictions such as bounding
    boxes, or segmentation maps.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ç­‰ä»»åŠ¡ï¼Œ`ImageProcessor`æä¾›åå¤„ç†æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰æ„ä¹‰çš„é¢„æµ‹ï¼Œå¦‚è¾¹ç•Œæ¡†æˆ–åˆ†å‰²åœ°å›¾ã€‚
- en: Pad
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¡«å……
- en: In some cases, for instance, when fine-tuning [DETR](./model_doc/detr), the
    model applies scale augmentation at training time. This may cause images to be
    different sizes in a batch. You can use `DetrImageProcessor.pad()` from [DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)
    and define a custom `collate_fn` to batch images together.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨å¾®è°ƒ[DETR](./model_doc/detr)æ—¶ï¼Œæ¨¡å‹ä¼šåœ¨è®­ç»ƒæ—¶åº”ç”¨å°ºåº¦å¢å¼ºã€‚è¿™å¯èƒ½å¯¼è‡´æ‰¹å¤„ç†ä¸­çš„å›¾åƒå¤§å°ä¸åŒã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ¥è‡ª[DetrImageProcessor](/docs/transformers/v4.37.2/en/model_doc/detr#transformers.DetrImageProcessor)çš„`DetrImageProcessor.pad()`ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„`collate_fn`æ¥å°†å›¾åƒæ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Multimodal
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šæ¨¡æ€
- en: For tasks involving multimodal inputs, youâ€™ll need a [processor](main_classes/processors)
    to prepare your dataset for the model. A processor couples together two processing
    objects such as as tokenizer and feature extractor.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¶‰åŠå¤šæ¨¡æ€è¾“å…¥çš„ä»»åŠ¡ï¼Œæ‚¨å°†éœ€è¦ä¸€ä¸ª[å¤„ç†å™¨](ä¸»è¦ç±»/å¤„ç†å™¨)æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ‚¨çš„æ•°æ®é›†ã€‚å¤„ç†å™¨å°†ä¸¤ä¸ªå¤„ç†å¯¹è±¡ï¼ˆå¦‚æ ‡è®°å™¨å’Œç‰¹å¾æå–å™¨ï¼‰è€¦åˆåœ¨ä¸€èµ·ã€‚
- en: 'Load the [LJ Speech](https://huggingface.co/datasets/lj_speech) dataset (see
    the ğŸ¤— [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more
    details on how to load a dataset) to see how you can use a processor for automatic
    speech recognition (ASR):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½[LJ Speech](https://huggingface.co/datasets/lj_speech)æ•°æ®é›†ï¼ˆæŸ¥çœ‹ğŸ¤—[æ•°æ®é›†æ•™ç¨‹](https://huggingface.co/docs/datasets/load_hub)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰ï¼Œä»¥æŸ¥çœ‹å¦‚ä½•ä½¿ç”¨å¤„ç†å™¨è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼š
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For ASR, youâ€™re mainly focused on `audio` and `text` so you can remove the
    other columns:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºASRï¼Œæ‚¨ä¸»è¦å…³æ³¨`éŸ³é¢‘`å’Œ`æ–‡æœ¬`ï¼Œå› æ­¤å¯ä»¥åˆ é™¤å…¶ä»–åˆ—ï¼š
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now take a look at the `audio` and `text` columns:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨çœ‹ä¸€ä¸‹`éŸ³é¢‘`å’Œ`æ–‡æœ¬`åˆ—ï¼š
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Remember you should always [resample](preprocessing#audio) your audio datasetâ€™s
    sampling rate to match the sampling rate of the dataset used to pretrain a model!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œä½ åº”è¯¥å§‹ç»ˆ[é‡æ–°é‡‡æ ·](é¢„å¤„ç†#éŸ³é¢‘)ä½ çš„éŸ³é¢‘æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼Œä»¥åŒ¹é…ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†çš„é‡‡æ ·ç‡ï¼
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Load a processor with [AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[AutoProcessor.from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor.from_pretrained)åŠ è½½ä¸€ä¸ªå¤„ç†å™¨ï¼š
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a function to process the audio data contained in `array` to `input_values`,
    and tokenize `text` to `labels`. These are the inputs to the model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¤„ç†`array`ä¸­åŒ…å«çš„éŸ³é¢‘æ•°æ®ä¸º`input_values`ï¼Œå¹¶å°†`æ–‡æœ¬`æ ‡è®°åŒ–ä¸º`æ ‡ç­¾`ã€‚è¿™äº›æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼š
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Apply the `prepare_dataset` function to a sample:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†`prepare_dataset`å‡½æ•°åº”ç”¨åˆ°ä¸€ä¸ªæ ·æœ¬ä¸­ï¼š
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The processor has now added `input_values` and `labels`, and the sampling rate
    has also been correctly downsampled to 16kHz. You can pass your processed dataset
    to the model now!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å™¨ç°åœ¨å·²ç»æ·»åŠ äº†`input_values`å’Œ`labels`ï¼Œé‡‡æ ·ç‡ä¹Ÿå·²ç»æ­£ç¡®é™é‡‡æ ·åˆ°16kHzã€‚ç°åœ¨æ‚¨å¯ä»¥å°†å¤„ç†è¿‡çš„æ•°æ®é›†ä¼ é€’ç»™æ¨¡å‹äº†ï¼
