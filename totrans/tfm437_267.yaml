- en: EfficientFormer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/efficientformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/efficientformer)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The EfficientFormer model was proposed in [EfficientFormer: Vision Transformers
    at MobileNet Speed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan,
    Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.
    EfficientFormer proposes a dimension-consistent pure transformer that can be run
    on mobile devices for dense prediction tasks like image classification, object
    detection and semantic segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vision Transformers (ViT) have shown rapid progress in computer vision tasks,
    achieving promising results on various benchmarks. However, due to the massive
    number of parameters and model design, e.g., attention mechanism, ViT-based models
    are generally times slower than lightweight convolutional networks. Therefore,
    the deployment of ViT for real-time applications is particularly challenging,
    especially on resource-constrained hardware such as mobile devices. Recent efforts
    try to reduce the computation complexity of ViT through network architecture search
    or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory.
    This leads to an important question: can transformers run as fast as MobileNet
    while obtaining high performance? To answer this, we first revisit the network
    architecture and operators used in ViT-based models and identify inefficient designs.
    Then we introduce a dimension-consistent pure transformer (without MobileNet blocks)
    as a design paradigm. Finally, we perform latency-driven slimming to get a series
    of final models dubbed EfficientFormer. Extensive experiments show the superiority
    of EfficientFormer in performance and speed on mobile devices. Our fastest model,
    EfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6
    ms inference latency on iPhone 12 (compiled with CoreML), which { runs as fast
    as MobileNetV2×1.4 (1.6 ms, 74.7% top-1),} and our largest model, EfficientFormer-L7,
    obtains 83.3% accuracy with only 7.0 ms latency. Our work proves that properly
    designed transformers can reach extremely low latency on mobile devices while
    maintaining high performance.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [novice03](https://huggingface.co/novice03) and
    [Bearnardd](https://huggingface.co/Bearnardd). The original code can be found
    [here](https://github.com/snap-research/EfficientFormer). The TensorFlow version
    of this model was added by [D-Roberts](https://huggingface.co/D-Roberts).
  prefs: []
  type: TYPE_NORMAL
- en: Documentation resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Image classification task guide](../tasks/image_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientFormerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EfficientFormerConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/configuration_efficientformer.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`depths` (`List(int)`, *optional*, defaults to `[3, 2, 6, 4]`) — Depth of each
    stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_sizes` (`List(int)`, *optional*, defaults to `[48, 96, 224, 448]`)
    — Dimensionality of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`downsamples` (`List(bool)`, *optional*, defaults to `[True, True, True, True]`)
    — Whether or not to downsample inputs between two stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dim` (`int`, *optional*, defaults to 448) — Number of channels in Meta3D layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`key_dim` (`int`, *optional*, defaults to 32) — The size of the key in meta3D
    block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_ratio` (`int`, *optional*, defaults to 4) — Ratio of the dimension
    of the query and value to the dimension of the key in MSHA block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resolution` (`int`, *optional*, defaults to 7) — Size of each patch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 5) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the 3D MetaBlock.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_expansion_ratio` (`int`, *optional*, defaults to 4) — Ratio of size of
    the hidden dimensionality of an MLP to the dimensionality of its input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings and encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pool_size` (`int`, *optional*, defaults to 3) — Kernel size of pooling layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`downsample_patch_size` (`int`, *optional*, defaults to 3) — The size of patches
    in downsampling layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`downsample_stride` (`int`, *optional*, defaults to 2) — The stride of convolution
    kernels in downsampling layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`downsample_pad` (`int`, *optional*, defaults to 1) — Padding in downsampling
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`drop_path_rate` (`int`, *optional*, defaults to 0) — Rate at which to increase
    dropout probability in DropPath.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_meta3d_blocks` (`int`, *optional*, defaults to 1) — The number of 3D MetaBlocks
    in the last stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distillation` (`bool`, *optional*, defaults to `True`) — Whether to add a
    distillation head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_layer_scale` (`bool`, *optional*, defaults to `True`) — Whether to scale
    outputs from token mixers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_scale_init_value` (`float`, *optional*, defaults to 1e-5) — Factor by
    which outputs from token mixers are scaled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to `224`) — The size (resolution)
    of each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [EfficientFormerModel](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerModel).
    It is used to instantiate an EfficientFormer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the EfficientFormer
    [snap-research/efficientformer-l1](https://huggingface.co/snap-research/efficientformer-l1)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: EfficientFormerImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EfficientFormerImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/image_processing_efficientformer.py#L45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `(size["height"], size["width"])`.
    Can be overridden by the `do_resize` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`dict`, *optional*, defaults to `{"height" -- 224, "width": 224}`):
    Size of the output image after resizing. Can be overridden by the `size` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image to the specified `crop_size`. Can be overridden by `do_center_crop`
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]` *optional*, defaults to 224) — Size of the output
    image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in the `preprocess` method. do_normalize — Whether to normalize the
    image. Can be overridden by the `do_normalize` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a EfficientFormer image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/image_processing_efficientformer.py#L160)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Dictionary
    in the format `{"height": h, "width": w}` specifying the size of the output image
    after resizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling` filter, *optional*, defaults to `self.resample`)
    — `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`.
    Only has an effect if `do_resize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to use if `do_normalize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to use if `do_normalize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the channel dimension format of the input image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: EfficientFormerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EfficientFormerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L553)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare EfficientFormer Model transformer outputting raw hidden-states without
    any specific head on top. This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L569)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor).
    See [ViTImageProcessor.preprocess()](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor.preprocess)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [EfficientFormerModel](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: EfficientFormerForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EfficientFormerForImageClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L612)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientFormer Model transformer with an image classification head on top (a
    linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L634)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor).
    See [ViTImageProcessor.preprocess()](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor.preprocess)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the image classification/regression loss. Indices should be in `[0,
    ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is
    computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [EfficientFormerForImageClassification](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: EfficientFormerForImageClassificationWithTeacher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.EfficientFormerForImageClassificationWithTeacher`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L734)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientFormer Model transformer with image classification heads on top (a
    linear layer on top of the final hidden state of the [CLS] token and a linear
    layer on top of the final hidden state of the distillation token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model supports inference-only. Fine-tuning with distillation (i.e. with
    a teacher) is not yet supported.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_efficientformer.py#L766)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [ViTImageProcessor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor).
    See [ViTImageProcessor.preprocess()](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTImageProcessor.preprocess)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.efficientformer.modeling_efficientformer.EfficientFormerForImageClassificationWithTeacherOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.efficientformer.modeling_efficientformer.EfficientFormerForImageClassificationWithTeacherOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Prediction scores as the average of the cls_logits and distillation logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Prediction scores of the classification head (i.e. the linear layer on top of
    the final hidden state of the class token).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distillation_logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Prediction scores of the distillation head (i.e. the linear layer on top of
    the final hidden state of the distillation token).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [EfficientFormerForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFEfficientFormerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFEfficientFormerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L943)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare EfficientFormer Model transformer outputting raw hidden-states without
    any specific head on top. This model is a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer).
    Use it as a regular TensorFlow Module and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L953)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` ((`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [EfficientFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFEfficientFormerModel](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.TFEfficientFormerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: TFEfficientFormerForImageClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFEfficientFormerForImageClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L988)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientFormer Model transformer with an image classification head on top of
    pooled last hidden state, e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer).
    Use it as a regular TensorFlow Module and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1010)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` ((`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [EfficientFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — Labels for computing
    the image classification/regression loss. Indices should be in `[0, ..., config.num_labels
    - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square
    loss), If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.modeling_tf_outputs.TFImageClassifierOutput` or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.modeling_tf_outputs.TFImageClassifierOutput` or a tuple of `tf.Tensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings, if the model has an embedding layer, + one for
    the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states (also called feature maps) of the model at the output of each stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, patch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFEfficientFormerForImageClassification](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: TFEfficientFormerForImageClassificationWithTeacher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFEfficientFormerForImageClassificationWithTeacher`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientFormer Model transformer with image classification heads on top (a
    linear layer on top of the final hidden state and a linear layer on top of the
    final hidden state of the distillation token) e.g. for ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: '.. warning:: This model supports inference-only. Fine-tuning with distillation
    (i.e. with a teacher) is not yet supported.'
  prefs: []
  type: TYPE_NORMAL
- en: This model is a TensorFlow [tf.keras.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer).
    Use it as a regular TensorFlow Module and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/efficientformer/modeling_tf_efficientformer.py#L1132)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` ((`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [EfficientFormerImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.efficientformer.modeling_tf_efficientformer.TFEfficientFormerForImageClassificationWithTeacherOutput`
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.efficientformer.modeling_tf_efficientformer.TFEfficientFormerForImageClassificationWithTeacherOutput`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([EfficientFormerConfig](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The [TFEfficientFormerForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassificationWithTeacher)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '`Output` type of [EfficientFormerForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher).
    logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Prediction scores
    as the average of the cls_logits and distillation logits. cls_logits (`tf.Tensor`
    of shape `(batch_size, config.num_labels)`) — Prediction scores of the classification
    head (i.e. the linear layer on top of the final hidden state of the class token).
    distillation_logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`) —
    Prediction scores of the distillation head (i.e. the linear layer on top of the
    final hidden state of the distillation token). hidden_states (`tuple(tf.Tensor)`,
    *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states
    of the model at the output of each layer plus the initial embedding outputs. attentions
    (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed
    or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for each
    layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`. Attentions
    weights after the attention softmax, used to compute the weighted average in the
    self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
