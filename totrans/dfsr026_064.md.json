["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/instruct_pix2pix\npip install -r requirements.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_instruct_pix2pix.py \\\n  --resolution=512 \\\n```", "```py\nin_channels = 8\nout_channels = unet.conv_in.out_channels\nunet.register_to_config(in_channels=in_channels)\n\nwith torch.no_grad():\n    new_conv_in = nn.Conv2d(\n        in_channels, out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding\n    )\n    new_conv_in.weight.zero_()\n    new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)\n    unet.conv_in = new_conv_in\n```", "```py\noptimizer = optimizer_cls(\n    unet.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```", "```py\ndef preprocess_train(examples):\n    preprocessed_images = preprocess_images(examples)\n\n    original_images, edited_images = preprocessed_images.chunk(2)\n    original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)\n    edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)\n\n    examples[\"original_pixel_values\"] = original_images\n    examples[\"edited_pixel_values\"] = edited_images\n\n    captions = list(examples[edit_prompt_column])\n    examples[\"input_ids\"] = tokenize_captions(captions)\n    return examples\n```", "```py\nlatents = vae.encode(batch[\"edited_pixel_values\"].to(weight_dtype)).latent_dist.sample()\nlatents = latents * vae.config.scaling_factor\n```", "```py\nencoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\noriginal_image_embeds = vae.encode(batch[\"original_pixel_values\"].to(weight_dtype)).latent_dist.mode()\n\nif args.conditioning_dropout_prob is not None:\n    random_p = torch.rand(bsz, device=latents.device, generator=generator)\n    prompt_mask = random_p < 2 * args.conditioning_dropout_prob\n    prompt_mask = prompt_mask.reshape(bsz, 1, 1)\n    null_conditioning = text_encoder(tokenize_captions([\"\"]).to(accelerator.device))[0]\n    encoder_hidden_states = torch.where(prompt_mask, null_conditioning, encoder_hidden_states)\n\n    image_mask_dtype = original_image_embeds.dtype\n    image_mask = 1 - (\n        (random_p >= args.conditioning_dropout_prob).to(image_mask_dtype)\n        * (random_p < 3 * args.conditioning_dropout_prob).to(image_mask_dtype)\n    )\n    image_mask = image_mask.reshape(bsz, 1, 1, 1)\n    original_image_embeds = image_mask * original_image_embeds\n```", "```py\naccelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\n    --dataset_name=$DATASET_ID \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=256 \\\n    --random_flip \\\n    --train_batch_size=4 \\\n    --gradient_accumulation_steps=4 \\\n    --gradient_checkpointing \\\n    --max_train_steps=15000 \\\n    --checkpointing_steps=5000 \\\n    --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 \\\n    --max_grad_norm=1 \\\n    --lr_warmup_steps=0 \\\n    --conditioning_dropout_prob=0.05 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --push_to_hub\n```", "```py\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline\nfrom diffusers.utils import load_image\n\npipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"your_cool_model\", torch_dtype=torch.float16).to(\"cuda\")\ngenerator = torch.Generator(\"cuda\").manual_seed(0)\n\nimage = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\nprompt = \"add some ducks to the lake\"\nnum_inference_steps = 20\nimage_guidance_scale = 1.5\nguidance_scale = 10\n\nedited_image = pipeline(\n   prompt,\n   image=image,\n   num_inference_steps=num_inference_steps,\n   image_guidance_scale=image_guidance_scale,\n   guidance_scale=guidance_scale,\n   generator=generator,\n).images[0]\nedited_image.save(\"edited_image.png\")\n```"]