- en: Gradient Synchronization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦åŒæ­¥
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization](https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization](https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: PyTorchâ€™s distributed module operates by communicating back and forth between
    all of the GPUs in your system. This communication takes time, and ensuring all
    processes know the states of each other happens at particular triggerpoints when
    using the `ddp` module.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„åˆ†å¸ƒå¼æ¨¡å—é€šè¿‡åœ¨ç³»ç»Ÿä¸­çš„æ‰€æœ‰GPUä¹‹é—´æ¥å›é€šä¿¡æ¥è¿è¡Œã€‚è¿™ç§é€šä¿¡éœ€è¦æ—¶é—´ï¼Œå¹¶ä¸”ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœ¨ä½¿ç”¨`ddp`æ¨¡å—æ—¶åœ¨ç‰¹å®šè§¦å‘ç‚¹çŸ¥é“å½¼æ­¤çš„çŠ¶æ€ã€‚
- en: 'These triggerpoints are added to the PyTorch model, specifically their `forward()`
    and `backward()` methods. This happens when the model is wrapped with `DistributedDataParallel`:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è§¦å‘ç‚¹è¢«æ·»åŠ åˆ°PyTorchæ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬çš„`forward()`å’Œ`backward()`æ–¹æ³•ã€‚å½“æ¨¡å‹è¢«åŒ…è£…ä¸º`DistributedDataParallel`æ—¶ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼š
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In ğŸ¤— Accelerate this conversion happens automatically when calling [prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    and passing in your model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Accelerateä¸­ï¼Œå½“è°ƒç”¨[prepare()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.prepare)å¹¶ä¼ å…¥æ‚¨çš„æ¨¡å‹æ—¶ï¼Œæ­¤è½¬æ¢ä¼šè‡ªåŠ¨å‘ç”Ÿã€‚
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The slowdown in gradient accumulation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯ä¸­çš„å‡é€Ÿ
- en: You now understand that PyTorch adds hooks to the `forward` and `backward` method
    of your PyTorch model when training in a distributed setup. But how does this
    risk slowing down your code?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ç°åœ¨äº†è§£åˆ°ï¼Œåœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒæ—¶ï¼ŒPyTorchä¼šå‘æ‚¨çš„PyTorchæ¨¡å‹çš„`forward`å’Œ`backward`æ–¹æ³•æ·»åŠ é’©å­ã€‚ä½†æ˜¯è¿™å¦‚ä½•ä¼šå¯¼è‡´æ‚¨çš„ä»£ç å‡é€Ÿå‘¢ï¼Ÿ
- en: In DDP (distributed data parallel), the specific order in which processes are
    performed and ran are expected at specific points and these must also occur at
    roughly the same time before moving on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨DDPï¼ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼‰ä¸­ï¼Œæ‰§è¡Œå’Œè¿è¡Œè¿›ç¨‹çš„ç‰¹å®šé¡ºåºé¢„æœŸåœ¨ç‰¹å®šç‚¹å‘ç”Ÿï¼Œå¹¶ä¸”åœ¨ç»§ç»­ä¹‹å‰å¿…é¡»å¤§è‡´åŒæ—¶å‘ç”Ÿã€‚
- en: The most direct example is when you update model parameters through `optimizer.step()`.
    Without gradient accumulation, all instances of the model need to have updated
    their gradients computed, collated, and updated before moving on to the next batch
    of data. When performing gradient accumulation, you accumulate `n` loss gradients
    and skip `optimizer.step()` until `n` batches have been reached. As all training
    processes only need to synchronize by the time `optimizer.step()` is called, without
    any modification to your training step, this needless inter-process communication
    can cause a significant slowdown.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç›´æ¥çš„ä¾‹å­æ˜¯å½“æ‚¨é€šè¿‡`optimizer.step()`æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ã€‚æ²¡æœ‰æ¢¯åº¦ç´¯ç§¯ï¼Œæ‰€æœ‰æ¨¡å‹å®ä¾‹éœ€è¦åœ¨ç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ä¹‹å‰æ›´æ–°å®ƒä»¬çš„æ¢¯åº¦è®¡ç®—ã€æ•´åˆå’Œæ›´æ–°ã€‚è¿›è¡Œæ¢¯åº¦ç´¯ç§¯æ—¶ï¼Œæ‚¨ç´¯ç§¯`n`ä¸ªæŸå¤±æ¢¯åº¦ï¼Œå¹¶åœ¨è¾¾åˆ°`n`ä¸ªæ‰¹æ¬¡ä¹‹å‰è·³è¿‡`optimizer.step()`ã€‚ç”±äºæ‰€æœ‰è®­ç»ƒè¿‡ç¨‹åªéœ€è¦åœ¨è°ƒç”¨`optimizer.step()`æ—¶åŒæ­¥ï¼Œè€Œä¸éœ€è¦å¯¹è®­ç»ƒæ­¥éª¤è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œè¿™ç§ä¸å¿…è¦çš„è¿›ç¨‹é—´é€šä¿¡å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—çš„å‡é€Ÿã€‚
- en: How can you avoid this overhead?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•é¿å…è¿™ç§å¼€é”€ï¼Ÿ
- en: Solving the slowdown problem
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£å†³å‡é€Ÿé—®é¢˜
- en: Since you are skipping model parameter updates when training on these batches,
    their gradients do not need to be synchronized until the point where `optimizer.step()`
    is actually called. PyTorch cannot automagically tell when you need to do this,
    but they do provide a tool to help through the [`no_sync`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync)
    context manager that is added to your model after converting it to DDP.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨è¿™äº›æ‰¹æ¬¡ä¸Šè®­ç»ƒæ—¶è·³è¿‡æ¨¡å‹å‚æ•°æ›´æ–°ï¼Œå®ƒä»¬çš„æ¢¯åº¦ä¸éœ€è¦åœ¨å®é™…è°ƒç”¨`optimizer.step()`æ—¶åŒæ­¥ã€‚PyTorchæ— æ³•è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶éœ€è¦æ‰§è¡Œæ­¤æ“ä½œï¼Œä½†ä»–ä»¬æä¾›äº†ä¸€ä¸ªå·¥å…·æ¥å¸®åŠ©ï¼Œé€šè¿‡åœ¨å°†æ¨¡å‹è½¬æ¢ä¸ºDDPåæ·»åŠ åˆ°æ‚¨çš„æ¨¡å‹çš„[`no_sync`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync)ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚
- en: 'Under this context manager, PyTorch will skip synchronizing the gradients when
    `.backward()` is called, and the first call to `.backward()` outside this context
    manager will trigger the synchronization. See an example below:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸‹ï¼Œå½“è°ƒç”¨`.backward()`æ—¶ï¼ŒPyTorchå°†è·³è¿‡åŒæ­¥æ¢¯åº¦ï¼Œå¹¶ä¸”åœ¨æ­¤ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¹‹å¤–ç¬¬ä¸€æ¬¡è°ƒç”¨`.backward()`æ—¶å°†è§¦å‘åŒæ­¥ã€‚è¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In ğŸ¤— Accelerate to make this an API that can be called no matter the training
    device (though it may not do anything if you are not in a distributed system!),
    `ddp_model.no_sync` gets replaced with [no_sync()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.no_sync)
    and operates the same way:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ğŸ¤— Accelerateä¸­ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå¯ä»¥è°ƒç”¨çš„APIï¼Œæ— è®ºè®­ç»ƒè®¾å¤‡å¦‚ä½•ï¼ˆå°½ç®¡å¦‚æœæ‚¨ä¸åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­å¯èƒ½ä¸ä¼šæ‰§è¡Œä»»ä½•æ“ä½œï¼ï¼‰ï¼Œ`ddp_model.no_sync`è¢«æ›¿æ¢ä¸º[no_sync()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.no_sync)ï¼Œå¹¶ä¸”æ“ä½œæ–¹å¼ç›¸åŒï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you may expect, the [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    function wraps around this conditional check by keeping track of the current batch
    number, leaving you with the final gradient accumulation API:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€æœŸæœ›çš„é‚£æ ·ï¼Œ[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)å‡½æ•°é€šè¿‡è·Ÿè¸ªå½“å‰æ‰¹æ¬¡å·ç æ¥åŒ…è£…è¿™ä¸ªæ¡ä»¶æ£€æŸ¥ï¼Œä½¿æ‚¨å¾—åˆ°æœ€ç»ˆçš„æ¢¯åº¦ç´¯ç§¯APIï¼š
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As a result, you should either use *`accelerator.accumulate` or `accelerator.no_sync`*
    when it comes to API choice.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨é€‰æ‹©APIæ—¶ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨*`accelerator.accumulate`æˆ–`accelerator.no_sync`*ã€‚
- en: Just how much of a slowdown is there, and easy mistakes you can make
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ‰å¤šå°‘å‡é€Ÿï¼Œä»¥åŠæ‚¨å¯èƒ½çŠ¯çš„ç®€å•é”™è¯¯
- en: 'To set up a realistic example, consider the following setup:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºç«‹ä¸€ä¸ªç°å®çš„ä¾‹å­ï¼Œè€ƒè™‘ä»¥ä¸‹è®¾ç½®ï¼š
- en: Two single-GPU T4 nodes and one node with two GPUs
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå•GPU T4èŠ‚ç‚¹å’Œä¸€ä¸ªæœ‰ä¸¤ä¸ªGPUçš„èŠ‚ç‚¹
- en: Each GPU is a T4, and are hosted on GCP
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPUéƒ½æ˜¯T4ï¼Œå¹¶ä¸”æ‰˜ç®¡åœ¨GCPä¸Š
- en: The script used is a modification of the [NLP Example](https://github.com/muellerzr/timing_experiments/blob/main/baseline.py)
    script
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„è„šæœ¬æ˜¯[NLPç¤ºä¾‹](https://github.com/muellerzr/timing_experiments/blob/main/baseline.py)è„šæœ¬çš„ä¿®æ”¹
- en: Batch size per GPU is 16, and gradients are accumulated every 4 steps
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°ä¸º16ï¼Œå¹¶ä¸”æ¯4æ­¥ç´¯ç§¯æ¢¯åº¦ã€‚
- en: All scripts are available in [this repository](https://github.com/muellerzr/timing_experiments).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è„šæœ¬éƒ½åœ¨[æ­¤å­˜å‚¨åº“](https://github.com/muellerzr/timing_experiments)ä¸­å¯ç”¨ã€‚
- en: If not careful about gradient synchronization and GPU communication, a *large*
    amount of time can be wasted from when these GPUs communicate to each other during
    unnecessary periods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸æ³¨æ„æ¢¯åº¦åŒæ­¥å’ŒGPUé€šä¿¡ï¼Œé‚£ä¹ˆè¿™äº›GPUåœ¨ä¸å¿…è¦çš„æ—¶æœŸè¿›è¡Œé€šä¿¡æ—¶ä¼šæµªè´¹å¤§é‡æ—¶é—´ã€‚
- en: By how much?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå°‘ï¼Ÿ
- en: 'Reference:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å‚è€ƒï¼š
- en: 'Baseline: uses no synchronization practices discussed here'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºçº¿ï¼šä¸ä½¿ç”¨æ­¤å¤„è®¨è®ºçš„ä»»ä½•åŒæ­¥å®è·µ
- en: '`no_sync` improperly: `no_sync` only around the `backward` call, not the `forward`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_sync`ä¸æ­£ç¡®ï¼š`no_sync`ä»…åœ¨`backward`è°ƒç”¨å‘¨å›´ï¼Œè€Œä¸æ˜¯`forward`'
- en: '`no_sync`: using the `no_sync` pattern properly'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_sync`: æ­£ç¡®ä½¿ç”¨`no_sync`æ¨¡å¼'
- en: '`accumulate`: using [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    properly'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accumulate`: ä½¿ç”¨[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)æ­£ç¡®åœ°'
- en: 'Below are the average seconds per batch iterating over 29 batches of data for
    each setup on both a single node and on the dual-node setup:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨å•èŠ‚ç‚¹å’ŒåŒèŠ‚ç‚¹è®¾ç½®ä¸Šè¿­ä»£29æ‰¹æ•°æ®çš„å¹³å‡ç§’æ•°ï¼Œå¯¹æ¯”æ¯ç§è®¾ç½®ï¼š
- en: '|  | Baseline | `no_sync` improperly | `no_sync` | `accumulate` |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | åŸºçº¿ | `no_sync`ä¸æ­£ç¡® | `no_sync` | `accumulate` |'
- en: '| :-: | :-: | :-: | :-: | :-: |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| :-: | :-: | :-: | :-: | :-: |'
- en: '| Multi-Node | 2Â±0.01s | 2.13Â±0.08s | **0.91Â±0.11s** | **0.91Â±0.11s** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| å¤šèŠ‚ç‚¹ | 2Â±0.01ç§’ | 2.13Â±0.08ç§’ | **0.91Â±0.11ç§’** | **0.91Â±0.11ç§’** |'
- en: '| Single Node | 0.50Â±0.01s | 0.50Â±0.01s | **0.41Â±0.015s** | **0.41Â±0.015s**
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| å•èŠ‚ç‚¹ | 0.50Â±0.01ç§’ | 0.50Â±0.01ç§’ | **0.41Â±0.015ç§’** | **0.41Â±0.015ç§’** |'
- en: As you can see, if you are not careful about how you set up your gradient synchronization,
    you can get upwards of more than a 2x slowdown during training!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå¦‚æœä¸æ³¨æ„å¦‚ä½•è®¾ç½®æ¢¯åº¦åŒæ­¥ï¼Œè®­ç»ƒæœŸé—´å¯èƒ½ä¼šå‡ºç°è¶…è¿‡2å€çš„å‡é€Ÿï¼
- en: If you are worried about making sure everything is done properly, we highly
    recommend utilizing the [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    function and passing in `gradient_accumulation_steps` or `gradient_accumulation_plugin`
    to the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object so Accelerate can handle this for you.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ‹…å¿ƒç¡®ä¿ä¸€åˆ‡éƒ½åšå¾—æ­£ç¡®ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®åˆ©ç”¨[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)å‡½æ•°ï¼Œå¹¶å‘[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡ä¼ é€’`gradient_accumulation_steps`æˆ–`gradient_accumulation_plugin`ï¼Œä»¥ä¾¿Accelerateå¯ä»¥ä¸ºæ‚¨å¤„ç†è¿™äº›ã€‚
