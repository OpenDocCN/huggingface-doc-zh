- en: 'The Bellman Equation: simplify our value estimation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation](https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation **simplifies our state value or state-action value calculation.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/2bf6b05b2e64c9c62e78603164e99c30.png)'
  prefs: []
  type: TYPE_IMG
- en: With what we have learned so far, we know that if we calculate<math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​) (the
    value of a state), we need to calculate the return starting at that state and
    then follow the policy forever after. **(The policy we defined in the following
    example is a Greedy Policy; for simplification, we don’t discount the reward).**
  prefs: []
  type: TYPE_NORMAL
- en: 'So to calculate<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​),
    we need to calculate the sum of the expected rewards. Hence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 1: the sum of rewards if the agent started
    in that state and then followed the greedy policy (taking actions that leads to
    the best states values) for all the time steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, to calculate the<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​),
    we need to calculate the return starting at that state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/168b16071cc51b3d9fd633df82adf056.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 2: the sum of rewards **if the agent started
    in that state**, and then followed the **policy for all the time steps.**'
  prefs: []
  type: TYPE_NORMAL
- en: So you may have noticed, we’re repeating the computation of the value of different
    states, which can be tedious if you need to do it for each state value or state-action
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of calculating the expected return for each state or each state-action
    pair, **we can use the Bellman equation.** (hint: if you know what Dynamic Programming
    is, this is very similar! if you don’t know what it is, no worries!)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bellman equation is a recursive equation that works like this: instead
    of starting for each state from the beginning and calculating the return, we can
    consider the value of any state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + the
    discounted value of the state that follows (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+1})</annotation></semantics></math> gamma∗V(St+1​) ) .**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/a9dac28cddf8b65584ecce42d2ee7bb0.png)'
  prefs: []
  type: TYPE_IMG
- en: If we go back to our example, we can say that the value of State 1 is equal
    to the expected cumulative return if we start at that state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 1: the sum of rewards **if the agent started
    in that state 1** and then followed the **policy for all the time steps.**'
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t})</annotation></semantics></math>V(St​)
    = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + Discounted
    value of the next state <math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/41ddd09d9f3e231dd44290e310a72771.png)'
  prefs: []
  type: TYPE_IMG
- en: For simplification, here we don’t discount so gamma = 1.
  prefs: []
  type: TYPE_NORMAL
- en: In the interest of simplicity, here we don’t discount, so gamma = 1. But you’ll
    study an example with gamma = 0.99 in the Q-Learning section of this unit.
  prefs: []
  type: TYPE_NORMAL
- en: The value of <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
    V(St+1​) = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+2}</annotation></semantics></math>Rt+2​ + Discounted
    value of the next state (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+2})</annotation></semantics></math>gamma∗V(St+2​) ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To recap, the idea of the Bellman equation is that instead of calculating each
    value as the sum of the expected return, **which is a long process**, we calculate
    the value as **the sum of immediate reward + the discounted value of the state
    that follows.**
  prefs: []
  type: TYPE_NORMAL
- en: Before going to the next section, think about the role of gamma in the Bellman
    equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)?
    What happens if the value is 1? What happens if the value is very high, such as
    a million?
  prefs: []
  type: TYPE_NORMAL
