- en: 'The Bellman Equation: simplify our value estimation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation](https://huggingface.co/learn/deep-rl-course/unit2/bellman-equation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation **simplifies our state value or state-action value calculation.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/2bf6b05b2e64c9c62e78603164e99c30.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: With what we have learned so far, we know that if we calculate<math><semantics><mrow><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation
    encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​) (the
    value of a state), we need to calculate the return starting at that state and
    then follow the policy forever after. **(The policy we defined in the following
    example is a Greedy Policy; for simplification, we don’t discount the reward).**
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'So to calculate<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_t)</annotation></semantics></math>V(St​),
    we need to calculate the sum of the expected rewards. Hence:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 1: the sum of rewards if the agent started
    in that state and then followed the greedy policy (taking actions that leads to
    the best states values) for all the time steps.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Then, to calculate the<math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>V(St+1​),
    we need to calculate the return starting at that state<math><semantics><mrow><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">S_{t+1}</annotation></semantics></math>St+1​.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/168b16071cc51b3d9fd633df82adf056.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 2: the sum of rewards **if the agent started
    in that state**, and then followed the **policy for all the time steps.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: So you may have noticed, we’re repeating the computation of the value of different
    states, which can be tedious if you need to do it for each state value or state-action
    value.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of calculating the expected return for each state or each state-action
    pair, **we can use the Bellman equation.** (hint: if you know what Dynamic Programming
    is, this is very similar! if you don’t know what it is, no worries!)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bellman equation is a recursive equation that works like this: instead
    of starting for each state from the beginning and calculating the return, we can
    consider the value of any state as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**The immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + the
    discounted value of the state that follows (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+1})</annotation></semantics></math> gamma∗V(St+1​) ) .**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/a9dac28cddf8b65584ecce42d2ee7bb0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: If we go back to our example, we can say that the value of State 1 is equal
    to the expected cumulative return if we start at that state.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Bellman equation](../Images/f3af14f7f97044c041b7bab60378775a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'To calculate the value of State 1: the sum of rewards **if the agent started
    in that state 1** and then followed the **policy for all the time steps.**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t})</annotation></semantics></math>V(St​)
    = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + Discounted
    value of the next state <math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于 <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t})</annotation></semantics></math>V(St​)
    = 立即奖励 <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+1}</annotation></semantics></math>Rt+1​ + 下一个状态的折现价值
    <math><semantics><mrow><mi>γ</mi><mo>∗</mo><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\gamma
    * V(S_{t+1})</annotation></semantics></math>γ∗V(St+1​)
- en: '![Bellman equation](../Images/41ddd09d9f3e231dd44290e310a72771.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![贝尔曼方程](../Images/41ddd09d9f3e231dd44290e310a72771.png)'
- en: For simplification, here we don’t discount so gamma = 1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为简化起见，这里我们不打折，所以γ = 1。
- en: In the interest of simplicity, here we don’t discount, so gamma = 1. But you’ll
    study an example with gamma = 0.99 in the Q-Learning section of this unit.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，这里我们不打折，所以γ = 1。但在本单元的Q学习部分，您将学习一个γ = 0.99的示例。
- en: The value of <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
    V(St+1​) = Immediate reward <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+2}</annotation></semantics></math>Rt+2​ + Discounted
    value of the next state (<math><semantics><mrow><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+2})</annotation></semantics></math>gamma∗V(St+2​) ).
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <math><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S_{t+1})</annotation></semantics></math>
    V(St+1​) = 立即奖励 <math><semantics><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow><annotation
    encoding="application/x-tex">R_{t+2}</annotation></semantics></math>Rt+2​ + 下一个状态的折现价值
    (<math><semantics><mrow><mi>γ</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mo>∗</mo><mi>V</mi><mo
    stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo
    stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gamma *
    V(S_{t+2})</annotation></semantics></math>gamma∗V(St+2​) ).
- en: And so on.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: To recap, the idea of the Bellman equation is that instead of calculating each
    value as the sum of the expected return, **which is a long process**, we calculate
    the value as **the sum of immediate reward + the discounted value of the state
    that follows.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，贝尔曼方程的思想是，**不是将每个值计算为预期回报的总和，这是一个漫长的过程**，而是将值计算为**立即奖励的总和 + 随后状态的折现价值**。
- en: Before going to the next section, think about the role of gamma in the Bellman
    equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)?
    What happens if the value is 1? What happens if the value is very high, such as
    a million?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请考虑贝尔曼方程中γ的作用。如果γ的值非常低（例如0.1甚至为0），会发生什么？如果值为1会发生什么？如果值非常高，比如一百万会发生什么？
