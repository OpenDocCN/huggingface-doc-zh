["```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n>>> processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.Tensor([image.size[::-1]])\n>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\n>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n>>> text = texts[i]\n>>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n>>> for box, score, label in zip(boxes, scores, labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]\nDetected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]\n```", "```py\n>>> from transformers import OwlViTTextConfig, OwlViTTextModel\n\n>>> # Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration\n>>> configuration = OwlViTTextConfig()\n\n>>> # Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration\n>>> model = OwlViTTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import OwlViTVisionConfig, OwlViTVisionModel\n\n>>> # Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration\n>>> configuration = OwlViTVisionConfig()\n\n>>> # Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration\n>>> model = OwlViTVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, OwlViTModel\n\n>>> model = OwlViTModel.from_pretrained(\"google/owlvit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(text=[[\"a photo of a cat\", \"a photo of a dog\"]], images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoProcessor, OwlViTModel\n\n>>> model = OwlViTModel.from_pretrained(\"google/owlvit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> inputs = processor(\n...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n... )\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, OwlViTModel\n\n>>> model = OwlViTModel.from_pretrained(\"google/owlvit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoProcessor, OwlViTTextModel\n\n>>> model = OwlViTTextModel.from_pretrained(\"google/owlvit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> inputs = processor(\n...     text=[[\"a photo of a cat\", \"a photo of a dog\"], [\"photo of a astranaut\"]], return_tensors=\"pt\"\n... )\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, OwlViTVisionModel\n\n>>> model = OwlViTVisionModel.from_pretrained(\"google/owlvit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import AutoProcessor, OwlViTForObjectDetection\n\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n>>> model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n>>> inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.Tensor([image.size[::-1]])\n>>> # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n>>> results = processor.post_process_object_detection(\n...     outputs=outputs, threshold=0.1, target_sizes=target_sizes\n... )\n\n>>> i = 0  # Retrieve predictions for the first image for the corresponding text queries\n>>> text = texts[i]\n>>> boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n\n>>> for box, score, label in zip(boxes, scores, labels):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nDetected a photo of a cat with confidence 0.707 at location [324.97, 20.44, 640.58, 373.29]\nDetected a photo of a cat with confidence 0.717 at location [1.46, 55.26, 315.55, 472.17]\n```", "```py\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n>>> from transformers import AutoProcessor, OwlViTForObjectDetection\n\n>>> processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch16\")\n>>> model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch16\")\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> query_url = \"http://images.cocodataset.org/val2017/000000001675.jpg\"\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)\n>>> inputs = processor(images=image, query_images=query_image, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model.image_guided_detection(**inputs)\n>>> # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n>>> target_sizes = torch.Tensor([image.size[::-1]])\n>>> # Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> results = processor.post_process_image_guided_detection(\n...     outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes\n... )\n>>> i = 0  # Retrieve predictions for the first image\n>>> boxes, scores = results[i][\"boxes\"], results[i][\"scores\"]\n>>> for box, score in zip(boxes, scores):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(f\"Detected similar object with confidence {round(score.item(), 3)} at location {box}\")\nDetected similar object with confidence 0.856 at location [10.94, 50.4, 315.8, 471.39]\nDetected similar object with confidence 1.0 at location [334.84, 25.33, 636.16, 374.71]\n```"]