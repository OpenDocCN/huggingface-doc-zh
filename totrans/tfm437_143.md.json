["```py\n>>> from transformers import BlenderbotSmallConfig, BlenderbotSmallModel\n\n>>> # Initializing a BlenderbotSmall facebook/blenderbot_small-90M style configuration\n>>> configuration = BlenderbotSmallConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/blenderbot_small-90M style configuration\n>>> model = BlenderbotSmallModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallModel\n\n>>> model = BlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_inputs = tokenizer(\"Studies show that\", return_tensors=\"pt\")  # Batch size 1\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 3, 512]\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot_small-90M\"\n>>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\nHuman:  My friends are cool but they eat too many carbs.\n\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\nBot:  what kind of carbs do they eat? i don't know much about carbs.\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\nHuman: I'm not sure\n\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.__end__ __start__what kind of carbs do they eat? \"\n...     \"i don't know much about carbs__end__ \"\n...     \"__start__ I'm not sure.\"\n... )\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\nBot:  they eat a lot of carbs. carbs are high in fat, protein, and fats.\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = BlenderbotSmallForCausalLM.from_pretrained(\"facebook/blenderbot_small-90M\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotSmallModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = TFBlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotSmallForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot_small-90M\"\n>>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\nwhat kind of carbs do they eat? i don't know much about carbs.\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.</s> \"\n...     \"<s>what kind of carbs do they eat? i don't know much about carbs.</s> \"\n...     \"<s>I'm not sure.\"\n... )\n\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n>>> inputs.pop(\"token_type_ids\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = FlaxBlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"np\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n>>> values, predictions = jax.lax.top_k(probs)\n\n>>> tokenizer.decode(predictions).split()\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]