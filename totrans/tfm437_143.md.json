["```py\n( vocab_size = 50265 max_position_embeddings = 512 encoder_layers = 8 encoder_ffn_dim = 2048 encoder_attention_heads = 16 decoder_layers = 8 decoder_ffn_dim = 2048 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 512 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 1 scale_embedding = False pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 forced_eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import BlenderbotSmallConfig, BlenderbotSmallModel\n\n>>> # Initializing a BlenderbotSmall facebook/blenderbot_small-90M style configuration\n>>> configuration = BlenderbotSmallConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/blenderbot_small-90M style configuration\n>>> model = BlenderbotSmallModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file bos_token = '__start__' eos_token = '__end__' unk_token = '__unk__' pad_token = '__null__' **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';A list of integers in the range [0, 1]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None merges_file = None unk_token = '<|endoftext|>' bos_token = '<|endoftext|>' eos_token = '<|endoftext|>' add_prefix_space = False trim_offsets = True **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: BlenderbotSmallConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallModel\n\n>>> model = BlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_inputs = tokenizer(\"Studies show that\", return_tensors=\"pt\")  # Batch size 1\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 3, 512]\n```", "```py\n( config: BlenderbotSmallConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Union = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot_small-90M\"\n>>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\nHuman:  My friends are cool but they eat too many carbs.\n\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\nBot:  what kind of carbs do they eat? i don't know much about carbs.\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\nHuman: I'm not sure\n\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.__end__ __start__what kind of carbs do they eat? \"\n...     \"i don't know much about carbs__end__ \"\n...     \"__start__ I'm not sure.\"\n... )\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\nBot:  they eat a lot of carbs. carbs are high in fat, protein, and fats.\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, BlenderbotSmallForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = BlenderbotSmallForCausalLM.from_pretrained(\"facebook/blenderbot_small-90M\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n( config: BlenderbotSmallConfig *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: List[tf.Tensor] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotSmallModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = TFBlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[TFBaseModelOutput] = None past_key_values: List[tf.Tensor] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFBlenderbotSmallForConditionalGeneration\n\n>>> mname = \"facebook/blenderbot_small-90M\"\n>>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n\n>>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n>>> print(\"Human: \", UTTERANCE)\n>>> inputs = tokenizer([UTTERANCE], return_tensors=\"tf\")\n\n>>> reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\nwhat kind of carbs do they eat? i don't know much about carbs.\n\n>>> REPLY = \"I'm not sure\"\n>>> print(\"Human: \", REPLY)\n>>> NEXT_UTTERANCE = (\n...     \"My friends are cool but they eat too many carbs.</s> \"\n...     \"<s>what kind of carbs do they eat? i don't know much about carbs.</s> \"\n...     \"<s>I'm not sure.\"\n... )\n\n>>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"tf\")\n>>> inputs.pop(\"token_type_ids\")\n>>> next_reply_ids = model.generate(**inputs)\n>>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n```", "```py\n( config: BlenderbotSmallConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> model = FlaxBlenderbotSmallModel.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: BlenderbotSmallConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"np\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n>>> values, predictions = jax.lax.top_k(probs)\n\n>>> tokenizer.decode(predictions).split()\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None deterministic: bool = True params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBlenderbotSmallForConditionalGeneration\n\n>>> model = FlaxBlenderbotSmallForConditionalGeneration.from_pretrained(\"facebook/blenderbot_small-90M\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot_small-90M\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]