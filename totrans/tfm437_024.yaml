- en: Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/summarization)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/357.48e5eb08.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Youtube.e1129c6f.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/DocNotebookDropdown.3e6b3817.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI](https://www.youtube-nocookie.com/embed/yHnr5Dk2zCI)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarization creates a shorter version of a document or an article that captures
    all the important information. Along with translation, it is another example of
    a task that can be formulated as a sequence-to-sequence task. Summarization can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extractive: extract the most relevant information from a document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abstractive: generate new text that captures the most relevant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This guide will show you how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Finetune [T5](https://huggingface.co/t5-small) on the California state bill
    subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive
    summarization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your finetuned model for inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[BART](../model_doc/bart), [BigBird-Pegasus](../model_doc/bigbird_pegasus),
    [Blenderbot](../model_doc/blenderbot), [BlenderbotSmall](../model_doc/blenderbot-small),
    [Encoder decoder](../model_doc/encoder-decoder), [FairSeq Machine-Translation](../model_doc/fsmt),
    [GPTSAN-japanese](../model_doc/gptsan-japanese), [LED](../model_doc/led), [LongT5](../model_doc/longt5),
    [M2M100](../model_doc/m2m_100), [Marian](../model_doc/marian), [mBART](../model_doc/mbart),
    [MT5](../model_doc/mt5), [MVP](../model_doc/mvp), [NLLB](../model_doc/nllb), [NLLB-MOE](../model_doc/nllb-moe),
    [Pegasus](../model_doc/pegasus), [PEGASUS-X](../model_doc/pegasus_x), [PLBart](../model_doc/plbart),
    [ProphetNet](../model_doc/prophetnet), [SeamlessM4T](../model_doc/seamless_m4t),
    [SeamlessM4Tv2](../model_doc/seamless_m4t_v2), [SwitchTransformers](../model_doc/switch_transformers),
    [T5](../model_doc/t5), [UMT5](../model_doc/umt5), [XLM-ProphetNet](../model_doc/xlm-prophetnet)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Load BillSum dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start by loading the smaller California state bill subset of the BillSum dataset
    from the ðŸ¤— Datasets library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.train_test_split)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two fields that youâ€™ll want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text`: the text of the bill whichâ€™ll be the input to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary`: a condensed version of `text` whichâ€™ll be the model target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to load a T5 tokenizer to process `text` and `summary`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessing function you want to create needs to:'
  prefs: []
  type: TYPE_NORMAL
- en: Prefix the input with a prompt so T5 knows this is a summarization task. Some
    models capable of multiple NLP tasks require prompting for specific tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the keyword `text_target` argument when tokenizing labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Truncate sequences to be no longer than the maximum length set by the `max_length`
    parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    method. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now create a batch of examples using [DataCollatorForSeq2Seq](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq).
    Itâ€™s more efficient to *dynamically pad* the sentences to the longest length in
    a batch during collation, instead of padding the whole dataset to the maximum
    length.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Evaluate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load a evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)
    metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then create a function that passes your predictions and labels to `compute`
    to calculate the ROUGE metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you setup your training.
  prefs: []
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  prefs: []
  type: TYPE_NORMAL
- en: 'Youâ€™re ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, only three steps remain:'
  prefs: []
  type: TYPE_NORMAL
- en: Define your training hyperparameters in [Seq2SeqTrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the ROUGE metric and save the training checkpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the training arguments to [Seq2SeqTrainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Seq2SeqTrainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  prefs: []
  type: TYPE_NORMAL
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can load T5 with [TFAutoModelForSeq2SeqLM](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method).
    Note that Transformers models all have a default task-relevant loss function,
    so you donâ€™t need to specify one unless you want to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The last two things to setup before you start training is to compute the ROUGE
    score from the predictions, and provide a way to push your model to the Hub. Both
    are done by using [Keras callbacks](../main_classes/keras_callbacks).
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass your `compute_metrics` function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify where to push your model and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then bundle your callbacks together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, youâ€™re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callbacks
    to finetune the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  prefs: []
  type: TYPE_NORMAL
- en: For a more in-depth example of how to finetune a model for summarization, take
    a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  prefs: []
  type: TYPE_NORMAL
- en: 'Come up with some text youâ€™d like to summarize. For T5, you need to prefix
    your input depending on the task youâ€™re working on. For summarization you should
    prefix your input as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for summarization with your model, and pass your text
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the text and return the `input_ids` as PyTorch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Use the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    method to create the summarization. For more details about the different text
    generation strategies and parameters for controlling generation, check out the
    [Text Generation](../main_classes/text_generation) API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Decode the generated token ids back into text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the text and return the `input_ids` as TensorFlow tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Use the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.TFGenerationMixin.generate)
    method to create the summarization. For more details about the different text
    generation strategies and parameters for controlling generation, check out the
    [Text Generation](../main_classes/text_generation) API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Decode the generated token ids back into text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
