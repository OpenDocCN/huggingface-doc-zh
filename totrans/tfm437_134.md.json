["```py\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> line = \"Ch\u00fang t\u00f4i l\u00e0 nh\u1eefng nghi\u00ean c\u1ee9u vi\u00ean.\"\n\n>>> input_ids = tokenizer(line, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     features = bartpho(**input_ids)  # Models outputs are now tuples\n\n>>> # With TensorFlow 2.0+:\n>>> from transformers import TFAutoModel\n\n>>> bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n>>> input_ids = tokenizer(line, return_tensors=\"tf\")\n>>> features = bartpho(**input_ids)\n```", "```py\n>>> from transformers import MBartForConditionalGeneration\n\n>>> bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n>>> TXT = \"Ch\u00fang t\u00f4i l\u00e0 <mask> nghi\u00ean c\u1ee9u vi\u00ean.\"\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = bartpho(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n>>> print(tokenizer.decode(predictions).split())\n```", "```py\n( vocab_file monolingual_vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```"]