["```py\n>>> import torch\n>>> from transformers import AutoModel, AutoTokenizer\n\n>>> bartpho = AutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\")\n\n>>> line = \"Ch\u00fang t\u00f4i l\u00e0 nh\u1eefng nghi\u00ean c\u1ee9u vi\u00ean.\"\n\n>>> input_ids = tokenizer(line, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     features = bartpho(**input_ids)  # Models outputs are now tuples\n\n>>> # With TensorFlow 2.0+:\n>>> from transformers import TFAutoModel\n\n>>> bartpho = TFAutoModel.from_pretrained(\"vinai/bartpho-syllable\")\n>>> input_ids = tokenizer(line, return_tensors=\"tf\")\n>>> features = bartpho(**input_ids)\n```", "```py\n>>> from transformers import MBartForConditionalGeneration\n\n>>> bartpho = MBartForConditionalGeneration.from_pretrained(\"vinai/bartpho-syllable\")\n>>> TXT = \"Ch\u00fang t\u00f4i l\u00e0 <mask> nghi\u00ean c\u1ee9u vi\u00ean.\"\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = bartpho(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n>>> print(tokenizer.decode(predictions).split())\n```"]