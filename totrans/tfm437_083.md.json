["```py\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n)\n```", "```py\n# Generate random inputs for the model.\nbatch_size = 16\ninput_vector_dim = 10\nrandom_inputs = tf.random.normal((batch_size, input_vector_dim))\n\n# Run a forward pass.\n_ = model(random_inputs)\n```", "```py\nxla_fn = tf.function(model, jit_compile=True)\n_ = xla_fn(random_inputs)\n```", "```py\nmy_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n```", "```py\npip install transformers --upgrade\n```", "```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\n# Will error if the minimal version of Transformers is not installed.\nfrom transformers.utils import check_min_version\n\ncheck_min_version(\"4.21.0\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\ninput_string = [\"TensorFlow is\"]\n\n# One line to create an XLA generation function\nxla_generate = tf.function(model.generate, jit_compile=True)\n\ntokenized_input = tokenizer(input_string, return_tensors=\"tf\")\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\n\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the\n```", "```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\ninput_string = [\"TensorFlow is\"]\n\nxla_generate = tf.function(model.generate, jit_compile=True)\n\n# Here, we call the tokenizer with padding options.\ntokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n```", "```py\nimport time\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nxla_generate = tf.function(model.generate, jit_compile=True)\n\nfor input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n    start = time.time_ns()\n    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n    end = time.time_ns()\n    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n```", "```py\nExecution time -- 30819.6 ms\n\nExecution time -- 79.0 ms\n\nExecution time -- 78.9 ms\n```"]