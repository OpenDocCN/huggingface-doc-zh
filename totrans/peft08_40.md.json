["```py\n( peft_type: Union = None auto_mapping: Optional = None base_model_name_or_path: Optional = None revision: Optional = None task_type: Union = None inference_mode: bool = False r: int = 8 target_modules: Optional[Union[list[str], str]] = None lora_alpha: int = 8 lora_dropout: float = 0.0 fan_in_fan_out: bool = False bias: Literal['none', 'all', 'lora_only'] = 'none' use_rslora: bool = False modules_to_save: Optional[list[str]] = None init_lora_weights: bool | Literal['gaussian', 'loftq'] = True layers_to_transform: Optional[Union[list[int], int]] = None layers_pattern: Optional[Union[list[str], str]] = None rank_pattern: Optional[dict] = <factory> alpha_pattern: Optional[dict] = <factory> megatron_config: Optional[dict] = None megatron_core: Optional[str] = 'megatron.core' loftq_config: Union[LoftQConfig, dict] = <factory> )\n```", "```py\n( model config adapter_name ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n>>> from transformers import AutoModelForSeq2SeqLM\n>>> from peft import LoraModel, LoraConfig\n\n>>> config = LoraConfig(\n...     task_type=\"SEQ_2_SEQ_LM\",\n...     r=8,\n...     lora_alpha=32,\n...     target_modules=[\"q\", \"v\"],\n...     lora_dropout=0.01,\n... )\n\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n>>> lora_model = LoraModel(model, config, \"default\")\n```", "```py\n>>> import transformers\n>>> from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_int8_training\n\n>>> target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\", \"wte\"]\n>>> config = LoraConfig(\n...     r=4, lora_alpha=16, target_modules=target_modules, lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n... )\n\n>>> model = transformers.GPTJForCausalLM.from_pretrained(\n...     \"kakaobrain/kogpt\",\n...     revision=\"KoGPT6B-ryan1.5b-float16\",  # or float32 version: revision=KoGPT6B-ryan1.5b\n...     pad_token_id=tokenizer.eos_token_id,\n...     use_cache=False,\n...     device_map={\"\": rank},\n...     torch_dtype=torch.float16,\n...     load_in_8bit=True,\n... )\n>>> model = prepare_model_for_int8_training(model)\n>>> lora_model = get_peft_model(model, config)\n```", "```py\n( adapters weights adapter_name combination_type = 'svd' svd_rank = None svd_clamp = None svd_full_matrices = True svd_driver = None density = None majority_sign_method: Literal['total', 'frequency'] = 'total' )\n```", "```py\n( adapter_name: str )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( progressbar: bool = False safe_merge: bool = False adapter_names: Optional[list[str]] = None )\n```", "```py\n>>> from transformers import AutoModelForCausalLM\n>>> from peft import PeftModel\n\n>>> base_model = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-40b\")\n>>> peft_model_id = \"smangrul/falcon-40B-int4-peft-lora-sfttrainer-sample\"\n>>> model = PeftModel.from_pretrained(base_model, peft_model_id)\n>>> merged_model = model.merge_and_unload()\n```", "```py\n( adapter_name: str | list[str] )\n```", "```py\n>>> for name, param in model_peft.named_parameters():\n...     if ...:  # some check on name (ex. if 'lora' in name)\n...         param.requires_grad = False\n```", "```py\n( )\n```"]