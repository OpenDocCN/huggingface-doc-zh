- en: PatchTSMixer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtsmixer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtsmixer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model
    for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf)
    by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer
    architecture. In this HuggingFace implementation, we provide PatchTSMixer’s capabilities
    to effortlessly facilitate lightweight mixing across patches, channels, and hidden
    features for effective multivariate time-series modeling. It also supports various
    attention mechanisms starting from simple gated attention to more complex self-attention
    blocks that can be customized accordingly. The model can be pretrained and subsequently
    used for various downstream tasks such as forecasting, classification and regression.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '*TSMixer is a lightweight neural architecture exclusively composed of multi-layer
    perceptron (MLP) modules designed for multivariate forecasting and representation
    learning on patched time series. Our model draws inspiration from the success
    of MLP-Mixer models in computer vision. We demonstrate the challenges involved
    in adapting Vision MLP-Mixer for time series and introduce empirically validated
    components to enhance accuracy. This includes a novel design paradigm of attaching
    online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling
    the time-series properties such as hierarchy and channel-correlations. We also
    propose a Hybrid channel modeling approach to effectively handle noisy channel
    interactions and generalization across diverse datasets, a common challenge in
    existing patch channel-mixing methods. Additionally, a simple gated attention
    mechanism is introduced in the backbone to prioritize important features. By incorporating
    these lightweight components, we significantly enhance the learning capability
    of simple MLP structures, outperforming complex Transformer models with minimal
    computing usage. Moreover, TSMixer’s modular design enables compatibility with
    both supervised and masked self-supervised learning methods, making it a promising
    building block for time-series Foundation Models. TSMixer outperforms state-of-the-art
    MLP and Transformer models in forecasting by a considerable margin of 8-60%. It
    also outperforms the latest strong benchmarks of Patch-Transformer models (by
    1-2%) with a significant reduction in memory and runtime (2-3X).*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [ajati](https://huggingface.co/ajati), [vijaye12](https://huggingface.co/vijaye12),
    [gsinthong](https://huggingface.co/gsinthong), [namctin](https://huggingface.co/namctin),
    [wmgifford](https://huggingface.co/wmgifford), [kashif](https://huggingface.co/kashif).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Sample usage
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Usage tips
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model can also be used for time series classification and time series regression.
    See the respective [PatchTSMixerForTimeSeriesClassification](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForTimeSeriesClassification)
    and [PatchTSMixerForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForRegression)
    classes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: PatchTSMixerConfig
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSMixerConfig`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/configuration_patchtsmixer.py#L30)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '`context_length` (`int`, *optional*, defaults to 32) — The context/history
    length for the input sequence.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_len` (`int`, *optional*, defaults to 8) — The patch length for the input
    sequence.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_input_channels` (`int`, *optional*, defaults to 1) — Number of input variates.
    For Univariate, set it to 1.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_stride` (`int`, *optional*, defaults to 8) — Determines the overlap
    between two consecutive patches. Set it to patch_length (or greater), if we want
    non-overlapping patches.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_stride` (`int`, *optional*, 默认为8) — 确定两个连续补丁之间的重叠。如果我们想要非重叠的补丁，则将其设置为patch_length（或更大）。'
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples to generate in parallel for probabilistic forecast.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_parallel_samples` (`int`, *optional*, 默认为100) — 用于概率预测并行生成的样本数量。'
- en: '`d_model` (`int`, *optional*, defaults to 8) — Hidden dimension of the model.
    Recommended to set it as a multiple of patch_length (i.e. 2-5X of patch_len).
    Larger value indicates more complex model.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, 默认为8) — 模型的隐藏维度。建议将其设置为patch_length的倍数（即patch_len的2-5倍）。较大的值表示更复杂的模型。'
- en: '`expansion_factor` (`int`, *optional*, defaults to 2) — Expansion factor to
    use inside MLP. Recommended range is 2-5\. Larger value indicates more complex
    model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expansion_factor` (`int`, *optional*, 默认为2) — 在MLP内部使用的扩展因子。推荐范围为2-5。较大的值表示更复杂的模型。'
- en: '`num_layers` (`int`, *optional*, defaults to 3) — Number of layers to use.
    Recommended range is 3-15\. Larger value indicates more complex model.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers` (`int`, *optional*, 默认为3) — 要使用的层数。推荐范围为3-15。较大的值表示更复杂的模型。'
- en: '`dropout` (`float`, *optional*, defaults to 0.2) — The dropout probability
    the `PatchTSMixer` backbone. Recommended range is 0.2-0.7'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为0.2) — `PatchTSMixer`主干的丢失概率。推荐范围为0.2-0.7'
- en: '`mode` (`str`, *optional*, defaults to `"common_channel"`) — Mixer Mode. Determines
    how to process the channels. Allowed values: “common_channel”, “mix_channel”.
    In “common_channel” mode, we follow Channel-independent modelling with no explicit
    channel-mixing. Channel mixing happens in an implicit manner via shared weights
    across channels. (preferred first approach) In “mix_channel” mode, we follow explicit
    channel-mixing in addition to patch and feature mixer. (preferred approach when
    channel correlations are very important to model)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode` (`str`, *optional*, 默认为`"common_channel"`) — Mixer模式。确定如何处理通道。允许的值：“common_channel”、“mix_channel”。在“common_channel”模式中，我们遵循独立于通道的建模，没有显式的通道混合。通道混合通过跨通道共享权重以隐式方式发生。（首选第一种方法）在“mix_channel”模式中，我们遇到显式的通道混合以及补丁和特征混合。（当通道相关性对模型非常重要时，首选方法）'
- en: '`gated_attn` (`bool`, *optional*, defaults to `True`) — Enable Gated Attention.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gated_attn` (`bool`, *optional*, 默认为`True`) — 启用门控注意力。'
- en: '`norm_mlp` (`str`, *optional*, defaults to `"LayerNorm"`) — Normalization layer
    (BatchNorm or LayerNorm).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_mlp` (`str`, *optional*, 默认为`"LayerNorm"`) — 归一化层（BatchNorm或LayerNorm）。'
- en: '`self_attn` (`bool`, *optional*, defaults to `False`) — Enable Tiny self attention
    across patches. This can be enabled when the output of Vanilla PatchTSMixer with
    gated attention is not satisfactory. Enabling this leads to explicit pair-wise
    attention and modelling across patches.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self_attn` (`bool`, *optional*, 默认为`False`) — 在补丁之间启用小型自注意力。当Vanilla PatchTSMixer的门控注意力的输出不理想时，可以启用此功能。启用此功能会导致显式的成对注意力和跨补丁建模。'
- en: '`self_attn_heads` (`int`, *optional*, defaults to 1) — Number of self-attention
    heads. Works only when `self_attn` is set to `True`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self_attn_heads` (`int`, *optional*, 默认为1) — 自注意力头的数量。仅当`self_attn`设置为`True`时才有效。'
- en: '`use_positional_encoding` (`bool`, *optional*, defaults to `False`) — Enable
    the use of positional embedding for the tiny self-attention layers. Works only
    when `self_attn` is set to `True`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_positional_encoding` (`bool`, *optional*, 默认为`False`) — 启用小型自注意力层的位置嵌入使用。仅当`self_attn`设置为`True`时才有效。'
- en: '`positional_encoding_type` (`str`, *optional*, defaults to `"sincos"`) — Positional
    encodings. Options `"random"` and `"sincos"` are supported. Works only when `use_positional_encoding`
    is set to `True`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`positional_encoding_type` (`str`, *optional*, 默认为`"sincos"`) — 位置编码。支持选项`"random"`和`"sincos"`。仅当`use_positional_encoding`设置为`True`时才有效。'
- en: '`scaling` (`string` or `bool`, *optional*, defaults to `"std"`) — Whether to
    scale the input targets via “mean” scaler, “std” scaler or no scaler if `None`.
    If `True`, the scaler is set to “mean”.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling` (`string`或`bool`, *optional*, 默认为`"std"`) — 是否通过“mean”缩放器、“std”缩放器或如果为`None`则不缩放输入目标。如果为`True`，则缩放器设置为“mean”。'
- en: '`loss` (`string`, *optional*, defaults to `"mse"`) — The loss function for
    the model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (“nll”) and for point estimates it is the mean
    squared error “mse”.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`string`, *optional*, 默认为`"mse"`) — 与`distribution_output`头对应的模型的损失函数。对于参数分布，它是负对数似然（“nll”），对于点估计，它是均方误差“mse”。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, 默认为0.02) — 截断正态权重初始化分布的标准差。'
- en: '`post_init` (`bool`, *optional*, defaults to `False`) — Whether to use custom
    weight initialization from `transformers` library, or the default initialization
    in `PyTorch`. Setting it to `False` performs `PyTorch` weight initialization.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`post_init` (`bool`, *optional*, 默认为`False`) — 是否使用`transformers`库中的自定义权重初始化，或者使用`PyTorch`中的默认初始化。将其设置为`False`执行`PyTorch`权重初始化。'
- en: '`norm_eps` (`float`, *optional*, defaults to 1e-05) — A value added to the
    denominator for numerical stability of normalization.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_eps` (`float`, *optional*, 默认为1e-05) — 用于归一化的分母的数值稳定性的值。'
- en: '`mask_type` (`str`, *optional*, defaults to `"random"`) — Type of masking to
    use for Masked Pretraining mode. Allowed values are “random”, “forecast”. In Random
    masking, points are masked randomly. In Forecast masking, points are masked towards
    the end.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_type` (`str`, *optional*, 默认为`"random"`) — 用于掩码预训练模式的掩码类型。允许的值为“random”、“forecast”。在随机掩码中，点被随机掩盖。在预测掩码中，点被朝向末尾掩盖。'
- en: '`random_mask_ratio` (`float`, *optional*, defaults to 0.5) — Masking ratio
    to use when `mask_type` is `random`. Higher value indicates more masking.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_mask_ratio` (`float`, *optional*, 默认为0.5) — 当`mask_type`为`random`时使用的掩码比例。较高的值表示更多的掩码。'
- en: '`num_forecast_mask_patches` (`int` or `list`, *optional*, defaults to `[2]`)
    — Number of patches to be masked at the end of each batch sample. If it is an
    integer, all the samples in the batch will have the same number of masked patches.
    If it is a list, samples in the batch will be randomly masked by numbers defined
    in the list. This argument is only used for forecast pretraining.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_forecast_mask_patches` (`int` or `list`, *optional*, defaults to `[2]`)
    — 每个批次样本末尾要屏蔽的补丁数量。如果是整数，则批次中的所有样本将具有相同数量的屏蔽补丁。如果是列表，则批次中的样本将随机屏蔽列表中定义的数字。此参数仅用于预测预训练。'
- en: '`mask_value` (`float`, *optional*, defaults to `0.0`) — Mask value to use.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_value` (`float`, *optional*, defaults to `0.0`) — 要使用的屏蔽值。'
- en: '`masked_loss` (`bool`, *optional*, defaults to `True`) — Whether to compute
    pretraining loss only at the masked portions, or on the entire output.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masked_loss` (`bool`, *optional*, defaults to `True`) — 是否仅在屏蔽部分计算预训练损失，还是在整个输出上计算。'
- en: '`channel_consistent_masking` (`bool`, *optional*, defaults to `True`) — When
    true, masking will be same across all channels of a timeseries. Otherwise, masking
    positions will vary across channels.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channel_consistent_masking` (`bool`, *optional*, defaults to `True`) — 当为
    True 时，屏蔽将在时间序列的所有通道上相同。否则，屏蔽位置将在通道之间变化。'
- en: '`unmasked_channel_indices` (`list`, *optional*) — Channels that are not masked
    during pretraining.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unmasked_channel_indices` (`list`, *optional*) — 预训练期间未屏蔽的通道。'
- en: '`head_dropout` (`float`, *optional*, defaults to 0.2) — The dropout probability
    the `PatchTSMixer` head.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_dropout` (`float`, *optional*, defaults to 0.2) — `PatchTSMixer` 头部的
    dropout 概率。'
- en: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model when loss is “nll”. Could be either “student_t”,
    “normal” or “negative_binomial”.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — 当损失为“nll”时，模型的分布发射头。可以是“student_t”、“normal”或“negative_binomial”。'
- en: '`prediction_length` (`int`, *optional*, defaults to 16) — Number of time steps
    to forecast for a forecasting task. Also known as the Forecast Horizon.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_length` (`int`, *optional*, defaults to 16) — 用于预测任务的预测时间步数。也称为预测视野。'
- en: '`prediction_channel_indices` (`list`, *optional*) — List of channel indices
    to forecast. If None, forecast all channels. Target data is expected to have all
    channels and we explicitly filter the channels in prediction and target before
    loss computation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_channel_indices` (`list`, *optional*) — 要预测的通道索引列表。如果为 None，则预测所有通道。目标数据预期具有所有通道，我们在损失计算之前明确过滤预测和目标中的通道。'
- en: '`num_targets` (`int`, *optional*, defaults to 3) — Number of targets (dimensionality
    of the regressed variable) for a regression task.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_targets` (`int`, *optional*, defaults to 3) — 回归任务的目标数量（回归变量的维度）。'
- en: '`output_range` (`list`, *optional*) — Output range to restrict for the regression
    task. Defaults to None.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_range` (`list`, *optional*) — 用于限制回归任务的输出范围。默认为 None。'
- en: '`head_aggregation` (`str`, *optional*, defaults to `"max_pool"`) — Aggregation
    mode to enable for classification or regression task. Allowed values are `None`,
    “use_last”, “max_pool”, “avg_pool”.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_aggregation` (`str`, *optional*, defaults to `"max_pool"`) — 用于分类或回归任务的聚合模式。允许的值为
    `None`、"use_last"、"max_pool"、"avg_pool"。'
- en: This is the configuration class to store the configuration of a [PatchTSMixerModel](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel).
    It is used to instantiate a PatchTSMixer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the PatchTSMixer [ibm/patchtsmixer-etth1-pretrain](https://huggingface.co/ibm/patchtsmixer-etth1-pretrain)
    architecture.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储 [PatchTSMixerModel](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel)
    的配置。根据指定的参数实例化一个 PatchTSMixer 模型，定义模型架构。使用默认值实例化配置将产生与 PatchTSMixer [ibm/patchtsmixer-etth1-pretrain](https://huggingface.co/ibm/patchtsmixer-etth1-pretrain)
    架构类似的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: PatchTSMixerModel
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSMixerModel
- en: '### `class transformers.PatchTSMixerModel`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSMixerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1299)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1299)'
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: '`mask_input` (`bool`, *optional*, defaults to `False`) — If True, Masking will
    be enabled. False otherwise.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_input` (`bool`, *optional*, defaults to `False`) — 如果为 True，则启用屏蔽。否则为
    False。'
- en: The PatchTSMixer Model for time-series forecasting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 用于时间序列预测的 PatchTSMixer 模型。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1327)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1327)'
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — Context values of the time series. For a pretraining task, this denotes the
    input time series to predict the masked portion. For a forecasting task, this
    denotes the history/past time series values. Similarly, for classification or
    regression tasks, it denotes the appropriate context values of the time series.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.FloatTensor`，形状为`(batch_size, seq_length, num_input_channels)`)
    — 时间序列的上下文值。对于预训练任务，这表示要预测掩码部分的输入时间序列。对于预测任务，这表示历史/过去的时间序列值。同样，对于分类或回归任务，它表示时间序列的适当上下文值。'
- en: For univariate time series, `num_input_channels` dimension should be 1\. For
    multivariate time series, it is greater than 1.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于单变量时间序列，`num_input_channels`维度应为1。对于多变量时间序列，它大于1。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`observed_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observed_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, num_input_channels)`，*可选*)
    — 布尔蒙版，指示哪些`past_values`是观察到的，哪些是缺失的。蒙版值选择在`[0, 1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`observed`的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`missing`的值（即被零替换的NaN）。
- en: Returns
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerModelOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    and inputs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig)）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_channels,
    num_patches, d_model)`) — Hidden-state at the output of the last layer of the
    model.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, num_channels, num_patches,
    d_model)`) — 模型最后一层的隐藏状态。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Hidden-states of
    the model at the output of each layer.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*) — 模型在每一层输出的隐藏状态。'
- en: '`patch_input` (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches,
    patch_length)`) — Patched input data to the model.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_input` (`torch.FloatTensor`，形状为`(batch_size, num_channels, num_patches,
    patch_length)`) — 输入到模型的补丁化数据。'
- en: '`mask:` (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches)`,*optional*)
    — Bool Tensor indicating True in masked patches and False otherwise.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask:` (`torch.FloatTensor`，形状为`(batch_size, num_channels, num_patches)`，*可选*)
    — 布尔张量，指示掩码补丁中的True和其他地方的False。'
- en: '`loc:` (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*)
    — Gives the mean of the context window per channel. Used for revin denorm outside
    the model, if revin enabled.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc:` (`torch.FloatTensor`，形状为`(batch_size, 1, num_channels)`，*可选*) — 给出每个通道上下文窗口的均值。如果启用了revin，则用于模型外的revin反归一化。'
- en: '`scale:` (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*)
    — Gives the std dev of the context window per channel. Used for revin denorm outside
    the model, if revin enabled.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale:` (`torch.FloatTensor`，形状为`(batch_size, 1, num_channels)`，*可选*) — 给出每个通道上下文窗口的标准差。如果启用了revin，则用于模型外的revin反归一化。'
- en: The [PatchTSMixerModel](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[PatchTSMixerModel](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: PatchTSMixerForPrediction
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSMixerForPrediction
- en: '### `class transformers.PatchTSMixerForPrediction`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSMixerForPrediction`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1597)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1597)'
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`PatchTSMixerConfig`, *required*) — Configuration.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` (`PatchTSMixerConfig`, *required*) — 配置。'
- en: '`PatchTSMixer` for forecasting application.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测应用的 `PatchTSMixer`。
- en: '#### `forward`'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1641)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1641)'
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — Context values of the time series. For a pretraining task, this denotes the
    input time series to predict the masked portion. For a forecasting task, this
    denotes the history/past time series values. Similarly, for classification or
    regression tasks, it denotes the appropriate context values of the time series.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — 时间序列的上下文值。对于预训练任务，这表示用于预测被屏蔽部分的输入时间序列。对于预测任务，这表示历史/过去的时间序列值。同样，对于分类或回归任务，它表示时间序列的适当上下文值。'
- en: For univariate time series, `num_input_channels` dimension should be 1\. For
    multivariate time series, it is greater than 1.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于单变量时间序列，`num_input_channels` 维度应为 1。对于多变量时间序列，它大于 1。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: '`observed_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observed_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — 布尔掩码，指示哪些 `past_values` 是观察到的，哪些是缺失的。掩码值选在
    `[0, 1]`：'
- en: 1 for values that are `observed`,
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `observed` 的值为 1，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `missing` 的值为 0（即被零替换的 NaN）。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)`
    for forecasting, — `(batch_size, num_targets)` for regression, or `(batch_size,)`
    for classification, *optional*): Target values of the time series, that serve
    as labels for the model. The `future_values` is what the Transformer needs during
    training to learn to output, given the `past_values`. Note that, this is NOT required
    for a pretraining task.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values` (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)`
    用于预测， — `(batch_size, num_targets)` 用于回归，或 `(batch_size,)` 用于分类， *optional*):
    时间序列的目标值，作为模型的标签。`future_values` 是训练期间 Transformer 需要的，以学习在给定 `past_values` 时输出。请注意，这对于预训练任务并非必需。'
- en: For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`.
    Even if we want to forecast only specific channels by setting the indices in `prediction_channel_indices`
    parameter, pass the target data with all channels, as channel Filtering for both
    prediction and target will be manually applied before the loss computation.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于预测任务，形状为 `(batch_size, target_len, num_input_channels)`。即使我们想通过在 `prediction_channel_indices`
    参数中设置索引来仅预测特定通道，也要传递带有所有通道的目标数据，因为在损失计算之前，预测和目标的通道过滤将手动应用。
- en: '`return_loss` (`bool`, *optional*) — Whether to return the loss in the `forward`
    call.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss` (`bool`, *optional*) — 是否在 `forward` 调用中返回损失。'
- en: Returns
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPredictionOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPredictionOutput`
    或 `tuple(torch.FloatTensor)`'
- en: A `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPredictionOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    and inputs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPredictionOutput`
    或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时）包含根据配置（[PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig)）和输入的不同元素。
- en: '`prediction_outputs` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_input_channels)`) — Prediction output from the forecast head.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_outputs` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_input_channels)`) — 预测头部的预测输出。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, d_model)`) — Backbone embeddings before passing through the head.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, d_model)`) — 通过头部之前的主干嵌入。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Hidden-states of
    the model at the output of each layer plus the optional initial embedding outputs.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — 模型在每一层输出处的隐藏状态，加上可选的初始嵌入输出。'
- en: '`loss` (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape
    `()`) — Total loss.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*optional*, 当提供 `y` 时返回，形状为 `()` 的 `torch.FloatTensor`) — 总损失。'
- en: '`loc` (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`)
    — Input mean'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc` (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`)
    — 输入均值'
- en: '`scale` (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`)
    — Input std dev'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale` (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`)
    — 输入标准差'
- en: The [PatchTSMixerForPrediction](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForPrediction)
    forward method, overrides the `__call__` special method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[PatchTSMixerForPrediction](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForPrediction)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: PatchTSMixerForTimeSeriesClassification
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSMixerForTimeSeriesClassification`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1830)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '`config` (`PatchTSMixerConfig`, *required*) — Configuration.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PatchTSMixer` for classification application.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1859)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — Context values of the time series. For a pretraining task, this denotes the
    input time series to predict the masked portion. For a forecasting task, this
    denotes the history/past time series values. Similarly, for classification or
    regression tasks, it denotes the appropriate context values of the time series.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For univariate time series, `num_input_channels` dimension should be 1\. For
    multivariate time series, it is greater than 1.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)`
    for forecasting, — `(batch_size, num_targets)` for regression, or `(batch_size,)`
    for classification, *optional*): Target values of the time series, that serve
    as labels for the model. The `future_values` is what the Transformer needs during
    training to learn to output, given the `past_values`. Note that, this is NOT required
    for a pretraining task.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`.
    Even if we want to forecast only specific channels by setting the indices in `prediction_channel_indices`
    parameter, pass the target data with all channels, as channel Filtering for both
    prediction and target will be manually applied before the loss computation.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a classification task, it has a shape of `(batch_size,)`.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a regression task, it has a shape of `(batch_size, num_targets)`.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_loss` (`bool`, *optional*) — Whether to return the loss in the `forward`
    call.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForTimeSeriesClassificationOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForTimeSeriesClassificationOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    and inputs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction_outputs` (`torch.FloatTensor` of shape `(batch_size, num_labels)`)
    — Prediction output from the classfication head.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, d_model)`) — Backbone embeddings before passing through the head.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Hidden-states of
    the model at the output of each layer plus the optional initial embedding outputs.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss` (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape
    `()`) — Total loss.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [PatchTSMixerForTimeSeriesClassification](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForTimeSeriesClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: PatchTSMixerForPretraining
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSMixerForPretraining
- en: '### `class transformers.PatchTSMixerForPretraining`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSMixerForPretraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1415)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1415)'
- en: '[PRE9]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`PatchTSMixerConfig`, *required*) — Configuration.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`PatchTSMixerConfig`，*必需*）— 配置。'
- en: '`PatchTSMixer` for mask pretraining.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`PatchTSMixer`用于掩码预训练。'
- en: '#### `forward`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1438)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L1438)'
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — Context values of the time series. For a pretraining task, this denotes the
    input time series to predict the masked portion. For a forecasting task, this
    denotes the history/past time series values. Similarly, for classification or
    regression tasks, it denotes the appropriate context values of the time series.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, seq_length, num_input_channels)`的`torch.FloatTensor`）—
    时间序列的上下文值。对于预训练任务，这表示要预测掩码部分的输入时间序列。对于预测任务，这表示历史/过去的时间序列值。同样，对于分类或回归任务，它表示时间序列的适当上下文值。'
- en: For univariate time series, `num_input_channels` dimension should be 1\. For
    multivariate time series, it is greater than 1.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于单变量时间序列，`num_input_channels`维度应为1。对于多变量时间序列，它大于1。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`observed_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`observed_mask`（形状为`(batch_size, sequence_length, num_input_channels)`的`torch.FloatTensor`，*可选*）—
    布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0, 1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`observed`的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`missing`的值（即被零替换的NaN）。
- en: '`return_loss` (`bool`, *optional*) — Whether to return the loss in the `forward`
    call.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss`（`bool`，*可选*）— 是否在`forward`调用中返回损失。'
- en: Returns
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPreTrainingOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPreTrainingOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPreTrainingOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    and inputs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForPreTrainingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含各种元素，取决于配置（[PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig)）和输入。
- en: '`prediction_outputs` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, patch_length)`) — Prediction output from the pretrain head.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_outputs`（形状为`(batch_size, num_input_channels, num_patches, patch_length)`的`torch.FloatTensor`）—
    来自预训练头部的预测输出。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Hidden-states of
    the model at the output of each layer.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*）— 模型在每一层输出的隐藏状态。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, d_model)`) — Backbone embeddings before passing through the head.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_input_channels, num_patches, d_model)`的`torch.FloatTensor`）—
    通过头部之前的主干嵌入。'
- en: '`loss` (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape
    `()`) — Total loss'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（*可选*，在提供`y`时返回，形状为`()`的`torch.FloatTensor`）— 总损失'
- en: The [PatchTSMixerForPretraining](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForPretraining)
    forward method, overrides the `__call__` special method.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[PatchTSMixerForPretraining](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForPretraining)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: PatchTSMixerForRegression
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSMixerForRegression
- en: '### `class transformers.PatchTSMixerForRegression`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSMixerForRegression`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L2001)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L2001)'
- en: '[PRE11]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` (`PatchTSMixerConfig`, *required*) — Configuration.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（`PatchTSMixerConfig`，*必需*）— 配置。'
- en: '`PatchTSMixer` for regression application.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`PatchTSMixer`用于回归应用。'
- en: '#### `forward`'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L2052)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtsmixer/modeling_patchtsmixer.py#L2052)'
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`)
    — Context values of the time series. For a pretraining task, this denotes the
    input time series to predict the masked portion. For a forecasting task, this
    denotes the history/past time series values. Similarly, for classification or
    regression tasks, it denotes the appropriate context values of the time series.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, seq_length, num_input_channels)`的`torch.FloatTensor`）：时间序列的上下文值。对于预训练任务，这表示要预测掩码部分的输入时间序列。对于预测任务，这表示历史/过去的时间序列值。同样，对于分类或回归任务，它表示时间序列的适当上下文值。'
- en: For univariate time series, `num_input_channels` dimension should be 1\. For
    multivariate time series, it is greater than 1.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于单变量时间序列，`num_input_channels`维度应为1。对于多变量时间序列，它大于1。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）：是否返回所有层的隐藏状态。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）：是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)`
    for forecasting, — `(batch_size, num_targets)` for regression, or `(batch_size,)`
    for classification, *optional*): Target values of the time series, that serve
    as labels for the model. The `future_values` is what the Transformer needs during
    training to learn to output, given the `past_values`. Note that, this is NOT required
    for a pretraining task.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values`（形状为`(batch_size, target_len, num_input_channels)`的`torch.FloatTensor`用于预测，形状为`(batch_size,
    num_targets)`用于回归，或形状为`(batch_size,)`用于分类，*可选*）：时间序列的目标值，作为模型的标签。`future_values`是Transformer在训练期间需要的，以便学习在给定`past_values`时输出。请注意，这对于预训练任务是不需要的。'
- en: For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`.
    Even if we want to forecast only specific channels by setting the indices in `prediction_channel_indices`
    parameter, pass the target data with all channels, as channel Filtering for both
    prediction and target will be manually applied before the loss computation.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于一个预测任务，形状应为`(batch_size, target_len, num_input_channels)`。即使我们只想通过在`prediction_channel_indices`参数中设置索引来预测特定通道，也要传递带有所有通道的目标数据，因为在损失计算之前，预测和目标的通道过滤将手动应用。
- en: For a classification task, it has a shape of `(batch_size,)`.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于分类任务，形状为`(batch_size,)`。
- en: For a regression task, it has a shape of `(batch_size, num_targets)`.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于回归任务，形状为`(batch_size, num_targets)`。
- en: '`return_loss` (`bool`, *optional*) — Whether to return the loss in the `forward`
    call.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss`（`bool`，*可选*）：是否在`forward`调用中返回损失。'
- en: Returns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForRegressionOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForRegressionOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForRegressionOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig))
    and inputs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.patchtsmixer.modeling_patchtsmixer.PatchTSMixerForRegressionOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[PatchTSMixerConfig](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig)）和输入的不同元素。
- en: '`prediction_outputs` (`torch.FloatTensor` of shape `(batch_size, num_targets)`)
    — Prediction output from the regression head.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_outputs`（形状为`(batch_size, num_targets)`的`torch.FloatTensor`）：回归头部的预测输出。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, num_input_channels,
    num_patches, d_model)`) — Backbone embeddings before passing through the head.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, num_input_channels, num_patches, d_model)`的`torch.FloatTensor`）：通过头部之前的主干嵌入。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — Hidden-states of
    the model at the output of each layer plus the optional initial embedding outputs.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*）：模型在每一层输出的隐藏状态以及可选的初始嵌入输出。'
- en: '`loss` (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape
    `()`) — Total loss.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（*可选*，在提供`y`时返回，形状为`()`的`torch.FloatTensor`）：总损失。'
- en: The [PatchTSMixerForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForRegression)
    forward method, overrides the `__call__` special method.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[PatchTSMixerForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtsmixer#transformers.PatchTSMixerForRegression)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
