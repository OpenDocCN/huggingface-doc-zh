["```py\naccelerate config\n```", "```py\naccelerate launch your_script.py\n```", "```py\naccelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script\n```", "```py\naccelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_your_script.py --all_arguments_of_the_script\n```", "```py\nds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\nif ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n    with ds_plugin.zero3_init_context_manager(enable=False):\n        sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\nelse:\n    sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)\n```", "```py\nimport torch\nfrom transformers import GPT2Tokenizer\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n\n# 1\\. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# 2\\. define config\nppo_config = {'batch_size': 1, 'learning_rate':1e-5}\nconfig = PPOConfig(**ppo_config)\n\n# 2\\. Create optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\n\n# 3\\. initialize trainer\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)\n```", "```py\nimport torch\nimport bitsandbytes as bnb\n\nfrom transformers import GPT2Tokenizer\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n\n# 1\\. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# 2\\. define config\nppo_config = {'batch_size': 1, 'learning_rate':1e-5}\nconfig = PPOConfig(**ppo_config)\n\n# 2\\. Create optimizer\noptimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)\n\n# 3\\. initialize trainer\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)\n```", "```py\noptimizer = Lion(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.config.learning_rate)\n\n...\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer)\n```", "```py\nimport torch\nfrom transformers import GPT2Tokenizer\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n\n# 1\\. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# 2\\. define config\nppo_config = {'batch_size': 1, 'learning_rate':1e-5}\nconfig = PPOConfig(**ppo_config)\n\n# 2\\. Create optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate)\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n# 3\\. initialize trainer\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer, optimizer=optimizer, lr_scheduler=lr_scheduler)\n```", "```py\nimport torch\nfrom transformers import AutoTokenizer\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n\n# 1\\. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')\nmodel_ref = create_reference_model(model, num_shared_layers=6)\ntokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n\n# 2\\. initialize trainer\nppo_config = {'batch_size': 1}\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n```", "```py\n# 0\\. imports\n# pip install bitsandbytes\nimport torch\nfrom transformers import AutoTokenizer\nfrom trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n\n# 1\\. load a pretrained model\nmodel = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')\nmodel_ref = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m', device_map=\"auto\", load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n\n# 2\\. initialize trainer\nppo_config = {'batch_size': 1}\nconfig = PPOConfig(**ppo_config)\nppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)\n```", "```py\nconfig = PPOConfig(..., optimize_cuda_cache=True)\n```", "```py\nfrom trl import PPOConfig\n\nppo_config = {\n    use_score_scaling=True,\n    use_score_norm=True,\n    score_clip=0.5,\n}\nconfig = PPOConfig(**ppo_config)\n```", "```py\npython examples/scripts/ppo.py --log_with wandb --use_score_scaling --use_score_norm --score_clip 0.5\n```"]