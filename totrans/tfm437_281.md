# 金字塔视觉变换器（PVT）

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pvt)

## 概述

PVT模型由Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao在[金字塔视觉变换器：一种用于密集预测的多功能骨干网络而无需卷积](https://arxiv.org/abs/2102.12122)中提出。PVT是一种利用金字塔结构的视觉变换器，使其成为密集预测任务的有效骨干。具体来说，它允许使用更精细的输入（每个补丁4 x 4像素），同时随着深度的增加缩短变换器的序列长度，从而降低计算成本。此外，还使用了空间缩减注意力（SRA）层，进一步降低学习高分辨率特征时的资源消耗。

论文摘要如下：

*尽管卷积神经网络（CNNs）在计算机视觉领域取得了巨大成功，但本研究探讨了一种简单的、无需卷积的骨干网络，适用于许多密集预测任务。与最近提出的专为图像分类而设计的Vision Transformer（ViT）不同，我们引入了金字塔视觉变换器（PVT），它克服了将Transformer移植到各种密集预测任务的困难。与通常产生低分辨率输出并导致高计算和内存成本的ViT不同，PVT不仅可以在图像的密集分区上进行训练以实现高输出分辨率，这对于密集预测非常重要，而且还使用逐渐缩小的金字塔来减少大特征图的计算量。PVT继承了CNN和Transformer的优点，使其成为各种视觉任务的统一骨干，无需卷积，可以直接替代CNN骨干。我们通过大量实验证实了PVT的有效性，显示它提升了许多下游任务的性能，包括目标检测、实例和语义分割。例如，具有相同参数数量的PVT+RetinaNet在COCO数据集上实现了40.4 AP，超过了ResNet50+RetinNet（36.3 AP）4.1个绝对AP（见图2）。我们希望PVT可以作为像素级预测的替代和有用的骨干，并促进未来的研究。*

此模型由[Xrenya](<[https://huggingface.co/Xrenya](https://huggingface.co/Xrenya))贡献。原始代码可在[此处](https://github.com/whai362/PVT)找到。

+   PVTv1在ImageNet-1K上

| **模型变体** | **大小** | **准确率@1** | **参数（百万）** |
| --- | :-: | :-: | :-: |
| PVT-Tiny | 224 | 75.1 | 13.2 |
| PVT-Small | 224 | 79.8 | 24.5 |
| PVT-Medium | 224 | 81.2 | 44.2 |
| PVT-Large | 224 | 81.7 | 61.4 |

## PvtConfig

### `class transformers.PvtConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/configuration_pvt.py#L37)

```py
( image_size: int = 224 num_channels: int = 3 num_encoder_blocks: int = 4 depths: List = [2, 2, 2, 2] sequence_reduction_ratios: List = [8, 4, 2, 1] hidden_sizes: List = [64, 128, 320, 512] patch_sizes: List = [4, 2, 2, 2] strides: List = [4, 2, 2, 2] num_attention_heads: List = [1, 2, 5, 8] mlp_ratios: List = [8, 8, 4, 4] hidden_act: Mapping = 'gelu' hidden_dropout_prob: float = 0.0 attention_probs_dropout_prob: float = 0.0 initializer_range: float = 0.02 drop_path_rate: float = 0.0 layer_norm_eps: float = 1e-06 qkv_bias: bool = True num_labels: int = 1000 **kwargs )
```

参数

+   `image_size` (`int`, *optional*, defaults to 224) — 输入图像大小

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道数。

+   `num_encoder_blocks` (`int`, *optional*, defaults to 4) — 编码器块的数量（即Mix Transformer编码器中的阶段数）。

+   `depths` (`List[int]`, *optional*, defaults to `[2, 2, 2, 2]`) — 每个编码器块中的层数。

+   `sequence_reduction_ratios` (`List[int]`, *optional*, defaults to `[8, 4, 2, 1]`) — 每个编码器块中的序列缩减比率。

+   `hidden_sizes` (`List[int]`, *optional*, defaults to `[64, 128, 320, 512]`) — 每个编码器块的维度。

+   `patch_sizes` (`List[int]`, *optional*, defaults to `[4, 2, 2, 2]`) — 每个编码器块之前的补丁大小。

+   `strides` (`List[int]`, *optional*, defaults to `[4, 2, 2, 2]`) — 每个编码器块之前的步幅。

+   `num_attention_heads`（`List[int]`，*可选*，默认为`[1, 2, 5, 8]`）— 每个Transformer编码器块中每个注意力层的注意力头数。

+   `mlp_ratios`（`List[int]`，*可选*，默认为`[8, 8, 4, 4]`）— 与Transformer编码器块中输入层大小相比的隐藏层大小比例。

+   `hidden_act`（`str`或`function`，*可选*，默认为`"gelu"`）— 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`，`"relu"`，`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob`（`float`，*可选*，默认为0.0）— 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob`（`float`，*可选*，默认为0.0）— 注意力概率的dropout比率。

+   `initializer_range`（`float`，*可选*，默认为0.02）— 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。

+   `drop_path_rate`（`float`，*可选*，默认为0.0）— 用于Transformer编码器块中随机深度的dropout概率。

+   `layer_norm_eps`（`float`，*可选*，默认为1e-06）— 层归一化层使用的epsilon。

+   `qkv_bias`（`bool`，*可选*，默认为`True`）— 是否应向查询、键和值添加可学习偏置。

+   `num_labels`（`int`，*可选*，默认为1000）— 类别数。

这是一个配置类，用于存储[PvtModel](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtModel)的配置。根据指定的参数实例化Pvt模型，定义模型架构。使用默认值实例化配置将产生类似于Pvt [Xrenya/pvt-tiny-224](https://huggingface.co/Xrenya/pvt-tiny-224)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import PvtModel, PvtConfig

>>> # Initializing a PVT Xrenya/pvt-tiny-224 style configuration
>>> configuration = PvtConfig()

>>> # Initializing a model from the Xrenya/pvt-tiny-224 style configuration
>>> model = PvtModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## PvtImageProcessor

### `class transformers.PvtImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/image_processing_pvt.py#L41)

```py
( do_resize: bool = True size: Optional = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None **kwargs )
```

参数

+   `do_resize`（`bool`，*可选*，默认为`True`）— 是否将图像的（高度，宽度）尺寸调整为指定的（`size["height"]`，`size["width"]`）。可以通过`preprocess`方法中的`do_resize`参数覆盖。

+   `size`（`dict`，*可选*，默认为`{"height" -- 224, "width": 224}`）：调整大小后的输出图像大小。可以通过`preprocess`方法中的`size`参数覆盖。

+   `resample`（`PILImageResampling`，*可选*，默认为`Resampling.BILINEAR`）— 如果调整图像大小，则要使用的重采样滤波器。可以通过`preprocess`方法中的`resample`参数覆盖。

+   `do_rescale`（`bool`，*可选*，默认为`True`）— 是否按指定比例`rescale_factor`重新缩放图像。可以通过`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor`（`int`或`float`，*可选*，默认为`1/255`）— 如果重新缩放图像，则要使用的比例因子。可以通过`preprocess`方法中的`rescale_factor`参数覆盖。

+   `do_normalize`（`bool`，*可选*，默认为`True`）— 是否对图像进行归一化。可以通过`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean`（`float`或`List[float]`，*可选*，默认为`IMAGENET_DEFAULT_MEAN`）— 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_DEFAULT_STD`) — 如果归一化图像，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

构造一个 PVT 图像处理器。

#### `预处理`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/image_processing_pvt.py#L147)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个图像或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 以 `{"height": h, "width": w}` 格式指定调整大小后输出图像的大小的字典。

+   `resample` (`PILImageResampling` 过滤器, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则使用的 `PILImageResampling` 过滤器，例如 `PILImageResampling.BILINEAR`。仅在 `do_resize` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放在 [0 - 1] 之间。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则用于重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 如果 `do_normalize` 设置为 `True`，则使用的图像均值。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 如果 `do_normalize` 设置为 `True`，则使用的图像标准差。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置: 返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回类型为 `tf.Tensor` 的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回类型为 `torch.Tensor` 的批处理。

    +   `TensorType.NUMPY` 或 `'np'`: 返回类型为 `np.ndarray` 的批处理。

    +   `TensorType.JAX` 或 `'jax'`: 返回类型为 `jax.numpy.ndarray` 的批处理。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   未设置: 使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以 (高度, 宽度) 格式。

预处理一个图像或一批图像。

## PvtForImageClassification

### `class transformers.PvtForImageClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/modeling_pvt.py#L582)

```py
( config: PvtConfig )
```

参数

+   `config` ([~PvtConfig](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

Pvt 模型变压器，顶部带有图像分类头（在 [CLS] 令牌的最终隐藏状态之上的线性层），例如用于 ImageNet。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/modeling_pvt.py#L604)

```py
( pixel_values: Optional labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[PvtImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[PvtConfig](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtConfig)）和输入。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类（或回归，如果`config.num_labels==1`）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）— 分类（或回归，如果`config.num_labels==1`）分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每个阶段的输出的隐藏状态（也称为特征图）。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[PvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtForImageClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, PvtForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("Zetatech/pvt-tiny-224")
>>> model = PvtForImageClassification.from_pretrained("Zetatech/pvt-tiny-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## PvtModel

### `class transformers.PvtModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/modeling_pvt.py#L520)

```py
( config: PvtConfig )
```

参数

+   `config`（[~PvtConfig](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸Pvt编码器输出原始隐藏状态，没有特定的头部。这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pvt/modeling_pvt.py#L543)

```py
( pixel_values: FloatTensor output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[PvtImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[PvtConfig](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtConfig)）和输入不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每层输出的一个）。

    模型在每一层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[PvtModel](/docs/transformers/v4.37.2/en/model_doc/pvt#transformers.PvtModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, PvtModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("Zetatech/pvt-tiny-224")
>>> model = PvtModel.from_pretrained("Zetatech/pvt-tiny-224")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 50, 512]
```
