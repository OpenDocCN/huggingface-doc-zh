- en: Informer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Informer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/informer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/informer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/informer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/informer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Informer model was proposed in [Informer: Beyond Efficient Transformer
    for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by
    Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and
    Wancai Zhang.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Informer模型由周浩毅、张尚航、彭杰琦、张帅、李建新、熊辉和张万才在[Informer: Beyond Efficient Transformer
    for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)中提出。'
- en: This method introduces a Probabilistic Attention mechanism to select the “active”
    queries rather than the “lazy” queries and provides a sparse Transformer thus
    mitigating the quadratic compute and memory requirements of vanilla attention.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法引入了一种概率注意机制，用于选择“活跃”查询而不是“懒惰”查询，并提供了一种稀疏Transformer，从而减轻了传统注意力的二次计算和内存需求。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Many real-world applications require the prediction of long sequence time-series,
    such as electricity consumption planning. Long sequence time-series forecasting
    (LSTF) demands a high prediction capacity of the model, which is the ability to
    capture precise long-range dependency coupling between output and input efficiently.
    Recent studies have shown the potential of Transformer to increase the prediction
    capacity. However, there are several severe issues with Transformer that prevent
    it from being directly applicable to LSTF, including quadratic time complexity,
    high memory usage, and inherent limitation of the encoder-decoder architecture.
    To address these issues, we design an efficient transformer-based model for LSTF,
    named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention
    mechanism, which achieves O(L logL) in time complexity and memory usage, and has
    comparable performance on sequences’ dependency alignment. (ii) the self-attention
    distilling highlights dominating attention by halving cascading layer input, and
    efficiently handles extreme long input sequences. (iii) the generative style decoder,
    while conceptually simple, predicts the long time-series sequences at one forward
    operation rather than a step-by-step way, which drastically improves the inference
    speed of long-sequence predictions. Extensive experiments on four large-scale
    datasets demonstrate that Informer significantly outperforms existing methods
    and provides a new solution to the LSTF problem.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*许多现实应用需要预测长序列时间序列，例如电力消耗规划。长序列时间序列预测（LSTF）要求模型具有高预测能力，即能够有效捕捉输出和输入之间精确的长程依赖关系耦合。最近的研究表明Transformer有增加预测能力的潜力。然而，Transformer存在几个严重问题，阻碍了其直接应用于LSTF，包括二次时间复杂度、高内存使用和编码器-解码器架构的固有限制。为了解决这些问题，我们设计了一种高效的基于Transformer的LSTF模型，名为Informer，具有三个独特特征：(i)
    ProbSparse自注意机制，实现了O(L logL)的时间复杂度和内存使用，并在序列的依赖对齐上具有可比性的性能。(ii) 自注意力提取突出主导注意力，通过减半级联层输入，有效处理极长输入序列。(iii)
    生成式解码器，概念上简单，可以一次性预测长时间序列序列，而不是逐步预测，大大提高了长序列预测的推理速度。对四个大规模数据集的广泛实验表明，Informer明显优于现有方法，并为LSTF问题提供了新的解决方案。*'
- en: This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).
    The original code can be found [here](https://github.com/zhouhaoyi/Informer2020).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[elisim](https://huggingface.co/elisim)和[kashif](https://huggingface.co/kashif)贡献。原始代码可在[此处](https://github.com/zhouhaoyi/Informer2020)找到。
- en: Resources
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started. If you’re interested in submitting a resource to be included
    here, please feel free to open a Pull Request and we’ll review it! The resource
    should ideally demonstrate something new instead of duplicating an existing resource.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些官方Hugging Face和社区（由🌎表示）资源，可帮助您入门。如果您有兴趣提交资源以包含在此处，请随时打开Pull Request，我们将进行审查！资源应该展示出一些新东西，而不是重复现有资源。
- en: 'Check out the Informer blog-post in HuggingFace blog: [Multivariate Probabilistic
    Time Series Forecasting with Informer](https://huggingface.co/blog/informer)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在HuggingFace博客中查看Informer博文：[Multivariate Probabilistic Time Series Forecasting
    with Informer](https://huggingface.co/blog/informer)
- en: InformerConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InformerConfig
- en: '### `class transformers.InformerConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/configuration_informer.py#L33)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/configuration_informer.py#L33)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prediction_length` (`int`) — The prediction length for the decoder. In other
    words, the prediction horizon of the model. This value is typically dictated by
    the dataset and we recommend to set it appropriately.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_length` (`int`) — 解码器的预测长度。换句话说，模型的预测范围。这个值通常由数据集决定，我们建议适当设置。'
- en: '`context_length` (`int`, *optional*, defaults to `prediction_length`) — The
    context length for the encoder. If `None`, the context length will be the same
    as the `prediction_length`.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length` (`int`, *optional*, defaults to `prediction_length`) — 编码器的上下文长度。如果为`None`，上下文长度将与`prediction_length`相同。'
- en: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model. Could be either “student_t”, “normal”
    or “negative_binomial”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_output` (`string`, *optional*, defaults to `"student_t"`) — 模型的分布发射头。可以是“student_t”、“normal”或“negative_binomial”之一。'
- en: '`loss` (`string`, *optional*, defaults to `"nll"`) — The loss function for
    the model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (nll) - which currently is the only supported
    one.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`string`, *optional*, defaults to `"nll"`) — 该模型对应`distribution_output`头部的损失函数。对于参数分布，它是负对数似然(nll)
    - 目前是唯一支持的。'
- en: '`input_size` (`int`, *optional*, defaults to 1) — The size of the target variable
    which by default is 1 for univariate targets. Would be > 1 in case of multivariate
    targets.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_size` (`int`, *optional*, defaults to 1) — 目标变量的大小，默认为1表示单变量目标。对于多变量目标，应该大于1。'
- en: '`scaling` (`string` or `bool`, *optional* defaults to `"mean"`) — Whether to
    scale the input targets via “mean” scaler, “std” scaler or no scaler if `None`.
    If `True`, the scaler is set to “mean”.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling` (`string` or `bool`, *optional* defaults to `"mean"`) — 是否通过“mean”缩放器、“std”缩放器或无缩放器（如果为`None`）来缩放输入目标。如果为`True`，则缩放器设置为“mean”。'
- en: '`lags_sequence` (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`)
    — The lags of the input time series as covariates often dictated by the frequency
    of the data. Default is `[1, 2, 3, 4, 5, 6, 7]` but we recommend to change it
    based on the dataset appropriately.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lags_sequence` (`list[int]`, *optional*, defaults to `[1, 2, 3, 4, 5, 6, 7]`)
    — 输入时间序列的滞后作为协变量，通常由数据频率决定。默认为`[1, 2, 3, 4, 5, 6, 7]`，但建议根据数据集适当更改。'
- en: '`num_time_features` (`int`, *optional*, defaults to 0) — The number of time
    features in the input time series.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_time_features` (`int`, *optional*, defaults to 0) — 输入时间序列中的时间特征数量。'
- en: '`num_dynamic_real_features` (`int`, *optional*, defaults to 0) — The number
    of dynamic real valued features.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_dynamic_real_features` (`int`, *optional*, defaults to 0) — 动态实值特征的数量。'
- en: '`num_static_categorical_features` (`int`, *optional*, defaults to 0) — The
    number of static categorical features.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_categorical_features` (`int`, *optional*, defaults to 0) — 静态分类特征的数量。'
- en: '`num_static_real_features` (`int`, *optional*, defaults to 0) — The number
    of static real valued features.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_static_real_features` (`int`, *optional*, defaults to 0) — 静态实值特征的数量。'
- en: '`cardinality` (`list[int]`, *optional*) — The cardinality (number of different
    values) for each of the static categorical features. Should be a list of integers,
    having the same length as `num_static_categorical_features`. Cannot be `None`
    if `num_static_categorical_features` is > 0.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cardinality` (`list[int]`, *optional*) — 静态分类特征的基数（不同值的数量）。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`embedding_dimension` (`list[int]`, *optional*) — The dimension of the embedding
    for each of the static categorical features. Should be a list of integers, having
    the same length as `num_static_categorical_features`. Cannot be `None` if `num_static_categorical_features`
    is > 0.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dimension` (`list[int]`, *optional*) — 每个静态分类特征的嵌入维度。应该是一个整数列表，长度与`num_static_categorical_features`相同。如果`num_static_categorical_features`大于0，则不能为`None`。'
- en: '`d_model` (`int`, *optional*, defaults to 64) — Dimensionality of the transformer
    layers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, defaults to 64) — transformer层的维度。'
- en: '`encoder_layers` (`int`, *optional*, defaults to 2) — Number of encoder layers.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layers` (`int`, *optional*, defaults to 2) — 编码器层数。'
- en: '`decoder_layers` (`int`, *optional*, defaults to 2) — Number of decoder layers.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layers` (`int`, *optional*, defaults to 2) — 解码器层数。'
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attention_heads` (`int`, *optional*, defaults to 2) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_heads` (`int`, *optional*, defaults to 2) — Transformer解码器中每个注意力层的注意力头数。'
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in encoder.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_ffn_dim` (`int`, *optional*, defaults to 32) — 编码器中“中间”（通常称为前馈）层的维度。'
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — Dimension of the “intermediate”
    (often named feed-forward) layer in decoder.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_ffn_dim` (`int`, *optional*, defaults to 32) — 解码器中“中间”（通常称为前馈）层的维度。'
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — The non-linear activation function (function or string) in the encoder and decoder.
    If string, `"gelu"` and `"relu"` are supported.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`)
    — 编码器和解码器中的非线性激活函数（函数或字符串）。如果是字符串，则支持`"gelu"`和`"relu"`。'
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the encoder, and decoder.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, defaults to 0.1) — 编码器和解码器中所有全连接层的dropout概率。'
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each encoder layer.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个编码器层的注意力和全连接层的dropout概率。'
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention and fully connected layers for each decoder layer.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.1) — 每个解码器层的注意力和全连接层的dropout概率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the attention probabilities.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    used between the two layers of the feed-forward networks.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 前馈网络两层之间使用的dropout概率。'
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples to generate in parallel for each time step of inference.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — 每个推断时间步生成的并行样本数量。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, defaults to 0.02) — 截断正态权重初始化分布的标准差。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to use the past
    key/values attentions (if applicable to the model) to speed up decoding.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 是否使用过去的键/值注意力（如果适用于模型）以加速解码。'
- en: '`attention_type` (`str`, *optional*, defaults to “prob”) — Attention used in
    encoder. This can be set to “prob” (Informer’s ProbAttention) or “full” (vanilla
    transformer’s canonical self-attention).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_type` (`str`, *optional*, defaults to “prob”) — 编码器中使用的注意力机制。可以设置为“prob”（Informer的ProbAttention）或“full”（传统transformer的自注意力机制）。'
- en: '`sampling_factor` (`int`, *optional*, defaults to 5) — ProbSparse sampling
    factor (only makes affect when `attention_type`=“prob”). It is used to control
    the reduced query matrix (Q_reduce) input length.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_factor`（`int`，*可选*，默认为5）- ProbSparse采样因子（仅在`attention_type`=“prob”时生效）。它用于控制减少的查询矩阵（Q_reduce）的输入长度。'
- en: '`distil` (`bool`, *optional*, defaults to `True`) — Whether to use distilling
    in encoder.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distil`（`bool`，*可选*，默认为`True`）- 是否在编码器中使用蒸馏。'
- en: This is the configuration class to store the configuration of an [InformerModel](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerModel).
    It is used to instantiate an Informer model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Informer [huggingface/informer-tourism-monthly](https://huggingface.co/huggingface/informer-tourism-monthly)
    architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[InformerModel](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerModel)的配置。根据指定的参数实例化一个Informer模型，定义模型架构。使用默认值实例化配置将产生类似于Informer
    [huggingface/informer-tourism-monthly](https://huggingface.co/huggingface/informer-tourism-monthly)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: InformerModel
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InformerModel
- en: '### `class transformers.InformerModel`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1442)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1442)'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare Informer Model outputting raw hidden-states without any specific head
    on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 裸Informer模型输出原始隐藏状态，没有特定的头部。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前进
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1585)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1585)'
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`) — Past values of the time series,
    that serve as context in order to predict the future. The sequence size of this
    tensor must be larger than the `context_length` of the model, since the model
    will use the larger size to construct lag features, i.e. additional values from
    the past which are added in order to serve as “extra context”.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.FloatTensor`）- 时间序列的过去值，用作上下文以预测未来。此张量的序列大小必须大于模型的`context_length`，因为模型将使用较大的大小来构建滞后特征，即从过去添加的额外值，以充当“额外上下文”。'
- en: The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`,
    which if no `lags_sequence` is configured, is equal to `config.context_length`
    + 7 (as by default, the largest look-back index in `config.lags_sequence` is 7).
    The property `_past_length` returns the actual length of the past.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`sequence_length`等于`config.context_length` + `max(config.lags_sequence)`，如果没有配置`lags_sequence`，则等于`config.context_length`
    + 7（因为默认情况下，`config.lags_sequence`中最大的回溯索引为7）。属性`_past_length`返回过去的实际长度。
- en: The `past_values` is what the Transformer encoder gets as input (with optional
    additional features, such as `static_categorical_features`, `static_real_features`,
    `past_time_features` and lags).
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`past_values`是Transformer编码器作为输入的内容（可选的附加特征，如`static_categorical_features`、`static_real_features`、`past_time_features`和滞后）。'
- en: Optionally, missing values need to be replaced with zeros and indicated via
    the `past_observed_mask`.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选地，缺失值需要用零替换，并通过`past_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，需要`input_size` > 1维，并对应于每个时间步长中时间序列中的变量数量。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`) — Required time features, which the model internally will add
    to `past_values`. These could be things like “month of year”, “day of the month”,
    etc. encoded as vectors (for instance as Fourier features). These could also be
    so-called “age” features, which basically help the model know “at which point
    in life” a time-series is. Age features have small values for distant past time
    steps and increase monotonically the more we approach the current time step. Holiday
    features are also a good example of time features.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`（形状为`(batch_size, sequence_length, num_features)`的`torch.FloatTensor`）-
    必需的时间特征，模型将内部将这些特征添加到`past_values`中。这些可能是“年份的月份”，“月份的日期”等，编码为向量（例如作为傅立叶特征）。这也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假日特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征充当输入的“位置编码”。因此，与像BERT这样的模型相反，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实数协变量连接到此张量中，但这些特征必须在预测时已知。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `past_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）- 用于指示哪些`past_values`被观察到，哪些缺失。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被观察到的数值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`missing`的值为0（即被替换为零的NaN）。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features`（形状为`(batch_size, number of static categorical
    features)`的`torch.LongTensor`，*可选*）- 模型将学习一个嵌入，将这些静态分类特征添加到时间序列的值中。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步的值相同的特征（随时间保持不变）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features`（形状为`(batch_size, number of static real features)`的`torch.FloatTensor`，*可选*）-
    模型将将这些静态实数特征添加到时间序列的值中。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征是所有时间步的值相同的特征（随时间保持不变）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实数特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`
    or `(batch_size, prediction_length, input_size)`, *optional*) — Future values
    of the time series, that serve as labels for the model. The `future_values` is
    what the Transformer needs during training to learn to output, given the `past_values`.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values`（形状为`(batch_size, prediction_length)`或`(batch_size, prediction_length,
    input_size)`的`torch.FloatTensor`，*可选*）- 时间序列的未来值，作为模型的标签。`future_values`是Transformer在训练期间需要学习输出的内容，给定`past_values`。'
- en: The sequence length here is equal to `prediction_length`.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`prediction_length`。
- en: See the demo notebook and code snippets for details.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅演示笔记本和代码片段。
- en: Optionally, during training any missing values need to be replaced with zeros
    and indicated via the `future_observed_mask`.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练期间，任何缺失值都需要用零替换，并通过`future_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，需要`input_size` > 1维度，对应于时间序列每个时间步中的变量数量。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`) — Required time features for the prediction window, which the
    model internally will add to `future_values`. These could be things like “month
    of year”, “day of the month”, etc. encoded as vectors (for instance as Fourier
    features). These could also be so-called “age” features, which basically help
    the model know “at which point in life” a time-series is. Age features have small
    values for distant past time steps and increase monotonically the more we approach
    the current time step. Holiday features are also a good example of time features.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features`（形状为`(batch_size, prediction_length, num_features)`的`torch.FloatTensor`）-
    预测窗口的必需时间特征，模型将内部将这些特征添加到`future_values`中。这些可能是“年份的月份”，“月份的日期”等，编码为向量（例如作为傅立叶特征）。这也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假日特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。因此，与BERT这样的模型不同，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实际协变量连接到此张量中，但必须在预测时已知这些特征。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`future_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `future_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）— 布尔掩码，指示哪些`future_values`被观察到，哪些缺失。掩码值选定在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`已观察到`的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`缺失`的值（即被零替换的NaN）。
- en: This mask is used to filter out missing values for the final loss calculation.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此掩码用于过滤出最终损失计算中的缺失值。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在某些标记索引上执行注意力的掩码。掩码值选定在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示标记`未被掩码`，
- en: 0 for tokens that are `masked`.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    用于避免在某些标记索引上执行注意力的掩码。默认情况下，将使用因果掩码，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使编码器中的注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使解码器中的注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）—
    用于使交叉注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部`未被掩码`，
- en: 0 indicates the head is `masked`.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`掩码`。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包含`last_hidden_state`、`hidden_states`（*可选*）和`attentions`（*可选*）`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`（*可选*）是编码器最后一层的输出的隐藏状态序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size,
    num_heads, encoder_sequence_length, embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后`decoder_input_ids`（即那些没有将它们的过去键值状态提供给此模型的输入）而不是形状为`(batch_size,
    sequence_length)`的所有`decoder_input_ids`。
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([InformerConfig](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerConfig))
    and inputs.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[InformerConfig](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerConfig)）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型解码器最后一层的隐藏状态序列。'
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads,
    sequence_length, embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length,
    embed_size_per_head)`的张量。'
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器在每一层输出处的隐藏状态，以及可选的初始嵌入输出。
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层编码器的隐藏状态以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — 用于给模型输入相同数量级的每个时间序列上下文窗口的偏移值，然后用于将其偏移回原始数量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — 用于给模型输入相同数量级的每个时间序列上下文窗口的缩放值，然后用于将其重新缩放回原始数量级。'
- en: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — 每个时间序列在批处理中的静态特征，在推断时被复制到协变量中。'
- en: The [InformerModel](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[InformerModel](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: InformerForPrediction
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: InformerForPrediction
- en: '### `class transformers.InformerForPrediction`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.InformerForPrediction`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1704)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1704)'
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TimeSeriesTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Informer Model with a distribution head on top for time-series forecasting.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Informer模型在时间序列预测的顶部具有分布头。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1749)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/informer/modeling_informer.py#L1749)'
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`) — Past values of the time series,
    that serve as context in order to predict the future. The sequence size of this
    tensor must be larger than the `context_length` of the model, since the model
    will use the larger size to construct lag features, i.e. additional values from
    the past which are added in order to serve as “extra context”.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.FloatTensor`）- 时间序列的过去值，用作上下文以预测未来。此张量的序列大小必须大于模型的`context_length`，因为模型将使用较大的大小来构建滞后特征，即从过去添加的额外值，以充当“额外上下文”。'
- en: The `sequence_length` here is equal to `config.context_length` + `max(config.lags_sequence)`,
    which if no `lags_sequence` is configured, is equal to `config.context_length`
    + 7 (as by default, the largest look-back index in `config.lags_sequence` is 7).
    The property `_past_length` returns the actual length of the past.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`sequence_length`等于`config.context_length` + `max(config.lags_sequence)`，如果没有配置`lags_sequence`，则等于`config.context_length`
    + 7（默认情况下，`config.lags_sequence`中最大的回顾索引为7）。属性`_past_length`返回过去的实际长度。
- en: The `past_values` is what the Transformer encoder gets as input (with optional
    additional features, such as `static_categorical_features`, `static_real_features`,
    `past_time_features` and lags).
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`past_values`是Transformer编码器作为输入接收的内容（还可以包括额外特征，如`static_categorical_features`、`static_real_features`、`past_time_features`和滞后）。'
- en: Optionally, missing values need to be replaced with zeros and indicated via
    the `past_observed_mask`.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选地，缺失值需要用零替换，并通过`past_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，`input_size` > 1维度是必需的，对应于时间序列中每个时间步的变量数量。
- en: '`past_time_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    num_features)`) — Required time features, which the model internally will add
    to `past_values`. These could be things like “month of year”, “day of the month”,
    etc. encoded as vectors (for instance as Fourier features). These could also be
    so-called “age” features, which basically help the model know “at which point
    in life” a time-series is. Age features have small values for distant past time
    steps and increase monotonically the more we approach the current time step. Holiday
    features are also a good example of time features.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_time_features`（形状为`(batch_size, sequence_length, num_features)`的`torch.FloatTensor`）-
    必需的时间特征，模型将内部添加到`past_values`中。这些可能是诸如“年份的月份”，“月份的日期”等编码为向量的内容（例如作为傅立叶特征）。这也可能是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于远处的过去时间步具有较小的值，并且随着我们接近当前时间步而单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征充当输入的“位置编码”。因此，与像BERT这样的模型相反，BERT模型中的位置编码是从头开始内部学习的模型参数，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实协变量连接到此张量中，但必须在预测时知道这些特征。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `past_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）- 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`observed`的值为1，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`missing`的值为0（即用零替换的NaN）。
- en: '`static_categorical_features` (`torch.LongTensor` of shape `(batch_size, number
    of static categorical features)`, *optional*) — Optional static categorical features
    for which the model will learn an embedding, which it will add to the values of
    the time series.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_categorical_features`（形状为`(batch_size, number of static categorical
    features)`的`torch.LongTensor`，*可选*）- 模型将学习嵌入的可选静态分类特征，然后将其添加到时间序列值中。'
- en: Static categorical features are features which have the same value for all time
    steps (static over time).
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征是所有时间步的值相同的特征（随时间保持不变）。
- en: A typical example of a static categorical feature is a time series ID.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态分类特征的典型示例是时间序列ID。
- en: '`static_real_features` (`torch.FloatTensor` of shape `(batch_size, number of
    static real features)`, *optional*) — Optional static real features which the
    model will add to the values of the time series.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_real_features`（形状为`(batch_size, number of static real features)`的`torch.FloatTensor`，*可选*）-
    模型将添加到时间序列值中的可选静态实特征。'
- en: Static real features are features which have the same value for all time steps
    (static over time).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实特征是所有时间步的值相同的特征（随时间保持不变）。
- en: A typical example of a static real feature is promotion information.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 静态实特征的典型示例是促销信息。
- en: '`future_values` (`torch.FloatTensor` of shape `(batch_size, prediction_length)`
    or `(batch_size, prediction_length, input_size)`, *optional*) — Future values
    of the time series, that serve as labels for the model. The `future_values` is
    what the Transformer needs during training to learn to output, given the `past_values`.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values`（形状为`(batch_size, prediction_length)`或`(batch_size, prediction_length,
    input_size)`的`torch.FloatTensor`，*可选*）- 时间序列的未来值，用作模型的标签。`future_values`是Transformer在训练期间需要学习输出的内容，给定`past_values`。'
- en: The sequence length here is equal to `prediction_length`.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的序列长度等于`prediction_length`。
- en: See the demo notebook and code snippets for details.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看演示笔记本和代码片段以获取详细信息。
- en: Optionally, during training any missing values need to be replaced with zeros
    and indicated via the `future_observed_mask`.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练期间，任何缺失值都需要被替换为零，并通过`future_observed_mask`指示。
- en: For multivariate time series, the `input_size` > 1 dimension is required and
    corresponds to the number of variates in the time series per time step.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多变量时间序列，`input_size` > 1维度是必需的，对应于每个时间步长中时间序列中的变量数。
- en: '`future_time_features` (`torch.FloatTensor` of shape `(batch_size, prediction_length,
    num_features)`) — Required time features for the prediction window, which the
    model internally will add to `future_values`. These could be things like “month
    of year”, “day of the month”, etc. encoded as vectors (for instance as Fourier
    features). These could also be so-called “age” features, which basically help
    the model know “at which point in life” a time-series is. Age features have small
    values for distant past time steps and increase monotonically the more we approach
    the current time step. Holiday features are also a good example of time features.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_time_features`（形状为`(batch_size, prediction_length, num_features)`的`torch.FloatTensor`）-
    预测窗口所需的时间特征，模型内部将这些特征添加到`future_values`中。这些特征可以是诸如“年份的月份”，“月份的日期”等。编码为向量（例如傅立叶特征）。这些也可以是所谓的“年龄”特征，基本上帮助模型知道时间序列处于“生命中的哪个阶段”。年龄特征对于过去的时间步具有较小的值，并且随着我们接近当前时间步，值会单调增加。假期特征也是时间特征的一个很好的例子。'
- en: These features serve as the “positional encodings” of the inputs. So contrary
    to a model like BERT, where the position encodings are learned from scratch internally
    as parameters of the model, the Time Series Transformer requires to provide additional
    time features. The Time Series Transformer only learns additional embeddings for
    `static_categorical_features`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些特征作为输入的“位置编码”。因此，与BERT这样的模型相反，BERT的位置编码是从头开始内部作为模型的参数学习的，时间序列Transformer需要提供额外的时间特征。时间序列Transformer仅为`static_categorical_features`学习额外的嵌入。
- en: Additional dynamic real covariates can be concatenated to this tensor, with
    the caveat that these features must but known at prediction time.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以将额外的动态实际协变量连接到这个张量中，但必须在预测时知道这些特征。
- en: The `num_features` here is equal to `config.`num_time_features`+`config.num_dynamic_real_features`.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的`num_features`等于`config.num_time_features`+`config.num_dynamic_real_features`。
- en: '`future_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`
    or `(batch_size, sequence_length, input_size)`, *optional*) — Boolean mask to
    indicate which `future_values` were observed and which were missing. Mask values
    selected in `[0, 1]`:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_observed_mask`（形状为`(batch_size, sequence_length)`或`(batch_size, sequence_length,
    input_size)`的`torch.BoolTensor`，*可选*）- 布尔掩码，指示哪些`future_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`observed`的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`missing`的值（即被零替换的NaN）。
- en: This mask is used to filter out missing values for the final loss calculation.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此掩码用于过滤出最终损失计算中的缺失值。
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免对某些标记索引执行注意力的掩码。掩码值选在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Mask to avoid performing attention on certain token indices. By
    default, a causal mask will be used, to make sure the model can only look at previous
    inputs in order to predict the future.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    用于避免对某些标记索引执行注意力的掩码。默认情况下，将使用因果掩码，以确保模型只能查看以前的输入以预测未来。'
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）-
    用于将编码器中注意力模块的选定头部置零的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`,
- en: 0 indicates the head is `masked`.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）-
    用于将解码器中注意力模块的选定头部置零的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`,
- en: 0 indicates the head is `masked`.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）-
    用于将交叉注意力模块的选定头部置零的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 indicates the head is `not masked`,
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示头部未被`masked`,
- en: 0 indicates the head is `masked`.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示头部被`masked`。
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（元组（元组（`torch.FloatTensor`）, *可选*）- 元组由`last_hidden_state`、`hidden_states`（*可选*）和`attentions`（*可选*）组成，`last_hidden_state`的形状为`(batch_size,
    sequence_length, hidden_size)`（*可选*）是编码器最后一层的隐藏状态的序列。用于解码器的交叉注意力。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqTSModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqTSModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([InformerConfig](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerConfig))
    and inputs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the decoder of the model.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    模型编码器最后一层的隐藏状态序列。'
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the encoder at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器在每一层输出的隐藏状态，以及可选的初始嵌入输出。
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`loc` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Shift values of each time series’ context window which is used to
    give the model inputs of the same magnitude and then used to shift back to the
    original magnitude.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc`（形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`，*可选*）-
    每个时间序列上下文窗口的偏移值，用于给模型输入相同数量级的值，然后用于将其偏移回原始数量级。'
- en: '`scale` (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`,
    *optional*) — Scaling values of each time series’ context window which is used
    to give the model inputs of the same magnitude and then used to rescale back to
    the original magnitude.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`（形状为`(batch_size,)`或`(batch_size, input_size)`的`torch.FloatTensor`，*可选*）-
    每个时间序列上下文窗口的缩放值，用于给模型输入相同数量级的值，然后用于将其重新缩放回原始数量级。'
- en: '`static_features` (`torch.FloatTensor` of shape `(batch_size, feature size)`,
    *optional*) — Static features of each time series’ in a batch which are copied
    to the covariates at inference time.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`static_features`（形状为`(batch_size, feature size)`的`torch.FloatTensor`，*可选*）-
    批处理中每个时间序列的静态特征，在推断时复制到协变量中。'
- en: The [InformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerForPrediction)
    forward method, overrides the `__call__` special method.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[InformerForPrediction](/docs/transformers/v4.37.2/en/model_doc/informer#transformers.InformerForPrediction)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
