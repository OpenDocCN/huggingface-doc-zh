["```py\n( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 use_cache = True pad_token_id = 0 position_embedding_type = 'absolute' classifier_dropout = None enable_pronunciation = True enable_shape = True pronunciation_embed_dim = 768 pronunciation_vocab_size = 910 shape_embed_dim = 512 shape_vocab_size = 24858 concat_input = True **kwargs )\n```", "```py\n>>> from transformers import RoCBertModel, RoCBertConfig\n\n>>> # Initializing a RoCBert weiweishi/roc-bert-base-zh style configuration\n>>> configuration = RoCBertConfig()\n\n>>> # Initializing a model from the weiweishi/roc-bert-base-zh style configuration\n>>> model = RoCBertModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file word_shape_file word_pronunciation_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None cls_token_id: int = None sep_token_id: int = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n| first sequence    | second sequence |\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( config add_pooling_layer = True )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n>>> model = RoCBertModel.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None attack_input_ids: Optional = None attack_input_shape_ids: Optional = None attack_input_pronunciation_ids: Optional = None attack_attention_mask: Optional = None attack_token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels_input_ids: Optional = None labels_input_shape_ids: Optional = None labels_input_pronunciation_ids: Optional = None labels_attention_mask: Optional = None labels_token_type_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertForPreTraining\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n>>> model = RoCBertForPreTraining.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n\n>>> inputs = tokenizer(\"\u4f60\u597d\uff0c\u5f88\u9ad8\u5174\u8ba4\u8bc6\u4f60\", return_tensors=\"pt\")\n>>> attack_inputs = {}\n>>> for key in list(inputs.keys()):\n...     attack_inputs[f\"attack_{key}\"] = inputs[key]\n>>> label_inputs = {}\n>>> for key in list(inputs.keys()):\n...     label_inputs[f\"labels_{key}\"] = inputs[key]\n\n>>> inputs.update(label_inputs)\n>>> inputs.update(attack_inputs)\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> logits.shape\ntorch.Size([1, 11, 21128])\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None past_key_values: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertForCausalLM, RoCBertConfig\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n>>> config = RoCBertConfig.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n>>> config.is_decoder = True\n>>> model = RoCBertForCausalLM.from_pretrained(\"weiweishi/roc-bert-base-zh\", config=config)\n\n>>> inputs = tokenizer(\"\u4f60\u597d\uff0c\u5f88\u9ad8\u5174\u8ba4\u8bc6\u4f60\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> prediction_logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, RoCBertForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/dummy-rocbert-seq\")\n>>> model = RoCBertForSequenceClassification.from_pretrained(\"ArthurZ/dummy-rocbert-seq\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'financial news'\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = RoCBertForSequenceClassification.from_pretrained(\"ArthurZ/dummy-rocbert-seq\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n2.31\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, RoCBertForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/dummy-rocbert-seq\")\n>>> model = RoCBertForSequenceClassification.from_pretrained(\"ArthurZ/dummy-rocbert-seq\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = RoCBertForSequenceClassification.from_pretrained(\n...     \"ArthurZ/dummy-rocbert-seq\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MultipleChoiceModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertForMultipleChoice\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n>>> model = RoCBertForMultipleChoice.from_pretrained(\"weiweishi/roc-bert-base-zh\")\n\n>>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n>>> choice0 = \"It is eaten with a fork and a knife.\"\n>>> choice1 = \"It is eaten while held in the hand.\"\n>>> labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\n>>> encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n>>> outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n\n>>> # the linear classifier still needs to be trained\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertForTokenClassification\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/dummy-rocbert-ner\")\n>>> model = RoCBertForTokenClassification.from_pretrained(\"ArthurZ/dummy-rocbert-ner\")\n\n>>> inputs = tokenizer(\n...     \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n... )\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_token_class_ids = logits.argmax(-1)\n\n>>> # Note that tokens are classified rather then input words which means that\n>>> # there might be more predicted token classes than words.\n>>> # Multiple token classes might account for the same word\n>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n>>> predicted_tokens_classes\n['S-EVENT', 'S-FAC', 'I-ORDINAL', 'I-ORDINAL', 'E-ORG', 'E-LANGUAGE', 'E-ORG', 'E-ORG', 'E-ORG', 'E-ORG', 'I-EVENT', 'S-TIME', 'S-TIME', 'E-LANGUAGE', 'S-TIME', 'E-DATE', 'I-ORDINAL', 'E-QUANTITY', 'E-LANGUAGE', 'S-TIME', 'B-ORDINAL', 'S-PRODUCT', 'E-LANGUAGE', 'E-LANGUAGE', 'E-ORG', 'E-LOC', 'S-TIME', 'I-ORDINAL', 'S-FAC', 'O', 'S-GPE', 'I-EVENT', 'S-GPE', 'E-LANGUAGE', 'E-ORG', 'S-EVENT', 'S-FAC', 'S-FAC', 'S-FAC', 'E-ORG', 'S-FAC', 'E-ORG', 'S-GPE']\n\n>>> labels = predicted_token_class_ids\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n3.62\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None input_shape_ids: Optional = None input_pronunciation_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.QuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RoCBertForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"ArthurZ/dummy-rocbert-qa\")\n>>> model = RoCBertForQuestionAnswering.from_pretrained(\"ArthurZ/dummy-rocbert-qa\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n''\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n3.75\n```"]