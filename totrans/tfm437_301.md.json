["```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = [512, 864] patch_size = 16 num_channels = 3 qkv_bias = True num_detection_tokens = 100 use_mid_position_embeddings = True auxiliary_loss = False class_cost = 1 bbox_cost = 5 giou_cost = 2 bbox_loss_coefficient = 5 giou_loss_coefficient = 2 eos_coefficient = 0.1 **kwargs )\n```", "```py\n>>> from transformers import YolosConfig, YolosModel\n\n>>> # Initializing a YOLOS hustvl/yolos-base style configuration\n>>> configuration = YolosConfig()\n\n>>> # Initializing a model (with random weights) from the hustvl/yolos-base style configuration\n>>> model = YolosModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( format: Union = <AnnotationFormat.COCO_DETECTION: 'coco_detection'> do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True **kwargs )\n```", "```py\n( images: Union annotations: Union = None return_segmentation_masks: bool = None masks_path: Union = None do_resize: Optional = None size: Optional = None resample = None do_rescale: Optional = None rescale_factor: Union = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None format: Union = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( images: List constant_values: Union = 0 return_pixel_mask: bool = False return_tensors: Union = None data_format: Optional = None input_data_format: Union = None )\n```", "```py\n( outputs threshold: float = 0.5 target_sizes: Union = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( *args **kwargs )\n```", "```py\n( images **kwargs )\n```", "```py\n( images: List constant_values: Union = 0 return_pixel_mask: bool = False return_tensors: Union = None data_format: Optional = None input_data_format: Union = None )\n```", "```py\n( outputs threshold: float = 0.5 target_sizes: Union = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( config: YolosConfig add_pooling_layer: bool = True )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, YolosModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-small\")\n>>> model = YolosModel.from_pretrained(\"hustvl/yolos-small\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 3401, 384]\n```", "```py\n( config: YolosConfig )\n```", "```py\n( pixel_values: FloatTensor labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.yolos.modeling_yolos.YolosObjectDetectionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, AutoModelForObjectDetection\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n>>> model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> # convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n>>> target_sizes = torch.tensor([image.size[::-1]])\n>>> results = image_processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=target_sizes)[\n...     0\n... ]\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected remote with confidence 0.994 at location [46.96, 72.61, 181.02, 119.73]\nDetected remote with confidence 0.975 at location [340.66, 79.19, 372.59, 192.65]\nDetected cat with confidence 0.984 at location [12.27, 54.25, 319.42, 470.99]\nDetected remote with confidence 0.922 at location [41.66, 71.96, 178.7, 120.33]\nDetected cat with confidence 0.914 at location [342.34, 21.48, 638.64, 372.46]\n```"]