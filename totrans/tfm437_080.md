# GPU推理

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_gpu_one)

与CPU不同，GPU是机器学习的标准硬件选择，因为它们针对内存带宽和并行性进行了优化。为了跟上现代模型的更大尺寸或在现有和较旧的硬件上运行这些大型模型，您可以使用几种优化方法来加速GPU推理。在本指南中，您将学习如何使用FlashAttention-2（一种更节省内存的注意力机制）、BetterTransformer（PyTorch本地快速执行路径）和bitsandbytes将模型量化为较低精度。最后，学习如何使用🤗 Optimum在Nvidia和AMD GPU上加速推理。

这里描述的大多数优化也适用于多GPU设置！

## FlashAttention-2

FlashAttention-2是实验性的，未来版本可能会发生较大变化。

[FlashAttention-2](https://huggingface.co/papers/2205.14135)是标准注意力机制的更快、更高效的实现，可以通过以下方式显著加速推理：

1.  此外，可以通过在序列长度上并行化注意力计算来优化

1.  将工作分区在GPU线程之间，以减少它们之间的通信和共享内存读/写

目前支持以下架构的FlashAttention-2：

+   [Bark](https://huggingface.co/docs/transformers/model_doc/bark#transformers.BarkModel)

+   [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)

+   [DistilBert](https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel)

+   [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)

+   [GPTNeo](https://huggingface.co/docs/transformers/model_doc/gpt_neo#transformers.GPTNeoModel)

+   [GPTNeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox#transformers.GPTNeoXModel)

+   [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)

+   [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)

+   [Llava](https://huggingface.co/docs/transformers/model_doc/llava)

+   [VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)

+   [MBart](https://huggingface.co/docs/transformers/model_doc/mbart#transformers.MBartModel)

+   [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)

+   [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)

+   [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)

+   [Phi](https://huggingface.co/docs/transformers/model_doc/phi#transformers.PhiModel)

+   [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)

+   [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)

您可以通过打开GitHub Issue或Pull Request来请求为另一个模型添加FlashAttention-2支持。

在开始之前，请确保已安装FlashAttention-2。

NVIDIAAMD

```py
pip install flash-attn --no-build-isolation
```

我们强烈建议参考详细的[安装说明](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)以了解更多支持的硬件和数据类型！

要启用FlashAttention-2，请将参数`attn_implementation="flash_attention_2"`传递给[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)：

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
```

只有当模型的dtype为`fp16`或`bf16`时，才能使用FlashAttention-2。在使用FlashAttention-2之前，请确保将模型转换为适当的dtype并加载到支持的设备上。

您还可以设置`use_flash_attention_2=True`来启用FlashAttention-2，但已被弃用，推荐使用`attn_implementation="flash_attention_2"`。

FlashAttention-2可以与其他优化技术（如量化）结合，以进一步加速推理。例如，您可以将FlashAttention-2与8位或4位量化结合使用：

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "tiiuae/falcon-7b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 8bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_8bit=True,
    attn_implementation="flash_attention_2",
)

# load in 4bit
model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)
```

### 预期的加速

您可以从推理中获得相当大的加速，特别是对于具有长序列的输入。但是，由于FlashAttention-2不支持使用填充令牌计算注意力分数，因此在序列包含填充令牌时，您必须手动填充/取消填充注意力分数以进行批量推理。这会导致使用填充令牌进行批量生成时出现显着减速。

为了克服这一点，在训练期间应该使用不带填充令牌的FlashAttention-2（通过打包数据集或[连接序列](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)直到达到最大序列长度）。

对于在[tiiuae/falcon-7b](https://hf.co/tiiuae/falcon-7b)上进行单次前向传递，序列长度为4096，各种批量大小且没有填充令牌，预期的加速是：

![](../Images/463d3f3c66f2489865a258a5082f46f7.png)

对于在[meta-llama/Llama-7b-hf](https://hf.co/meta-llama/Llama-7b-hf)上进行单次前向传递，序列长度为4096，各种批量大小且没有填充令牌，预期的加速是：

![](../Images/7ae0be2e7a0a9e0f4d275f8884f4d7d3.png)

对于具有填充令牌的序列（使用填充令牌生成），您需要取消填充/填充输入序列以正确计算注意力分数。对于相对较小的序列长度，单次前向传递会产生额外开销，导致轻微加速（在下面的示例中，输入的30%填充有填充令牌）：

![](../Images/44a86fa9a8504c27decb2bf7133621cb.png)

但是对于更大的序列长度，您可以期望获得更多的加速效益：

FlashAttention更具内存效率，这意味着您可以在更大的序列长度上进行训练，而不会遇到内存不足的问题。对于更大的序列长度，您可以将内存使用量降低多达20倍。查看[flash-attention](https://github.com/Dao-AILab/flash-attention)存储库以获取更多详细信息。

![](../Images/36c674b28857433397883375e3a3644d.png)

## PyTorch缩放点积注意力

PyTorch的[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)（SDPA）也可以在底层调用FlashAttention和内存高效的注意力核。当可用实现时，SDPA支持目前正在Transformers中本地添加，并且在`torch>=2.1.1`时默认用于`torch`。

目前，Transformers支持以下架构的SDPA推理和训练：

+   [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)

+   [GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode#transformers.GPTBigCodeModel)

+   [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)

+   [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)

+   [Idefics](https://huggingface.co/docs/transformers/model_doc/idefics#transformers.IdeficsModel)

+   [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)

+   [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)

+   [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)

+   [Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2#transformers.Qwen2Model)

FlashAttention只能用于具有`fp16`或`bf16` torch类型的模型，因此请确保首先将您的模型转换为适当的类型。

默认情况下，SDPA选择最高效的可用内核，但您可以使用[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)作为上下文管理器来检查在给定设置（硬件、问题大小）中是否有可用的后端：

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", torch_dtype=torch.float16).to("cuda")
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

如果您看到下面的回溯中有错误，请尝试使用PyTorch的夜间版本，这可能对FlashAttention有更广泛的覆盖范围：

```py
RuntimeError: No available kernel. Aborting execution.

# install PyTorch nightly
pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118
```

## BetterTransformer

一些BetterTransformer功能正在被上游到Transformers，支持本机`torch.nn.scaled_dot_product_attention`。BetterTransformer仍然比Transformers SDPA集成具有更广泛的覆盖范围，但您可以期望越来越多的架构在Transformers中本地支持SDPA。

查看我们在[PyTorch 2.0中使用BetterTransformer和缩放点积注意力的开箱即用加速和内存节省](https://pytorch.org/blog/out-of-the-box-acceleration/)中的基准测试，并在[BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2)博客文章中了解更多关于快速执行的信息。

BetterTransformer通过其快速路径（Transformer函数的本机PyTorch专用实现）执行加速推断。快速路径执行中的两个优化是：

1.  融合，将多个连续操作组合成一个单一的“内核”，以减少计算步骤的数量

1.  跳过填充令牌的固有稀疏性，以避免使用嵌套张量进行不必要的计算

BetterTransformer还将所有注意力操作转换为更节省内存的[scaled dot product attention (SDPA)](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)，并在底层调用优化的内核，如[FlashAttention](https://huggingface.co/papers/2205.14135)。

在开始之前，请确保您已安装🤗 Optimum [（已安装）](https://huggingface.co/docs/optimum/installation)。

然后，您可以使用[PreTrainedModel.to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)方法启用BetterTransformer：

```py
model = model.to_bettertransformer()
```

您可以使用[reverse_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.reverse_bettertransformer)方法返回原始的Transformers模型。在保存模型之前，应该使用这个方法来使用规范的Transformers建模：

```py
model = model.reverse_bettertransformer()
model.save_pretrained("saved_model")
```

## bitsandbytes

bitsandbytes是一个包含对4位和8位量化支持的量化库。与其原生全精度版本相比，量化可以减小模型大小，使其更容易适应内存有限的GPU。

确保您已安装bitsandbytes和🤗 Accelerate：

```py
# these versions support 8-bit and 4-bit
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# install Transformers
pip install transformers
```

### 4位

要在4位模型中进行推断，使用`load_in_4bit`参数。`device_map`参数是可选的，但我们建议将其设置为`"auto"`，以便🤗 Accelerate根据环境中的可用资源自动高效地分配模型。

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_4bit=True)
```

要在多个GPU上加载4位模型进行推断，您可以控制要为每个GPU分配多少GPU RAM。例如，将600MB的内存分配给第一个GPU，将1GB的内存分配给第二个GPU：

```py
max_memory_mapping = {0: "600MB", 1: "1GB"}
model_name = "bigscience/bloom-3b"
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_4bit=True, max_memory=max_memory_mapping
)
```

### 8位

如果您对8位量化的概念感兴趣并想了解更多信息，请阅读[Hugging Face Transformers、Accelerate和bitsandbytes使用规模化变压器进行8位矩阵乘法的初步介绍](https://huggingface.co/blog/hf-bitsandbytes-integration)博客文章。

要在8位模型中进行推断，使用`load_in_8bit`参数。`device_map`参数是可选的，但我们建议将其设置为`"auto"`，以便🤗 Accelerate根据环境中的可用资源自动高效地分配模型：

```py
from transformers import AutoModelForCausalLM

model_name = "bigscience/bloom-2b5"
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)
```

如果您要加载8位模型进行文本生成，应该使用[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法，而不是未经优化的[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)函数，后者对8位模型不适用且速度较慢。一些采样策略，如核采样，也不受[Pipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.Pipeline)支持。您还应该将所有输入放在与模型相同的设备上：

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "bigscience/bloom-2b5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True)

prompt = "Hello, my llama is cute"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
```

要在多个GPU上加载4位模型进行推断，您可以控制要为每个GPU分配多少GPU RAM。例如，要将1GB内存分配给第一个GPU，将2GB内存分配给第二个GPU：

```py
max_memory_mapping = {0: "1GB", 1: "2GB"}
model_name = "bigscience/bloom-3b"
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory_mapping
)
```

随意尝试在Google Colab的免费GPU上运行一个拥有110亿参数的[T5模型](https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing)或30亿参数的[BLOOM模型](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing)进行推断！

## 🤗 Optimum

了解有关在[NVIDIA GPU上进行加速推断](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#accelerated-inference-on-nvidia-gpus)和[AMD GPU上进行加速推断](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu#accelerated-inference-on-amd-gpus)的指南中使用ORT的更多详细信息。本节仅提供简要且简单的示例。

ONNX Runtime（ORT）是一个模型加速器，支持在Nvidia GPU和使用[ROCm](https://www.amd.com/en/products/software/rocm.html)堆栈的AMD GPU上进行加速推断。ORT使用优化技术，如将常见操作融合为单个节点和常量折叠，以减少执行的计算量并加快推断速度。ORT还将计算密集型操作放在GPU上，其余操作放在CPU上，智能地在两个设备之间分配工作负载。

ORT受🤗 Optimum支持，可以在🤗 Transformers中使用。您需要使用一个[ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)来解决您的任务，并指定`provider`参数，可以设置为[`CUDAExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#cudaexecutionprovider)、[`ROCMExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu)或[`TensorrtExecutionProvider`](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#tensorrtexecutionprovider)。如果要加载尚未导出为ONNX的模型，可以设置`export=True`将您的模型即时转换为ONNX格式：

```py
from optimum.onnxruntime import ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained(
  "distilbert-base-uncased-finetuned-sst-2-english",
  export=True,
  provider="CUDAExecutionProvider",
)
```

现在您可以自由地使用模型进行推断：

```py
from optimum.pipelines import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

pipeline = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
result = pipeline("Both the music and visual were astounding, not to mention the actors performance.")
```

## 结合优化

通常可以结合上述描述的多种优化技术，以获得最佳的推断性能。例如，您可以加载一个4位模型，然后启用带有FlashAttention的BetterTransformer：

```py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# load model in 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m", quantization_config=quantization_config)

# enable BetterTransformer
model = model.to_bettertransformer()

input_text = "Hello my dog is cute and"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# enable FlashAttention
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
