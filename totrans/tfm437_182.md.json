["```py\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"AI-Sweden-Models/gpt-sw3-356m\")\n>>> model = AutoModelForCausalLM.from_pretrained(\"AI-Sweden-Models/gpt-sw3-356m\")\n\n>>> input_ids = tokenizer(\"Tr\u00e4d \u00e4r fina f\u00f6r att\", return_tensors=\"pt\")[\"input_ids\"]\n\n>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]\n\n>>> print(tokenizer.decode(generated_token_ids))\nTr\u00e4d \u00e4r fina f\u00f6r att de \u00e4r f\u00e4rgstarka. Men ibland \u00e4r det fint\n```", "```py\n>>> from transformers import GPTSw3Tokenizer\n\n>>> tokenizer = GPTSw3Tokenizer.from_pretrained(\"AI-Sweden-Models/gpt-sw3-126m\")\n>>> tokenizer(\"Svenska \u00e4r kul!\")[\"input_ids\"]\n[1814, 377, 3617, 63504]\n```"]