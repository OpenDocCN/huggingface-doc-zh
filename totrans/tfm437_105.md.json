["```py\n!pip install transformers accelerate bitsandbytes optimum\n```", "```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\", device_map=\"auto\", pad_token_id=0)\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\", pad_token_id=0)\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n```", "```py\nprompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\"\n\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```", "```py\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```", "```py\\n\\nThis function takes a single\n```", "```py\ndef bytes_to_giga_bytes(bytes):\n  return bytes / 1024 / 1024 / 1024\n```", "```py\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```", "```py\n29.0260648727417\n```", "```py\ndel pipe\ndel model\n\nimport gc\nimport torch\n\ndef flush():\n  gc.collect()\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n```", "```py\nflush()\n```", "```py\nfrom accelerate.utils import release_memory\n# ...\n\nrelease_memory(model)\n```", "```py\n!pip install bitsandbytes\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_8bit=True, pad_token_id=0)\n```", "```py\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```", "```py\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```", "```py\\n\\nThis function takes a single\n```", "```py\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```", "```py\n15.219234466552734\n```", "```py\ndel model\ndel pipe\n```", "```py\nflush()\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", load_in_4bit=True, low_cpu_mem_usage=True, pad_token_id=0)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nresult = pipe(prompt, max_new_tokens=60)[0][\"generated_text\"][len(prompt):]\nresult\n```", "```py\nHere is a Python function that transforms bytes to Giga bytes:\\n\\n```", "```py\\n\\nThis function takes a single argument\n```", "```py\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```", "```py\n9.543574333190918\n```", "```py\ndel model\ndel pipe\n```", "```py\nflush()\n```", "```py\nsystem_prompt = \"\"\"Below are a series of dialogues between various people and an AI technical assistant.\nThe assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.\nThe assistant is happy to help with code questions and will do their best to understand exactly what is needed.\nIt also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.\nThat said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.\n\nThe Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).\nThe model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.\n\n-----\n\nQuestion: Write a function that takes two lists and returns a list that has alternating elements from each input list.\n\nAnswer: Sure. Here is a function that does that.\n\ndef alternating(list1, list2):\n   results = []\n   for i in range(len(list1)):\n       results.append(list1[i])\n       results.append(list2[i])\n   return results\n\nQuestion: Can you write some test cases for this function?\n\nAnswer: Sure, here are some tests.\n\nassert alternating([10, 20, 30], [1, 2, 3]) == [10, 1, 20, 2, 30, 3]\nassert alternating([True, False], [4, 5]) == [True, 4, False, 5]\nassert alternating([], []) == []\n\nQuestion: Modify the function so that it returns all input elements when the lists have uneven length. The elements from the longer list should be at the end.\n\nAnswer: Here is the modified function.\n\ndef alternating(list1, list2):\n   results = []\n   for i in range(min(len(list1), len(list2))):\n       results.append(list1[i])\n       results.append(list2[i])\n   if len(list1) > len(list2):\n       results.extend(list1[i+1:])\n   else:\n       results.extend(list2[i+1:])\n   return results\n\n-----\n\"\"\"\n```", "```py\nlong_prompt = 10 * system_prompt + prompt\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", torch_dtype=torch.bfloat16, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"bigcode/octocoder\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n```", "```py\nimport time\n\nstart_time = time.time()\nresult = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n\nprint(f\"Generated in {time.time() - start_time} seconds.\")\nresult\n```", "```py\nGenerated in 10.96854019165039 seconds.\nSure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n```", "```py\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```", "```py\n37.668193340301514\n```", "```py\nflush()\n```", "```py\nmodel.to_bettertransformer()\n```", "```py\nstart_time = time.time()\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]\n\nprint(f\"Generated in {time.time() - start_time} seconds.\")\nresult\n```", "```py\nGenerated in 3.0211617946624756 seconds.\n Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n```", "```py\nbytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n```", "```py\n32.617331981658936\n```", "```py\nflush()\n```", "```py\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n  next_logits = model(input_ids)[\"logits\"][:, -1:]\n  next_token_id = torch.argmax(next_logits,dim=-1)\n\n  input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n  print(\"shape of input_ids\", input_ids.shape)\n\ngenerated_text = tokenizer.batch_decode(input_ids[:, -5:])\ngenerated_text\n```", "```py\nshape of input_ids torch.Size([1, 21])\nshape of input_ids torch.Size([1, 22])\nshape of input_ids torch.Size([1, 23])\nshape of input_ids torch.Size([1, 24])\nshape of input_ids torch.Size([1, 25])\n[' Here is a Python function']\n```", "```py\npast_key_values = None # past_key_values is the key-value cache\ngenerated_tokens = []\nnext_token_id = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n\nfor _ in range(5):\n  next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()\n  next_logits = next_logits[:, -1:]\n  next_token_id = torch.argmax(next_logits, dim=-1)\n\n  print(\"shape of input_ids\", next_token_id.shape)\n  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n  generated_tokens.append(next_token_id.item())\n\ngenerated_text = tokenizer.batch_decode(generated_tokens)\ngenerated_text\n```", "```py\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 20\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 21\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 22\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 23\nshape of input_ids torch.Size([1, 1])\nlength of key-value cache 24 [' Here', ' is', ' a', ' Python', ' function']\n```", "```py\nUser: How many people live in France?\nAssistant: Roughly 75 million people live in France\nUser: And how many are in Germany?\nAssistant: Germany has ca. 81 million inhabitants\n```", "```py\n# Generation as usual\nprompt = system_prompt + \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer: Here\"\nmodel_inputs = tokenizer(prompt, return_tensors='pt')\ngeneration_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)\ndecoded_output = tokenizer.batch_decode(generation_output.sequences)[0]\n\n# Piping the returned `past_key_values` to speed up the next conversation round\nprompt = decoded_output + \"\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here\"\nmodel_inputs = tokenizer(prompt, return_tensors='pt')\ngeneration_output = model.generate(\n  **model_inputs,\n  past_key_values=generation_output.past_key_values,\n  max_new_tokens=60,\n  return_dict_in_generate=True\n)\ntokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]\n```", "```py\n is a modified version of the function that returns Mega bytes instead.\n\ndef bytes_to_megabytes(bytes):\n   return bytes / 1024 / 1024\n\nAnswer: The function takes a number of bytes as input and returns the number of\n```", "```py\nconfig = model.config\n2 * 16_000 * config.n_layer * config.n_head * config.n_embd // config.n_head\n```", "```py\n7864320000\n```"]