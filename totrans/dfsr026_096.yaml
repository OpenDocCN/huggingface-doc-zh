- en: Evaluating Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ‰©æ•£æ¨¡å‹
- en: 'Original text: [https://huggingface.co/docs/diffusers/conceptual/evaluation](https://huggingface.co/docs/diffusers/conceptual/evaluation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/diffusers/conceptual/evaluation](https://huggingface.co/docs/diffusers/conceptual/evaluation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![åœ¨Colabä¸­æ‰“å¼€](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb)'
- en: Evaluation of generative models like [Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)
    is subjective in nature. But as practitioners and researchers, we often have to
    make careful choices amongst many different possibilities. So, when working with
    different generative models (like GANs, Diffusion, etc.), how do we choose one
    over the other?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåƒ[Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion)è¿™æ ·çš„ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æ˜¯ä¸»è§‚çš„ã€‚ä½†ä½œä¸ºä»ä¸šè€…å’Œç ”ç©¶äººå‘˜ï¼Œæˆ‘ä»¬ç»å¸¸ä¸å¾—ä¸åœ¨è®¸å¤šä¸åŒçš„å¯èƒ½æ€§ä¹‹é—´åšå‡ºè°¨æ…çš„é€‰æ‹©ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANã€Diffusionç­‰ï¼‰æ—¶ï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©å…¶ä¸­ä¸€ä¸ªè€Œä¸æ˜¯å¦ä¸€ä¸ªå‘¢ï¼Ÿ
- en: Qualitative evaluation of such models can be error-prone and might incorrectly
    influence a decision. However, quantitative metrics donâ€™t necessarily correspond
    to image quality. So, usually, a combination of both qualitative and quantitative
    evaluations provides a stronger signal when choosing one model over the other.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹è¿™äº›æ¨¡å‹çš„å®šæ€§è¯„ä¼°å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œå¹¶å¯èƒ½é”™è¯¯åœ°å½±å“å†³ç­–ã€‚ç„¶è€Œï¼Œå®šé‡æŒ‡æ ‡ä¸ä¸€å®šå¯¹åº”å›¾åƒè´¨é‡ã€‚å› æ­¤ï¼Œé€šå¸¸åœ¨é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è€Œä¸æ˜¯å¦ä¸€ä¸ªæ—¶ï¼Œå®šæ€§å’Œå®šé‡è¯„ä¼°çš„ç»“åˆæä¾›äº†æ›´å¼ºçš„ä¿¡å·ã€‚
- en: In this document, we provide a non-exhaustive overview of qualitative and quantitative
    methods to evaluate Diffusion models. For quantitative methods, we specifically
    focus on how to implement them alongside `diffusers`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…³äºå®šæ€§å’Œå®šé‡æ–¹æ³•è¯„ä¼°æ‰©æ•£æ¨¡å‹çš„éè¯¦å°½æ¦‚è¿°ã€‚å¯¹äºå®šé‡æ–¹æ³•ï¼Œæˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¦‚ä½•å°†å®ƒä»¬ä¸`diffusers`ä¸€èµ·å®æ–½ã€‚
- en: The methods shown in this document can also be used to evaluate different [noise
    schedulers](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)
    keeping the underlying generation model fixed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ¡£ä¸­æ˜¾ç¤ºçš„æ–¹æ³•ä¹Ÿå¯ç”¨äºè¯„ä¼°ä¸åŒçš„[å™ªå£°è°ƒåº¦å™¨](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç”Ÿæˆæ¨¡å‹ä¸å˜ã€‚
- en: Scenarios
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœºæ™¯
- en: 'We cover Diffusion models with the following pipelines:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¶µç›–ä»¥ä¸‹ç®¡é“çš„æ‰©æ•£æ¨¡å‹ï¼š
- en: Text-guided image generation (such as the [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼ˆå¦‚[`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)ï¼‰ã€‚
- en: Text-guided image generation, additionally conditioned on an input image (such
    as the [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img)
    and [`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼Œå¦å¤–è¿˜ä¾èµ–äºè¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚[`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img)å’Œ[`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)ï¼‰ã€‚
- en: Class-conditioned image generation models (such as the [`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ«æ¡ä»¶çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚[`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)ï¼‰ã€‚
- en: Qualitative Evaluation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šæ€§è¯„ä¼°
- en: Qualitative evaluation typically involves human assessment of generated images.
    Quality is measured across aspects such as compositionality, image-text alignment,
    and spatial relations. Common prompts provide a degree of uniformity for subjective
    metrics. DrawBench and PartiPrompts are prompt datasets used for qualitative benchmarking.
    DrawBench and PartiPrompts were introduced by [Imagen](https://imagen.research.google/)
    and [Parti](https://parti.research.google/) respectively.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®šæ€§è¯„ä¼°é€šå¸¸æ¶‰åŠå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œäººç±»è¯„ä¼°ã€‚è´¨é‡é€šè¿‡è¯¸å¦‚ç»„æˆæ€§ã€å›¾åƒæ–‡æœ¬å¯¹é½å’Œç©ºé—´å…³ç³»ç­‰æ–¹é¢æ¥è¡¡é‡ã€‚å¸¸è§æç¤ºä¸ºä¸»è§‚æŒ‡æ ‡æä¾›äº†ä¸€å®šç¨‹åº¦çš„ç»Ÿä¸€æ€§ã€‚DrawBenchå’ŒPartiPromptsæ˜¯ç”¨äºå®šæ€§åŸºå‡†æµ‹è¯•çš„æç¤ºæ•°æ®é›†ã€‚DrawBenchå’ŒPartiPromptsåˆ†åˆ«ç”±[Imagen](https://imagen.research.google/)å’Œ[Parti](https://parti.research.google/)å¼•å…¥ã€‚
- en: 'From the [official Parti website](https://parti.research.google/):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ª[å®˜æ–¹Partiç½‘ç«™](https://parti.research.google/)ï¼š
- en: PartiPrompts (P2) is a rich set of over 1600 prompts in English that we release
    as part of this work. P2 can be used to measure model capabilities across various
    categories and challenge aspects.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: PartiPromptsï¼ˆP2ï¼‰æ˜¯æˆ‘ä»¬ä½œä¸ºè¿™é¡¹å·¥ä½œçš„ä¸€éƒ¨åˆ†å‘å¸ƒçš„ä¸€ç»„è¶…è¿‡1600ä¸ªè‹±æ–‡æç¤ºã€‚P2å¯ç”¨äºè¡¡é‡æ¨¡å‹åœ¨å„ç§ç±»åˆ«å’ŒæŒ‘æˆ˜æ–¹é¢çš„èƒ½åŠ›ã€‚
- en: '![parti-prompts](../Images/a33baa9f457f5cb38b2e24aaacf3265c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts](../Images/a33baa9f457f5cb38b2e24aaacf3265c.png)'
- en: 'PartiPrompts has the following columns:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PartiPromptså…·æœ‰ä»¥ä¸‹åˆ—ï¼š
- en: Prompt
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æç¤º
- en: Category of the prompt (such as â€œAbstractâ€, â€œWorld Knowledgeâ€, etc.)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æç¤ºçš„ç±»åˆ«ï¼ˆå¦‚â€œæŠ½è±¡â€ï¼Œâ€œä¸–ç•ŒçŸ¥è¯†â€ç­‰ï¼‰
- en: Challenge reflecting the difficulty (such as â€œBasicâ€, â€œComplexâ€, â€œWriting &
    Symbolsâ€, etc.)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åæ˜ éš¾åº¦çš„æŒ‘æˆ˜ï¼ˆå¦‚â€œåŸºæœ¬â€ï¼Œâ€œå¤æ‚â€ï¼Œâ€œå†™ä½œä¸ç¬¦å·â€ç­‰ï¼‰
- en: These benchmarks allow for side-by-side human evaluation of different image
    generation models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŸºå‡†å…è®¸å¯¹ä¸åŒå›¾åƒç”Ÿæˆæ¨¡å‹è¿›è¡Œå¹¶æ’äººç±»è¯„ä¼°ã€‚
- en: 'For this, the ğŸ§¨ Diffusers team has built **Open Parti Prompts**, which is a
    community-driven qualitative benchmark based on Parti Prompts to compare state-of-the-art
    open-source diffusion models:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼ŒğŸ§¨ Diffuserså›¢é˜Ÿæ„å»ºäº†**Open Parti Prompts**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºParti Promptsçš„ç¤¾åŒºé©±åŠ¨çš„å®šæ€§åŸºå‡†ï¼Œç”¨äºæ¯”è¾ƒæœ€å…ˆè¿›çš„å¼€æºæ‰©æ•£æ¨¡å‹ï¼š
- en: '[Open Parti Prompts Game](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts):
    For 10 parti prompts, 4 generated images are shown and the user selects the image
    that suits the prompt best.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ‰“å¼€Parti Promptsæ¸¸æˆ](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts)ï¼šå±•ç¤º10ä¸ªpartiæç¤ºï¼Œç”¨æˆ·é€‰æ‹©æœ€é€‚åˆæç¤ºçš„å›¾åƒã€‚'
- en: '[Open Parti Prompts Leaderboard](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard):
    The leaderboard comparing the currently best open-sourced diffusion models to
    each other.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ‰“å¼€Parti Promptsæ’è¡Œæ¦œ](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)ï¼šå°†å½“å‰æœ€ä½³çš„å¼€æºæ‰©æ•£æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚'
- en: To manually compare images, letâ€™s see how we can use `diffusers` on a couple
    of PartiPrompts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰‹åŠ¨æ¯”è¾ƒå›¾åƒï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å‡ ä¸ªPartiPromptsä¸Šä½¿ç”¨`diffusers`ã€‚
- en: 'Below we show some prompts sampled across different challenges: Basic, Complex,
    Linguistic Structures, Imagination, and Writing & Symbols. Here we are using PartiPrompts
    as a [dataset](https://huggingface.co/datasets/nateraw/parti-prompts).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸åŒæŒ‘æˆ˜ä¸­æŠ½æ ·çš„ä¸€äº›æç¤ºï¼šåŸºæœ¬ã€å¤æ‚ã€è¯­è¨€ç»“æ„ã€æƒ³è±¡åŠ›å’Œä¹¦å†™ä¸ç¬¦å·ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨PartiPromptsä½œä¸ºä¸€ä¸ª[æ•°æ®é›†](https://huggingface.co/datasets/nateraw/parti-prompts)ã€‚
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can use these prompts to generate some images using Stable Diffusion
    ([v1-4 checkpoint](https://huggingface.co/CompVis/stable-diffusion-v1-4)):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æç¤ºæ¥ä½¿ç”¨Stable Diffusionï¼ˆ[v1-4æ£€æŸ¥ç‚¹](https://huggingface.co/CompVis/stable-diffusion-v1-4)ï¼‰ç”Ÿæˆä¸€äº›å›¾åƒï¼š
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![parti-prompts-14](../Images/bee6f1ffad12dea358f2f7f0b461f9a4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts-14](../Images/bee6f1ffad12dea358f2f7f0b461f9a4.png)'
- en: 'We can also set `num_images_per_prompt` accordingly to compare different images
    for the same prompt. Running the same pipeline but with a different checkpoint
    ([v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)), yields:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ç›¸åº”åœ°è®¾ç½®`num_images_per_prompt`ä»¥æ¯”è¾ƒç›¸åŒæç¤ºçš„ä¸åŒå›¾åƒã€‚ä½¿ç”¨ä¸åŒæ£€æŸ¥ç‚¹ï¼ˆ[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ï¼‰è¿è¡Œç›¸åŒçš„ç®¡é“ï¼Œäº§ç”Ÿï¼š
- en: '![parti-prompts-15](../Images/8809682b839cee78ffe96b7e472b1f04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![parti-prompts-15](../Images/8809682b839cee78ffe96b7e472b1f04.png)'
- en: Once several images are generated from all the prompts using multiple models
    (under evaluation), these results are presented to human evaluators for scoring.
    For more details on the DrawBench and PartiPrompts benchmarks, refer to their
    respective papers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½¿ç”¨å¤šä¸ªæ¨¡å‹ï¼ˆæ­£åœ¨è¯„ä¼°ä¸­ï¼‰ä»æ‰€æœ‰æç¤ºç”Ÿæˆäº†å‡ å¹…å›¾åƒï¼Œè¿™äº›ç»“æœå°†å‘ˆç°ç»™äººç±»è¯„ä¼°è€…è¿›è¡Œè¯„åˆ†ã€‚æœ‰å…³DrawBenchå’ŒPartiPromptsåŸºå‡†çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒå®ƒä»¬å„è‡ªçš„è®ºæ–‡ã€‚
- en: It is useful to look at some inference samples while a model is training to
    measure the training progress. In our [training scripts](https://github.com/huggingface/diffusers/tree/main/examples/),
    we support this utility with additional support for logging to TensorBoard and
    Weights & Biases.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡å‹è®­ç»ƒæ—¶æŸ¥çœ‹ä¸€äº›æ¨ç†æ ·æœ¬æ˜¯æœ‰ç”¨çš„ï¼Œä»¥è¡¡é‡è®­ç»ƒè¿›åº¦ã€‚åœ¨æˆ‘ä»¬çš„[è®­ç»ƒè„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/)ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒè¿™ç§å®ç”¨ç¨‹åºï¼Œå¹¶é¢å¤–æ”¯æŒè®°å½•åˆ°TensorBoardå’ŒWeights
    & Biasesã€‚
- en: Quantitative Evaluation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šé‡è¯„ä¼°
- en: 'In this section, we will walk you through how to evaluate three different diffusion
    pipelines using:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŒ‡å¯¼æ‚¨å¦‚ä½•ä½¿ç”¨ä»¥ä¸‹å†…å®¹è¯„ä¼°ä¸‰ç§ä¸åŒçš„æ‰©æ•£ç®¡é“ï¼š
- en: CLIP score
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIPåˆ†æ•°
- en: CLIP directional similarity
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIPæ–¹å‘ç›¸ä¼¼æ€§
- en: FID
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FID
- en: Text-guided image generation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆ
- en: '[CLIP score](https://arxiv.org/abs/2104.08718) measures the compatibility of
    image-caption pairs. Higher CLIP scores imply higher compatibility ğŸ”¼. The CLIP
    score is a quantitative measurement of the qualitative concept â€œcompatibilityâ€.
    Image-caption pair compatibility can also be thought of as the semantic similarity
    between the image and the caption. CLIP score was found to have high correlation
    with human judgement.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPåˆ†æ•°](https://arxiv.org/abs/2104.08718)è¡¡é‡äº†å›¾åƒ-æ ‡é¢˜å¯¹çš„å…¼å®¹æ€§ã€‚æ›´é«˜çš„CLIPåˆ†æ•°æ„å‘³ç€æ›´é«˜çš„å…¼å®¹æ€§ğŸ”¼ã€‚CLIPåˆ†æ•°æ˜¯å¯¹å®šæ€§æ¦‚å¿µâ€œå…¼å®¹æ€§â€çš„å®šé‡æµ‹é‡ã€‚å›¾åƒ-æ ‡é¢˜å¯¹çš„å…¼å®¹æ€§ä¹Ÿå¯ä»¥è¢«è§†ä¸ºå›¾åƒå’Œæ ‡é¢˜ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å‘ç°CLIPåˆ†æ•°ä¸äººç±»åˆ¤æ–­å…·æœ‰å¾ˆé«˜çš„ç›¸å…³æ€§ã€‚'
- en: 'Letâ€™s first load a [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸€ä¸ª[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)ï¼š
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Generate some images with multiple prompts:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªæç¤ºç”Ÿæˆä¸€äº›å›¾åƒï¼š
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And then, we calculate the CLIP score.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—CLIPåˆ†æ•°ã€‚
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above example, we generated one image per prompt. If we generated multiple
    images per prompt, we would have to take the average score from the generated
    images per prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆäº†ä¸€å¹…å›¾åƒã€‚å¦‚æœæˆ‘ä»¬ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆå¤šå¹…å›¾åƒï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ä»æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒä¸­å–å¹³å‡åˆ†æ•°ã€‚
- en: 'Now, if we wanted to compare two checkpoints compatible with the [StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)
    we should pass a generator while calling the pipeline. First, we generate images
    with a fixed seed with the [v1-4 Stable Diffusion checkpoint](https://huggingface.co/CompVis/stable-diffusion-v1-4):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ¯”è¾ƒä¸¤ä¸ªä¸[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)å…¼å®¹çš„æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨è°ƒç”¨ç®¡é“æ—¶ä¼ é€’ä¸€ä¸ªç”Ÿæˆå™¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨[v1-4
    Stable Diffusionæ£€æŸ¥ç‚¹](https://huggingface.co/CompVis/stable-diffusion-v1-4)ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we load the [v1-5 checkpoint](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    to generate images:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åŠ è½½[v1-5æ£€æŸ¥ç‚¹](https://huggingface.co/runwayml/stable-diffusion-v1-5)æ¥ç”Ÿæˆå›¾åƒï¼š
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And finally, we compare their CLIP scores:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒå®ƒä»¬çš„CLIPåˆ†æ•°ï¼š
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It seems like the [v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    checkpoint performs better than its predecessor. Note, however, that the number
    of prompts we used to compute the CLIP scores is quite low. For a more practical
    evaluation, this number should be way higher, and the prompts should be diverse.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼¼ä¹[v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)æ£€æŸ¥ç‚¹çš„æ€§èƒ½ä¼˜äºå…¶å‰èº«ã€‚ç„¶è€Œï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç”¨äºè®¡ç®—CLIPåˆ†æ•°çš„æç¤ºæ•°é‡ç›¸å½“ä½ã€‚ä¸ºäº†è¿›è¡Œæ›´å®é™…çš„è¯„ä¼°ï¼Œè¿™ä¸ªæ•°å­—åº”è¯¥æ›´é«˜ï¼Œå¹¶ä¸”æç¤ºåº”è¯¥æ›´åŠ å¤šæ ·åŒ–ã€‚
- en: By construction, there are some limitations in this score. The captions in the
    training dataset were crawled from the web and extracted from `alt` and similar
    tags associated an image on the internet. They are not necessarily representative
    of what a human being would use to describe an image. Hence we had to â€œengineerâ€
    some prompts here.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ„é€ ä¸Šçœ‹ï¼Œè¿™ä¸ªåˆ†æ•°å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚è®­ç»ƒæ•°æ®é›†ä¸­çš„æ ‡é¢˜æ˜¯ä»ç½‘ç»œä¸Šçˆ¬å–çš„ï¼Œå¹¶ä»äº’è”ç½‘ä¸Šä¸å›¾åƒç›¸å…³è”çš„`alt`å’Œç±»ä¼¼æ ‡ç­¾ä¸­æå–çš„ã€‚å®ƒä»¬ä¸ä¸€å®šä»£è¡¨äººç±»ç”¨æ¥æè¿°å›¾åƒçš„æ–¹å¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨è¿™é‡Œâ€œè®¾è®¡â€ä¸€äº›æç¤ºã€‚
- en: Image-conditioned text-to-image generation
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å›¾åƒæ¡ä»¶çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
- en: In this case, we condition the generation pipeline with an input image as well
    as a text prompt. Letâ€™s take the [StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline),
    as an example. It takes an edit instruction as an input prompt and an input image
    to be edited.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆç®¡é“ä¸è¾“å…¥å›¾åƒä»¥åŠæ–‡æœ¬æç¤ºä¸€èµ·è¿›è¡Œæ¡ä»¶åŒ–ã€‚è®©æˆ‘ä»¬ä»¥[StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline)ä¸ºä¾‹ã€‚å®ƒå°†ç¼–è¾‘æŒ‡ä»¤ä½œä¸ºè¾“å…¥æç¤ºï¼Œå¹¶è¾“å…¥è¦ç¼–è¾‘çš„å›¾åƒã€‚
- en: 'Here is one example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªä¾‹å­ï¼š
- en: '![edit-instruction](../Images/dd654c9963d1e235d32cfd4a6a4ce36b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![edit-instruction](../Images/dd654c9963d1e235d32cfd4a6a4ce36b.png)'
- en: One strategy to evaluate such a model is to measure the consistency of the change
    between the two images (in [CLIP](https://huggingface.co/docs/transformers/model_doc/clip)
    space) with the change between the two image captions (as shown in [CLIP-Guided
    Domain Adaptation of Image Generators](https://arxiv.org/abs/2108.00946)). This
    is referred to as the â€**CLIP directional similarity**â€œ.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°è¿™æ ·ä¸€ä¸ªæ¨¡å‹çš„ä¸€ç§ç­–ç•¥æ˜¯æµ‹é‡ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å˜åŒ–çš„ä¸€è‡´æ€§ï¼ˆåœ¨[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)ç©ºé—´ä¸­ï¼‰ä¸ä¸¤ä¸ªå›¾åƒæ ‡é¢˜ä¹‹é—´çš„å˜åŒ–ä¹‹é—´çš„ä¸€è‡´æ€§ï¼ˆå¦‚[CLIPå¼•å¯¼çš„å›¾åƒç”Ÿæˆé¢†åŸŸè‡ªé€‚åº”](https://arxiv.org/abs/2108.00946)æ‰€ç¤ºï¼‰ã€‚è¿™è¢«ç§°ä¸ºâ€œ**CLIPæ–¹å‘ç›¸ä¼¼æ€§**â€ã€‚
- en: Caption 1 corresponds to the input image (image 1) that is to be edited.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡é¢˜1å¯¹åº”äºè¦ç¼–è¾‘çš„è¾“å…¥å›¾åƒï¼ˆå›¾åƒ1ï¼‰ã€‚
- en: Caption 2 corresponds to the edited image (image 2). It should reflect the edit
    instruction.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡é¢˜2å¯¹åº”äºç¼–è¾‘åçš„å›¾åƒï¼ˆå›¾åƒ2ï¼‰ã€‚å®ƒåº”è¯¥åæ˜ ç¼–è¾‘æŒ‡ä»¤ã€‚
- en: 'Following is a pictorial overview:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªå›¾è§£æ¦‚è¿°ï¼š
- en: '![edit-consistency](../Images/4246d7a1ea8f4bbc62b049bedecedc25.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![edit-consistency](../Images/4246d7a1ea8f4bbc62b049bedecedc25.png)'
- en: We have prepared a mini dataset to implement this metric. Letâ€™s first load the
    dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªå°å‹æ•°æ®é›†æ¥å®ç°è¿™ä¸ªæŒ‡æ ‡ã€‚è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®é›†ã€‚
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here we have:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ï¼š
- en: '`input` is a caption corresponding to the `image`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input` æ˜¯ä¸ `image` å¯¹åº”çš„æ ‡é¢˜ã€‚'
- en: '`edit` denotes the edit instruction.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`edit` è¡¨ç¤ºç¼–è¾‘æŒ‡ä»¤ã€‚'
- en: '`output` denotes the modified caption reflecting the `edit` instruction.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output` è¡¨ç¤ºåæ˜  `edit` æŒ‡ä»¤çš„ä¿®æ”¹åæ ‡é¢˜ã€‚'
- en: Letâ€™s take a look at a sample.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç¤ºä¾‹ã€‚
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And here is the image:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å›¾ç‰‡ï¼š
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![edit-dataset](../Images/d57605cd3c5d841cf20ff0af193b5321.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![edit-dataset](../Images/d57605cd3c5d841cf20ff0af193b5321.png)'
- en: We will first edit the images of our dataset with the edit instruction and compute
    the directional similarity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆä½¿ç”¨ç¼–è¾‘æŒ‡ä»¤ç¼–è¾‘æ•°æ®é›†çš„å›¾åƒï¼Œå¹¶è®¡ç®—æ–¹å‘ç›¸ä¼¼æ€§ã€‚
- en: 'Letâ€™s first load the [StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆåŠ è½½[StableDiffusionInstructPix2PixPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pix2pix#diffusers.StableDiffusionInstructPix2PixPipeline)ï¼š
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we perform the edits:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬è¿›è¡Œç¼–è¾‘ï¼š
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To measure the directional similarity, we first load CLIPâ€™s image and text
    encoders:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹é‡æ–¹å‘ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½CLIPçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼š
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that we are using a particular CLIP checkpoint, i.e.,Â `openai/clip-vit-large-patch14`.
    This is because the Stable Diffusion pre-training was performed with this CLIP
    variant. For more details, refer to theÂ [documentation](https://huggingface.co/docs/transformers/model_doc/clip).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ç‰¹å®šçš„CLIPæ£€æŸ¥ç‚¹ï¼Œå³ `openai/clip-vit-large-patch14`ã€‚è¿™æ˜¯å› ä¸ºStable Diffusionçš„é¢„è®­ç»ƒæ˜¯ä½¿ç”¨è¿™ä¸ªCLIPå˜ä½“è¿›è¡Œçš„ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/clip)ã€‚
- en: 'Next, we prepare a PyTorchÂ `nn.Module`Â to compute directional similarity:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªPyTorch `nn.Module` æ¥è®¡ç®—æ–¹å‘ç›¸ä¼¼æ€§ï¼š
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Letâ€™s putÂ `DirectionalSimilarity`Â to use now.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ `DirectionalSimilarity`ã€‚
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Like the CLIP Score, the higher the CLIP directional similarity, the better
    it is.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸CLIPåˆ†æ•°ç±»ä¼¼ï¼ŒCLIPæ–¹å‘ç›¸ä¼¼æ€§è¶Šé«˜ï¼Œæ•ˆæœè¶Šå¥½ã€‚
- en: It should be noted that theÂ `StableDiffusionInstructPix2PixPipeline`Â exposes
    two arguments, namely,Â `image_guidance_scale`Â andÂ `guidance_scale`Â that let you
    control the quality of the final edited image. We encourage you to experiment
    with these two arguments and see the impact of that on the directional similarity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥æ³¨æ„çš„æ˜¯ `StableDiffusionInstructPix2PixPipeline` å…¬å¼€äº†ä¸¤ä¸ªå‚æ•°ï¼Œå³ `image_guidance_scale`
    å’Œ `guidance_scale`ï¼Œè®©æ‚¨æ§åˆ¶æœ€ç»ˆç¼–è¾‘å›¾åƒçš„è´¨é‡ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•è¿™ä¸¤ä¸ªå‚æ•°ï¼Œå¹¶æŸ¥çœ‹å¯¹æ–¹å‘ç›¸ä¼¼æ€§çš„å½±å“ã€‚
- en: We can extend the idea of this metric to measure how similar the original image
    and edited version are. To do that, we can just doÂ `F.cosine_similarity(img_feat_two,
    img_feat_one)`. For these kinds of edits, we would still want the primary semantics
    of the images to be preserved as much as possible, i.e., a high similarity score.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ‰©å±•è¿™ä¸ªæŒ‡æ ‡çš„æ€æƒ³ï¼Œæ¥è¡¡é‡åŸå§‹å›¾åƒå’Œç¼–è¾‘ç‰ˆæœ¬æœ‰å¤šç›¸ä¼¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€æ‰§è¡Œ `F.cosine_similarity(img_feat_two,
    img_feat_one)`ã€‚å¯¹äºè¿™ç§ç±»å‹çš„ç¼–è¾‘ï¼Œæˆ‘ä»¬ä»ç„¶å¸Œæœ›å°½å¯èƒ½ä¿ç•™å›¾åƒçš„ä¸»è¦è¯­ä¹‰ï¼Œå³é«˜ç›¸ä¼¼æ€§åˆ†æ•°ã€‚
- en: We can use these metrics for similar pipelines such as the [`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æŒ‡æ ‡æ¥è¯„ä¼°ç±»ä¼¼çš„æµæ°´çº¿ï¼Œæ¯”å¦‚[`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline)ã€‚
- en: Both CLIP score and CLIP direction similarity rely on the CLIP model, which
    can make the evaluations biased.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CLIPåˆ†æ•°å’ŒCLIPæ–¹å‘ç›¸ä¼¼æ€§éƒ½ä¾èµ–äºCLIPæ¨¡å‹ï¼Œè¿™å¯èƒ½ä¼šä½¿è¯„ä¼°äº§ç”Ÿåè§ã€‚
- en: '***Extending metrics like IS, FID (discussed later), or KID can be difficult***
    when the model under evaluation was pre-trained on a large image-captioning dataset
    (such as the [LAION-5B dataset](https://laion.ai/blog/laion-5b/)). This is because
    underlying these metrics is an InceptionNet (pre-trained on the ImageNet-1k dataset)
    used for extracting intermediate image features. The pre-training dataset of Stable
    Diffusion may have limited overlap with the pre-training dataset of InceptionNet,
    so it is not a good candidate here for feature extraction.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '***å½“è¯„ä¼°çš„æ¨¡å‹åœ¨å¤§å‹å›¾åƒå­—å¹•æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ˆä¾‹å¦‚[LAION-5Bæ•°æ®é›†](https://laion.ai/blog/laion-5b/)ï¼‰æ—¶ï¼Œæ‰©å±•è¯¸å¦‚ISã€FIDï¼ˆç¨åè®¨è®ºï¼‰æˆ–KIDç­‰æŒ‡æ ‡å¯èƒ½ä¼šå¾ˆå›°éš¾ã€‚***è¿™æ˜¯å› ä¸ºè¿™äº›æŒ‡æ ‡çš„åŸºç¡€æ˜¯InceptionNetï¼ˆåœ¨ImageNet-1kæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼‰ç”¨äºæå–ä¸­é—´å›¾åƒç‰¹å¾ã€‚Stable
    Diffusionçš„é¢„è®­ç»ƒæ•°æ®é›†å¯èƒ½ä¸InceptionNetçš„é¢„è®­ç»ƒæ•°æ®é›†æœ‰é™çš„é‡å ï¼Œå› æ­¤åœ¨è¿™é‡Œä¸æ˜¯è¿›è¡Œç‰¹å¾æå–çš„å¥½é€‰æ‹©ã€‚'
- en: '***Using the above metrics helps evaluate models that are class-conditioned.
    For example, [DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit).
    It was pre-trained being conditioned on the ImageNet-1k classes.***'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½¿ç”¨ä¸Šè¿°æŒ‡æ ‡æœ‰åŠ©äºè¯„ä¼°ç±»åˆ«æ¡ä»¶çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œ[DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)ã€‚å®ƒæ˜¯åœ¨ImageNet-1kç±»åˆ«ä¸Šè¿›è¡Œæ¡ä»¶é¢„è®­ç»ƒçš„ã€‚***'
- en: Class-conditioned image generation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç±»åˆ«æ¡ä»¶çš„å›¾åƒç”Ÿæˆ
- en: Class-conditioned generative models are usually pre-trained on a class-labeled
    dataset such as [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k). Popular
    metrics for evaluating these models include FrÃ©chet Inception Distance (FID),
    Kernel Inception Distance (KID), and Inception Score (IS). In this document, we
    focus on FID ([Heusel et al.](https://arxiv.org/abs/1706.08500)). We show how
    to compute it with the [`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit),
    which uses the [DiT model](https://arxiv.org/abs/2212.09748) under the hood.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«æ¡ä»¶ç”Ÿæˆæ¨¡å‹é€šå¸¸æ˜¯åœ¨ç±»åˆ«æ ‡è®°çš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ï¼Œä¾‹å¦‚[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ã€‚ç”¨äºè¯„ä¼°è¿™äº›æ¨¡å‹çš„æµè¡ŒæŒ‡æ ‡åŒ…æ‹¬FrÃ©chet
    Inception Distanceï¼ˆFIDï¼‰ã€Kernel Inception Distanceï¼ˆKIDï¼‰å’ŒInception Scoreï¼ˆISï¼‰ã€‚åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºFIDï¼ˆ[Heuselç­‰äºº](https://arxiv.org/abs/1706.08500)ï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)æ¥è®¡ç®—å®ƒï¼Œè¯¥å·¥å…·åœ¨åº•å±‚ä½¿ç”¨[DiTæ¨¡å‹](https://arxiv.org/abs/2212.09748)ã€‚
- en: 'FID aims to measure how similar are two datasets of images. As per [this resource](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: FIDæ—¨åœ¨è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æ ¹æ®[è¿™ä¸ªèµ„æº](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid)ï¼š
- en: FrÃ©chet Inception Distance is a measure of similarity between two datasets of
    images. It was shown to correlate well with the human judgment of visual quality
    and is most often used to evaluate the quality of samples of Generative Adversarial
    Networks. FID is calculated by computing the FrÃ©chet distance between two Gaussians
    fitted to feature representations of the Inception network.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: FrÃ©chet Inception Distanceæ˜¯è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†ä¹‹é—´ç›¸ä¼¼æ€§çš„æŒ‡æ ‡ã€‚å®ƒè¢«è¯æ˜ä¸äººç±»å¯¹è§†è§‰è´¨é‡çš„åˆ¤æ–­ç›¸å…³ï¼Œå¹¶ä¸”æœ€å¸¸ç”¨äºè¯„ä¼°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ ·æœ¬çš„è´¨é‡ã€‚FIDé€šè¿‡è®¡ç®—é€‚åº”äºInceptionç½‘ç»œç‰¹å¾è¡¨ç¤ºçš„ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»æ¥è®¡ç®—ã€‚
- en: These two datasets are essentially the dataset of real images and the dataset
    of fake images (generated images in our case). FID is usually calculated with
    two large datasets. However, for this document, we will work with two mini datasets.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªæ•°æ®é›†æœ¬è´¨ä¸Šæ˜¯çœŸå®å›¾åƒæ•°æ®é›†å’Œå‡å›¾åƒæ•°æ®é›†ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ç”Ÿæˆçš„å›¾åƒï¼‰ã€‚FIDé€šå¸¸æ˜¯ä½¿ç”¨ä¸¤ä¸ªå¤§å‹æ•°æ®é›†æ¥è®¡ç®—çš„ã€‚ä½†æ˜¯ï¼Œåœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªå°å‹æ•°æ®é›†ã€‚
- en: 'Letâ€™s first download a few images from the ImageNet-1k training set:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆä»ImageNet-1kè®­ç»ƒé›†ä¸­ä¸‹è½½ä¸€äº›å›¾åƒï¼š
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'These are 10 images from the following ImageNet-1k classes: â€œcassette_playerâ€,
    â€œchain_sawâ€ (x2), â€œchurchâ€, â€œgas_pumpâ€ (x3), â€œparachuteâ€ (x2), and â€œtenchâ€.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ¥è‡ªä»¥ä¸‹ImageNet-1kç±»åˆ«çš„10å¼ å›¾åƒï¼šâ€œå¡å¸¦æ’­æ”¾å™¨â€ã€â€œé“¾é”¯â€ï¼ˆx2ï¼‰ã€â€œæ•™å ‚â€ã€â€œåŠ æ²¹æ³µâ€ï¼ˆx3ï¼‰ã€â€œé™è½ä¼â€ï¼ˆx2ï¼‰å’Œâ€œé²‘é±¼â€ã€‚
- en: '![real-images](../Images/f9f0dfbeb754ed8d4f547460644fd7bd.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![çœŸå®å›¾åƒ](../Images/f9f0dfbeb754ed8d4f547460644fd7bd.png)'
- en: '*Real images.*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*çœŸå®å›¾åƒã€‚*'
- en: Now that the images are loaded, letâ€™s apply some lightweight pre-processing
    on them to use them for FID calculation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å›¾åƒå·²åŠ è½½ï¼Œè®©æˆ‘ä»¬å¯¹å®ƒä»¬è¿›è¡Œä¸€äº›è½»é‡çº§é¢„å¤„ç†ï¼Œä»¥ä¾¿ç”¨äºFIDè®¡ç®—ã€‚
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We now load theÂ [`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)
    to generate images conditioned on the above-mentioned classes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åŠ è½½[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)æ¥ç”ŸæˆåŸºäºä¸Šè¿°ç±»åˆ«çš„å›¾åƒã€‚
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we can compute the FID usingÂ [`torchmetrics`](https://torchmetrics.readthedocs.io/).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[`torchmetrics`](https://torchmetrics.readthedocs.io/)æ¥è®¡ç®—FIDã€‚
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The lower the FID, the better it is. Several things can influence FID here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: FIDå€¼è¶Šä½ï¼Œæ•ˆæœè¶Šå¥½ã€‚è¿™é‡Œæœ‰å‡ ä¸ªå› ç´ å¯èƒ½ä¼šå½±å“FIDï¼š
- en: Number of images (both real and fake)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒæ•°é‡ï¼ˆçœŸå®å’Œå‡çš„ï¼‰
- en: Randomness induced in the diffusion process
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥çš„éšæœºæ€§
- en: Number of inference steps in the diffusion process
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ¨ç†æ­¥éª¤æ•°é‡
- en: The scheduler being used in the diffusion process
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è°ƒåº¦ç¨‹åº
- en: For the last two points, it is, therefore, a good practice to run the evaluation
    across different seeds and inference steps, and then report an average result.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæœ€åä¸¤ç‚¹ï¼Œå› æ­¤æœ€å¥½çš„åšæ³•æ˜¯åœ¨ä¸åŒçš„ç§å­å’Œæ¨ç†æ­¥éª¤ä¸Šè¿è¡Œè¯„ä¼°ï¼Œç„¶åæŠ¥å‘Šå¹³å‡ç»“æœã€‚
- en: 'FID results tend to be fragile as they depend on a lot of factors:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FIDçš„ç»“æœå¾€å¾€å¾ˆè„†å¼±ï¼Œå› ä¸ºå®ƒå–å†³äºè®¸å¤šå› ç´ ï¼š
- en: The specific Inception model used during computation.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ç‰¹å®šInceptionæ¨¡å‹ã€‚
- en: The implementation accuracy of the computation.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—çš„å®ç°å‡†ç¡®æ€§ã€‚
- en: The image format (not the same if we start from PNGs vs JPGs).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒæ ¼å¼ï¼ˆå¦‚æœæˆ‘ä»¬ä»PNGå¼€å§‹ä¸ä»JPGå¼€å§‹ä¸åŒï¼‰ã€‚
- en: Keeping that in mind, FID is often most useful when comparing similar runs,
    but it is hard to reproduce paper results unless the authors carefully disclose
    the FID measurement code.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¢è®°è¿™ä¸€ç‚¹ï¼ŒFIDåœ¨æ¯”è¾ƒç›¸ä¼¼çš„è¿è¡Œæ—¶é€šå¸¸æœ€æœ‰ç”¨ï¼Œä½†è¦å¤ç°è®ºæ–‡ç»“æœå¾ˆéš¾ï¼Œé™¤éä½œè€…ä»”ç»†æŠ«éœ²FIDæµ‹é‡ä»£ç ã€‚
- en: These points apply to other related metrics too, such as KID and IS.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›è¦ç‚¹ä¹Ÿé€‚ç”¨äºå…¶ä»–ç›¸å…³æŒ‡æ ‡ï¼Œå¦‚KIDå’ŒISã€‚
- en: As a final step, letâ€™s visually inspect theÂ `fake_images`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæœ€åä¸€æ­¥ï¼Œè®©æˆ‘ä»¬ç›´è§‚åœ°æ£€æŸ¥ä¸€ä¸‹`fake_images`ã€‚
- en: '![fake-images](../Images/d9d2a296bc3c224017b8ff324b7d9c84.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![å‡å›¾åƒ](../Images/d9d2a296bc3c224017b8ff324b7d9c84.png)'
- en: '*Fake images.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‡å›¾åƒã€‚*'
