- en: Pre-tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/tokenizers/api/pre-tokenizers](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    PythonRustNode
  prefs: []
  type: TYPE_NORMAL
- en: BertPreTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.BertPreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: BertPreTokenizer
  prefs: []
  type: TYPE_NORMAL
- en: This pre-tokenizer splits tokens on spaces, and also on punctuation. Each occurence
    of a punctuation character will be treated separately.
  prefs: []
  type: TYPE_NORMAL
- en: ByteLevel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.ByteLevel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) — Whether to add
    a space to the first word if there isn’t already one. This lets us treat *hello*
    exactly like *say hello*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_regex` (`bool`, *optional*, defaults to `True`) — Set this to `False`
    to prevent this *pre_tokenizer* from using the GPT2 specific regexp for spliting
    on whitespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ByteLevel PreTokenizer
  prefs: []
  type: TYPE_NORMAL
- en: This pre-tokenizer takes care of replacing all bytes of the given string with
    a corresponding representation, as well as splitting into words.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `alphabet`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[str]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of characters that compose the alphabet
  prefs: []
  type: TYPE_NORMAL
- en: Returns the alphabet used by this PreTokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Since the ByteLevel works as its name suggests, at the byte level, it encodes
    each byte value to a unique visible character. This means that there is a total
    of 256 different characters composing this alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: CharDelimiterSplit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.CharDelimiterSplit`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This pre-tokenizer simply splits on the provided char. Works like `.split(delimiter)`
  prefs: []
  type: TYPE_NORMAL
- en: Digits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Digits`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`individual_digits` (`bool`, *optional*, defaults to `False`) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pre-tokenizer simply splits using the digits in separate tokens
  prefs: []
  type: TYPE_NORMAL
- en: 'If set to True, digits will each be separated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If set to False, digits will grouped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Metaspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Metaspace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`replacement` (`str`, *optional*, defaults to `▁`) — The replacement character.
    Must be exactly one character. By default we use the *▁* (U+2581) meta symbol
    (Same as in SentencePiece).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_prefix_space` (`bool`, *optional*, defaults to `True`) — Whether to add
    a space to the first word if there isn’t already one. This lets us treat *hello*
    exactly like *say hello*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaspace pre-tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: This pre-tokenizer replaces any whitespace by the provided replacement character.
    It then tries to split on these spaces.
  prefs: []
  type: TYPE_NORMAL
- en: PreTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.PreTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Base class for all pre-tokenizers
  prefs: []
  type: TYPE_NORMAL
- en: This class is not supposed to be instantiated directly. Instead, any implementation
    of a PreTokenizer will return an instance of this class when instantiated.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pre_tokenize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretok` (`~tokenizers.PreTokenizedString) -- The pre-tokenized string on which
    to apply this :class:`~tokenizers.pre_tokenizers.PreTokenizer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-tokenize a `~tokenizers.PyPreTokenizedString` in-place
  prefs: []
  type: TYPE_NORMAL
- en: This method allows to modify a `PreTokenizedString` to keep track of the pre-tokenization,
    and leverage the capabilities of the `PreTokenizedString`. If you just want to
    see the result of the pre-tokenization of a raw string, you can use `pre_tokenize_str()`
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pre_tokenize_str`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` (`str`) — A string to pre-tokeize'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[Tuple[str, Offsets]]`'
  prefs: []
  type: TYPE_NORMAL
- en: A list of tuple with the pre-tokenized parts and their offsets
  prefs: []
  type: TYPE_NORMAL
- en: Pre tokenize the given string
  prefs: []
  type: TYPE_NORMAL
- en: This method provides a way to visualize the effect of a [PreTokenizer](/docs/tokenizers/v0.13.4.rc2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer)
    but it does not keep track of the alignment, nor does it provide all the capabilities
    of the `PreTokenizedString`. If you need some of these, you can use `pre_tokenize()`
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Punctuation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`behavior` (`SplitDelimiterBehavior`) — The behavior to use when splitting.
    Choices: “removed”, “isolated” (default), “merged_with_previous”, “merged_with_next”,
    “contiguous”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pre-tokenizer simply splits on punctuation as individual characters.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Sequence`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This pre-tokenizer composes other pre_tokenizers and applies them in sequence
  prefs: []
  type: TYPE_NORMAL
- en: Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Split`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pattern` (`str` or `Regex`) — A pattern used to split the string. Usually
    a string or a a regex built with *tokenizers.Regex*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`behavior` (`SplitDelimiterBehavior`) — The behavior to use when splitting.
    Choices: “removed”, “isolated”, “merged_with_previous”, “merged_with_next”, “contiguous”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invert` (`bool`, *optional*, defaults to `False`) — Whether to invert the
    pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split PreTokenizer
  prefs: []
  type: TYPE_NORMAL
- en: This versatile pre-tokenizer splits using the provided pattern and according
    to the provided behavior. The pattern can be inverted by making use of the invert
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: UnicodeScripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.UnicodeScripts`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This pre-tokenizer splits on characters that belong to different language family
    It roughly follows [https://github.com/google/sentencepiece/blob/master/data/Scripts.txt](https://github.com/google/sentencepiece/blob/master/data/Scripts.txt)
    Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too. This
    mimicks SentencePiece Unigram implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Whitespace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.Whitespace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This pre-tokenizer simply splits using the following regex: `\w+|[^\w\s]+`'
  prefs: []
  type: TYPE_NORMAL
- en: WhitespaceSplit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class tokenizers.pre_tokenizers.WhitespaceSplit`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This pre-tokenizer simply splits on the whitespace. Works like `.split()`
  prefs: []
  type: TYPE_NORMAL
