["```py\n( vae: Union text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None image: Union = None mask_image: Union = None masked_image_latents: FloatTensor = None height: Optional = None width: Optional = None padding_mask_crop: Optional = None strength: float = 1.0 num_inference_steps: int = 50 timesteps: List = None guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: int = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import PIL\n>>> import requests\n>>> import torch\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionInpaintPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n>>> mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\n>>> init_image = download_image(img_url).resize((512, 512))\n>>> mask_image = download_image(mask_url).resize((512, 512))\n\n>>> pipe = StableDiffusionInpaintPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n>>> image = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```", "```py\n( slice_size: Union = 'auto' )\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n( )\n```", "```py\n( attention_op: Optional = None )\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\n( )\n```", "```py\n( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )\n```", "```py\n( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )\n```", "```py\n( unet: bool = True vae: bool = True )\n```", "```py\n( w embedding_dim = 512 dtype = torch.float32 ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( unet: bool = True vae: bool = True )\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```", "```py\n( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )\n```", "```py\n( prompt_ids: Array mask: Array masked_image: Array params: Union prng_seed: Array num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 latents: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) \u2192 export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import jax\n>>> import numpy as np\n>>> from flax.jax_utils import replicate\n>>> from flax.training.common_utils import shard\n>>> import PIL\n>>> import requests\n>>> from io import BytesIO\n>>> from diffusers import FlaxStableDiffusionInpaintPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n>>> mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\n>>> init_image = download_image(img_url).resize((512, 512))\n>>> mask_image = download_image(mask_url).resize((512, 512))\n\n>>> pipeline, params = FlaxStableDiffusionInpaintPipeline.from_pretrained(\n...     \"xvjiarui/stable-diffusion-2-inpainting\"\n... )\n\n>>> prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n>>> prng_seed = jax.random.PRNGKey(0)\n>>> num_inference_steps = 50\n\n>>> num_samples = jax.device_count()\n>>> prompt = num_samples * [prompt]\n>>> init_image = num_samples * [init_image]\n>>> mask_image = num_samples * [mask_image]\n>>> prompt_ids, processed_masked_images, processed_masks = pipeline.prepare_inputs(\n...     prompt, init_image, mask_image\n... )\n# shard inputs and rng\n\n>>> params = replicate(params)\n>>> prng_seed = jax.random.split(prng_seed, jax.device_count())\n>>> prompt_ids = shard(prompt_ids)\n>>> processed_masked_images = shard(processed_masked_images)\n>>> processed_masks = shard(processed_masks)\n\n>>> images = pipeline(\n...     prompt_ids, processed_masks, processed_masked_images, params, prng_seed, num_inference_steps, jit=True\n... ).images\n>>> images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```", "```py\n( images: ndarray nsfw_content_detected: List )\n```", "```py\n( **updates )\n```"]