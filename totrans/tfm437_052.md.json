["```py\n>>> from transformers import DistilBertConfig\n\n>>> config = DistilBertConfig()\n>>> print(config)\nDistilBertConfig {\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```", "```py\n>>> my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n>>> print(my_config)\nDistilBertConfig {\n  \"activation\": \"relu\",\n  \"attention_dropout\": 0.4,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```", "```py\n>>> my_config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n```", "```py\n>>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n```", "```py\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n```", "```py\n>>> from transformers import DistilBertModel\n\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n>>> model = DistilBertModel(my_config)\n```", "```py\n>>> model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", config=my_config)\n```", "```py\n>>> from transformers import TFDistilBertModel\n\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n>>> tf_model = TFDistilBertModel(my_config)\n```", "```py\n>>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\", config=my_config)\n```", "```py\n>>> from transformers import DistilBertForSequenceClassification\n\n>>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import DistilBertForQuestionAnswering\n\n>>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import TFDistilBertForSequenceClassification\n\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import TFDistilBertForQuestionAnswering\n\n>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import DistilBertTokenizer\n\n>>> my_tokenizer = DistilBertTokenizer(vocab_file=\"my_vocab_file.txt\", do_lower_case=False, padding_side=\"left\")\n```", "```py\n>>> from transformers import DistilBertTokenizer\n\n>>> slow_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import DistilBertTokenizerFast\n\n>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n```", "```py\n>>> from transformers import ViTImageProcessor\n\n>>> vit_extractor = ViTImageProcessor()\n>>> print(vit_extractor)\nViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"size\": 224\n}\n```", "```py\n>>> from transformers import ViTImageProcessor\n\n>>> my_vit_extractor = ViTImageProcessor(resample=\"PIL.Image.BOX\", do_normalize=False, image_mean=[0.3, 0.3, 0.3])\n>>> print(my_vit_extractor)\nViTImageProcessor {\n  \"do_normalize\": false,\n  \"do_resize\": true,\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_mean\": [\n    0.3,\n    0.3,\n    0.3\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": \"PIL.Image.BOX\",\n  \"size\": 224\n}\n```", "```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> w2v2_extractor = Wav2Vec2FeatureExtractor()\n>>> print(w2v2_extractor)\nWav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}\n```", "```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)\n>>> print(w2v2_extractor)\nWav2Vec2FeatureExtractor {\n  \"do_normalize\": false,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 8000\n}\n```", "```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)\n```", "```py\n>>> from transformers import Wav2Vec2CTCTokenizer\n\n>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=\"my_vocab_file.txt\")\n```", "```py\n>>> from transformers import Wav2Vec2Processor\n\n>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```"]