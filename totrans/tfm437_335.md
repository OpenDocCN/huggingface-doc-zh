# BLIP-2

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)

## æ¦‚è¿°

BLIP-2 æ¨¡å‹ç”± Junnan Liã€Dongxu Liã€Silvio Savareseã€Steven Hoi åœ¨[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)ä¸­æå‡ºã€‚BLIP-2 åˆ©ç”¨å†»ç»“çš„é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé€šè¿‡åœ¨å®ƒä»¬ä¹‹é—´è®­ç»ƒä¸€ä¸ªè½»é‡çº§çš„ 12 å±‚ Transformer ç¼–ç å™¨ï¼Œå®ç°äº†å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒBLIP-2 åœ¨é›¶æ ·æœ¬ VQAv2 ä¸Šæ¯”[Flamingo](https://arxiv.org/abs/2204.14198)ï¼ˆä¸€ä¸ª 80 äº¿å‚æ•°æ¨¡å‹ï¼‰æé«˜äº† 8.7%ï¼Œå¹¶ä¸”å¯è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº† 54 å€ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*ç”±äºå¤§è§„æ¨¡æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè§†è§‰-è¯­è¨€é¢„è®­ç»ƒçš„æˆæœ¬å˜å¾—è¶Šæ¥è¶Šé«˜ã€‚æœ¬æ–‡æå‡ºäº† BLIP-2ï¼Œä¸€ç§é€šç”¨ä¸”é«˜æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼Œä»ç°æˆçš„å†»ç»“é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œå†»ç»“å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼•å¯¼è§†è§‰-è¯­è¨€é¢„è®­ç»ƒã€‚BLIP-2 é€šè¿‡è½»é‡çº§çš„ Querying Transformer æ¶ˆé™¤äº†æ¨¡æ€å·®å¼‚ï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸¤ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µä»å†»ç»“å›¾åƒç¼–ç å™¨å¼•å¯¼è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚ç¬¬äºŒé˜¶æ®µä»å†»ç»“è¯­è¨€æ¨¡å‹å¼•å¯¼è§†è§‰-è¯­è¨€ç”Ÿæˆå­¦ä¹ ã€‚å°½ç®¡å¯è®­ç»ƒå‚æ•°æ•°é‡æ˜æ˜¾å°‘äºç°æœ‰æ–¹æ³•ï¼Œä½† BLIP-2 åœ¨å„ç§è§†è§‰-è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬ VQAv2 ä¸Šæ¯” Flamingo80B æé«˜äº† 8.7%ï¼Œå¹¶ä¸”å¯è®­ç»ƒå‚æ•°æ•°é‡å‡å°‘äº† 54 å€ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ¨¡å‹çš„æ–°å…´èƒ½åŠ›ï¼Œå³é›¶æ ·æœ¬å›¾åƒåˆ°æ–‡æœ¬ç”Ÿæˆï¼Œå¯ä»¥éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚*

![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 æ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2301.12597)

æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   BLIP-2 å¯ç”¨äºåœ¨ç»™å®šå›¾åƒå’Œå¯é€‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹è¿›è¡Œæ¡ä»¶æ–‡æœ¬ç”Ÿæˆã€‚åœ¨æ¨ç†æ—¶ï¼Œå»ºè®®ä½¿ç”¨ `generate` æ–¹æ³•ã€‚

+   å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)æ¥ä¸ºæ¨¡å‹å‡†å¤‡å›¾åƒï¼Œå¹¶å°†é¢„æµ‹çš„æ ‡è®°IDè§£ç å›æ–‡æœ¬ã€‚

## èµ„æº

å®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ BLIP-2ã€‚

+   BLIP-2 çš„æ¼”ç¤ºç¬”è®°æœ¬ç”¨äºå›¾åƒå­—å¹•ã€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œç±»ä¼¼å¯¹è¯çš„ä¼šè¯ï¼Œå¯åœ¨[æ­¤å¤„](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2)æ‰¾åˆ°ã€‚

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æäº¤æ‹‰å–è¯·æ±‚ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## Blip2Config

### `class transformers.Blip2Config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)

```py
( vision_config = None qformer_config = None text_config = None num_query_tokens = 32 **kwargs )
```

å‚æ•°

+   `vision_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–[Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `qformer_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–[Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `text_config` (`dict`, *å¯é€‰*) â€” ç”¨äºåˆå§‹åŒ–ä»»ä½•[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„é…ç½®é€‰é¡¹å­—å…¸ã€‚

+   `num_query_tokens` (`int`, *optional*, defaults to 32) â€” é€šè¿‡Transformerä¼ é€’çš„æŸ¥è¯¢ä»¤ç‰Œæ•°é‡ã€‚

+   `kwargs` (*optional*) â€” å…³é”®å­—å‚æ•°çš„å­—å…¸ã€‚

[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)æ˜¯ç”¨äºå­˜å‚¨[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªBLIP-2æ¨¡å‹ï¼Œå®šä¹‰è§†è§‰æ¨¡å‹ã€Q-Formeræ¨¡å‹å’Œè¯­è¨€æ¨¡å‹é…ç½®ã€‚ä½¿ç”¨é»˜è®¤é…ç½®å®ä¾‹åŒ–å°†äº§ç”Ÿç±»ä¼¼äºBLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import (
...     Blip2VisionConfig,
...     Blip2QFormerConfig,
...     OPTConfig,
...     Blip2Config,
...     Blip2ForConditionalGeneration,
... )

>>> # Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2Config()

>>> # Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2ForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig

>>> # Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations
>>> vision_config = Blip2VisionConfig()
>>> qformer_config = Blip2QFormerConfig()
>>> text_config = OPTConfig()

>>> config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)
```

#### `from_vision_qformer_text_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)

```py
( vision_config: Blip2VisionConfig qformer_config: Blip2QFormerConfig text_config: PretrainedConfig **kwargs ) â†’ export const metadata = 'undefined';Blip2Config
```

è¿”å›

[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)

é…ç½®å¯¹è±¡çš„å®ä¾‹

ä»BLIP-2è§†è§‰æ¨¡å‹ã€Q-Formerå’Œè¯­è¨€æ¨¡å‹é…ç½®ä¸­å®ä¾‹åŒ–ä¸€ä¸ª[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)ï¼ˆæˆ–æ´¾ç”Ÿç±»ï¼‰ã€‚

## Blip2VisionConfig

### `class transformers.Blip2VisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)

```py
( hidden_size = 1408 intermediate_size = 6144 num_hidden_layers = 39 num_attention_heads = 16 image_size = 224 patch_size = 14 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True **kwargs )
```

å‚æ•°

+   `hidden_size` (`int`, *optional*, defaults to 1408) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 6144) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 39) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 16) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°é‡ã€‚

+   `image_size` (`int`, *optional*, defaults to 224) â€” æ¯ä¸ªå›¾åƒçš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `patch_size` (`int`, *optional*, defaults to 14) â€” æ¯ä¸ªè¡¥ä¸çš„å¤§å°ï¼ˆåˆ†è¾¨ç‡ï¼‰ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"` `"gelu"`ã€‚layer_norm_eps (`float`, *optional*, defaults to 1e-5): å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ä¸ºæŸ¥è¯¢å’Œå€¼æ·»åŠ åç½®ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)é…ç½®çš„é…ç½®ç±»ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªBLIP-2è§†è§‰ç¼–ç å™¨ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚å®ä¾‹åŒ–é»˜è®¤é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºBLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Blip2VisionConfig, Blip2VisionModel

>>> # Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2VisionConfig()

>>> # Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2VisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Blip2QFormerConfig

### `class transformers.Blip2QFormerConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' cross_attention_frequency = 2 encoder_hidden_size = 1408 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 30522) â€” Q-Former æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰åœ¨è°ƒç”¨æ¨¡å‹æ—¶ä¼ é€’çš„ `inputs_ids` å¯è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 12) â€” æ¯ä¸ªæ³¨æ„åŠ›å±‚ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆé€šå¸¸ç§°ä¸ºå‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ `"gelu"`ã€`"relu"`ã€`"silu"` å’Œ `"gelu_new"`ã€‚

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å…¨è¿æ¥å±‚çš„ dropout æ¦‚ç‡ã€‚

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ dropout æ¯”ç‡ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) â€” æ­¤æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é€šå¸¸è®¾ç½®ä¸ºè¾ƒå¤§çš„å€¼ä»¥é˜²ä¸‡ä¸€ï¼ˆä¾‹å¦‚ï¼Œ512ã€1024æˆ–2048ï¼‰ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) â€” ä½ç½®åµŒå…¥çš„ç±»å‹ã€‚é€‰æ‹© `"absolute"`ã€`"relative_key"`ã€`"relative_key_query"` ä¸­çš„ä¸€ä¸ªã€‚å¯¹äºä½ç½®åµŒå…¥ï¼Œè¯·ä½¿ç”¨ `"absolute"`ã€‚æœ‰å…³ `"relative_key"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)ã€‚æœ‰å…³ `"relative_key_query"` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658) ä¸­çš„ *Method 4*ã€‚

+   `cross_attention_frequency` (`int`, *optional*, defaults to 2) â€” å‘ Transformer å±‚æ·»åŠ äº¤å‰æ³¨æ„åŠ›çš„é¢‘ç‡ã€‚

+   `encoder_hidden_size` (`int`, *optional*, defaults to 1408) â€” äº¤å‰æ³¨æ„åŠ›ä¸­éšè—çŠ¶æ€çš„éšè—å¤§å°ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª BLIP-2 Querying Transformer (Q-Former) æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) æ¶æ„çš„é…ç½®ã€‚é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

è¯·æ³¨æ„ï¼Œ[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)ä¸[BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)éå¸¸ç›¸ä¼¼ï¼Œå…·æœ‰äº¤é”™çš„äº¤å‰æ³¨æ„åŠ›ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Blip2QFormerConfig, Blip2QFormerModel

>>> # Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2QFormerConfig()

>>> # Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2QFormerModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## Blip2Processor

### `class transformers.Blip2Processor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)

```py
( image_processor tokenizer )
```

å‚æ•°

+   `image_processor`ï¼ˆ`BlipImageProcessor`ï¼‰â€” [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)çš„ä¸€ä¸ªå®ä¾‹ã€‚å›¾åƒå¤„ç†å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ`AutoTokenizer`ï¼‰â€” [â€˜PreTrainedTokenizerâ€™]çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ªBLIP-2å¤„ç†å™¨ï¼Œå°†BLIPå›¾åƒå¤„ç†å™¨å’ŒOPT/T5åˆ†è¯å™¨å°è£…åˆ°ä¸€ä¸ªå¤„ç†å™¨ä¸­ã€‚

[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)æä¾›äº†[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)å’Œ[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)çš„æ‰€æœ‰åŠŸèƒ½ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…`__call__()`å’Œ[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

è§£ç 

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†å…¶æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

## Blip2VisionModel

### `class transformers.Blip2VisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)

```py
( config: Blip2VisionConfig )
```

å‰è¿›

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å¾—ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, hidden_size)`ï¼‰- ç»è¿‡ç”¨äºè¾…åŠ©é¢„è®­ç»ƒä»»åŠ¡çš„å±‚è¿›ä¸€æ­¥å¤„ç†åï¼Œåºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰çš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚ä¾‹å¦‚ï¼Œå¯¹äºBERTç³»åˆ—æ¨¡å‹ï¼Œè¿™è¿”å›ç»è¿‡çº¿æ€§å±‚å’Œtanhæ¿€æ´»å‡½æ•°å¤„ç†åçš„åˆ†ç±»æ ‡è®°ã€‚çº¿æ€§å±‚çš„æƒé‡æ˜¯åœ¨é¢„è®­ç»ƒæœŸé—´ä»ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹ï¼ˆåˆ†ç±»ï¼‰ç›®æ ‡ä¸­è®­ç»ƒçš„ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## Blip2QFormerModel

### `class transformers.Blip2QFormerModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)

```py
( config: Blip2QFormerConfig )
```

æŸ¥è¯¢å˜å‹å™¨ï¼ˆQ-Formerï¼‰ï¼Œç”¨äºBLIP-2ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)

```py
( query_embeds: FloatTensor attention_mask: Optional = None head_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

encoder_hidden_statesï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ`å¯é€‰`ï¼‰ï¼šç¼–ç å™¨æœ€åä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åºåˆ—ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚encoder_attention_maskï¼ˆ`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ`å¯é€‰`ï¼‰ï¼šé¿å…å¯¹ç¼–ç å™¨è¾“å…¥çš„å¡«å……æ ‡è®°ç´¢å¼•æ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚å¦‚æœæ¨¡å‹é…ç½®ä¸ºè§£ç å™¨ï¼Œåˆ™åœ¨äº¤å‰æ³¨æ„åŠ›ä¸­ä½¿ç”¨ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

+   å¯¹äºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ä¸º1ï¼Œ

+   å¯¹äºè¢«â€œæ©ç›–â€çš„æ ‡è®°ä¸º0ã€‚past_key_valuesï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰4ä¸ªå¼ é‡ï¼šå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`ï¼‰ï¼šåŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚use_cacheï¼ˆ`bool`ï¼Œ`å¯é€‰`ï¼‰ï¼šå¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

## Blip2Model

### `class transformers.Blip2Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)

```py
( config: Blip2Config )
```

å‚æ•°

+   `config`ï¼ˆ[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ç”¨äºç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒç‰¹å¾çš„BLIP-2æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±è§†è§‰ç¼–ç å™¨ã€æŸ¥è¯¢å˜æ¢å™¨ï¼ˆQ-Formerï¼‰å’Œè¯­è¨€æ¨¡å‹ç»„æˆã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)

```py
( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚è¾“å…¥æ ‡è®°å¯ä»¥é€‰æ‹©æ€§åœ°æä¾›ä½œä¸ºæ–‡æœ¬æç¤ºï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ã€‚

    å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1 ç”¨äºâ€œæœªæ©ç â€æ ‡è®°çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äºâ€œæ©ç â€æ ‡è®°çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥IDï¼Ÿ](../glossary#decoder-input-ids)

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥`decoder_input_ids`ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput` æˆ– `tuple(torch.FloatTensor)`

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput` æˆ– `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`ï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾› `labels` æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º `(1,)` çš„ `torch.FloatTensor`ï¼‰â€” è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)` çš„ `torch.FloatTensor`ï¼‰â€” è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ã€‚

+   `vision_outputs`ï¼ˆ`BaseModelOutputWithPooling`ï¼‰â€” è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºã€‚

+   `qformer_outputs`ï¼ˆ`BaseModelOutputWithPoolingAndCrossAttentions`ï¼‰â€” Q-Formerï¼ˆQuerying Transformerï¼‰çš„è¾“å‡ºã€‚

+   `language_model_outputs`ï¼ˆ`CausalLMOutputWithPast` æˆ– `Seq2SeqLMOutput`ï¼‰â€” è¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2Model
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
>>> model.to(device)
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> prompt = "Question: how many cats are there? Answer:"
>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device, torch.float16)

>>> outputs = model(**inputs)
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';text_outputs (CausalLMOutputWithPast, or tuple(torch.FloatTensor) if return_dict=False)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.LongTensor`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[ä»€ä¹ˆæ˜¯è¾“å…¥ IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š

    +   å¯¹äº `æœªè¢«æ©ç›–` çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äº `è¢«æ©ç›–` çš„æ ‡è®°ä¸º 0ã€‚[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, target_sequence_length)` çš„ `torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è¯æ±‡è¡¨ä¸­è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) å’Œ [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è§£ç å™¨è¾“å…¥ IDï¼Ÿ](../glossary#decoder-input-ids)

    T5 ä½¿ç”¨ `pad_token_id` ä½œä¸º `decoder_input_ids` ç”Ÿæˆçš„èµ·å§‹æ ‡è®°ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™å¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„ `decoder_input_ids`ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚

    è¦äº†è§£æœ‰å…³å¦‚ä½•ä¸ºé¢„è®­ç»ƒå‡†å¤‡ `decoder_input_ids` çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [T5 Training](./t5#training)ã€‚

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, target_sequence_length)` çš„ `torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¿½ç•¥ `decoder_input_ids` ä¸­å¡«å……æ ‡è®°çš„å¼ é‡ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸­çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

text_outputs (`CausalLMOutputWithPast`ï¼Œæˆ–è€…å¦‚æœ`return_dict=False`åˆ™ä¸º`tuple(torch.FloatTensor)`)

è¯­è¨€æ¨¡å‹è¾“å‡ºã€‚å¦‚æœ`return_dict=True`ï¼Œåˆ™è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«è¯­è¨€æ¨¡å‹logitsã€è¿‡å»çš„é”®å€¼å’Œéšè—çŠ¶æ€ï¼ˆå¦‚æœ`output_hidden_states=True`ï¼‰çš„`CausalLMOutputWithPast`ã€‚

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoTokenizer, Blip2Model

>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> inputs = tokenizer(["a photo of a cat"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

vision_outputs (`BaseModelOutputWithPooling`æˆ–`torch.FloatTensor`çš„å…ƒç»„ï¼‰

è§†è§‰æ¨¡å‹è¾“å‡ºã€‚å¦‚æœ`return_dict=True`ï¼Œåˆ™è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«å›¾åƒç‰¹å¾ã€æ± åŒ–å›¾åƒç‰¹å¾å’Œéšè—çŠ¶æ€ï¼ˆå¦‚æœ`output_hidden_states=True`ï¼‰çš„`BaseModelOutputWithPooling`ã€‚

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Blip2Model

>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> image_outputs = model.get_image_features(**inputs)
```

#### `get_qformer_features`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`) â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

+   `input_ids` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” è¯­è¨€æ¨¡å‹è¯æ±‡ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚å¯ä»¥æä¾›è¾“å…¥æ ‡è®°ä½œä¸ºæ–‡æœ¬æç¤ºï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ç”Ÿæˆã€‚

    å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`Blip2Processor.__call__()`ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   å¯¹äº`not masked`çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äº`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯­è¨€æ¨¡å‹è¯æ±‡ä¸­çš„ç´¢å¼•ã€‚ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)æ¥è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚[è§£ç å™¨è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#decoder-input-ids)

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

è¿”å›

vision_outputsï¼ˆ`BaseModelOutputWithPooling`æˆ–`torch.FloatTensor`å…ƒç»„ï¼‰

è§†è§‰æ¨¡å‹è¾“å‡ºã€‚å¦‚æœ`return_dict=True`ï¼Œåˆ™è¾“å‡ºæ˜¯åŒ…å«å›¾åƒç‰¹å¾ã€æ± åŒ–å›¾åƒç‰¹å¾å’Œéšè—çŠ¶æ€ï¼ˆå¦‚æœ`output_hidden_states=True`ï¼‰çš„`BaseModelOutputWithPooling`ã€‚

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2Model

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> qformer_outputs = model.get_qformer_features(**inputs)
```

## Blip2ForConditionalGeneration

### `class transformers.Blip2ForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1524)

```py
( config: Blip2Config )
```

å‚æ•°

+   `config`ï¼ˆ[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)ï¼‰â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

ç”¨äºæ ¹æ®å›¾åƒå’Œå¯é€‰æ–‡æœ¬æç¤ºç”Ÿæˆæ–‡æœ¬çš„BLIP-2æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”±è§†è§‰ç¼–ç å™¨ã€æŸ¥è¯¢å˜æ¢å™¨ï¼ˆQ-Formerï¼‰å’Œè¯­è¨€æ¨¡å‹ç»„æˆã€‚

å¯ä»¥é€‰æ‹©å°†`input_ids`ä¼ é€’ç»™æ¨¡å‹ï¼Œä½œä¸ºæ–‡æœ¬æç¤ºï¼Œä»¥ä½¿è¯­è¨€æ¨¡å‹ç»§ç»­æç¤ºã€‚å¦åˆ™ï¼Œè¯­è¨€æ¨¡å‹å°†ä»[BOS]ï¼ˆåºåˆ—å¼€å§‹ï¼‰æ ‡è®°å¼€å§‹ç”Ÿæˆæ–‡æœ¬ã€‚

è¯·æ³¨æ„ï¼ŒFlan-T5æ£€æŸ¥ç‚¹ä¸èƒ½è½¬æ¢ä¸ºfloat16ã€‚å®ƒä»¬æ˜¯ä½¿ç”¨bfloat16è¿›è¡Œé¢„è®­ç»ƒçš„ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)

```py
( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`Blip2Processor.__call__()`ã€‚

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚è¾“å…¥æ ‡è®°å¯ä»¥é€‰æ‹©ä½œä¸ºæ–‡æœ¬æç¤ºæä¾›ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥ç»§ç»­ã€‚

    å¯ä»¥ä½¿ç”¨[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`Blip2Processor.__call__()`ã€‚

    è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ

    +   å¯¹äºâ€œå±è”½â€çš„æ ‡è®°ä¸º0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `decoder_input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- è§£ç å™¨è¾“å…¥åºåˆ—æ ‡è®°åœ¨è¯­è¨€æ¨¡å‹è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ã€‚ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è§£ç å™¨è¾“å…¥IDæ˜¯ä»€ä¹ˆï¼Ÿ

+   `decoder_attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰- é»˜è®¤è¡Œä¸ºï¼šç”Ÿæˆä¸€ä¸ªå¼ é‡ï¼Œå¿½ç•¥`decoder_input_ids`ä¸­çš„å¡«å……æ ‡è®°ã€‚å› æœæ©ç ä¹Ÿå°†é»˜è®¤ä½¿ç”¨ã€‚

    ä»…åœ¨ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆå¦‚T5ï¼‰æ—¶ç›¸å…³ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰- è¯­è¨€æ¨¡å‹çš„è¯­è¨€å»ºæ¨¡æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰- è¯­è¨€æ¨¡å‹å¤´çš„é¢„æµ‹åˆ†æ•°ã€‚

+   `vision_outputs`ï¼ˆ`BaseModelOutputWithPooling`ï¼‰- è§†è§‰ç¼–ç å™¨çš„è¾“å‡ºã€‚

+   `qformer_outputs`ï¼ˆ`BaseModelOutputWithPoolingAndCrossAttentions`ï¼‰- Q-Formerï¼ˆQuerying Transformerï¼‰çš„è¾“å‡ºã€‚

+   `language_model_outputs`ï¼ˆ`CausalLMOutputWithPast`æˆ–`Seq2SeqLMOutput`ï¼‰- è¯­è¨€æ¨¡å‹çš„è¾“å‡ºã€‚

[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

å‡†å¤‡å¤„ç†å™¨ã€æ¨¡å‹å’Œå›¾åƒè¾“å…¥

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2ForConditionalGeneration
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2ForConditionalGeneration.from_pretrained(
...     "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
... )  # doctest: +IGNORE_RESULT

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
```

å›¾åƒå­—å¹•ï¼ˆä¸æä¾›æ–‡æœ¬æç¤ºï¼‰ï¼š

```py
>>> inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two cats laying on a couch
```

è§†è§‰é—®ç­”ï¼ˆæç¤º=é—®é¢˜ï¼‰ï¼š

```py
>>> prompt = "Question: how many cats are there? Answer:"
>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device="cuda", dtype=torch.float16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two
```

è¯·æ³¨æ„ï¼Œä¹Ÿæ”¯æŒé€šè¿‡[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)è¿›è¡Œint8æ¨ç†ã€‚è¿™å¤§å¤§å‡å°‘äº†æ¨¡å‹ä½¿ç”¨çš„å†…å­˜é‡ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„æ€§èƒ½ã€‚

```py
>>> model = Blip2ForConditionalGeneration.from_pretrained(
...     "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.bfloat16
... )  # doctest: +IGNORE_RESULT

>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device="cuda", dtype=torch.bfloat16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two
```

#### `generate`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)

```py
( pixel_values: FloatTensor input_ids: Optional = None attention_mask: Optional = None **generate_kwargs ) â†’ export const metadata = 'undefined';captions (list)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º(batch_size, num_channels, height, width)çš„`torch.FloatTensor`ï¼‰â€”è¦å¤„ç†çš„è¾“å…¥å›¾åƒã€‚

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º(batch_size, sequence_length)çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨ä½œç”Ÿæˆæç¤ºçš„åºåˆ—ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º(batch_size, sequence_length)çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç 

è¿”å›

å­—å¹•ï¼ˆåˆ—è¡¨ï¼‰

ä¸€ä¸ªé•¿åº¦ä¸ºbatch_size * num_captionsçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

è¦†ç›–`generate`å‡½æ•°ä»¥èƒ½å¤Ÿå°†æ¨¡å‹ç”¨ä½œæ¡ä»¶ç”Ÿæˆå™¨ã€‚
