# BLIP-2

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip-2)

## 概述

BLIP-2 模型由 Junnan Li、Dongxu Li、Silvio Savarese、Steven Hoi 在[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)中提出。BLIP-2 利用冻结的预训练图像编码器和大型语言模型（LLMs），通过在它们之间训练一个轻量级的 12 层 Transformer 编码器，实现了各种视觉-语言任务的最先进性能。值得注意的是，BLIP-2 在零样本 VQAv2 上比[Flamingo](https://arxiv.org/abs/2204.14198)（一个 80 亿参数模型）提高了 8.7%，并且可训练参数数量减少了 54 倍。

论文摘要如下：

*由于大规模模型的端到端训练，视觉-语言预训练的成本变得越来越高。本文提出了 BLIP-2，一种通用且高效的预训练策略，从现成的冻结预训练图像编码器和冻结大型语言模型中引导视觉-语言预训练。BLIP-2 通过轻量级的 Querying Transformer 消除了模态差异，该模型经过两个阶段的预训练。第一阶段从冻结图像编码器引导视觉-语言表示学习。第二阶段从冻结语言模型引导视觉-语言生成学习。尽管可训练参数数量明显少于现有方法，但 BLIP-2 在各种视觉-语言任务上实现了最先进的性能。例如，我们的模型在零样本 VQAv2 上比 Flamingo80B 提高了 8.7%，并且可训练参数数量减少了 54 倍。我们还展示了模型的新兴能力，即零样本图像到文本生成，可以遵循自然语言指令。*

![drawing](../Images/a2556d3b5fb4c6456c1324aff8278927.png) BLIP-2 架构。摘自[原始论文。](https://arxiv.org/abs/2301.12597)

此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207)找到。

## 使用提示

+   BLIP-2 可用于在给定图像和可选文本提示的情况下进行条件文本生成。在推理时，建议使用 `generate` 方法。

+   可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)来为模型准备图像，并将预测的标记ID解码回文本。

## 资源

官方 Hugging Face 和社区（由🌎表示）资源列表，帮助您开始使用 BLIP-2。

+   BLIP-2 的演示笔记本用于图像字幕、视觉问答（VQA）和类似对话的会话，可在[此处](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2)找到。

如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。

## Blip2Config

### `class transformers.Blip2Config`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L251)

```py
( vision_config = None qformer_config = None text_config = None num_query_tokens = 32 **kwargs )
```

参数

+   `vision_config` (`dict`, *可选*) — 用于初始化[Blip2VisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionConfig)的配置选项字典。

+   `qformer_config` (`dict`, *可选*) — 用于初始化[Blip2QFormerConfig](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerConfig)的配置选项字典。

+   `text_config` (`dict`, *可选*) — 用于初始化任何[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的配置选项字典。

+   `num_query_tokens` (`int`, *optional*, defaults to 32) — 通过Transformer传递的查询令牌数量。

+   `kwargs` (*optional*) — 关键字参数的字典。

[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)是用于存储[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)配置的配置类。根据指定的参数实例化一个BLIP-2模型，定义视觉模型、Q-Former模型和语言模型配置。使用默认配置实例化将产生类似于BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import (
...     Blip2VisionConfig,
...     Blip2QFormerConfig,
...     OPTConfig,
...     Blip2Config,
...     Blip2ForConditionalGeneration,
... )

>>> # Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2Config()

>>> # Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2ForConditionalGeneration(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig

>>> # Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations
>>> vision_config = Blip2VisionConfig()
>>> qformer_config = Blip2QFormerConfig()
>>> text_config = OPTConfig()

>>> config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)
```

#### `from_vision_qformer_text_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L335)

```py
( vision_config: Blip2VisionConfig qformer_config: Blip2QFormerConfig text_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';Blip2Config
```

返回

[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)

配置对象的实例

从BLIP-2视觉模型、Q-Former和语言模型配置中实例化一个[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)（或派生类）。

## Blip2VisionConfig

### `class transformers.Blip2VisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L33)

```py
( hidden_size = 1408 intermediate_size = 6144 num_hidden_layers = 39 num_attention_heads = 16 image_size = 224 patch_size = 14 hidden_act = 'gelu' layer_norm_eps = 1e-06 attention_dropout = 0.0 initializer_range = 1e-10 qkv_bias = True **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 1408) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 6144) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 39) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数量。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 14) — 每个补丁的大小（分辨率）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"gelu"`。layer_norm_eps (`float`, *optional*, defaults to 1e-5): 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否在自注意力层中为查询和值添加偏置。

这是用于存储[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)配置的配置类。根据指定的参数实例化一个BLIP-2视觉编码器，定义模型架构。实例化默认配置将产生类似于BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import Blip2VisionConfig, Blip2VisionModel

>>> # Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2VisionConfig()

>>> # Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2VisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Blip2QFormerConfig

### `class transformers.Blip2QFormerConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/configuration_blip_2.py#L132)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 0 position_embedding_type = 'absolute' cross_attention_frequency = 2 encoder_hidden_size = 1408 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 30522) — Q-Former 模型的词汇量。定义在调用模型时传递的 `inputs_ids` 可表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer 编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — 每个注意力层中的注意力头数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的 dropout 比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如，512、1024或2048）。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — 位置嵌入的类型。选择 `"absolute"`、`"relative_key"`、`"relative_key_query"` 中的一个。对于位置嵌入，请使用 `"absolute"`。有关 `"relative_key"` 的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关 `"relative_key_query"` 的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658) 中的 *Method 4*。

+   `cross_attention_frequency` (`int`, *optional*, defaults to 2) — 向 Transformer 层添加交叉注意力的频率。

+   `encoder_hidden_size` (`int`, *optional*, defaults to 1408) — 交叉注意力中隐藏状态的隐藏大小。

这是一个配置类，用于存储[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)的配置。根据指定的参数实例化一个 BLIP-2 Querying Transformer (Q-Former) 模型，定义模型架构。使用默认值实例化配置将产生类似于 BLIP-2 [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b) 架构的配置。配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

请注意，[Blip2QFormerModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2QFormerModel)与[BertLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertLMHeadModel)非常相似，具有交错的交叉注意力。

示例：

```py
>>> from transformers import Blip2QFormerConfig, Blip2QFormerModel

>>> # Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration
>>> configuration = Blip2QFormerConfig()

>>> # Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration
>>> model = Blip2QFormerModel(configuration)
>>> # Accessing the model configuration
>>> configuration = model.config
```

## Blip2Processor

### `class transformers.Blip2Processor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L27)

```py
( image_processor tokenizer )
```

参数

+   `image_processor`（`BlipImageProcessor`）— [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)的一个实例。图像处理器是必需的输入。

+   `tokenizer`（`AutoTokenizer`）— [‘PreTrainedTokenizer’]的一个实例。分词器是必需的输入。

构建一个BLIP-2处理器，将BLIP图像处理器和OPT/T5分词器封装到一个处理器中。

[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor)提供了[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)的文档字符串。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L135)

```py
( *args **kwargs )
```

此方法将其所有参数转发到PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。

解码

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/processing_blip_2.py#L143)

```py
( *args **kwargs )
```

此方法将其所有参数转发到PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。

## Blip2VisionModel

### `class transformers.Blip2VisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L501)

```py
( config: Blip2VisionConfig )
```

前进

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L516)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获得。有关详细信息，请参阅`Blip2Processor.__call__()`。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`）和输入的各种元素。

+   `last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）- 模型最后一层的隐藏状态序列。

+   `pooler_output`（`torch.FloatTensor`，形状为`(batch_size, hidden_size)`）- 经过用于辅助预训练任务的层进一步处理后，序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。

    模型在每一层输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[Blip2VisionModel](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2VisionModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行前处理和后处理步骤，而后者会默默地忽略它们。

## Blip2QFormerModel

### `class transformers.Blip2QFormerModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L989)

```py
( config: Blip2QFormerConfig )
```

查询变压器（Q-Former），用于BLIP-2。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1064)

```py
( query_embeds: FloatTensor attention_mask: Optional = None head_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

encoder_hidden_states（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，`可选`）：编码器最后一层的输出的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。encoder_attention_mask（`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，`可选`）：避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。选择的掩码值在`[0, 1]`中：

+   对于未被“掩盖”的标记为1，

+   对于被“掩盖”的标记为0。past_key_values（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个张量：形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。use_cache（`bool`，`可选`）：如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

## Blip2Model

### `class transformers.Blip2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1178)

```py
( config: Blip2Config )
```

参数

+   `config`（[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

用于生成文本和图像特征的BLIP-2模型。该模型由视觉编码器、查询变换器（Q-Former）和语言模型组成。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1397)

```py
( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 语言模型词汇表中输入序列标记的索引。输入标记可以选择性地提供作为文本提示，语言模型可以继续。

    可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参阅`Blip2Processor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1 用于“未掩码”标记的标记，

    +   0 用于“掩码”标记的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 语言模型词汇表中解码器输入序列标记的索引。仅在使用编码器-解码器语言模型（如T5）时相关。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是解码器输入ID？](../glossary#decoder-input-ids)

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）— 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

    仅在使用编码器-解码器语言模型（如T5）时相关。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput` 或 `tuple(torch.FloatTensor)`

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput` 或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，取决于配置（`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`）和输入。

+   `loss`（`torch.FloatTensor`，*可选*，在提供 `labels` 时返回，形状为 `(1,)` 的 `torch.FloatTensor`）— 语言模型的语言建模损失。

+   `logits`（形状为 `(batch_size, sequence_length, config.vocab_size)` 的 `torch.FloatTensor`）— 语言模型的语言建模头的预测分数。

+   `vision_outputs`（`BaseModelOutputWithPooling`）— 视觉编码器的输出。

+   `qformer_outputs`（`BaseModelOutputWithPoolingAndCrossAttentions`）— Q-Former（Querying Transformer）的输出。

+   `language_model_outputs`（`CausalLMOutputWithPast` 或 `Seq2SeqLMOutput`）— 语言模型的输出。

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2Model
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)
>>> model.to(device)
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> prompt = "Question: how many cats are there? Answer:"
>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device, torch.float16)

>>> outputs = model(**inputs)
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1235)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_outputs (CausalLMOutputWithPast, or tuple(torch.FloatTensor) if return_dict=False)
```

参数

+   `input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入 ID？](../glossary#input-ids)

+   `attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   对于 `未被掩盖` 的标记为 1，

    +   对于 `被掩盖` 的标记为 0。[什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为 `(batch_size, target_sequence_length)` 的 `torch.LongTensor`，*可选*）— 词汇表中解码器输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入 ID？](../glossary#decoder-input-ids)

    T5 使用 `pad_token_id` 作为 `decoder_input_ids` 生成的起始标记。如果使用了 `past_key_values`，则可选择仅输入最后的 `decoder_input_ids`（参见 `past_key_values`）。

    要了解有关如何为预训练准备 `decoder_input_ids` 的更多信息，请查看 [T5 Training](./t5#training)。

+   `decoder_attention_mask`（形状为 `(batch_size, target_sequence_length)` 的 `torch.BoolTensor`，*可选*）— 默认行为：生成一个忽略 `decoder_input_ids` 中填充标记的张量。因果掩码也将默认使用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

text_outputs (`CausalLMOutputWithPast`，或者如果`return_dict=False`则为`tuple(torch.FloatTensor)`)

语言模型输出。如果`return_dict=True`，则输出是一个包含语言模型logits、过去的键值和隐藏状态（如果`output_hidden_states=True`）的`CausalLMOutputWithPast`。

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, Blip2Model

>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> tokenizer = AutoTokenizer.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> inputs = tokenizer(["a photo of a cat"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1294)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

vision_outputs (`BaseModelOutputWithPooling`或`torch.FloatTensor`的元组）

视觉模型输出。如果`return_dict=True`，则输出是一个包含图像特征、池化图像特征和隐藏状态（如果`output_hidden_states=True`）的`BaseModelOutputWithPooling`。

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, Blip2Model

>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> image_outputs = model.get_image_features(**inputs)
```

#### `get_qformer_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1338)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';vision_outputs (BaseModelOutputWithPooling or tuple of torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参阅`Blip2Processor.__call__()`。

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 语言模型词汇中输入序列标记的索引。可以提供输入标记作为文本提示，语言模型可以继续生成。

    可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参阅`Blip2Processor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*optional*) — 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   对于`not masked`的标记为1，

    +   对于`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）— 解码器输入序列标记在语言模型词汇中的索引。仅在使用编码器-解码器语言模型（如T5）时相关。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[解码器输入ID是什么？](../glossary#decoder-input-ids)

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）— 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

    仅在使用编码器-解码器语言模型（如T5）时相关。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

vision_outputs（`BaseModelOutputWithPooling`或`torch.FloatTensor`元组）

视觉模型输出。如果`return_dict=True`，则输出是包含图像特征、池化图像特征和隐藏状态（如果`output_hidden_states=True`）的`BaseModelOutputWithPooling`。

[Blip2Model](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Model)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2Model

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2Model.from_pretrained("Salesforce/blip2-opt-2.7b")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> qformer_outputs = model.get_qformer_features(**inputs)
```

## Blip2ForConditionalGeneration

### `class transformers.Blip2ForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1524)

```py
( config: Blip2Config )
```

参数

+   `config`（[Blip2Config](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Config)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

用于根据图像和可选文本提示生成文本的BLIP-2模型。该模型由视觉编码器、查询变换器（Q-Former）和语言模型组成。

可以选择将`input_ids`传递给模型，作为文本提示，以使语言模型继续提示。否则，语言模型将从[BOS]（序列开始）标记开始生成文本。

请注意，Flan-T5检查点不能转换为float16。它们是使用bfloat16进行预训练的。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1610)

```py
( pixel_values: FloatTensor input_ids: FloatTensor attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取像素值。有关详细信息，请参见`Blip2Processor.__call__()`。

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 语言模型词汇表中输入序列标记的索引。输入标记可以选择作为文本提示提供，语言模型可以继续。

    可以使用[Blip2Processor](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2Processor)获取索引。有关详细信息，请参见`Blip2Processor.__call__()`。

    输入ID是什么？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于“未屏蔽”的标记，

    +   对于“屏蔽”的标记为0。

    什么是注意力掩码？

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 解码器输入序列标记在语言模型词汇表中的索引。仅在使用编码器-解码器语言模型（如T5）时相关。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。解码器输入ID是什么？

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）- 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

    仅在使用编码器-解码器语言模型（如T5）时相关。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'>`）和输入的各种元素。

+   `loss`（`torch.FloatTensor`，*可选*，在提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`）- 语言模型的语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）- 语言模型头的预测分数。

+   `vision_outputs`（`BaseModelOutputWithPooling`）- 视觉编码器的输出。

+   `qformer_outputs`（`BaseModelOutputWithPoolingAndCrossAttentions`）- Q-Former（Querying Transformer）的输出。

+   `language_model_outputs`（`CausalLMOutputWithPast`或`Seq2SeqLMOutput`）- 语言模型的输出。

[Blip2ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

准备处理器、模型和图像输入

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import Blip2Processor, Blip2ForConditionalGeneration
>>> import torch

>>> device = "cuda" if torch.cuda.is_available() else "cpu"

>>> processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
>>> model = Blip2ForConditionalGeneration.from_pretrained(
...     "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.float16
... )  # doctest: +IGNORE_RESULT

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
```

图像字幕（不提供文本提示）：

```py
>>> inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two cats laying on a couch
```

视觉问答（提示=问题）：

```py
>>> prompt = "Question: how many cats are there? Answer:"
>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device="cuda", dtype=torch.float16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two
```

请注意，也支持通过[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)进行int8推理。这大大减少了模型使用的内存量，同时保持相同的性能。

```py
>>> model = Blip2ForConditionalGeneration.from_pretrained(
...     "Salesforce/blip2-opt-2.7b", load_in_8bit=True, device_map={"": 0}, torch_dtype=torch.bfloat16
... )  # doctest: +IGNORE_RESULT

>>> inputs = processor(images=image, text=prompt, return_tensors="pt").to(device="cuda", dtype=torch.bfloat16)

>>> generated_ids = model.generate(**inputs)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
two
```

#### `generate`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip_2/modeling_blip_2.py#L1773)

```py
( pixel_values: FloatTensor input_ids: Optional = None attention_mask: Optional = None **generate_kwargs ) → export const metadata = 'undefined';captions (list)
```

参数

+   `pixel_values`（形状为(batch_size, num_channels, height, width)的`torch.FloatTensor`）—要处理的输入图像。

+   `input_ids`（形状为(batch_size, sequence_length)的`torch.LongTensor`，*可选*）—用作生成提示的序列。

+   `attention_mask`（形状为(batch_size, sequence_length)的`torch.LongTensor`，*可选*）—避免在填充标记索引上执行注意力的掩码

返回

字幕（列表）

一个长度为batch_size * num_captions的字符串列表。

覆盖`generate`函数以能够将模型用作条件生成器。
