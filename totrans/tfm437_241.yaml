- en: UL2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/270.bcdad36c.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf)
    by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster,
    Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Existing pre-trained models are generally geared towards a particular class
    of problems. To date, there seems to be still no consensus on what the right architecture
    and pre-training setup should be. This paper presents a unified framework for
    pre-training models that are universally effective across datasets and setups.
    We begin by disentangling architectural archetypes with pre-training objectives
    — two concepts that are commonly conflated. Next, we present a generalized and
    unified perspective for self-supervision in NLP and show how different pre-training
    objectives can be cast as one another and how interpolating between different
    objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training
    objective that combines diverse pre-training paradigms together. We furthermore
    introduce a notion of mode switching, wherein downstream fine-tuning is associated
    with specific pre-training schemes. We conduct extensive ablative experiments
    to compare multiple pre-training objectives and find that our method pushes the
    Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse
    setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance
    on 50 well-established supervised NLP tasks ranging from language generation (with
    automated and human evaluation), language understanding, text classification,
    question answering, commonsense reasoning, long text reasoning, structured knowledge
    grounding and information retrieval. Our model also achieve strong results at
    in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling
    the performance of T5-XXL on one-shot summarization.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [DanielHesslow](https://huggingface.co/Seledorn).
    The original code can be found [here](https://github.com/google-research/google-research/tree/master/ul2).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions
    as well as fine-tuned on an array of downstream tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UL2 has the same architecture as [T5v1.1](t5v1.1) but uses the Gated-SiLU activation
    function instead of Gated-GELU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors release checkpoints of one architecture which can be seen [here](https://huggingface.co/google/ul2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As UL2 has the same architecture as T5v1.1, refer to [T5’s documentation page](t5)
    for API reference, tips, code examples and notebooks.
  prefs: []
  type: TYPE_NORMAL
