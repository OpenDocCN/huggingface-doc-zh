- en: UL2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UL2
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/ul2)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf)
    by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster,
    Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: T5模型在[Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal
    Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler撰写的《统一语言学习范式》](https://arxiv.org/pdf/2205.05131v1.pdf)中被提出。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Existing pre-trained models are generally geared towards a particular class
    of problems. To date, there seems to be still no consensus on what the right architecture
    and pre-training setup should be. This paper presents a unified framework for
    pre-training models that are universally effective across datasets and setups.
    We begin by disentangling architectural archetypes with pre-training objectives
    — two concepts that are commonly conflated. Next, we present a generalized and
    unified perspective for self-supervision in NLP and show how different pre-training
    objectives can be cast as one another and how interpolating between different
    objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training
    objective that combines diverse pre-training paradigms together. We furthermore
    introduce a notion of mode switching, wherein downstream fine-tuning is associated
    with specific pre-training schemes. We conduct extensive ablative experiments
    to compare multiple pre-training objectives and find that our method pushes the
    Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse
    setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance
    on 50 well-established supervised NLP tasks ranging from language generation (with
    automated and human evaluation), language understanding, text classification,
    question answering, commonsense reasoning, long text reasoning, structured knowledge
    grounding and information retrieval. Our model also achieve strong results at
    in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling
    the performance of T5-XXL on one-shot summarization.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*现有的预训练模型通常针对特定类别的问题。迄今为止，关于正确的架构和预训练设置应该是什么，似乎仍然没有共识。本文提出了一个统一的框架，用于预训练模型在数据集和设置上具有普遍有效性。我们首先通过解开具有预训练目标的架构原型来开始——这两个概念通常被混淆。接下来，我们提出了自监督在NLP中的泛化和统一视角，并展示了不同的预训练目标如何相互转换以及如何在不同目标之间插值可以是有效的。然后，我们提出了去噪器混合（MoD），这是一个将不同的预训练范式结合在一起的预训练目标。此外，我们引入了一种模式切换的概念，其中下游微调与特定的预训练方案相关联。我们进行了大量的消融实验，比较了多个预训练目标，并发现我们的方法通过在多个不同设置中优于T5和/或类似GPT的模型来推动帕累托前沿。最后，通过将我们的模型扩展到20B参数，我们在50个建立良好的监督NLP任务上实现了SOTA性能，涵盖了语言生成（自动化和人工评估）、语言理解、文本分类、问题回答、常识推理、长文本推理、结构化知识基础和信息检索。我们的模型还在上下文学习方面取得了强大的结果，在零样本SuperGLUE上优于175B的GPT-3，并在一次性摘要上将T5-XXL的性能提高了三倍。*'
- en: This model was contributed by [DanielHesslow](https://huggingface.co/Seledorn).
    The original code can be found [here](https://github.com/google-research/google-research/tree/master/ul2).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[DanielHesslow](https://huggingface.co/Seledorn)贡献。原始代码可以在[这里](https://github.com/google-research/google-research/tree/master/ul2)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: UL2 is an encoder-decoder model pre-trained on a mixture of denoising functions
    as well as fine-tuned on an array of downstream tasks.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UL2是一个编码器-解码器模型，预先在一系列去噪函数的混合上进行了预训练，并在一系列下游任务上进行了微调。
- en: UL2 has the same architecture as [T5v1.1](t5v1.1) but uses the Gated-SiLU activation
    function instead of Gated-GELU.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UL2与[T5v1.1](t5v1.1)具有相同的架构，但使用了门控SiLU激活函数，而不是门控GELU。
- en: The authors release checkpoints of one architecture which can be seen [here](https://huggingface.co/google/ul2)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者发布了一个架构的检查点，可以在[这里](https://huggingface.co/google/ul2)看到
- en: As UL2 has the same architecture as T5v1.1, refer to [T5’s documentation page](t5)
    for API reference, tips, code examples and notebooks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于UL2与T5v1.1具有相同的架构，请参考[T5的文档页面](t5)获取API参考、提示、代码示例和笔记本。
