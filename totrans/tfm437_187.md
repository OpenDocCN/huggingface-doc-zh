# LLaMA

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/llama)

## æ¦‚è¿°

Hugo Touvronã€Thibaut Lavrilã€Gautier Izacardã€Xavier Martinetã€Marie-Anne Lachauxã€TimothÃ©e Lacroixã€Baptiste RoziÃ¨reã€Naman Goyalã€Eric Hambroã€Faisal Azharã€Aurelien Rodriguezã€Armand Joulinã€Edouard Graveã€Guillaume Lampleåœ¨[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)ä¸­æå‡ºäº†LLaMAæ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«ä»7Båˆ°65Bå‚æ•°çš„åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆã€‚

è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬ä»‹ç»LLaMAï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ä»7Båˆ°65Bå‚æ•°çš„åŸºç¡€è¯­è¨€æ¨¡å‹çš„é›†åˆã€‚æˆ‘ä»¬åœ¨æ•°ä¸‡äº¿æ ‡è®°ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¯ä»¥ä»…ä½¿ç”¨å…¬å¼€å¯ç”¨çš„æ•°æ®é›†è®­ç»ƒæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä½¿ç”¨ä¸“æœ‰å’Œä¸å¯è®¿é—®çš„æ•°æ®é›†ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLaMA-13Båœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºGPT-3ï¼ˆ175Bï¼‰ï¼Œè€ŒLLaMA-65Bä¸æœ€ä½³æ¨¡å‹Chinchilla-70Bå’ŒPaLM-540Bç«äº‰ã€‚æˆ‘ä»¬å‘ç ”ç©¶ç¤¾åŒºå‘å¸ƒäº†æ‰€æœ‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚*

æ­¤æ¨¡å‹ç”±[zphang](https://huggingface.co/zphang)è´¡çŒ®ï¼Œ[BlackSamorez](https://huggingface.co/BlackSamorez)ä¹Ÿæœ‰è´¡çŒ®ã€‚ Hugging Faceä¸­çš„å®ç°ä»£ç åŸºäºGPT-NeoX [è¿™é‡Œ](https://github.com/EleutherAI/gpt-neox)ã€‚ä½œè€…çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/llama)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   LLaMAæ¨¡å‹çš„æƒé‡å¯ä»¥é€šè¿‡å¡«å†™[æ­¤è¡¨æ ¼](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form)è·å¾—ã€‚

+   ä¸‹è½½æƒé‡åï¼Œéœ€è¦ä½¿ç”¨[è½¬æ¢è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)å°†å…¶è½¬æ¢ä¸ºHugging Face Transformersæ ¼å¼ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ï¼ˆç¤ºä¾‹ï¼‰å‘½ä»¤è°ƒç”¨è„šæœ¬ï¼š

```py
python src/transformers/models/llama/convert_llama_weights_to_hf.py \
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
```

+   è½¬æ¢åï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼š

```py
from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")
```

è¯·æ³¨æ„ï¼Œæ‰§è¡Œè„šæœ¬éœ€è¦è¶³å¤Ÿçš„CPU RAMæ¥æ‰˜ç®¡æ•´ä¸ªæ¨¡å‹çš„float16ç²¾åº¦ï¼ˆå³ä½¿æœ€å¤§ç‰ˆæœ¬åˆ†ä¸ºå‡ ä¸ªæ£€æŸ¥ç‚¹ï¼Œå®ƒä»¬æ¯ä¸ªéƒ½åŒ…å«æ¨¡å‹çš„æ¯ä¸ªæƒé‡çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å…¨éƒ¨åŠ è½½åˆ°RAMä¸­ï¼‰ã€‚å¯¹äº65Bæ¨¡å‹ï¼Œå› æ­¤éœ€è¦130GBçš„RAMã€‚

+   LLaMAåˆ†è¯å™¨æ˜¯åŸºäº[sentencepiece](https://github.com/google/sentencepiece)çš„BPEæ¨¡å‹ã€‚sentencepieceçš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯ï¼Œåœ¨è§£ç åºåˆ—æ—¶ï¼Œå¦‚æœç¬¬ä¸€ä¸ªæ ‡è®°æ˜¯å•è¯çš„å¼€å¤´ï¼ˆä¾‹å¦‚â€œBananaâ€ï¼‰ï¼Œåˆ†è¯å™¨ä¸ä¼šåœ¨å­—ç¬¦ä¸²å‰æ·»åŠ å‰ç¼€ç©ºæ ¼ã€‚

æ­¤æ¨¡å‹ç”±[zphang](https://huggingface.co/zphang)è´¡çŒ®ï¼Œ[BlackSamorez](https://huggingface.co/BlackSamorez)ä¹Ÿæœ‰è´¡çŒ®ã€‚ Hugging Faceä¸­çš„å®ç°ä»£ç åŸºäºGPT-NeoX [è¿™é‡Œ](https://github.com/EleutherAI/gpt-neox)ã€‚ä½œè€…çš„åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/llama)æ‰¾åˆ°ã€‚å®ç°çš„Flaxç‰ˆæœ¬ç”±[afmck](https://huggingface.co/afmck)è´¡çŒ®ï¼Œå®ç°ä¸­çš„ä»£ç åŸºäºHugging Faceçš„Flax GPT-Neoã€‚

åŸºäºåŸå§‹LLaMAæ¨¡å‹ï¼ŒMeta AIå‘å¸ƒäº†ä¸€äº›åç»­ä½œå“ï¼š

+   **Llama2**ï¼šLlama2æ˜¯Llamaçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå…·æœ‰ä¸€äº›æ¶æ„è°ƒæ•´ï¼ˆGrouped Query Attentionï¼‰ï¼Œå¹¶ä¸”åœ¨2ä¸‡äº¿æ ‡è®°ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚è¯·å‚è€ƒå¯ä»¥åœ¨[è¿™é‡Œ](llama2)æ‰¾åˆ°çš„Llama2çš„æ–‡æ¡£ã€‚

## èµ„æº

ä¸€ç³»åˆ—å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨LLaMAã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

æ–‡æœ¬åˆ†ç±»

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨æç¤ºè°ƒæ•´æ¥é€‚åº”LLaMAæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2)ã€‚

é—®ç­”

+   [StackLLaMA: ç”¨RLHFè®­ç»ƒLLaMAçš„å®ç”¨æŒ‡å—](https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf)ï¼Œä¸€ç¯‡å…³äºå¦‚ä½•ä½¿ç”¨RLHFè®­ç»ƒLLaMAä»¥å›ç­”[Stack Exchange](https://stackexchange.com/)ä¸Šé—®é¢˜çš„åšæ–‡ã€‚

âš—ï¸ ä¼˜åŒ–

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨xturingåº“åœ¨GPUä¸Šå¾®è°ƒLLaMAæ¨¡å‹çš„[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing)ï¼Œè¯¥GPUå…·æœ‰æœ‰é™çš„å†…å­˜ã€‚ğŸŒ

âš¡ï¸ æ¨ç†

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨ğŸ¤— PEFTåº“ä¸­çš„PeftModelè¿è¡ŒLLaMAæ¨¡å‹çš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb)ã€‚ğŸŒ

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨LangChainåŠ è½½PEFTé€‚é…å™¨LLaMAæ¨¡å‹çš„[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing)ã€‚ğŸŒ

ğŸš€ éƒ¨ç½²

+   ä¸€ä¸ªå…³äºå¦‚ä½•ä½¿ç”¨LoRAæ–¹æ³•é€šè¿‡ğŸ¤— PEFTåº“è¿›è¡ŒLLaMAæ¨¡å‹å¾®è°ƒçš„[ç¬”è®°æœ¬](https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T)ã€‚ğŸŒ

+   ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨Amazon SageMakerä¸Šéƒ¨ç½²Open-LLaMAæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆçš„[ç¬”è®°æœ¬](https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb)ã€‚ğŸŒ

## LlamaConfig

### `class transformers.LlamaConfig`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/configuration_llama.py#L31)

```py
( vocab_size = 32000 hidden_size = 4096 intermediate_size = 11008 num_hidden_layers = 32 num_attention_heads = 32 num_key_value_heads = None hidden_act = 'silu' max_position_embeddings = 2048 initializer_range = 0.02 rms_norm_eps = 1e-06 use_cache = True pad_token_id = None bos_token_id = 1 eos_token_id = 2 pretraining_tp = 1 tie_word_embeddings = False rope_theta = 10000.0 rope_scaling = None attention_bias = False attention_dropout = 0.0 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, é»˜è®¤ä¸º32000) â€” LLaMAæ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰åœ¨è°ƒç”¨[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°çš„æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, é»˜è®¤ä¸º4096) â€” éšè—è¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `intermediate_size` (`int`, *optional*, é»˜è®¤ä¸º11008) â€” MLPè¡¨ç¤ºçš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, é»˜è®¤ä¸º32) â€” Transformerè§£ç å™¨ä¸­çš„éšè—å±‚æ•°ã€‚

+   `num_attention_heads` (`int`, *optional*, é»˜è®¤ä¸º32) â€” Transformerè§£ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `num_key_value_heads` (`int`, *optional*) â€” è¿™æ˜¯åº”è¯¥ç”¨æ¥å®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›çš„key_valueå¤´çš„æ•°é‡ã€‚å¦‚æœ`num_key_value_heads=num_attention_heads`ï¼Œæ¨¡å‹å°†ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ï¼Œå¦‚æœ`num_key_value_heads=1`ï¼Œæ¨¡å‹å°†ä½¿ç”¨å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰ï¼Œå¦åˆ™å°†ä½¿ç”¨GQAã€‚å°†å¤šå¤´æ£€æŸ¥ç‚¹è½¬æ¢ä¸ºGQAæ£€æŸ¥ç‚¹æ—¶ï¼Œåº”é€šè¿‡å¯¹è¯¥ç»„ä¸­æ‰€æœ‰åŸå§‹å¤´è¿›è¡Œå‡å€¼æ± åŒ–æ¥æ„å»ºæ¯ä¸ªç»„é”®å’Œå€¼å¤´ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤è®ºæ–‡](https://arxiv.org/pdf/2305.13245.pdf)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†é»˜è®¤ä¸º`num_attention_heads`ã€‚

+   `hidden_act` (`str` æˆ– `function`, *optional*, é»˜è®¤ä¸º`"silu"`) â€” è§£ç å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `max_position_embeddings` (`int`, *optional*, é»˜è®¤ä¸º2048) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚Llama 1æ”¯æŒæœ€å¤š2048ä¸ªæ ‡è®°ï¼ŒLlama 2æ”¯æŒæœ€å¤š4096ä¸ªæ ‡è®°ï¼ŒCodeLlamaæ”¯æŒæœ€å¤š16384ä¸ªæ ‡è®°ã€‚

+   `initializer_range` (`float`, *optional*, é»˜è®¤ä¸º0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `rms_norm_eps` (`float`, *optional*, é»˜è®¤ä¸º1e-06) â€” rmså½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `use_cache` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚

+   `pad_token_id` (`int`, *å¯é€‰*) â€” å¡«å……æ ‡è®°idã€‚

+   `bos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æµçš„å¼€å§‹æ ‡è®°idã€‚

+   `eos_token_id` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” æµçš„ç»“æŸæ ‡è®°idã€‚

+   `pretraining_tp` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” å®éªŒæ€§åŠŸèƒ½ã€‚åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„å¼ é‡å¹¶è¡Œæ€§ç­‰çº§ã€‚è¯·å‚è€ƒ[æ­¤æ–‡æ¡£](https://huggingface.co/docs/transformers/parallelism)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤å€¼å¯¹äºç¡®ä¿é¢„è®­ç»ƒç»“æœçš„ç²¾ç¡®å¯é‡ç°æ€§æ˜¯å¿…è¦çš„ã€‚è¯·å‚è€ƒ[æ­¤é—®é¢˜](https://github.com/pytorch/pytorch/issues/76232)ã€‚

+   `tie_word_embeddings` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ç»‘å®šæƒé‡åµŒå…¥

+   `rope_theta` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 10000.0) â€” RoPEåµŒå…¥çš„åŸºæœ¬å‘¨æœŸã€‚

+   `rope_scaling` (`Dict`, *å¯é€‰*) â€” åŒ…å«RoPEåµŒå…¥çš„ç¼©æ”¾é…ç½®çš„å­—å…¸ã€‚å½“å‰æ”¯æŒä¸¤ç§ç¼©æ”¾ç­–ç•¥ï¼šçº¿æ€§å’ŒåŠ¨æ€ã€‚å®ƒä»¬çš„ç¼©æ”¾å› å­å¿…é¡»æ˜¯å¤§äº1çš„æµ®ç‚¹æ•°ã€‚é¢„æœŸæ ¼å¼ä¸º`{"type": ç­–ç•¥åç§°, "factor": ç¼©æ”¾å› å­}`ã€‚ä½¿ç”¨æ­¤æ ‡å¿—æ—¶ï¼Œä¸è¦å°†`max_position_embeddings`æ›´æ–°ä¸ºé¢„æœŸçš„æ–°æœ€å¤§å€¼ã€‚æœ‰å…³è¿™äº›ç¼©æ”¾ç­–ç•¥è¡Œä¸ºçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…ä»¥ä¸‹ä¸»é¢˜ï¼š[https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)ã€‚è¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§åŠŸèƒ½ï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿç ´åæ€§APIæ›´æ”¹ã€‚

+   `attention_bias` (`bool`, é»˜è®¤ä¸º `False`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºæŠ•å½±å±‚ä¸­æ˜¯å¦ä½¿ç”¨åç½®ã€‚

+   `attention_dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¤±æ¯”ç‡ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[LLamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)çš„é…ç½®ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–LLaMAæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLLaMA-7Bçš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

```py
>>> from transformers import LlamaModel, LlamaConfig

>>> # Initializing a LLaMA llama-7b style configuration
>>> configuration = LlamaConfig()

>>> # Initializing a model from the llama-7b style configuration
>>> model = LlamaModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LlamaTokenizer

### `class transformers.LlamaTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L66)

```py
( vocab_file unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' pad_token = None sp_model_kwargs: Optional = None add_bos_token = True add_eos_token = False clean_up_tokenization_spaces = False use_default_system_prompt = False spaces_between_special_tokens = False legacy = None **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” è¯æ±‡æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `unk_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `bos_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

+   `eos_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åºåˆ—çš„ç»“æŸæ ‡è®°ã€‚

+   `pad_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*) â€” ç”¨äºä½¿æ ‡è®°æ•°ç»„åœ¨æ‰¹å¤„ç†ç›®çš„ä¸Šå…·æœ‰ç›¸åŒå¤§å°çš„ç‰¹æ®Šæ ‡è®°ã€‚ç„¶åå°†è¢«æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚

+   `sp_model_kwargs` (`Dict[str, Any]`, `Optional`, *å¯é€‰*) â€” å°†ä¼ é€’ç»™`SentencePieceProcessor.__init__()`æ–¹æ³•ã€‚[SentencePieceçš„PythonåŒ…è£…å™¨](https://github.com/google/sentencepiece/tree/master/python)å¯ç”¨äºè®¾ç½®ï¼š

    +   `enable_sampling`: å¯ç”¨å­è¯æ­£åˆ™åŒ–ã€‚

    +   `nbest_size`: ç”¨äºunigramçš„é‡‡æ ·å‚æ•°ã€‚å¯¹äºBPE-Dropoutæ— æ•ˆã€‚

        +   `nbest_size = {0,1}`: ä¸æ‰§è¡Œé‡‡æ ·ã€‚

        +   `nbest_size > 1`ï¼šä»å‰ nbest_size ä¸ªç»“æœä¸­é‡‡æ ·ã€‚

        +   `nbest_size < 0`ï¼šå‡è®¾ `nbest_size` ä¸ºæ— é™å¤§ï¼Œå¹¶ä½¿ç”¨å‰å‘è¿‡æ»¤å’Œåå‘é‡‡æ ·ç®—æ³•ä»æ‰€æœ‰å‡è®¾ï¼ˆæ ¼å­ï¼‰ä¸­é‡‡æ ·ã€‚

    +   `alpha`ï¼šç”¨äºå•å­—é‡‡æ ·çš„å¹³æ»‘å‚æ•°ï¼Œä»¥åŠ BPE-dropout åˆå¹¶æ“ä½œçš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `add_bos_token` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ  `bos_token`ã€‚

+   `add_eos_token` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ  `eos_token`ã€‚

+   `clean_up_tokenization_spaces` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è§£ç åæ¸…é™¤ç©ºæ ¼ï¼Œæ¸…é™¤åŒ…æ‹¬åˆ é™¤é¢å¤–ç©ºæ ¼ç­‰æ½œåœ¨çš„ç‘•ç–µã€‚

+   `use_default_system_prompt` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ Llama çš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚

+   `spaces_between_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ç‰¹æ®Šæ ‡è®°ä¹‹é—´æ·»åŠ ç©ºæ ¼ã€‚

+   `legacy` (`bool`, *optional*) â€” æ˜¯å¦ä½¿ç”¨åˆ†è¯å™¨çš„ `legacy` è¡Œä¸ºã€‚åœ¨åˆå¹¶ #24622 å’Œ #25224 ä¹‹å‰çš„é—ç•™ç‰ˆæœ¬ä¸­ï¼Œä¿®å¤äº†åœ¨ç‰¹æ®Šæ ‡è®°åå‡ºç°çš„æ ‡è®°çš„é—®é¢˜ã€‚ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

    +   `legacy=True`:

æ„å»ºä¸€ä¸ª Llama åˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚é»˜è®¤çš„å¡«å……æ ‡è®°æœªè®¾ç½®ï¼Œå› ä¸ºåŸå§‹æ¨¡å‹ä¸­æ²¡æœ‰å¡«å……æ ‡è®°ã€‚

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L333)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L344)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” IDåˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” å¯é€‰çš„ç¬¬äºŒä¸ªIDåˆ—è¡¨ï¼Œç”¨äºåºåˆ—å¯¹ã€‚

+   `already_has_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

`List[int]`

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º [0, 1]ï¼š1 è¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œ0 è¡¨ç¤ºåºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—IDã€‚å½“ä½¿ç”¨åˆ†è¯å™¨çš„ `prepare_for_model` æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œä¼šè°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L381)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” IDåˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *optional*) â€” å¯é€‰çš„ç¬¬äºŒä¸ªIDåˆ—è¡¨ï¼Œç”¨äºåºåˆ—å¯¹ã€‚

è¿”å›

`List[int]`

æ ¹æ®ç»™å®šåºåˆ—çš„ [æ ‡è®°ç±»å‹ID](../glossary#token-type-ids) åˆ—è¡¨ã€‚

ä»ä¼ é€’çš„ä¸¤ä¸ªåºåˆ—åˆ›å»ºä¸€ä¸ªç”¨äºåºåˆ—å¯¹åˆ†ç±»ä»»åŠ¡çš„æ©ç ã€‚ä¸€ä¸ª ALBERT

åºåˆ—å¯¹æ©ç çš„æ ¼å¼å¦‚ä¸‹ï¼š

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

å¦‚æœ `token_ids_1` ä¸º Noneï¼Œåˆ™åªè¿”å›æ©ç çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ0ï¼‰ã€‚

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama.py#L306)

```py
( save_directory filename_prefix: Optional = None ) â†’ export const metadata = 'undefined';Tuple(str)
```

å‚æ•°

+   `save_directory` (`str`) â€” ä¿å­˜è¯æ±‡è¡¨çš„ç›®å½•ã€‚

è¿”å›

`Tuple(str)`

ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ã€‚

å°†è¯æ±‡è¡¨å’Œç‰¹æ®Šæ ‡è®°æ–‡ä»¶ä¿å­˜åˆ°ç›®å½•ä¸­ã€‚

## LlamaTokenizerFast

### `class transformers.LlamaTokenizerFast`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L57)

```py
( vocab_file = None tokenizer_file = None clean_up_tokenization_spaces = False unk_token = '<unk>' bos_token = '<s>' eos_token = '</s>' add_bos_token = True add_eos_token = False use_default_system_prompt = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`, *optional*) â€” åŒ…å«å®ä¾‹åŒ–åˆ†è¯å™¨æ‰€éœ€è¯æ±‡è¡¨çš„ [SentencePiece](https://github.com/google/sentencepiece) æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰ .model æ‰©å±•åï¼‰ã€‚

+   `tokenizer_file` (`str`, *optional*) â€” åŒ…å«åŠ è½½åˆ†è¯å™¨æ‰€éœ€çš„æ‰€æœ‰å†…å®¹çš„ [tokenizers](https://github.com/huggingface/tokenizers) æ–‡ä»¶ï¼ˆé€šå¸¸å…·æœ‰ .json æ‰©å±•åï¼‰ã€‚

+   `clean_up_tokenization_spaces` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨è§£ç åæ¸…ç†ç©ºæ ¼ï¼Œæ¸…ç†åŒ…æ‹¬åˆ é™¤é¢å¤–çš„ç©ºæ ¼ç­‰æ½œåœ¨æ®‹ç•™ç‰©ã€‚

+   `unk_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸ºIDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `bos_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—å¼€å§‹æ ‡è®°ã€‚å¯ç”¨ä½œåºåˆ—åˆ†ç±»å™¨æ ‡è®°ã€‚

+   `eos_token` (`str` æˆ– `tokenizers.AddedToken`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `add_bos_token` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦åœ¨åºåˆ—å¼€å¤´æ·»åŠ  `bos_token`ã€‚

+   `add_eos_token` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨åºåˆ—æœ«å°¾æ·»åŠ  `eos_token`ã€‚

+   `use_default_system_prompt` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨ Llama çš„é»˜è®¤ç³»ç»Ÿæç¤ºã€‚

æ„å»ºä¸€ä¸ª Llama åˆ†è¯å™¨ã€‚åŸºäºå­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ã€‚

è¿™é‡Œç‰¹åˆ«ä½¿ç”¨äº† ByteFallback å’Œæ— æ ‡å‡†åŒ–ã€‚

```py
>>> from transformers import LlamaTokenizerFast

>>> tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
>>> tokenizer.encode("Hello this is a test")
[1, 15043, 445, 338, 263, 1243]
```

å¦‚æœè¦æ›´æ”¹ `bos_token` æˆ– `eos_token`ï¼Œè¯·ç¡®ä¿åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶æŒ‡å®šå®ƒä»¬ï¼Œæˆ–è€…è°ƒç”¨ `tokenizer.update_post_processor()` ç¡®ä¿åå¤„ç†æ­£ç¡®å®Œæˆï¼ˆå¦åˆ™ç¼–ç åºåˆ—çš„ç¬¬ä¸€ä¸ªæ ‡è®°å’Œæœ€åä¸€ä¸ªæ ‡è®°çš„å€¼å°†ä¸æ­£ç¡®ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[åå¤„ç†å™¨] ([https://huggingface.co/docs/tokenizers/api/post-processors](https://huggingface.co/docs/tokenizers/api/post-processors)) æ–‡æ¡£ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)ï¼Œå…¶ä¸­åŒ…å«å¤§éƒ¨åˆ†ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `build_inputs_with_special_tokens`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L272)

```py
( token_ids_0 token_ids_1 = None )
```

#### `get_special_tokens_mask`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3772)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) â†’ export const metadata = 'undefined';A list of integers in the range [0, 1]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ç¬¬ä¸€ä¸ªåºåˆ—çš„IDåˆ—è¡¨ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” ç¬¬äºŒä¸ªåºåˆ—çš„IDåˆ—è¡¨ã€‚

+   `already_has_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ ‡è®°åˆ—è¡¨æ˜¯å¦å·²ç»æ ¼å¼åŒ–ä¸ºæ¨¡å‹çš„ç‰¹æ®Šæ ‡è®°ã€‚

è¿”å›

ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ŒèŒƒå›´ä¸º [0, 1]

1 ä»£è¡¨ç‰¹æ®Šæ ‡è®°ï¼Œ0 ä»£è¡¨åºåˆ—æ ‡è®°ã€‚

ä»æ²¡æœ‰æ·»åŠ ç‰¹æ®Šæ ‡è®°çš„æ ‡è®°åˆ—è¡¨ä¸­æ£€ç´¢åºåˆ—IDã€‚å½“ä½¿ç”¨åˆ†è¯å™¨çš„ `prepare_for_model` æˆ– `encode_plus` æ–¹æ³•æ·»åŠ ç‰¹æ®Šæ ‡è®°æ—¶ï¼Œå°†è°ƒç”¨æ­¤æ–¹æ³•ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)

```py
( token_ids_0: List token_ids_1: Optional = None ) â†’ export const metadata = 'undefined';List[int]
```

å‚æ•°

+   `token_ids_0` (`List[int]`) â€” ç¬¬ä¸€ä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

+   `token_ids_1` (`List[int]`, *å¯é€‰*) â€” ç¬¬äºŒä¸ªæ ‡è®°åŒ–åºåˆ—ã€‚

è¿”å›

`List[int]`

æ ‡è®°ç±»å‹IDã€‚

åˆ›å»ºä¸ä¼ é€’çš„åºåˆ—å¯¹åº”çš„æ ‡è®°ç±»å‹IDã€‚[ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

å¦‚æœæ¨¡å‹æœ‰ç‰¹æ®Šçš„æ„å»ºæ–¹å¼ï¼Œåˆ™åº”è¯¥åœ¨å­ç±»ä¸­é‡å†™æ­¤æ–¹æ³•ã€‚

#### `update_post_processor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L146)

```py
( )
```

ä½¿ç”¨å½“å‰çš„ `bos_token` å’Œ `eos_token` æ›´æ–°åº•å±‚åå¤„ç†å™¨ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/tokenization_llama_fast.py#L190)

```py
( save_directory: str filename_prefix: Optional = None )
```

## LlamaModel

### `class transformers.LlamaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L939)

```py
( config: LlamaConfig )
```

å‚æ•°

+   `config`ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚config - LlamaConfig

è£¸çš„LLaMAæ¨¡å‹è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

ç”±*config.num_hidden_layers*å±‚ç»„æˆçš„Transformerè§£ç å™¨ã€‚æ¯ä¸€å±‚éƒ½æ˜¯`LlamaDecoderLayer`ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L974)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºæ ‡è®°æœªè¢«é®ç½©ï¼Œ

    +   0è¡¨ç¤ºæ ‡è®°è¢«é®ç½©ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›é®ç½©ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯é€‰åœ°åªéœ€è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«é®ç½©ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«é®ç½©ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.n_positions - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰- é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹;

    +   å…ƒç»„`tuple(torch.FloatTensor)`çš„é•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæœªä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

[LlamaModel](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## LlamaForCausalLM

### `class transformers.LlamaForCausalLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1106)

```py
( config )
```

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1136)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`æ©ç `ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`æ©ç `ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ - æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚é€‰æ‹©èŒƒå›´åœ¨`[0, config.n_positions - 1]`å†…ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰ - é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›

    +   é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`çš„å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ - å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ - æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

    å‚æ•° - æ ‡ç­¾ï¼ˆ`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*ï¼‰ï¼šç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”è¯¥åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…ï¼Œæˆ–è€…ä¸º-100ï¼ˆå‚è§`input_ids`æ–‡æ¡£ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚

è¿”å›

[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutputWithPast](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast)æˆ–`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰ï¼ŒåŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰ - è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[LlamaForCausalLM](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForCausalLM)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, LlamaForCausalLM

>>> model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
>>> tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")

>>> prompt = "Hey, are you conscious? Can you talk to me?"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(inputs.input_ids, max_length=30)
>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
```

## LlamaForSequenceClassification

### `class transformers.LlamaForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1295)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

LLaMaæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰ã€‚

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification)ä½¿ç”¨æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒå…¶ä»–å› æœæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-2ï¼‰ä¸€æ ·ã€‚

ç”±äºå®ƒå¯¹æœ€åä¸€ä¸ªæ ‡è®°è¿›è¡Œåˆ†ç±»ï¼Œå› æ­¤éœ€è¦çŸ¥é“æœ€åä¸€ä¸ªæ ‡è®°çš„ä½ç½®ã€‚å¦‚æœåœ¨é…ç½®ä¸­å®šä¹‰äº†`pad_token_id`ï¼Œåˆ™åœ¨æ¯è¡Œä¸­æ‰¾åˆ°ä¸æ˜¯å¡«å……æ ‡è®°çš„æœ€åä¸€ä¸ªæ ‡è®°ã€‚å¦‚æœæœªå®šä¹‰`pad_token_id`ï¼Œåˆ™ç®€å•åœ°å–æ‰¹æ¬¡ä¸­æ¯è¡Œçš„æœ€åä¸€ä¸ªå€¼ã€‚å½“ä¼ é€’`inputs_embeds`è€Œä¸æ˜¯`input_ids`æ—¶ï¼Œæ— æ³•çŒœæµ‹å¡«å……æ ‡è®°ï¼Œå› æ­¤æ‰§è¡Œç›¸åŒæ“ä½œï¼ˆå–æ‰¹æ¬¡ä¸­æ¯è¡Œçš„æœ€åä¸€ä¸ªå€¼ï¼‰ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼Œè°ƒæ•´è¾“å…¥åµŒå…¥ï¼Œä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªPyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_llama.py#L1326)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›ï¼Œå°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.Tensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   1è¡¨ç¤ºæœªè¢«â€œæ©ç›–â€çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºè¢«â€œæ©ç›–â€çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«â€œæ©ç›–â€ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«â€œæ©ç›–â€ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.n_positions - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values`ï¼ˆ`Cache`æˆ–`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼‰- é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—å’Œäº¤å‰æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ã€‚è¿™é€šå¸¸åŒ…æ‹¬æ¨¡å‹åœ¨å…ˆå‰è§£ç é˜¶æ®µè¿”å›çš„`past_key_values`ï¼Œå½“`use_cache=True`æˆ–`config.use_cache=True`æ—¶ã€‚

    å…è®¸ä¸¤ç§æ ¼å¼ï¼š

    +   ä¸€ä¸ª[Cache](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Cache)å®ä¾‹ï¼›

    +   å…ƒç»„ï¼Œé•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰2ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    æ¨¡å‹å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„ç¼“å­˜æ ¼å¼ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’`past_key_values`ï¼Œåˆ™å°†è¿”å›ä¼ ç»Ÿçš„ç¼“å­˜æ ¼å¼ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`input_ids`ï¼ˆå³æœªå°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„é‚£äº›ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`input_ids`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™å°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚é˜…`past_key_values`ï¼‰ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

[LlamaForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaForSequenceClassification) å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜å¿½ç•¥å®ƒä»¬ã€‚

## FlaxLlamaModel

### `class transformers.FlaxLlamaModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_flax_llama.py#L634)

```py
( config: LlamaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *optional*, é»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`æˆ–`jax.numpy.bfloat16`ä¹‹ä¸€ã€‚

    è¿™å¯ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸ä¼šå½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœå¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚ 

è£¸ Llama æ¨¡å‹å˜å‹å™¨ï¼Œè¾“å‡ºåŸå§‹éšè—çŠ¶æ€è€Œä¸å¸¦ä»»ä½•ç‰¹å®šå¤´éƒ¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒ JAX çš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_flax_llama.py#L458)

```py
( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`numpy.ndarray` of shape `(batch_size, input_ids_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›å¡«å……ï¼Œåˆ™å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œæ‚¨åº”è¯¥é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨æœªè¢«`masked`ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.n_positions - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `past_key_values` (`Dict[str, np.ndarray]`, *optional*, ç”±`init_cache`è¿”å›æˆ–åœ¨ä¼ é€’å…ˆå‰çš„`past_key_values`æ—¶è¿”å›) â€” é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰çš„å­—å…¸ï¼Œå¯ç”¨äºå¿«é€Ÿè‡ªå›å½’è§£ç ã€‚é¢„å…ˆè®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º*[batch_size, max_length]*ã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨è‡ªæ³¨æ„åŠ›å¤´ä¸­ç”¨äºè®¡ç®—åŠ æƒå¹³å‡å€¼çš„æ³¨æ„åŠ›æƒé‡ä¹‹åçš„æ³¨æ„åŠ› softmaxã€‚

`FlaxLlamaPreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

æ­¤ç¤ºä¾‹ä½¿ç”¨ä¸€ä¸ªéšæœºæ¨¡å‹ï¼Œå› ä¸ºçœŸå®çš„æ¨¡å‹éƒ½éå¸¸å¤§ã€‚ä¸ºäº†è·å¾—æ­£ç¡®çš„ç»“æœï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ openlm-research/open_llama_3b_v2 è€Œä¸æ˜¯ afmck/testing-llama-tinyã€‚å¦‚æœåœ¨åŠ è½½è¯¥æ£€æŸ¥ç‚¹æ—¶å‡ºç°å†…å­˜ä¸è¶³çš„æƒ…å†µï¼Œæ‚¨å¯ä»¥å°è¯•åœ¨ `from_pretrained` è°ƒç”¨ä¸­æ·»åŠ  `device_map="auto"`ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxLlamaModel

>>> tokenizer = AutoTokenizer.from_pretrained("afmck/testing-llama-tiny")
>>> model = FlaxLlamaModel.from_pretrained("afmck/testing-llama-tiny")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FlaxLlamaForCausalLM

### `class transformers.FlaxLlamaForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_flax_llama.py#L695)

```py
( config: LlamaConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config` ([LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º `jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯ `jax.numpy.float32`ã€`jax.numpy.float16` æˆ– `jax.numpy.bfloat16` ä¸­çš„ä¸€ä¸ªã€‚

    è¿™å¯ä»¥ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº† `dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„ `dtype` æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„ dtypeï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„ dtypeã€‚`

    å¦‚æœæ‚¨å¸Œæœ›æ›´æ”¹æ¨¡å‹å‚æ•°çš„ dtypeï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

å¸¦æœ‰è¯­è¨€å»ºæ¨¡å¤´ï¼ˆçº¿æ€§å±‚ï¼‰çš„ Llama æ¨¡å‹å˜å‹å™¨ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒ JAX çš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ç¼–è¯‘ (JIT)](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/llama/modeling_flax_llama.py#L458)

```py
( input_ids attention_mask = None position_ids = None params: dict = None past_key_values: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, input_ids_length)`çš„`numpy.ndarray`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚é»˜è®¤æƒ…å†µä¸‹å°†å¿½ç•¥å¡«å……ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../æœ¯è¯­è¡¨#è¾“å…¥ID)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../æœ¯è¯­è¡¨#æ³¨æ„åŠ›æ©ç )

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)å’Œ[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯èƒ½åªéœ€è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

    å¦‚æœè¦æ›´æ”¹å¡«å……è¡Œä¸ºï¼Œåº”é˜…è¯»`modeling_opt._prepare_decoder_attention_mask`å¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³é»˜è®¤ç­–ç•¥çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/1910.13461)ä¸­çš„å›¾è¡¨1ã€‚

    +   1è¡¨ç¤ºå¤´éƒ¨`æœªè¢«æ©ç `ï¼Œ

    +   0è¡¨ç¤ºå¤´éƒ¨`è¢«æ©ç `ã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`numpy.ndarray`ï¼Œ*å¯é€‰*ï¼‰- æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.n_positions - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../æœ¯è¯­è¡¨#ä½ç½®ID)

+   `past_key_values`ï¼ˆ`Dict[str, np.ndarray]`ï¼Œ*å¯é€‰*ï¼Œç”±`init_cache`è¿”å›æˆ–ä¼ é€’å…ˆå‰çš„`past_key_values`æ—¶è¿”å›ï¼‰- é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€å­—å…¸ï¼ˆæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºå¿«é€Ÿè‡ªå›å½’è§£ç ã€‚é¢„å…ˆè®¡ç®—çš„é”®å’Œå€¼éšè—çŠ¶æ€çš„å½¢çŠ¶ä¸º*[batch_size, max_length]*ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[LlamaConfig](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`jnp.ndarray`ï¼‰- è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) â€” Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) â€” Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxLlamaPreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åçš„å¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

è¿™ä¸ªä¾‹å­ä½¿ç”¨ä¸€ä¸ªéšæœºæ¨¡å‹ï¼Œå› ä¸ºçœŸå®çš„æ¨¡å‹éƒ½éå¸¸åºå¤§ã€‚ä¸ºäº†è·å¾—æ­£ç¡®çš„ç»“æœï¼Œæ‚¨åº”è¯¥ä½¿ç”¨ openlm-research/open_llama_3b_v2ï¼Œè€Œä¸æ˜¯ afmck/testing-llama-tinyã€‚å¦‚æœåœ¨åŠ è½½è¯¥æ£€æŸ¥ç‚¹æ—¶å‡ºç°å†…å­˜ä¸è¶³çš„æƒ…å†µï¼Œæ‚¨å¯ä»¥å°è¯•åœ¨ `from_pretrained` è°ƒç”¨ä¸­æ·»åŠ  `device_map="auto"`ã€‚

ä¾‹å¦‚ï¼š

```py
>>> from transformers import AutoTokenizer, FlaxLlamaForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("afmck/testing-llama-tiny")
>>> model = FlaxLlamaForCausalLM.from_pretrained("afmck/testing-llama-tiny")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
>>> outputs = model(**inputs)

>>> # retrieve logts for next token
>>> next_token_logits = outputs.logits[:, -1]
```
