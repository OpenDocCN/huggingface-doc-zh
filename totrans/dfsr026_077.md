# 加速推理

> 原始文本：[https://huggingface.co/docs/diffusers/optimization/fp16](https://huggingface.co/docs/diffusers/optimization/fp16)

有几种方法可以优化🤗 Diffusers以提高推理速度。作为一个经验法则，我们建议在PyTorch 2.0中使用[xFormers](xformers)或`torch.nn.functional.scaled_dot_product_attention`进行内存高效的注意力。

在许多情况下，为了提高速度或内存而进行优化会导致另一方面的性能提升，因此您应该尽可能在两者上进行优化。本指南侧重于推理速度，但您可以在[减少内存使用](memory)指南中了解更多关于保留内存的信息。

下面的结果是在Nvidia Titan RTX上使用50个DDIM步骤从提示`a photo of an astronaut riding a horse on mars`生成单个512x512图像获得的，展示了您可以期望的加速。

|  | 延迟 | 加速 |
| --- | --- | --- |
| 原始 | 9.50秒 | x1 |
| fp16 | 3.61秒 | x2.63 |
| 通道最后 | 3.30秒 | x2.88 |
| 追踪的UNet | 3.21秒 | x2.96 |
| 内存高效的注意力 | 2.63秒 | x3.61 |

## 使用TensorFloat-32

在安培及更高版本的CUDA设备上，矩阵乘法和卷积可以使用[张量浮点32（TF32）](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)模式进行更快但略微不准确的计算。默认情况下，PyTorch为卷积启用TF32模式，但不为矩阵乘法启用。除非您的网络需要完整的float32精度，否则我们建议为矩阵乘法启用TF32。它可以显着加快计算速度，通常几乎不会损失数值精度。

```py
import torch

torch.backends.cuda.matmul.allow_tf32 = True
```

您可以在[混合精度训练](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#tf32)指南中了解有关TF32的更多信息。

## 半精度权重

为了节省GPU内存并获得更快速度，请尝试直接在半精度或float16中加载和运行模型权重：

```py
import torch
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
```

在任何流水线中不要使用[`torch.autocast`](https://pytorch.org/docs/stable/amp.html#torch.autocast)，因为它可能导致黑色图像，并且始终比纯float16精度慢。
