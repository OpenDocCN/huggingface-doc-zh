- en: Efficient Training on CPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 CPU 上高效训练
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_cpu](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_cpu)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_cpu](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_cpu)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This guide focuses on training large models efficiently on CPU.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南侧重于在 CPU 上高效训练大型模型。
- en: Mixed precision with IPEX
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 IPEX 的混合精度
- en: IPEX is optimized for CPUs with AVX-512 or above, and functionally works for
    CPUs with only AVX2\. So, it is expected to bring performance benefit for Intel
    CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs
    or older Intel CPUs) might result in a better performance under IPEX, but not
    guaranteed. IPEX provides performance optimizations for CPU training with both
    Float32 and BFloat16\. The usage of BFloat16 is the main focus of the following
    sections.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: IPEX 优化了 AVX-512 或更高版本的 CPU，并且在仅具有 AVX2 的 CPU 上也可以正常工作。因此，预计在具有 AVX-512 或更高版本的英特尔
    CPU 世代中，IPEX 将带来性能优势，而仅具有 AVX2 的 CPU（例如 AMD CPU 或较旧的英特尔 CPU）在 IPEX 下可能会获得更好的性能，但不能保证。IPEX
    为使用 Float32 和 BFloat16 进行 CPU 训练提供了性能优化。以下部分的主要重点是使用 BFloat16。
- en: Low precision data type BFloat16 has been natively supported on the 3rd Generation
    Xeon® Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will
    be supported on the next generation of Intel® Xeon® Scalable Processors with Intel®
    Advanced Matrix Extensions (Intel® AMX) instruction set with further boosted performance.
    The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10\.
    At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and
    BFloat16 optimization of operators has been massively enabled in Intel® Extension
    for PyTorch, and partially upstreamed to PyTorch master branch. Users can get
    better performance and user experience with IPEX Auto Mixed Precision.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 低精度数据类型 BFloat16 已经在第三代 Xeon® Scalable Processors（又称 Cooper Lake）上本地支持，具有 AVX512
    指令集，并将在下一代英特尔® Xeon® Scalable Processors 上支持，该处理器具有 Intel® Advanced Matrix Extensions（Intel®
    AMX）指令集，进一步提升性能。自 PyTorch-1.10 以来，已启用了 CPU 后端的自动混合精度。同时，Intel® Extension for PyTorch
    大规模启用了 CPU 和 BFloat16 优化运算符的自动混合精度，并部分上游到 PyTorch 主分支。用户可以通过 IPEX 自动混合精度获得更好的性能和用户体验。
- en: Check more detailed information for [Auto Mixed Precision](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多关于 [自动混合精度](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html)
    的详细信息。
- en: 'IPEX installation:'
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IPEX 安装：
- en: 'IPEX release is following PyTorch, to install via pip:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: IPEX 发布遵循 PyTorch，可以通过 pip 安装：
- en: '| PyTorch Version | IPEX version |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch 版本 | IPEX 版本 |'
- en: '| :-: | :-: |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| :-: | :-: |'
- en: '| 2.1.x | 2.1.100+cpu |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 2.1.x | 2.1.100+cpu |'
- en: '| 2.0.x | 2.0.100+cpu |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 2.0.x | 2.0.100+cpu |'
- en: '| 1.13 | 1.13.0+cpu |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 1.13 | 1.13.0+cpu |'
- en: '| 1.12 | 1.12.300+cpu |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 1.12 | 1.12.300+cpu |'
- en: Please run `pip list | grep torch` to get your `pytorch_version`, so you can
    get the `IPEX version_name`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请运行 `pip list | grep torch` 以获取您的 `pytorch_version`，这样您就可以获得 `IPEX version_name`。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can check the latest versions in [ipex-whl-stable-cpu](https://developer.intel.com/ipex-whl-stable-cpu)
    if needed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以在 [ipex-whl-stable-cpu](https://developer.intel.com/ipex-whl-stable-cpu)
    中查看最新版本。
- en: Check more approaches for [IPEX installation](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多关于 [IPEX 安装](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html)
    的方法。
- en: Usage in Trainer
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Trainer 中的用法
- en: To enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`,
    `bf16` and `no_cuda` in training command arguments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Trainer 中启用 IPEX 的自动混合精度，用户应该在训练命令参数中添加 `use_ipex`、`bf16` 和 `no_cuda`。
- en: Take an example of the use cases on [Transformers question-answering](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以 [Transformers 问答](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)
    用例为例
- en: 'Training with IPEX using BF16 auto mixed precision on CPU:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BF16 自动混合精度在 CPU 上进行 IPEX 训练：
- en: '[PRE1]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to enable `use_ipex` and `bf16` in your script, add these parameters
    to `TrainingArguments` like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要在脚本中启用 `use_ipex` 和 `bf16`，请像这样将这些参数添加到 `TrainingArguments` 中：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Practice example
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实践示例
- en: 'Blog: [Accelerating PyTorch Transformers with Intel Sapphire Rapids](https://huggingface.co/blog/intel-sapphire-rapids)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 博客：[使用英特尔 Sapphire Rapids 加速 PyTorch Transformers](https://huggingface.co/blog/intel-sapphire-rapids)
