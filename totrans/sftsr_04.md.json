["```py\nfrom safetensors.torch import load_model, save_model\n\nsave_model(model, \"model.safetensors\")\n# Instead of save_file(model.state_dict(), \"model.safetensors\")\n\nload_model(model, \"model.safetensors\")\n# Instead of model.load_state_dict(load_file(\"model.safetensors\"))\n```", "```py\nfrom torch import nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Linear(100, 100)\n        self.b = self.a\n\n    def forward(self, x):\n        return self.b(self.a(x))\n\nmodel = Model()\nprint(model.state_dict())\n# odict_keys(['a.weight', 'a.bias', 'b.weight', 'b.bias'])\ntorch.save(model.state_dict(), \"model.bin\")\n# This file is now 41k instead of ~80k, because A and B are the same weight hence only 1 is saved on disk with both `a` and `b` pointing to the same buffer\n```", "```py\n    with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n        a = f.get_tensor(\"a\")\n        b = f.get_tensor(\"b\")\n    ```", "```py\n    a = torch.zeros((100, 100))\n    b = a[:1, :]\n    torch.save({\"b\": b}, \"model.bin\")\n    # File is 41k instead of the expected 400 bytes\n    # In practice it could happen that you save several 10GB instead of 1GB.\n    ```"]