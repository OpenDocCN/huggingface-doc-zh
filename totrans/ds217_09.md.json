["```py\npip install transformers\n```", "```py\n>>> from transformers import AutoTokenizer\n>>> from datasets import load_dataset\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n```", "```py\n>>> tokenizer(dataset[0][\"text\"])\n{'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102], \n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```", "```py\n>>> def tokenization(example):\n...     return tokenizer(example[\"text\"])\n\n>>> dataset = dataset.map(tokenization, batched=True)\n```", "```py\n>>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n>>> dataset.format['type']\n'torch'\n```", "```py\n>>> from transformers import DataCollatorWithPadding\n\n>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n>>> tf_dataset = dataset.to_tf_dataset(\n...     columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n...     label_cols=[\"label\"],\n...     batch_size=2,\n...     collate_fn=data_collator,\n...     shuffle=True\n... )\n```", "```py\n>>> from transformers import AutoFeatureExtractor\n>>> from datasets import load_dataset, Audio\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n```", "```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 8000}\n```", "```py\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```", "```py\n>>> def preprocess_function(examples):\n...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n...     )\n...     return inputs\n\n>>> dataset = dataset.map(preprocess_function, batched=True)\n```", "```py\n>>> from transformers import AutoFeatureExtractor\n>>> from datasets import load_dataset, Image\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n>>> dataset = load_dataset(\"beans\", split=\"train\")\n```", "```py\n>>> dataset[0][\"image\"]\n```", "```py\n>>> from torchvision.transforms import RandomRotation\n\n>>> rotate = RandomRotation(degrees=(0, 90))\n>>> def transforms(examples):\n...     examples[\"pixel_values\"] = [rotate(image.convert(\"RGB\")) for image in examples[\"image\"]]\n...     return examples\n```", "```py\n>>> dataset.set_transform(transforms)\n>>> dataset[0][\"pixel_values\"]\n```"]