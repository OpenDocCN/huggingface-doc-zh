- en: CLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The CLIP model was proposed in [Learning Transferable Visual Models From Natural
    Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong
    Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
    Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.
    CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on
    a variety of (image, text) pairs. It can be instructed in natural language to
    predict the most relevant text snippet, given an image, without directly optimizing
    for the task, similarly to the zero-shot capabilities of GPT-2 and 3.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型是由Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini
    Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger、Ilya
    Sutskever在[从自然语言监督中学习可转移的视觉模型](https://arxiv.org/abs/2103.00020)中提出的。CLIP（对比语言-图像预训练）是一个在各种（图像，文本）对上训练的神经网络。它可以用自然语言指导来预测最相关的文本片段，给定一个图像，而不直接为任务进行优化，类似于GPT-2和3的零-shot能力。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*State-of-the-art computer vision systems are trained to predict a fixed set
    of predetermined object categories. This restricted form of supervision limits
    their generality and usability since additional labeled data is needed to specify
    any other visual concept. Learning directly from raw text about images is a promising
    alternative which leverages a much broader source of supervision. We demonstrate
    that the simple pre-training task of predicting which caption goes with which
    image is an efficient and scalable way to learn SOTA image representations from
    scratch on a dataset of 400 million (image, text) pairs collected from the internet.
    After pre-training, natural language is used to reference learned visual concepts
    (or describe new ones) enabling zero-shot transfer of the model to downstream
    tasks. We study the performance of this approach by benchmarking on over 30 different
    existing computer vision datasets, spanning tasks such as OCR, action recognition
    in videos, geo-localization, and many types of fine-grained object classification.
    The model transfers non-trivially to most tasks and is often competitive with
    a fully supervised baseline without the need for any dataset specific training.
    For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot
    without needing to use any of the 1.28 million training examples it was trained
    on. We release our code and pre-trained model weights at this https URL.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*最先进的计算机视觉系统被训练来预测一组固定的预定对象类别。这种受限的监督形式限制了它们的普遍性和可用性，因为需要额外的标记数据来指定任何其他视觉概念。直接从关于图像的原始文本中学习是一个有前途的替代方案，它利用了更广泛的监督来源。我们证明了预测哪个标题与哪个图像相匹配的简单预训练任务是一种有效且可扩展的方式，可以从互联网收集的4亿（图像，文本）对数据集上从头开始学习SOTA图像表示。预训练后，自然语言用于引用学习的视觉概念（或描述新的概念），从而实现模型对下游任务的零-shot转移。我们通过在超过30个不同的现有计算机视觉数据集上进行基准测试来研究这种方法的性能，涵盖了OCR、视频中的动作识别、地理定位以及许多类型的细粒度对象分类等任务。该模型对大多数任务进行了非平凡的转移，并且通常与完全监督的基线具有竞争力，而无需进行任何特定数据集的训练。例如，我们在ImageNet零-shot上匹配了原始ResNet-50的准确率，而无需使用其训练的128万个训练示例中的任何一个。我们在此https
    URL上发布我们的代码和预训练模型权重。*'
- en: This model was contributed by [valhalla](https://huggingface.co/valhalla). The
    original code can be found [here](https://github.com/openai/CLIP).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[valhalla](https://huggingface.co/valhalla)贡献的。原始代码可以在[这里](https://github.com/openai/CLIP)找到。
- en: Usage tips and example
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示和示例
- en: CLIP is a multi-modal vision and language model. It can be used for image-text
    similarity and for zero-shot image classification. CLIP uses a ViT like transformer
    to get visual features and a causal language model to get the text features. Both
    the text and visual features are then projected to a latent space with identical
    dimension. The dot product between the projected image and text features is then
    used as a similar score.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP是一个多模态视觉和语言模型。它可用于图像文本相似性和零-shot图像分类。CLIP使用类似ViT的transformer获取视觉特征，并使用因果语言模型获取文本特征。然后将文本和视觉特征投影到具有相同维度的潜在空间。然后使用投影图像和文本特征之间的点积作为相似分数。
- en: To feed images to the Transformer encoder, each image is split into a sequence
    of fixed-size non-overlapping patches, which are then linearly embedded. A [CLS]
    token is added to serve as representation of an entire image. The authors also
    add absolute position embeddings, and feed the resulting sequence of vectors to
    a standard Transformer encoder. The [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    can be used to resize (or rescale) and normalize images for the model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像输入Transformer编码器，每个图像被分割成一系列固定大小且不重叠的补丁，然后进行线性嵌入。添加一个[CLS]标记作为整个图像的表示。作者还添加了绝对位置嵌入，并将结果向量序列馈送到标准Transformer编码器。[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)可用于调整（或重新缩放）和规范化图像以供模型使用。
- en: The [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    is used to encode the text. The [CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)
    wraps [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    and [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    into a single instance to both encode the text and prepare the images. The following
    example shows how to get the image-text similarity scores using [CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)
    and [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)对文本进行编码。[CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)将[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)和[CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)包装成单个实例，用于同时对文本进行编码和准备图像。以下示例展示了如何使用[CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)和[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)获取图像文本相似度分数。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Resources
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: A list of official Hugging Face and community (indicated by 🌎) resources to
    help you get started with CLIP.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 官方Hugging Face和社区（🌎标志）资源列表，帮助您开始使用CLIP。
- en: '[Fine tuning CLIP with Remote Sensing (Satellite) images and captions](https://huggingface.co/blog/fine-tune-clip-rsicd),
    a blog post about how to fine-tune CLIP with [RSICD dataset](https://github.com/201528014227051/RSICD_optimal)
    and comparison of performance changes due to data augmentation.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用遥感（卫星）图像和标题微调CLIP](https://huggingface.co/blog/fine-tune-clip-rsicd)，一篇关于如何使用[RSICD数据集](https://github.com/201528014227051/RSICD_optimal)微调CLIP并比较由于数据增强而导致的性能变化的博客文章。'
- en: This [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)
    shows how to train a CLIP-like vision-text dual encoder model using a pre-trained
    vision and text encoder using [COCO dataset](https://cocodataset.org/#home).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)展示了如何使用预训练的视觉和文本编码器训练类似CLIP的视觉-文本双编码器模型，使用[COCO数据集](https://cocodataset.org/#home)。
- en: ​ Image-to-Text
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ​ 图像到文本
- en: A [notebook](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)
    on how to use a pretrained CLIP for inference with beam search for image captioning.
    🌎
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的CLIP进行推理，使用波束搜索进行图像字幕生成的[笔记本](https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing)。🌎
- en: '**Image retrieval**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像检索**'
- en: A [notebook](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing)
    on image retrieval using pretrained CLIP and computing MRR(Mean Reciprocal Rank)
    score. 🌎
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的CLIP进行图像检索并计算MRR（平均倒数排名）分数的[笔记本](https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing)。🌎
- en: A [notebook](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb)
    on image retrieval and showing the similarity score. 🌎
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于图像检索和显示相似度分数的[笔记本](https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb)。🌎
- en: A [notebook](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing)
    on how to map images and texts to the same vector space using Multilingual CLIP.
    🌎
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Multilingual CLIP将图像和文本映射到相同向量空间的[笔记本](https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing)。🌎
- en: A [notebook](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR)
    on how to run CLIP on semantic image search using [Unsplash](https://unsplash.com)
    and [TMBD](https://www.themoviedb.org/) datasets. 🌎
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何在[Unsplash](https://unsplash.com)和[TMBD](https://www.themoviedb.org/)数据集上运行语义图像搜索的CLIP的[笔记本](https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR)。🌎
- en: '**Explainability**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释性**'
- en: A [notebook](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb)
    on how to visualize similarity between input token and image segment. 🌎
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何可视化输入标记和图像段之间相似性的[笔记本](https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb)。🌎
- en: If you’re interested in submitting a resource to be included here, please feel
    free to open a Pull Request and we will review it. The resource should ideally
    demonstrate something new instead of duplicating an existing resource.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣提交资源以包含在此处，请随时提交拉取请求，我们将进行审核。资源应该尽可能展示新内容，而不是重复现有资源。
- en: CLIPConfig
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPConfig
- en: '### `class transformers.CLIPConfig`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L266)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L266)'
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [CLIPTextConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextConfig).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_config` (`dict`, *可选*) — 用于初始化[CLIPTextConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextConfig)的配置选项字典。'
- en: '`vision_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [CLIPVisionConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionConfig).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_config` (`dict`, *可选*) — 用于初始化[CLIPVisionConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionConfig)的配置选项字典。'
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and vision projection layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *可选*, 默认为512) — 文本和视觉投影层的维度。'
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — The inital
    value of the *logit_scale* paramter. Default is used as per the original CLIP
    implementation.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logit_scale_init_value` (`float`, *可选*, 默认为2.6592) — *logit_scale*参数的初始值。默认值根据原始CLIP实现使用。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*可选*) — 关键字参数字典。'
- en: '[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)
    is the configuration class to store the configuration of a [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel).
    It is used to instantiate a CLIP model according to the specified arguments, defining
    the text model and vision model configs. Instantiating a configuration with the
    defaults will yield a similar configuration to that of the CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
    architecture.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)
    是用于存储[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)配置的类。它用于根据指定的参数实例化一个CLIP模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生与CLIP
    [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
    架构类似的配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `from_text_vision_configs`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_text_vision_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L402)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L402)'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Returns
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)'
- en: An instance of a configuration object
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)
    (or a derived class) from clip text model configuration and clip vision model
    configuration.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从clip文本模型配置和clip视觉模型配置实例化一个[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)（或派生类）。
- en: CLIPTextConfig
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPTextConfig
- en: '### `class transformers.CLIPTextConfig`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPTextConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L39)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L39)'
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 49408) — Vocabulary size of the
    CLIP text model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 49408) — CLIP文本模型的词汇量。定义了在调用[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)时可以表示的不同标记数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 512) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 512) — 编码器层和池化层的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 2048) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 2048) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and vision projection layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *optional*, defaults to 512) — 文本和视觉投影层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 77) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 77) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512或1024或2048）。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` `"quick_gelu"` are supported.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`)
    — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"` `"quick_gelu"`。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 1) — Padding token id.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, defaults to 1) — 填充标记id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 49406) — Beginning of stream
    token id.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, defaults to 49406) — 流开始标记id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 49407) — End of stream token
    id.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, defaults to 49407) — 流结束标记id。'
- en: This is the configuration class to store the configuration of a [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).
    It is used to instantiate a CLIP text encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the text encoder of the CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
    architecture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)配置的配置类。根据指定的参数实例化一个CLIP文本编码器，定义模型架构。使用默认值实例化配置将产生类似于CLIP
    [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)架构的文本编码器的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: CLIPVisionConfig
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPVisionConfig
- en: '### `class transformers.CLIPVisionConfig`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPVisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L157)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/configuration_clip.py#L157)'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and vision projection layers.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *可选*, 默认为512) — 文本和视觉投影层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *可选*, 默认为3) — 输入通道的数量。'
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *可选*, 默认为224) — 每个图像的大小（分辨率）。'
- en: '`patch_size` (`int`, *optional*, defaults to 32) — The size (resolution) of
    each patch.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *可选*, 默认为32) — 每个补丁的大小（分辨率）。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"quick_gelu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` ``"quick_gelu"` are supported.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *可选*, 默认为`"quick_gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`以及`"quick_gelu"`。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-05) — 层归一化层使用的epsilon。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *可选*, 默认为0.0) — 注意力概率的dropout比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 1.0) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *可选*, 默认为1.0) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。'
- en: This is the configuration class to store the configuration of a [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).
    It is used to instantiate a CLIP vision encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the vision encoder of the CLIP [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)
    architecture.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)配置的配置类。根据指定的参数实例化一个CLIP视觉编码器，定义模型架构。使用默认值实例化配置将产生类似于CLIP
    [openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32)架构的视觉编码器的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: CLIPTokenizer
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPTokenizer
- en: '### `class transformers.CLIPTokenizer`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L272)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L272)'
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — Path to the vocabulary file.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 词汇表文件的路径。'
- en: '`merges_file` (`str`) — Path to the merges file.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file` (`str`) — 合并文件的路径。'
- en: '`errors` (`str`, *optional*, defaults to `"replace"`) — Paradigm to follow
    when decoding bytes to UTF-8\. See [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)
    for more information.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errors`（`str`，*可选*，默认为`"replace"`）— 解码字节为UTF-8时要遵循的范例。有关更多信息，请参阅[bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode)。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|startoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*，默认为`“<|startoftext|>”`）--序列标记的开头。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The token
    used for padding, for example when batching sequences of different lengths.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--用于填充的令牌，例如，在批处理不同长度的序列时。'
- en: Construct a CLIP tokenizer. Based on byte-level Byte-Pair-Encoding.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个CLIP分词器。基于字节级字节对编码。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L359)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L359)'
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊令牌的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊令牌的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A CLIP sequence has the following
    format:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊令牌，从序列或序列对构建用于序列分类任务的模型输入。CLIP序列具有以下格式：
- en: 'single sequence: `<|startoftext|> X <|endoftext|>`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`<|startoftext|> X <|endoftext|>`
- en: Pairs of sequences are not the expected use case, but they will be handled without
    a separator.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 序列对不是预期的使用情况，但它们将在没有分隔符的情况下处理。
- en: '#### `get_special_tokens_mask`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L386)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L386)'
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens`（`bool`，*可选*，默认为`False`）— 令牌列表是否已经为模型格式化了特殊令牌。'
- en: Returns
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为[0, 1]：1表示特殊令牌，0表示序列令牌。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊令牌的令牌列表中检索序列id。当使用tokenizer的`prepare_for_model`方法添加特殊令牌时，将调用此方法。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L414)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L414)'
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 零列表。
- en: Create a mask from the two sequences passed. CLIP does not make use of token
    type ids, therefore a list of zeros is returned.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个掩码。CLIP不使用令牌类型id，因此返回一个零列表。
- en: '#### `save_vocabulary`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L509)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip.py#L509)'
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: CLIPTokenizerFast
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPTokenizerFast
- en: '### `class transformers.CLIPTokenizerFast`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L50)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L50)'
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`, *optional*) — Path to the vocabulary file.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file`（`str`，*可选*）— 词汇文件的路径。'
- en: '`merges_file` (`str`, *optional*) — Path to the merges file.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges_file`（`str`，*可选*）— 合并文件的路径。'
- en: '`tokenizer_file` (`str`, *optional*) — The path to a tokenizer file to use
    instead of the vocab file.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file`（`str`，*可选*）— 要使用的分词器文件的路径，而不是词汇文件。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The unknown
    token. A token that is not in the vocabulary cannot be converted to an ID and
    is set to be this token instead.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--未知令牌。词汇表中没有的令牌无法转换为ID，而是设置为该令牌。'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|startoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*，默认为`“<|startoftext|>”`）--序列标记的开头。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列结束标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The token
    used for padding, for example when batching sequences of different lengths.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--用于填充的令牌，例如，在批处理不同长度的序列时。'
- en: Construct a “fast” CLIP tokenizer (backed by HuggingFace’s *tokenizers* library).
    Based on byte-level Byte-Pair-Encoding.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”CLIP分词器（由HuggingFace的*tokenizers*库支持）。基于字节级字节对编码。
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L127)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L127)'
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0`（`List[int]`）— 将添加特殊令牌的ID列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1`（`List[int]`，*可选*）— 序列对的可选第二个ID列表。'
- en: Returns
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 具有适当特殊令牌的[输入ID](../glossary#input-ids)列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A CLIP sequence has the following
    format:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `<|startoftext|> X <|endoftext|>`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pairs of sequences are not the expected use case, but they will be handled without
    a separator.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/tokenization_clip_fast.py#L154)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: List of zeros.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed. CLIP does not make use of token
    type ids, therefore a list of zeros is returned.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: CLIPImageProcessor
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.CLIPImageProcessor`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/image_processing_clip.py#L50)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by `do_resize` in the `preprocess` method.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`):
    Size of the image after resizing. The shortest edge of the image is resized to
    size[“shortest_edge”], with the longest edge resized to keep the input aspect
    ratio. Can be overridden by `size` in the `preprocess` method.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Can be overridden by `resample`
    in the `preprocess` method.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image to the specified `crop_size`. Can be overridden by `do_center_crop`
    in the `preprocess` method.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]` *optional*, defaults to 224) — Size of the output
    image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`
    method.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale`
    in the `preprocess` method.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by `rescale_factor` in
    the `preprocess` method.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by `do_normalize` in the `preprocess` method.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `[0.48145466,
    0.4578275, 0.40821073]`) — Mean to use if normalizing the image. This is a float
    or list of floats the length of the number of channels in the image. Can be overridden
    by the `image_mean` parameter in the `preprocess` method.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `[0.26862954,
    0.26130258, 0.27577711]`) — Standard deviation to use if normalizing the image.
    This is a float or list of floats the length of the number of channels in the
    image. Can be overridden by the `image_std` parameter in the `preprocess` method.
    Can be overridden by the `image_std` parameter in the `preprocess` method.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `True`) — Whether to convert
    the image to RGB.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a CLIP image processor.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/image_processing_clip.py#L177)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image after resizing. Shortest edge of the image is resized to size[“shortest_edge”],
    with the longest edge resized to keep the input aspect ratio.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — 调整大小后的图像尺寸。图像的最短边被调整为size[“shortest_edge”]，最长边被调整以保持输入的长宽比。'
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`.
    Only has an effect if `do_resize` is set to `True`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`int`, *optional*, defaults to `self.resample`) — 如果调整图像大小，则使用的重采样滤波器。可以是枚举`PILImageResampling`之一。仅在`do_resize`设置为`True`时有效。'
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    是否对图像进行中心裁剪。'
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    中心裁剪的尺寸。仅在`do_center_crop`设置为`True`时有效。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — 是否重新缩放图像。'
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    如果`do_rescale`设置为`True`，则用于重新缩放图像的缩放因子。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — 是否对图像进行归一化。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to use for normalization. Only has an effect if `do_normalize` is
    set to `True`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float`或`List[float]`, *optional*, defaults to `self.image_mean`)
    — 用于归一化的图像均值。仅在`do_normalize`设置为`True`时有效。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to use for normalization. Only has an effect if `do_normalize`
    is set to `True`.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float`或`List[float]`, *optional*, defaults to `self.image_std`)
    — 用于归一化的图像标准差。仅在`do_normalize`设置为`True`时有效。'
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    Whether to convert the image to RGB.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    是否将图像转换为RGB。'
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`或`TensorType`, *optional*) — 要返回的张量类型。可以是以下之一：'
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：返回一个`np.ndarray`列表。
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.TENSORFLOW`或`''tf''`：返回一个`tf.Tensor`类型的批次。'
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.PYTORCH`或`''pt''`：返回一个`torch.Tensor`类型的批次。'
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.NUMPY`或`''np''`：返回一个`np.ndarray`类型的批次。'
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TensorType.JAX`或`''jax''`：返回一个`jax.numpy.ndarray`类型的批次。'
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`ChannelDimension`或`str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — 输出图像的通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未设置：使用输入图像的通道维度格式。
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`ChannelDimension`或`str`, *optional*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：'
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。'
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。'
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。'
- en: Preprocess an image or batch of images.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理一张图像或一批图像。
- en: CLIPFeatureExtractor
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPFeatureExtractor
- en: '### `class transformers.CLIPFeatureExtractor`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPFeatureExtractor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/feature_extraction_clip.py#L26)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/feature_extraction_clip.py#L26)'
- en: '[PRE18]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: CLIPProcessor
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPProcessor
- en: '### `class transformers.CLIPProcessor`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L25)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L25)'
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` ([CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor),
    *optional*) — The image processor is a required input.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` ([CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor),
    *optional*) — 图像处理器是必需的输入。'
- en: '`tokenizer` ([CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast),
    *optional*) — The tokenizer is a required input.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` ([CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast),
    *optional*) — 分词器是必需的输入。'
- en: Constructs a CLIP processor which wraps a CLIP image processor and a CLIP tokenizer
    into a single processor.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个CLIP处理器，将CLIP图像处理器和CLIP分词器封装成一个处理器。
- en: '[CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)
    offers all the functionalities of [CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)
    and [CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor.decode)
    for more information.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor)提供了[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)和[CLIPTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizerFast)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPProcessor.decode)以获取更多信息。'
- en: '#### `batch_decode`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `批量解码`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L114)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L114)'
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This method forwards all its arguments to CLIPTokenizerFast’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给CLIPTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。
- en: '#### `decode`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `解码`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L121)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/processing_clip.py#L121)'
- en: '[PRE21]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This method forwards all its arguments to CLIPTokenizerFast’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将其所有参数转发给CLIPTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。
- en: PytorchHide Pytorch content
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: CLIPModel
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPModel
- en: '### `class transformers.CLIPModel`'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L925)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L925)'
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)）—
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `前向`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1057)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1057)'
- en: '[PRE23]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。默认情况下，如果提供填充，将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0,
    1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下会忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss` (`bool`, *可选*) — 是否返回对比损失。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_clip.CLIPOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_clip.CLIPOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.clip.modeling_clip.CLIPOutput` or a tuple of `torch.FloatTensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`)
    and inputs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_clip.CLIPOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class
    'transformers.models.clip.configuration_clip.CLIPConfig'>`)和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for image-text similarity.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当`return_loss`为`True`时返回)
    — 图像-文本相似度的对比损失。'
- en: '`logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`)
    — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似度分数。'
- en: '`logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`)
    — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似度分数。'
- en: '`text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The text
    embeddings obtained by applying the projection layer to the pooled output of [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的池化输出获得的文本嵌入。'
- en: '`image_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — The
    image embeddings obtained by applying the projection layer to the pooled output
    of [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的池化输出获得的图像嵌入。'
- en: '`text_model_output(BaseModelOutputWithPooling):` The output of the [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output(BaseModelOutputWithPooling):` [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的输出。'
- en: '`vision_model_output(BaseModelOutputWithPooling):` The output of the [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_model_output(BaseModelOutputWithPooling):` [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的输出。'
- en: The [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE24]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#### `get_text_features`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_text_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L961)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L961)'
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    输入序列标记在词汇表中的索引。默认情况下会忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../术语表#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../术语表#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../术语表#position-ids)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: text_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将投影层应用于[CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的池化输出获得的文本嵌入。
- en: The [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#### `get_image_features`'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_image_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1008)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1008)'
- en: '[PRE27]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）—
    像素值。默认情况下将忽略填充。如果提供，可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参见[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: image_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）
- en: The image embeddings obtained by applying the projection layer to the pooled
    output of [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将投影层应用于[CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的池化输出获得的图像嵌入。
- en: The [CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE28]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: CLIPTextModel
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPTextModel
- en: '### `class transformers.CLIPTextModel`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPTextModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L747)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L747)'
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The text model from CLIP without any head or projection on top. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 中的文本模型没有任何头部或顶部的投影。该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L768)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L768)'
- en: '[PRE30]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.Tensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩码的标记为 1，
- en: 0 for tokens that are `masked`.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩码的标记为 0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置 ID？](../glossary#position-ids)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是普通元组。'
- en: Returns
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)
    and inputs.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    或 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False`
    时），包括根据配置（`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor`）—
    模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是从预训练期间的下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax后的注意力权重。
- en: The [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)
    forward method, overrides the `__call__` special method.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE31]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: CLIPTextModelWithProjection
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPTextModelWithProjection
- en: '### `class transformers.CLIPTextModelWithProjection`'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPTextModelWithProjection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1151)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1151)'
- en: '[PRE32]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: CLIP Text Model with a projection layer on top (a linear layer on top of the
    pooled output).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部具有投影层（在池化输出的顶部的线性层）的CLIP文本模型。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1178)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1178)'
- en: '[PRE33]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Padding will be ignored by
    default should you provide it.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0,
    1]`范围内。'
- en: 1 for tokens that are `not masked`,
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。查看返回张量中的`attentions`以获取更多细节。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。查看返回张量中的`hidden_states`以获取更多细节。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_clip.CLIPTextModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_clip.CLIPTextModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.clip.modeling_clip.CLIPTextModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)
    and inputs.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_clip.CLIPTextModelOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者`config.return_dict=False`）包含各种元素，取决于配置（`<class
    'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入。
- en: '`text_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional*
    returned when model is initialized with `with_projection=True`) — The text embeddings
    obtained by applying the projection layer to the pooler_output.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，在使用`with_projection=True`初始化模型时返回）—
    通过将投影层应用于pooler_output获得的文本嵌入。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`）—
    模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [CLIPTextModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)
    forward method, overrides the `__call__` special method.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPTextModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModelWithProjection)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE34]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: CLIPVisionModelWithProjection
  id: totrans-403
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPVisionModelWithProjection
- en: '### `class transformers.CLIPVisionModelWithProjection`'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPVisionModelWithProjection`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1232)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1232)'
- en: '[PRE35]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法加载模型权重。'
- en: CLIP Vision Model with a projection layer on top (a linear layer on top of the
    pooled output).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有投影层的CLIP Vision模型（在池化输出的顶部有一个线性层）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1255)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L1255)'
- en: '[PRE36]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下将忽略填充。如果提供，可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。'
- en: Returns
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_clip.CLIPVisionModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_clip.CLIPVisionModelOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.clip.modeling_clip.CLIPVisionModelOutput` or a tuple
    of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`)
    and inputs.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_clip.CLIPVisionModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（`<class
    'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入。
- en: '`image_embeds` (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional*
    returned when model is initialized with `with_projection=True`) — The image embeddings
    obtained by applying the projection layer to the pooler_output.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim)` *可选*，当模型初始化时带有`with_projection=True`时返回)
    — 通过将投影层应用于pooler_output获得的图像嵌入。'
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个层输出的模型的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [CLIPVisionModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)
    forward method, overrides the `__call__` special method.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPVisionModelWithProjection](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用`Module`实例，而不是在这里调用，因为前者会处理运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE37]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: CLIPVisionModel
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIPVisionModel
- en: '### `class transformers.CLIPVisionModel`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.CLIPVisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L866)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L866)'
- en: '[PRE38]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The vision model from CLIP without any head or projection on top. This model
    inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP中的视觉模型，没有顶部的头部或投影。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L884)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_clip.py#L884)'
- en: '[PRE39]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`torch.FloatTensor`元组'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`)
    and inputs.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class
    'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`）和输入的各种元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后的序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是从下一个句子预测（分类）目标在预训练期间训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个
    + 每个层的输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是此函数，因为前者会负责运行预处理和后处理步骤，而后者会默默忽略它们。
- en: 'Examples:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE40]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: TensorFlowHide TensorFlow content
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFCLIPModel
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCLIPModel
- en: '### `class transformers.TFCLIPModel`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFCLIPModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1307)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1307)'
- en: '[PRE41]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该“只需工作” -
    只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果要在Keras方法之外使用第二种格式，例如在使用Keras`Functional`
    API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1398)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1398)'
- en: '[PRE42]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str,
    np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`） — 输入序列标记在词汇表中的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values`（形状为`(batch_size, num_channels, height, width)`的`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str,
    tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例必须具有相同形状）— 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）—
    避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：'
- en: 1 for tokens that are `not masked`,
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被遮罩的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被遮罩的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`，*可选*）—
    每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`return_loss` (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_loss`（`bool`，*可选*）— 是否返回对比损失。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅可在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数仅可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.clip.modeling_tf_clip.TFCLIPOutput` or `tuple(tf.Tensor)`'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.clip.modeling_tf_clip.TFCLIPOutput` or a tuple of `tf.Tensor`
    (if `return_dict=False` is passed or when `config.return_dict=False`) comprising
    various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`)
    and inputs.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class
    'transformers.models.clip.configuration_clip.CLIPConfig'>`）和输入的不同元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True`) — Contrastive loss for image-text similarity.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`tf.Tensor`，*可选*，当`return_loss`为`True`时返回）— 图像-文本相似性的对比损失。'
- en: '`logits_per_image:(tf.Tensor` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_image:(形状为`(image_batch_size, text_batch_size)`的`tf.Tensor`) —
    `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。'
- en: '`logits_per_text:(tf.Tensor` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_per_text:(形状为`(text_batch_size, image_batch_size)`的`tf.Tensor`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。'
- en: '`text_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — The text embeddings
    obtained by applying the projection layer to the pooled output of [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel).'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_embeds(tf.Tensor`的形状为`(batch_size, output_dim)`）— 通过将投影层应用于[TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的汇聚输出获得的文本嵌入。'
- en: '`image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — The image embeddings
    obtained by applying the projection layer to the pooled output of [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel).'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将池化输出应用于[TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的投影层获得的图像嵌入。'
- en: '`text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` The output
    of the [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel).'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的输出。'
- en: '`vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` The
    output of the [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel).'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的输出。'
- en: The [TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE43]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#### `get_text_features`'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_text_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1316)'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1316)'
- en: '[PRE44]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    或 `Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, sequence_length)`) — 词汇表中输入序列令牌的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`np.ndarray` 或 `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-520
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未被掩码`的令牌为1，
- en: 0 for tokens that are `masked`.
  id: totrans-521
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被掩码`的令牌为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`np.ndarray` 或 `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *可选*) — 每个输入序列令牌的位置在位置嵌入中的索引。选择范围为`[0, config.max_position_embeddings - 1]`。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数仅在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *可选*, 默认为`False“) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: text_features (`tf.Tensor` of shape `(batch_size, output_dim`)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: text_features (`tf.Tensor` of shape `(batch_size, output_dim`)
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel).
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将池化输出应用于[TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的投影层获得的文本嵌入。
- en: The [TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE45]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#### `get_image_features`'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_image_features`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1356)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1356)'
- en: '[PRE46]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details. output_attentions (`bool`, *optional*): Whether or not to return
    the attentions tensors of all attention layers. See `attentions` under returned
    tensors for more detail. This argument can be used only in eager mode, in graph
    mode the value in the config will be used instead.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, num_channels, height, width)`)
    — 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。output_attentions
    (`bool`，*可选*): 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为
    True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`，*可选*，默认为 `False“) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: image_features (`tf.Tensor` of shape `(batch_size, output_dim`)
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: image_features (`tf.Tensor`，形状为 `(batch_size, output_dim`)
- en: The image embeddings obtained by applying the projection layer to the pooled
    output of [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将投影层应用于[TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的汇总输出获得的图像嵌入。
- en: The [TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)
    forward method, overrides the `__call__` special method.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCLIPModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPModel)的前向方法，覆盖了
    `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE47]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: TFCLIPTextModel
  id: totrans-551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCLIPTextModel
- en: '### `class transformers.TFCLIPTextModel`'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFCLIPTextModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1185)'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1185)'
- en: '[PRE48]'
  id: totrans-554
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '#### `call`'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1193)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1193)'
- en: '[PRE49]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    或 `Dict[str, np.ndarray]`，每个示例的形状必须为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。'
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入 ID？](../glossary#input-ids)'
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 代表未被屏蔽的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 代表被屏蔽的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`np.ndarray`或形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*)
    — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training` (`bool`, *可选*, 默认为`False“) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或`tf.Tensor`元组'
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)
    and inputs.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（`<class
    'transformers.models.clip.configuration_clip.CLIPTextConfig'>`）和输入。
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`)
    — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过线性层和Tanh激活函数进一步处理。线性层的权重在预训练期间从下一个句子预测（分类）目标中训练。'
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个输出通常*不是*输入的语义内容的好摘要，您通常最好对整个输入序列的隐藏状态进行平均或池化。
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自注意力头中的注意力softmax之后的注意力权重，用于计算加权平均值。
- en: The [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)
    forward method, overrides the `__call__` special method.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Examples:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE50]'
  id: totrans-585
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: TFCLIPVisionModel
  id: totrans-586
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFCLIPVisionModel
- en: '### `class transformers.TFCLIPVisionModel`'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFCLIPVisionModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1245)'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1245)'
- en: '[PRE51]'
  id: totrans-589
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '#### `call`'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_tf_clip.py#L1254)'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-592
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Parameters
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_channels, height, width)`) — Pixel values. Pixel values can be obtained using
    [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details. output_attentions (`bool`, *optional*): Whether or not to return
    the attentions tensors of all attention layers. See `attentions` under returned
    tensors for more detail. This argument can be used only in eager mode, in graph
    mode the value in the config will be used instead.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`)
    and inputs.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)
    forward method, overrides the `__call__` special method.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: JAXHide JAX content
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: FlaxCLIPModel
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxCLIPModel`'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1262)'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Parameters
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([CLIPConfig](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`) —
    The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16`
    (on GPUs) and `jax.numpy.bfloat16` (on TPUs).'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be used to enable mixed-precision training or half-precision inference
    on GPUs or TPUs. If specified all the computation will be performed with the given
    `dtype`.
  id: totrans-620
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Note that this only specifies the dtype of the computation and does not influence
    the dtype of model parameters.`'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you wish to change the dtype of the model parameters, see [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)
    and [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16).
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This model inherits from [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading, saving and converting weights from PyTorch
    models)
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)
    subclass. Use it as a regular Flax linen Module and refer to the Flax documentation
    for all matter related to general usage and behavior.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this model supports inherent JAX features such as:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L820)'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-632
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Parameters
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-638
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-639
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-642
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pixel_values` (`numpy.ndarray` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPConfig'>`)
    and inputs.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: '`logits_per_image:(jnp.ndarray` of shape `(image_batch_size, text_batch_size)`)
    — The scaled dot product scores between `image_embeds` and `text_embeds`. This
    represents the image-text similarity scores.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_per_text:(jnp.ndarray` of shape `(text_batch_size, image_batch_size)`)
    — The scaled dot product scores between `text_embeds` and `image_embeds`. This
    represents the text-image similarity scores.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — The text embeddings
    obtained by applying the projection layer to the pooled output of [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — The image
    embeddings obtained by applying the projection layer to the pooled output of [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel).'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_model_output(FlaxBaseModelOutputWithPooling):` The output of the [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vision_model_output(FlaxBaseModelOutputWithPooling):` The output of the [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel).'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `FlaxCLIPPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '#### `get_text_features`'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L865)'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-662
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Parameters
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: text_features (`jnp.ndarray` of shape `(batch_size, output_dim`)
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: The text embeddings obtained by applying the projection layer to the pooled
    output of [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '#### `get_image_features`'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L932)'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-674
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Parameters
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`numpy.ndarray` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: image_features (`jnp.ndarray` of shape `(batch_size, output_dim`)
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: The image embeddings obtained by applying the projection layer to the pooled
    output of [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel)
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-681
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: FlaxCLIPTextModel
  id: totrans-682
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxCLIPTextModel`'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1012)'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '#### `__call__`'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L665)'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Parameters
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-691
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-694
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-695
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-696
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)
    and inputs.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) — Last
    layer hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxCLIPTextPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-714
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: FlaxCLIPTextModelWithProjection
  id: totrans-715
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxCLIPTextModelWithProjection`'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1083)'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-718
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '#### `__call__`'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L665)'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Parameters
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. Padding will be ignored by default
    should you provide it.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  id: totrans-724
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-725
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`numpy.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-727
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-728
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-729
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*)
    — Indices of positions of each input sequence tokens in the position embeddings.
    Selected in the range `[0, config.max_position_embeddings - 1]`.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-731
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput` or `tuple(torch.FloatTensor)`'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPTextConfig'>`)
    and inputs.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: '`text_embeds` (`jnp.ndarray` of shape `(batch_size, output_dim`) — The text
    embeddings obtained by applying the projection layer to the pooled output of [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel).'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-743
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxCLIPTextPreTrainedModel` forward method, overrides the `__call__` special
    method.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-747
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: FlaxCLIPVisionModel
  id: totrans-748
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxCLIPVisionModel`'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L1137)'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-751
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '#### `__call__`'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/clip/modeling_flax_clip.py#L745)'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-754
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Parameters
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`numpy.ndarray` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Padding will be ignored by default should you provide
    it. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'>`)
    and inputs.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`jnp.ndarray` of shape `(batch_size, hidden_size)`) — Last
    layer hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `jnp.ndarray`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-766
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `jnp.ndarray` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-768
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `FlaxCLIPVisionPreTrainedModel` forward method, overrides the `__call__`
    special method.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
