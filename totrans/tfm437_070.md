# 多 GPU 上的高效训练

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_many`](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_many)

如果在单个 GPU 上训练模型太慢或者模型的权重无法适应单个 GPU 的内存，则过渡到多 GPU 设置可能是一个可行的选择。在进行此过渡之前，彻底探索在单个 GPU 上进行高效训练的方法和工具中涵盖的所有策略，因为它们普遍适用于任意数量的 GPU 上的模型训练。一旦您采用了这些策略并发现它们在单个 GPU 上不足以满足您的情况时，请考虑转移到多个 GPU。

从单个 GPU 过渡到多个 GPU 需要引入某种形式的并行性，因为工作负载必须分布在资源之间。可以采用多种技术来实现并行性，例如数据并行性，张量并行性和管道并行性。重要的是要注意，没有一种大小适合所有的解决方案，最佳设置取决于您正在使用的特定硬件配置。

本指南提供了对各种并行性类型的深入概述，以及有关如何组合的指导

技术和选择适当的方法。有关分布式训练的逐步教程，请参考[🤗加速文档](https://huggingface.co/docs/accelerate/index)。

尽管本指南讨论的主要概念可能适用于各种框架，但在这里我们专注于基于 PyTorch 的实现。

在深入研究每种技术的具体细节之前，让我们回顾一下在大型基础设施上训练大型模型时的大致决策过程。

## 可扩展性策略

首先估算训练模型所需的 vRAM 量。对于托管在🤗 Hub 上的模型，请使用我们的[模型内存计算器](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)，该计算器可以在几个百分点的范围内给出准确的计算结果。

**单节点/多 GPU 设置的并行化策略**

在单节点上使用多个 GPU 训练模型时，您选择的并行化策略可能会显著影响性能。以下是您的选择：

**情况 1：您的模型适合单个 GPU**

如果您的模型可以轻松适应单个 GPU，则有两个主要选项：

1.  DDP - 分布式数据并行

1.  ZeRO - 根据所使用的情况和配置，这种方法可能会更快，但是，值得尝试一下。

**情况 2：您的模型不适合单个 GPU：**

如果您的模型太大而无法适应单个 GPU，您有几种替代方案可考虑：

1.  PipelineParallel（PP）

1.  ZeRO

1.  TensorParallel（TP）

具有非常快的节点间连接（例如 NVLINK 或 NVSwitch）时，所有三种策略（PP，ZeRO，TP）应该会产生类似的性能。但是，如果没有这些，PP 将比 TP 或 ZeRO 更快。 TP 的程度也可能会有所不同。最好根据您的特定设置进行实验，以确定最合适的策略。

TP 几乎总是在单个节点内使用。即 TP 大小<=每个节点的 GPU 数。

**情况 3：您的模型最大层不适合单个 GPU**

1.  如果您没有使用 ZeRO，则必须使用 TensorParallel（TP），因为仅使用 PipelineParallel（PP）将无法容纳大层。

1.  如果您使用 ZeRO，另外采用来自在单个 GPU 上进行高效训练的方法和工具的技术。

**多节点/多 GPU 设置的并行化策略**

+   当您具有快速的节点间连接（例如 NVLINK 或 NVSwitch）时，请考虑使用以下选项之一：

    1.  ZeRO - 因为它几乎不需要对模型进行修改

    1.  将 PipelineParallel（PP）与 TensorParallel（TP）和 DataParallel（DP）结合使用-这种方法将减少通信量，但需要对模型进行重大更改

+   当您的节点间连接速度慢且 GPU 内存不足时：

    1.  使用 DataParallel(DP) 与 PipelineParallel(PP)、TensorParallel(TP) 和 ZeRO 的组合。

在本指南的后续部分中，我们将深入探讨这些不同的并行方法是如何工作的。

## 数据并行

即使只有 2 个 GPU，您也可以充分利用 PyTorch 内置功能提供的加速训练能力，例如 `DataParallel` (DP) 和 `DistributedDataParallel` (DDP)。请注意，[PyTorch 文档](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html)建议优先选择 `DistributedDataParallel` (DDP) 而不是 `DataParallel` (DP) 用于多 GPU 训练，因为它适用于所有模型。让我们看看这两种方法是如何工作的以及它们之间的区别。

### DataParallel vs DistributedDataParallel

为了了解这两种方法之间的 GPU 间通信开销的关键差异，让我们回顾每批的过程：

[DDP](https://pytorch.org/docs/master/notes/ddp.html):

+   在开始时，主进程将模型从 GPU 0 复制到其他 GPU

+   然后对于每批：

    1.  每个 GPU 直接消耗其数据的小批量。

    1.  在 `backward` 过程中，一旦本地梯度准备就绪，它们将在所有进程中进行平均。

[DP](https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html):

对于每批：

1.  GPU 0 读取数据批次，然后将一个小批量发送到每个 GPU。

1.  最新的模型从 GPU 0 复制到每个 GPU。

1.  `forward` 被执行，每个 GPU 的输出被发送到 GPU 0 来计算损失。

1.  损失从 GPU 0 分发到所有 GPU，并运行 `backward`。

1.  每个 GPU 的梯度被发送到 GPU 0 并进行平均。

关键差异包括：

1.  DDP 每批只执行一次通信 - 发送梯度，而 DP 每批执行五次不同的数据交换。DDP 使用 [torch.distributed](https://pytorch.org/docs/master/distributed.html) 复制数据，而 DP 通过 Python 线程在进程内复制数据（这会引入与 GIL 相关的限制）。因此，**`DistributedDataParallel` (DDP) 通常比 `DataParallel` (DP) 更快**，除非您的 GPU 卡之间的连接速度很慢。

1.  在 DP 下，GPU 0 执行的工作明显多于其他 GPU，导致 GPU 利用率不足。

1.  DDP 支持跨多台机器的分布式训练，而 DP 不支持。

这并不是 DP 和 DDP 之间的所有差异的详尽列表，然而，其他细微差别超出了本指南的范围。您可以通过阅读这篇[文章](https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/)来更深入地了解这些方法。

让我们通过一个实验来说明 DP 和 DDP 之间的差异。我们将基准测试 DP 和 DDP 之间的差异，并增加 NVLink 存在的背景：

+   硬件：2x TITAN RTX 24GB 每个 + 带有 2 个 NVLink 的 NVlink（`nvidia-smi topo -m` 中的 `NV2`）。

+   软件：`pytorch-1.8-to-be` + `cuda-11.0` / `transformers==4.3.0.dev0`。

为了在其中一个基准测试中禁用 NVLink 功能，我们使用 `NCCL_P2P_DISABLE=1`。

这里是基准测试代码和输出：

**DP**

```py
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
python examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 110.5948, 'train_samples_per_second': 1.808, 'epoch': 0.69}
```

**DDP w/ NVlink**

```py
rm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}
```

**DDP w/o NVlink**

```py
rm -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}
```

这里是为方便起见在表格中收集的相同基准测试结果：

| 类型 | NVlink | 时间 |
| :-- | --- | --: |
| 2:DP | Y | 110 秒 |
| 2:DDP | Y | 101 秒 |
| 2:DDP | N | 131 秒 |

如您所见，在这种情况下，DP 比 DDP with NVlink 慢约 10%，但比 DDP without NVlink 快约 15%。真正的差异取决于每个 GPU 需要与其他 GPU 同步多少数据 - 同步的数据越多，慢速链接就会阻碍整体运行时间。

## ZeRO 数据并行

ZeRO 动力数据并行（ZeRO-DP）在这篇[博文](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)中有详细说明。

![DeepSpeed-Image-1](img/61da03a3a1a0e2c5e704642f87f2f216.png)

虽然看起来复杂，但这与`DataParallel`（DP）非常相似。不同之处在于，每个 GPU 只存储其一部分，而不是复制完整的模型参数、梯度和优化器状态。然后，在运行时，当需要完整的层参数时，所有 GPU 会同步以互相提供它们缺少的部分。

为了说明这个想法，考虑一个具有 3 层（La，Lb 和 Lc）的简单模型，其中每层有 3 个参数。例如，层 La 有权重 a0，a1 和 a2：

```py
La | Lb | Lc
---|----|---
a0 | b0 | c0
a1 | b1 | c1
a2 | b2 | c2
```

如果我们有 3 个 GPU，ZeRO-DP 将模型分割到 3 个 GPU 上：

```py
GPU0:
La | Lb | Lc
---|----|---
a0 | b0 | c0

GPU1:
La | Lb | Lc
---|----|---
a1 | b1 | c1

GPU2:
La | Lb | Lc
---|----|---
a2 | b2 | c2
```

在某种程度上，这与张量并行性相同的水平切片，与垂直切片相对，其中将整个层组放在不同的 GPU 上。现在让我们看看这是如何工作的：

这些 GPU 中的每一个都将像 DP 中那样获得通常的小批量：

```py
x0 => GPU0
x1 => GPU1
x2 => GPU2
```

输入被传递而不进行修改，就好像原始模型会处理它们一样。

首先，输入到达层`La`。在这一点上会发生什么？

在 GPU0 上：x0 小批量需要 a0，a1，a2 参数通过层进行前向路径，但 GPU0 只有 a0。它将从 GPU1 获取 a1，从 GPU2 获取 a2，将模型的所有部分汇集在一起。

同时，GPU1 获得另一个小批量-x1。GPU1 具有 a1 参数，但需要 a0 和 a2，因此它从 GPU0 和 GPU2 获取这些。GPU2 也发生同样的情况，它获得小批量 x2。它从 GPU0 和 GPU1 获取 a0 和 a1。

这样，每个 3 个 GPU 都会得到完整的张量重建，并使用自己的小批量进行前向传递。一旦计算完成，不再需要的数据将被丢弃-它只在计算过程中使用。重建通过预取高效地完成。

然后整个过程会为层 Lb 重复，然后是 Lc 的前向，然后是 Lc -> Lb -> La 的后向。

这种机制类似于一种高效的团体背包策略：A 人携带帐篷，B 人携带炉灶，C 人携带斧头。每晚他们都分享自己拥有的东西，并从他人那里得到他们没有的东西，早上他们收拾好自己分配的装备类型，继续前进。这就是 ZeRO DP/Sharded DDP。将这种策略与每个人都必须携带自己的帐篷、炉灶和斧头的简单策略进行比较（类似于 PyTorch 中的 DataParallel（DP 和 DDP）），后者将更加低效。

在阅读这个主题的文献时，您可能会遇到以下同义词：分片，分区。如果您仔细注意 ZeRO 如何分割模型的权重-它看起来非常类似于张量并行性，稍后将对此进行讨论。这是因为它分割/分片每个层的权重，与接下来将讨论的垂直模型并行性不同。

实施：

+   [DeepSpeed](https://www.deepspeed.ai/tutorials/zero/) ZeRO-DP 阶段 1+2+3

+   [`加速`集成](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)

+   `transformers` 集成

## 从天真的模型并行性到管道并行性

解释管道并行性，我们首先将研究天真的模型并行性（MP），也称为垂直 MP。这种方法涉及通过使用`.to()`将模型层组分配到多个 GPU 上，将特定层分配给特定 GPU。当数据流经这些层时，它被移动到与该层相同的 GPU 上，而其他层保持不变。

我们将这种模型并行性称为“垂直”，因为模型通常是如何可视化的。例如，以下图表显示将 8 层模型垂直分割成两个切片，将层 0-3 放在 GPU0 上，将层 4-7 放在 GPU1 上：

```py
===================  ===================
| 0 | 1 | 2 | 3 |  | 4 | 5 | 6 | 7 |
===================  ===================
        GPU0                 GPU1
```

在这个例子中，当数据从第 0 层移动到第 3 层时，与常规前向传递没有区别。然而，将数据从第 3 层传递到第 4 层需要将其从 GPU0 移动到 GPU1，引入了通信开销。如果参与的 GPU 在同一计算节点上（例如同一台物理机器），这种复制是快速的，但如果 GPU 分布在不同的计算节点上（例如多台机器），通信开销可能会大大增加。

接下来，第 4 到第 7 层的工作方式与原始模型中的工作方式相同。在完成第 7 层后，通常需要将数据发送回第 0 层，那里有标签（或者将标签发送到最后一层）。现在可以计算损失并让优化器开始工作。

天真的模型并行存在一些缺点：

+   **除了一个 GPU 外，任何时刻都有空闲的 GPU**：如果使用 4 个 GPU，几乎等同于将单个 GPU 的内存量增加四倍，并忽略其他硬件。

+   **设备之间数据传输的开销**：例如，使用天真的 MP，4 张 6GB 的卡片可以容纳与 1 张 24GB 卡片相同大小的模型，但是单张 24GB 卡片会更快地完成训练，因为它没有数据复制的开销。但是，假设您有 40GB 的卡片，需要适应一个 45GB 的模型，您可以使用 4 张 40GB 的卡片（但是可能很勉强，因为梯度和优化器状态）。

+   **复制共享的嵌入**：共享的嵌入可能需要在 GPU 之间来回复制。

现在您已经了解了模型并行的天真方法以及其缺点，让我们来看看管道并行（PP）。PP 几乎与天真的 MP 相同，但通过将传入的批次分成微批次并人为地创建一个管道来解决 GPU 空闲问题，这允许不同的 GPU 同时参与计算过程。

以下来自[GPipe 论文](https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html)的插图显示了顶部的天真 MP 和底部的 PP：

![MP vs PP](img/ba7b60c4785caa5f69f0731aec905196.png)

在图表底部，您可以观察到管道并行（PP）方法最小化了空闲 GPU 区域的数量，称为“气泡”。图表的两部分显示了并行度为 4 的水平，意味着有 4 个 GPU 参与了管道。您可以看到有一个由 4 个管道阶段（F0、F1、F2 和 F3）组成的前向路径，然后是一个反向路径，顺序相反（B3、B2、B1 和 B0）。

PP 引入了一个新的超参数来调整 - `chunks`，它确定通过同一管道阶段连续发送多少数据块。例如，在底部图表中，您可以看到`chunks=4`。GPU0 在块 0、1、2 和 3（F0,0、F0,1、F0,2、F0,3）上执行相同的前向路径，然后等待其他 GPU 完成它们的工作。只有当其他 GPU 开始完成它们的工作时，GPU0 才开始再次工作，为块 3、2、1 和 0（B0,3、B0,2、B0,1、B0,0）执行反向路径。

请注意，这与梯度累积步骤的概念相同。PyTorch 使用`chunks`，而 DeepSpeed 将同一超参数称为梯度累积步骤。

由于 chunks，PP 引入了微批次（MBS）的概念。DP 将全局数据批次大小分成小批次，因此如果 DP 度为 4，则全局批次大小为 1024 将分成 4 个每个 256 的小批次（1024/4）。如果`chunks`（或 GAS）的数量为 32，则我们最终得到一个微批次大小为 8（256/32）。每个管道阶段一次处理一个微批次。要计算 DP + PP 设置的全局批次大小，请使用公式：`mbs * chunks * dp_degree`（`8 * 32 * 4 = 1024`）。当`chunks=1`时，您将得到天真的 MP，这是低效的。当`chunks`值很大时，您将得到微小的微批次大小，这也是低效的。因此，我们鼓励尝试不同的`chunks`值，以找到导致最有效的 GPU 利用率的值。

您可能会注意到图表上的“死”时间泡泡，因为最后的`forward`阶段必须等待`backward`完成管道。找到`chunks`的最佳值的目的是实现所有参与 GPU 之间的高并发 GPU 利用率，从而最小化泡泡的大小。

Pipeline API 解决方案已在以下平台中实施：

+   PyTorch

+   DeepSpeed

+   Megatron-LM

这些存在一些缺点：

+   他们必须对模型进行相当大的修改，因为 Pipeline 要求将模块的正常流重写为相同的`nn.Sequential`序列，这可能需要对模型的设计进行更改。

+   目前，Pipeline API 非常受限制。如果在管道的第一个阶段传递了一堆 Python 变量，您将不得不找到解决方法。目前，管道接口要求作为唯一输入和输出的是单个张量或张量元组。这些张量的批次大小必须作为第一个维度，因为管道将小批次分成微批次。这里正在讨论可能的改进[`github.com/pytorch/pytorch/pull/50693`](https://github.com/pytorch/pytorch/pull/50693)

+   在管道阶段的条件控制流不可能-例如，编码器-解码器模型（如 T5）需要特殊的解决方案来处理条件编码器阶段。

+   他们必须安排每一层，以便一层的输出成为另一层的输入。

更近期的解决方案包括：

+   Varuna

+   Sagemaker

我们尚未尝试 Varuna 和 SageMaker，但他们的论文报告称，他们已经克服了上述问题列表，并且需要对用户模型进行较小的更改。

实施：

+   [PyTorch](https://pytorch.org/docs/stable/pipeline.html)（在 pytorch-1.8 中提供初始支持，并在 1.9 中逐渐改进，在 1.10 中更是如此）。一些[示例](https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py)

+   [DeepSpeed](https://www.deepspeed.ai/tutorials/pipeline/)

+   [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)具有内部实现-没有 API。

+   [Varuna](https://github.com/microsoft/varuna)

+   [SageMaker](https://arxiv.org/abs/2111.05972) - 这是一种专有解决方案，只能在 AWS 上使用。

+   [OSLO](https://github.com/tunib-ai/oslo) - 这是基于 Hugging Face Transformers 实现的。

🤗 Transformers 状态：截至目前，没有模型支持完整的 PP。GPT2 和 T5 模型具有天真的 MP 支持。主要障碍是无法将模型转换为`nn.Sequential`并使所有输入为张量。这是因为当前模型包含许多使转换非常复杂的特性，并且需要将其删除才能实现转换。

DeepSpeed 和 Megatron-LM 集成可在[🤗 Accelerate](https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed)中找到

其他方法：

DeepSpeed、Varuna 和 SageMaker 使用[交错管道](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html)的概念

![交错管道执行](img/fe4c5248602104a9d60765bc134bfd01.png)

在这里，通过优先考虑反向传递来进一步减少气泡（空闲时间）。Varuna 通过使用模拟来发现最有效的调度来尝试改进时间表。

OSLO 具有基于 Transformers 的管道并行实现，无需`nn.Sequential`转换。

## 张量并行

在张量并行中，每个 GPU 处理张量的一个切片，并且仅在需要时聚合完整的张量进行操作。为了描述这种方法，本指南的这一部分依赖于[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)论文中的概念和图表：[在 GPU 集群上进行高效的大规模语言模型训练](https://arxiv.org/abs/2104.04473)。

任何 transformer 的主要构建块是一个完全连接的`nn.Linear`，后面跟着一个非线性激活`GeLU`。它的点积部分，根据 Megatron 的论文符号表示法，可以写成`Y = GeLU(XA)`，其中`X`是输入向量，`Y`是输出向量，`A`是权重矩阵。

如果我们以矩阵形式看计算，您可以看到矩阵乘法如何在多个 GPU 之间分割：

![并行 GEMM](img/24e76c89c6e86afa3bc79043047b8082.png)

如果我们将权重矩阵`A`在`N`个 GPU 上按列划分，并在并行执行矩阵乘法`XA_1`到`XA_n`，那么我们将得到`N`个输出向量`Y_1, Y_2, ..., Y_n`，可以独立地输入到`GeLU`中：

![独立 GeLU](img/03ee256eff323b38a197978ce627170d.png)

使用这个原则，我们可以更新任意深度的多层感知器，而无需在 GPU 之间进行任何同步，直到最后，在那里我们需要从碎片中重建输出向量。Megatron-LM 论文的作者为此提供了一个有用的插图：

![并行碎片处理](img/31d292bdb9b6fa0edbf59e631b8b21c8.png)

并行化多头注意力层甚至更简单，因为它们已经天生是并行的，由于具有多个独立的头！

![并行自注意力](img/6fc3dbac64ede57965d6026e9a2fe8c2.png)

特殊考虑：TP 需要非常快的网络，因此不建议在多个节点之间进行 TP。实际上，如果一个节点有 4 个 GPU，则最高的 TP 度数为 4。如果您需要 8 的 TP 度数，则需要使用至少有 8 个 GPU 的节点。

本节基于原始的更详细的[TP 概述](https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530)。by [@anton-l](https://github.com/anton-l)。

替代名称：

+   DeepSpeed 称之为[张量切片](https://www.deepspeed.ai/training/#model-parallelism)

实施：

+   [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)具有内部实现，因为它非常特定于模型。

+   [parallelformers](https://github.com/tunib-ai/parallelformers)（目前仅推理）

+   [SageMaker](https://arxiv.org/abs/2111.05972) - 这是一个专有解决方案，只能在 AWS 上使用。

+   [OSLO](https://github.com/tunib-ai/oslo)具有基于 Transformers 的张量并行实现。

SageMaker 将 TP 与 DP 结合起来，以实现更高效的处理。

🤗 Transformers 状态：

+   核心：核心尚未实现

+   但是如果您需要推理[parallelformers](https://github.com/tunib-ai/parallelformers)为我们大多数模型提供了支持。因此，在核心实现之前，您可以使用它们的支持。希望训练模式也会得到支持。

+   Deepspeed-Inference 还支持我们的 BERT、GPT-2 和 GPT-Neo 模型，采用超快的基于 CUDA 内核的推理模式，更多信息请参见[这里](https://www.deepspeed.ai/tutorials/inference-tutorial/)

🤗 Accelerate 集成了[Megatron-LM 的 TP](https://huggingface.co/docs/accelerate/v0.23.0/en/usage_guides/megatron_lm)。

## 数据并行 + 管道并行

来自 DeepSpeed [pipeline 教程](https://www.deepspeed.ai/tutorials/pipeline/)的以下图表演示了如何将 DP 与 PP 结合使用。

![DP + PP-2d](img/2aea4e58b8f741571e02a891090ecb48.png)

在这里，重要的是看到 DP 等级 0 看不到 GPU2，DP 等级 1 看不到 GPU3。对于 DP 来说，只有 GPU 0 和 1，它们像只有 2 个 GPU 一样提供数据。GPU0“秘密”地将一些负载转移到 GPU2 上，使用 PP。GPU1 也通过将 GPU3 列入其援助来做同样的事情。

由于每个维度至少需要 2 个 GPU，所以这里至少需要 4 个 GPU。

实现：

+   [DeepSpeed](https://github.com/microsoft/DeepSpeed)

+   [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

+   [Varuna](https://github.com/microsoft/varuna)

+   [SageMaker](https://arxiv.org/abs/2111.05972)

+   [OSLO](https://github.com/tunib-ai/oslo)

🤗 Transformers 状态：尚未实现

## 数据并行 + 流水线并行 + 张量并行

为了获得更高效的训练，使用了 3D 并行，其中 PP 与 TP 和 DP 结合使用。可以在以下图表中看到。

![dp-pp-tp-3d](img/576dd96e11eaad105f11f0b16e8458b1.png)

这个图表来自一篇博文[3D 并行：扩展到万亿参数模型](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)，也是一篇很好的阅读。

由于每个维度至少需要 2 个 GPU，所以这里至少需要 8 个 GPU。

实现：

+   [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed 还包括一个更高效的 DP，他们称之为 ZeRO-DP。

+   [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

+   [Varuna](https://github.com/microsoft/varuna)

+   [SageMaker](https://arxiv.org/abs/2111.05972)

+   [OSLO](https://github.com/tunib-ai/oslo)

🤗 Transformers 状态：尚未实现，因为我们没有 PP 和 TP。

## ZeRO 数据并行 + 流水线并行 + 张量并行

DeepSpeed 的主要特点之一是 ZeRO，它是 DP 的一个超可扩展扩展。已经在 ZeRO 数据并行中讨论过。通常它是一个独立的功能，不需要 PP 或 TP。但它可以与 PP 和 TP 结合使用。

当 ZeRO-DP 与 PP（和可选的 TP）结合时，通常只启用 ZeRO 阶段 1（优化器分片）。

虽然理论上可以使用 ZeRO 阶段 2（梯度分片）与流水线并行，但会对性能产生负面影响。每个微批次都需要额外的 reduce-scatter 集合来聚合梯度，然后再进行分片，这会增加潜在的显著通信开销。由于流水线并行的特性，使用小微批次，而重点是尝试平衡算术强度（微批次大小）和最小化流水线气泡（微批次数量）。因此，这些通信成本将影响性能。

此外，由于 PP 已经减少了梯度大小`1/PP`，所以梯度分片的节省并不像纯 DP 那样显著。

ZeRO 阶段 3 也不是一个好选择，原因是需要更多的节点间通信。

由于我们有 ZeRO，另一个好处是 ZeRO-Offload。由于这是阶段 1 优化器状态可以转移到 CPU。

实现：

+   [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)和[BigScience 的 Megatron-Deepspeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)，这是前一个存储库的分支。

+   [OSLO](https://github.com/tunib-ai/oslo)

重要论文：

+   使用 DeepSpeed 和 Megatron 训练 Megatron-Turing NLG 530B，一个大规模生成式语言模型

🤗 Transformers 状态：尚未实现，因为我们没有 PP 和 TP。

## FlexFlow

[FlexFlow](https://github.com/flexflow/FlexFlow) 也以略有不同的方式解决了并行化问题。

论文：[“Beyond Data and Model Parallelism for Deep Neural Networks” by Zhihao Jia, Matei Zaharia, Alex Aiken](https://arxiv.org/abs/1807.05358)

它执行一种 4D 并行化，涵盖样本-操作-属性-参数。

1.  样本 = 数据并行化（样本方向并行）

1.  操作 = 将单个操作并行化为多个子操作

1.  属性 = 数据并行化（长度方向并行）

1.  参数 = 模型并行化（无论维度是水平还是垂直）

示例：

+   样本

让我们拿 10 批次的序列长度为 512。如果我们按样本维度将它们并行化为 2 个设备，我们得到 10 x 512，这将变为 5 x 2 x 512。

+   操作

如果我们执行层归一化，首先计算标准差，然后计算均值，然后我们可以对数据进行归一化。操作并行性允许并行计算标准差和均值。因此，如果我们按操作维度将它们并行化为 2 个设备（cuda:0，cuda:1），首先将输入数据复制到两个设备中，cuda:0 同时计算标准差，cuda:1 计算均值。

+   属性

我们有 10 批次，每个长度为 512。如果我们按属性维度将它们并行化为 2 个设备，10 x 512 将变为 10 x 2 x 256。

+   参数

这与张量模型并行化或天真的逐层模型并行化类似。

![flex-flow-soap](img/afd0fc78628855e58b9faf6fa5778e42.png)

这个框架的重要性在于，它以算法方式占用资源，如（1）GPU/TPU/CPU vs.（2）RAM/DRAM vs.（3）快速内部连接/慢速外部连接，并自动优化所有这些，决定在哪里使用哪种并行化。

一个非常重要的方面是，FlexFlow 专为优化具有静态和固定工作负载的 DNN 并行化而设计，因为具有动态行为的模型可能会在迭代中更喜欢不同的并行化策略。

因此，这个承诺非常吸引人 - 它在所选集群上运行 30 分钟的模拟，并提出了最佳策略来利用这个特定环境。如果添加/删除/替换任何部分，它将运行并重新优化该计划。然后您可以进行训练。不同的设置将有自己的定制优化。

🤗 Transformers 状态：Transformers 模型可以通过[transformers.utils.fx](https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py)进行 FX-trace-able，这是 FlexFlow 的先决条件，但需要在 FlexFlow 方面进行更改以使其与 Transformers 模型配合使用。

## GPU 选择

在多个 GPU 上训练时，您可以指定要使用的 GPU 数量和顺序。例如，当您有计算能力不同的 GPU 并希望首先使用速度更快的 GPU 时，这可能很有用。选择过程适用于[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)和[DataParallel](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)，以仅使用可用 GPU 的子集，您不需要 Accelerate 或 DeepSpeed integration。

### GPU 的数量

例如，如果您有 4 个 GPU，但只想使用前 2 个：

torchrunAccelerateDeepSpeed

使用`--nproc_per_node`来选择使用多少个 GPU。

```py
torchrun --nproc_per_node=2  trainer-program.py ...
```

### GPU 的顺序

现在，要选择要使用的 GPU 及其顺序，您将使用`CUDA_VISIBLE_DEVICES`环境变量。最简单的方法是在`~/bashrc`或其他启动配置文件中设置环境变量。`CUDA_VISIBLE_DEVICES`用于映射要使用的 GPU。例如，如果您有 4 个 GPU（0、1、2、3），但只想运行 GPU 0 和 2：

```py
CUDA_VISIBLE_DEVICES=0,2 torchrun trainer-program.py ...
```

只有 2 个物理 GPU（0 和 2）对 PyTorch 是“可见的”，它们分别映射到`cuda:0`和`cuda:1`。您还可以颠倒 GPU 的顺序以先使用 2 个。现在，映射是 GPU 0 为`cuda:1`，GPU 2 为`cuda:0`。

```py
CUDA_VISIBLE_DEVICES=2,0 torchrun trainer-program.py ...
```

您还可以将 `CUDA_VISIBLE_DEVICES` 环境变量设置为空值，以创建一个没有 GPU 的环境。

```py
CUDA_VISIBLE_DEVICES= python trainer-program.py ...
```

与任何环境变量一样，它们可以被导出，而不是添加到命令行中。然而，这并不推荐，因为如果您忘记了环境变量的设置方式，最终使用了错误的 GPU，会让人感到困惑。相反，通常的做法是在同一命令行上为特定的训练运行设置环境变量。

`CUDA_DEVICE_ORDER` 是一个替代环境变量，您可以使用它来控制 GPU 的顺序。您可以按照以下方式对它们进行排序：

1.  与 [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface) 和 [`rocm-smi`](https://rocm.docs.amd.com/projects/rocm_smi_lib/en/latest/.doxygen/docBin/html/index.html) 分别匹配的 PCIe 总线 ID，用于 NVIDIA 和 AMD GPU

```py
export CUDA_DEVICE_ORDER=PCI_BUS_ID
```

1.  GPU 计算能力

```py
export CUDA_DEVICE_ORDER=FASTEST_FIRST
```

如果您的训练设置包括一台较旧和一台较新的 GPU，其中较旧的 GPU 显示在前，但您无法物理交换卡片使较新的 GPU 显示在前，那么 `CUDA_DEVICE_ORDER` 就特别有用。在这种情况下，设置 `CUDA_DEVICE_ORDER=FASTEST_FIRST`，始终使用较新和更快的 GPU（`nvidia-smi` 或 `rocm-smi` 仍然按照 PCIe 顺序报告 GPU）。或者您也可以设置 `export CUDA_VISIBLE_DEVICES=1,0`。
