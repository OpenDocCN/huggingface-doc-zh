["```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n```", "```py\n>>> dataset[\"label\"][:10]\n[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n>>> sorted_dataset = dataset.sort(\"label\")\n>>> sorted_dataset[\"label\"][:10]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n>>> sorted_dataset[\"label\"][-10:]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n```", "```py\n>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)\n>>> shuffled_dataset[\"label\"][:10]\n[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n```", "```py\n>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)\n>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)\n```", "```py\n>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])\n>>> len(small_dataset)\n6\n```", "```py\n>>> start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n>>> len(start_with_ar)\n6\n>>> start_with_ar[\"sentence1\"]\n['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',\n'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',\n'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',\n\"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .\",\n'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'\n]\n```", "```py\n>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)\n>>> len(even_dataset)\n1834\n>>> len(dataset) / 2\n1834.0\n```", "```py\n>>> dataset.train_test_split(test_size=0.1)\n{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),\n'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}\n>>> 0.1 * len(dataset)\n366.8\n```", "```py\n>>> from datasets import load_dataset\n>>> datasets = load_dataset(\"imdb\", split=\"train\")\n>>> print(dataset)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 25000\n})\n```", "```py\n>>> dataset.shard(num_shards=4, index=0)\nDataset({\n    features: ['text', 'label'],\n    num_rows: 6250\n})\n>>> print(25000/4)\n6250.0\n```", "```py\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.rename_column(\"sentence1\", \"sentenceA\")\n>>> dataset = dataset.rename_column(\"sentence2\", \"sentenceB\")\n>>> dataset\nDataset({\n    features: ['sentenceA', 'sentenceB', 'label', 'idx'],\n    num_rows: 3668\n})\n```", "```py\n>>> dataset = dataset.remove_columns(\"label\")\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.remove_columns([\"sentence1\", \"sentence2\"])\n>>> dataset\nDataset({\n    features: ['idx'],\n    num_rows: 3668\n})\n```", "```py\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'label', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])\n>>> dataset\nDataset({\n    features: ['sentence1', 'sentence2', 'idx'],\n    num_rows: 3668\n})\n>>> dataset = dataset.select_columns('idx')\n>>> dataset\nDataset({\n    features: ['idx'],\n    num_rows: 3668\n})\n```", "```py\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n'idx': Value(dtype='int32', id=None)}\n\n>>> from datasets import ClassLabel, Value\n>>> new_features = dataset.features.copy()\n>>> new_features[\"label\"] = ClassLabel(names=[\"negative\", \"positive\"])\n>>> new_features[\"idx\"] = Value(\"int64\")\n>>> dataset = dataset.cast(new_features)\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),\n'idx': Value(dtype='int64', id=None)}\n```", "```py\n>>> dataset.features\n{'audio': Audio(sampling_rate=44100, mono=True, id=None)}\n\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> dataset.features\n{'audio': Audio(sampling_rate=16000, mono=True, id=None)}\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset(\"squad\", split=\"train\")\n>>> dataset.features\n{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n'context': Value(dtype='string', id=None),\n'id': Value(dtype='string', id=None),\n'question': Value(dtype='string', id=None),\n'title': Value(dtype='string', id=None)}\n```", "```py\n>>> flat_dataset = dataset.flatten()\n>>> flat_dataset\nDataset({\n    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n num_rows: 87599\n})\n```", "```py\n>>> def add_prefix(example):\n...     example[\"sentence1\"] = 'My sentence: ' + example[\"sentence1\"]\n...     return example\n```", "```py\n>>> updated_dataset = small_dataset.map(add_prefix)\n>>> updated_dataset[\"sentence1\"][:5]\n['My sentence: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n\"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n]\n```", "```py\n>>> updated_dataset = dataset.map(lambda example: {\"new_sentence\": example[\"sentence1\"]}, remove_columns=[\"sentence1\"])\n>>> updated_dataset.column_names\n['sentence2', 'label', 'idx', 'new_sentence']\n```", "```py\n>>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, with_indices=True)\n>>> updated_dataset[\"sentence2\"][:5]\n['0: Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n \"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n \"2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'\n]\n```", "```py\n>>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, num_proc=4)\n```", "```py\n>>> import torch\n>>> from multiprocess import set_start_method\n>>> from transformers import AutoTokenizer, AutoModelForCausalLM \n>>> from datasets import load_dataset\n>>> \n>>> # Get an example dataset\n>>> dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\n>>> \n>>> # Get an example model and its tokenizer \n>>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\").eval()\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n>>>\n>>> def gpu_computation(batch, rank):\n...     # Move the model on the right GPU if it's not there already\n...     device = f\"cuda:{(rank or 0) % torch.cuda.device_count()}\"\n...     model.to(device)\n...     \n...     # Your big GPU call goes here, for example:\n...     chats = [[\n...         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n...         {\"role\": \"user\", \"content\": prompt}\n...     ] for prompt in batch[\"prompt\"]]\n...     texts = [tokenizer.apply_chat_template(\n...         chat,\n...         tokenize=False,\n...         add_generation_prompt=True\n...     ) for chat in chats]\n...     model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n...     with torch.no_grad():\n...         outputs = model.generate(**model_inputs, max_new_tokens=512)\n...     batch[\"output\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n...     return batch\n>>>\n>>> if __name__ == \"__main__\":\n...     set_start_method(\"spawn\")\n...     updated_dataset = dataset.map(\n...         gpu_computation,\n...         batched=True,\n...         batch_size=16,\n...         with_rank=True,\n...         num_proc=torch.cuda.device_count(),  # one process per GPU\n...     )\n```", "```py\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.\n```", "```py\n>>> def chunk_examples(examples):\n...     chunks = []\n...     for sentence in examples[\"sentence1\"]:\n...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n...     return {\"chunks\": chunks}\n```", "```py\n>>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)\n>>> chunked_dataset[:10]\n{'chunks': ['Amrozi accused his brother , whom he called \" the ',\n            'witness \" , of deliberately distorting his evidenc',\n            'e .',\n            \"Yucaipa owned Dominick 's before selling the chain\",\n            ' to Safeway in 1998 for $ 2.5 billion .',\n            'They had published an advertisement on the Interne',\n            't on June 10 , offering the cargo for sale , he ad',\n            'ded .',\n            'Around 0335 GMT , Tab shares were up 19 cents , or',\n            ' 4.4 % , at A $ 4.56 , having earlier set a record']}\n```", "```py\n>>> dataset\nDataset({\n features: ['sentence1', 'sentence2', 'label', 'idx'],\n num_rows: 3668\n})\n>>> chunked_dataset\nDataset(schema: {'chunks': 'string'}, num_rows: 10470)\n```", "```py\n>>> from random import randint\n>>> from transformers import pipeline\n\n>>> fillmask = pipeline(\"fill-mask\", model=\"roberta-base\")\n>>> mask_token = fillmask.tokenizer.mask_token\n>>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)\n```", "```py\n>>> def augment_data(examples):\n...     outputs = []\n...     for sentence in examples[\"sentence1\"]:\n...         words = sentence.split(' ')\n...         K = randint(1, len(words)-1)\n...         masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n...         predictions = fillmask(masked_sentence)\n...         augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)]\n...         outputs += [sentence] + augmented_sequences\n...\n...     return {\"data\": outputs}\n```", "```py\n>>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)\n>>> augmented_dataset[:9][\"data\"]\n['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'Amrozi accused his brother, whom he called \" the witness \", of deliberately withholding his evidence.',\n 'Amrozi accused his brother, whom he called \" the witness \", of deliberately suppressing his evidence.',\n 'Amrozi accused his brother, whom he called \" the witness \", of deliberately destroying his evidence.',\n \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n 'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',\n \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.\",\n 'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'\n]\n```", "```py\n>>> from datasets import load_dataset\n\n# load all the splits\n>>> dataset = load_dataset('glue', 'mrpc')\n>>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples[\"sentence1\"]), batched=True)\n>>> encoded_dataset[\"train\"][0]\n{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n'label': 1,\n'idx': 0,\n'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],\n'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n}\n```", "```py\n>>> from datasets import Dataset\n>>> import torch.distributed\n\n>>> dataset1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n\n>>> if training_args.local_rank > 0:\n...     print(\"Waiting for main process to perform the mapping\")\n...     torch.distributed.barrier()\n\n>>> dataset2 = dataset1.map(lambda x: {\"a\": x[\"a\"] + 1})\n\n>>> if training_args.local_rank == 0:\n...     print(\"Loading results from main process\")\n...     torch.distributed.barrier()\n```", "```py\n>>> from datasets import concatenate_datasets, load_dataset\n\n>>> bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n>>> wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n\n>>> assert bookcorpus.features.type == wiki.features.type\n>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])\n```", "```py\n>>> from datasets import Dataset\n>>> bookcorpus_ids = Dataset.from_dict({\"ids\": list(range(len(bookcorpus)))})\n>>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)\n```", "```py\n>>> seed = 42\n>>> probabilities = [0.3, 0.5, 0.2]\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)\n>>> dataset[\"a\"]\n[10, 11, 20, 12, 0, 21, 13]\n```", "```py\n>>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n>>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n>>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n>>> dataset[\"a\"]\n[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]\n```", "```py\n>>> import torch\n>>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n```", "```py\n>>> dataset = dataset.with_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n```", "```py\n>>> dataset.format\n{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}\n>>> dataset.reset_format()\n>>> dataset.format\n{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\n```", "```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> def encode(batch):\n...     return tokenizer(batch[\"sentence1\"], padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n>>> dataset.set_transform(encode)\n>>> dataset.format\n{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\n```", "```py\n>>> import numpy as np\n>>> from pydub import AudioSegment\n\n>>> audio_dataset_amr = Dataset.from_dict({\"audio\": [\"audio_samples/audio.amr\"]})\n\n>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):\n...     def pydub_decode_file(audio_path):\n...         sound = AudioSegment.from_file(audio_path)\n...         if sound.frame_rate != sampling_rate:\n...             sound = sound.set_frame_rate(sampling_rate)\n...         channel_sounds = sound.split_to_mono()\n...         samples = [s.get_array_of_samples() for s in channel_sounds]\n...         fp_arr = np.array(samples).T.astype(np.float32)\n...         fp_arr /= np.iinfo(samples[0].typecode).max\n...         return fp_arr\n...\n...     batch[\"audio\"] = [pydub_decode_file(audio_path) for audio_path in batch[\"audio\"]]\n...     return batch\n\n>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)\n```", "```py\n>>> encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\")\n```", "```py\n>>> from datasets import load_from_disk\n>>> reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\")\n```", "```py\n>>> encoded_dataset.to_csv(\"path/of/my/dataset.csv\")\n```"]