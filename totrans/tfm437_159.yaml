- en: DPR
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DPR
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpr)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpr](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/dpr)
- en: '[![Models](../Images/a633be26dafb6fe85c98626540cb1520.png)](https://huggingface.co/models?filter=dpr)
    [![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/dpr-question_encoder-bert-base-multilingual)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![模型](../Images/a633be26dafb6fe85c98626540cb1520.png)](https://huggingface.co/models?filter=dpr)
    [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/dpr-question_encoder-bert-base-multilingual)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art
    open-domain Q&A research. It was introduced in [Dense Passage Retrieval for Open-Domain
    Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas
    Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau
    Yih.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Dense Passage Retrieval（DPR）是用于最先进的开放域问答研究的一组工具和模型。它是由Vladimir Karpukhin、Barlas
    Oğuz、Sewon Min、Patrick Lewis、Ledell Wu、Sergey Edunov、Danqi Chen、Wen-tau Yih在[用于开放域问答的密集段落检索](https://arxiv.org/abs/2004.04906)中介绍的。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Open-domain question answering relies on efficient passage retrieval to select
    candidate contexts, where traditional sparse vector space models, such as TF-IDF
    or BM25, are the de facto method. In this work, we show that retrieval can be
    practically implemented using dense representations alone, where embeddings are
    learned from a small number of questions and passages by a simple dual-encoder
    framework. When evaluated on a wide range of open-domain QA datasets, our dense
    retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in
    terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system
    establish new state-of-the-art on multiple open-domain QA benchmarks.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*开放域问答依赖于高效的段落检索来选择候选上下文，传统的稀疏向量空间模型，如TF-IDF或BM25，是事实上的方法。在这项工作中，我们展示了检索可以仅使用密集表示来实现，其中通过简单的双编码器框架从少量问题和段落中学习嵌入。在广泛的开放域QA数据集上评估时，我们的密集检索器在前20个段落检索准确性方面大幅优于强大的Lucene-BM25系统，帮助我们的端到端QA系统在多个开放域QA基准上建立了新的最先进水平。*'
- en: This model was contributed by [lhoestq](https://huggingface.co/lhoestq). The
    original code can be found [here](https://github.com/facebookresearch/DPR).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[lhoestq](https://huggingface.co/lhoestq)贡献。原始代码可在[此处](https://github.com/facebookresearch/DPR)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: 'DPR consists in three models:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPR包括三个模型：
- en: 'Question encoder: encode questions as vectors'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题编码器：将问题编码为向量
- en: 'Context encoder: encode contexts as vectors'
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文编码器：将上下文编码为向量
- en: 'Reader: extract the answer of the questions inside retrieved contexts, along
    with a relevance score (high if the inferred span actually answers the question).'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读器：提取检索到的上下文中问题的答案，以及相关性得分（如果推断的跨度实际上回答了问题，则得分高）。
- en: DPRConfig
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRConfig
- en: '### `class transformers.DPRConfig`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/configuration_dpr.py#L45)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/configuration_dpr.py#L45)'
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    DPR model. Defines the different tokens that can be represented by the *inputs_ids*
    passed to the forward method of [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, defaults to 30522) — DPR模型的词汇表大小。定义了传递给[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)的*inputs_ids*的不同标记。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢弃比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the *token_type_ids* passed into [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_size` (`int`, *optional*, defaults to 2) — 传递给[BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)的*token_type_ids*的词汇表大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — Padding token id.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *可选*, 默认为0) — 填充标记id。'
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_embedding_type` (`str`, *可选*, 默认为`"absolute"`) — 位置嵌入的类型。选择`"absolute"`、`"relative_key"`或`"relative_key_query"`之一。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention
    with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve
    Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)中的*Method
    4*。'
- en: '`projection_dim` (`int`, *optional*, defaults to 0) — Dimension of the projection
    for the context and question encoders. If it is set to zero (default), then no
    projection is done.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projection_dim` (`int`, *可选*, 默认为0) — 上下文和问题编码器投影的维度。如果设置为零（默认），则不进行投影。'
- en: '[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)
    is the configuration class to store the configuration of a *DPRModel*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)
    是用来存储*DPRModel*配置的配置类。'
- en: This is the configuration class to store the configuration of a [DPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoder),
    [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    or a [DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader).
    It is used to instantiate the components of the DPR model according to the specified
    arguments, defining the model component architectures. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the DPRContextEncoder
    [facebook/dpr-ctx_encoder-single-nq-base](https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base)
    architecture.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用来存储[DPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoder)、[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)或[DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)配置的配置类。根据指定的参数实例化DPR模型的组件，定义模型组件的架构。使用默认值实例化配置将产生类似于DPRContextEncoder
    [facebook/dpr-ctx_encoder-single-nq-base](https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base)架构的配置。
- en: This class is a subclass of [BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig).
    Please check the superclass for the documentation of all kwargs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是[BertConfig](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertConfig)的子类。请查看超类以获取所有kwargs的文档。
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DPRContextEncoderTokenizer
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRContextEncoderTokenizer
- en: '### `class transformers.DPRContextEncoderTokenizer`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRContextEncoderTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L113)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L113)'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Construct a DPRContextEncoder tokenizer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个DPRContextEncoder分词器。
- en: '[DPRContextEncoderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer)
    is identical to [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    and runs end-to-end tokenization: punctuation splitting and wordpiece.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRContextEncoderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer)
    与[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)相同，进行端到端的标记化：标点符号拆分和wordpiece。'
- en: Refer to superclass [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    for usage examples and documentation concerning parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 参考超类[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)以获取用法示例和有关参数的文档。
- en: DPRContextEncoderTokenizerFast
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRContextEncoderTokenizerFast
- en: '### `class transformers.DPRContextEncoderTokenizerFast`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRContextEncoderTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L114)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L114)'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Construct a “fast” DPRContextEncoder tokenizer (backed by HuggingFace’s *tokenizers*
    library).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速”的DPRContextEncoder分词器（由HuggingFace的*tokenizers*库支持）。
- en: '[DPRContextEncoderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast)
    is identical to [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    and runs end-to-end tokenization: punctuation splitting and wordpiece.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRContextEncoderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast)
    与[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)相同，进行端到端的标记化：标点符号拆分和wordpiece。'
- en: Refer to superclass [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    for usage examples and documentation concerning parameters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参考超类[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)以获取用法示例和有关参数的文档。
- en: DPRQuestionEncoderTokenizer
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRQuestionEncoderTokenizer
- en: '### `class transformers.DPRQuestionEncoderTokenizer`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRQuestionEncoderTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L129)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L129)'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Constructs a DPRQuestionEncoder tokenizer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个DPRQuestionEncoder分词器。
- en: '[DPRQuestionEncoderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer)
    is identical to [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    and runs end-to-end tokenization: punctuation splitting and wordpiece.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRQuestionEncoderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer)
    与 [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    相同，并且进行端到端的标记化：标点符号拆分和词片。'
- en: Refer to superclass [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    for usage examples and documentation concerning parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有关用法示例和有关参数的文档，请参考超类 [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)。
- en: DPRQuestionEncoderTokenizerFast
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRQuestionEncoderTokenizerFast
- en: '### `class transformers.DPRQuestionEncoderTokenizerFast`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRQuestionEncoderTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L131)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L131)'
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Constructs a “fast” DPRQuestionEncoder tokenizer (backed by HuggingFace’s *tokenizers*
    library).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速” DPRQuestionEncoder 分词器（由 HuggingFace 的 *tokenizers* 库支持）。
- en: '[DPRQuestionEncoderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast)
    is identical to [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    and runs end-to-end tokenization: punctuation splitting and wordpiece.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRQuestionEncoderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast)
    与 [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    相同，并且进行端到端的标记化：标点符号拆分和词片。'
- en: Refer to superclass [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    for usage examples and documentation concerning parameters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有关用法示例和有关参数的文档，请参考超类 [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)。
- en: DPRReaderTokenizer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRReaderTokenizer
- en: '### `class transformers.DPRReaderTokenizer`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRReaderTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L394)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr.py#L394)'
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`questions` (`str` or `List[str]`) — The questions to be encoded. You can specify
    one question for many passages. In this case, the question will be duplicated
    like `[questions] * n_passages`. Otherwise you have to specify as many questions
    as in `titles` or `texts`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`questions` (`str` 或 `List[str]`) — 要编码的问题。您可以为许多段落指定一个问题。在这种情况下，问题将被复制，如 `[questions]
    * n_passages`。否则，您必须指定与 `titles` 或 `texts` 中的问题数量相同的问题。'
- en: '`titles` (`str` or `List[str]`) — The passages titles to be encoded. This can
    be a string or a list of strings if there are several passages.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`titles` (`str` 或 `List[str]`) — 要编码的段落标题。如果有多个段落，则可以是一个字符串或一个字符串列表。'
- en: '`texts` (`str` or `List[str]`) — The passages texts to be encoded. This can
    be a string or a list of strings if there are several passages.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`texts` (`str` 或 `List[str]`) — 要编码的段落文本。如果有多个段落，则可以是一个字符串或一个字符串列表。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*，默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则填充到模型可接受的最大输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即，可以输出长度不同的序列批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*，默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则将逐个标记截断，从一对序列中最长的序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则只会截断第一个序列。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则截断到模型可接受的最大输入长度。如果提供了一对序列（或一批序列），则只会截断第二个序列。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：无截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度，如果截断/填充参数需要最大长度。如果模型没有特定的最大输入长度（如 XLNet），则截断/填充到最大长度将被停用。
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether or not to return the
    attention mask. If not set, will return the attention mask according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果未设置，将根据特定标记器的默认值返回注意力掩码，该默认值由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: Returns
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, List[List[int]]]`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, List[List[int]]]`'
- en: 'A dictionary with the following keys:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有以下键的字典：
- en: '`input_ids`: List of token ids to be fed to a model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`: 要提供给模型的标记 id 列表。'
- en: '`attention_mask`: List of indices specifying which tokens should be attended
    to by the model.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`: 指定哪些标记应该被模型关注的索引列表。'
- en: Construct a DPRReader tokenizer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 DPRReader 标记器。
- en: '[DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer)
    is almost identical to [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    and runs end-to-end tokenization: punctuation splitting and wordpiece. The difference
    is that is has three inputs strings: question, titles and texts that are combined
    to be fed to the [DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)
    model.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer)
    几乎与 [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    相同，并且进行端到端的标记化：标点符号拆分和 wordpiece。不同之处在于它有三个输入字符串：问题、标题和文本，这些字符串被组合在一起供[DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)模型使用。'
- en: Refer to superclass [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    for usage examples and documentation concerning parameters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有关用法示例和参数文档，请参考超类 [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)。
- en: Return a dictionary with the token ids of the input strings and other information
    to give to `.decode_best_spans`. It converts the strings of a question and different
    passages (title and text) in a sequence of IDs (integers), using the tokenizer
    and vocabulary. The resulting `input_ids` is a matrix of size `(n_passages, sequence_length)`
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个字典，其中包含输入字符串的标记 id 和其他信息，以提供给 `.decode_best_spans`。它使用标记器和词汇将问题和不同段落（标题和文本）的字符串转换为
    ID（整数）序列。生成的 `input_ids` 是大小为 `(n_passages, sequence_length)` 的矩阵。
- en: 'with the format:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 格式如下：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: DPRReaderTokenizerFast
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRReaderTokenizerFast
- en: '### `class transformers.DPRReaderTokenizerFast`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRReaderTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L392)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L392)'
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`questions` (`str` or `List[str]`) — The questions to be encoded. You can specify
    one question for many passages. In this case, the question will be duplicated
    like `[questions] * n_passages`. Otherwise you have to specify as many questions
    as in `titles` or `texts`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`questions` (`str` 或 `List[str]`) — 要编码的问题。您可以为许多段落指定一个问题。在这种情况下，问题将被复制，如 `[questions]
    * n_passages`。否则，您必须指定与 `titles` 或 `texts` 中相同数量的问题。'
- en: '`titles` (`str` or `List[str]`) — The passages titles to be encoded. This can
    be a string or a list of strings if there are several passages.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`titles` (`str` 或 `List[str]`) — 要编码的段落标题。如果有多个段落，则可以是字符串或字符串列表。'
- en: '`texts` (`str` or `List[str]`) — The passages texts to be encoded. This can
    be a string or a list of strings if there are several passages.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`texts` (`str` 或 `List[str]`) — 要编码的段落文本。如果有多个段落，则可以是字符串或字符串列表。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为 `False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`: 填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: 填充到由参数 `max_length` 指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：无填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) — Activates and controls truncation. Accepts
    the following values:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *可选*, 默认为 `False`) — 激活和控制截断。接受以下值：'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest_first''`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将逐个标记截断，如果提供了一对序列（或一批对序列），则从最长序列中删除一个标记。'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将仅截断一对序列中的第一个序列，如果提供了一对序列（或一批对序列）。'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`：截断到指定的最大长度，该长度由参数 `max_length` 指定，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将仅截断一对序列中的第二个序列，如果提供了一对序列（或一批对序列）。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出序列长度大于模型最大可接受输入大小的批次）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数之一使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为 `None`，则将使用预定义的模型最大长度（如果截断/填充参数需要最大长度）。如果模型没有特定的最大输入长度（如 XLNet），则将禁用截断/填充到最大长度。
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`：返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`：返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`：返回 Numpy `np.ndarray` 对象。'
- en: '`return_attention_mask` (`bool`, *optional*) — Whether or not to return the
    attention mask. If not set, will return the attention mask according to the specific
    tokenizer’s default, defined by the `return_outputs` attribute.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *可选*) — 是否返回注意力蒙版。如果未设置，将根据特定分词器的默认值返回注意力蒙版，由
    `return_outputs` 属性定义。'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力蒙版？](../glossary#attention-mask)'
- en: Returns
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Dict[str, List[List[int]]]`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, List[List[int]]]`'
- en: 'A dictionary with the following keys:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含以下键的字典：
- en: '`input_ids`: List of token ids to be fed to a model.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`：要馈送给模型的标记 ID 列表。'
- en: '`attention_mask`: List of indices specifying which tokens should be attended
    to by the model.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`：指定哪些标记应该被模型关注的索引列表。'
- en: Constructs a “fast” DPRReader tokenizer (backed by HuggingFace’s *tokenizers*
    library).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个“快速” DPRReader 分词器（由 HuggingFace 的 *tokenizers* 库支持）。
- en: '[DPRReaderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizerFast)
    is almost identical to [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    and runs end-to-end tokenization: punctuation splitting and wordpiece. The difference
    is that is has three inputs strings: question, titles and texts that are combined
    to be fed to the [DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)
    model.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRReaderTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizerFast)
    几乎与 [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    相同，并且运行端到端的分词：标点符号拆分和词片。不同之处在于它有三个输入字符串：问题、标题和文本，这些字符串被组合在一起馈送给 [DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)
    模型。'
- en: Refer to superclass [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    for usage examples and documentation concerning parameters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 参考超类 [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)
    以获取用法示例和有关参数的文档。
- en: 'Return a dictionary with the token ids of the input strings and other information
    to give to `.decode_best_spans`. It converts the strings of a question and different
    passages (title and text) in a sequence of IDs (integers), using the tokenizer
    and vocabulary. The resulting `input_ids` is a matrix of size `(n_passages, sequence_length)`
    with the format:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个包含输入字符串的标记 ID 和其他信息以提供给 `.decode_best_spans` 的字典。它将问题和不同段落（标题和文本）的字符串转换为
    ID（整数）序列，使用分词器和词汇表。生成的 `input_ids` 是一个大小为 `(n_passages, sequence_length)` 的矩阵，格式如下：
- en: '[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS] <问题标记 ID> [SEP] <标题 ID> [SEP] <文本 ID>'
- en: DPR specific outputs
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPR 特定输出
- en: '### `class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L61)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L61)'
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`)
    — The DPR encoder outputs the *pooler_output* that corresponds to the context
    representation. Last layer hidden-state of the first token of the sequence (classification
    token) further processed by a Linear layer. This output is to be used to embed
    contexts for nearest neighbors queries with questions embeddings.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, embeddings_size)`) —
    DPR编码器输出*pooler_output*对应于上下文表示。序列的第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层处理。此输出用于将上下文嵌入到问题嵌入的最近邻查询中。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Class for outputs of [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)的输出类。'
- en: '### `class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L89)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L89)'
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`)
    — The DPR encoder outputs the *pooler_output* that corresponds to the question
    representation. Last layer hidden-state of the first token of the sequence (classification
    token) further processed by a Linear layer. This output is to be used to embed
    questions for nearest neighbors queries with context embeddings.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, embeddings_size)`) —
    DPR编码器输出*pooler_output*对应于问题表示。序列的第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层处理。此输出用于将问题嵌入到上下文嵌入的最近邻查询中。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Class for outputs of [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)的输出类。'
- en: '### `class transformers.DPRReaderOutput`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRReaderOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L117)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L117)'
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`start_logits` (`torch.FloatTensor` of shape `(n_passages, sequence_length)`)
    — Logits of the start index of the span for each passage.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits` (`torch.FloatTensor`，形状为`(n_passages, sequence_length)`) — 每个段落跨度的开始索引的logits。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(n_passages, sequence_length)`)
    — Logits of the end index of the span for each passage.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits` (`torch.FloatTensor`，形状为`(n_passages, sequence_length)`) — 每个段落跨度的结束索引的logits。'
- en: '`relevance_logits` (`torch.FloatTensor` of shape `(n_passages, )`) — Outputs
    of the QA classifier of the DPRReader that corresponds to the scores of each passage
    to answer the question, compared to all the other passages.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relevance_logits` (`torch.FloatTensor`，形状为`(n_passages, )`) — DPRReader的QA分类器的输出，对应于每个段落回答问题的分数，与所有其他段落进行比较。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: Class for outputs of [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 用于[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)的输出类。
- en: PytorchHide Pytorch content
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: DPRContextEncoder
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRContextEncoder
- en: '### `class transformers.DPRContextEncoder`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRContextEncoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L433)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L433)'
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRContextEncoder transformer outputting pooler outputs as context
    representations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DPRContextEncoder变压器输出池化器输出作为上下文表示。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L445)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L445)'
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. To match pretraining, DPR
    input sequence should be formatted with [CLS] and [SEP] tokens as follows:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） - 词汇表中输入序列标记的索引。为了匹配预训练，DPR输入序列应按照以下格式进行格式化：[CLS]和[SEP]标记。'
- en: '(a) For sequence pairs (for a pair title+text for example):'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （a）对于序列对（例如标题+文本对）：
- en: Returns
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）和输入的各种元素。
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`)
    — The DPR encoder outputs the *pooler_output* that corresponds to the context
    representation. Last layer hidden-state of the first token of the sequence (classification
    token) further processed by a Linear layer. This output is to be used to embed
    contexts for nearest neighbors queries with questions embeddings.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, embeddings_size)`的`torch.FloatTensor`） - DPR编码器输出对应于上下文表示的*pooler_output*。序列的第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层处理。此输出用于嵌入上下文以进行最近邻查询与问题嵌入。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出以及初始嵌入输出的隐藏状态。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [DPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoder)
    forward method, overrides the `__call__` special method.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRContextEncoder)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: DPRQuestionEncoder
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRQuestionEncoder
- en: '### `class transformers.DPRQuestionEncoder`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRQuestionEncoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L514)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L514)'
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRQuestionEncoder transformer outputting pooler outputs as question
    representations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DPRQuestionEncoder变压器输出池化器输出作为问题表示。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: '#### `forward`'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L526)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L526)'
- en: '[PRE16]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. To match pretraining, DPR
    input sequence should be formatted with [CLS] and [SEP] tokens as follows:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。为了匹配预训练，DPR输入序列应该按照以下格式进行格式化，包括[CLS]和[SEP]标记：'
- en: '(a) For sequence pairs (for a pair title+text for example):'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 对于序列对（例如标题+文本对）：
- en: Returns
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))和输入的各种元素。
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`)
    — The DPR encoder outputs the *pooler_output* that corresponds to the question
    representation. Last layer hidden-state of the first token of the sequence (classification
    token) further processed by a Linear layer. This output is to be used to embed
    questions for nearest neighbors queries with context embeddings.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor`，形状为`(batch_size, embeddings_size)`) —
    DPR编码器输出*pooler_output*对应于问题表示。序列的第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层处理。此输出用于嵌入问题以进行带有上下文嵌入的最近邻查询。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    forward method, overrides the `__call__` special method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE17]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: DPRReader
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DPRReader
- en: '### `class transformers.DPRReader`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DPRReader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L596)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L596)'
- en: '[PRE18]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRReader transformer outputting span predictions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 裸DPRReader变压器输出跨度预测。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。
- en: '#### `forward`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L608)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_dpr.py#L608)'
- en: '[PRE19]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Tuple[torch.LongTensor]` of shapes `(n_passages, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. It has to be a sequence
    triplet with 1) the question and 2) the passages titles and 3) the passages texts
    To match pretraining, DPR `input_ids` sequence should be formatted with [CLS]
    and [SEP] with the format:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(n_passages, sequence_length)`的`Tuple[torch.LongTensor]`）—
    词汇表中输入序列标记的索引。它必须是一个序列三元组，包括1）问题、2）段落标题和3）段落文本。为了匹配预训练，DPR的`input_ids`序列应该以[CLS]和[SEP]的格式进行格式化：'
- en: '`[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>`'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[CLS] <问题标记id> [SEP] <标题id> [SEP] <文本id>`'
- en: DPR is a model with absolute position embeddings so it’s usually advised to
    pad the inputs on the right rather than the left.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DPR是一个具有绝对位置嵌入的模型，因此通常建议在右侧而不是左侧填充输入。
- en: Indices can be obtained using [DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer).
    See this class documentation for more details.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer)获取索引。有关更多详细信息，请参阅此类文档。
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(n_passages, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(n_passages, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(n_passages, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(n_passages, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: Returns
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.dpr.modeling_dpr.DPRReaderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.dpr.modeling_dpr.DPRReaderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderOutput)或`torch.FloatTensor`元组'
- en: A [transformers.models.dpr.modeling_dpr.DPRReaderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.dpr.modeling_dpr.DPRReaderOutput](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）和输入。
- en: '`start_logits` (`torch.FloatTensor` of shape `(n_passages, sequence_length)`)
    — Logits of the start index of the span for each passage.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits`（形状为`(n_passages, sequence_length)`的`torch.FloatTensor`） — 每个段落跨度的开始索引的logits。'
- en: '`end_logits` (`torch.FloatTensor` of shape `(n_passages, sequence_length)`)
    — Logits of the end index of the span for each passage.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits`（形状为`(n_passages, sequence_length)`的`torch.FloatTensor`） — 每个段落跨度的结束索引的logits。'
- en: '`relevance_logits` (`torch.FloatTensor` of shape `(n_passages, )`) — Outputs
    of the QA classifier of the DPRReader that corresponds to the scores of each passage
    to answer the question, compared to all the other passages.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relevance_logits`（形状为`(n_passages, )`的`torch.FloatTensor`） — DPRReader的QA分类器的输出，对应于每个段落回答问题的分数，与所有其他段落进行比较。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)
    forward method, overrides the `__call__` special method.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[DPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReader)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE20]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TensorFlowHide TensorFlow content
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: TFDPRContextEncoder
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFDPRContextEncoder
- en: '### `class transformers.TFDPRContextEncoder`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFDPRContextEncoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L547)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L547)'
- en: '[PRE21]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRContextEncoder transformer outputting pooler outputs as context
    representations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DPRContextEncoder变压器输出池化器输出作为上下文表示。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典的第一个位置参数。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，您应该可以“轻松使用”
    - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras `Functional`
    API 创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])` 或
    `model([input_ids, attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些内容，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L563)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L563)'
- en: '[PRE22]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. To match pretraining, DPR
    input sequence should be formatted with [CLS] and [SEP] tokens as follows:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`Numpy array` 或 `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — 输入序列标记在词汇表中的索引。为了匹配预训练，DPR输入序列应该按照以下格式进行格式化：[CLS] 和 [SEP] 标记。'
- en: '(a) For sequence pairs (for a pair title+text for example):'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 对于序列对（例如标题+文本对）：
- en: Returns
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput` or `tuple(tf.Tensor)`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput` 或 `tuple(tf.Tensor)`'
- en: A `transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput` or a tuple
    of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）和输入的不同元素。
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, embeddings_size)`) — The
    DPR encoder outputs the *pooler_output* that corresponds to the context representation.
    Last layer hidden-state of the first token of the sequence (classification token)
    further processed by a Linear layer. This output is to be used to embed contexts
    for nearest neighbors queries with questions embeddings.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`tf.Tensor` of shape `(batch_size, embeddings_size)`) — DPR编码器输出与上下文表示相对应的*pooler_output*。序列的第一个标记（分类标记）的最后一层隐藏状态进一步由线性层处理。此输出用于嵌入上下文以进行最近邻查询与问题嵌入。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每一层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在自注意力头中用于计算加权平均值的注意力softmax之后的注意力权重。
- en: The [TFDPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRContextEncoder)
    forward method, overrides the `__call__` special method.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFDPRContextEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRContextEncoder)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TFDPRQuestionEncoder
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFDPRQuestionEncoder
- en: '### `class transformers.TFDPRQuestionEncoder`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFDPRQuestionEncoder`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L636)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L636)'
- en: '[PRE24]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRQuestionEncoder transformer outputting pooler outputs as question
    representations.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DPRQuestionEncoder变压器输出池化器输出作为问题表示。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档，了解与一般用法和行为相关的所有事项。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典放在第一个位置参数中。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种格式得到支持的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，在使用诸如`model.fit()`之类的方法时，对您来说应该“只需工作”
    - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras `Functional`
    API 创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度不定的列表，其中包含一个或多个按照文档字符串中给定的顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L652)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L652)'
- en: '[PRE25]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. To match pretraining, DPR
    input sequence should be formatted with [CLS] and [SEP] tokens as follows:'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`Numpy数组`或`tf.Tensor`） - 词汇表中输入序列标记的索引。为了匹配预训练，DPR输入序列应按照以下格式进行格式化，包括[CLS]和[SEP]标记：'
- en: '(a) For sequence pairs (for a pair title+text for example):'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （a）对于序列对（例如一对标题+文本）：
- en: Returns
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput` or `tuple(tf.Tensor)`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput` or a
    tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput`或`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）和输入。'
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, embeddings_size)`) — The
    DPR encoder outputs the *pooler_output* that corresponds to the question representation.
    Last layer hidden-state of the first token of the sequence (classification token)
    further processed by a Linear layer. This output is to be used to embed questions
    for nearest neighbors queries with context embeddings.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output`（形状为`(batch_size, embeddings_size)`的`tf.Tensor`） - DPR编码器输出*pooler_output*对应于问题表示。序列的第一个标记（分类标记）的最后一层隐藏状态，进一步由线性层处理。此输出用于嵌入问题以进行具有上下文嵌入的最近邻查询。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每个层的输出处的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    forward method, overrides the `__call__` special method.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)的前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: TFDPRReader
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFDPRReader
- en: '### `class transformers.TFDPRReader`'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFDPRReader`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L724)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L724)'
- en: '[PRE27]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare DPRReader transformer outputting span predictions.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的DPRReader变压器输出跨度预测。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有信息。
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`中的TensorFlow模型和层接受两种格式的输入：'
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为关键字参数（类似于PyTorch模型），或
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有输入作为列表、元组或字典的第一个位置参数。
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有此支持，当使用`model.fit()`等方法时，应该“只需工作”
    - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果要在Keras方法之外使用第二种格式，例如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可用于收集所有输入张量在第一个位置参数中： '
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids,
    attention_mask, token_type_ids])`
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids":
    token_type_ids})`'
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！
- en: '#### `call`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L740)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/dpr/modeling_tf_dpr.py#L740)'
- en: '[PRE28]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`Numpy array` or `tf.Tensor` of shapes `(n_passages, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. It has to be a sequence
    triplet with 1) the question and 2) the passages titles and 3) the passages texts
    To match pretraining, DPR `input_ids` sequence should be formatted with [CLS]
    and [SEP] with the format:'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(n_passages, sequence_length)`的`Numpy`数组或`tf.Tensor`）- 词汇表中输入序列标记的索引。它必须是一个序列三元组，包括1）问题、2）段落标题和3）段落文本。为了匹配预训练，DPR
    `input_ids` 序列应该使用[CLS]和[SEP]格式化：'
- en: '`[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>`'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`[CLS] <问题标记id> [SEP] <标题id> [SEP] <文本id>`'
- en: DPR is a model with absolute position embeddings so it’s usually advised to
    pad the inputs on the right rather than the left.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DPR是一个带有绝对位置嵌入的模型，因此通常建议在右侧而不是左侧填充输入。
- en: Indices can be obtained using [DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer).
    See this class documentation for more details.
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以使用[DPRReaderTokenizer](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRReaderTokenizer)获取索引。有关更多详细信息，请参阅此类文档。
- en: '`attention_mask` (`Numpy array` or `tf.Tensor` of shape `(n_passages, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(n_passages, sequence_length)`的`Numpy`数组或`tf.Tensor`，*可选*）-
    避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`inputs_embeds` (`Numpy array` or `tf.Tensor` of shape `(n_passages, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(n_passages, sequence_length, hidden_size)`的`Numpy`数组或`tf.Tensor`，*可选*）-
    可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为True。'
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`（`bool`，*可选*，默认为`False`）- 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。'
- en: Returns
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput` or `tuple(tf.Tensor)`'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput` or a tuple of
    `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig))
    and inputs.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput`或一组`tf.Tensor`（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[DPRConfig](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRConfig)）和输入的不同元素。
- en: '`start_logits` (`tf.Tensor` of shape `(n_passages, sequence_length)`) — Logits
    of the start index of the span for each passage.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_logits`（形状为`(n_passages, sequence_length)`的`tf.Tensor`）- 每个段落跨度的开始索引的logits。'
- en: '`end_logits` (`tf.Tensor` of shape `(n_passages, sequence_length)`) — Logits
    of the end index of the span for each passage.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end_logits`（形状为`(n_passages, sequence_length)`的`tf.Tensor`）- 每个段落跨度的结束索引的logits。'
- en: '`relevance_logits` (`tf.Tensor` of shape `(n_passages, )`) — Outputs of the
    QA classifier of the DPRReader that corresponds to the scores of each passage
    to answer the question, compared to all the other passages.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`relevance_logits`（形状为`(n_passages, )`的`tf.Tensor`）- DPRReader的QA分类器的输出，对应于每个段落回答问题的分数，与所有其他段落进行比较。'
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TFDPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRReader)
    forward method, overrides the `__call__` special method.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFDPRReader](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRReader)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE29]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
