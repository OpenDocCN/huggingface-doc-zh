# InstructPix2Pix

> åŸæ–‡é“¾æ¥ï¼š[`huggingface.co/docs/diffusers/training/instructpix2pix`](https://huggingface.co/docs/diffusers/training/instructpix2pix)

[InstructPix2Pix](https://hf.co/papers/2211.09800)æ˜¯ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ ¹æ®äººç±»æä¾›çš„æŒ‡ä»¤ç¼–è¾‘å›¾åƒã€‚ä¾‹å¦‚ï¼Œæ‚¨çš„æç¤ºå¯ä»¥æ˜¯â€œå°†äº‘å˜æˆé›¨â€ï¼Œæ¨¡å‹å°†ç›¸åº”åœ°ç¼–è¾‘è¾“å…¥å›¾åƒã€‚è¯¥æ¨¡å‹æ˜¯åŸºäºæ–‡æœ¬æç¤ºï¼ˆæˆ–ç¼–è¾‘æŒ‡ä»¤ï¼‰å’Œè¾“å…¥å›¾åƒçš„ã€‚

æœ¬æŒ‡å—å°†æ¢ç´¢[train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)è®­ç»ƒè„šæœ¬ï¼Œå¸®åŠ©æ‚¨ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•ä¸ºè‡ªå·±çš„ç”¨ä¾‹è¿›è¡Œé€‚åº”ã€‚

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»æºä»£ç å®‰è£…åº“ï¼š

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

ç„¶åè½¬åˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…æ‚¨æ­£åœ¨ä½¿ç”¨çš„è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š

```py
cd examples/instruct_pix2pix
pip install -r requirements.txt
```

ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šä¸ª GPU/TPU ä¸Šè¿›è¡Œè®­ç»ƒæˆ–ä½¿ç”¨æ··åˆç²¾åº¦çš„åº“ã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®æ‚¨çš„è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)ä»¥äº†è§£æ›´å¤šã€‚

åˆå§‹åŒ–ä¸€ä¸ªğŸ¤— Accelerate ç¯å¢ƒï¼š

```py
accelerate config
```

è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerate ç¯å¢ƒè€Œä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼š

```py
accelerate config default
```

æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼ shellï¼Œæ¯”å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚

ä»¥ä¸‹éƒ¨åˆ†çªå‡ºäº†è®­ç»ƒè„šæœ¬çš„é‡è¦éƒ¨åˆ†ï¼Œä»¥å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¿®æ”¹å®ƒï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯»[è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æ‚¨æ˜¯å¦æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚

## è„šæœ¬å‚æ•°

è®­ç»ƒè„šæœ¬æœ‰è®¸å¤šå‚æ•°å¯å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰å‚æ•°åŠå…¶æè¿°éƒ½å¯ä»¥åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚å¤§å¤šæ•°å‚æ•°éƒ½æä¾›äº†é»˜è®¤å€¼ï¼Œæ•ˆæœç›¸å½“ä¸é”™ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚

ä¾‹å¦‚ï¼Œè¦å¢åŠ è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡ï¼š

```py
accelerate launch train_instruct_pix2pix.py \
  --resolution=512 \
```

è®¸å¤šåŸºæœ¬å’Œé‡è¦çš„å‚æ•°åœ¨æ–‡æœ¬åˆ°å›¾åƒè®­ç»ƒæŒ‡å—ä¸­æœ‰æè¿°ï¼Œå› æ­¤æœ¬æŒ‡å—åªå…³æ³¨ InstructPix2Pix ç›¸å…³å‚æ•°ï¼š

+   `--original_image_column`ï¼šç¼–è¾‘å‰çš„åŸå§‹å›¾åƒ

+   `--edited_image_column`ï¼šç¼–è¾‘å®Œæˆåçš„å›¾åƒ

+   `--edit_prompt_column`ï¼šç¼–è¾‘å›¾åƒçš„æŒ‡ä»¤

+   `--conditioning_dropout_prob`ï¼šåœ¨è®­ç»ƒæœŸé—´ç”¨äºç¼–è¾‘å›¾åƒå’Œç¼–è¾‘æç¤ºçš„ä¸¢å¤±æ¦‚ç‡ï¼Œä»è€Œå®ç°æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆCFGï¼‰ç”¨äºä¸€ä¸ªæˆ–ä¸¤ä¸ªè°ƒèŠ‚è¾“å…¥

## è®­ç»ƒè„šæœ¬

æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯å¯ä»¥åœ¨[`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374)å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è¿™æ˜¯æ‚¨å°†å¯¹è®­ç»ƒè„šæœ¬è¿›è¡Œæ›´æ”¹ä»¥é€‚åº”è‡ªå·±ç”¨ä¾‹çš„åœ°æ–¹ã€‚

ä¸è„šæœ¬å‚æ•°ä¸€æ ·ï¼Œåœ¨æ–‡æœ¬åˆ°å›¾åƒè®­ç»ƒæŒ‡å—ä¸­æä¾›äº†è®­ç»ƒè„šæœ¬çš„è¯¦ç»†è¯´æ˜ã€‚ç›¸åï¼Œæœ¬æŒ‡å—å°†é‡ç‚¹ä»‹ç»è„šæœ¬ä¸­ä¸ InstructPix2Pix ç›¸å…³çš„éƒ¨åˆ†ã€‚

è„šæœ¬é¦–å…ˆé€šè¿‡ä¿®æ”¹ UNet çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°é‡æ¥è€ƒè™‘ InstructPix2Pix çš„é¢å¤–è°ƒèŠ‚å›¾åƒï¼š

```py
in_channels = 8
out_channels = unet.conv_in.out_channels
unet.register_to_config(in_channels=in_channels)

with torch.no_grad():
    new_conv_in = nn.Conv2d(
        in_channels, out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding
    )
    new_conv_in.weight.zero_()
    new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)
    unet.conv_in = new_conv_in
```

è¿™äº› UNet å‚æ•°ç”±ä¼˜åŒ–å™¨è¿›è¡Œäº†æ›´æ–°ï¼š

```py
optimizer = optimizer_cls(
    unet.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

æ¥ä¸‹æ¥ï¼Œç¼–è¾‘åçš„å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤è¢«é¢„å¤„ç†å¹¶è¿›è¡Œäº†æ ‡è®°åŒ–ã€‚é‡è¦çš„æ˜¯å¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘åçš„å›¾åƒåº”ç”¨ç›¸åŒçš„å›¾åƒè½¬æ¢ã€‚

```py
def preprocess_train(examples):
    preprocessed_images = preprocess_images(examples)

    original_images, edited_images = preprocessed_images.chunk(2)
    original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)
    edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)

    examples["original_pixel_values"] = original_images
    examples["edited_pixel_values"] = edited_images

    captions = list(examples[edit_prompt_column])
    examples["input_ids"] = tokenize_captions(captions)
    return examples
```

æœ€åï¼Œåœ¨[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730)ä¸­ï¼Œå®ƒé¦–å…ˆå°†ç¼–è¾‘åçš„å›¾åƒç¼–ç ä¸ºæ½œåœ¨ç©ºé—´ï¼š

```py
latents = vae.encode(batch["edited_pixel_values"].to(weight_dtype)).latent_dist.sample()
latents = latents * vae.config.scaling_factor
```

ç„¶åï¼Œè„šæœ¬å¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤åµŒå…¥åº”ç”¨äº†è¾å­¦ä»¥æ”¯æŒ CFGã€‚è¿™å°±æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿè°ƒèŠ‚ç¼–è¾‘æŒ‡ä»¤å’ŒåŸå§‹å›¾åƒå¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“çš„åŸå› ã€‚

```py
encoder_hidden_states = text_encoder(batch["input_ids"])[0]
original_image_embeds = vae.encode(batch["original_pixel_values"].to(weight_dtype)).latent_dist.mode()

if args.conditioning_dropout_prob is not None:
    random_p = torch.rand(bsz, device=latents.device, generator=generator)
    prompt_mask = random_p < 2 * args.conditioning_dropout_prob
    prompt_mask = prompt_mask.reshape(bsz, 1, 1)
    null_conditioning = text_encoder(tokenize_captions([""]).to(accelerator.device))[0]
    encoder_hidden_states = torch.where(prompt_mask, null_conditioning, encoder_hidden_states)

    image_mask_dtype = original_image_embeds.dtype
    image_mask = 1 - (
        (random_p >= args.conditioning_dropout_prob).to(image_mask_dtype)
        * (random_p < 3 * args.conditioning_dropout_prob).to(image_mask_dtype)
    )
    image_mask = image_mask.reshape(bsz, 1, 1, 1)
    original_image_embeds = image_mask * original_image_embeds
```

å°±æ˜¯è¿™æ ·ï¼é™¤äº†è¿™é‡Œæè¿°çš„å·®å¼‚ä¹‹å¤–ï¼Œå…¶ä½™éƒ¨åˆ†çš„è„šæœ¬ä¸ Text-to-image è®­ç»ƒè„šæœ¬éå¸¸ç›¸ä¼¼ï¼Œæ‰€ä»¥è¯·éšæ—¶æŸ¥çœ‹æ›´å¤šç»†èŠ‚ã€‚å¦‚æœæ‚¨æƒ³äº†è§£è®­ç»ƒå¾ªç¯çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥çœ‹ç†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦å™¨æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹è¯¦ç»†ä»‹ç»äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚

## å¯åŠ¨è„šæœ¬

ä¸€æ—¦æ‚¨å¯¹è„šæœ¬çš„æ›´æ”¹æ»¡æ„ï¼Œæˆ–è€…å¦‚æœæ‚¨å¯¹é»˜è®¤é…ç½®æ»¡æ„ï¼Œæ‚¨å°±å¯ä»¥å¯åŠ¨è®­ç»ƒè„šæœ¬äº†ï¼ğŸš€

æœ¬æŒ‡å—ä½¿ç”¨[fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples)æ•°æ®é›†ï¼Œè¿™æ˜¯[åŸå§‹æ•°æ®é›†](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered)çš„ä¸€ä¸ªè¾ƒå°ç‰ˆæœ¬ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå’Œä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼ˆè¯·å‚é˜…åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†æŒ‡å—ï¼‰ã€‚

å°†`MODEL_NAME`ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºæ¨¡å‹çš„åç§°ï¼ˆå¯ä»¥æ˜¯ Hub ä¸Šçš„æ¨¡å‹ ID æˆ–æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ï¼‰ï¼Œå°†`DATASET_ID`è®¾ç½®ä¸º Hub ä¸Šæ•°æ®é›†çš„åç§°ã€‚è„šæœ¬å°†åˆ›å»ºå¹¶ä¿å­˜æ‰€æœ‰ç»„ä»¶ï¼ˆç‰¹å¾æå–å™¨ã€è°ƒåº¦å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€UNet ç­‰ï¼‰åˆ°æ‚¨çš„å­˜å‚¨åº“çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚

ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒã€‚æˆ‘ä»¬åªåœ¨è¾ƒå°è§„æ¨¡çš„æ•°æ®é›†ä¸Šæµ‹è¯•äº†è¿™ä¸ªè®­ç»ƒè„šæœ¬ã€‚

è¦ä½¿ç”¨ Weights and Biases ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œè¯·åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ `--report_to=wandb`å‚æ•°ï¼Œå¹¶ä½¿ç”¨`--val_image_url`æŒ‡å®šä¸€ä¸ªéªŒè¯å›¾åƒå’Œ`--validation_prompt`æŒ‡å®šä¸€ä¸ªéªŒè¯æç¤ºã€‚è¿™å¯¹äºè°ƒè¯•æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚

å¦‚æœæ‚¨æ­£åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯·åœ¨`accelerate launch`å‘½ä»¤ä¸­æ·»åŠ `--multi_gpu`å‚æ•°ã€‚

```py
accelerate launch --mixed_precision="fp16" train_instruct_pix2pix.py \
    --pretrained_model_name_or_path=$MODEL_NAME \
    --dataset_name=$DATASET_ID \
    --enable_xformers_memory_efficient_attention \
    --resolution=256 \
    --random_flip \
    --train_batch_size=4 \
    --gradient_accumulation_steps=4 \
    --gradient_checkpointing \
    --max_train_steps=15000 \
    --checkpointing_steps=5000 \
    --checkpoints_total_limit=1 \
    --learning_rate=5e-05 \
    --max_grad_norm=1 \
    --lr_warmup_steps=0 \
    --conditioning_dropout_prob=0.05 \
    --mixed_precision=fp16 \
    --seed=42 \
    --push_to_hub
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„ InstructPix2Pix è¿›è¡Œæ¨ç†ï¼š

```py
import PIL
import requests
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline
from diffusers.utils import load_image

pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained("your_cool_model", torch_dtype=torch.float16).to("cuda")
generator = torch.Generator("cuda").manual_seed(0)

image = load_image("https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png")
prompt = "add some ducks to the lake"
num_inference_steps = 20
image_guidance_scale = 1.5
guidance_scale = 10

edited_image = pipeline(
   prompt,
   image=image,
   num_inference_steps=num_inference_steps,
   image_guidance_scale=image_guidance_scale,
   guidance_scale=guidance_scale,
   generator=generator,
).images[0]
edited_image.save("edited_image.png")
```

æ‚¨åº”è¯¥å°è¯•ä¸åŒçš„`num_inference_steps`ã€`image_guidance_scale`å’Œ`guidance_scale`å€¼ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ¨ç†é€Ÿåº¦å’Œè´¨é‡ã€‚æŒ‡å¯¼æ¯”ä¾‹å‚æ•°å°¤å…¶é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ§åˆ¶åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤å¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“ç¨‹åº¦ã€‚

## ç¨³å®šçš„ Diffusion XL

ç¨³å®šæ‰©æ•£ XLï¼ˆSDXLï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…¶æ¶æ„ä¸­æ·»åŠ äº†ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ã€‚ä½¿ç”¨[`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py)è„šæœ¬æ¥è®­ç»ƒä¸€ä¸ª SDXL æ¨¡å‹ï¼Œä»¥éµå¾ªå›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚

SDXL è®­ç»ƒè„šæœ¬åœ¨ SDXL è®­ç»ƒæŒ‡å—ä¸­æœ‰æ›´è¯¦ç»†çš„è®¨è®ºã€‚

## ä¸‹ä¸€æ­¥

æ­å–œæ‚¨è®­ç»ƒè‡ªå·±çš„ InstructPix2Pix æ¨¡å‹ï¼ğŸ¥³ è¦äº†è§£æ›´å¤šå…³äºè¯¥æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯èƒ½æœ‰åŠ©äºï¼š

+   é˜…è¯»[Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd)åšå®¢æ–‡ç« ï¼Œäº†è§£æˆ‘ä»¬å¯¹ InstructPix2Pixã€æ•°æ®é›†å‡†å¤‡ä»¥åŠä¸åŒæŒ‡ä»¤çš„å®éªŒç»“æœçš„æ›´å¤šä¿¡æ¯ã€‚
