["```py\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\ntok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\nexample_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\nbatch = tok(example_english_phrase, return_tensors=\"pt\")\ngenerated_ids = model.generate(batch[\"input_ids\"])\nassert tok.batch_decode(generated_ids, skip_special_tokens=True) == [\n    \"UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\"\n]\n```", "```py\n>>> from transformers import BartConfig, BartModel\n\n>>> # Initializing a BART facebook/bart-large style configuration\n>>> configuration = BartConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/bart-large style configuration\n>>> model = BartModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import BartTokenizer\n\n>>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import BartTokenizerFast\n\n>>> tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import AutoTokenizer, BartModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = BartModel.from_pretrained(\"facebook/bart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n>>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n>>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n```", "```py\n>>> from transformers import AutoTokenizer, BartForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n['not', 'good', 'healthy', 'great', 'very']\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\n>>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n>>> model.config.id2label[predicted_class_id]\n'POSITIVE'\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n>>> round(loss.item(), 2)\n0.0\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, BartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\n>>> model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = BartForSequenceClassification.from_pretrained(\n...     \"valhalla/bart-large-sst2\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, BartForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n>>> model = BartForQuestionAnswering.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n>>> tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n' nice puppet'\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n>>> round(loss.item(), 2)\n0.59\n```", "```py\n>>> from transformers import AutoTokenizer, BartForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = BartForCausalLM.from_pretrained(\"facebook/bart-base\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n>>> from transformers import AutoTokenizer, TFBartModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n>>> model = TFBartModel.from_pretrained(\"facebook/bart-large\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFBartForConditionalGeneration\n\n>>> model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"tf\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5)\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, TFBartForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"tf\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n>>> probs = tf.nn.softmax(logits[0])\n>>> # probs[5] is associated with the mask token\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = FlaxBartModel.from_pretrained(\"facebook/bart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"]).sequences\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> import jax\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n>>> input_ids = tokenizer([TXT], return_tensors=\"jax\")[\"input_ids\"]\n\n>>> logits = model(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero()[0].item()\n>>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n>>> values, predictions = jax.lax.top_k(probs, k=1)\n\n>>> tokenizer.decode(predictions).split()\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = FlaxBartForSequenceClassification.from_pretrained(\"facebook/bart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = FlaxBartForQuestionAnswering.from_pretrained(\"facebook/bart-base\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n>>> inputs = tokenizer(question, text, return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxBartForConditionalGeneration\n\n>>> model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxBartForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n>>> model = FlaxBartForCausalLM.from_pretrained(\"facebook/bart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"np\")\n>>> outputs = model(**inputs)\n\n>>> # retrieve logts for next token\n>>> next_token_logits = outputs.logits[:, -1]\n```"]