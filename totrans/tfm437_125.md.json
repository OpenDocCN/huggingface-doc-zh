["```py\n( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None )\n```", "```py\n( callback )\n```", "```py\n( cache_enabled: Optional = True )\n```", "```py\n( model inputs return_outputs = False )\n```", "```py\n( )\n```", "```py\n( language: Optional = None license: Optional = None tags: Union = None model_name: Optional = None finetuned_from: Optional = None tasks: Union = None dataset_tags: Union = None dataset: Union = None dataset_args: Union = None )\n```", "```py\n( )\n```", "```py\n( num_training_steps: int )\n```", "```py\n( num_training_steps: int optimizer: Optimizer = None )\n```", "```py\n( eval_dataset: Union = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )\n```", "```py\n( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )\n```", "```py\n( inputs: Dict ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( model )\n```", "```py\n( eval_dataset: Optional = None )\n```", "```py\n( args: TrainingArguments )\n```", "```py\n( test_dataset: Dataset )\n```", "```py\n( )\n```", "```py\n( hp_space: Optional = None compute_objective: Optional = None n_trials: int = 20 direction: Union = 'minimize' backend: Union = None hp_name: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';[trainer_utils.BestRun or List[trainer_utils.BestRun]]\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( logs: Dict )\n```", "```py\n( split metrics )\n```", "```py\ninit_mem_cpu_alloc_delta   =     1301MB\ninit_mem_cpu_peaked_delta  =      154MB\ninit_mem_gpu_alloc_delta   =      230MB\ninit_mem_gpu_peaked_delta  =        0MB\ntrain_mem_cpu_alloc_delta  =     1345MB\ntrain_mem_cpu_peaked_delta =        0MB\ntrain_mem_gpu_alloc_delta  =      693MB\ntrain_mem_gpu_peaked_delta =        7MB\n```", "```py\n( metrics: Dict ) \u2192 export const metadata = 'undefined';metrics (Dict[str, float])\n```", "```py\n( dataloader: DataLoader )\n```", "```py\n( train_dl: DataLoader max_steps: Optional = None )\n```", "```py\n( callback ) \u2192 export const metadata = 'undefined';TrainerCallback\n```", "```py\n( test_dataset: Dataset ignore_keys: Optional = None metric_key_prefix: str = 'test' )\n```", "```py\n( dataloader: DataLoader description: str prediction_loss_only: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' )\n```", "```py\n( model: Module inputs: Dict prediction_loss_only: bool ignore_keys: Optional = None ) \u2192 export const metadata = 'undefined';Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n```", "```py\n( auto_find_batch_size = False )\n```", "```py\n( commit_message: Optional = 'End of training' blocking: bool = True **kwargs )\n```", "```py\n( callback )\n```", "```py\n( split metrics combined = True )\n```", "```py\n( output_dir: Optional = None _internal_call: bool = False )\n```", "```py\n( )\n```", "```py\n( resume_from_checkpoint: Union = None trial: Union = None ignore_keys_for_eval: Optional = None **kwargs )\n```", "```py\n( model: Module inputs: Dict ) \u2192 export const metadata = 'undefined';torch.Tensor\n```", "```py\n( model: Union = None args: TrainingArguments = None data_collator: Optional = None train_dataset: Optional = None eval_dataset: Union = None tokenizer: Optional = None model_init: Optional = None compute_metrics: Optional = None callbacks: Optional = None optimizers: Tuple = (None, None) preprocess_logits_for_metrics: Optional = None )\n```", "```py\n( eval_dataset: Optional = None ignore_keys: Optional = None metric_key_prefix: str = 'eval' **gen_kwargs )\n```", "```py\n( test_dataset: Dataset ignore_keys: Optional = None metric_key_prefix: str = 'test' **gen_kwargs )\n```", "```py\n( output_dir: str overwrite_output_dir: bool = False do_train: bool = False do_eval: bool = False do_predict: bool = False evaluation_strategy: Union = 'no' prediction_loss_only: bool = False per_device_train_batch_size: int = 8 per_device_eval_batch_size: int = 8 per_gpu_train_batch_size: Optional = None per_gpu_eval_batch_size: Optional = None gradient_accumulation_steps: int = 1 eval_accumulation_steps: Optional = None eval_delay: Optional = 0 learning_rate: float = 5e-05 weight_decay: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 max_grad_norm: float = 1.0 num_train_epochs: float = 3.0 max_steps: int = -1 lr_scheduler_type: Union = 'linear' lr_scheduler_kwargs: Optional = <factory> warmup_ratio: float = 0.0 warmup_steps: int = 0 log_level: Optional = 'passive' log_level_replica: Optional = 'warning' log_on_each_node: bool = True logging_dir: Optional = None logging_strategy: Union = 'steps' logging_first_step: bool = False logging_steps: float = 500 logging_nan_inf_filter: bool = True save_strategy: Union = 'steps' save_steps: float = 500 save_total_limit: Optional = None save_safetensors: Optional = True save_on_each_node: bool = False save_only_model: bool = False no_cuda: bool = False use_cpu: bool = False use_mps_device: bool = False seed: int = 42 data_seed: Optional = None jit_mode_eval: bool = False use_ipex: bool = False bf16: bool = False fp16: bool = False fp16_opt_level: str = 'O1' half_precision_backend: str = 'auto' bf16_full_eval: bool = False fp16_full_eval: bool = False tf32: Optional = None local_rank: int = -1 ddp_backend: Optional = None tpu_num_cores: Optional = None tpu_metrics_debug: bool = False debug: Union = '' dataloader_drop_last: bool = False eval_steps: Optional = None dataloader_num_workers: int = 0 past_index: int = -1 run_name: Optional = None disable_tqdm: Optional = None remove_unused_columns: Optional = True label_names: Optional = None load_best_model_at_end: Optional = False metric_for_best_model: Optional = None greater_is_better: Optional = None ignore_data_skip: bool = False fsdp: Union = '' fsdp_min_num_params: int = 0 fsdp_config: Optional = None fsdp_transformer_layer_cls_to_wrap: Optional = None deepspeed: Optional = None label_smoothing_factor: float = 0.0 optim: Union = 'adamw_torch' optim_args: Optional = None adafactor: bool = False group_by_length: bool = False length_column_name: Optional = 'length' report_to: Optional = None ddp_find_unused_parameters: Optional = None ddp_bucket_cap_mb: Optional = None ddp_broadcast_buffers: Optional = None dataloader_pin_memory: bool = True dataloader_persistent_workers: bool = False skip_memory_metrics: bool = True use_legacy_prediction_loop: bool = False push_to_hub: bool = False resume_from_checkpoint: Optional = None hub_model_id: Optional = None hub_strategy: Union = 'every_save' hub_token: Optional = None hub_private_repo: bool = False hub_always_push: bool = False gradient_checkpointing: bool = False gradient_checkpointing_kwargs: Optional = None include_inputs_for_metrics: bool = False fp16_backend: str = 'auto' push_to_hub_model_id: Optional = None push_to_hub_organization: Optional = None push_to_hub_token: Optional = None mp_parameters: str = '' auto_find_batch_size: bool = False full_determinism: bool = False torchdynamo: Optional = None ray_scope: Optional = 'last' ddp_timeout: Optional = 1800 torch_compile: bool = False torch_compile_backend: Optional = None torch_compile_mode: Optional = None dispatch_batches: Optional = None split_batches: Optional = False include_tokens_per_second: Optional = False include_num_input_tokens_seen: Optional = False neftune_noise_alpha: float = None )\n```", "```py\n( )\n```", "```py\n( num_training_steps: int )\n```", "```py\n( local = True desc = 'work' )\n```", "```py\n( train_batch_size: int = 8 eval_batch_size: int = 8 drop_last: bool = False num_workers: int = 0 pin_memory: bool = True persistent_workers: bool = False auto_find_batch_size: bool = False ignore_data_skip: bool = False sampler_seed: Optional = None )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n>>> args.per_device_train_batch_size\n16\n```", "```py\n( strategy: Union = 'no' steps: int = 500 batch_size: int = 8 accumulation_steps: Optional = None delay: Optional = None loss_only: bool = False jit_mode: bool = False )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_evaluate(strategy=\"steps\", steps=100)\n>>> args.eval_steps\n100\n```", "```py\n( strategy: Union = 'steps' steps: int = 500 report_to: Union = 'none' level: str = 'passive' first_step: bool = False nan_inf_filter: bool = False on_each_node: bool = False replica_level: str = 'passive' )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_logging(strategy=\"steps\", steps=100)\n>>> args.logging_steps\n100\n```", "```py\n( name: Union = 'linear' num_epochs: float = 3.0 max_steps: int = -1 warmup_ratio: float = 0 warmup_steps: int = 0 )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n>>> args.warmup_ratio\n0.05\n```", "```py\n( name: Union = 'adamw_torch' learning_rate: float = 5e-05 weight_decay: float = 0 beta1: float = 0.9 beta2: float = 0.999 epsilon: float = 1e-08 args: Optional = None )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n>>> args.optim\n'adamw_torch'\n```", "```py\n( model_id: str strategy: Union = 'every_save' token: Optional = None private_repo: bool = False always_push: bool = False )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_push_to_hub(\"me/awesome-model\")\n>>> args.hub_model_id\n'me/awesome-model'\n```", "```py\n( strategy: Union = 'steps' steps: int = 500 total_limit: Optional = None on_each_node: bool = False )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_save(strategy=\"steps\", steps=100)\n>>> args.save_steps\n100\n```", "```py\n( batch_size: int = 8 loss_only: bool = False jit_mode: bool = False )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_testing(batch_size=32)\n>>> args.per_device_eval_batch_size\n32\n```", "```py\n( learning_rate: float = 5e-05 batch_size: int = 8 weight_decay: float = 0 num_epochs: float = 3 max_steps: int = -1 gradient_accumulation_steps: int = 1 seed: int = 42 gradient_checkpointing: bool = False )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> args = TrainingArguments(\"working_dir\")\n>>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n>>> args.learning_rate\n1e-4\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( output_dir: str overwrite_output_dir: bool = False do_train: bool = False do_eval: bool = False do_predict: bool = False evaluation_strategy: Union = 'no' prediction_loss_only: bool = False per_device_train_batch_size: int = 8 per_device_eval_batch_size: int = 8 per_gpu_train_batch_size: Optional = None per_gpu_eval_batch_size: Optional = None gradient_accumulation_steps: int = 1 eval_accumulation_steps: Optional = None eval_delay: Optional = 0 learning_rate: float = 5e-05 weight_decay: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 max_grad_norm: float = 1.0 num_train_epochs: float = 3.0 max_steps: int = -1 lr_scheduler_type: Union = 'linear' lr_scheduler_kwargs: Optional = <factory> warmup_ratio: float = 0.0 warmup_steps: int = 0 log_level: Optional = 'passive' log_level_replica: Optional = 'warning' log_on_each_node: bool = True logging_dir: Optional = None logging_strategy: Union = 'steps' logging_first_step: bool = False logging_steps: float = 500 logging_nan_inf_filter: bool = True save_strategy: Union = 'steps' save_steps: float = 500 save_total_limit: Optional = None save_safetensors: Optional = True save_on_each_node: bool = False save_only_model: bool = False no_cuda: bool = False use_cpu: bool = False use_mps_device: bool = False seed: int = 42 data_seed: Optional = None jit_mode_eval: bool = False use_ipex: bool = False bf16: bool = False fp16: bool = False fp16_opt_level: str = 'O1' half_precision_backend: str = 'auto' bf16_full_eval: bool = False fp16_full_eval: bool = False tf32: Optional = None local_rank: int = -1 ddp_backend: Optional = None tpu_num_cores: Optional = None tpu_metrics_debug: bool = False debug: Union = '' dataloader_drop_last: bool = False eval_steps: Optional = None dataloader_num_workers: int = 0 past_index: int = -1 run_name: Optional = None disable_tqdm: Optional = None remove_unused_columns: Optional = True label_names: Optional = None load_best_model_at_end: Optional = False metric_for_best_model: Optional = None greater_is_better: Optional = None ignore_data_skip: bool = False fsdp: Union = '' fsdp_min_num_params: int = 0 fsdp_config: Optional = None fsdp_transformer_layer_cls_to_wrap: Optional = None deepspeed: Optional = None label_smoothing_factor: float = 0.0 optim: Union = 'adamw_torch' optim_args: Optional = None adafactor: bool = False group_by_length: bool = False length_column_name: Optional = 'length' report_to: Optional = None ddp_find_unused_parameters: Optional = None ddp_bucket_cap_mb: Optional = None ddp_broadcast_buffers: Optional = None dataloader_pin_memory: bool = True dataloader_persistent_workers: bool = False skip_memory_metrics: bool = True use_legacy_prediction_loop: bool = False push_to_hub: bool = False resume_from_checkpoint: Optional = None hub_model_id: Optional = None hub_strategy: Union = 'every_save' hub_token: Optional = None hub_private_repo: bool = False hub_always_push: bool = False gradient_checkpointing: bool = False gradient_checkpointing_kwargs: Optional = None include_inputs_for_metrics: bool = False fp16_backend: str = 'auto' push_to_hub_model_id: Optional = None push_to_hub_organization: Optional = None push_to_hub_token: Optional = None mp_parameters: str = '' auto_find_batch_size: bool = False full_determinism: bool = False torchdynamo: Optional = None ray_scope: Optional = 'last' ddp_timeout: Optional = 1800 torch_compile: bool = False torch_compile_backend: Optional = None torch_compile_mode: Optional = None dispatch_batches: Optional = None split_batches: Optional = False include_tokens_per_second: Optional = False include_num_input_tokens_seen: Optional = False neftune_noise_alpha: float = None sortish_sampler: bool = False predict_with_generate: bool = False generation_max_length: Optional = None generation_num_beams: Optional = None generation_config: Union = None )\n```", "```py\n( )\n```"]