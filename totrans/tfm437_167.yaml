- en: FLAN-T5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-t5](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flan-t5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/128.ae6daec2.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)
    - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can directly use FLAN-T5 weights without finetuning the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: FLAN-T5 includes the same improvements as T5 version 1.1 (see [here](https://huggingface.co/docs/transformers/model_doc/t5v1.1)
    for the full details of the model’s improvements.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Google has released the following variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/flan-t5-base](https://huggingface.co/google/flan-t5-base)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/flan-t5-large](https://huggingface.co/google/flan-t5-large)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/flan-t5-xl](https://huggingface.co/google/flan-t5-xl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints).
  prefs: []
  type: TYPE_NORMAL
- en: Refer to [T5’s documentation page](t5) for all API reference, code examples
    and notebooks. For more details regarding training and evaluation of the FLAN-T5,
    refer to the model card.
  prefs: []
  type: TYPE_NORMAL
