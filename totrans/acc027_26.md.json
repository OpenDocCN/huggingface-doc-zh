["```py\nconda create --name ml\n```", "```py\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n```", "```py\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\ncd ..\n```", "```py\npip install git+https://github.com/huggingface/Megatron-LM.git\n```", "```py\n:~$ accelerate config --config_file \"megatron_gpt_config.yaml\"\nIn which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\nWhich type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2\nHow many different machines will you use (use more than 1 for multi-node training)? [1]: \nDo you want to use DeepSpeed? [yes/NO]: \nDo you want to use FullyShardedDataParallel? [yes/NO]: \nDo you want to use Megatron-LM ? [yes/NO]: yes\nWhat is the Tensor Parallelism degree/size? [1]:2\nDo you want to enable Sequence Parallelism? [YES/no]: \nWhat is the Pipeline Parallelism degree/size? [1]:2\nWhat is the number of micro-batches? [1]:2\nDo you want to enable selective activation recomputation? [YES/no]: \nDo you want to use distributed optimizer which shards optimizer state and gradients across data parallel ranks? [YES/no]: \nWhat is the gradient clipping value based on global L2 Norm (0 to disable)? [1.0]: \nHow many GPU(s) should be used for distributed training? [1]:4\nDo you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16\n```", "```py\n~$ cat megatron_gpt_config.yaml \ncompute_environment: LOCAL_MACHINE\ndeepspeed_config: {}\ndistributed_type: MEGATRON_LM\ndowncast_bf16: 'no'\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config:\n  megatron_lm_gradient_clipping: 1.0\n  megatron_lm_num_micro_batches: 2\n  megatron_lm_pp_degree: 2\n  megatron_lm_recompute_activations: true\n  megatron_lm_sequence_parallelism: true\n  megatron_lm_tp_degree: 2\n  megatron_lm_use_distributed_optimizer: true\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```", "```py\nfrom accelerate.utils import MegatronLMDummyScheduler\n\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    lr_scheduler = MegatronLMDummyScheduler(\n        optimizer=optimizer,\n        total_num_steps=args.max_train_steps,\n        warmup_num_steps=args.num_warmup_steps,\n    )\nelse:\n    lr_scheduler = get_scheduler(\n        name=args.lr_scheduler_type,\n        optimizer=optimizer,\n        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,\n        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n    )\n```", "```py\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size\nelse:\n    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n```", "```py\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    losses.append(loss)\nelse:\n    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    losses = torch.tensor(losses)\nelse:\n    losses = torch.cat(losses)\n```", "```py\nif accelerator.distributed_type == DistributedType.MEGATRON_LM:\n    accelerator.save_state(args.output_dir)\nelse:\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\n        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n    )\n```", "```py\naccelerate launch --config_file megatron_gpt_config.yaml \\\nexamples/by_feature/megatron_lm_gpt_pretraining.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name wikitext \\\n--dataset_config_name wikitext-2-raw-v1 \\\n--block_size 1024 \\\n--learning_rate 5e-5 \\\n--per_device_train_batch_size 24 \\\n--per_device_eval_batch_size 24 \\\n--num_train_epochs 5 \\\n--with_tracking \\\n--report_to \"wandb\" \\\n--output_dir \"awesome_model\"\n```", "```py\nLoading extension module fused_dense_cuda...\n>>> done with compiling and loading fused kernels. Compilation time: 3.569 seconds\n > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)\nBuilding gpt model in the pre-training mode.\nThe Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.\nPreparing dataloader\nPreparing dataloader\nPreparing model\n > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 210753280\n > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 209445120\n > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 210753280\n > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 209445120\nPreparing optimizer\nPreparing scheduler\n> learning rate decay style: linear\n10/10/2022 22:57:22 - INFO - __main__ - ***** Running training *****\n10/10/2022 22:57:22 - INFO - __main__ -   Num examples = 2318\n10/10/2022 22:57:22 - INFO - __main__ -   Num Epochs = 5\n10/10/2022 22:57:22 - INFO - __main__ -   Instantaneous batch size per device = 24\n10/10/2022 22:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 48\n10/10/2022 22:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1\n10/10/2022 22:57:22 - INFO - __main__ -   Total optimization steps = 245\n 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                                                 | 49/245 [01:04<04:09,  1.27s/it]\n 10/10/2022 22:58:29 - INFO - __main__ - epoch 0: perplexity: 1222.1594275215962 eval_loss: 7.10837459564209\n 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                                     | 98/245 [02:10<03:07,  1.28s/it]\n 10/10/2022 22:59:35 - INFO - __main__ - epoch 1: perplexity: 894.5236583794557 eval_loss: 6.796291351318359\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                        | 147/245 [03:16<02:05,  1.28s/it]\n 10/10/2022 23:00:40 - INFO - __main__ - epoch 2: perplexity: 702.8458788508042 eval_loss: 6.555137634277344\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a            | 196/245 [04:22<01:02,  1.28s/it]\n 10/10/2022 23:01:46 - INFO - __main__ - epoch 3: perplexity: 600.3220028695281 eval_loss: 6.39746618270874\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 245/245 [05:27<00:00,  1.28s/it]\n```", "```py\nfrom accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group\n\n# Custom loss function for the Megatron model\nclass GPTTrainStepWithCustomLoss(GPTTrainStep):\n    def __init__(self, megatron_args, **kwargs):\n        super().__init__(megatron_args)\n        self.kwargs = kwargs\n\n    def get_loss_func(self):\n        def loss_func(inputs, loss_mask, output_tensor):\n            batch_size, seq_length = output_tensor.shape\n            losses = output_tensor.float()\n            loss_mask = loss_mask.view(-1).float()\n            loss = losses.view(-1) * loss_mask\n\n            # Resize and average loss per sample\n            loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)\n            loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)\n            loss_per_sample = loss_per_sample / loss_mask_per_sample\n\n            # Calculate and scale weighting\n            weights = torch.stack([(inputs == kt).float() for kt in self.kwargs[\"keytoken_ids\"]]).sum(axis=[0, 2])\n            weights = 1.0 + self.kwargs[\"alpha\"] * weights\n            # Calculate weighted average\n            weighted_loss = (loss_per_sample * weights).mean()\n\n            # Reduce loss across data parallel groups\n            averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])\n\n            return weighted_loss, {\"lm loss\": averaged_loss[0]}\n\n        return loss_func\n\n    def get_forward_step_func(self):\n        def forward_step(data_iterator, model):\n            \"\"\"Forward step.\"\"\"\n            # Get the batch.\n            tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)\n            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n\n            return output_tensor, partial(self.loss_func, tokens, loss_mask)\n\n        return forward_step\n\ndef main():\n    # Custom loss function for the Megatron model\n    keytoken_ids = []\n    keywords = [\"plt\", \"pd\", \"sk\", \"fit\", \"predict\", \" plt\", \" pd\", \" sk\", \" fit\", \" predict\"]\n    for keyword in keywords:\n        ids = tokenizer([keyword]).input_ids[0]\n        if len(ids) == 1:\n            keytoken_ids.append(ids[0])\n    accelerator.print(f\"Keytoken ids: {keytoken_ids}\")\n    accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss\n    accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {\n        \"keytoken_ids\": keytoken_ids,\n        \"alpha\": 0.25,\n    }\n```", "```py\nfrom accelerate.utils import MegatronLMDummyDataLoader\n\nmegatron_dataloader_config = {\n    \"data_path\": args.data_path,\n    \"splits_string\": args.splits_string,\n    \"seq_length\": args.block_size,\n    \"micro_batch_size\": args.per_device_train_batch_size,\n}\nmegatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)\naccelerator.state.megatron_lm_plugin.megatron_dataset_flag = True\n```", "```py\nmodel, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(\n    model, optimizer, lr_scheduler, megatron_dataloader, megatron_dataloader, megatron_dataloader\n)\n```", "```py\nwhile completed_steps < args.max_train_steps:\n    model.train()\n    batch = next(train_dataloader) if train_dataloader is not None else {}\n    outputs = model(**batch)\n    loss = outputs.loss\n    ...\n\n    if completed_steps % eval_interval == 0:\n        eval_completed_steps = 0\n        losses = []\n        while eval_completed_steps < eval_iters:\n            model.eval()\n            with torch.no_grad():\n                batch = next(eval_dataloader) if eval_dataloader is not None else {}\n                outputs = model(**batch)\n```", "```py\npython checkpoint_reshaping_and_interoperability.py \\\n--convert_checkpoint_from_megatron_to_transformers \\\n--load_path \"gpt/iter_0005000\" \\\n--save_path \"gpt/trfs_checkpoint\" \\\n--max_shard_size \"200MB\" \\\n--tokenizer_name \"gpt2\" \\\n--print-checkpoint-structure\n```", "```py\npython checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \\\n--load_path \"gpt/trfs_checkpoint\" \\\n--save_path \"gpt/megatron_lm_checkpoint\" \\\n--target_tensor_model_parallel_size 2 \\\n--target_pipeline_model_parallel_size 2 \\\n--target_data_parallel_size 2 \\\n--target_params_dtype \"bf16\" \\\n--make_vocab_size_divisible_by 128 \\\n--use_distributed_optimizer \\\n--print-checkpoint-structure\n```", "```py\nmegatron_lm_plugin = MegatronLMPlugin(return_logits=True)\n```", "```py\n# specifying tokenizer's vocab and merges file\nvocab_file = os.path.join(args.resume_from_checkpoint, \"vocab.json\")\nmerge_file = os.path.join(args.resume_from_checkpoint, \"merges.txt\")\nother_megatron_args = {\"vocab_file\": vocab_file, \"merge_file\": merge_file}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n\n# inference using `megatron_generate` functionality\ntokenizer.pad_token = tokenizer.eos_token\nmax_new_tokens = 64\nbatch_texts = [\n    \"Are you human?\",\n    \"The purpose of life is\",\n    \"The arsenal was constructed at the request of\",\n    \"How are you doing these days?\",\n]\nbatch_encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\n\n# top-p sampling\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    top_p=0.8,\n    top_p_decay=0.5,\n    temperature=0.9,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n\n# top-k sampling\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    top_k=50,\n    temperature=0.9,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n\n# adding `bos` token at the start\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"], batch_encodings[\"attention_mask\"], max_new_tokens=max_new_tokens, add_BOS=True\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n\n# beam search => only takes single prompt\nbatch_texts = [\"The purpose of life is\"]\nbatch_encodings = tokenizer(batch_texts, return_tensors=\"pt\", padding=True)\ngenerated_tokens = model.megatron_generate(\n    batch_encodings[\"input_ids\"],\n    batch_encodings[\"attention_mask\"],\n    max_new_tokens=max_new_tokens,\n    num_beams=20,\n    length_penalty=1.5,\n)\ndecoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())\naccelerator.print(decoded_preds)\n```", "```py\nother_megatron_args = {\"position_embedding_type\": \"alibi\"}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n```", "```py\nother_megatron_args = {\"attention_head_type\": \"multiquery\"}\nmegatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)\n```"]