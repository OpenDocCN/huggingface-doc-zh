["```py\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\", src_lang=\"en_XX\", tgt_lang=\"python\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> expected_translation_english = \"Returns the maximum value of a b c.\"\n>>> inputs = tokenizer(example_python_phrase, text_target=expected_translation_english, return_tensors=\"pt\")\n>>> model(**inputs)\n```", "```py\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> inputs = tokenizer(example_python_phrase, return_tensors=\"pt\")\n>>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")\n>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Returns the maximum value of a b c.\"\n```", "```py\n>>> from transformers import PLBartConfig, PLBartModel\n\n>>> # Initializing a PLBART uclanlp/plbart-base style configuration\n>>> configuration = PLBartConfig()\n\n>>> # Initializing a model (with random weights) from the uclanlp/plbart-base style configuration\n>>> model = PLBartModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> expected_translation_english = \"Returns the maximum value of a b c.\"\n>>> inputs = tokenizer(example_python_phrase, text_target=expected_translation_english, return_tensors=\"pt\")\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartModel.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartForConditionalGeneration\n\n>>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> # en_XX is the language symbol id <LID> for English\n>>> TXT = \"<s> Is 0 the <mask> Fibonacci number ? </s> en_XX\"\n>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> logits = model(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n['first', 'same', 'highest', 'result', 'number']\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, PLBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, PLBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = PLBartForSequenceClassification.from_pretrained(\n...     \"uclanlp/plbart-base\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForCausalLM.from_pretrained(\"uclanlp/plbart-base\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```"]