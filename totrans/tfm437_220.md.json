["```py\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\", src_lang=\"en_XX\", tgt_lang=\"python\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> expected_translation_english = \"Returns the maximum value of a b c.\"\n>>> inputs = tokenizer(example_python_phrase, text_target=expected_translation_english, return_tensors=\"pt\")\n>>> model(**inputs)\n```", "```py\n>>> from transformers import PLBartForConditionalGeneration, PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> inputs = tokenizer(example_python_phrase, return_tensors=\"pt\")\n>>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-python-en_XX\")\n>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"Returns the maximum value of a b c.\"\n```", "```py\n( vocab_size = 50005 max_position_embeddings = 1024 encoder_layers = 6 encoder_ffn_dim = 3072 encoder_attention_heads = 12 decoder_layers = 6 decoder_ffn_dim = 3072 decoder_attention_heads = 12 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 768 dropout = 0.1 attention_dropout = 0.1 activation_dropout = 0.0 init_std = 0.02 classifier_dropout = 0.0 scale_embedding = True pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 forced_eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import PLBartConfig, PLBartModel\n\n>>> # Initializing a PLBART uclanlp/plbart-base style configuration\n>>> configuration = PLBartConfig()\n\n>>> # Initializing a model (with random weights) from the uclanlp/plbart-base style configuration\n>>> model = PLBartModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' language_codes = 'base' tokenizer_file = None src_lang = None tgt_lang = None sp_model_kwargs: Optional = None additional_special_tokens = None **kwargs )\n```", "```py\n>>> from transformers import PLBartTokenizer\n\n>>> tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-python-en_XX\", src_lang=\"python\", tgt_lang=\"en_XX\")\n>>> example_python_phrase = \"def maximum(a,b,c):NEW_LINE_INDENTreturn max([a,b,c])\"\n>>> expected_translation_english = \"Returns the maximum value of a b c.\"\n>>> inputs = tokenizer(example_python_phrase, text_target=expected_translation_english, return_tensors=\"pt\")\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: PLBartConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartModel.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: PLBartConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartForConditionalGeneration\n\n>>> model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> # en_XX is the language symbol id <LID> for English\n>>> TXT = \"<s> Is 0 the <mask> Fibonacci number ? </s> en_XX\"\n>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n>>> logits = model(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n['first', 'same', 'highest', 'result', 'number']\n```", "```py\n( config: PLBartConfig **kwargs )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, PLBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, PLBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForSequenceClassification.from_pretrained(\"uclanlp/plbart-base\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = PLBartForSequenceClassification.from_pretrained(\n...     \"uclanlp/plbart-base\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PLBartForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n>>> model = PLBartForCausalLM.from_pretrained(\"uclanlp/plbart-base\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```"]