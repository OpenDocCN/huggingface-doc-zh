- en: RAG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag)
- en: '[![Models](../Images/8d827715f6e5e46ae48b9169a2927210.png)](https://huggingface.co/models?filter=rag)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[![Models](../Images/8d827715f6e5e46ae48b9169a2927210.png)](https://huggingface.co/models?filter=rag)'
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Retrieval-augmented generation (“RAG”) models combine the powers of pretrained
    dense retrieval (DPR) and sequence-to-sequence models. RAG models retrieve documents,
    pass them to a seq2seq model, then marginalize to generate outputs. The retriever
    and seq2seq modules are initialized from pretrained models, and fine-tuned jointly,
    allowing both retrieval and generation to adapt to downstream tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（“RAG”）模型结合了预训练的密集检索（DPR）和序列到序列模型的能力。RAG模型检索文档，将其传递给seq2seq模型，然后进行边缘化以生成输出。检索器和seq2seq模块是从预训练模型初始化的，并进行联合微调，使得检索和生成都能够适应下游任务。
- en: It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara
    Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike
    Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于论文[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
    by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin,
    Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian
    Riedel, Douwe Kiela。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Large pre-trained language models have been shown to store factual knowledge
    in their parameters, and achieve state-of-the-art results when fine-tuned on downstream
    NLP tasks. However, their ability to access and precisely manipulate knowledge
    is still limited, and hence on knowledge-intensive tasks, their performance lags
    behind task-specific architectures. Additionally, providing provenance for their
    decisions and updating their world knowledge remain open research problems. Pre-trained
    models with a differentiable access mechanism to explicit nonparametric memory
    can overcome this issue, but have so far been only investigated for extractive
    downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented
    generation (RAG) — models which combine pre-trained parametric and non-parametric
    memory for language generation. We introduce RAG models where the parametric memory
    is a pre-trained seq2seq model and the non-parametric memory is a dense vector
    index of Wikipedia, accessed with a pre-trained neural retriever. We compare two
    RAG formulations, one which conditions on the same retrieved passages across the
    whole generated sequence, the other can use different passages per token. We fine-tune
    and evaluate our models on a wide range of knowledge-intensive NLP tasks and set
    the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq
    models and task-specific retrieve-and-extract architectures. For language generation
    tasks, we find that RAG models generate more specific, diverse and factual language
    than a state-of-the-art parametric-only seq2seq baseline.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*已经证明，大型预训练语言模型在其参数中存储了事实知识，并且在下游自然语言处理任务上进行微调时取得了最先进的结果。然而，它们访问和精确操作知识的能力仍然有限，因此在知识密集型任务上，它们的性能落后于特定任务的架构。此外，为它们的决策提供来源并更新它们的世界知识仍然是一个开放的研究问题。具有可微分访问机制到显式非参数化内存的预训练模型可以解决这个问题，但迄今为止仅用于提取式下游任务进行了调查。我们探索了一种用于检索增强生成（RAG）的通用微调配方
    — 这些模型结合了预训练的参数化和非参数化内存进行语言生成。我们介绍了RAG模型，其中参数化内存是一个预训练的seq2seq模型，非参数化内存是维基百科的密集向量索引，通过预训练的神经检索器访问。我们比较了两种RAG公式，一种是在整个生成序列中条件于相同的检索段落，另一种可以使用每个标记的不同段落。我们在广泛的知识密集型自然语言处理任务上对我们的模型进行微调和评估，并在三个开放领域问答任务上取得了最先进的成绩，优于参数化seq2seq模型和特定任务的检索和提取架构。对于语言生成任务，我们发现RAG模型生成比最先进的仅参数化seq2seq基线更具体、多样化和事实性的语言。*'
- en: This model was contributed by [ola13](https://huggingface.co/ola13).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型由[ola13](https://huggingface.co/ola13)贡献。
- en: Usage tips
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: Retrieval-augmented generation (“RAG”) models combine the powers of pretrained
    dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them
    to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq
    modules are initialized from pretrained models, and fine-tuned jointly, allowing
    both retrieval and generation to adapt to downstream tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（“RAG”）模型结合了预训练的密集检索（DPR）和Seq2Seq模型的能力。RAG模型检索文档，将其传递给seq2seq模型，然后进行边缘化以生成输出。检索器和seq2seq模块是从预训练模型初始化的，并进行联合微调，使得检索和生成都能够适应下游任务。
- en: RagConfig
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagConfig
- en: '### `class transformers.RagConfig`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L80)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L80)'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`title_sep` (`str`, *optional*, defaults to `" / "`) — Separator inserted between
    the title and the text of the retrieved document when calling [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`title_sep` (`str`, *可选*, 默认为`" / "`) — 在调用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)时插入在标题和检索到的文档文本之间的分隔符。'
- en: '`doc_sep` (`str`, *optional*, defaults to `" // "`) — Separator inserted between
    the text of the retrieved document and the original input when calling [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_sep` (`str`, *可选*, 默认为`" // "`) — 在调用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)时插入在检索到的文档文本和原始输入之间的分隔符。'
- en: '`n_docs` (`int`, *optional*, defaults to 5) — Number of documents to retrieve.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`, *可选*, 默认为5) — 要检索的文档数量。'
- en: '`max_combined_length` (`int`, *optional*, defaults to 300) — Max length of
    contextualized input returned by `__call__()`.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_combined_length` (`int`, *可选*, 默认为300) — `__call__()`返回的上下文化输入的最大长度。'
- en: '`retrieval_vector_size` (`int`, *optional*, defaults to 768) — Dimensionality
    of the document embeddings indexed by [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieval_vector_size` (`int`, *可选*, 默认为768) — 由[RagRetriever](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagRetriever)索引的文档嵌入的维度。'
- en: '`retrieval_batch_size` (`int`, *optional*, defaults to 8) — Retrieval batch
    size, defined as the number of queries issues concurrently to the faiss index
    encapsulated [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieval_batch_size` (`int`, *可选*, 默认为8) — 检索批量大小，定义为同时发出给封装的faiss索引的查询数量[RagRetriever](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagRetriever)。'
- en: '`dataset` (`str`, *optional*, defaults to `"wiki_dpr"`) — A dataset identifier
    of the indexed dataset in HuggingFace Datasets (list all available datasets and
    ids using `datasets.list_datasets()`).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset` (`str`, *可选*, 默认为`"wiki_dpr"`) — 在HuggingFace Datasets中索引数据集的数据集标识符（使用`datasets.list_datasets()`列出所有可用数据集和ID）。'
- en: '`dataset_split` (`str`, *optional*, defaults to `"train"`) — Which split of
    the `dataset` to load.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset_split` (`str`, *可选*, 默认为`"train"`) — 要加载的`dataset`的哪个拆分。'
- en: '`index_name` (`str`, *optional*, defaults to `"compressed"`) — The index name
    of the index associated with the `dataset`. One can choose between `"legacy"`,
    `"exact"` and `"compressed"`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_name` (`str`, *可选*, 默认为`"compressed"`) — 与`dataset`关联的索引的索引名称。可以在`"legacy"`、`"exact"`和`"compressed"`之间进行选择。'
- en: '`index_path` (`str`, *optional*) — The path to the serialized faiss index on
    disk.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_path` (`str`, *可选*) — 磁盘上序列化faiss索引的路径。'
- en: '`passages_path` (`str`, *optional*) — A path to text passages compatible with
    the faiss index. Required if using `LegacyIndex`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`passages_path` (`str`, *可选*) — 与faiss索引兼容的文本段落的路径。如果使用`LegacyIndex`，则需要。'
- en: '`use_dummy_dataset` (`bool`, *optional*, defaults to `False`) — Whether to
    load a “dummy” variant of the dataset specified by `dataset`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_dummy_dataset` (`bool`, *可选*, 默认为`False`) — 是否加载由`dataset`指定的数据集的“虚拟”变体。 '
- en: '`label_smoothing` (`float`, *optional*, defaults to 0.0) — Only relevant if
    `return_loss` is set to `True`. Controls the `epsilon` parameter value for label
    smoothing in the loss calculation. If set to 0, no label smoothing is performed.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_smoothing` (`float`, *可选*, 默认为0.0) — 仅在`return_loss`设置为`True`时相关。控制损失计算中标签平滑的`epsilon`参数值。如果设置为0，则不执行标签平滑。'
- en: '`do_marginalize` (`bool`, *optional*, defaults to `False`) — If `True`, the
    logits are marginalized over all documents by making use of `torch.nn.functional.log_softmax`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_marginalize` (`bool`, *可选*, 默认为`False`) — 如果为`True`，则通过使用`torch.nn.functional.log_softmax`对所有文档的logits进行边际化。'
- en: '`reduce_loss` (`bool`, *optional*, defaults to `False`) — Whether or not to
    reduce the NLL loss using the `torch.Tensor.sum` operation.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_loss` (`bool`, *可选*, 默认为`False`) — 是否使用`torch.Tensor.sum`操作减少NLL损失。'
- en: '`do_deduplication` (`bool`, *optional*, defaults to `True`) — Whether or not
    to deduplicate the generations from different context documents for a given input.
    Has to be set to `False` if used while training with distributed backend.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_deduplication` (`bool`, *可选*, 默认为`True`) — 是否对给定输入的不同上下文文档的生成进行去重。如果在使用分布式后端进行训练时使用，必须将其设置为`False`。'
- en: '`exclude_bos_score` (`bool`, *optional*, defaults to `False`) — Whether or
    not to disregard the BOS token when computing the loss.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_bos_score` (`bool`, *可选*, 默认为`False`) — 在计算损失时是否忽略BOS标记。'
- en: '`output_retrieved(bool,` *optional*, defaults to `False`) — If set to `True`,
    `retrieved_doc_embeds`, `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`
    are returned. See returned tensors for more detail.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved(bool,` *可选*, 默认为`False`) — 如果设置为`True`，则返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。查看返回的张量以获取更多详细信息。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`forced_eos_token_id` (`int`, *optional*) — The id of the token to force as
    the last generated token when `max_length` is reached. Usually set to `eos_token_id`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forced_eos_token_id` (`int`, *可选*) — 当达到`max_length`时，要强制作为最后生成的标记的标记ID。通常设置为`eos_token_id`。'
- en: '[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)
    stores the configuration of a *RagModel*. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagConfig](/docs/transformers/v4.37.2/zh/model_doc/rag#transformers.RagConfig)
    存储了*RagModel*的配置。配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/zh/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自[PretrainedConfig](/docs/transformers/v4.37.2/zh/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。'
- en: '#### `from_question_encoder_generator_configs`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `from_question_encoder_generator_configs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L169)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L169)'
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Returns
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[EncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[EncoderDecoderConfig](/docs/transformers/v4.37.2/zh/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)'
- en: An instance of a configuration object
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象的一个实例
- en: Instantiate a [EncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)
    (or a derived class) from a pre-trained encoder model configuration and decoder
    model configuration.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练的编码器模型配置和解码器模型配置实例化一个[EncoderDecoderConfig](/docs/transformers/v4.37.2/zh/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)（或派生类）。
- en: RagTokenizer
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagTokenizer
- en: '### `class transformers.RagTokenizer`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/tokenization_rag.py#L28)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/tokenization_rag.py#L28)'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Rag specific outputs
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rag特定输出
- en: '### `class transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L38)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L38)'
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回） —
    语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边际化。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回）
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_heads,
    sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, 当*output_retrieved=True*时返回） — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, 当*output_retrieved=True*时返回） — 检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理得到的输入id。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — 模型问题编码器汇总输出的最后一层的隐藏状态序列。'
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器每一层的输出隐藏状态以及初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — 模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器每一层的隐藏状态以及初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: Base class for retriever augmented marginalized models outputs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检索器增强边际化模型输出的基类。
- en: '### `class transformers.models.rag.modeling_rag.RetrievAugLMOutput`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.rag.modeling_rag.RetrievAugLMOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L133)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L133)'
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边际化。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[torch.FloatTensor]`, *可选*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量的形状为`(2, batch_size, num_heads,
    sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs,
    hidden_size)`，*可选*，当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids` (`torch.LongTensor`，形状为`(batch_size, config.n_docs)`，*可选*，当*output_retrieved=True*时返回)
    — 检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回)
    — 从检索文档和检索器的问题编码器`input_ids`后处理得到的输入id。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs,
    config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索文档和检索器的问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size,
    sequence_length, hidden_size)`，*optional*) — 问题编码器最后一层的隐藏状态序列模型的汇聚输出。'
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: RagRetriever
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagRetriever
- en: '### `class transformers.RagRetriever`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagRetriever`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L337)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L337)'
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — The configuration of the RAG model this Retriever is used with. Contains parameters
    indicating which `Index` to build. You can load your own custom dataset with `config.index_name="custom"`
    or use a canonical one (default) from the datasets library with `config.index_name="wiki_dpr"`
    for example.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）—
    用于RAG模型的Retriever的配置。包含指示要构建哪个`Index`的参数。您可以使用`config.index_name="custom"`加载自己的自定义数据集，或者使用数据集库中的一个规范的数据集（默认）例如`config.index_name="wiki_dpr"`。'
- en: '`question_encoder_tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that was used to tokenize the question. It is used to decode the
    question and then use the generator_tokenizer.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    用于标记化问题的分词器。它用于解码问题，然后使用生成器的分词器。'
- en: '`generator_tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer used for the generator part of the RagModel.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)）—
    用于RagModel生成器部分的分词器。'
- en: '`index` (`Index`, optional, defaults to the one defined by the configuration)
    — If specified, use this index instead of the one built using the configuration'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index`（`Index`，可选，默认为配置中定义的索引）— 如果指定，则使用此索引，而不是使用配置构建的索引'
- en: Retriever used to get documents from vector queries. It retrieves the documents
    embeddings as well as the documents contents, and it formats them to be used with
    a RagModel.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 用于从向量查询获取文档的检索器。它检索文档嵌入以及文档内容，并将它们格式化以供RagModel使用。
- en: 'Examples:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#### `init_retrieval`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `init_retrieval`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L471)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L471)'
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Retriever initialization function. It loads the index into memory.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器初始化函数。将索引加载到内存中。
- en: '#### `postprocess_docs`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `postprocess_docs`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L479)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L479)'
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`docs` (`dict`) — Retrieved documents.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docs`（`dict`）— 检索到的文档。'
- en: '`input_strings` (`str`) — Input strings decoded by `preprocess_query`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_strings`（`str`）— 由`preprocess_query`解码的输入字符串。'
- en: '`prefix` (`str`) — Prefix added at the beginning of each input, typically used
    with T5-based models.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix`（`str`）— 添加到每个输入开头的前缀，通常与基于T5的模型一起使用。'
- en: Returns
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tuple(tensors)`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`tuple(tensors)`'
- en: 'a tuple consisting of two elements: contextualized `input_ids` and a compatible
    `attention_mask`.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含两个元素的元组：上下文化的`input_ids`和一个兼容的`attention_mask`。
- en: Postprocessing retrieved `docs` and combining them with `input_strings`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理检索到的`docs`并将它们与`input_strings`组合。
- en: '#### `retrieve`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `retrieve`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L551)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L551)'
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`question_hidden_states` (`np.ndarray` of shape `(batch_size, vector_size)`)
    — A batch of query vectors to retrieve with.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_hidden_states`（形状为`(batch_size, vector_size)`的`np.ndarray`）— 一批要检索的查询向量。'
- en: '`n_docs` (`int`) — The number of docs retrieved per query.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs`（`int`）— 每个查询检索的文档数。'
- en: Returns
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`Tuple[np.ndarray, np.ndarray, List[dict]]`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple[np.ndarray, np.ndarray, List[dict]]`'
- en: 'A tuple with the following objects:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含以下对象的元组：
- en: '`retrieved_doc_embeds` (`np.ndarray` of shape `(batch_size, n_docs, dim)`)
    — The retrieval embeddings of the retrieved docs per query.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds`（形状为`(batch_size, n_docs, dim)`的`np.ndarray`）— 检索到的文档的检索嵌入，每个查询一个。'
- en: '`doc_ids` (`np.ndarray` of shape `(batch_size, n_docs)`) — The ids of the documents
    in the index'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_ids`（形状为`(batch_size, n_docs)`的`np.ndarray`）— 索引中文档的id'
- en: '`doc_dicts` (`List[dict]`): The `retrieved_doc_embeds` examples per query.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_dicts`（`List[dict]`）：每个查询的`retrieved_doc_embeds`示例。'
- en: Retrieves documents for specified `question_hidden_states`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为指定的`question_hidden_states`检索文档。
- en: PytorchHide Pytorch content
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: RagModel
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagModel
- en: '### `class transformers.RagModel`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L489)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L489)'
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）—
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）—
    与`retriever`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）—
    用作RAG架构中生成器的seq2seq模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）—
    一个检索器类，封装了一个faiss索引，用于获取当前输入的上下文文档。'
- en: The [RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)
    forward method, overrides the `__call__` special method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传播的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默地忽略它们。
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个seq2seq模型，它包含两个核心组件：一个问题编码器和一个生成器。在前向传播过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将这些文档添加到输入中。这样的上下文化输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出结合在多个步骤中使用---请参阅示例以获取更多详细信息。该模型兼容任何*自编码*模型作为`question_encoder`，任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。
- en: '#### `forward`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L533)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L533)'
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`（batch_size，sequence_length）`的`torch.LongTensor`）—词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器标记器。使用该标记器类获取索引。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`（batch_size，sequence_length）`的`torch.Tensor`，*可选*）—避免对填充标记索引执行注意力的掩码。选择的掩码值在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1，
- en: 0 for tokens that are `masked`.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple（tuple（torch.FloatTensor）`，*可选*）—元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`（batch_size，n_docs
    * sequence_length，hidden_size）`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码过程中由（[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)）模型使用。
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`torch.LongTensor`，形状为`(batch_size, target_sequence_length)`，*可选*)
    — 为生成任务提供。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor`，形状为`(batch_size, target_sequence_length)`，*可选*)
    — 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — 元组包含两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）
    — 从检索文档和问题编码器`input_ids`后处理的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs,
    config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回） — 从检索文档和问题编码器`input_ids`后处理的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*可选*，默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`，*可选*，默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。'
- en: Returns
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入。
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores`（`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`）— 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）—
    长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads,
    sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds`（`torch.FloatTensor`，形状为`(batch_size, config.n_docs,
    hidden_size)`，*可选*，当*output_retrieved=True*时返回）— 由检索器检索的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids`（`torch.LongTensor`，形状为`(batch_size, config.n_docs)`，*可选*，当*output_retrieved=True*时返回）—
    检索器检索的嵌入文档的索引。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）—
    从检索的文档和问题编码器input_ids后处理得到的输入id。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（`torch.LongTensor`，形状为`(batch_size * config.n_docs,
    config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*）— 模型问题编码器输出的最后一层的隐藏状态序列。'
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层的输出隐藏状态加上初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state`（`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*可选*）— 模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层的输出隐藏状态加上初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层的隐藏状态加上初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: The [RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)
    forward method, overrides the `__call__` special method.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE12]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: RagSequenceForGeneration
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagSequenceForGeneration
- en: '### `class transformers.RagSequenceForGeneration`'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagSequenceForGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L730)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L730)'
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）-
    与`检索器`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）-
    在RAG架构中用作生成器的seq2seq模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`检索器`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）-
    一个封装了faiss索引的检索器类，用于获取当前输入的上下文文档。'
- en: The [RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: A RAG-sequence model implementation. It performs RAG-sequence specific marginalization
    in the forward pass.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: RAG-sequence模型实现。它在前向传递中执行RAG-sequence特定的边际化。
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个seq2seq模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递期间，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入之前。这样的上下文化输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自动编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，或与检索器的输出结合使用多步骤---查看更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L765)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L765)'
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类获取索引。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”掉的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“masked”掉的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）— 元组包括(`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`)。形状为`(batch_size,
    n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码期间由（[RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)）模型使用。
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）—
    用于生成任务。默认为`None`，根据您使用的RAG实例的生成模型的指示构建。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）—
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`）— 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和底层生成器的`past_key_values`。可用于加速解码。`past_key_values`在解码期间由（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型使用。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores`（形状为`(batch_size, config.n_docs)`的`torch.FloatTensor`）- 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）-
    从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）-
    从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*，默认为`True`）- 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（布尔值，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（布尔值，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量中的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved`（布尔值，*可选*）- 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要生成答案的文档数量。'
- en: '`exclude_bos_score` (`bool`, *optional*) — Only relevant if `labels` is passed.
    If `True`, the score of the BOS token is disregarded when computing the loss.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_bos_score`（布尔值，*可选*）- 仅在传递了`labels`时相关。如果为`True`，在计算损失时将忽略BOS标记的得分。'
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `torch.Tensor.sum` operation.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_loss`（`bool`，*可选*）- 仅在传递了`labels`时相关。如果为`True`，则使用`torch.Tensor.sum`操作减少NLL损失。'
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, any]`，可选，默认为*{}*）- 遗留字典，模型可以使用*generate()*函数。'
- en: Returns
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，这取决于配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）-
    语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores`（形状为`(batch_size, config.n_docs)`的`torch.FloatTensor`）- 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads,
    sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds`（形状为`(batch_size, config.n_docs, hidden_size)`的`torch.FloatTensor`，*可选*，当`output_retrieved=True`时返回）-
    检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids`（形状为`(batch_size, config.n_docs)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）-
    检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）-
    从检索到的文档和问题编码器输入id后处理得到的输入id。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当`output_retrieved=True`时返回）-
    从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    问题编码器最后一层的隐藏状态序列，模型的池化输出。'
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入输出，一个用于每一层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层的输出的隐藏状态加上初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的交叉注意力权重，在注意力softmax之后使用，用于计算交叉注意力头中的加权平均值。
- en: The [RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: 'Example:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE15]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#### `generate`'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L906)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L906)'
- en: '[PRE16]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩盖的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩盖的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 在*output_retrieved=True*时返回) — 从检索到的文档和问题编码器input_ids经过后处理得到的输入ID。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 在*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。'
- en: If the model is not initialized with a `retriever` or `input_ids` is not given,
    `context_input_ids` and `context_attention_mask` have to be provided to the forward
    pass. They are returned by `__call__()`.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未初始化为`retriever`或未给出`input_ids`，则必须在前向传递中提供`context_input_ids`和`context_attention_mask`。它们由`__call__()`返回。
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    每个检索到的文档嵌入（见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: If the model is not initialized with a `retriever` or `input_ids` is not given,
    `doc_scores` has to be provided to the forward pass. `doc_scores` are returned
    by `__call__()`.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未初始化为`retriever`或未给出`input_ids`，则必须在前向传递中提供`doc_scores`。`doc_scores`由`__call__()`返回。
- en: '`do_deduplication` (`bool`, *optional*) — Whether or not to deduplicate the
    generations from different context documents for a given input. Has to be set
    to `False` if used while training with distributed backend.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_deduplication` (`bool`, *optional*) — 是否对给定输入的不同上下文文档的生成进行去重。如果在使用分布式后端进行训练时使用，必须将其设置为`False`。'
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch. Note that this is not
    the value we pass to the `generator`’s `[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    function, where we set `num_return_sequences` to `num_beams`.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_return_sequences(int,` *optional*, 默认为1) — 每个批次元素的独立计算返回序列的数量。请注意，这不是我们传递给`generator`的`[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`函数的值，其中我们将`num_return_sequences`设置为`num_beams`。'
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *optional*, defaults to 1) — Beam search的beam数量。1表示没有beam
    search。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要生成答案的文档数量。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs will be passed
    to [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）- 额外的kwargs将传递给[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)。'
- en: Returns
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size * num_return_sequences, sequence_length)`的`torch.LongTensor`
- en: The generated sequences. The second dimension (sequence length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。
- en: Implements RAG sequence “thorough” decoding. Read the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    documentation for more information on how to set other generate input parameters.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 实现RAG序列“彻底”解码。阅读[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)文档，了解如何设置其他生成输入参数的更多信息。
- en: RagTokenForGeneration
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RagTokenForGeneration
- en: '### `class transformers.RagTokenForGeneration`'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.RagTokenForGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1128)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1128)'
- en: '[PRE17]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）-
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）-
    与`retriever`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)）-
    在RAG架构中用作生成器的seq2seq模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)）-
    封装了一个faiss索引的检索器类，用于获取当前输入的上下文文档。'
- en: The [RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)前向方法，覆盖`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: A RAG-token model implementation. It performs RAG-token specific marginalization
    in the forward pass.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: RAG-token模型实现。它在前向传递中执行RAG-token特定的边际化。
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个seq2seq模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递期间，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入中。这样的上下文化输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自动编码*模型，最好是[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出结合在多个步骤中使用---有关更多详细信息，请参见示例。该模型与任何*自动编码*模型兼容，作为`question_encoder`，以及任何带有语言模型头的*seq2seq*模型，作为`generator`。已经使用[DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)作为`question_encoder`，以及[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)或[T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)作为`generator`进行测试。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1234)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1234)'
- en: '[PRE18]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类来获取索引。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的蒙版。选择在`[0,
    1]`中的蒙版值：'
- en: 1 for tokens that are `not masked`,
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力蒙版？](../glossary#attention-mask)'
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`(batch_size,
    n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码期间，由([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))模型使用。
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）-
    用于生成任务。默认为`None`，根据您使用的生成器模型与您的RAG实例的指令构建。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）-
    默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果蒙版也将默认使用。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(torch.FloatTensor))`）- 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))模型中使用。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *可选*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的输入ID。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*可选*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（请参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`, *可选*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。'
- en: '`do_marginalize` (`bool`, *optional*) — If `True`, the logits are marginalized
    over all documents by making use of `torch.nn.functional.log_softmax`.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_marginalize` (`bool`, *可选*) — 如果为`True`，则通过使用`torch.nn.functional.log_softmax`对所有文档进行边际化。'
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `torch.Tensor.sum` operation.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_loss` (`bool`, *可选*) — 仅在传递了`labels`时相关。如果为`True`，则使用`torch.Tensor.sum`操作减少NLL损失。'
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, any]`, 可选，默认为*{}*) — 遗留字典，模型可以使用*generate()*函数。'
- en: Returns
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    或 `tuple(torch.FloatTensor)`'
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入而异的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能对每个词汇标记在所有文档上进行边际化。'
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。'
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[torch.FloatTensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`torch.FloatTensor`列表，每个张量形状为`(2, batch_size, num_heads,
    sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds`（形状为`(batch_size, config.n_docs, hidden_size)`的`torch.FloatTensor`，*可选*，当*output_retrieved=True*时返回）-
    检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids`（形状为`(batch_size, config.n_docs)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）-
    检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）-
    从检索到的文档和问题编码器输入id经过后处理得到的输入id。'
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`torch.LongTensor`，*可选*，当*output_retrieved=True*时返回）-
    从检索到的文档和问题编码器`input_ids`经过后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    问题编码器最后一层输出的隐藏状态序列，模型的汇总输出。'
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）-
    模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每一层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器每层输出的隐藏状态以及初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的交叉注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。
- en: The [RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE19]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#### `generate`'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1375)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1375)'
- en: '[PRE20]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。'
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) —
    用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩盖`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩盖`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`torch.LongTensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回)
    — 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`torch.LongTensor`，形状为`(batch_size * config.n_docs,
    config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`torch.FloatTensor`，形状为`(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`，*可选*，默认为`config.n_docs`) — 要检索的文档数量和/或要为其生成答案的文档数量。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    has the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config` (`~generation.GenerationConfig`, *optional*) — 用作生成调用的基本参数化的生成配置。传递给生成的`**kwargs`匹配`generation_config`的属性将覆盖它们。如果未提供`generation_config`，将使用默认配置，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以对生成进行参数化。'
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the previously
    generated tokens `inputs_ids` and the batch ID `batch_id`. This argument is useful
    for constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — 如果提供，此函数将每一步的beam搜索限制为仅允许的标记。如果未提供，则不应用约束。此函数接受2个参数`inputs_ids`和批次ID`batch_id`。它必须返回一个列表，其中包含下一代步的允许标记，条件是先前生成的标记`inputs_ids`和批次ID`batch_id`。此参数对于受前缀约束的生成很有用，如[自回归实体检索](https://arxiv.org/abs/2010.00904)中所述。'
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and a model’s
    config. If a logit processor is passed that is already created with the arguments
    or a model’s config an error is thrown.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor` (`LogitsProcessorList`, *optional*) — 自定义logits处理器，补充从参数和模型配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或模型配置创建，则会引发错误。'
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a model’s config. If a stopping criteria is passed that is already created with
    the arguments or a model’s config an error is thrown.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — 自定义停止标准，补充从参数和模型配置构建的默认停止标准。如果传递的停止标准已经使用参数或模型配置创建，则会引发错误。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *optional*) — `generate_config`的特定参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。'
- en: Returns
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(batch_size * num_return_sequences, sequence_length)`的`torch.LongTensor`
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。
- en: Implements RAG token decoding.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了RAG标记解码。
- en: TensorFlowHide TensorFlow content
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏了TensorFlow内容
- en: TFRagModel
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFRagModel
- en: '### `class transformers.TFRagModel`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFRagModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L496)'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L496)'
- en: '[PRE21]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 与`retriever`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 在RAG架构中用作生成器的seq2seq模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — 封装了用于获取当前输入上下文文档的faiss索引的检索器类。'
- en: The [TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)
    forward method, overrides the `__call__` special method.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个序列到序列模型，包含两个核心组件：一个问题编码器和一个生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将这些文档添加到输入之前。这样的上下文化输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自动编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成初始化，或者与检索器的输出结合在多个步骤中使用---请参阅更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，以及任何带有语言模型头的*seq2seq*模型作为`generator`。已经测试过使用[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)作为`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)作为`generator`。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型目前仅在急切模式下完全支持，并且可能无法以SavedModel格式导出。
- en: '#### `call`'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L547)'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L547)'
- en: '[PRE22]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`） — 词汇表中输入序列标记的索引。用于初始化模型的[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类获取索引。'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*） — 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0,
    1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被“掩码”（masked）的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被“掩码”（masked）的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs`（元组（元组（`tf.Tensor`）的形状，*可选*） — 元组由（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）组成。形状为`(batch_size,
    n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码过程中由（[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)）模型使用。
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`tf.Tensor`，*可选*）
    — 用于生成任务。默认为`None`，根据您正在使用的RAG实例的生成器模型的说明进行构造。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.BoolTensor`，*可选*）-
    默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`tuple(tuple(tf.Tensor))`）- 元组包含两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。'
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores`（形状为`(batch_size, config.n_docs)`的`tf.Tensor`）- 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）-
    从检索文档和问题编码器`input_ids`后处理的输入ID。'
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。`context_attention_mask`（形状为`(batch_size
    * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）：从检索文档和问题编码器`input_ids`后处理的注意力掩码。
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache`（`bool`，*可选*，默认为`True`）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved`（`bool`，*可选*）- 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）- 是否返回`TFRetrievAugLMOutput`而不是普通元组。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要为其生成答案的文档数量。'
- en: Returns
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` or `tuple(tf.Tensor)`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` or a tuple
    of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入而异的各种元素。
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）-
    语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。'
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values`（`List[tf.Tensor]`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）-
    长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含解码器的预计算隐藏状态（注意力块中的键和值），可用于加速顺序解码。
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds` (`tf.Tensor`，形状为`(batch_size, config.n_docs, hidden_size)`，*optional*，当*output_retrieved=True*时返回)
    — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`tf.Tensor` of shape `(batch_size, config.n_docs)`, *optional*,
    returned when *output_retrieved=True*) — The indexes of the embedded documents
    retrieved by the retriever.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`，*optional*，当*output_retrieved=True*时返回)
    — 检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input ids post-processed
    from the retrieved documents and the question encoder input_ids by the retriever.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*optional*，当*output_retrieved=True*时返回)
    — 从检索到的文档和问题编码器输入ids后处理得到的输入ids。'
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*optional*，当*output_retrieved=True*时返回)
    — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 问题编码器池化输出模型最后一层的隐藏状态序列。'
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length,
    hidden_size)`，*optional*) — 模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每个层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层输出的隐藏状态以及初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions` (`tuple(tf.Tensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)
    forward method, overrides the `__call__` special method.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE23]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TFRagSequenceForGeneration
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFRagSequenceForGeneration
- en: '### `class transformers.TFRagSequenceForGeneration`'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFRagSequenceForGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1313)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1313)'
- en: '[PRE24]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 与`retriever`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — 用作RAG架构中生成器的序列到序列模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — 一个检索器类，封装了一个faiss索引，用于获取当前输入的上下文文档。'
- en: The [TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: A TF RAG-sequence model implementation. It performs RAG-sequence specific marginalization
    in the forward pass.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: TF RAG-sequence模型实现。它在前向传递中执行RAG-sequence特定的边际化。
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个序列到序列模型，封装了两个核心组件：一个问题编码器和一个生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入中。这样的上下文化输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自动编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*序列到序列*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行端到端生成，也可以与检索器的输出组合在多个步骤中使用---查看更多详细信息的示例。该模型兼容任何*自动编码*模型作为`question_encoder`，兼容任何带有语言模型头的*序列到序列*模型作为`generator`。已经测试过使用[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)作为`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)作为`generator`。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型目前处于开发状态，现在仅完全支持即时模式，并且可能无法以SavedModel格式导出。
- en: '#### `call`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1366)'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1366)'
- en: '[PRE25]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor`的形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定兼容的生成器分词器。使用该分词器类获取这些索引。'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`tf.Tensor`的形状为`(batch_size, sequence_length)`，*可选*) — 避免对填充标记索引执行注意力的掩码。选择在`[0,
    1]`中的掩码值:'
- en: 1 for tokens that are `not masked`,
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码?](../glossary#attention-mask)'
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *可选*) — 元组包括(`generator_enc_last_hidden_state`,
    *可选*: `generator_enc_hidden_states`, *可选*: `generator_enc_attentions`)。形状为`(batch_size,
    n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码期间，由([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))模型使用。
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`tf.Tensor`的形状为`(batch_size, target_sequence_length)`，*可选*)
    — 用于生成任务。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor`的形状为`(batch_size, target_sequence_length)`，*可选*)
    — 默认行为: 生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — 元组包括RAG模型的`encoder_outputs`（参见`encoder_outputs`）和底层生成器的`past_key_values`两个元素。可用于加速解码。在解码期间，`past_key_values`在([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))模型中使用。'
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor`的形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未初始化为`retriever`，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，详细信息请参见示例。'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`tf.Tensor`的形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回)
    — 从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。'
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如果模型未初始化为`retriever`，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。context_attention_mask
    (`tf.Tensor`的形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回):
    从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, 默认为`True`) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved(bool,` *optional*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回`TFRetrievAugLMOutput`而不是普通元组。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`, *optional*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。'
- en: '`exclude_bos_score` (`bool`, *optional*) — Only relevant if `labels` is passed.
    If `True`, the score of the BOS token is disregarded when computing the loss.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exclude_bos_score` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则在计算损失时忽略BOS标记的分数。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the cross entropy classification loss according
    to Rag-Sequence model formulation See [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)
    Section 2.1 for details about Rag-Sequence formulation. Indices should be in `[0,
    ..., config.vocab_size - 1]`.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — 根据Rag-Sequence模型公式计算交叉熵分类损失的标签。有关Rag-Sequence公式的详细信息，请参见[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)第2.1节。索引应在`[0,
    ..., config.vocab_size - 1]`范围内。'
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `tf.Tensor.sum` operation.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_loss` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则使用`tf.Tensor.sum`操作减少NLL损失。'
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, any]`, optional, 默认为*{}*) — 遗留字典，模型可以使用*generate()*函数所需。'
- en: Returns
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or `tuple(tf.Tensor)`'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or a
    tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Language modeling loss.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。'
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2,
    batch_size, num_heads, sequence_length, embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '包含预先计算的隐藏状态（解码器中的键和值在注意力块中）的`tf.Tensor`，可用于加速顺序解码（请参见`past_key_values`输入）。 '
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — 检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的分数。'
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — 由检索器检索的嵌入文档的索引。'
- en: '`context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（`tf.Tensor`（int32）形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）—
    从检索到的文档和问题编码器input_ids后处理得到的输入id。'
- en: '`context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（`tf.Tensor`（int32）形状为`(batch_size * config.n_docs,
    config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回）— 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）—
    问题编码器最后一层的隐藏状态序列模型的池化输出。'
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层的输出加上初始嵌入输出的隐藏状态。
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）—
    模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层的输出加上初始嵌入输出的隐藏状态。
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层的输出加上初始嵌入输出的隐藏状态。
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE26]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#### `generate`'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1601)'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1601)'
- en: '[PRE27]'
  id: totrans-565
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`: - 1 for tokens that are **not masked**, - 0 for tokens that are **masked**.
    [What are attention masks?](../glossary#attention-mask)'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`范围内：- 1表示**未屏蔽**的标记，- 0表示**已屏蔽**的标记。[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder input_ids by the retriever.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回)
    — 从检索文档和问题编码器input_ids后处理得到的输入ID。'
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` or `input_ids` is not given, `context_input_ids` and `context_attention_mask`
    have to be provided to the forward pass. They are returned by `__call__()`.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，当*output_retrieved=True*时返回)
    — 从检索文档和问题编码器input_ids后处理得到的注意力掩码。如果模型未使用`retriever`初始化或未提供`input_ids`，则必须在前向传递中提供`context_input_ids`和`context_attention_mask`。它们由`__call__()`返回。'
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` or `input_ids` is not given,
    `doc_scores` has to be provided to the forward pass. `doc_scores` are returned
    by `__call__()`.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化或未提供`input_ids`，则必须在前向传递中提供`doc_scores`。`doc_scores`由`__call__()`返回。'
- en: '`do_deduplication` (`bool`, *optional*) — Whether or not to deduplicate the
    generations from different context documents for a given input. Has to be set
    to `False` if used while training with distributed backend.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_deduplication` (`bool`, *可选*) — 是否对给定输入的不同上下文文档生成进行去重。如果在使用分布式后端进行训练时，必须将其设置为`False`。'
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch. Note that this is not
    the value we pass to the `generator`’s `[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    function, where we set `num_return_sequences` to `num_beams`.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_return_sequences(int,` *可选*，默认为1) — 每个批次元素的独立计算返回序列的数量。请注意，这不是我们传递给`generator`的`[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`函数的值，其中我们将`num_return_sequences`设置为`num_beams`。'
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_beams` (`int`, *可选*, 默认为1) — Beam搜索的数量。1表示没有beam搜索。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`，*可选*，默认为`config.n_docs`) — 要检索的文档数量和/或要为其生成答案的文档数量。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs will be passed
    to [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`，*可选*) — 额外的kwargs将传递给[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)'
- en: Returns
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Tensor`，形状为`(batch_size * num_return_sequences, sequence_length)`'
- en: The generated sequences. The second dimension (sequence length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的序列。第二维（序列长度）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。
- en: Implements RAG sequence “thorough” decoding. Read the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    documentation for more information on how to set other generate input parameters
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了RAG序列“彻底”解码。阅读[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)文档以获取有关如何设置其他生成输入参数的更多信息
- en: TFRagTokenForGeneration
  id: totrans-581
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TFRagTokenForGeneration
- en: '### `class transformers.TFRagTokenForGeneration`'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TFRagTokenForGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L733)'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L733)'
- en: '[PRE28]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder`（[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel））-
    与`retriever`封装的faiss索引兼容的编码器模型。'
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator`（[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel））-
    在RAG架构中用作生成器的seq2seq模型。'
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retriever`（[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever））-
    封装了一个faiss索引的检索器类，用于获取当前输入的上下文文档。'
- en: The [TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: A TF RAG-token model implementation. It performs RAG-token specific marginalization
    in the forward pass.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: TF RAG-token模型实现。它在前向传递中执行RAG-token特定的边际化。
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个序列到序列模型，封装了两个核心组件：问题编码器和生成器。在前向传递过程中，我们使用问题编码器对输入进行编码，并将其传递给检索器以提取相关的上下文文档。然后将文档添加到输入之前。这样上下文化的输入被传递给生成器。
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编码器可以是任何*自编码*模型，最好是[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)，生成器可以是任何*seq2seq*模型，最好是[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)。
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以使用[RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)进行初始化以进行端到端生成，或者与检索器的输出组合在多个步骤中使用---请参阅示例以获取更多详细信息。该模型与*自编码*模型兼容，如`question_encoder`，以及具有语言模型头部的*seq2seq*模型，如`generator`。已经测试了将[TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)用作`question_encoder`和[TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)用作`generator`。
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF
    2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型目前处于开发状态，因为它现在仅在急切模式下完全支持，并且可能无法以SavedModel格式导出。
- en: '#### `call`'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `call`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L852)'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L852)'
- en: '[PRE29]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`）- 词汇表中输入序列标记的索引。[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)用于初始化模型，指定要使用的生成器，还指定了兼容的生成器分词器。使用该分词器类来获取这些索引。'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0,
    1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1。
- en: 0 for tokens that are `masked`.
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`，*可选*) — 元组包括（`generator_enc_last_hidden_state`，*可选*：`generator_enc_hidden_states`，*可选*：`generator_enc_attentions`）。形状为`(batch_size,
    n_docs * sequence_length, hidden_size)`的`generator_enc_last_hidden_state`是生成器编码器最后一层的隐藏状态序列。'
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码期间由（[TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)）模型使用。
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_input_ids` (`tf.Tensor`，形状为`(batch_size, target_sequence_length)`，*可选*)
    — 用于生成任务。默认为`None`，根据您使用的RAG实例的生成器模型的说明构建。'
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_attention_mask` (`torch.BoolTensor`，形状为`(batch_size, target_sequence_length)`，*可选*)
    — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。默认情况下还将使用因果掩码。'
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — 元组包括两个元素：RAG模型的`encoder_outputs`（参见`encoder_outputs`）和基础生成器的`past_key_values`。可用于加速解码。在解码期间，`past_key_values`在（[RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)）模型中使用。'
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor`，形状为`(batch_size, config.n_docs)`) — 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。如果模型未使用`retriever`初始化，则必须在前向传递中提供`doc_scores`。`doc_scores`可以通过`question_encoder_last_hidden_state`和`retrieved_doc_embeds`计算，有关更多信息，请参见示例。'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`tf.Tensor`，形状为`(batch_size * config.n_docs, config.max_combined_length)`，*可选*，在*output_retrieved=True*时返回）
    — 从检索文档和问题编码器`input_ids`后处理的输入ID。'
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-615
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。`context_attention_mask`（形状为`(batch_size
    * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，在*output_retrieved=True*时返回）：从检索文档和问题编码器`input_ids`后处理的注意力掩码。
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  id: totrans-616
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`初始化，则必须在前向传递中提供`context_attention_mask`。`context_attention_mask`由`__call__()`返回。
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`，*可选*，默认为`True`) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量中的`hidden_states`。'
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_retrieved(bool,` *可选*) — 是否返回`retrieved_doc_embeds`、`retrieved_doc_ids`、`context_input_ids`和`context_attention_mask`。有关更多详细信息，请参见返回的张量。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回`TFRetrievAugLMOutput`而不是普通元组。'
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs` (`int`, *optional*, 默认为`config.n_docs“) — 要检索的文档数量和/或要生成答案的文档数量。'
- en: '`do_marginalize` (`bool`, *optional*) — If `True`, the logits are marginalized
    over all documents by making use of `torch.nn.functional.log_softmax`.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_marginalize` (`bool`，*可选*) — 如果为`True`，通过使用`torch.nn.functional.log_softmax`将对数归一化到所有文档上。'
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the cross entropy classification loss according
    to Rag-Token model formulation See [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)
    Section 2.1 for details about Rag-Token formulation. Indices should be in `[0,
    ..., config.vocab_size - 1]`.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — 根据Rag-Token模型公式计算交叉熵分类损失的标签。有关Rag-Token公式的详细信息，请参阅[https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)第2.1节。索引应在`[0,
    ..., config.vocab_size - 1]`范围内。'
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `tf.Tensor.sum` operation.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce_loss` (`bool`, *optional*) — 仅在传递`labels`时相关。如果为`True`，则使用`tf.Tensor.sum`操作减少NLL损失。'
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, any]`, optional, 默认为*{}*) — 旧字典，模型可以使用*generate()*函数所需。'
- en: Returns
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or `tuple(tf.Tensor)`'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或`tuple(tf.Tensor)`'
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or a
    tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)）和输入的各种元素。
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Language modeling loss.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失。'
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数。该分数可能针对每个词汇标记在所有文档上进行边缘化。'
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回)
    — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`。'
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含可以用于加速顺序解码的解码器的预计算隐藏状态（注意块中的键和值）（参见`past_key_values`输入）。
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — 每个检索到的文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, 当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档。与`question_encoder_last_hidden_state`一起用于计算`doc_scores`。'
- en: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, 当*output_retrieved=True*时返回) — 检索器检索到的嵌入文档的索引。'
- en: '`context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器输入ids后处理得到的输入ids。'
- en: '`context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, 当*output_retrieved=True*时返回) — 从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — 模型问题编码器输出的最后一层的隐藏状态序列。'
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）—
    模型生成器编码器最后一层的隐藏状态序列。'
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-646
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器在每一层的隐藏状态加上初始嵌入输出。
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_enc_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-648
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）—
    形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，一个用于每一层的输出）。'
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  id: totrans-650
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器在每一层的隐藏状态加上初始嵌入输出。
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator_dec_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。'
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成器解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '[TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE30]'
  id: totrans-656
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '#### `generate`'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `generate`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1007)'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1007)'
- en: '[PRE31]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用作生成提示的序列。如果未传递`input_ids`，则必须提供`context_input_ids`。'
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0,
    1]`中：'
- en: 1 for tokens that are `not masked`,
  id: totrans-663
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被`masked`的标记，值为1，
- en: 0 for tokens that are `masked`.
  id: totrans-664
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被`masked`的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_input_ids`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，当*output_retrieved=True*时返回）—
    从检索到的文档和问题编码器`input_ids`后处理得到的输入ID。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_attention_mask`（形状为`(batch_size * config.n_docs, config.max_combined_length)`的`tf.Tensor`，*可选*，当*output_retrieved=True*时返回）—
    从检索到的文档和问题编码器`input_ids`后处理得到的注意力掩码。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-669
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`进行初始化，则必须在前向传递中提供`context_input_ids`。`context_input_ids`由`__call__()`返回。
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doc_scores`（形状为`(batch_size, config.n_docs)`的`tf.Tensor`）- 每个检索文档嵌入（参见`retrieved_doc_embeds`）与`question_encoder_last_hidden_state`之间的得分。'
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  id: totrans-671
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型未使用`retriever`初始化，则必须提供`context_input_ids`进行前向传递。`context_input_ids`由`__call__()`返回。
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_docs`（`int`，*可选*，默认为`config.n_docs`）- 要检索的文档数量和/或要为其生成答案的文档数量。'
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generation_config`（`~generation.GenerationConfig`，*可选*）- 用作生成调用的基本参数化的生成配置。传递给生成匹配`generation_config`属性的`**kwargs`将覆盖它们。如果未提供`generation_config`，将使用默认值，其加载优先级如下：1）从`generation_config.json`模型文件中，如果存在；2）从模型配置中。请注意，未指定的参数将继承[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)的默认值，应检查其文档以参数化生成。'
- en: '`logits_processor` (`TFLogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and a model’s
    config. If a logit processor is passed that is already created with the arguments
    or a model’s config an error is thrown.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_processor`（`TFLogitsProcessorList`，*可选*）- 自定义logits处理器，补充从参数和模型配置构建的默认logits处理器。如果传递的logit处理器已经使用参数或模型配置创建，则会抛出错误。'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`（`Dict[str, Any]`，*可选*）- `generate_config`的特定于模型的参数化和/或将转发到模型的`forward`函数的其他模型特定kwargs。'
- en: Returns
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tf.Tensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Tensor`的形状为`(batch_size * num_return_sequences, sequence_length)`'
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的序列。第二维（sequence_length）要么等于`max_length`，要么如果所有批次由于`eos_token_id`而提前完成，则要短。
- en: Implements TFRAG token decoding.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 实现TFRAG令牌解码。
