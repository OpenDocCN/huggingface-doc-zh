- en: RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/rag)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/228.5db0cd20.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">[![Models](../Images/8d827715f6e5e46ae48b9169a2927210.png)](https://huggingface.co/models?filter=rag)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (“RAG”) models combine the powers of pretrained
    dense retrieval (DPR) and sequence-to-sequence models. RAG models retrieve documents,
    pass them to a seq2seq model, then marginalize to generate outputs. The retriever
    and seq2seq modules are initialized from pretrained models, and fine-tuned jointly,
    allowing both retrieval and generation to adapt to downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara
    Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike
    Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large pre-trained language models have been shown to store factual knowledge
    in their parameters, and achieve state-of-the-art results when fine-tuned on downstream
    NLP tasks. However, their ability to access and precisely manipulate knowledge
    is still limited, and hence on knowledge-intensive tasks, their performance lags
    behind task-specific architectures. Additionally, providing provenance for their
    decisions and updating their world knowledge remain open research problems. Pre-trained
    models with a differentiable access mechanism to explicit nonparametric memory
    can overcome this issue, but have so far been only investigated for extractive
    downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented
    generation (RAG) — models which combine pre-trained parametric and non-parametric
    memory for language generation. We introduce RAG models where the parametric memory
    is a pre-trained seq2seq model and the non-parametric memory is a dense vector
    index of Wikipedia, accessed with a pre-trained neural retriever. We compare two
    RAG formulations, one which conditions on the same retrieved passages across the
    whole generated sequence, the other can use different passages per token. We fine-tune
    and evaluate our models on a wide range of knowledge-intensive NLP tasks and set
    the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq
    models and task-specific retrieve-and-extract architectures. For language generation
    tasks, we find that RAG models generate more specific, diverse and factual language
    than a state-of-the-art parametric-only seq2seq baseline.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [ola13](https://huggingface.co/ola13).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval-augmented generation (“RAG”) models combine the powers of pretrained
    dense retrieval (DPR) and Seq2Seq models. RAG models retrieve docs, pass them
    to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq
    modules are initialized from pretrained models, and fine-tuned jointly, allowing
    both retrieval and generation to adapt to downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: RagConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`title_sep` (`str`, *optional*, defaults to `" / "`) — Separator inserted between
    the title and the text of the retrieved document when calling [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_sep` (`str`, *optional*, defaults to `" // "`) — Separator inserted between
    the text of the retrieved document and the original input when calling [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to 5) — Number of documents to retrieve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_combined_length` (`int`, *optional*, defaults to 300) — Max length of
    contextualized input returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieval_vector_size` (`int`, *optional*, defaults to 768) — Dimensionality
    of the document embeddings indexed by [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieval_batch_size` (`int`, *optional*, defaults to 8) — Retrieval batch
    size, defined as the number of queries issues concurrently to the faiss index
    encapsulated [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset` (`str`, *optional*, defaults to `"wiki_dpr"`) — A dataset identifier
    of the indexed dataset in HuggingFace Datasets (list all available datasets and
    ids using `datasets.list_datasets()`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_split` (`str`, *optional*, defaults to `"train"`) — Which split of
    the `dataset` to load.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name` (`str`, *optional*, defaults to `"compressed"`) — The index name
    of the index associated with the `dataset`. One can choose between `"legacy"`,
    `"exact"` and `"compressed"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_path` (`str`, *optional*) — The path to the serialized faiss index on
    disk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`passages_path` (`str`, *optional*) — A path to text passages compatible with
    the faiss index. Required if using `LegacyIndex`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_dummy_dataset` (`bool`, *optional*, defaults to `False`) — Whether to
    load a “dummy” variant of the dataset specified by `dataset`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`label_smoothing` (`float`, *optional*, defaults to 0.0) — Only relevant if
    `return_loss` is set to `True`. Controls the `epsilon` parameter value for label
    smoothing in the loss calculation. If set to 0, no label smoothing is performed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_marginalize` (`bool`, *optional*, defaults to `False`) — If `True`, the
    logits are marginalized over all documents by making use of `torch.nn.functional.log_softmax`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce_loss` (`bool`, *optional*, defaults to `False`) — Whether or not to
    reduce the NLL loss using the `torch.Tensor.sum` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_deduplication` (`bool`, *optional*, defaults to `True`) — Whether or not
    to deduplicate the generations from different context documents for a given input.
    Has to be set to `False` if used while training with distributed backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_bos_score` (`bool`, *optional*, defaults to `False`) — Whether or
    not to disregard the BOS token when computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*, defaults to `False`) — If set to `True`,
    `retrieved_doc_embeds`, `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`
    are returned. See returned tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forced_eos_token_id` (`int`, *optional*) — The id of the token to force as
    the last generated token when `max_length` is reached. Usually set to `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig)
    stores the configuration of a *RagModel*. Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_question_encoder_generator_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/configuration_rag.py#L169)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[EncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [EncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig)
    (or a derived class) from a pre-trained encoder model configuration and decoder
    model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: RagTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/tokenization_rag.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Rag specific outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Base class for retriever augmented marginalized models outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.models.rag.modeling_rag.RetrievAugLMOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L133)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RagRetriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagRetriever`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L337)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — The configuration of the RAG model this Retriever is used with. Contains parameters
    indicating which `Index` to build. You can load your own custom dataset with `config.index_name="custom"`
    or use a canonical one (default) from the datasets library with `config.index_name="wiki_dpr"`
    for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer that was used to tokenize the question. It is used to decode the
    question and then use the generator_tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer))
    — The tokenizer used for the generator part of the RagModel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index` (`Index`, optional, defaults to the one defined by the configuration)
    — If specified, use this index instead of the one built using the configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retriever used to get documents from vector queries. It retrieves the documents
    embeddings as well as the documents contents, and it formats them to be used with
    a RagModel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#### `init_retrieval`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L471)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Retriever initialization function. It loads the index into memory.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `postprocess_docs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L479)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`docs` (`dict`) — Retrieved documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_strings` (`str`) — Input strings decoded by `preprocess_query`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix` (`str`) — Prefix added at the beginning of each input, typically used
    with T5-based models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tuple(tensors)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'a tuple consisting of two elements: contextualized `input_ids` and a compatible
    `attention_mask`.'
  prefs: []
  type: TYPE_NORMAL
- en: Postprocessing retrieved `docs` and combining them with `input_strings`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `retrieve`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/retrieval_rag.py#L551)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`question_hidden_states` (`np.ndarray` of shape `(batch_size, vector_size)`)
    — A batch of query vectors to retrieve with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`) — The number of docs retrieved per query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`Tuple[np.ndarray, np.ndarray, List[dict]]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A tuple with the following objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`np.ndarray` of shape `(batch_size, n_docs, dim)`)
    — The retrieval embeddings of the retrieved docs per query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_ids` (`np.ndarray` of shape `(batch_size, n_docs)`) — The ids of the documents
    in the index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_dicts` (`List[dict]`): The `retrieved_doc_embeds` examples per query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieves documents for specified `question_hidden_states`.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: RagModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L489)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L533)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: RagSequenceForGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagSequenceForGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L730)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: A RAG-sequence model implementation. It performs RAG-sequence specific marginalization
    in the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L765)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_bos_score` (`bool`, *optional*) — Only relevant if `labels` is passed.
    If `True`, the score of the BOS token is disregarded when computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `torch.Tensor.sum` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [RagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L906)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is not initialized with a `retriever` or `input_ids` is not given,
    `context_input_ids` and `context_attention_mask` have to be provided to the forward
    pass. They are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model is not initialized with a `retriever` or `input_ids` is not given,
    `doc_scores` has to be provided to the forward pass. `doc_scores` are returned
    by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`do_deduplication` (`bool`, *optional*) — Whether or not to deduplicate the
    generations from different context documents for a given input. Has to be set
    to `False` if used while training with distributed backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch. Note that this is not
    the value we pass to the `generator`’s `[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    function, where we set `num_return_sequences` to `num_beams`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs will be passed
    to [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated sequences. The second dimension (sequence length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  prefs: []
  type: TYPE_NORMAL
- en: Implements RAG sequence “thorough” decoding. Read the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    documentation for more information on how to set other generate input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: RagTokenForGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.RagTokenForGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1128)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: A RAG-token model implementation. It performs RAG-token specific marginalization
    in the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a seq2seq model which encapsulates two core components: a question encoder
    and a generator. During a forward pass, we encode the input with the question
    encoder and pass it to the retriever to extract relevant context documents. The
    documents are then prepended to the input. Such contextualized inputs is passed
    to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [DPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.DPRQuestionEncoder)
    as the `question_encoder` and [BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)
    or [T5ForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/t5#transformers.T5ForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1234)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([RagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`) — Tuple consists of two
    elements: `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`. If the model has is not initialized
    with a `retriever` `doc_scores` has to be provided to the forward pass. `doc_scores`
    can be computed via `question_encoder_last_hidden_state` and `retrieved_doc_embeds`,
    see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever. If the model was not initialized with a `retriever`
    ``context_input_ids` has to be provided to the forward pass. `context_input_ids`
    are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`,*optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` `context_attention_mask` has to be provided to the forward pass.
    `context_attention_mask` are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_marginalize` (`bool`, *optional*) — If `True`, the logits are marginalized
    over all documents by making use of `torch.nn.functional.log_softmax`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `torch.Tensor.sum` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[torch.FloatTensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `torch.FloatTensor` of length
    `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length,
    embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`torch.FloatTensor` of shape `(batch_size, config.n_docs,
    hidden_size)`, *optional*, returned when *output_retrieved=True*) — Embedded documents
    retrieved by the retriever. Is used with `question_encoder_last_hidden_state`
    to compute the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`torch.LongTensor` of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden states at the
    output of the last layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size,
    sequence_length, hidden_size)`, *optional*) — Sequence of hidden-states at the
    output of the last layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_attentions=True` is passed or when `config.output_attentions=True`)
    — Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-attentions weights of the generator decoder, after the attention softmax,
    used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_rag.py#L1375)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`context_input_ids` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input IDs post-processed from the retrieved documents and the question encoder
    `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`torch.LongTensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`) —
    Score between each retrieved document embeddings (see `retrieved_doc_embeds`)
    and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    has the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`, *optional*)
    — If provided, this function constraints the beam search to allowed tokens only
    at each step. If not provided no constraint is applied. This function takes 2
    arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with
    the allowed tokens for the next generation step conditioned on the previously
    generated tokens `inputs_ids` and the batch ID `batch_id`. This argument is useful
    for constrained generation conditioned on the prefix, as described in [Autoregressive
    Entity Retrieval](https://arxiv.org/abs/2010.00904).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`LogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and a model’s
    config. If a logit processor is passed that is already created with the arguments
    or a model’s config an error is thrown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_criteria` (`StoppingCriteriaList`, *optional*) — Custom stopping
    criteria that complement the default stopping criteria built from arguments and
    a model’s config. If a stopping criteria is passed that is already created with
    the arguments or a model’s config an error is thrown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  prefs: []
  type: TYPE_NORMAL
- en: Implements RAG token decoding.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFRagModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFRagModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L496)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L547)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput` or a tuple
    of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`tf.Tensor` of shape `(batch_size, config.n_docs)`, *optional*,
    returned when *output_retrieved=True*) — The indexes of the embedded documents
    retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input ids post-processed
    from the retrieved documents and the question encoder input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: TFRagSequenceForGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFRagSequenceForGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1313)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: A TF RAG-sequence model implementation. It performs RAG-sequence specific marginalization
    in the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1366)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`exclude_bos_score` (`bool`, *optional*) — Only relevant if `labels` is passed.
    If `True`, the score of the BOS token is disregarded when computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the cross entropy classification loss according
    to Rag-Sequence model formulation See [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)
    Section 2.1 for details about Rag-Sequence formulation. Indices should be in `[0,
    ..., config.vocab_size - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `tf.Tensor.sum` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or a
    tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFRagSequenceForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagSequenceForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1601)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`: - 1 for tokens that are **not masked**, - 0 for tokens that are **masked**.
    [What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever. If the model has is not initialized with
    a `retriever` or `input_ids` is not given, `context_input_ids` and `context_attention_mask`
    have to be provided to the forward pass. They are returned by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` or `input_ids` is not given,
    `doc_scores` has to be provided to the forward pass. `doc_scores` are returned
    by `__call__()`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_deduplication` (`bool`, *optional*) — Whether or not to deduplicate the
    generations from different context documents for a given input. Has to be set
    to `False` if used while training with distributed backend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_return_sequences(int,` *optional*, defaults to 1) — The number of independently
    computed returned sequences for each element in the batch. Note that this is not
    the value we pass to the `generator`’s `[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    function, where we set `num_return_sequences` to `num_beams`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`, *optional*, defaults to 1) — Number of beams for beam search.
    1 means no beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs will be passed
    to [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated sequences. The second dimension (sequence length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  prefs: []
  type: TYPE_NORMAL
- en: Implements RAG sequence “thorough” decoding. Read the [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)`
    documentation for more information on how to set other generate input parameters
  prefs: []
  type: TYPE_NORMAL
- en: TFRagTokenForGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFRagTokenForGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L733)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — An encoder model compatible with the faiss index encapsulated by the `retriever`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` ([TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel))
    — A seq2seq model used as the generator in the RAG architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retriever` ([RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever))
    — A retriever class encapsulating a faiss index queried to obtain context documents
    for current inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: A TF RAG-token model implementation. It performs RAG-token specific marginalization
    in the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is a sequence-to-sequence model which encapsulates two core components:
    a question encoder and a generator. During a forward pass, we encode the input
    with the question encoder and pass it to the retriever to extract relevant context
    documents. The documents are then prepended to the input. Such contextualized
    inputs is passed to the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: The question encoder can be any *autoencoding* model, preferably [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder),
    and the generator can be any *seq2seq* model, preferably [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration).
  prefs: []
  type: TYPE_NORMAL
- en: The model can be initialized with a [RagRetriever](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagRetriever)
    for end-to-end generation or used in combination with the outputs of a retriever
    in multiple steps---see examples for more details. The model is compatible any
    *autoencoding* model as the `question_encoder` and any *seq2seq* model with language
    model head as the `generator`. It has been tested with [TFDPRQuestionEncoder](/docs/transformers/v4.37.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder)
    as the `question_encoder` and [TFBartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.TFBartForConditionalGeneration)
    as the `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a Tensorflow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The model is in a developing state as it is now fully supports in eager-mode
    only, and may not be exported in SavedModel format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L852)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary. [RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig),
    used to initialize the model, specifies which generator to use, it also specifies
    a compatible generator tokenizer. Use that tokenizer class to obtain the indices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(tf.Tensor)`, *optional*) — Tuple consists of
    (`generator_enc_last_hidden_state`, *optional*: `generator_enc_hidden_states`,
    *optional*: `generator_enc_attentions`). `generator_enc_last_hidden_state` of
    shape `(batch_size, n_docs * sequence_length, hidden_size)` is a sequence of hidden-states
    at the output of the last layer of the generator’s encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Used by the ([TFRagModel](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagModel))
    model during decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Provide for generation tasks. `None` by default, construct as per
    instructions for the generator model you’re using with your RAG instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(tf.Tensor))`) — Tuple consists of two elements:
    `encoder_outputs` of the RAG model (see `encoder_outputs`) and `past_key_values`
    of the underlying generator. Can be used to speed up decoding. `past_key_values`
    are used in the ([RagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagTokenForGeneration))
    model during decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.
    If the model has is not initialized with a `retriever` `doc_scores` has to be
    provided to the forward pass. `doc_scores` can be computed via `question_encoder_last_hidden_state`
    and `retrieved_doc_embeds`, see examples for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the model has is not initialized with a `retriever` ``context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
    context_attention_mask (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*): Attention mask post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever` `context_attention_mask`
    has to be provided to the forward pass. `context_attention_mask` are returned
    by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — If set to `True`, `past_key_values`
    key value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_retrieved(bool,` *optional*) — Whether or not to return the `retrieved_doc_embeds`,
    `retrieved_doc_ids`, `context_input_ids` and `context_attention_mask`. See returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `TFRetrievAugLMOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs“) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_marginalize` (`bool`, *optional*) — If `True`, the logits are marginalized
    over all documents by making use of `torch.nn.functional.log_softmax`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the cross entropy classification loss according
    to Rag-Token model formulation See [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)
    Section 2.1 for details about Rag-Token formulation. Indices should be in `[0,
    ..., config.vocab_size - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reduce_loss` (`bool`, *optional*) — Only relevant if `labels` is passed. If
    `True`, the NLL loss is reduced using the `tf.Tensor.sum` operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, any]`, optional, defaults to *{}*) — Legacy dictionary,
    which is required so that model can use *generate()* function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput` or a
    tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([RagConfig](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.RagConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is
    provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head. The score is possibly marginalized
    over all documents for each vocabulary token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains precomputed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_embeds` (`tf.Tensor` of shape `(batch_size, config.n_docs, hidden_size)`,
    *optional*, returned when *output_retrieved=True*) — Embedded documents retrieved
    by the retriever. Is used with `question_encoder_last_hidden_state` to compute
    the `doc_scores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieved_doc_ids` (`tf.Tensor` (int32) of shape `(batch_size, config.n_docs)`,
    *optional*, returned when *output_retrieved=True*) — The indexes of the embedded
    documents retrieved by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor`(int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Input ids post-processed from the retrieved documents and the question encoder
    input_ids by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`tf.Tensor` (int32) of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden states at the output of the last
    layer of the question encoder pooled output of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the question encoder at the output of each layer plus the initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`question_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the question encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the generator encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator_enc_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator encoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_enc_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator encoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `tf.Tensor` (one for the output of the embeddings and one for the output
    of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden states of the generator decoder at the output of each layer plus the
    initial embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`generator_dec_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the generator decoder, after the attention softmax, used
    to compute the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFRagTokenForGeneration](/docs/transformers/v4.37.2/en/model_doc/rag#transformers.TFRagTokenForGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#### `generate`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/rag/modeling_tf_rag.py#L1007)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — The sequence used as a prompt for the generation. If `input_ids` is not passed,
    then `context_input_ids` has to be provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Mask to avoid performing attention on padding token indices. Mask values selected
    in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`context_input_ids` (`tf.Tensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`,
    *optional*, returned when *output_retrieved=True*) — Input IDs post-processed
    from the retrieved documents and the question encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`context_attention_mask` (`tf.Tensor` of shape `(batch_size * config.n_docs,
    config.max_combined_length)`, *optional*, returned when *output_retrieved=True*)
    — Attention mask post-processed from the retrieved documents and the question
    encoder `input_ids` by the retriever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`doc_scores` (`tf.Tensor` of shape `(batch_size, config.n_docs)`) — Score between
    each retrieved document embeddings (see `retrieved_doc_embeds`) and `question_encoder_last_hidden_state`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model has is not initialized with a `retriever`, `context_input_ids`
    has to be provided to the forward pass. `context_input_ids` are returned by `__call__()`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`n_docs` (`int`, *optional*, defaults to `config.n_docs`) — Number of documents
    to retrieve and/or number of documents for which to generate an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generation_config` (`~generation.GenerationConfig`, *optional*) — The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)’s
    default values, whose documentation should be checked to parameterize generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_processor` (`TFLogitsProcessorList`, *optional*) — Custom logits processors
    that complement the default logits processors built from arguments and a model’s
    config. If a logit processor is passed that is already created with the arguments
    or a model’s config an error is thrown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Ad hoc parametrization of `generate_config`
    and/or additional model-specific kwargs that will be forwarded to the `forward`
    function of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  prefs: []
  type: TYPE_NORMAL
- en: Implements TFRAG token decoding.
  prefs: []
  type: TYPE_NORMAL
