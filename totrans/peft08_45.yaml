- en: P-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/package_reference/p_tuning](https://huggingface.co/docs/peft/package_reference/p_tuning)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[P-tuning](https://hf.co/papers/2103.10385) adds trainable prompt embeddings
    to the input that is optimized by a prompt encoder to find a better prompt, eliminating
    the need to manually design prompts. The prompt tokens can be added anywhere in
    the input sequence, and p-tuning also introduces anchor tokens for improving performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*While GPTs with traditional fine-tuning fail to achieve strong results on
    natural language understanding (NLU), we show that GPTs can be better than or
    comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning —
    which employs trainable continuous prompt embeddings. On the knowledge probing
    (LAMA) benchmark, the best GPT recovers 64\% (P@1) of world knowledge without
    any additional text provided during test time, which substantially improves the
    previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve
    comparable and sometimes better performance to similar-sized BERTs in supervised
    learning. Importantly, we find that P-tuning also improves BERTs’ performance
    in both few-shot and supervised settings while largely reducing the need for prompt
    engineering. Consequently, P-tuning outperforms the state-of-the-art approaches
    on the few-shot SuperGlue benchmark.*.'
  prefs: []
  type: TYPE_NORMAL
- en: PromptEncoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PromptEncoderConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/config.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_reparameterization_type` (Union[`PromptEncoderReparameterizationType`,
    `str`]) — The type of reparameterization to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_size` (`int`) — The hidden size of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_num_layers` (`int`) — The number of layers of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_dropout` (`float`) — The dropout probability of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder).
  prefs: []
  type: TYPE_NORMAL
- en: PromptEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PromptEncoder`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/tuners/p_tuning/model.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PromptEncoderConfig](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoderConfig))
    — The configuration of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt encoder network that is used to generate the virtual token embeddings
    for p-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embedding` (`torch.nn.Embedding`) — The embedding layer of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_head` (`torch.nn.Sequential`) — The MLP head of the prompt encoder if
    `inference_mode=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstm_head` (`torch.nn.LSTM`) — The LSTM head of the prompt encoder if `inference_mode=False`
    and `encoder_reparameterization_type="LSTM"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_dim` (`int`) — The hidden embedding dimension of the base transformer
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_size` (`int`) — The input size of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_size` (`int`) — The output size of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`) — The hidden size of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_virtual_tokens` (`int`): The total number of virtual tokens of the prompt
    encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_type` (Union[`PromptEncoderReparameterizationType`, `str`]): The encoder
    type of the prompt encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input shape: (`batch_size`, `total_virtual_tokens`)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output shape: (`batch_size`, `total_virtual_tokens`, `token_dim`)'
  prefs: []
  type: TYPE_NORMAL
