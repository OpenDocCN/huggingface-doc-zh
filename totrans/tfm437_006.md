# 用于推断的管道

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial)

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)使得在任何语言、计算机视觉、语音和多模态任务上使用Hub中的任何模型进行推断变得简单。即使您没有使用特定模态的经验或不熟悉模型背后的代码，您仍然可以使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)进行推断！本教程将教您：

+   用于推断的[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)。

+   使用特定的分词器或模型。

+   为音频、视觉和多模态任务使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)。

查看[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)文档，了解支持的任务和可用参数的完整列表。

## 管道用法

虽然每个任务都有一个相关的[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)，但使用包含所有特定任务管道的通用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)抽象更简单。[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)会自动加载默认模型和适用于您任务的推断预处理类。让我们以使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)进行自动语音识别（ASR）或语音转文本为例。

1.  首先创建一个[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)，并指定推断任务：

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

1.  将您的输入传递给[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)。在语音识别的情况下，这是一个音频输入文件：

```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

不是您想要的结果？查看Hub上一些[最受欢迎的自动语音识别模型](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)，看看是否可以获得更好的转录。

让我们尝试来自OpenAI的[Whisper large-v2](https://huggingface.co/openai/whisper-large)模型。Whisper比Wav2Vec2晚发布了2年，训练数据接近10倍。因此，它在大多数下游基准测试中击败了Wav2Vec2。它还具有预测标点和大小写的附加好处，而这两者在Wav2Vec2中都不可能。

Wav2Vec2。

让我们在这里尝试一下，看看它的表现如何：

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

现在这个结果看起来更准确了！要深入比较Wav2Vec2和Whisper，请参考[音频变换器课程](https://huggingface.co/learn/audio-course/chapter5/asr_models)。我们真的鼓励您查看Hub中不同语言的模型、专门针对您领域的模型等。您可以直接从Hub在浏览器上查看和比较模型结果，看看它是否比其他模型更适合或更好地处理边缘情况。如果您找不到适用于您用例的模型，您始终可以开始[训练](training)您自己的模型！

如果您有多个输入，可以将输入作为列表传递：

```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

管道对于实验很有用，因为从一个模型切换到另一个模型很简单；然而，有一些方法可以优化它们以处理比实验更大的工作量。查看以下指南，深入探讨如何迭代整个数据集或在web服务器中使用管道：文档中的：

+   [在数据集上使用管道](#using-pipelines-on-a-dataset)

+   [在web服务器上使用管道](./pipeline_webserver)

## 参数

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)支持许多参数；一些是任务特定的，一些是所有管道通用的。一般来说，您可以在任何地方指定参数：

```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

让我们看看3个重要的参数：

### 设备

如果您使用`device=n`，管道会自动将模型放在指定的设备上。无论您使用PyTorch还是Tensorflow，这都可以工作。

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

如果模型对单个GPU来说太大，并且您使用的是PyTorch，您可以设置`device_map="auto"`来自动确定如何加载和存储模型权重。使用`device_map`参数需要 🤗 [Accelerate](https://huggingface.co/docs/accelerate) 软件包：

```py
pip install --upgrade accelerate
```

以下代码会自动在设备之间加载和存储模型权重：

```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

请注意，如果传递了`device_map="auto"`，在实例化您的`pipeline`时无需添加参数`device=device`，否则可能会遇到一些意外行为！

### 批处理大小

默认情况下，管道不会批量推理，原因在[这里](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)有详细解释。原因是批处理不一定更快，在某些情况下实际上可能会更慢。

但如果在您的使用案例中有效，您可以使用：

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

这会在提供的4个音频文件上运行管道，但会将它们分批传递给模型（模型在GPU上，批处理更有可能有所帮助），而无需您进一步编写任何代码。输出应始终与没有批处理时收到的结果相匹配。这只是一种帮助您从管道中获得更快速度的方法。

管道还可以减轻一些批处理的复杂性，因为对于某些管道，单个项目（如长音频文件）需要被分成多个部分才能被模型处理。管道会为您执行这种[*块批处理*](./main_classes/pipelines#pipeline-chunk-batching)。

### 任务特定参数

所有任务都提供任务特定参数，这些参数允许额外的灵活性和选项，帮助您完成工作。例如，[transformers.AutomaticSpeechRecognitionPipeline.**call**()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__)方法有一个`return_timestamps`参数，对于为视频添加字幕听起来很有希望：

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

正如您所看到的，模型推断了文本，并且还输出了各个句子的发音时间。

每个任务都有许多可用的参数，因此请查看每个任务的API参考，看看您可以调整哪些参数！例如，[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)有一个`chunk_length_s`参数，对于处理非常长的音频文件（例如，为整部电影或长达一小时的视频添加字幕）非常有帮助，这是模型通常无法独立处理的：

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
{'text': " Chapter 16\. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening...
```

如果找不到一个真正有帮助的参数，请随时[请求](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)！

## 在数据集上使用管道

管道还可以在大型数据集上运行推理。我们建议的最简单方法是使用迭代器：

```py
def data():
    for i in range(1000):
        yield f"My example {i}"

pipe = pipeline(model="gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

迭代器`data()`会产生每个结果，管道会自动识别输入是可迭代的，并在继续在GPU上处理数据的同时开始获取数据（这在底层使用[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)）。这很重要，因为您不必为整个数据集分配内存，可以尽可能快地将数据馈送到GPU。

由于批处理可能加快速度，尝试调整这里的`batch_size`参数可能会有用。

迭代数据集的最简单方法就是从 🤗 [Datasets](https://github.com/huggingface/datasets/) 加载一个：

```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## 在web服务器上使用管道

创建推理引擎是一个复杂的主题，值得有自己的页面。

[链接](./pipeline_webserver)

## 视觉管道

对于视觉任务使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)几乎是相同的。

指定您的任务并将图像传递给分类器。图像可以是链接，本地路径或base64编码的图像。例如，下面显示了什么品种的猫？

![pipeline-cat-chonk](../Images/ed522d0b7df733cc4426cbf9141d11c3.png)

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## 文本管道

对于NLP任务使用[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)几乎是相同的。

```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## 多模态管道

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)支持多种模态。例如，视觉问答（VQA）任务结合了文本和图像。随意使用您喜欢的任何图像链接和您想要询问有关图像的问题。图像可以是URL或图像的本地路径。

例如，如果您使用这个[发票图像](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png)：

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

要运行上面的示例，您需要安装[`pytesseract`](https://pypi.org/project/pytesseract/)以及🤗 Transformers：

```py
sudo apt install -y tesseract-ocr
pip install pytesseract
```

## 在大型模型上使用🤗加速器：

您可以轻松地在大型模型上使用🤗`accelerate`运行`pipeline`！首先确保您已经安装了`accelerate`和`pip install accelerate`。

首先使用`device_map="auto"`加载您的模型！我们将在示例中使用`facebook/opt-1.3b`。

```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

如果安装了`bitsandbytes`并添加参数`load_in_8bit=True`，还可以传递8位加载的模型

```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

请注意，您可以用支持大型模型加载的任何Hugging Face模型替换检查点，例如BLOOM！
