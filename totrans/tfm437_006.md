# ç”¨äºæ¨æ–­çš„ç®¡é“

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/pipeline_tutorial)

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä½¿å¾—åœ¨ä»»ä½•è¯­è¨€ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šä½¿ç”¨Hubä¸­çš„ä»»ä½•æ¨¡å‹è¿›è¡Œæ¨æ–­å˜å¾—ç®€å•ã€‚å³ä½¿æ‚¨æ²¡æœ‰ä½¿ç”¨ç‰¹å®šæ¨¡æ€çš„ç»éªŒæˆ–ä¸ç†Ÿæ‚‰æ¨¡å‹èƒŒåçš„ä»£ç ï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)è¿›è¡Œæ¨æ–­ï¼æœ¬æ•™ç¨‹å°†æ•™æ‚¨ï¼š

+   ç”¨äºæ¨æ–­çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚

+   ä½¿ç”¨ç‰¹å®šçš„åˆ†è¯å™¨æˆ–æ¨¡å‹ã€‚

+   ä¸ºéŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚

æŸ¥çœ‹[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ–‡æ¡£ï¼Œäº†è§£æ”¯æŒçš„ä»»åŠ¡å’Œå¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ã€‚

## ç®¡é“ç”¨æ³•

è™½ç„¶æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªç›¸å…³çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ï¼Œä½†ä½¿ç”¨åŒ…å«æ‰€æœ‰ç‰¹å®šä»»åŠ¡ç®¡é“çš„é€šç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æŠ½è±¡æ›´ç®€å•ã€‚[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹å’Œé€‚ç”¨äºæ‚¨ä»»åŠ¡çš„æ¨æ–­é¢„å¤„ç†ç±»ã€‚è®©æˆ‘ä»¬ä»¥ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–è¯­éŸ³è½¬æ–‡æœ¬ä¸ºä¾‹ã€‚

1.  é¦–å…ˆåˆ›å»ºä¸€ä¸ª[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ï¼Œå¹¶æŒ‡å®šæ¨æ–­ä»»åŠ¡ï¼š

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition")
```

1.  å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ã€‚åœ¨è¯­éŸ³è¯†åˆ«çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸€ä¸ªéŸ³é¢‘è¾“å…¥æ–‡ä»¶ï¼š

```py
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}
```

ä¸æ˜¯æ‚¨æƒ³è¦çš„ç»“æœï¼ŸæŸ¥çœ‹Hubä¸Šä¸€äº›[æœ€å—æ¬¢è¿çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥è·å¾—æ›´å¥½çš„è½¬å½•ã€‚

è®©æˆ‘ä»¬å°è¯•æ¥è‡ªOpenAIçš„[Whisper large-v2](https://huggingface.co/openai/whisper-large)æ¨¡å‹ã€‚Whisperæ¯”Wav2Vec2æ™šå‘å¸ƒäº†2å¹´ï¼Œè®­ç»ƒæ•°æ®æ¥è¿‘10å€ã€‚å› æ­¤ï¼Œå®ƒåœ¨å¤§å¤šæ•°ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸­å‡»è´¥äº†Wav2Vec2ã€‚å®ƒè¿˜å…·æœ‰é¢„æµ‹æ ‡ç‚¹å’Œå¤§å°å†™çš„é™„åŠ å¥½å¤„ï¼Œè€Œè¿™ä¸¤è€…åœ¨Wav2Vec2ä¸­éƒ½ä¸å¯èƒ½ã€‚

Wav2Vec2ã€‚

è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼š

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
```

ç°åœ¨è¿™ä¸ªç»“æœçœ‹èµ·æ¥æ›´å‡†ç¡®äº†ï¼è¦æ·±å…¥æ¯”è¾ƒWav2Vec2å’ŒWhisperï¼Œè¯·å‚è€ƒ[éŸ³é¢‘å˜æ¢å™¨è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter5/asr_models)ã€‚æˆ‘ä»¬çœŸçš„é¼“åŠ±æ‚¨æŸ¥çœ‹Hubä¸­ä¸åŒè¯­è¨€çš„æ¨¡å‹ã€ä¸“é—¨é’ˆå¯¹æ‚¨é¢†åŸŸçš„æ¨¡å‹ç­‰ã€‚æ‚¨å¯ä»¥ç›´æ¥ä»Hubåœ¨æµè§ˆå™¨ä¸ŠæŸ¥çœ‹å’Œæ¯”è¾ƒæ¨¡å‹ç»“æœï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æ¯”å…¶ä»–æ¨¡å‹æ›´é€‚åˆæˆ–æ›´å¥½åœ°å¤„ç†è¾¹ç¼˜æƒ…å†µã€‚å¦‚æœæ‚¨æ‰¾ä¸åˆ°é€‚ç”¨äºæ‚¨ç”¨ä¾‹çš„æ¨¡å‹ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥å¼€å§‹[è®­ç»ƒ](training)æ‚¨è‡ªå·±çš„æ¨¡å‹ï¼

å¦‚æœæ‚¨æœ‰å¤šä¸ªè¾“å…¥ï¼Œå¯ä»¥å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼š

```py
transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
)
```

ç®¡é“å¯¹äºå®éªŒå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºä»ä¸€ä¸ªæ¨¡å‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹å¾ˆç®€å•ï¼›ç„¶è€Œï¼Œæœ‰ä¸€äº›æ–¹æ³•å¯ä»¥ä¼˜åŒ–å®ƒä»¬ä»¥å¤„ç†æ¯”å®éªŒæ›´å¤§çš„å·¥ä½œé‡ã€‚æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•è¿­ä»£æ•´ä¸ªæ•°æ®é›†æˆ–åœ¨webæœåŠ¡å™¨ä¸­ä½¿ç”¨ç®¡é“ï¼šæ–‡æ¡£ä¸­çš„ï¼š

+   [åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨ç®¡é“](#using-pipelines-on-a-dataset)

+   [åœ¨webæœåŠ¡å™¨ä¸Šä½¿ç”¨ç®¡é“](./pipeline_webserver)

## å‚æ•°

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ”¯æŒè®¸å¤šå‚æ•°ï¼›ä¸€äº›æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œä¸€äº›æ˜¯æ‰€æœ‰ç®¡é“é€šç”¨çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ‚¨å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹æŒ‡å®šå‚æ•°ï¼š

```py
transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
```

è®©æˆ‘ä»¬çœ‹çœ‹3ä¸ªé‡è¦çš„å‚æ•°ï¼š

### è®¾å¤‡

å¦‚æœæ‚¨ä½¿ç”¨`device=n`ï¼Œç®¡é“ä¼šè‡ªåŠ¨å°†æ¨¡å‹æ”¾åœ¨æŒ‡å®šçš„è®¾å¤‡ä¸Šã€‚æ— è®ºæ‚¨ä½¿ç”¨PyTorchè¿˜æ˜¯Tensorflowï¼Œè¿™éƒ½å¯ä»¥å·¥ä½œã€‚

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0)
```

å¦‚æœæ¨¡å‹å¯¹å•ä¸ªGPUæ¥è¯´å¤ªå¤§ï¼Œå¹¶ä¸”æ‚¨ä½¿ç”¨çš„æ˜¯PyTorchï¼Œæ‚¨å¯ä»¥è®¾ç½®`device_map="auto"`æ¥è‡ªåŠ¨ç¡®å®šå¦‚ä½•åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ã€‚ä½¿ç”¨`device_map`å‚æ•°éœ€è¦ ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate) è½¯ä»¶åŒ…ï¼š

```py
pip install --upgrade accelerate
```

ä»¥ä¸‹ä»£ç ä¼šè‡ªåŠ¨åœ¨è®¾å¤‡ä¹‹é—´åŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼š

```py
transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto")
```

è¯·æ³¨æ„ï¼Œå¦‚æœä¼ é€’äº†`device_map="auto"`ï¼Œåœ¨å®ä¾‹åŒ–æ‚¨çš„`pipeline`æ—¶æ— éœ€æ·»åŠ å‚æ•°`device=device`ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ„å¤–è¡Œä¸ºï¼

### æ‰¹å¤„ç†å¤§å°

é»˜è®¤æƒ…å†µä¸‹ï¼Œç®¡é“ä¸ä¼šæ‰¹é‡æ¨ç†ï¼ŒåŸå› åœ¨[è¿™é‡Œ](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)æœ‰è¯¦ç»†è§£é‡Šã€‚åŸå› æ˜¯æ‰¹å¤„ç†ä¸ä¸€å®šæ›´å¿«ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å®é™…ä¸Šå¯èƒ½ä¼šæ›´æ…¢ã€‚

ä½†å¦‚æœåœ¨æ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­æœ‰æ•ˆï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š

```py
transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames)
```

è¿™ä¼šåœ¨æä¾›çš„4ä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šè¿è¡Œç®¡é“ï¼Œä½†ä¼šå°†å®ƒä»¬åˆ†æ‰¹ä¼ é€’ç»™æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨GPUä¸Šï¼Œæ‰¹å¤„ç†æ›´æœ‰å¯èƒ½æœ‰æ‰€å¸®åŠ©ï¼‰ï¼Œè€Œæ— éœ€æ‚¨è¿›ä¸€æ­¥ç¼–å†™ä»»ä½•ä»£ç ã€‚è¾“å‡ºåº”å§‹ç»ˆä¸æ²¡æœ‰æ‰¹å¤„ç†æ—¶æ”¶åˆ°çš„ç»“æœç›¸åŒ¹é…ã€‚è¿™åªæ˜¯ä¸€ç§å¸®åŠ©æ‚¨ä»ç®¡é“ä¸­è·å¾—æ›´å¿«é€Ÿåº¦çš„æ–¹æ³•ã€‚

ç®¡é“è¿˜å¯ä»¥å‡è½»ä¸€äº›æ‰¹å¤„ç†çš„å¤æ‚æ€§ï¼Œå› ä¸ºå¯¹äºæŸäº›ç®¡é“ï¼Œå•ä¸ªé¡¹ç›®ï¼ˆå¦‚é•¿éŸ³é¢‘æ–‡ä»¶ï¼‰éœ€è¦è¢«åˆ†æˆå¤šä¸ªéƒ¨åˆ†æ‰èƒ½è¢«æ¨¡å‹å¤„ç†ã€‚ç®¡é“ä¼šä¸ºæ‚¨æ‰§è¡Œè¿™ç§[*å—æ‰¹å¤„ç†*](./main_classes/pipelines#pipeline-chunk-batching)ã€‚

### ä»»åŠ¡ç‰¹å®šå‚æ•°

æ‰€æœ‰ä»»åŠ¡éƒ½æä¾›ä»»åŠ¡ç‰¹å®šå‚æ•°ï¼Œè¿™äº›å‚æ•°å…è®¸é¢å¤–çš„çµæ´»æ€§å’Œé€‰é¡¹ï¼Œå¸®åŠ©æ‚¨å®Œæˆå·¥ä½œã€‚ä¾‹å¦‚ï¼Œ[transformers.AutomaticSpeechRecognitionPipeline.**call**()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline.__call__)æ–¹æ³•æœ‰ä¸€ä¸ª`return_timestamps`å‚æ•°ï¼Œå¯¹äºä¸ºè§†é¢‘æ·»åŠ å­—å¹•å¬èµ·æ¥å¾ˆæœ‰å¸Œæœ›ï¼š

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}
```

æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæ¨¡å‹æ¨æ–­äº†æ–‡æœ¬ï¼Œå¹¶ä¸”è¿˜è¾“å‡ºäº†å„ä¸ªå¥å­çš„å‘éŸ³æ—¶é—´ã€‚

æ¯ä¸ªä»»åŠ¡éƒ½æœ‰è®¸å¤šå¯ç”¨çš„å‚æ•°ï¼Œå› æ­¤è¯·æŸ¥çœ‹æ¯ä¸ªä»»åŠ¡çš„APIå‚è€ƒï¼Œçœ‹çœ‹æ‚¨å¯ä»¥è°ƒæ•´å“ªäº›å‚æ•°ï¼ä¾‹å¦‚ï¼Œ[AutomaticSpeechRecognitionPipeline](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)æœ‰ä¸€ä¸ª`chunk_length_s`å‚æ•°ï¼Œå¯¹äºå¤„ç†éå¸¸é•¿çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œä¸ºæ•´éƒ¨ç”µå½±æˆ–é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘æ·»åŠ å­—å¹•ï¼‰éå¸¸æœ‰å¸®åŠ©ï¼Œè¿™æ˜¯æ¨¡å‹é€šå¸¸æ— æ³•ç‹¬ç«‹å¤„ç†çš„ï¼š

```py
>>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
{'text': " Chapter 16\. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening...
```

å¦‚æœæ‰¾ä¸åˆ°ä¸€ä¸ªçœŸæ­£æœ‰å¸®åŠ©çš„å‚æ•°ï¼Œè¯·éšæ—¶[è¯·æ±‚](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)ï¼

## åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨ç®¡é“

ç®¡é“è¿˜å¯ä»¥åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿è¡Œæ¨ç†ã€‚æˆ‘ä»¬å»ºè®®çš„æœ€ç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨è¿­ä»£å™¨ï¼š

```py
def data():
    for i in range(1000):
        yield f"My example {i}"

pipe = pipeline(model="gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"])
```

è¿­ä»£å™¨`data()`ä¼šäº§ç”Ÿæ¯ä¸ªç»“æœï¼Œç®¡é“ä¼šè‡ªåŠ¨è¯†åˆ«è¾“å…¥æ˜¯å¯è¿­ä»£çš„ï¼Œå¹¶åœ¨ç»§ç»­åœ¨GPUä¸Šå¤„ç†æ•°æ®çš„åŒæ—¶å¼€å§‹è·å–æ•°æ®ï¼ˆè¿™åœ¨åº•å±‚ä½¿ç”¨[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ï¼‰ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºæ‚¨ä¸å¿…ä¸ºæ•´ä¸ªæ•°æ®é›†åˆ†é…å†…å­˜ï¼Œå¯ä»¥å°½å¯èƒ½å¿«åœ°å°†æ•°æ®é¦ˆé€åˆ°GPUã€‚

ç”±äºæ‰¹å¤„ç†å¯èƒ½åŠ å¿«é€Ÿåº¦ï¼Œå°è¯•è°ƒæ•´è¿™é‡Œçš„`batch_size`å‚æ•°å¯èƒ½ä¼šæœ‰ç”¨ã€‚

è¿­ä»£æ•°æ®é›†çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä» ğŸ¤— [Datasets](https://github.com/huggingface/datasets/) åŠ è½½ä¸€ä¸ªï¼š

```py
# KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)
```

## åœ¨webæœåŠ¡å™¨ä¸Šä½¿ç”¨ç®¡é“

åˆ›å»ºæ¨ç†å¼•æ“æ˜¯ä¸€ä¸ªå¤æ‚çš„ä¸»é¢˜ï¼Œå€¼å¾—æœ‰è‡ªå·±çš„é¡µé¢ã€‚

[é“¾æ¥](./pipeline_webserver)

## è§†è§‰ç®¡é“

å¯¹äºè§†è§‰ä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚

æŒ‡å®šæ‚¨çš„ä»»åŠ¡å¹¶å°†å›¾åƒä¼ é€’ç»™åˆ†ç±»å™¨ã€‚å›¾åƒå¯ä»¥æ˜¯é“¾æ¥ï¼Œæœ¬åœ°è·¯å¾„æˆ–base64ç¼–ç çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¾ç¤ºäº†ä»€ä¹ˆå“ç§çš„çŒ«ï¼Ÿ

![pipeline-cat-chonk](../Images/ed522d0b7df733cc4426cbf9141d11c3.png)

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]
```

## æ–‡æœ¬ç®¡é“

å¯¹äºNLPä»»åŠ¡ä½¿ç”¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡ ä¹æ˜¯ç›¸åŒçš„ã€‚

```py
>>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}
```

## å¤šæ¨¡æ€ç®¡é“

[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)æ”¯æŒå¤šç§æ¨¡æ€ã€‚ä¾‹å¦‚ï¼Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒã€‚éšæ„ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒé“¾æ¥å’Œæ‚¨æƒ³è¦è¯¢é—®æœ‰å…³å›¾åƒçš„é—®é¢˜ã€‚å›¾åƒå¯ä»¥æ˜¯URLæˆ–å›¾åƒçš„æœ¬åœ°è·¯å¾„ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨è¿™ä¸ª[å‘ç¥¨å›¾åƒ](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png)ï¼š

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
```

è¦è¿è¡Œä¸Šé¢çš„ç¤ºä¾‹ï¼Œæ‚¨éœ€è¦å®‰è£…[`pytesseract`](https://pypi.org/project/pytesseract/)ä»¥åŠğŸ¤— Transformersï¼š

```py
sudo apt install -y tesseract-ocr
pip install pytesseract
```

## åœ¨å¤§å‹æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤—åŠ é€Ÿå™¨ï¼š

æ‚¨å¯ä»¥è½»æ¾åœ°åœ¨å¤§å‹æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤—`accelerate`è¿è¡Œ`pipeline`ï¼é¦–å…ˆç¡®ä¿æ‚¨å·²ç»å®‰è£…äº†`accelerate`å’Œ`pip install accelerate`ã€‚

é¦–å…ˆä½¿ç”¨`device_map="auto"`åŠ è½½æ‚¨çš„æ¨¡å‹ï¼æˆ‘ä»¬å°†åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨`facebook/opt-1.3b`ã€‚

```py
# pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

å¦‚æœå®‰è£…äº†`bitsandbytes`å¹¶æ·»åŠ å‚æ•°`load_in_8bit=True`ï¼Œè¿˜å¯ä»¥ä¼ é€’8ä½åŠ è½½çš„æ¨¡å‹

```py
# pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95)
```

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥ç”¨æ”¯æŒå¤§å‹æ¨¡å‹åŠ è½½çš„ä»»ä½•Hugging Faceæ¨¡å‹æ›¿æ¢æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚BLOOMï¼
