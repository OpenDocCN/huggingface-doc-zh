- en: Run training on Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/sagemaker/train](https://huggingface.co/docs/sagemaker/train)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/embed/ok3hetb42gU](https://www.youtube.com/embed/ok3hetb42gU)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how to train a ü§ó Transformers model with the `HuggingFace`
    SageMaker Python SDK. Learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Install and setup your training environment](#installation-and-setup).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prepare a training script](#prepare-a-transformers-fine-tuning-script).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Create a Hugging Face Estimator](#create-a-hugging-face-estimator).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run training with the `fit` method](#execute-training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Access your trained model](#access-trained-model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Perform distributed training](#distributed-training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Create a spot instance](#spot-instances).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Load a training script from a GitHub repository](#git-repository).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Collect training metrics](#sagemaker-metrics).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation and setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can train a ü§ó Transformers model with SageMaker, you need to sign
    up for an AWS account. If you don‚Äôt have an AWS account yet, learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have an AWS account, get started using one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To start training locally, you need to setup an appropriate [IAM role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Upgrade to the latest `sagemaker` version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**SageMaker environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setup your SageMaker environment as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: The execution role is only available when running a notebook within
    SageMaker. If you run `get_execution_role` in a notebook not on SageMaker, expect
    a `region` error.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local environment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setup your local environment as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Prepare a ü§ó Transformers fine-tuning script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our training script is very similar to a training script you might run outside
    of SageMaker. However, you can access useful properties about the training environment
    through various environment variables (see [here](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md)
    for a complete list), such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SM_MODEL_DIR`: A string representing the path to which the training job writes
    the model artifacts. After training, artifacts in this directory are uploaded
    to S3 for model hosting. `SM_MODEL_DIR` is always set to `/opt/ml/model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SM_NUM_GPUS`: An integer representing the number of GPUs available to the
    host.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SM_CHANNEL_XXXX:` A string representing the path to the directory that contains
    the input data for the specified channel. For example, when you specify `train`
    and `test` in the Hugging Face Estimator `fit` method, the environment variables
    are set to `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `hyperparameters` defined in the [Hugging Face Estimator](#create-an-huggingface-estimator)
    are passed as named arguments and processed by `ArgumentParser()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*Note that SageMaker doesn‚Äôt support argparse actions. For example, if you
    want to use a boolean hyperparameter, specify `type` as `bool` in your script
    and provide an explicit `True` or `False` value.*'
  prefs: []
  type: TYPE_NORMAL
- en: Look [here](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py)
    for a complete example of a ü§ó Transformers training script.
  prefs: []
  type: TYPE_NORMAL
- en: Training Output Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If `output_dir` in the `TrainingArguments` is set to ‚Äò/opt/ml/model‚Äô the Trainer
    saves all training artifacts, including logs, checkpoints, and models. Amazon
    SageMaker archives the whole ‚Äò/opt/ml/model‚Äô directory as `model.tar.gz` and uploads
    it at the end of the training job to Amazon S3\. Depending on your Hyperparameters
    and `TrainingArguments` this could lead to a large artifact (> 5GB), which can
    slow down deployment for Amazon SageMaker Inference. You can control how checkpoints,
    logs, and artifacts are saved by customization the [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments).
    For example by providing `save_total_limit` as `TrainingArgument` you can control
    the limit of the total amount of checkpoints. Deletes the older checkpoints in
    `output_dir` if new ones are saved and the maximum limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the options already mentioned above, there is another option
    to save the training artifacts during the training session. Amazon SageMaker supports
    [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html),
    which allows you to continuously save your artifacts during training to Amazon
    S3 rather than at the end of your training. To enable [Checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)
    you need to provide the `checkpoint_s3_uri` parameter pointing to an Amazon S3
    location in the `HuggingFace` estimator and set `output_dir` to `/opt/ml/checkpoints`.
    *Note: If you set `output_dir` to `/opt/ml/checkpoints` make sure to call `trainer.save_model("/opt/ml/model")`
    or model.save_pretrained(‚Äú/opt/ml/model‚Äù)/`tokenizer.save_pretrained("/opt/ml/model")`
    at the end of your training to be able to deploy your model seamlessly to Amazon
    SageMaker for Inference.*'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Hugging Face Estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run ü§ó Transformers training scripts on SageMaker by creating a [Hugging Face
    Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator).
    The Estimator handles end-to-end SageMaker training. There are several parameters
    you should define in the Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '`entry_point` specifies which fine-tuning script to use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`instance_type` specifies an Amazon instance to launch. Refer [here](https://aws.amazon.com/sagemaker/pricing/)
    for a complete list of instance types.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`hyperparameters` specifies training hyperparameters. View additional available
    hyperparameters [here](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code sample shows how to train with a custom script `train.py`
    with three hyperparameters (`epochs`, `per_device_train_batch_size`, and `model_name_or_path`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you are running a `TrainingJob` locally, define `instance_type='local'` or
    `instance_type='local_gpu'` for GPU usage. Note that this will not work with SageMaker
    Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Execute training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start your `TrainingJob` by calling `fit` on a Hugging Face Estimator. Specify
    your input training data in `fit`. The input training data can be a:'
  prefs: []
  type: TYPE_NORMAL
- en: S3 URI such as `s3://my-bucket/my-training-data`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileSystemInput` for Amazon Elastic File System or FSx for Lustre. See [here](https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs)
    for more details about using these file systems as input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Call `fit` to begin training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'SageMaker starts and manages all the required EC2 instances and initiates the
    `TrainingJob` by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Access trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once training is complete, you can access your model through the [AWS console](https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin)
    or download it directly from S3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Distributed training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker provides two strategies for distributed training: data parallelism
    and model parallelism. Data parallelism splits a training set across several GPUs,
    while model parallelism splits a model across several GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
    supports SageMaker‚Äôs data parallelism library. If your training script uses the
    Trainer API, you only need to define the distribution parameter in the Hugging
    Face Estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: üìì Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    for an example of how to run the data parallelism library with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Hugging Face [Trainer] also supports SageMaker‚Äôs model parallelism library.
    If your training script uses the Trainer API, you only need to define the distribution
    parameter in the Hugging Face Estimator (see [here](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters)
    for more detailed information about using model parallelism):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: üìì Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)
    for an example of how to run the model parallelism library.
  prefs: []
  type: TYPE_NORMAL
- en: Spot instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Hugging Face extension for the SageMaker Python SDK means we can benefit
    from [fully-managed EC2 spot instances](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html).
    This can help you save up to 90% of training costs!
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Unless your training job completes quickly, we recommend you use [checkpointing](https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html)
    with managed spot training. In this case, you need to define the `checkpoint_s3_uri`.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set `use_spot_instances=True` and define your `max_wait` and `max_run` time
    in the Estimator to use spot instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: üìì Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)
    for an example of how to use spot instances.
  prefs: []
  type: TYPE_NORMAL
- en: Git repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Hugging Face Estimator can load a training script [stored in a GitHub repository](https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository).
    Provide the relative path to the training script in `entry_point` and the relative
    path to the directory in `source_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using `git_config` to run the [ü§ó Transformers example scripts](https://github.com/huggingface/transformers/tree/main/examples),
    you need to configure the correct `'branch'` in `transformers_version` (e.g. if
    you use `transformers_version='4.4.2` you have to use `'branch':'v4.4.2'`).
  prefs: []
  type: TYPE_NORMAL
- en: '*Tip: Save your model to S3 by setting `output_dir=/opt/ml/model` in the hyperparameter
    of your training script.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: SageMaker metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SageMaker metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics)
    automatically parses training job logs for metrics and sends them to CloudWatch.
    If you want SageMaker to parse the logs, you must specify the metric‚Äôs name and
    a regular expression for SageMaker to use to find the metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: üìì Open the [notebook](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)
    for an example of how to capture metrics in SageMaker.
  prefs: []
  type: TYPE_NORMAL
