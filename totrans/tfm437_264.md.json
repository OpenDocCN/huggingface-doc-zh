["```py\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\nmodel = AutoModel.from_pretrained('facebook/dinov2-base')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs[0]\n\n# We have to force return_dict=False for tracing\nmodel.config.return_dict = False\n\nwith torch.no_grad():\n    traced_model = torch.jit.trace(model, [inputs.pixel_values])\n    traced_outputs = traced_model(inputs.pixel_values)\n\nprint((last_hidden_states - traced_outputs[0]).abs().max())\n```", "```py\n( hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 mlp_ratio = 4 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-06 image_size = 224 patch_size = 16 num_channels = 3 qkv_bias = True layerscale_value = 1.0 drop_path_rate = 0.0 use_swiglu_ffn = False out_features = None out_indices = None apply_layernorm = True reshape_hidden_states = True **kwargs )\n```", "```py\n>>> from transformers import Dinov2Config, Dinov2Model\n\n>>> # Initializing a Dinov2 dinov2-base-patch16-224 style configuration\n>>> configuration = Dinov2Config()\n\n>>> # Initializing a model (with random weights) from the dinov2-base-patch16-224 style configuration\n>>> model = Dinov2Model(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config: Dinov2Config )\n```", "```py\n( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, Dinov2Model\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n>>> model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 257, 768]\n```", "```py\n( config: Dinov2Config )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, Dinov2ForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small-imagenet1k-1-layer\")\n>>> model = Dinov2ForImageClassification.from_pretrained(\"facebook/dinov2-small-imagenet1k-1-layer\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntabby, tabby cat\n```"]