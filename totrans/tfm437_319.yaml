- en: UniSpeech
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UniSpeech
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/unispeech)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The UniSpeech model was proposed in [UniSpeech: Unified Speech Representation
    Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by
    Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
    Zeng, Xuedong Huang .'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'UniSpeech模型是由Chengyi Wang、Yu Wu、Yao Qian、Kenichi Kumatani、Shujie Liu、Furu Wei、Michael
    Zeng、Xuedong Huang在[UniSpeech: Unified Speech Representation Learning with Labeled
    and Unlabeled Data](https://arxiv.org/abs/2101.07597)中提出的。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*In this paper, we propose a unified pre-training approach called UniSpeech
    to learn speech representations with both unlabeled and labeled data, in which
    supervised phonetic CTC learning and phonetically-aware contrastive self-supervised
    learning are conducted in a multi-task learning manner. The resultant representations
    can capture information more correlated with phonetic structures and improve the
    generalization across languages and domains. We evaluate the effectiveness of
    UniSpeech for cross-lingual representation learning on public CommonVoice corpus.
    The results show that UniSpeech outperforms self-supervised pretraining and supervised
    transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative
    phone error rate reductions respectively (averaged over all testing languages).
    The transferability of UniSpeech is also demonstrated on a domain-shift speech
    recognition task, i.e., a relative word error rate reduction of 6% against the
    previous approach.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们提出了一种统一的预训练方法UniSpeech，用于学习具有未标记和标记数据的语音表示，其中通过多任务学习方式进行监督语音CTC学习和语音感知对比自监督学习。所得到的表示可以捕获更多与语音结构相关的信息，并提高跨语言和领域的泛化能力。我们在公共CommonVoice语料库上评估了UniSpeech在跨语言表示学习方面的有效性。结果显示，UniSpeech相对于自监督预训练和监督迁移学习在语音识别方面分别最多减少了13.4%和17.8%的相对电话错误率（在所有测试语言上平均）。UniSpeech的可迁移性也在领域转移语音识别任务中得到了证明，即相对于先前方法减少了6%的相对词错误率。*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The Authors’ code can be found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献。作者的代码可以在[这里](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech)找到。
- en: Usage tips
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: UniSpeech is a speech model that accepts a float array corresponding to the
    raw waveform of the speech signal. Please use [Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)
    for the feature extraction.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UniSpeech是一个语音模型，接受与语音信号的原始波形对应的浮点数组。请使用[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor)进行特征提取。
- en: UniSpeech model can be fine-tuned using connectionist temporal classification
    (CTC) so the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UniSpeech模型可以使用连接主义时间分类（CTC）进行微调，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。
- en: Resources
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Audio classification task guide](../tasks/audio_classification)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[音频分类任务指南](../tasks/audio_classification)'
- en: '[Automatic speech recognition task guide](../tasks/asr)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动语音识别任务指南](../tasks/asr)'
- en: UniSpeechConfig
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechConfig
- en: '### `class transformers.UniSpeechConfig`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/configuration_unispeech.py#L34)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/configuration_unispeech.py#L34)'
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 32) — Vocabulary size of the UniSpeech
    model. Defines the number of different tokens that can be represented by the `inputs_ids`
    passed when calling [UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel).
    Vocabulary size of the model. Defines the different tokens that can be represented
    by the *inputs_ids* passed to the forward method of [UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *可选*, 默认为32) — UniSpeech模型的词汇表大小。定义了在调用[UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel)时可以表示的不同标记的数量。模型的词汇表大小。定义了在调用[UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel)的forward方法时可以表示的不同标记。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`或`function`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。'
- en: '`hidden_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout` (`float`, *可选*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。'
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for activations inside the fully connected layer.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_dropout` (`float`, *optional*, defaults to 0.1) — 全连接层内激活的dropout比率。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — The dropout ratio
    for the attention probabilities.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。'
- en: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for output of the feature encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的dropout概率。'
- en: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for the output of the feature encoder that’s used by the quantizer.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — 用于量化器使用的特征编码器输出的dropout概率。'
- en: '`final_dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for the final projection layer of [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`final_dropout` (`float`, *optional*, defaults to 0.1) — [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)的最终投影层的dropout概率。'
- en: '`layerdrop` (`float`, *optional*, defaults to 0.1) — The LayerDrop probability.
    See the [LayerDrop paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))
    for more details.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop概率。有关详细信息，请参阅[LayerDrop
    paper](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的epsilon。'
- en: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — The norm to
    be applied to 1D convolutional layers in feature encoder. One of `"group"` for
    group normalization of only the first 1D convolutional layer or `"layer"` for
    layer normalization of all 1D convolutional layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于特征编码器中1D卷积层的规范化方式。`"group"`表示仅对第一个1D卷积层进行组归一化，`"layer"`表示对所有1D卷积层进行层归一化。'
- en: '`feat_extract_activation` (`str, *optional*, defaults to` “gelu”`) -- The non-linear
    activation function (function or string) in the 1D convolutional layers of the
    feature extractor. If string,` “gelu”`,` “relu”`,` “selu”`and`“gelu_new”` are
    supported.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feat_extract_activation` (`str, *optional*, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。'
- en: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — A tuple of integers defining the number of input
    and output channels of each 1D convolutional layer in the feature encoder. The
    length of *conv_dim* defines the number of 1D convolutional layers.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512,
    512, 512, 512, 512, 512)`) — 一个整数元组，定义特征编码器中每个1D卷积层的输入和输出通道数。*conv_dim*的长度定义了1D卷积层的数量。'
- en: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — A tuple of integers defining the stride of each 1D convolutional
    layer in the feature encoder. The length of *conv_stride* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2,
    2, 2, 2, 2, 2)`) — 一个整数元组，定义特征编码器中每个1D卷积层的步幅。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 2, 2)`) — A tuple of integers defining the kernel size of each 1D convolutional
    layer in the feature encoder. The length of *conv_kernel* defines the number of
    convolutional layers and has to match the length of *conv_dim*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3,
    3, 3, 3, 2, 2)`) — 一个整数元组，定义特征编码器中每个1D卷积层的内核大小。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。'
- en: '`conv_bias` (`bool`, *optional*, defaults to `False`) — Whether the 1D convolutional
    layers have a bias.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否具有偏置。'
- en: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — Number of
    convolutional positional embeddings. Defines the kernel size of 1D convolutional
    positional embeddings layer.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的内核大小。'
- en: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — Number
    of groups of 1D convolutional positional embeddings layer.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。'
- en: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — Whether
    to apply *stable* layer norm architecture of the Transformer encoder. `do_stable_layer_norm
    is True` corresponds to applying layer norm before the attention layer, whereas
    `do_stable_layer_norm is False` corresponds to applying layer norm after the attention
    layer.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否应用Transformer编码器的*稳定*层归一化架构。`do_stable_layer_norm为True`表示在注意力层之前应用层归一化，而`do_stable_layer_norm为False`表示在注意力层之后应用层归一化。'
- en: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — Whether to
    apply *SpecAugment* data augmentation to the outputs of the feature encoder. For
    reference see [SpecAugment: A Simple Data Augmentation Method for Automatic Speech
    Recognition](https://arxiv.org/abs/1904.08779).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否对特征编码器的输出应用*SpecAugment*数据增强。有关详细信息，请参阅[SpecAugment:
    A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。'
- en: '`mask_time_prob` (`float`, *optional*, defaults to 0.05) — Percentage (between
    0 and 1) of all feature vectors along the time axis which will be masked. The
    masking procecure generates ”mask_time_prob*len(time_axis)/mask_time_length” independent
    masks over the axis. If reasoning from the propability of each feature vector
    to be chosen as the start of the vector span to be masked,* mask_time_prob *should
    be `prob_vector_start*mask_time_length`. Note that overlap may decrease the actual
    percentage of masked vectors. This is only relevant if` apply_spec_augment is
    True`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_prob` (`float`, *可选*, 默认为0.05) — 沿时间轴遮蔽的特征向量的百分比（介于0和1之间）。遮蔽过程在轴上生成“mask_time_prob*len(time_axis)/mask_time_length”个独立的遮蔽。如果从每个特征向量被选择为要遮蔽的向量跨度的起始的概率推理，*
    mask_time_prob *应为`prob_vector_start*mask_time_length`。请注意，重叠可能会降低实际遮蔽向量的百分比。仅在`apply_spec_augment`为True时相关。'
- en: '`mask_time_length` (`int`, *optional*, defaults to 10) — Length of vector span
    along the time axis.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_length` (`int`, *可选*, 默认为10) — 沿时间轴的向量跨度长度。'
- en: '`mask_time_min_masks` (`int`, *optional*, defaults to 2) — The minimum number
    of masks of length `mask_feature_length` generated along the time axis, each time
    step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_time_min_masks` (`int`, *可选*, 默认为2) — 沿时间轴生成的长度为`mask_feature_length`的遮蔽的最小数量，每个时间步，与`mask_feature_prob`无关。仅在“mask_time_prob*len(time_axis)/mask_time_length
    < mask_time_min_masks”时相关。'
- en: '`mask_feature_prob` (`float`, *optional*, defaults to 0.0) — Percentage (between
    0 and 1) of all feature vectors along the feature axis which will be masked. The
    masking procecure generates ”mask_feature_prob*len(feature_axis)/mask_time_length”
    independent masks over the axis. If reasoning from the propability of each feature
    vector to be chosen as the start of the vector span to be masked,* mask_feature_prob
    *should be `prob_vector_start*mask_feature_length`. Note that overlap may decrease
    the actual percentage of masked vectors. This is only relevant if` apply_spec_augment
    is True`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_prob` (`float`, *可选*, 默认为0.0) — 沿特征轴遮蔽的特征向量的百分比（介于0和1之间）。遮蔽过程在轴上生成“mask_feature_prob*len(feature_axis)/mask_time_length”个独立的遮蔽。如果从每个特征向量被选择为要遮蔽的向量跨度的起始的概率推理，*
    mask_feature_prob *应为`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低实际遮蔽向量的百分比。仅在`apply_spec_augment`为True时相关。'
- en: '`mask_feature_length` (`int`, *optional*, defaults to 10) — Length of vector
    span along the feature axis.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_length` (`int`, *可选*, 默认为10) — 沿特征轴的向量跨度长度。'
- en: '`mask_feature_min_masks` (`int`, *optional*, defaults to 0) — The minimum number
    of masks of length `mask_feature_length` generated along the feature axis, each
    time step, irrespectively of `mask_feature_prob`. Only relevant if ”mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_feature_min_masks` (`int`, *可选*, 默认为0) — 沿特征轴生成的长度为`mask_feature_length`的遮蔽的最小数量，每个时间步，与`mask_feature_prob`无关。仅在“mask_feature_prob*len(feature_axis)/mask_feature_length
    < mask_feature_min_masks”时相关。'
- en: '`num_codevectors_per_group` (`int`, *optional*, defaults to 320) — Number of
    entries in each quantization codebook (group).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevectors_per_group` (`int`, *可选*, 默认为320) — 每个量化码书（组）中的条目数。'
- en: '`num_codevector_groups` (`int`, *optional*, defaults to 2) — Number of codevector
    groups for product codevector quantization.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_codevector_groups` (`int`, *可选*, 默认为2) — 产品码矢量量化的码矢量组数。'
- en: '`contrastive_logits_temperature` (`float`, *optional*, defaults to 0.1) — The
    temperature *kappa* in the contrastive loss.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`contrastive_logits_temperature` (`float`, *可选*, 默认为0.1) — 对比损失中的温度*kappa*。'
- en: '`num_negatives` (`int`, *optional*, defaults to 100) — Number of negative samples
    for the contrastive loss.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_negatives` (`int`, *可选*, 默认为100) — 对比损失的负样本数量。'
- en: '`codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality of the
    quantized feature vectors.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`codevector_dim` (`int`, *可选*, 默认为256) — 量化特征向量的维度。'
- en: '`proj_codevector_dim` (`int`, *optional*, defaults to 256) — Dimensionality
    of the final projection of both the quantized and the transformer features.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`proj_codevector_dim` (`int`, *可选*, 默认为256) — 最终投影的维度，用于量化和变换特征。'
- en: '`diversity_loss_weight` (`int`, *optional*, defaults to 0.1) — The weight of
    the codebook diversity loss component.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`diversity_loss_weight` (`int`, *可选*, 默认为0.1) — 码本多样性损失组件的权重。'
- en: '`ctc_loss_reduction` (`str`, *optional*, defaults to `"mean"`) — Specifies
    the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when
    training an instance of [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_loss_reduction` (`str`, *可选*, 默认为`"mean"`) — 指定应用于`torch.nn.CTCLoss`输出的减少方式。仅在训练[UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)的实例时相关。'
- en: '`ctc_zero_infinity` (`bool`, *optional*, defaults to `False`) — Whether to
    zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite
    losses mainly occur when the inputs are too short to be aligned to the targets.
    Only relevant when training an instance of [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctc_zero_infinity` (`bool`, *可选*, 默认为`False`) — 是否将`torch.nn.CTCLoss`的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会出现无限损失。仅在训练[UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)的实例时相关。'
- en: '`use_weighted_layer_sum` (`bool`, *optional*, defaults to `False`) — Whether
    to use a weighted average of layer outputs with learned weights. Only relevant
    when using an instance of [UniSpeechForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_weighted_layer_sum` (`bool`, *可选*, 默认为`False`) — 是否使用具有学习权重的层输出的加权平均值。仅在使用[UniSpeechForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification)的实例时相关。'
- en: '`classifier_proj_size` (`int`, *optional*, defaults to 256) — Dimensionality
    of the projection before token mean-pooling for classification.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classifier_proj_size` (`int`, *可选*, 默认为256) — 用于分类的令牌均值池化之前的投影的维度。'
- en: '`num_ctc_classes` (`int`, *optional*, defaults to 80) — Specifies the number
    of classes (phoneme tokens and blank token) for phoneme-level CTC loss. Only relevant
    when using an instance of [UniSpeechForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForPreTraining).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_ctc_classes` (`int`, *optional*, 默认为80) — 指定音素级CTC损失的类别数（音素标记和空白标记）。仅在使用[UniSpeechForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForPreTraining)的实例时相关。'
- en: '`pad_token_id` (`int`, *optional*, defaults to 0) — The id of the padding token.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token_id` (`int`, *optional*, 默认为0) — 填充标记的id。'
- en: '`bos_token_id` (`int`, *optional*, defaults to 1) — The id of the “beginning-of-sequence”
    token.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token_id` (`int`, *optional*, 默认为1) — “序列开始”标记的id。'
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The id of the “end-of-sequence”
    token.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token_id` (`int`, *optional*, 默认为2) — “序列结束”标记的id。'
- en: '`replace_prob` (`float`, *optional*, defaults to 0.5) — Propability that transformer
    feature is replaced by quantized feature for pretraining.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_prob` (`float`，*optional*，默认为0.5) — 替换transformer特征为预训练的量化特征的概率。'
- en: This is the configuration class to store the configuration of a [UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel).
    It is used to instantiate an UniSpeech model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the UniSpeech [microsoft/unispeech-large-1500h-cv](https://huggingface.co/microsoft/unispeech-large-1500h-cv)
    architecture.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel)的配置。根据指定的参数实例化UniSpeech模型，定义模型架构。使用默认值实例化配置将产生类似于UniSpeech
    [microsoft/unispeech-large-1500h-cv](https://huggingface.co/microsoft/unispeech-large-1500h-cv)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: UniSpeech specific outputs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeech特定的输出
- en: '### `class transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L66)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L66)'
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (*optional*, returned when model is in train mode, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (*optional*，当模型处于训练模式时返回，形状为`(1,)`的`torch.FloatTensor`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。
    (分类) 损失。'
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`）
    — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。'
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`projected_quantized_states` (`torch.FloatTensor`，形状为`(batch_size, sequence_length,
    config.proj_codevector_dim)`） — 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: Output type of `UniSpeechForPreTrainingOutput`, with potential hidden states
    and attentions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`UniSpeechForPreTrainingOutput`的输出类型，具有潜在的隐藏状态和注意力。'
- en: UniSpeechModel
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechModel
- en: '### `class transformers.UniSpeechModel`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1084)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1084)'
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bare UniSpeech Model transformer outputting raw hidden-states without any
    specific head on top. UniSpeech was proposed in [UniSpeech: Unified Speech Representation
    Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by
    Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
    Zeng, Xuedong Huang.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1153)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    and inputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`extract_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    conv_dim[-1])`) — Sequence of extracted feature vectors of the last convolutional
    layer of the model.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [UniSpeechModel](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechModel)
    forward method, overrides the `__call__` special method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: UniSpeechForCTC
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.UniSpeechForCTC`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1355)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_lang` (`str`, *optional*) — Language id of adapter weights. Adapter
    weights are stored in the format adapter.<lang>.safetensors or adapter.<lang>.bin.
    Only relevant when using an instance of [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)
    with adapters. Uses ‘eng’ by default.</lang></lang>'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UniSpeech Model with a `language modeling` head on top for Connectionist Temporal
    Classification (CTC). UniSpeech was proposed in [UniSpeech: Unified Speech Representation
    Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by
    Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
    Zeng, Xuedong Huang.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1438)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip
    install soundfile`）。要将数组准备成`input_values`，应该使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。选择的掩码值在`[0, 1]`中。'
- en: 1 for tokens that are `not masked`,
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示未被`masked`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示被`masked`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 只有当相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask
    == False`的模型，当进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — Labels for connectionist temporal classification. Note that `target_length`
    has to be smaller or equal to the sequence length of the output logits. Indices
    are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100`
    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size
    - 1]`.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*)
    — 连接主义时间分类的标签。请注意，`target_length`必须小于或等于输出logits的序列长度。索引在`[-100, 0, ..., config.vocab_size
    - 1]`中选择。所有设置为`-100`的标签都被忽略（被`masked`），损失仅计算在`[0, ..., config.vocab_size - 1]`中的标签。'
- en: Returns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    and inputs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig)）和输入的不同元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss (for next-token prediction).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) —
    语言建模损失（用于下一个标记预测）。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每个层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层模型的输出处的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）—
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)
    forward method, overrides the `__call__` special method.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[UniSpeechForCTC](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForCTC)的前向方法覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: UniSpeechForSequenceClassification
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UniSpeechForSequenceClassification
- en: '### `class transformers.UniSpeechForSequenceClassification`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.UniSpeechForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1518)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1518)'
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig)）—
    包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: UniSpeech Model with a sequence classification head on top (a linear layer over
    the pooled output) for tasks like SUPERB Keyword Spotting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 带有顶部序列分类头（池化输出上的线性层）的UniSpeech模型，用于类似SUPERB关键词检测的任务。
- en: 'UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning
    with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi
    Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong
    Huang.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 'UniSpeech是由Chengyi Wang、Yu Wu、Yao Qian、Kenichi Kumatani、Shujie Liu、Furu Wei、Michael
    Zeng、Xuedong Huang在[UniSpeech: Unified Speech Representation Learning with Labeled
    and Unlabeled Data](https://arxiv.org/abs/2101.07597)中提出的。'
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（例如下载或保存等）。
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1573)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1573)'
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip
    install soundfile`）。为了准备好数组为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。查看[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)获取详细信息。'
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示“非掩码”标记，
- en: 0 for tokens that are `masked`.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示“掩码”标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    and inputs.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [UniSpeechForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: UniSpeechForPreTraining
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.UniSpeechForPreTraining`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1209)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
    UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning with
    Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang,
    Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong
    Huang.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving etc.).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/unispeech/modeling_unispeech.py#L1273)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '`input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    — Float values of input raw speech waveform. Values can be obtained by loading
    a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`,
    *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array
    into `input_values`, the [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)
    should be used for padding and conversion into a tensor of type `torch.FloatTensor`.
    See [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)
    for details.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask
    == True`. For all models whose processor has `config.return_attention_mask ==
    False`, `attention_mask` should `not` be passed to avoid degraded performance
    when doing batched inference. For such models `input_values` should simply be
    padded with 0 and passed without `attention_mask`. Be aware that these models
    also yield slightly different results depending on whether `input_values` is padded
    or not.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_time_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices to mask extracted features for contrastive loss. When in
    training mode, model learns to predict masked extracted features in *config.proj_codevector_dim*
    space.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampled_negative_indices` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_negatives)`, *optional*) — Indices indicating which quantized target vectors
    are used as negative sampled vectors in contrastive loss. Required input for pre-training.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([UniSpeechConfig](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechConfig))
    and inputs.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (*optional*, returned when model is in train mode, `torch.FloatTensor`
    of shape `(1,)`) — Total loss as the sum of the contrastive loss (L_m) and the
    diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf)
    . (classification) loss.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Hidden-states of the model projected to *config.proj_codevector_dim*
    that can be used to predict the masked projected quantized states.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    config.proj_codevector_dim)`) — Quantized extracted feature vectors projected
    to *config.proj_codevector_dim* representing the positive target vectors for contrastive
    loss.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [UniSpeechForPreTraining](/docs/transformers/v4.37.2/en/model_doc/unispeech#transformers.UniSpeechForPreTraining)
    forward method, overrides the `__call__` special method.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
