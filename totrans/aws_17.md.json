["```py\nneuronxcc-2.12.54.0+f631c2365\n\u251c\u2500\u2500 0_REGISTRY\n\u2502   \u2514\u2500\u2500 0.0.18\n\u2502       \u2514\u2500\u2500 llama \u2502           \u2514\u2500\u2500 meta-llama \u2502               \u2514\u2500\u2500 Llama-2-7b-chat-hf \u2502                   \u2514\u2500\u2500 54c1f6689cd88f246fce.json\n```", "```py\n$ optimum-cli neuron cache lookup meta-llama/Llama-2-7b-chat-hf\n\n*** 1 entrie(s) found in cache for meta-llama/Llama-2-7b-chat-hf ***\n\ntask: text-generation\nbatch_size: 1\nnum_cores: 24\nauto_cast_type: fp16\nsequence_length: 2048\ncompiler_type: neuronx-cc\ncompiler_version: 2.12.54.0+f631c2365\ncheckpoint_id: meta-llama/Llama-2-7b-chat-hf\ncheckpoint_revision: c1b0db933684edbfe29a06fa47eb19cc48025e93\n```", "```py\noptimum-cli neuron cache create --help\n\nusage: optimum-cli neuron cache create [-h] [-n NAME] [--public]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -n NAME, --name NAME  The name of the repo that will be used as a remote cache for the compilation files.\n  --public              If set, the created repo will be public. By default the cache repo is private.\n\n```", "```py\noptimum-cli neuron cache create\n\nNeuron cache created on the Hugging Face Hub: michaelbenayoun/optimum-neuron-cache [private].\nNeuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache.\n```", "```py\nusage: optimum-cli neuron cache set [-h] name\n\npositional arguments:\n  name        The name of the repo to use as remote cache.\n\noptional arguments:\n  -h, --help  show this help message and exit\n```", "```py\noptimum-cli neuron cache set michaelbenayoun/optimum-neuron-cache\n\nNeuron cache name set locally to michaelbenayoun/optimum-neuron-cache in /home/michael/.cache/huggingface/optimum_neuron_custom_cache\n```", "```py\nCUSTOM_CACHE_REPO=\"michaelbenayoun/my_custom_cache_repo\" torchrun ...\n```", "```py\nexport CUSTOM_CACHE_REPO=\"michaelbenayoun/my_custom_cache_repo\"\ntorchrun ...\n```", "```py\nusage: optimum-cli neuron cache [-h] {create,set,add,list} ...\n\npositional arguments:\n  {create,set,add,list,synchronize,lookup}\n    create              Create a model repo on the Hugging Face Hub to store Neuron X compilation files.\n    set                 Set the name of the Neuron cache repo to use locally (trainium only).\n    add                 Add a model to the cache of your choice (trainium only).\n    list                List models in a cache repo (trainium only).\n    synchronize         Synchronize local compiler cache with the hub cache (inferentia only).\n    lookup              Lookup the neuronx compiler hub cache for the specified model id (inferentia only).\n\noptional arguments:\n  -h, --help            show this help message and exit\n```", "```py\nusage: optimum-cli neuron cache add [-h] -m MODEL --task TASK --train_batch_size TRAIN_BATCH_SIZE [--eval_batch_size EVAL_BATCH_SIZE] [--sequence_length SEQUENCE_LENGTH]\n                                    [--encoder_sequence_length ENCODER_SEQUENCE_LENGTH] [--decoder_sequence_length DECODER_SEQUENCE_LENGTH]\n                                    [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS] --precision {fp,bf16} --num_cores\n                                    {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32} [--max_steps MAX_STEPS]\n```", "```py\noptimum-cli neuron cache add \\\n  --model prajjwal1/bert-tiny \\\n  --task text-classification \\\n  --train_batch_size 16 \\\n  --eval_batch_size 16 \\\n  --sequence_length 128 \\\n  --gradient_accumulation_steps 32 \\\n  --num_cores 32 \\\n  --precision bf16\n```", "```py\nusage: optimum-cli neuron cache list [-h] [-m MODEL] [-v VERSION] [name]\n\npositional arguments:\n  name                  The name of the repo to list. Will use the locally saved cache repo if left unspecified.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -m MODEL, --model MODEL\n                        The model name or path of the model to consider. If left unspecified, will list all available models.\n  -v VERSION, --version VERSION\n                        The version of the Neuron X Compiler to consider. Will list all available versions if left unspecified.\n```", "```py\noptimum-cli neuron cache list aws-neuron/optimum-neuron-cache\n```"]