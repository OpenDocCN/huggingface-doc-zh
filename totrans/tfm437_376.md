# VisionTextDualEncoder

> 原文链接: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder)

## 概述

[VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) 可以用于使用任何预训练的视觉自编码模型作为视觉编码器（如 [ViT](vit), [BEiT](beit), [DeiT](deit)）和任何预训练的文本自编码模型作为文本编码器（如 [RoBERTa](roberta), [BERT](bert)）初始化视觉文本双编码器模型。在视觉和文本编码器的顶部添加了两个投影层，将输出嵌入投影到共享的潜在空间。投影层是随机初始化的，因此模型应该在下游任务上进行微调。该模型可用于使用类似 CLIP 的对比图像文本训练来对齐视觉文本嵌入，然后可用于零样本视觉任务，如图像分类或检索。

在 [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) 中展示了如何利用预训练的（锁定/冻结）图像和文本模型进行对比学习，从而在新的零样本视觉任务（如图像分类或检索）上取得显著改进。

## VisionTextDualEncoderConfig

### `class transformers.VisionTextDualEncoderConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L27)

```py
( projection_dim = 512 logit_scale_init_value = 2.6592 **kwargs )
```

参数

+   `projection_dim` (`int`, *optional*, 默认为 512) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`, *optional*, 默认为 2.6592) — *logit_scale* 参数的初始值。默认值根据原始 CLIP 实现使用。

+   `kwargs` (*optional*) — 关键字参数的字典。

[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig) 是用于存储 [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) 配置的配置类。它用于根据指定的参数实例化 [VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel) 模型，定义文本模型和视觉模型配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例:

```py
>>> from transformers import ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel

>>> # Initializing a BERT and ViT configuration
>>> config_vision = ViTConfig()
>>> config_text = BertConfig()

>>> config = VisionTextDualEncoderConfig.from_vision_text_configs(config_vision, config_text, projection_dim=512)

>>> # Initializing a BERT and ViT model (with random weights)
>>> model = VisionTextDualEncoderModel(config=config)

>>> # Accessing the model configuration
>>> config_vision = model.config.vision_config
>>> config_text = model.config.text_config

>>> # Saving the model, including its configuration
>>> model.save_pretrained("vit-bert")

>>> # loading model and config from pretrained folder
>>> vision_text_config = VisionTextDualEncoderConfig.from_pretrained("vit-bert")
>>> model = VisionTextDualEncoderModel.from_pretrained("vit-bert", config=vision_text_config)
```

#### `from_vision_text_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py#L100)

```py
( vision_config: PretrainedConfig text_config: PretrainedConfig **kwargs ) → export const metadata = 'undefined';VisionTextDualEncoderConfig
```

返回

[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)

配置对象的一个实例

从文本模型配置和视觉模型配置实例化一个 [VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)（或派生类）。

## VisionTextDualEncoderProcessor

### `class transformers.VisionTextDualEncoderProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L25)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor` ([AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor), *optional*) — 图像处理器是必需的输入。

+   `tokenizer`（[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，*可选*）— 标记器是必需的输入。

构建一个VisionTextDualEncoder处理器，将图像处理器和标记器包装成一个单一处理器。

[VisionTextDualEncoderProcessor](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor)提供了[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)和[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor.decode)。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L117)

```py
( *args **kwargs )
```

该方法将其所有参数转发给VisionTextDualEncoderTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py#L124)

```py
( *args **kwargs )
```

该方法将其所有参数转发给VisionTextDualEncoderTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。

PytorchHide Pytorch内容

## VisionTextDualEncoderModel

### `class transformers.VisionTextDualEncoderModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L161)

```py
( config: Optional = None vision_model: Optional = None text_model: Optional = None )
```

参数

+   `config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

此类可用于使用任何预训练的视觉自编码模型作为视觉编码器和任何预训练的文本模型作为文本编码器初始化视觉文本双编码器模型。视觉和文本编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法加载。投影层会自动添加到模型中，并应在下游任务（如对比图像文本建模）上进行微调。

在[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)中展示了如何利用预训练（锁定/冻结）图像和文本模型进行对比学习，对新的零样本视觉任务（如图像分类或检索）产生了显著的改进。

训练/微调了这样一个Vision-Text-Dual-Encoder模型之后，它可以像任何其他模型一样保存/加载（有关更多信息，请参阅示例）。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py#L293)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None token_type_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_clip.CLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参见 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`：

    +   对于未被“masked”的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用图像处理器获取像素值（例如，如果您使用 ViT 作为编码器，应该使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参见 [ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

返回

`transformers.models.clip.modeling_clip.CLIPOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.clip.modeling_clip.CLIPOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，这取决于配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当 `return_loss` 为 `True` 时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`) — `image_embeds` 和 `text_embeds` 之间的缩放点积分数。这代表了图像-文本相似性分数。

+   `logits_per_text:(torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`) — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表了文本-图像相似性分数。

+   `text_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的汇总输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的汇总输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [CLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [CLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModel)的输出。

[VisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel)的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import (
...     VisionTextDualEncoderModel,
...     VisionTextDualEncoderProcessor,
...     AutoImageProcessor,
...     AutoTokenizer,
... )

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)
>>> model = VisionTextDualEncoderModel.from_vision_text_pretrained(
...     "google/vit-base-patch16-224", "bert-base-uncased"
... )

>>> # contrastive training
>>> urls = [
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
...     "https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg",
... ]
>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]
>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=images, return_tensors="pt", padding=True
... )
>>> outputs = model(
...     input_ids=inputs.input_ids,
...     attention_mask=inputs.attention_mask,
...     pixel_values=inputs.pixel_values,
...     return_loss=True,
... )
>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score

>>> # save and load from pretrained
>>> model.save_pretrained("vit-bert")
>>> model = VisionTextDualEncoderModel.from_pretrained("vit-bert")

>>> # inference
>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

TensorFlowHide TensorFlow content

## FlaxVisionTextDualEncoderModel

### `class transformers.FlaxVisionTextDualEncoderModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py#L219)

```py
( config: VisionTextDualEncoderConfig input_shape: Optional = None seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config` ([VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选*，默认为`jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可以用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

此类可用于使用任何预训练视觉自编码模型作为视觉编码器和任何预训练文本模型作为文本编码器初始化视觉文本双编码器模型。视觉和文本编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法加载。投影层会自动添加到模型中，并应在下游任务（如对比图像文本建模）上进行微调。

在[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)中，展示了如何利用预训练（锁定/冻结）图像和文本模型进行对比学习，从而在新的零样本视觉任务（如图像分类或检索）上取得显著改进。

训练/微调了这样一个视觉文本双编码器模型后，它可以像其他模型一样保存/加载（有关更多信息，请参阅示例）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入大小、修剪头等）。

此模型还是一个[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)子类。将其用作常规的Flax亚麻模块，并参考Flax文档以获取与一般用法和行为相关的所有内容。

最后，此模型支持内在的JAX特性，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [向量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py#L270)

```py
( input_ids pixel_values attention_mask = None position_ids = None token_type_ids = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1 用于未被“掩码”的标记，

    +   0 用于被“掩码”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。可以使用图像处理器获取像素值（例如，如果您使用ViT作为编码器，则应使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入的各种元素。

+   `logits_per_image:(jnp.ndarray` of shape `(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(jnp.ndarray` of shape `(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — 通过将[FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)的池化输出应用于投影层获得的文本嵌入。

+   `image_embeds(jnp.ndarray` of shape `(batch_size, output_dim`) — 通过将[FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel)的池化输出应用于投影层获得的图像嵌入。

+   `text_model_output(FlaxBaseModelOutputWithPooling):` [FlaxCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)的输出。

+   `vision_model_output(FlaxBaseModelOutputWithPooling):` [FlaxCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPVisionModel)的输出。

[FlaxVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel)的前向方法重写了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是调用此函数，因为前者会负责运行前后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> import jax
>>> from transformers import (
...     FlaxVisionTextDualEncoderModel,
...     VisionTextDualEncoderProcessor,
...     AutoImageProcessor,
...     AutoTokenizer,
... )

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> image_processor = AutoImageProcesor.from_pretrained("google/vit-base-patch16-224")
>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)
>>> model = FlaxVisionTextDualEncoderModel.from_vision_text_pretrained(
...     "google/vit-base-patch16-224", "bert-base-uncased"
... )

>>> # contrastive training
>>> urls = [
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
...     "https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg",
... ]
>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]
>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=images, return_tensors="np", padding=True
... )
>>> outputs = model(
...     input_ids=inputs.input_ids,
...     attention_mask=inputs.attention_mask,
...     pixel_values=inputs.pixel_values,
... )
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score

>>> # save and load from pretrained
>>> model.save_pretrained("vit-bert")
>>> model = FlaxVisionTextDualEncoderModel.from_pretrained("vit-bert")

>>> # inference
>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```

JAXHide JAX content

## TFVisionTextDualEncoderModel

### `class transformers.TFVisionTextDualEncoderModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L175)

```py
( config: Optional[VisionTextDualEncoderConfig] = None vision_model: Optional[TFPreTrainedModel] = None text_model: Optional[TFPreTrainedModel] = None )
```

参数

+   `config`（[VisionEncoderDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig)） — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

此类可用于使用任何预训练的视觉自编码模型作为视觉编码器和任何预训练的文本模型作为文本编码器初始化视觉文本双编码器模型。视觉和文本编码器通过[from_pretrained()](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained)方法加载。投影层会自动添加到模型中，并应在下游任务（如对比图像-文本建模）上进行微调。

在[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991)中展示了如何利用预训练（锁定/冻结）的图像和文本模型进行对比学习，从而在新的零样本视觉任务（如图像分类或检索）上取得显著改进。

经过训练/微调的Vision-Text-Dual-Encoder模型可以像其他模型一样保存/加载（有关更多信息，请参见示例）。

该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是Keras [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规Keras模型，并参考TF文档以获取与一般使用和行为相关的所有信息。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py#L347)

```py
( input_ids: tf.Tensor | None = None pixel_values: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_loss: Optional[bool] = None token_type_ids: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.clip.modeling_tf_clip.TFCLIPOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，如果提供了填充，将被忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。默认情况下，如果提供了填充，将被忽略。可以使用图像处理器获取像素值（例如，如果您使用ViT作为编码器，应该使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)）。有关详细信息，请参阅[ViTImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或`tuple(tf.Tensor)`

一个`transformers.models.clip.modeling_tf_clip.TFCLIPOutput`或一组`tf.Tensor`（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（[VisionTextDualEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig)）和输入的各种元素。

+   `loss` (`tf.Tensor` of shape `(1,)`, *可选*, 当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(tf.Tensor` of shape `(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(tf.Tensor` of shape `(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的汇总输出获得的文本嵌入。

+   `image_embeds(tf.Tensor` of shape `(batch_size, output_dim`) — 通过将投影层应用于[TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的汇总输出获得的图像嵌入。

+   `text_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPTextModel)的输出。

+   `vision_model_output(~modeling_tf_utils.TFBaseModelOutputWithPooling):` [TFCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.TFCLIPVisionModel)的输出。

[TFVisionTextDualEncoderModel](/docs/transformers/v4.37.2/en/model_doc/vision-text-dual-encoder#transformers.TFVisionTextDualEncoderModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import (
...     TFVisionTextDualEncoderModel,
...     VisionTextDualEncoderProcessor,
...     AutoImageProcessor,
...     AutoTokenizer,
... )

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
>>> processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)
>>> model = TFVisionTextDualEncoderModel.from_vision_text_pretrained(
...     "google/vit-base-patch16-224", "bert-base-uncased"
... )

>>> # contrastive training
>>> urls = [
...     "http://images.cocodataset.org/val2017/000000039769.jpg",
...     "https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg",
... ]
>>> images = [Image.open(requests.get(url, stream=True).raw) for url in urls]
>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=images, return_tensors="np", padding=True
... )
>>> outputs = model(
...     input_ids=inputs.input_ids,
...     attention_mask=inputs.attention_mask,
...     pixel_values=inputs.pixel_values,
...     return_loss=True,
... )
>>> loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score

>>> # save and load from pretrained
>>> model.save_pretrained("vit-bert")
>>> model = TFVisionTextDualEncoderModel.from_pretrained("vit-bert")

>>> # inference
>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```
