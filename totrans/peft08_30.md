# 模型

> 原始文本：[`huggingface.co/docs/peft/package_reference/peft_model`](https://huggingface.co/docs/peft/package_reference/peft_model)

PeftModel 是指定基础变换器模型和配置以应用 PEFT 方法的基础模型类。基础`PeftModel`包含从 Hub 加载和保存模型的方法。

## PeftModel

### `class peft.PeftModel`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L88)

```py
( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 用于 Peft 的基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 模型的配置。

+   `adapter_name` (`str`, *可选*) — 适配器的名称，默认为`"default"`。

包含各种 Peft 方法的基础模型。

**属性**：

+   `base_model` (`torch.nn.Module`) — 用于 Peft 的基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 模型的配置。

+   `modules_to_save` (`list` of `str`) — 保存模型时要保存的子模块名称的列表。

+   `prompt_encoder` (PromptEncoder) — 如果使用 PromptLearningConfig，则用于 Peft 的提示编码器。

+   `prompt_tokens` (`torch.Tensor`) — 如果使用 PromptLearningConfig，则用于 Peft 的虚拟提示标记。

+   `transformer_backbone_name` (`str`) — 如果使用 PromptLearningConfig，则是基础模型中变换器主干的名称。

+   `word_embeddings` (`torch.nn.Embedding`) — 如果使用 PromptLearningConfig，则是基础模型中变换器主干的词嵌入。

#### `add_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L587)

```py
( adapter_name: str peft_config: PeftConfig )
```

参数

+   `adapter_name` (`str`) — 要添加的适配器的名称。

+   `peft_config` (PeftConfig) — 要添加的适配器的配置。

根据传递的配置向模型添加适配器。

新适配器的名称应该是唯一的。

新适配器不会自动设置为活动适配器。使用 PeftModel.set_adapter()来设置活动适配器。

#### `create_or_update_model_card`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L777)

```py
( output_dir: str )
```

更新或创建模型卡以包含有关 peft 的信息：

1.  添加`peft`库标签

1.  添加 peft 版本

1.  添加基础模型信息

1.  如果使用了量化信息，则添加量化信息

#### `disable_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L547)

```py
( )
```

禁用适配器模块的上下文管理器。使用此功能在基础模型上运行推理。

示例：

```py
>>> with model.disable_adapter():
...     model(inputs)
```

#### `forward`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L533)

```py
( *args: Any **kwargs: Any )
```

模型的前向传递。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L283)

```py
( model: torch.nn.Module model_id: Union[str, os.PathLike] adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )
```

参数

+   `model` (`torch.nn.Module`) — 要适配的模型。对于🤗变换器模型，模型应该使用[from_pretrained](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)进行初始化。

+   `model_id` (`str`或`os.PathLike`) — 要使用的 PEFT 配置的名称。可以是：

    +   一个字符串，PEFT 配置的 `model id`，托管在 Hugging Face Hub 上的模型存储库中。

    +   包含使用 `save_pretrained` 方法保存的 PEFT 配置文件的目录路径（`./my_peft_config_directory/`）。

+   `adapter_name`（`str`，*可选*，默认为 `"default"`） — 要加载的适配器的名称。这对于加载多个适配器很有用。

+   `is_trainable`（`bool`，*可选*，默认为 `False`） — 适配器是否应该是可训练的。如果为 `False`，适配器将被冻结，只能用于推断。

+   `config`（PeftConfig，*可选*） — 要使用的配置对象，而不是自动加载的配置。此配置对象与 `model_id` 和 `kwargs` 互斥。在调用 `from_pretrained` 之前已加载配置时，这很有用。kwargs —（*可选*）：传递给特定 PEFT 配置类的其他关键字参数。

从预训练模型和加载的 PEFT 权重实例化一个 PEFT 模型。

注意，传递的 `model` 可能会被就地修改。

#### `get_base_model`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L577)

```py
( )
```

返回基础模型。

#### `get_nb_trainable_parameters`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L492)

```py
( )
```

返回模型中可训练参数的数量和所有参数的数量。

#### `get_prompt`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L446)

```py
( batch_size: int task_ids: Optional[torch.Tensor] = None )
```

返回用于 Peft 的虚拟提示。仅在使用提示学习方法时适用。

#### `get_prompt_embedding_to_save`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L427)

```py
( adapter_name: str )
```

在保存模型时返回提示嵌入。仅在使用提示学习方法时适用。

#### `load_adapter`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L652)

```py
( model_id: str adapter_name: str is_trainable: bool = False **kwargs: Any )
```

参数

+   `adapter_name`（`str`） — 要添加的适配器的名称。

+   `peft_config`（PeftConfig） — 要添加的适配器的配置。

+   `is_trainable`（`bool`，*可选*，默认为 `False`） — 适配器是否应该是可训练的。如果为 `False`，适配器将被冻结，只能用于推断。kwargs —（*可选*）：修改加载适配器方式的其他参数，例如 Hugging Face Hub 的令牌。

将训练好的适配器加载到模型中。

新适配器的名称应该是唯一的。

新适配器不会自动设置为活动适配器。使用 PeftModel.set_adapter() 来设置活动适配器。

#### `print_trainable_parameters`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L516)

```py
( )
```

打印模型中可训练参数的数量。

#### `save_pretrained`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L161)

```py
( save_directory: str safe_serialization: bool = True selected_adapters: Optional[list[str]] = None save_embedding_layers: Union[str, bool] = 'auto' is_main_process: bool = True **kwargs: Any )
```

参数

+   `save_directory`（`str`） — 适配器模型和配置文件将保存在其中的目录（如果不存在将创建）。

+   `safe_serialization`（`bool`，*可选*） — 是否以 safetensors 格式保存适配器文件，默认为 `True`。

+   `selected_adapters`（`List[str]`，*可选*） — 要保存的适配器列表。如果为 `None`，将默认为所有适配器。

+   `save_embedding_layers`（`Union[bool, str]`，*可选*，默认为 `"auto"`） — 如果为 `True`，除了适配器权重外还保存嵌入层。如果为 `auto`，则在可用时检查配置的 `target_modules` 中的常见嵌入层 `peft.utils.other.EMBEDDING_LAYER_NAMES`，并自动设置布尔标志。这仅适用于 🤗 transformers 模型。

+   `is_main_process` (`bool`, *可选*) — 调用此函数的进程是否为主进程。默认为 `True`。如果不是主进程，则不会保存检查点，这对于多设备设置（例如 DDP）很重要。

+   `kwargs`（额外的关键字参数，*可选*） — 传递给 `push_to_hub` 方法的额外关键字参数。

此函数将适配器模型和适配器配置文件保存到一个目录中，以便可以使用 PeftModel.from_pretrained() 类方法重新加载，并且也可以被 `PeftModel.push_to_hub()` 方法使用。

#### `set_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L743)

```py
( adapter_name: str )
```

参数

+   `adapter_name` (`str`) — 要设置为活动的适配器的名称。必须先加载适配器。

设置活动适配器。

一次只能有一个适配器处于活动状态。

此外，此函数将设置指定的适配器为可训练的（即，requires_grad=True）。如果不需要此功能，请使用以下代码。

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

## PeftModelForSequenceClassification

用于序列分类任务的 `PeftModel`。

### `class peft.PeftModelForSequenceClassification`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L830)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于序列分类任务的 Peft 模型。

**属性**:

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) — 基础模型的配置对象。

+   `cls_layer_name` (`str`) — 分类层的名称。

示例:

```py
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForSequenceClassification, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "SEQ_CLS",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 768,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 12,
...     "num_layers": 12,
...     "encoder_hidden_size": 768,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForSequenceClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```

## PeftModelForTokenClassification

用于标记分类任务的 `PeftModel`。

### `class peft.PeftModelForTokenClassification`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1460)

```py
( model: torch.nn.Module peft_config: PeftConfig = None adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于标记分类任务的 Peft 模型。

**属性**:

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) — 基础模型的配置对象。

+   `cls_layer_name` (`str`) — 分类层的名称。

示例:

```py
>>> from transformers import AutoModelForSequenceClassification
>>> from peft import PeftModelForTokenClassification, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "TOKEN_CLS",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 768,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 12,
...     "num_layers": 12,
...     "encoder_hidden_size": 768,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForTokenClassification(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117
```

## PeftModelForCausalLM

用于因果语言建模的 `PeftModel`。

### `class peft.PeftModelForCausalLM`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1021)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于因果语言建模的 Peft 模型。

示例:

```py
>>> from transformers import AutoModelForCausalLM
>>> from peft import PeftModelForCausalLM, get_peft_config

>>> config = {
...     "peft_type": "PREFIX_TUNING",
...     "task_type": "CAUSAL_LM",
...     "inference_mode": False,
...     "num_virtual_tokens": 20,
...     "token_dim": 1280,
...     "num_transformer_submodules": 1,
...     "num_attention_heads": 20,
...     "num_layers": 36,
...     "encoder_hidden_size": 1280,
...     "prefix_projection": False,
...     "postprocess_past_key_value_function": None,
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForCausalLM.from_pretrained("gpt2-large")
>>> peft_model = PeftModelForCausalLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544
```

## PeftModelForSeq2SeqLM

用于序列到序列语言建模的 `PeftModel`。

### `class peft.PeftModelForSeq2SeqLM`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1211)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于序列到序列语言建模的 Peft 模型。

示例:

```py
>>> from transformers import AutoModelForSeq2SeqLM
>>> from peft import PeftModelForSeq2SeqLM, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "SEQ_2_SEQ_LM",
...     "inference_mode": False,
...     "r": 8,
...     "target_modules": ["q", "v"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.1,
...     "fan_in_fan_out": False,
...     "enable_lora": None,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")
>>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566
```

## PeftModelForQuestionAnswering

用于问答的 `PeftModel`。

### `class peft.PeftModelForQuestionAnswering`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1635)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于抽取式问答的 Peft 模型。

**属性**：

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) — 基础模型的配置对象。

+   `cls_layer_name` (`str`) — 分类层的名称。

示例：

```py
>>> from transformers import AutoModelForQuestionAnswering
>>> from peft import PeftModelForQuestionAnswering, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "QUESTION_ANS",
...     "inference_mode": False,
...     "r": 16,
...     "target_modules": ["query", "value"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.05,
...     "fan_in_fan_out": False,
...     "bias": "none",
... }

>>> peft_config = get_peft_config(config)
>>> model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForQuestionAnswering(model, peft_config)
>>> peft_model.print_trainable_parameters()
trainable params: 592900 || all params: 108312580 || trainable%: 0.5473971721475013
```

## PeftModelForFeatureExtraction

用于从变换器模型中提取特征/嵌入的`PeftModel`。

### `class peft.PeftModelForFeatureExtraction`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/peft_model.py#L1830)

```py
( model: torch.nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — 基础变换器模型。

+   `peft_config` (PeftConfig) — Peft 配置。

用于从变换器模型中提取特征/嵌入的 Peft 模型。

**属性**：

+   `config` ([PretrainedConfig](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)) — 基础模型的配置对象。

示例：

```py
>>> from transformers import AutoModel
>>> from peft import PeftModelForFeatureExtraction, get_peft_config

>>> config = {
...     "peft_type": "LORA",
...     "task_type": "FEATURE_EXTRACTION",
...     "inference_mode": False,
...     "r": 16,
...     "target_modules": ["query", "value"],
...     "lora_alpha": 32,
...     "lora_dropout": 0.05,
...     "fan_in_fan_out": False,
...     "bias": "none",
... }
>>> peft_config = get_peft_config(config)
>>> model = AutoModel.from_pretrained("bert-base-cased")
>>> peft_model = PeftModelForFeatureExtraction(model, peft_config)
>>> peft_model.print_trainable_parameters()
```

## PeftMixedModel

用于混合不同适配器类型（例如 LoRA 和 LoHa）的`PeftModel`。

### `class peft.PeftMixedModel`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L83)

```py
( model: nn.Module peft_config: PeftConfig adapter_name: str = 'default' )
```

参数

+   `model` (`torch.nn.Module`) — 要调整的模型。

+   `config` (`PeftConfig`) — 要调整的模型的配置。适配器类型必须兼容。

+   `adapter_name` (`str`, `可选`, 默认为`"default"`) — 第一个适配器的名称。

用于加载不同类型适配器进行推断的 PeftMixedModel。

此类不支持加载/保存，通常不应直接初始化。而是使用`get_peft_model`并使用参数`mixed=True`。

阅读[Mixed adapter types](https://huggingface.co/docs/peft/en/developer_guides/mixed_models)指南，了解更多关于使用不同适配器类型的信息。

示例：

```py
>>> from peft import get_peft_model

>>> base_model = ...  # load the base model, e.g. from transformers
>>> peft_model = PeftMixedModel.from_pretrained(base_model, path_to_adapter1, "adapter1").eval()
>>> peft_model.load_adapter(path_to_adapter2, "adapter2")
>>> peft_model.set_adapter(["adapter1", "adapter2"])  # activate both adapters
>>> peft_model(data)  # forward pass using both adapters
```

#### `disable_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L203)

```py
( )
```

禁用适配器模块。

#### `forward`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L191)

```py
( *args: Any **kwargs: Any )
```

模型的前向传递。

#### `from_pretrained`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L329)

```py
( model: nn.Module model_id: str | os.PathLike adapter_name: str = 'default' is_trainable: bool = False config: Optional[PeftConfig] = None **kwargs: Any )
```

参数

+   `model` (`nn.Module`) — 要适应的模型。

+   `model_id` (`str`或`os.PathLike`) — 要使用的 PEFT 配置的名称。可以是：

    +   一个字符串，PEFT 配置的`模型 id`，托管在 Hugging Face Hub 上的模型存储库中。

    +   包含使用`save_pretrained`方法保存的 PEFT 配置文件的目录路径（`./my_peft_config_directory/`）。

+   `adapter_name` (`str`, *可选*, 默认为`"default"`) — 要加载的适配器的名称。这对于加载多个适配器很有用。

+   `is_trainable` (`bool`, *可选*, 默认为`False`) — 适配器是否可训练。如果为`False`，适配器将被冻结并用于推断。

+   `config` (PeftConfig, *可选*) — 要使用的配置对象，而不是自动加载的配置。此配置对象与`model_id`和`kwargs`互斥。在调用`from_pretrained`之前已加载配置时，这很有用。kwargs — (`可选`): 传递给特定 PEFT 配置类的其他关键字参数。

从预训练模型和加载的 PEFT 权重实例化一个 PEFT 混合模型。

请注意，传递的 `model` 可能会被原地修改。

#### `generate`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L197)

```py
( *args: Any **kwargs: Any )
```

生成输出。

#### `get_nb_trainable_parameters`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L146)

```py
( )
```

返回模型中可训练参数的数量和所有参数的数量。

#### `merge_and_unload`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L283)

```py
( *args: Any **kwargs: Any )
```

参数

+   `progressbar` (`bool`) — 是否显示指示卸载和合并过程的进度条

+   `safe_merge` (`bool`) — 是否激活安全合并检查以检查适配器权重中是否存在潜在的 NaN

+   `adapter_names` (`List[str]`, *optional*) — 应合并的适配器名称列表。如果为 None，则将合并所有活动适配器。默认为 `None`。

此方法将适配器层合并到基础模型中。如果有人想要将基础模型用作独立模型，则需要这样做。

#### `print_trainable_parameters`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L171)

```py
( )
```

打印模型中可训练参数的数量。

#### `set_adapter`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L237)

```py
( adapter_name: Union[str, list[str]] )
```

参数

+   `adapter_name` (`str` 或 `List[str]`) — 要激活的适配器的名称。

设置模型的活动适配器。

请注意，在前向传递期间应用适配器的顺序可能与将适配器传递给此函数的顺序不同。相反，在前向传递期间的顺序由适配器加载到模型中的顺序确定。活动适配器仅确定在前向传递期间哪些适配器处于活动状态，但不确定它们被应用的顺序。

此外，此函数将指定的适配器设置为可训练（即，requires_grad=True）。如果不需要这样做，请使用以下代码。

```py
>>> for name, param in model_peft.named_parameters():
...     if ...:  # some check on name (ex. if 'lora' in name)
...         param.requires_grad = False
```

#### `unload`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mixed_model.py#L300)

```py
( *args: Any **kwargs: Any )
```

通过删除所有适配器模块而不进行合并来获取基础模型。这将还原原始基础模型。

## 实用工具

#### `peft.cast_mixed_precision_params`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L523)

```py
( model dtype )
```

参数

+   `model` (`torch.nn.Module`) — 要转换不可训练参数的模型。

+   `dtype` (`torch.dtype`) — 要将不可训练参数转换为的 dtype。`dtype` 可以是 `torch.float16` 或

将模型的所有不可训练参数转换为给定的 `dtype`。`dtype` 可以是 `torch.float16` 或 `torch.bfloat16`，根据您正在执行的混合精度训练而定。可训练参数转换为全精度。这旨在通过使用半精度 dtype 减少使用 PEFT 方法时的 GPU 内存使用量。将可训练参数保留为全精度可在使用自动混合精度训练时保持训练稳定性。

`torch.bfloat16` 根据您正在执行的混合精度训练。

#### `peft.get_peft_model`

[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L106)

```py
( model: PreTrainedModel peft_config: PeftConfig adapter_name: str = 'default' mixed: bool = False )
```

参数

+   `model` ([transformers.PreTrainedModel](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)) — Model to be wrapped.

+   `peft_config` (PeftConfig) — 包含 Peft 模型参数的配置对象。

+   `adapter_name` (`str`, `optional`, 默认为 `"default"`) — 要注入的适配器的名称，如果未提供，则使用默认适配器名称（`"default"`）。

+   `mixed` (`bool`, `optional`, 默认为 `False`) — 是否允许混合不同（兼容的）适配器类型。

从模型和配置返回一个 Peft 模型对象。

#### `peft.inject_adapter_in_model`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/mapping.py#L139)

```py
( peft_config: PeftConfig model: torch.nn.Module adapter_name: str = 'default' )
```

参数

+   `peft_config`（`PeftConfig`）- 包含 Peft 模型参数的配置对象。

+   `model`（`torch.nn.Module`）- 要注入适配器的输入模型。

+   `adapter_name`（`str`，`可选`，默认为`"default"`）- 要注入的适配器的名称，如果未提供，则使用默认的适配器名称（`"default"`）。

一个简单的 API，用于在模型中创建并就地注入适配器。目前，该 API 不支持提示学习方法和适应提示。请确保在`peft_config`对象中设置了正确的`target_names`。API 在底层调用`get_peft_model`，但仅限于非提示学习方法。

#### `peft.get_peft_model_state_dict`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/save_and_load.py#L46)

```py
( model state_dict = None adapter_name = 'default' unwrap_compiled = False save_embedding_layers = 'auto' )
```

参数

+   `model`（PeftModel）- Peft 模型。在使用 torch.nn.DistributedDataParallel、DeepSpeed 或 FSDP 时，模型应为基础模型/取消包装模型（即 model.module）。

+   `state_dict`（`dict`，*可选*，默认为`None`）- 模型的状态字典。如果未提供，则将使用传递模型的状态字典。

+   `adapter_name`（`str`，*可选*，默认为`"default"`）- 应返回其状态字典的适配器的名称。

+   `unwrap_compiled`（`bool`，*可选*，默认为`False`）- 是否在使用 torch.compile 时取消包装模型。

+   `save_embedding_layers`（`Union[bool, str]`，*可选*，默认为`auto`）- 如果为`True`，则保存嵌入层以及适配器权重。如果为`auto`，则在配置的`target_modules`中检查常见的嵌入层`peft.utils.other.EMBEDDING_LAYER_NAMES`，并根据其设置布尔标志。这仅适用于🤗 transformers 模型。

获取 Peft 模型的状态字典。

#### `peft.prepare_model_for_kbit_training`

[<来源>](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/utils/other.py#L77)

```py
( model use_gradient_checkpointing = True gradient_checkpointing_kwargs = None )
```

参数

+   `model`（`transformers.PreTrainedModel`）- 从`transformers`加载的模型

+   `use_gradient_checkpointing`（`bool`，*可选*，默认为`True`）- 如果为 True，则使用梯度检查点以节省内存，但会导致反向传播速度变慢。

+   `gradient_checkpointing_kwargs`（`dict`，*可选*，默认为`None`）- 传递给梯度检查点函数的关键字参数，请参考`torch.utils.checkpoint.checkpoint`的文档，了解可以传递给该方法的参数的更多详细信息。请注意，这仅适用于最新的 transformers 版本（> 4.34.1）。

请注意，此方法仅适用于`transformers`模型。

此方法包装了准备模型在运行训练之前的整个协议。这包括：1- 将 layernorm 转换为 fp32 2- 使输出嵌入层需要梯度 3- 将 lm 头的升级转换为 fp32
