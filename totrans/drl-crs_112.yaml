- en: RLHF
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf](https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a **methodology for integrating
    human data labels into a RL-based optimization process**. It is motivated by the
    **challenge of modeling human preferences**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: For many questions, even if you could try and write down an equation for one
    ideal, humans differ on their preferences.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Updating models **based on measured data is an avenue to try and alleviate these
    inherently human ML problems**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Start Learning about RLHF
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start learning about RLHF:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Read this introduction: [Illustrating Reinforcement Learning from Human Feedback
    (RLHF)](https://huggingface.co/blog/rlhf).'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watch the recorded live we did some weeks ago, where Nathan covered the basics
    of Reinforcement Learning from Human Feedback (RLHF) and how this technology is
    being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk
    is an overview of the interconnected ML models. It covers the basics of Natural
    Language Processing and RL and how RLHF is used on large language models. We then
    conclude with open questions in RLHF.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/2MBJOuVq380](https://www.youtube-nocookie.com/embed/2MBJOuVq380)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Read other blogs on this topic, such as [Closed-API vs Open-source continues:
    RLHF, ChatGPT, data moats](https://robotic.substack.com/p/rlhf-chatgpt-data-moats).
    Let us know if there are more you like!'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additional readings
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Note, this is copied from the Illustrating RLHF blog post above*. Here is
    a list of the most prevalent papers on RLHF to date. The field was recently popularized
    with the emergence of DeepRL (around 2017) and has grown into a broader study
    of the applications of LLMs from many large technology companies. Here are some
    papers on RLHF that pre-date the LM focus:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[TAMER: Training an Agent Manually via Evaluative Reinforcement](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf)
    (Knox and Stone 2008): Proposed a learned agent where humans provided scores on
    the actions taken iteratively to learn a reward model.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interactive Learning from Policy-Dependent Human Feedback](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf)
    (MacGlashan et al. 2017): Proposed an actor-critic algorithm, COACH, where human
    feedback (both positive and negative) is used to tune the advantage function.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
    (Christiano et al. 2017): RLHF applied on preferences between Atari trajectories.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces](https://ojs.aaai.org/index.php/AAAI/article/view/11485)
    (Warnell et al. 2018): Extends the TAMER framework where a deep neural network
    is used to model the reward prediction.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And here is a snapshot of the growing set of papers that show RLHF’s performance
    for LMs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)
    (Zieglar et al. 2019): An early paper that studies the impact of reward learning
    on four specific tasks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learning to summarize with human feedback](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)
    (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, [Recursively
    Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI
    Alignment Team 2021), follow on work summarizing books.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)
    (OpenAI, 2021): Using RLHF to train an agent to navigate the web.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
    (OpenAI Alignment Team 2022): RLHF applied to a general language model [[Blog
    post](https://openai.com/blog/instruction-following/) on InstructGPT].'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'InstructGPT: [训练语言模型遵循人类反馈的指令](https://arxiv.org/abs/2203.02155) (OpenAI Alignment
    Team 2022年): 将RLHF应用于通用语言模型[[关于InstructGPT的博文](https://openai.com/blog/instruction-following/)]'
- en: 'GopherCite: [Teaching language models to support answers with verified quotes](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
    (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GopherCite: [教导语言模型支持带有验证引用的答案](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
    (Menick等人，2022年): 使用RLHF训练LM以返回带有特定引用的答案'
- en: 'Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375)
    (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sparrow: [通过有针对性的人类判断改善对话代理的对齐](https://arxiv.org/abs/2209.14375) (Glaese等人，2022年):
    使用RLHF对对话代理进行微调'
- en: '[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)
    (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat
    bot.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT: 优化语言模型用于对话](https://openai.com/blog/chatgpt/) (OpenAI 2022年): 使用RLHF训练LM，以适合作为通用聊天机器人使用'
- en: '[Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)
    (Gao et al. 2022): studies the scaling properties of the learned preference model
    in RLHF.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[奖励模型过度优化的扩展定律](https://arxiv.org/abs/2210.10760) (Gao等人，2022年): 研究RLHF中学习偏好模型的扩展特性'
- en: '[Training a Helpful and Harmless Assistant with Reinforcement Learning from
    Human Feedback](https://arxiv.org/abs/2204.05862) (Anthropic, 2022): A detailed
    documentation of training a LM assistant with RLHF.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用人类反馈的强化学习训练一个有用且无害的助手](https://arxiv.org/abs/2204.05862) (Anthropic, 2022年):
    对使用RLHF训练LM助手的详细文档'
- en: '[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and
    Lessons Learned](https://arxiv.org/abs/2209.07858) (Ganguli et al. 2022): A detailed
    documentation of efforts to “discover, measure, and attempt to reduce [language
    models] potentially harmful outputs.”'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗性团队语言模型以减少伤害: 方法、扩展行为和经验教训](https://arxiv.org/abs/2209.07858) (Ganguli等人，2022年):
    关于“发现、衡量和尝试减少[语言模型]潜在有害输出”的详细文档'
- en: '[Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning](https://arxiv.org/abs/2208.02294)
    (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended
    dialogue agent.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用强化学习进行开放式对话的动态规划](https://arxiv.org/abs/2208.02294) (Cohen等人，2022年): 使用RL增强开放式对话代理的会话技能'
- en: '[Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks,
    Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241)
    (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source
    tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization)
    as an alternative to PPO.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习是否适用于自然语言处理？: 自然语言政策优化的基准、基线和构建块](https://arxiv.org/abs/2210.01241) (Ramamurthy和Ammanabrolu等人，2022年):
    讨论RLHF中开源工具的设计空间，并提出一种新算法NLPO（自然语言政策优化）作为PPO的替代方案。'
- en: Author
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是由[Nathan Lambert](https://twitter.com/natolambert)撰写的
