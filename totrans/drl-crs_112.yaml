- en: RLHF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf](https://huggingface.co/learn/deep-rl-course/unitbonus3/rlhf)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback (RLHF) is a **methodology for integrating
    human data labels into a RL-based optimization process**. It is motivated by the
    **challenge of modeling human preferences**.
  prefs: []
  type: TYPE_NORMAL
- en: For many questions, even if you could try and write down an equation for one
    ideal, humans differ on their preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Updating models **based on measured data is an avenue to try and alleviate these
    inherently human ML problems**.
  prefs: []
  type: TYPE_NORMAL
- en: Start Learning about RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start learning about RLHF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read this introduction: [Illustrating Reinforcement Learning from Human Feedback
    (RLHF)](https://huggingface.co/blog/rlhf).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watch the recorded live we did some weeks ago, where Nathan covered the basics
    of Reinforcement Learning from Human Feedback (RLHF) and how this technology is
    being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk
    is an overview of the interconnected ML models. It covers the basics of Natural
    Language Processing and RL and how RLHF is used on large language models. We then
    conclude with open questions in RLHF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/2MBJOuVq380](https://www.youtube-nocookie.com/embed/2MBJOuVq380)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read other blogs on this topic, such as [Closed-API vs Open-source continues:
    RLHF, ChatGPT, data moats](https://robotic.substack.com/p/rlhf-chatgpt-data-moats).
    Let us know if there are more you like!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additional readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Note, this is copied from the Illustrating RLHF blog post above*. Here is
    a list of the most prevalent papers on RLHF to date. The field was recently popularized
    with the emergence of DeepRL (around 2017) and has grown into a broader study
    of the applications of LLMs from many large technology companies. Here are some
    papers on RLHF that pre-date the LM focus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[TAMER: Training an Agent Manually via Evaluative Reinforcement](https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/ICDL08-knox.pdf)
    (Knox and Stone 2008): Proposed a learned agent where humans provided scores on
    the actions taken iteratively to learn a reward model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interactive Learning from Policy-Dependent Human Feedback](http://proceedings.mlr.press/v70/macglashan17a/macglashan17a.pdf)
    (MacGlashan et al. 2017): Proposed an actor-critic algorithm, COACH, where human
    feedback (both positive and negative) is used to tune the advantage function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Reinforcement Learning from Human Preferences](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)
    (Christiano et al. 2017): RLHF applied on preferences between Atari trajectories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces](https://ojs.aaai.org/index.php/AAAI/article/view/11485)
    (Warnell et al. 2018): Extends the TAMER framework where a deep neural network
    is used to model the reward prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And here is a snapshot of the growing set of papers that show RLHF’s performance
    for LMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)
    (Zieglar et al. 2019): An early paper that studies the impact of reward learning
    on four specific tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learning to summarize with human feedback](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)
    (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, [Recursively
    Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862) (OpenAI
    Alignment Team 2021), follow on work summarizing books.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)
    (OpenAI, 2021): Using RLHF to train an agent to navigate the web.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'InstructGPT: [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
    (OpenAI Alignment Team 2022): RLHF applied to a general language model [[Blog
    post](https://openai.com/blog/instruction-following/) on InstructGPT].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GopherCite: [Teaching language models to support answers with verified quotes](https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes)
    (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparrow: [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/abs/2209.14375)
    (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)
    (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat
    bot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2210.10760)
    (Gao et al. 2022): studies the scaling properties of the learned preference model
    in RLHF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Training a Helpful and Harmless Assistant with Reinforcement Learning from
    Human Feedback](https://arxiv.org/abs/2204.05862) (Anthropic, 2022): A detailed
    documentation of training a LM assistant with RLHF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and
    Lessons Learned](https://arxiv.org/abs/2209.07858) (Ganguli et al. 2022): A detailed
    documentation of efforts to “discover, measure, and attempt to reduce [language
    models] potentially harmful outputs.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning](https://arxiv.org/abs/2208.02294)
    (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended
    dialogue agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks,
    Baselines, and Building Blocks for Natural Language Policy Optimization](https://arxiv.org/abs/2210.01241)
    (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source
    tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization)
    as an alternative to PPO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section was written by [Nathan Lambert](https://twitter.com/natolambert)
  prefs: []
  type: TYPE_NORMAL
