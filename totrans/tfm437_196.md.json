["```py\n>>> from transformers import MBartForConditionalGeneration, MBartTokenizer\n\n>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n>>> example_english_phrase = \"UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_romanian = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_tensors=\"pt\")\n\n>>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n>>> # forward pass\n>>> model(**inputs)\n```", "```py\n>>> from transformers import MBartForConditionalGeneration, MBartTokenizer\n\n>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\")\n>>> article = \"UN Chief Says There Is No Military Solution in Syria\"\n>>> inputs = tokenizer(article, return_tensors=\"pt\")\n>>> translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"ro_RO\"])\n>>> tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n\"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n```", "```py\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n\nsrc_text = \" UN Chief Says There Is No Military Solution in Syria\"\ntgt_text = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n\nmodel_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n\nmodel(**model_inputs)  # forward pass\n```", "```py\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\narticle_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\"\narticle_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\"\n\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n\n# translate Hindi to French\ntokenizer.src_lang = \"hi_IN\"\nencoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.\"\n\n# translate Arabic to English\ntokenizer.src_lang = \"ar_AR\"\nencoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\ngenerated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\ntokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n# => \"The Secretary-General of the United Nations says there is no military solution in Syria.\"\n```", "```py\n( vocab_size = 50265 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 classifier_dropout = 0.0 scale_embedding = False pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 forced_eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import MBartConfig, MBartModel\n\n>>> # Initializing a MBART facebook/mbart-large-cc25 style configuration\n>>> configuration = MBartConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/mbart-large-cc25 style configuration\n>>> model = MBartModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' tokenizer_file = None src_lang = None tgt_lang = None sp_model_kwargs: Optional = None additional_special_tokens = None **kwargs )\n```", "```py\n>>> from transformers import MBartTokenizer\n\n>>> tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n>>> example_english_phrase = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_romanian = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_tensors=\"pt\")\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( vocab_file = None tokenizer_file = None bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' src_lang = None tgt_lang = None additional_special_tokens = None **kwargs )\n```", "```py\n>>> from transformers import MBartTokenizerFast\n\n>>> tokenizer = MBartTokenizerFast.from_pretrained(\n...     \"facebook/mbart-large-en-ro\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\"\n... )\n>>> example_english_phrase = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> expected_translation_romanian = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> inputs = tokenizer(example_english_phrase, text_target=expected_translation_romanian, return_tensors=\"pt\")\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( src_lang )\n```", "```py\n( lang: str )\n```", "```py\n( vocab_file src_lang = None tgt_lang = None eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' sp_model_kwargs: Optional = None **kwargs )\n```", "```py\n>>> from transformers import MBart50Tokenizer\n\n>>> tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n>>> src_text = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> tgt_text = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n>>> # model(**model_inputs) should work\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( tokens )\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( src_lang: str )\n```", "```py\n( tgt_lang: str )\n```", "```py\n( vocab_file = None src_lang = None tgt_lang = None tokenizer_file = None eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' **kwargs )\n```", "```py\n>>> from transformers import MBart50TokenizerFast\n\n>>> tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n>>> src_text = \" UN Chief Says There Is No Military Solution in Syria\"\n>>> tgt_text = \"\u015eeful ONU declar\u0103 c\u0103 nu exist\u0103 o solu\u0163ie militar\u0103 \u00een Siria\"\n>>> model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n>>> # model(**model_inputs) should work\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( src_lang: str )\n```", "```py\n( tgt_lang: str )\n```", "```py\n( config: MBartConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MBartModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = MBartModel.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: MBartConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MBartForConditionalGeneration\n\n>>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\")\n\n>>> example_english_phrase = \"42 is the answer\"\n>>> inputs = tokenizer(example_english_phrase, return_tensors=\"pt\")\n\n>>> # Translate\n>>> generated_ids = model.generate(**inputs, num_beams=4, max_length=5)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'42 este r\u0103spuns'\n```", "```py\n>>> from transformers import AutoTokenizer, MBartForConditionalGeneration\n\n>>> model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> # de_DE is the language symbol id <LID> for German\n>>> TXT = \"</s> Meine Freunde sind <mask> nett aber sie essen zu viel Kuchen. </s> de_DE\"\n\n>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n['nett', 'sehr', 'ganz', 'nicht', 'so']\n```", "```py\n( config )\n```", "```py\n( input_ids: Tensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MBartForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = MBartForQuestionAnswering.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n( config: MBartConfig **kwargs )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = MBartForSequenceClassification.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = MBartForSequenceClassification.from_pretrained(\"facebook/mbart-large-cc25\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, MBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = MBartForSequenceClassification.from_pretrained(\"facebook/mbart-large-cc25\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = MBartForSequenceClassification.from_pretrained(\n...     \"facebook/mbart-large-cc25\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, MBartForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = MBartForCausalLM.from_pretrained(\"facebook/mbart-large-cc25\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n( config: MBartConfig *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFMBartModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = TFMBartModel.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None decoder_position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None cross_attn_head_mask: tf.Tensor | None = None encoder_outputs: Optional[TFBaseModelOutput] = None past_key_values: Tuple[Tuple[tf.Tensor]] = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: tf.Tensor | None = None training: Optional[bool] = False ) \u2192 export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFMBartForConditionalGeneration\n\n>>> model = TFMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-en-ro\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\")\n\n>>> example_english_phrase = \"42 is the answer\"\n>>> inputs = tokenizer(example_english_phrase, return_tensors=\"tf\")\n\n>>> # Translate\n>>> generated_ids = model.generate(**inputs, num_beams=4, max_length=5)\n>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'42 este r\u0103spuns'\n```", "```py\n>>> from transformers import AutoTokenizer, TFMBartForConditionalGeneration\n>>> import tensorflow as tf\n\n>>> model = TFMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> # de_DE is the language symbol id <LID> for German\n>>> TXT = \"</s> Meine Freunde sind <mask> nett aber sie essen zu viel Kuchen. </s> de_DE\"\n\n>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"tf\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = tf.where(input_ids[0] == tokenizer.mask_token_id)[0, 0]\n>>> probs = tf.nn.softmax(logits[0, masked_index], axis=0)\n>>> values, predictions = tf.math.top_k(probs, 5)\n\n>>> tokenizer.decode(predictions).split()\n['nett', 'sehr', 'ganz', 'nicht', 'so']\n```", "```py\n( config: MBartConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = FlaxMBartModel.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: MBartConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration, MBartConfig\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> ARTICLE_TO_SUMMARIZE = \"Meine Freunde sind cool, aber sie essen zu viel Kuchen.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"np\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=5).sequences\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> # de_DE is the language symbol id <LID> for German\n>>> TXT = \"</s> Meine Freunde sind <mask> nett aber sie essen zu viel Kuchen. </s> de_DE\"\n>>> input_ids = tokenizer([TXT], add_special_tokens=False, return_tensors=\"np\")[\"input_ids\"]\n\n>>> logits = model(input_ids).logits\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero()[0].item()\n>>> probs = logits[0, masked_index].softmax(dim=0)\n>>> values, predictions = probs.topk(5)\n\n>>> tokenizer.decode(predictions).split()\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```", "```py\n( config: MBartConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = FlaxMBartForSequenceClassification.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> logits = outputs.logits\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: MBartConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )\n```", "```py\n( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> model = FlaxMBartForQuestionAnswering.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n>>> inputs = tokenizer(question, text, return_tensors=\"jax\")\n\n>>> outputs = model(**inputs)\n>>> start_scores = outputs.start_logits\n>>> end_scores = outputs.end_logits\n```", "```py\n( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) \u2192 export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxMBartForConditionalGeneration\n\n>>> model = FlaxMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"jax\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```"]