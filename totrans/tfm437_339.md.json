["```py\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import CLIPProcessor, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import CLIPConfig, CLIPModel\n\n>>> # Initializing a CLIPConfig with openai/clip-vit-base-patch32 style configuration\n>>> configuration = CLIPConfig()\n\n>>> # Initializing a CLIPModel (with random weights) from the openai/clip-vit-base-patch32 style configuration\n>>> model = CLIPModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a CLIPConfig from a CLIPTextConfig and a CLIPVisionConfig\n>>> from transformers import CLIPTextConfig, CLIPVisionConfig\n\n>>> # Initializing a CLIPText and CLIPVision configuration\n>>> config_text = CLIPTextConfig()\n>>> config_vision = CLIPVisionConfig()\n\n>>> config = CLIPConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import CLIPTextConfig, CLIPTextModel\n\n>>> # Initializing a CLIPTextConfig with openai/clip-vit-base-patch32 style configuration\n>>> configuration = CLIPTextConfig()\n\n>>> # Initializing a CLIPTextModel (with random weights) from the openai/clip-vit-base-patch32 style configuration\n>>> model = CLIPTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import CLIPVisionConfig, CLIPVisionModel\n\n>>> # Initializing a CLIPVisionConfig with openai/clip-vit-base-patch32 style configuration\n>>> configuration = CLIPVisionConfig()\n\n>>> # Initializing a CLIPVisionModel (with random weights) from the openai/clip-vit-base-patch32 style configuration\n>>> model = CLIPVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPModel\n\n>>> model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPTextModel\n\n>>> model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPTextModelWithProjection\n\n>>> model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> text_embeds = outputs.text_embeds\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPVisionModelWithProjection\n\n>>> model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> image_embeds = outputs.image_embeds\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPVisionModel\n\n>>> model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> import tensorflow as tf\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFCLIPModel\n\n>>> model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"tf\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoTokenizer, TFCLIPModel\n\n>>> model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFCLIPModel\n\n>>> model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, TFCLIPTextModel\n\n>>> model = TFCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, TFCLIPVisionModel\n\n>>> model = TFCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"tf\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> import jax\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, FlaxCLIPModel\n\n>>> model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"np\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = jax.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxCLIPModel\n\n>>> model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"np\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, FlaxCLIPModel\n\n>>> model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"np\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxCLIPTextModel\n\n>>> model = FlaxCLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"np\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooler_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxCLIPTextModelWithProjection\n\n>>> model = FlaxCLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"np\")\n\n>>> outputs = model(**inputs)\n>>> text_embeds = outputs.text_embeds\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, FlaxCLIPVisionModel\n\n>>> model = FlaxCLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n>>> processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"np\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooler_output = outputs.pooler_output  # pooled CLS states\n```"]