# DreamBooth

> 原文链接：[https://huggingface.co/docs/diffusers/training/dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth)

[DreamBooth](https://huggingface.co/papers/2208.12242)是一种训练技术，通过仅对主题或风格的少量图像进行训练来更新整个扩散模型。它通过将提示中的特殊单词与示例图像关联起来工作。

如果您在具有有限vRAM的GPU上进行训练，应尝试在训练命令中启用`gradient_checkpointing`和`mixed_precision`参数。您还可以通过使用[xFormers](../optimization/xformers)的内存高效注意力来减少内存占用。JAX/Flax训练也支持在TPU和GPU上进行高效训练，但不支持梯度检查点或xFormers。如果您想要使用Flax更快地进行训练，则应该具有>30GB内存的GPU。

本指南将探讨[train_dreambooth.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py)脚本，以帮助您更熟悉它，并了解如何为自己的用例进行调整。

在运行脚本之前，请确保您从源代码安装库：

```py
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

导航到包含训练脚本的示例文件夹，并安装所需的依赖项以供您使用的脚本：

PyTorchFlax

```py
cd examples/dreambooth
pip install -r requirements.txt
```

🤗 Accelerate是一个帮助您在多个GPU/TPU上训练或使用混合精度的库。它将根据您的硬件和环境自动配置您的训练设置。查看🤗 Accelerate [快速入门](https://huggingface.co/docs/accelerate/quicktour)以了解更多。

初始化🤗 Accelerate环境：

```py
accelerate config
```

要设置默认的🤗 Accelerate环境而不选择任何配置：

```py
accelerate config default
```

或者，如果您的环境不支持交互式shell，比如笔记本，您可以使用：

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

最后，如果您想在自己的数据集上训练模型，请查看[创建用于训练的数据集](create_dataset)指南，了解如何创建适用于训练脚本的数据集。

以下部分突出了训练脚本中重要的部分，以帮助您了解如何修改它，但并未详细涵盖脚本的每个方面。如果您有兴趣了解更多，请随时阅读[脚本](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py)，并告诉我们如果您有任何问题或疑虑。

## 脚本参数

DreamBooth对训练超参数非常敏感，很容易过拟合。阅读[使用🧨 Diffusers训练稳定扩散的DreamBooth](https://huggingface.co/blog/dreambooth)博客文章，了解不同主题的推荐设置，以帮助您选择适当的超参数。

训练脚本提供了许多参数，用于自定义您的训练运行。所有参数及其描述均在[`parse_args()`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L228)函数中找到。这些参数设置了默认值，应该可以很好地开箱即用，但如果您愿意，也可以在训练命令中设置自己的值。

例如，以bf16格式进行训练：

```py
accelerate launch train_dreambooth.py \
    --mixed_precision="bf16"
```

一些基本和重要的参数需要了解和指定：

+   --pretrained_model_name_or_path：Hub上模型的名称或预训练模型的本地路径

+   --instance_data_dir：包含训练数据集（示例图像）的文件夹路径

+   --instance_prompt：包含示例图像的特殊单词的文本提示

+   --train_text_encoder：是否还要训练文本编码器

+   --output_dir：保存训练好的模型的位置

+   --push_to_hub：是否将训练好的模型推送到Hub

+   `--checkpointing_steps`：保存检查点的频率，这在训练中断时很有用，您可以通过在训练命令中添加 `--resume_from_checkpoint` 从该检查点继续训练

### Min-SNR 权重策略

[Min-SNR](https://huggingface.co/papers/2303.09556) 权重策略可以通过重新平衡损失来帮助训练，以实现更快的收敛。训练脚本支持预测 `epsilon`（噪音）或 `v_prediction`，但 Min-SNR 与两种预测类型兼容。这种权重策略仅由 PyTorch 支持，在 Flax 训练脚本中不可用。

添加 `--snr_gamma` 参数，并将其设置为推荐值 5.0：

```py
accelerate launch train_dreambooth.py \
  --snr_gamma=5.0
```

### 先前保留损失

先前的保留损失是一种方法，它利用模型自己生成的样本来帮助学习如何生成更多样化的图像。因为这些生成的样本图像属于您提供的图像相同的类别，它们帮助模型保留对该类别已学习的知识，以及如何利用已知的关于该类别的知识来生成新的组合。

+   `--with_prior_preservation`：是否使用先前保留损失

+   `--prior_loss_weight`：控制模型对先前保留损失的影响

+   `--class_data_dir`：包含生成的类别样本图像的文件夹路径

+   `--class_prompt`：描述生成样本图像类别的文本提示

```py
accelerate launch train_dreambooth.py \
  --with_prior_preservation \
  --prior_loss_weight=1.0 \
  --class_data_dir="path/to/class/images" \
  --class_prompt="text prompt describing class"
```

### 训练文本编码器

为了提高生成输出的质量，您还可以训练文本编码器以及 UNet。这需要额外的内存，您需要至少具有 24GB 的 vRAM 的 GPU。如果您有必要的硬件，那么训练文本编码器会产生更好的结果，特别是在生成面部图像时。通过以下选项启用此选项：

```py
accelerate launch train_dreambooth.py \
  --train_text_encoder
```

## 训练脚本

DreamBooth 自带其自己的数据集类：

+   [`DreamBoothDataset`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L604)：预处理图像和类别图像，并为训练令牌化提示

+   [`PromptDataset`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L738)：生成提示嵌入以生成类别图像

如果您启用了[先前保留损失](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L842)，则在此处生成类别图像：

```py
sample_dataset = PromptDataset(args.class_prompt, num_new_images)
sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)

sample_dataloader = accelerator.prepare(sample_dataloader)
pipeline.to(accelerator.device)

for example in tqdm(
    sample_dataloader, desc="Generating class images", disable=not accelerator.is_local_main_process
):
    images = pipeline(example["prompt"]).images
```

接下来是 [`main()`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L799) 函数，负责设置用于训练的数据集和训练循环本身。脚本加载 [tokenizer](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L898)、[调度器和模型](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L912C1-L912C1)：

```py
# Load the tokenizer
if args.tokenizer_name:
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)
elif args.pretrained_model_name_or_path:
    tokenizer = AutoTokenizer.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="tokenizer",
        revision=args.revision,
        use_fast=False,
    )

# Load scheduler and models
noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
text_encoder = text_encoder_cls.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
)

if model_has_vae(args):
    vae = AutoencoderKL.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision
    )
else:
    vae = None

unet = UNet2DConditionModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="unet", revision=args.revision
)
```

然后，是时候[创建训练数据集](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L1073)和从 `DreamBoothDataset` 创建 DataLoader 了：

```py
train_dataset = DreamBoothDataset(
    instance_data_root=args.instance_data_dir,
    instance_prompt=args.instance_prompt,
    class_data_root=args.class_data_dir if args.with_prior_preservation else None,
    class_prompt=args.class_prompt,
    class_num=args.num_class_images,
    tokenizer=tokenizer,
    size=args.resolution,
    center_crop=args.center_crop,
    encoder_hidden_states=pre_computed_encoder_hidden_states,
    class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,
    tokenizer_max_length=args.tokenizer_max_length,
)

train_dataloader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=args.train_batch_size,
    shuffle=True,
    collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),
    num_workers=args.dataloader_num_workers,
)
```

最后，[训练循环](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L1151) 处理剩余步骤，如将图像转换为潜在空间，向输入添加噪音，预测噪音残差，并计算损失。

如果您想了解训练循环的工作原理，请查看 [理解管道、模型和调度器](../using-diffusers/write_own_pipeline) 教程，该教程解析了去噪过程的基本模式。

## 启动脚本

您现在可以启动训练脚本了！🚀

对于这个指南，您将下载一些[狗](https://huggingface.co/datasets/diffusers/dog-example)的图像并将它们存储在一个目录中。但请记住，如果您愿意，您可以创建和使用自己的数据集（请参阅[创建用于训练的数据集](create_dataset)指南）。

```py
from huggingface_hub import snapshot_download

local_dir = "./dog"
snapshot_download(
    "diffusers/dog-example",
    local_dir=local_dir,
    repo_type="dataset",
    ignore_patterns=".gitattributes",
)
```

将环境变量`MODEL_NAME`设置为Hub上的模型ID或本地模型的路径，`INSTANCE_DIR`设置为您刚下载狗图片的路径，`OUTPUT_DIR`设置为您想要保存模型的路径。您将使用`sks`作为将训练绑定到的特殊词。

如果您有兴趣跟随训练过程，可以在训练进行时定期保存生成的图像。将以下参数添加到训练命令中：

```py
--validation_prompt="a photo of a sks dog"
--num_validation_images=4
--validation_steps=100
```

在启动脚本之前还有一件事！根据您拥有的GPU，您可能需要启用某些优化来训练DreamBooth。

16GB12GB8GB

在16GB GPU上，您可以使用bitsandbytes 8位优化器和梯度检查点来帮助您训练DreamBooth模型。安装bitsandbytes：

```py
pip install bitsandbytes
```

然后，将以下参数添加到您的训练命令中：

```py
accelerate launch train_dreambooth.py \
  --gradient_checkpointing \
  --use_8bit_adam \
```

PyTorchFlax

```py
export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export INSTANCE_DIR="./dog"
export OUTPUT_DIR="path_to_saved_model"

accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt="a photo of sks dog" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=400 \
  --push_to_hub
```

训练完成后，您可以使用新训练的模型进行推理！

在训练完成之前就迫不及待地尝试您的模型进行推理？🤭 确保您已安装最新版本的🤗 Accelerate。

```py
from diffusers import DiffusionPipeline, UNet2DConditionModel
from transformers import CLIPTextModel
import torch

unet = UNet2DConditionModel.from_pretrained("path/to/model/checkpoint-100/unet")

# if you have trained with `--args.train_text_encoder` make sure to also load the text encoder
text_encoder = CLIPTextModel.from_pretrained("path/to/model/checkpoint-100/checkpoint-100/text_encoder")

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", unet=unet, text_encoder=text_encoder, dtype=torch.float16,
).to("cuda")

image = pipeline("A photo of sks dog in a bucket", num_inference_steps=50, guidance_scale=7.5).images[0]
image.save("dog-bucket.png")
```

PyTorchFlax

```py
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained("path_to_saved_model", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
image = pipeline("A photo of sks dog in a bucket", num_inference_steps=50, guidance_scale=7.5).images[0]
image.save("dog-bucket.png")
```

## LoRA

LoRA是一种用于显著减少可训练参数数量的训练技术。因此，训练速度更快，存储结果权重更容易，因为它们要小得多（~100MB）。使用[train_dreambooth_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py)脚本来使用LoRA进行训练。

有关LoRA训练脚本的更多详细信息，请参阅[LoRA训练](lora)指南。

## Stable Diffusion XL

Stable Diffusion XL（SDXL）是一个强大的文本到图像模型，可以生成高分辨率图像，并在其架构中添加了第二个文本编码器。使用[train_dreambooth_lora_sdxl.py](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py)脚本来训练一个带有LoRA的SDXL模型。

有关SDXL训练脚本的更多详细信息，请参阅[SDXL训练](sdxl)指南。

## 下一步

恭喜您训练成功DreamBooth模型！要了解如何使用您的新模型，以下指南可能会有所帮助：

+   如果您使用LoRA训练了您的模型，学习如何[加载DreamBooth](../using-diffusers/loading_adapters)模型进行推理。
