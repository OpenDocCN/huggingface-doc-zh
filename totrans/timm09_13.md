# 优化

> 原始文本：[`huggingface.co/docs/timm/reference/optimizers`](https://huggingface.co/docs/timm/reference/optimizers)

此页面包含了`timm`中包含的学习率优化器的 API 参考文档。

## 优化器

### 工厂函数

#### `timm.optim.create_optimizer`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L182)

```py
( args model filter_bias_and_bn = True )
```

用于向后兼容的旧优化器工厂。注意：新代码请使用 create_optimizer_v2。

#### `timm.optim.create_optimizer_v2`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/optim_factory.py#L193)

```py
( model_or_params opt: str = 'sgd' lr: typing.Optional[float] = None weight_decay: float = 0.0 momentum: float = 0.9 foreach: typing.Optional[bool] = None filter_bias_and_bn: bool = True layer_decay: typing.Optional[float] = None param_group_fn: typing.Optional[typing.Callable] = None **kwargs )
```

参数

+   `model_or_params`（nn.Module）- 包含要优化的参数的模型 opt - 要创建的优化器的名称 lr - 初始学习率 weight_decay - 优化器应用的权重衰减 momentum - 动量用于基于动量的优化器（其他可能使用 kwargs 中的 betas）foreach - 如果为 True，则启用/禁用 foreach（多张量）操作。如果为 None，则选择安全默认值 filter_bias_and_bn - 从权重衰减中过滤出偏置、bn 和其他 1d 参数 **kwargs - 传递的额外优化器特定 kwargs

创建一个优化器。

TODO：目前传入模型并选择所有参数进行优化。为了更通用的用途，需要一个允许选择要优化的参数和 lr 组的接口之一：

+   一个过滤器 fn 接口，以一种与 weight_decay 兼容的方式进一步将参数分组

+   暴露参数接口，并由调用者决定

### 优化器类

### `class timm.optim.AdaBelief`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L6)

```py
( params lr = 0.001 betas = (0.9, 0.999) eps = 1e-16 weight_decay = 0 amsgrad = False decoupled_decay = True fixed_decay = False rectify = True degenerated_to_sgd = True )
```

参数

+   `params`（可迭代）- 要优化的参数的可迭代对象或定义参数组的字典

+   `lr`（浮点数，可选）- 学习率（默认值：1e-3）

+   `betas`（Tuple[float, float]，可选）- 用于计算梯度及其平方的运行平均值的系数（默认值：（0.9，0.999））

+   `eps`（浮点数，可选）- 添加到分母以提高数值稳定性的项（默认值：1e-16）

+   `weight_decay`（浮点数，可选）- 权重衰减（L2 惩罚）（默认值：0）

+   `amsgrad`（布尔值，可选）- 是否使用来自论文`On the Convergence of Adam and Beyond`_ 的 AMSGrad 变体（默认值：False）

+   `decoupled_decay`（布尔值，可选）- （默认值：True）如果设置为 True，则优化器使用类似于 AdamW 的解耦权重衰减

+   `fixed_decay`（布尔值，可选）- （默认值：False）当设置 weight*decouple 为 True 时使用。当 fixed_decay == True 时，权重衰减为$W*{new} = W*{old} - W*{old} \times decay$。当 fixed*decay == False 时，权重衰减为$W*{new} = W*{old} - W*{old} \times decay \times lr$。请注意，在这种情况下，权重衰减比率随学习率（lr）减少。

+   `rectify`（布尔值，可选）- （默认值：True）如果设置为 True，则执行类似于 RAdam 的矫正更新

+   `degenerated_to_sgd`（布尔值，可选）（默认值为 True）如果设置为 True，则在梯度方差较高时执行 SGD 更新

实现 AdaBelief 算法。修改自 PyTorch 中的 Adam

参考：AdaBelief Optimizer，通过对观察到的梯度的信念来调整步长，NeurIPS 2020

有关推荐超参数的完整表格，请参见[`github.com/juntang-zhuang/Adabelief-Optimizer’`](https://github.com/juntang-zhuang/Adabelief-Optimizer') 例如 EfficientNet 的 train/args，请参见这些 gists

+   训练脚本链接：[`gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037`](https://gist.github.com/juntang-zhuang/0a501dd51c02278d952cf159bc233037)

+   args.yaml 链接：[`gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3`](https://gist.github.com/juntang-zhuang/517ce3c27022b908bb93f78e4f786dc3)

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adabelief.py#L89)

```py
( closure = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Adafactor`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L16)

```py
( params lr = None eps = 1e-30 eps_scale = 0.001 clip_threshold = 1.0 decay_rate = -0.8 betas = None weight_decay = 0.0 scale_parameter = True warmup_init = False )
```

参数

+   `params`（可迭代对象）— 要优化的参数或定义参数组的字典的可迭代对象

+   `lr`（浮点数，可选）— 外部学习率（默认值：None）

+   `eps`（元组[浮点数，浮点数]）— 正则化常数，用于平方梯度和参数比例（默认值：（1e-30，1e-3））

+   `clip_threshold`（浮点数）— 最终梯度更新的均方根阈值（默认值：1.0）

+   `decay_rate`（浮点数）— 用于计算平方梯度的运行平均值的系数（默认值：-0.8）

+   `beta1`（浮点数）— 用于计算梯度的运行平均值的系数（默认值：None）

+   `weight_decay`（浮点数，可选）— 权重衰减（L2 惩罚）（默认值：0）

+   `scale_parameter`（布尔值）— 如果为 True，则学习率将按参数的均方根缩放（默认值：True）

+   `warmup_init`（布尔值）— 时间相关的学习率计算取决于是否正在使用热身初始化（默认值：False）

实现了 Adafactor 算法。此实现基于：`Adafactor：具有次线性内存成本的自适应学习率`（参见[`arxiv.org/abs/1804.04235`](https://arxiv.org/abs/1804.04235)）

请注意，此优化器根据*scale_parameter*、*relative_step*和*warmup_init*选项内部调整学习率。

要使用手动（外部）学习率调度，您应该设置`scale_parameter=False`和`relative_step=False`。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adafactor.py#L79)

```py
( closure = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Adahessian`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L9)

```py
( params lr = 0.1 betas = (0.9, 0.999) eps = 1e-08 weight_decay = 0.0 hessian_power = 1.0 update_each = 1 n_samples = 1 avg_conv_kernel = False )
```

参数

+   `params`（可迭代对象）— 要优化的参数或定义参数组的字典的可迭代对象

+   `lr`（浮点数，可选）— 学习率（默认值：0.1）

+   `betas`（（浮点数，浮点数），可选）— 用于计算梯度和平方海森迹的运行平均值的系数（默认值：（0.9，0.999））

+   `eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）

+   `weight_decay`（浮点数，可选）— 权重衰减（L2 惩罚）（默认值：0.0）

+   `hessian_power`（浮点数，可选）— 海森迹的指数（默认值：1.0）

+   `update_each`（整数，可选）— 仅在*此*步骤数后计算海森迹近似（以节省时间）（默认值：1）

+   `n_samples`（整数，可选）— 用于近似海森迹的`z`采样次数（默认值：1）

实现了“ADAHESSIAN：一种自适应的二阶优化器”中的 AdaHessian 算法

#### `get_params`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L58)

```py
( )
```

获取所有具有梯度的 param_groups 中的所有参数

#### `set_hessian`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L74)

```py
( )
```

计算 Hutchinson 近似的海森迹并为每个可训练参数累积它。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L102)

```py
( closure = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包（默认值为 None）

执行单个优化步骤。

#### `zero_hessian`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adahessian.py#L65)

```py
( )
```

将累积的海森迹清零。

### `class timm.optim.AdamP`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamp.py#L43)

```py
( params lr = 0.001 betas = (0.9, 0.999) eps = 1e-08 weight_decay = 0 delta = 0.1 wd_ratio = 0.1 nesterov = False )
```

### `class timm.optim.AdamW`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L12)

```py
( params lr = 0.001 betas = (0.9, 0.999) eps = 1e-08 weight_decay = 0.01 amsgrad = False )
```

参数

+   `params`（可迭代对象）—要优化的参数或定义参数组的字典

+   `lr`（float，可选）—学习率（默认值：1e-3）

+   `betas`（Tuple[float, float]，可选）—用于计算梯度及其平方的运行平均值的系数（默认值：（0.9，0.999））

+   `eps`（float，可选）—添加到分母以提高数值稳定性的项（默认值：1e-8）

+   `weight_decay`（float，可选）—权重衰减系数（默认值：1e-2）

+   `amsgrad`（布尔值，可选）—是否使用来自论文`Adam 及其收敛性之外`的 AMSGrad 变体的算法（默认值：False）

实现 AdamW 算法。

最初的 Adam 算法是在`Adam：随机优化方法`中提出的。AdamW 变体是在`解耦权重衰减正则化`中提出的。

.. _Adam：随机优化方法：[`arxiv.org/abs/1412.6980`](https://arxiv.org/abs/1412.6980) .. _ 解耦权重衰减正则化：[`arxiv.org/abs/1711.05101`](https://arxiv.org/abs/1711.05101) .. _Adam 及其收敛性之外：[`openreview.net/forum?id=ryQu7f-RZ`](https://openreview.net/forum?id=ryQu7f-RZ)

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/adamw.py#L58)

```py
( closure = None )
```

参数

+   `closure`（可调用对象，可选）—重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Lamb`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L60)

```py
( params lr = 0.001 bias_correction = True betas = (0.9, 0.999) eps = 1e-06 weight_decay = 0.01 grad_averaging = True max_grad_norm = 1.0 trust_clip = False always_adapt = False )
```

参数

+   `params`（可迭代对象）—要优化的参数或定义参数组的字典。

+   `lr`（float，可选）—学习率。（默认值：1e-3）

+   `betas`（Tuple[float, float]，可选）—用于计算梯度及其范数的运行平均值的系数。（默认值：（0.9，0.999））

+   `eps`（float，可选）—添加到分母以提高数值稳定性的项。（默认值：1e-8）

+   `weight_decay`（float，可选）—权重衰减（L2 惩罚）（默认值：0）

+   `grad_averaging`（bool，可选）—在计算梯度的运行平均值时是否应用（1-beta2）（默认值：True）

+   `max_grad_norm`（float，可选）—用于裁剪全局梯度范数的值（默认值：1.0）

+   `trust_clip`（bool）—启用 LAMBC 信任比例裁剪（默认值：False）

+   `always_adapt`（布尔值，可选）—将自适应学习率应用于 0.0 权重衰减参数（默认值：False）

实现了 FuseLAMB（NvLamb 变体）优化器的纯 pytorch 变体，来自 apex.optimizers.FusedLAMB 参考：[`github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py`](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py)

LAMB 是在`大批量优化深度学习：在 76 分钟内训练 BERT`中提出的。

.. _ 大批量优化深度学习 - 在 76 分钟内训练 BERT：[`arxiv.org/abs/1904.00962`](https://arxiv.org/abs/1904.00962) .. _Adam 及其收敛性之外：[`openreview.net/forum?id=ryQu7f-RZ`](https://openreview.net/forum?id=ryQu7f-RZ)

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lamb.py#L96)

```py
( closure = None )
```

参数

+   `closure`（可调用对象，可选）—重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Lars`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L17)

```py
( params lr = 1.0 momentum = 0 dampening = 0 weight_decay = 0 nesterov = False trust_coeff = 0.001 eps = 1e-08 trust_clip = False always_adapt = False )
```

参数

+   `params`（可迭代对象）—要优化的参数或定义参数组的字典。

+   `lr`（float，可选）—学习率（默认值：1.0）。

+   `momentum`（float，可选）—动量因子（默认值：0）

+   `weight_decay`（float，可选）—权重衰减（L2 惩罚）（默认值：0）

+   `dampening`（浮点数，可选）— 动量的阻尼（默认值：0）

+   `nesterov`（布尔值，可选）— 启用 Nesterov 动量（默认值：False）

+   `trust_coeff`（浮点数）— 用于计算自适应 lr / trust_ratio 的信任系数（默认值：0.001）

+   `eps`（浮点数）— 除法分母的 eps（默认值：1e-8）

+   `trust_clip`（布尔值）— 启用 LARC 信任比例剪裁（默认值：False）

+   `always_adapt`（布尔值）— 总是应用 LARS LR 适应，否则仅当组权重衰减！= 0 时（默认值：False）

PyTorch 的 LARS

论文：`卷积网络的大批量训练` - [`arxiv.org/pdf/1708.03888.pdf`](https://arxiv.org/pdf/1708.03888.pdf)

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lars.py#L75)

```py
( closure = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Lookahead`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/lookahead.py#L15)

```py
( base_optimizer alpha = 0.5 k = 6 )
```

### `class timm.optim.MADGRAD`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L24)

```py
( params: typing.Any lr: float = 0.01 momentum: float = 0.9 weight_decay: float = 0 eps: float = 1e-06 decoupled_decay: bool = False )
```

参数

+   `params`（可迭代对象）— 要优化的参数或定义参数组的字典。

+   `lr`（浮点数）— 学习率（默认值：1e-2）。

+   `momentum`（浮点数）— 动量值范围为[0,1)（默认值：0.9）。

+   `weight_decay`（浮点数）— 权重衰减，即 L2 惩罚（默认值：0）。

+   `eps`（浮点数）— 添加到根操作之外的分母以提高数值稳定性的项（默认值：1e-6）。

MADGRAD_: 一种用于随机优化的动量化，自适应，双平均梯度方法。

.. _MADGRAD: [`arxiv.org/abs/2101.11075`](https://arxiv.org/abs/2101.11075)

MADGRAD 是一种通用优化器，可以用来替代 SGD 或 Adam，可能会更快地收敛并更好地泛化。目前仅支持 GPU。通常，可以使用与 SGD 或 Adam 相同的学习率调度。总体学习率不能与任一方法相比，并且应由超参数扫描确定。

MADGRAD 需要比其他方法更少的权重衰减，通常甚至可以为零。在 SGD 或 Adam 的 beta1 中使用的动量值也可以在这里使用。

在稀疏问题上，weight_decay 和 momentum 都应该设置为 0。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/madgrad.py#L85)

```py
( closure: typing.Union[typing.Callable[[], float], NoneType] = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.Nadam`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L7)

```py
( params lr = 0.002 betas = (0.9, 0.999) eps = 1e-08 weight_decay = 0 schedule_decay = 0.004 )
```

参数

+   `params`（可迭代对象）— 要优化的参数或定义参数组的字典

+   `lr`（浮点数，可选）— 学习率（默认值：2e-3）

+   `betas`（Tuple[float, float]，可选）— 用于计算梯度及其平方的运行平均值的系数

+   `eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）

+   `weight_decay`（浮点数，可选）— 权重衰减（L2 惩罚）（默认值：0）

+   `schedule_decay`（浮点数，可选）— 动量调度衰减（默认值：4e-3）

实现 Nadam 算法（基于 Nesterov 动量的 Adam 变体）。

提出了`将 Nesterov 动量纳入 Adam`__。

**[`cs229.stanford.edu/proj2015/054_report.pdf`](http://cs229.stanford.edu/proj2015/054_report.pdf)** [`www.cs.toronto.edu/~fritz/absps/momentum.pdf`](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)

最初取自：[`github.com/pytorch/pytorch/pull/1408`](https://github.com/pytorch/pytorch/pull/1408) 注意：可能存在潜在问题，但在某些问题上效果很好。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nadam.py#L43)

```py
( closure = None )
```

参数

+   `closure`（可调用，可选）— 重新评估模型并返回损失的闭包。

执行单个优化步骤。

### `class timm.optim.NvNovoGrad`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L13)

```py
( params lr = 0.001 betas = (0.95, 0.98) eps = 1e-08 weight_decay = 0 grad_averaging = False amsgrad = False )
```

参数

+   `params`（可迭代对象）— 要优化的参数的可迭代对象或定义参数组的字典

+   `lr`（浮点数，可选）— 学习率（默认值：1e-3）

+   `betas`（Tuple[float, float]，可选）— 用于计算梯度及其平方的运行平均值的系数（默认值：（0.95，0.98））

+   `eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-8）

+   `weight_decay`（浮点数，可选）— 权重衰减（L2 惩罚）（默认值：0）grad_averaging — 梯度平均

+   `amsgrad`（布尔值，可选）— 是否使用来自论文`On the Convergence of Adam and Beyond`_ 的 AMSGrad 变体的算法（默认值：False）

实现 Novograd 算法。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/nvnovograd.py#L54)

```py
( closure = None )
```

参数

+   `closure`（可调用对象，可选）— 重新评估模型的闭包

+   `and` 返回损失。—

执行单个优化步骤。

### `class timm.optim.RAdam`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/radam.py#L10)

```py
( params lr = 0.001 betas = (0.9, 0.999) eps = 1e-08 weight_decay = 0 )
```

### `class timm.optim.RMSpropTF`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L14)

```py
( params lr = 0.01 alpha = 0.9 eps = 1e-10 weight_decay = 0 momentum = 0.0 centered = False decoupled_decay = False lr_in_momentum = True )
```

参数

+   `params`（可迭代对象）— 要优化的参数的可迭代对象或定义参数组的字典

+   `lr`（浮点数，可选）— 学习率（默认值：1e-2）

+   `momentum`（浮点数，可选）— 动量因子（默认值：0）

+   `alpha`（浮点数，可选）— 平滑（衰减）常数（默认值：0.9）

+   `eps`（浮点数，可选）— 添加到分母以提高数值稳定性的项（默认值：1e-10）

+   `centered`（布尔值，可选）— 如果为`True`，计算中心化的 RMSProp，梯度通过其方差的估计进行归一化

+   `weight_decay`（浮点数，可选）— 权重衰减（L2 惩罚）（默认值：0）

+   `decoupled_decay`（布尔值，可选）— 根据[`arxiv.org/abs/1711.05101`](https://arxiv.org/abs/1711.05101)进行解耦的权重衰减

+   `lr_in_momentum`（布尔值，可选）— 学习率缩放包含在动量缓冲区更新中，与 Tensorflow 中的默认值相同

实现 RMSprop 算法（TensorFlow 风格 epsilon）

注意：这是 PyTorch RMSprop 的直接剪切和粘贴，eps 应用在平方根之前，并对 Tensorflow 进行了一些其他修改以更接近匹配超参数。

值得注意的变化包括：

1.  Epsilon 应用在平方根内

1.  square_avg 初始化为 1

1.  在动量缓冲区中累积的更新的 LR 缩放

由 G. Hinton 在他的[课程](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)中提出。

中心化版本首次出现在[使用递归神经网络生成序列](https://arxiv.org/pdf/1308.0850v5.pdf)中。

#### `step`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/rmsprop_tf.py#L72)

```py
( closure = None )
```

参数

+   `closure`（可调用对象，可选）— 重新评估模型并返回损失。

执行单个优化步骤。

### `class timm.optim.SGDP`

[<来源>](https://github.com/huggingface/pytorch-image-models/blob/v0.9.12/timm/optim/sgdp.py#L19)

```py
( params lr = <required parameter> momentum = 0 dampening = 0 weight_decay = 0 nesterov = False eps = 1e-08 delta = 0.1 wd_ratio = 0.1 )
```
