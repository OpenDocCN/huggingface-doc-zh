["```py\n( enabled: bool = True cache_enabled: bool = None )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import AutocastKwargs\n\nkwargs = AutocastKwargs(cache_enabled=True)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( dim: int = 0 broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False check_reduction: bool = False gradient_as_bucket_view: bool = False static_graph: bool = False )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import DistributedDataParallelKwargs\n\nkwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( backend: Literal = 'MSAMP' opt_level: Literal = 'O2' margin: int = 0 interval: int = 1 fp8_format: Literal = 'E4M3' amax_history_len: int = 1 amax_compute_algo: Literal = 'most_recent' override_linear_precision: Tuple = (False, False, False) )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import FP8RecipeKwargs\n\nkwargs = FP8RecipeKwargs(backend=\"te\", fp8_format=\"HYBRID\")\naccelerator = Accelerator(mixed_precision=\"fp8\", kwargs_handlers=[kwargs])\n```", "```py\nkwargs = FP8RecipeKwargs(backend=\"msamp\", optimization_level=\"02\")\n```", "```py\n( init_scale: float = 65536.0 growth_factor: float = 2.0 backoff_factor: float = 0.5 growth_interval: int = 2000 enabled: bool = True )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import GradScalerKwargs\n\nkwargs = GradScalerKwargs(backoff_filter=0.25)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( backend: Optional = 'nccl' init_method: Optional = None timeout: timedelta = datetime.timedelta(seconds=1800) )\n```", "```py\nfrom datetime import timedelta\nfrom accelerate import Accelerator\nfrom accelerate.utils import InitProcessGroupKwargs\n\nkwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=800))\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```"]