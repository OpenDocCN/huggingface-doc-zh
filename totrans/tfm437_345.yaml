- en: FLAVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flava](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flava)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision
    Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang
    Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and
    Douwe Kiela and is accepted at CVPR 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: The paper aims at creating a single unified foundation model which can work
    across vision, language as well as vision-and-language multimodal tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*State-of-the-art vision and vision-and-language models rely on large-scale
    visio-linguistic pretraining for obtaining good performance on a variety of downstream
    tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
    (with earlier fusion) but not both; and they often only target specific modalities
    or tasks. A promising direction would be to use a single holistic universal model,
    as a “foundation”, that targets all modalities at once — a true vision and language
    foundation model should be good at vision tasks, language tasks, and cross- and
    multi-modal vision and language tasks. We introduce FLAVA as such a model and
    demonstrate impressive performance on a wide range of 35 tasks spanning these
    target modalities.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [aps](https://huggingface.co/aps). The original
    code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).
  prefs: []
  type: TYPE_NORMAL
- en: FlavaConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L468)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_config` (`dict`, *optional*) — Dictionary of configuration options used
    to initialize [FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_config` (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projection_dim` (`int`, *optional*, defaults to 512) — Dimentionality of text
    and image projection layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — The inital
    value of the *logit_scale* paramter. Default is used as per the original FLAVA/CLIP
    implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ce_ignore_index` (`int`, *optional*, defaults to -100) — Cross entropy index
    to ignore.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mim_weight` (`float`, *optional*, defaults to 1.0) — Weight to be assigned
    to MIM (Masked Image Modeling) unimodal loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlm_weight` (`float`, *optional*, defaults to 1.0) — Weight to be assigned
    to MLM (Masked Language Modeling) unimodal loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_contrastive_weight` (`float`, *optional*, defaults to 1.0) — Weight
    to be assigned to global contrastive cross-alignment loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`itm_weight` (`float`, *optional*, defaults to 1.0) — Weight to be assigned
    to image-text matching multimodal loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mmm_image_weight` (`float`, *optional*, defaults to 1.0) — Weight to be assigned
    to MMM loss’s image part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mmm_text_weight` (`float`, *optional*, defaults to 1.0) — Weight to be assigned
    to MMM loss’s text part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_backprop_contrastive` (`bool`, *optional*, defaults to `True`) — Whether
    to use global backpropgation through all workers in contrastive loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_unmasked_multimodal_encoder` (`bool`, *optional*, defaults to `True`)
    — Whether to skip running unmasked multimodal encoder whose outputs are not used
    by FLAVA losses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_loss` (`bool`, *optional*, defaults to `True`) — Whether to return
    loss or not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)
    is the configuration class to store the configuration of a [FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel).
    It is used to instantiate FLAVA model according to the specified arguments, defining
    the text model, image model, image codebook and multimodal model configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full) architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#### `from_configs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L742)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)'
  prefs: []
  type: TYPE_NORMAL
- en: An instance of a configuration object
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a [FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)
    (or a derived class) from flava text model configuration, flava image model configuration,
    flava multimodal model and flava codebook model configuration.
  prefs: []
  type: TYPE_NORMAL
- en: FlavaTextConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaTextConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L150)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    BERT model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids` passed when calling [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).
    Note that even though text encoder allows `token_type_ids`’s value as 2, for text-only
    pretraining and fine-tuning, only 1 is used similar to RoBERTa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed
    to model is 77.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — Type
    of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).
    It is used to instantiate an FLAVA model according to the specified arguments,
    defining the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating a configuration with the defaults will yield a similar configuration
    to that of the FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: FlavaImageConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaImageConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 224) — The size (resolution) of
    each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — The number of input channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_token` (`bool`, *optional*, defaults to `True`) — Whether to use a mask
    token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 8192) — Vocabulary size of the
    [FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook)
    used in conjunction with [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)
    for MIM (Masked Image Modeling) loss for FLAVA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).
    It is used to instantiate an FLAVA model according to the specified arguments,
    defining the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating a configuration with the defaults will yield a similar configuration
    to that of the FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: FlavaMultimodalConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaMultimodalConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L280)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 6) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to the queries, keys and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cls_token` (`bool`, *optional*, defaults to `True`) — Whether to use an
    extra CLS token for multimodal settings. Usually needed by the FLAVA model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel).
    It is used to instantiate an FLAVA model according to the specified arguments,
    defining the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating a configuration with the defaults will yield a similar configuration
    to that of the FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: FlavaImageCodebookConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaImageCodebookConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L383)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: FlavaProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` ([FlavaImageProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageProcessor),
    *optional*) — The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` ([BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast),
    *optional*) — The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a FLAVA processor which wraps a FLAVA image processor and a FLAVA
    tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[FlavaProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaProcessor)
    offers all the functionalities of [FlavaImageProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageProcessor)
    and [BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast).
    See the `__call__()` and [decode()](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaProcessor.decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to BertTokenizerFast’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to BertTokenizerFast’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: FlavaFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/feature_extraction_flava.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: FlavaImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/image_processing_flava.py#L135)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of the image after resizing. Can be overridden by the `size` parameter
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the images. Can be overridden by the `do_center_crop` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of image after the center crop `(crop_size["height"], crop_size["width"])`.
    Can be overridden by the `crop_size` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Can be overridden by the `rescale_factor`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_image_mask` (`bool`, *optional*, defaults to `False`) — Whether to
    return the image mask. Can be overridden by the `return_image_mask` parameter
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_size_patches` (`int`, *optional*, defaults to 14) — Number of patches
    in the image in height and width direction. 14x14 = 196 total patches. Can be
    overridden by the `input_size_patches` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_mask_patches` (`int`, *optional*, defaults to 75) — Total number of
    patches that should be masked. Can be overridden by the `total_mask_patches` parameter
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_min_patches` (`int`, *optional*, defaults to 16) — Minimum number
    of patches that should be masked. Can be overridden by the `mask_group_min_patches`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_max_patches` (`int`, *optional*) — Maximum number of patches that
    should be masked. Can be overridden by the `mask_group_max_patches` parameter
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_min_aspect_ratio` (`float`, *optional*, defaults to 0.3) — Minimum
    aspect ratio of the mask window. Can be overridden by the `mask_group_min_aspect_ratio`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_max_aspect_ratio` (`float`, *optional*) — Maximum aspect ratio
    of the mask window. Can be overridden by the `mask_group_max_aspect_ratio` parameter
    in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_resize` (`bool`, *optional*, defaults to `True`) — Whether to
    resize the input for codebook to a certain. Can be overridden by the `codebook_do_resize`
    parameter in `preprocess`. `codebook_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_size` (`Dict[str, int]`, *optional*, defaults to `{"height" -- 224,
    "width": 224}`): Resize the input for codebook to the given size. Can be overridden
    by the `codebook_size` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.LANCZOS`)
    — Resampling filter to use if resizing the codebook image. Can be overridden by
    the `codebook_resample` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_center_crop` (`bool`, *optional*, defaults to `True`) — Whether
    to crop the input for codebook at the center. If the input size is smaller than
    `codebook_crop_size` along any edge, the image is padded with 0’s and then center
    cropped. Can be overridden by the `codebook_do_center_crop` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_crop_size` (`Dict[str, int]`, *optional*, defaults to `{"height"
    -- 224, "width": 224}`): Desired output size for codebook input when applying
    center-cropping. Can be overridden by the `codebook_crop_size` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_rescale` (`bool`, *optional*, defaults to `True`) — Whether to
    rescale the input for codebook by the specified scale `codebook_rescale_factor`.
    Can be overridden by the `codebook_do_rescale` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`)
    — Defines the scale factor to use if rescaling the codebook image. Can be overridden
    by the `codebook_rescale_factor` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_map_pixels` (`bool`, *optional*, defaults to `True`) — Whether
    to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden
    by the `codebook_do_map_pixels` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_normalize` (`bool`, *optional*, defaults to `True`) — Whether
    or not to normalize the input for codebook with `codebook_image_mean` and `codebook_image_std`.
    Can be overridden by the `codebook_do_normalize` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_image_mean` (`Optional[Union[float, Iterable[float]]]`, *optional*,
    defaults to `[0, 0, 0]`) — The sequence of means for each channel, to be used
    when normalizing images for codebook. Can be overridden by the `codebook_image_mean`
    parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_image_std` (`Optional[Union[float, Iterable[float]]]`, *optional*,
    defaults to `[0.5, 0.5, 0.5]`) — The sequence of standard deviations for each
    channel, to be used when normalizing images for codebook. Can be overridden by
    the `codebook_image_std` parameter in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Flava image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/image_processing_flava.py#L447)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Size of the
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`int`, *optional*, defaults to `self.resample`) — Resampling filter
    to use if resizing the image. This can be one of the enum `PILImageResampling`,
    Only has an effect if `do_resize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_center_crop` (`bool`, *optional*, defaults to `self.do_center_crop`) —
    Whether to center crop the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_size` (`Dict[str, int]`, *optional*, defaults to `self.crop_size`) —
    Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`float`, *optional*, defaults to `self.rescale_factor`) —
    Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_image_mask` (`bool`, *optional*, defaults to `self.return_image_mask`)
    — Whether to return the image mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_size_patches` (`int`, *optional*, defaults to `self.input_size_patches`)
    — Size of the patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_mask_patches` (`int`, *optional*, defaults to `self.total_mask_patches`)
    — Total number of patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_min_patches` (`int`, *optional*, defaults to `self.mask_group_min_patches`)
    — Minimum number of patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_max_patches` (`int`, *optional*, defaults to `self.mask_group_max_patches`)
    — Maximum number of patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_min_aspect_ratio` (`float`, *optional*, defaults to `self.mask_group_min_aspect_ratio`)
    — Minimum aspect ratio of the patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_group_max_aspect_ratio` (`float`, *optional*, defaults to `self.mask_group_max_aspect_ratio`)
    — Maximum aspect ratio of the patches to extract from the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_codebook_pixels` (`bool`, *optional*, defaults to `self.return_codebook_pixels`)
    — Whether to return the codebook pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_resize` (`bool`, *optional*, defaults to `self.codebook_do_resize`)
    — Whether to resize the codebook pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_size` (`Dict[str, int]`, *optional*, defaults to `self.codebook_size`)
    — Size of the codebook pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_resample` (`int`, *optional*, defaults to `self.codebook_resample`)
    — Resampling filter to use if resizing the codebook pixels. This can be one of
    the enum `PILImageResampling`, Only has an effect if `codebook_do_resize` is set
    to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_center_crop` (`bool`, *optional*, defaults to `self.codebook_do_center_crop`)
    — Whether to center crop the codebook pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_crop_size` (`Dict[str, int]`, *optional*, defaults to `self.codebook_crop_size`)
    — Size of the center crop of the codebook pixels. Only has an effect if `codebook_do_center_crop`
    is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_rescale` (`bool`, *optional*, defaults to `self.codebook_do_rescale`)
    — Whether to rescale the codebook pixels values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_rescale_factor` (`float`, *optional*, defaults to `self.codebook_rescale_factor`)
    — Rescale factor to rescale the codebook pixels by if `codebook_do_rescale` is
    set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_map_pixels` (`bool`, *optional*, defaults to `self.codebook_do_map_pixels`)
    — Whether to map the codebook pixels values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_do_normalize` (`bool`, *optional*, defaults to `self.codebook_do_normalize`)
    — Whether to normalize the codebook pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_image_mean` (`float` or `List[float]`, *optional*, defaults to `self.codebook_image_mean`)
    — Codebook pixels mean to normalize the codebook pixels by if `codebook_do_normalize`
    is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_image_std` (`float` or `List[float]`, *optional*, defaults to `self.codebook_image_std`)
    — Codebook pixels standard deviation to normalize the codebook pixels by if `codebook_do_normalize`
    is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: FlavaForPreTraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaForPreTraining`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1714)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_codebook` (`nn.Module`) — If passed, the image codebook will be set
    to this. Otherwise. it will be initialized using the image_codebook_config defined
    in the config first as the first parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FLAVA model for pretraining which outputs losses, embeddings, logits and
    transformer outputs.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1764)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids_masked` (`torch.LongTensor` of shape `(batch_size, text_seq_len)`)
    — Indices of input sequence tokens in the vocabulary. These ones are the masked
    version of the original task to be used with MLM. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)
    along with `DataCollatorForMaskedLanguageModeling`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_len)`) — Indices
    of input sequence tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_len)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interpolate_pos_encoding` (`bool`, *optional*) — Whether to interpolate the
    pre-trained position encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_attention_mask` (`torch.FloatTensor` of shape `(batch_size, image_num_patches)`,
    *optional*) — Mask to avoid performing attention on padding token indices specifically
    for images. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_unmasked_multimodal_encoder` (*bool*, *optional*) — Skip any calculations
    for multimodal encoder for unmasked inputs. FLAVA pretraining doesn’t need unmasked
    multimodal embeddings or outputs as of now.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlm_labels` (`torch.LongTensor` of shape `(batch_size, text_seq_len)`, *optional*)
    — Labels for computing the left-to-right language and multimodal masked modeling
    loss (next word prediction). Indices should be in `[-100, 0, ..., text_config.vocab_size
    - 1]` (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
    (masked), the loss is only computed for the tokens with labels in `[0, ..., text_config.vocab_size
    - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mim_labels` (`torch.LongTensor` of shape `(batch_size, image_num_patches)`,
    *optional*) — Labels for computing the image and multimodal masked modeling loss.
    Indices should be in `[-100, 0, ..., image_config.vocab_size - 1]`. Tokens with
    indices set to `-100` are ignored (masked), the loss is only computed for the
    tokens with labels in `[0, ..., image_config.vocab_size - 1]`. If not passed,
    they are generated automatically using the image codebook assigned to the model.
    By default, it uses [FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook).
    See [FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook)
    to understand how to generate mim_labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`itm_labels` (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*) —
    Labels for computing the image-text matching loss. 0 means the pairs don’t match
    and 1 means they match. The pairs with 0 will be skipped for calculation of MMM
    and global contrastive losses as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_loss` (`bool`, *optional*, default to None) — Whether to return calculated
    loss or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, text_seq_len)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples —
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput` or a
    tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.flava.configuration_flava.FlavaConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor`, *optional*, returned when `return_loss` is True)
    — Total loss calculated for this model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss_info` (`FlavaLosses`) — Detailed info for FLAVA Pretraining losses. Check
    `FlavaLosses` class description for the information on the keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `pixel_values` are present) — The image embeddings which
    are basically the pooled output of [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_output` (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values`
    are present) — The output of the [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `input_ids` are present) — The text embeddings which
    are basically the pooled output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_output` (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids`
    are present) — The output of the [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `input_ids` and `pixel_values` are present and `skip_unmasked_multimodal_encoder`
    is `None` or `False`) — The multimodal embeddings which are basically the pooled
    output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_output` (`BaseModelOutputWithPooling`, returned when `input_ids`
    and `pixel_values` are present and `skip_unmasked_multimodal_encoder` is `None`
    or `False`) — The output of the [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_masked_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `pixel_values` are present) — The image embeddings which
    are basically the pooled output of [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).
    Uses `bool_masked_pos` to create masked images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_masked_output` (`BaseModelOutputWithPooling`, *optional*, returned when
    `pixel_values` are present) — The output of the [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).
    Uses `bool_masked_pos` to create masked images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_masked_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `input_ids_masked` are present) — The text embeddings
    which are basically the pooled output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_masked_output` (`BaseModelOutputWithPooling`, *optional*, returned when
    `input_ids_masked` are present) — The output of the [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_masked_embeddings` (`torch.FloatTensor` of shape `(batch_size,
    output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present)
    — The multimodal embeddings which are basically the pooled output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_masked_output` (`BaseModelOutputWithPooling`, returned when `input_ids_masked`
    and `pixel_values` are present) — The output of the [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mim_logits` (`torch.FloatTensor` of shape `(batch_size, num_image_patches,
    image_vocab_size)` or of shape `(total_masked_patches, image_vocab_size)` , *optional*,
    returned when `pixel_values` are present and `input_ids_masked` are not) — The
    logits for MIM unimodal loss. Uses `book_masked_pos` to get masked patches. The
    flattened output is returned when `bool_masked_pos` has some of the patches masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlm_logits` (`torch.FloatTensor` of shape `(batch_size, text_seq_length, text_vocab_size)`
    or of shape `(total_masked_seq_length, text_vocab_size)`, *optional*, returned
    when `input_ids_masked` are present and `pixel_values` are not) — The logits for
    MLM unimodal loss. The flattened output is returned when `input_ids_masked` has
    some of the tokens masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`itm_logits` (`torch.FloatTensor` of shape `(batch_size, 2)`, *optional*, returned
    when `input_ids_masked` and `pixel_values` are present) — The logits for ITM loss.
    Note that ITM loss is calculated on masked pairs in FLAVA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mmm_image_logits` (`torch.FloatTensor` of shape `(batch_size, num_image_patches,
    image_vocab_size)` or of shape`(total_masked_patches, image_vocab_size)`, *optional*,
    returned when `pixel_values` and `input_ids_masked` are present) — The logits
    for MMM image multimodal loss. Uses `book_masked_pos` to get masked patches. The
    flattened output is returned when `bool_masked_pos` has some of the patches masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mmm_text_logits` (`torch.FloatTensor` of shape `(batch_size, text_seq_length,
    text_vocab_size)` or of shape `(`(total_masked_seq_length, text_vocab_size)`),
    *optional*, returned when` pixel_values`and`input_ids_masked`are present) -- The
    logits for MMM text multimodal loss. The flattened output is returned when`input_ids_masked`
    has some of the tokens masked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contrastive_logits_per_image` (`torch.FloatTensor` of shape `(image_batch_size,
    text_batch_size)`) — The scaled dot product scores between `image_embeddings`
    and `text_embeddings` but passed through FLAVA’s `image_projection` and `text_projection`
    layers respectively. This represents the image-text similarity scores. This is
    calculated on unmasked images and texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contrastive_logits_per_text` (`torch.FloatTensor` of shape `(text_batch_size,
    image_batch_size)`) — The scaled dot product scores between `text_embeddings`
    and `image_embeddings` but passed through FLAVA’s `text_projection` and `image_projection`
    layers respectively. This is calculated on unmasked images and texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [FlavaForPreTraining](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaForPreTraining)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: FlavaModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare FLAVA Model transformer outputting raw hidden-states without any specific
    head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1323)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interpolate_pos_encoding` (`bool`, *optional*) — Whether to interpolate the
    pre-trained position encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, image_num_patches +
    text_seq_len)`) — Indices of input sequence tokens in the vocabulary. Indices
    can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, image_num_patches
    + text_seq_len)`, *optional*) — Segment token indices to indicate first and second
    portions of the inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, image_num_patches
    + text_seq_len)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_multimodal_encoder` (*bool*, *optional*) — Skip any calculations for
    multimodal encoder. Useful if multimodal encoding is not going to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.flava.modeling_flava.FlavaModelOutput` or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.flava.modeling_flava.FlavaModelOutput` or a tuple of
    `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration (`<class 'transformers.models.flava.configuration_flava.FlavaConfig'>`)
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`image_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `pixel_values` are present) — The image embeddings which
    are basically the pooled output of [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_output` (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values`
    are present) — The output of the [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `input_ids` are present) — The text embeddings which
    are basically the pooled output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_output` (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids`
    are present) — The output of the [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_dim)`,
    *optional*, returned when `input_ids` and `pixel_values` are present and `skip_multimodal_encoder`
    is `None` or `False`) — The multimodal embeddings which are basically the pooled
    output of [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodal_output` (`BaseModelOutputWithPooling`, returned when `input_ids`
    and `pixel_values` are present and `skip_multimodal_encoder` is `None` or `False`)
    — The output of the [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_text_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1229)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, text_seq_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_image_features`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1273)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interpolate_pos_encoding` (`bool`, *optional*) — Whether to interpolate the
    pre-trained position encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, image_num_patches)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: FlavaImageCodebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaImageCodebook`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1500)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaImageCodebookConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebookConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FLAVA’s image codebook model inspired from DALL-E’s original encoder. Outputs
    raw hidden states and can be used to generate image tokens for an image based
    on DALL-E’s vocab. Used to generate labels for MIM. Use `get_codebook_indices`
    to get image tokens for an image.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1590)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_codebook_indices`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1558)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#### `get_codebook_probs`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1586)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: FlavaTextModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaTextModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L976)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare FLAVA Text Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1011)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, text_seq_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: FlavaImageModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaImageModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L877)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare FLAVA Image Model transformer outputting raw hidden-states without
    any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L914)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor).
    See [FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bool_masked_pos` (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`)
    — Boolean masked positions. Indicates which patches are masked (1) and which aren’t
    (0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interpolate_pos_encoding` (`bool`, *optional*) — Whether to interpolate the
    pre-trained position encodings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, image_num_patches)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: FlavaMultimodalModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.FlavaMultimodalModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1081)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare FLAVA Multimodal Model transformer outputting raw hidden-states without
    any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_states` (`torch.FloatTensor` of shape `(batch_size, image_num_patches
    + text_seq_len, hidden_size)`) — The concatenated hidden states of unimodal encoders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, image_num_patches
    + text_seq_len)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
