["```py\n( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_pad: bool = True pad_size: int = 8 **kwargs )\n```", "```py\n( images: Union do_rescale: Optional = None rescale_factor: Optional = None do_pad: Optional = None pad_size: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image_size = 64 patch_size = 1 num_channels = 3 num_channels_out = None embed_dim = 180 depths = [6, 6, 6, 6, 6, 6] num_heads = [6, 6, 6, 6, 6, 6] window_size = 8 mlp_ratio = 2.0 qkv_bias = True hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 drop_path_rate = 0.1 hidden_act = 'gelu' use_absolute_embeddings = False initializer_range = 0.02 layer_norm_eps = 1e-05 upscale = 2 img_range = 1.0 resi_connection = '1conv' upsampler = 'pixelshuffle' **kwargs )\n```", "```py\n>>> from transformers import Swin2SRConfig, Swin2SRModel\n\n>>> # Initializing a Swin2SR caidas/swin2sr-classicalsr-x2-64 style configuration\n>>> configuration = Swin2SRConfig()\n\n>>> # Initializing a model (with random weights) from the caidas/swin2sr-classicalsr-x2-64 style configuration\n>>> model = Swin2SRModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config )\n```", "```py\n( pixel_values: FloatTensor head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, Swin2SRModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"huggingface/cats-image\")\n>>> image = dataset[\"test\"][\"image\"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n>>> model = Swin2SRModel.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n\n>>> inputs = image_processor(image, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 180, 488, 648]\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.ImageSuperResolutionOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> import numpy as np\n>>> from PIL import Image\n>>> import requests\n\n>>> from transformers import AutoImageProcessor, Swin2SRForImageSuperResolution\n\n>>> processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n>>> model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n\n>>> url = \"https://huggingface.co/spaces/jjourney1125/swin2sr/resolve/main/samples/butterfly.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> # prepare image for the model\n>>> inputs = processor(image, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> output = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n>>> output = np.moveaxis(output, source=0, destination=-1)\n>>> output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n>>> # you can visualize `output` with `Image.fromarray`\n```"]