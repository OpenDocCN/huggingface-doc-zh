- en: Custom hardware for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_hardware](https://huggingface.co/docs/transformers/v4.37.2/en/perf_hardware)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/317.8a4247f9.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: The hardware you use to run model training and inference can have a big effect
    on performance. For a deep dive into GPUs make sure to check out Tim Dettmer’s
    excellent [blog post](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at some practical advice for GPU setups.
  prefs: []
  type: TYPE_NORMAL
- en: GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you train bigger models you have essentially three options:'
  prefs: []
  type: TYPE_NORMAL
- en: bigger GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: more CPU and NVMe (offloaded to by [DeepSpeed-Infinity](main_classes/deepspeed#nvme-support))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start at the case where you have a single GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Power and Cooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you bought an expensive high end GPU make sure you give it the correct power
    and sufficient cooling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Power**:'
  prefs: []
  type: TYPE_NORMAL
- en: Some high end consumer GPU cards have 2 and sometimes 3 PCI-E 8-Pin power sockets.
    Make sure you have as many independent 12V PCI-E 8-Pin cables plugged into the
    card as there are sockets. Do not use the 2 splits at one end of the same cable
    (also known as pigtail cable). That is if you have 2 sockets on the GPU, you want
    2 PCI-E 8-Pin cables going from your PSU to the card and not one that has 2 PCI-E
    8-Pin connectors at the end! You won’t get the full performance out of your card
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Each PCI-E 8-Pin power cable needs to be plugged into a 12V rail on the PSU
    side and can supply up to 150W of power.
  prefs: []
  type: TYPE_NORMAL
- en: Some other cards may use a PCI-E 12-Pin connectors, and these can deliver up
    to 500-600W of power.
  prefs: []
  type: TYPE_NORMAL
- en: Low end cards may use 6-Pin connectors, which supply up to 75W of power.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally you want the high-end PSU that has stable voltage. Some lower quality
    ones may not give the card the stable voltage it needs to function at its peak.
  prefs: []
  type: TYPE_NORMAL
- en: And of course the PSU needs to have enough unused Watts to power the card.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cooling**:'
  prefs: []
  type: TYPE_NORMAL
- en: When a GPU gets overheated it will start throttling down and will not deliver
    full performance and it can even shutdown if it gets too hot.
  prefs: []
  type: TYPE_NORMAL
- en: It’s hard to tell the exact best temperature to strive for when a GPU is heavily
    loaded, but probably anything under +80C is good, but lower is better - perhaps
    70-75C is an excellent range to be in. The throttling down is likely to start
    at around 84-90C. But other than throttling performance a prolonged very high
    temperature is likely to reduce the lifespan of a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next let’s have a look at one of the most important aspects when having multiple
    GPUs: connectivity.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-GPU Connectivity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you use multiple GPUs the way cards are inter-connected can have a huge
    impact on the total training time. If the GPUs are on the same physical node,
    you can run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'and it will tell you how the GPUs are inter-connected. On a machine with dual-GPU
    and which are connected with NVLink, you will most likely see something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'on a different machine w/o NVLink we may see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The report includes this legend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So the first report `NV2` tells us the GPUs are interconnected with 2 NVLinks,
    and the second report `PHB` we have a typical consumer-level PCIe+Bridge setup.
  prefs: []
  type: TYPE_NORMAL
- en: Check what type of connectivity you have on your setup. Some of these will make
    the communication between cards faster (e.g. NVLink), others slower (e.g. PHB).
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the type of scalability solution used, the connectivity speed could
    have a major or a minor impact. If the GPUs need to sync rarely, as in DDP, the
    impact of a slower connection will be less significant. If the GPUs need to send
    messages to each other often, as in ZeRO-DP, then faster connectivity becomes
    super important to achieve faster training.
  prefs: []
  type: TYPE_NORMAL
- en: NVlink
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[NVLink](https://en.wikipedia.org/wiki/NVLink) is a wire-based serial multi-lane
    near-range communications link developed by Nvidia.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each new generation provides a faster bandwidth, e.g. here is a quote from
    [Nvidia Ampere GA102 GPU Architecture](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: Third-Generation NVLink® GA102 GPUs utilize NVIDIA’s third-generation NVLink
    interface, which includes four x4 links, with each link providing 14.0625 GB/sec
    bandwidth in each direction between two GPUs. Four links provide 56.25 GB/sec
    bandwidth in each direction, and 112.5 GB/sec total bandwidth between two GPUs.
    Two RTX 3090 GPUs can be connected together for SLI using NVLink. (Note that 3-Way
    and 4-Way SLI configurations are not supported.)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So the higher `X` you get in the report of `NVX` in the output of `nvidia-smi
    topo -m` the better. The generation will depend on your GPU architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compare the execution of a gpt2 language model training over a small sample
    of wikitext.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are:'
  prefs: []
  type: TYPE_NORMAL
- en: '| NVlink | Time |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| Y | 101s |'
  prefs: []
  type: TYPE_TB
- en: '| N | 131s |'
  prefs: []
  type: TYPE_TB
- en: You can see that NVLink completes the training ~23% faster. In the second benchmark
    we use `NCCL_P2P_DISABLE=1` to tell the GPUs not to use NVLink.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full benchmark code and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Hardware: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi
    topo -m`) Software: `pytorch-1.8-to-be` + `cuda-11.0` / `transformers==4.3.0.dev0`'
  prefs: []
  type: TYPE_NORMAL
