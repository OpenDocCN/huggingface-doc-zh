["```py\naccelerate config\n```", "```py\naccelerate launch my_script.py --args_to_my_script\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n gradient_accumulation_steps: 1\n gradient_clipping: 1.0\n offload_optimizer_device: none\n offload_param_device: none\n zero3_init_flag: true\n zero_stage: 2\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```", "```py\naccelerate launch examples/nlp_example.py --mixed_precision fp16\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```", "```py\naccelerate launch examples/nlp_example.py --mixed_precision fp16\n```", "```py\n`zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning\n`gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.\n`gradient_clipping`: Enable gradient clipping with value.\n`offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.\n`offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.\n`zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.\n`zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.\n`mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training.\n```", "```py\naccelerate config\n```", "```py\naccelerate launch my_script.py --args_to_my_script\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n deepspeed_config_file: /home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage2_config.json\n zero3_init_flag: true\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\",\n            \"torch_adam\": true,\n            \"adam_w_mode\": true\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"contiguous_gradients\": true\n    },\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```", "```py\naccelerate launch examples/by_feature/deepspeed_with_config_support.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name \"wikitext\" \\\n--dataset_config_name \"wikitext-2-raw-v1\" \\\n--block_size 128 \\\n--output_dir \"./clm/clm_deepspeed_stage2_accelerate\" \\\n--learning_rate 5e-4 \\\n--per_device_train_batch_size 24 \\\n--per_device_eval_batch_size 24 \\\n--num_train_epochs 3 \\\n--with_tracking \\\n--report_to \"wandb\"\\\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n deepspeed_config_file: /home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage3_offload_config.json\n zero3_init_flag: true\ndistributed_type: DEEPSPEED\nfsdp_config: {}\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nuse_cpu: false\n```", "```py\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupDecayLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\",\n            \"total_num_steps\": \"auto\"\n        }\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"sub_group_size\": 1e9,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\"\n    },\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```", "```py\naccelerate launch examples/by_feature/deepspeed_with_config_support.py \\\n--config_name \"gpt2-large\" \\\n--tokenizer_name \"gpt2-large\" \\\n--dataset_name \"wikitext\" \\\n--dataset_config_name \"wikitext-2-raw-v1\" \\\n--block_size 128 \\\n--output_dir \"./clm/clm_deepspeed_stage3_offload_accelerate\" \\\n--learning_rate 5e-4 \\\n--per_device_train_batch_size 32 \\\n--per_device_eval_batch_size 32 \\\n--num_train_epochs 3 \\\n--with_tracking \\\n--report_to \"wandb\"\\\n```", "```py\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"reduce_bucket_size\": \"auto\",\n\n        \"zero_quantized_weights\": true,\n        \"zero_hpz_partition_size\": 8,\n        \"zero_quantized_gradients\": true,\n\n        \"contiguous_gradients\": true,\n        \"overlap_comm\": true\n    }\n}\n```", "```py\n     # Creates Dummy Optimizer if `optimizer` was specified in the config file else creates Adam Optimizer\n     optimizer_cls = (\n         torch.optim.AdamW\n         if accelerator.state.deepspeed_plugin is None\n         or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n         else DummyOptim\n     )\n     optimizer = optimizer_cls(optimizer_grouped_parameters, lr=args.learning_rate)\n\n     # Creates Dummy Scheduler if `scheduler` was specified in the config file else creates `args.lr_scheduler_type` Scheduler\n     if (\n         accelerator.state.deepspeed_plugin is None\n         or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n     ):\n         lr_scheduler = get_scheduler(\n             name=args.lr_scheduler_type,\n             optimizer=optimizer,\n             num_warmup_steps=args.num_warmup_steps,\n             num_training_steps=args.max_train_steps,\n         )\n     else:\n         lr_scheduler = DummyScheduler(\n             optimizer, total_num_steps=args.max_train_steps, warmup_num_steps=args.num_warmup_steps\n         )\n    ```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.state import AcceleratorState\n\ndef main():\n    accelerator = Accelerator()\n    accelerator.print(f\"{AcceleratorState()}\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\ncommand_file: null\ncommands: null\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  gradient_accumulation_steps: 1\n  gradient_clipping: 1.0\n  offload_optimizer_device: 'cpu'\n  offload_param_device: 'cpu'\n  zero3_init_flag: true\n  zero3_save_16bit_model: true\n  zero_stage: 3\n  deepspeed_config_file: 'ds_config.json'\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\ngpu_ids: null\nmachine_rank: 0\nmain_process_ip: null\nmain_process_port: null\nmain_training_function: main\nmegatron_lm_config: {}\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_name: null\ntpu_zone: null\nuse_cpu: false\n```", "```py\n{\n    \"bf16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"stage3_gather_16bit_weights_on_model_save\": false,\n        \"offload_optimizer\": {\n            \"device\": \"none\"\n        },\n        \"offload_param\": {\n            \"device\": \"none\"\n        }\n    },\n    \"gradient_clipping\": 1.0,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": 10,\n    \"steps_per_print\": 2000000\n}\n```", "```py\nValueError: When using `deepspeed_config_file`, the following accelerate config variables will be ignored:\n['gradient_accumulation_steps', 'gradient_clipping', 'zero_stage', 'offload_optimizer_device', 'offload_param_device',\n'zero3_save_16bit_model', 'mixed_precision'].\nPlease specify them appropriately in the DeepSpeed config file.\nIf you are using an accelerate config file, remove others config variables mentioned in the above specified list.\nThe easiest method is to create a new config following the questionnaire via `accelerate config`.\nIt will only ask for the necessary config variables when using `deepspeed_config_file`.\n```", "```py\n$ accelerate config\n-------------------------------------------------------------------------------------------------------------------------------\nIn which compute environment are you running?\nThis machine\n-------------------------------------------------------------------------------------------------------------------------------\nWhich type of machine are you using?\nmulti-GPU\nHow many different machines will you use (use more than 1 for multi-node training)? [1]:\nDo you wish to optimize your script with torch dynamo?[yes/NO]:\nDo you want to use DeepSpeed? [yes/NO]: yes\nDo you want to specify a json file to a DeepSpeed config? [yes/NO]: yes\nPlease enter the path to the json DeepSpeed config file: ds_config.json\nDo you want to enable `deepspeed.zero.Init` when using ZeRO Stage-3 for constructing massive models? [yes/NO]: yes\nHow many GPU(s) should be used for distributed training? [1]:4\naccelerate configuration saved at ds_config_sample.yaml\n```", "```py\ncompute_environment: LOCAL_MACHINE\ndeepspeed_config:\n  deepspeed_config_file: ds_config.json\n  zero3_init_flag: true\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_backend: 'NO'\nfsdp_config: {}\nmachine_rank: 0\nmain_training_function: main\nmegatron_lm_config: {}\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\nuse_cpu: false\n```", "```py\nDistributed environment: DEEPSPEED  Backend: nccl\nNum processes: 4\nProcess index: 0\nLocal process index: 0\nDevice: cuda:0\nMixed precision type: bf16\nds_config: {'bf16': {'enabled': True}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': False, 'offload_optimizer': {'device': 'none'}, 'offload_param': {'device': 'none'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 10, 'steps_per_print': inf, 'fp16': {'enabled': False}}\n```", "```py\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"zero_optimization\": {\n        \"stage\": \"auto\",\n        \"stage3_gather_16bit_weights_on_model_save\": \"auto\",\n        \"offload_optimizer\": {\n            \"device\": \"auto\"\n        },\n        \"offload_param\": {\n            \"device\": \"auto\"\n        }\n    },\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": \"auto\",\n    \"steps_per_print\": 2000000\n}\n```", "```py\nDistributed environment: DEEPSPEED  Backend: nccl\nNum processes: 4\nProcess index: 0\nLocal process index: 0\nDevice: cuda:0\nMixed precision type: fp16\nds_config: {'bf16': {'enabled': False}, 'zero_optimization': {'stage': 3, 'stage3_gather_16bit_weights_on_model_save': True, 'offload_optimizer': {'device': 'nvme'}, 'offload_param': {'device': 'cpu'}}, 'gradient_clipping': 1.0, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 5, 'steps_per_print': inf, 'fp16': {'enabled': True, 'auto_cast': True}}\n```", "```py\n    unwrapped_model = accelerator.unwrap_model(model)\n\n    # New Code #\n    # Saves the whole/unpartitioned fp16 model when in ZeRO Stage-3 to the output directory if\n    # `stage3_gather_16bit_weights_on_model_save` is True in DeepSpeed Config file or\n    # `zero3_save_16bit_model` is True in DeepSpeed Plugin.\n    # For Zero Stages 1 and 2, models are saved as usual in the output directory.\n    # The model name saved is `pytorch_model.bin`\n    unwrapped_model.save_pretrained(\n        args.output_dir,\n        is_main_process=accelerator.is_main_process,\n        save_function=accelerator.save,\n        state_dict=accelerator.get_state_dict(model),\n    )\n    ```", "```py\n    success = model.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)\n    status_msg = \"checkpointing: PATH={}, ckpt_id={}\".format(PATH, ckpt_id)\n    if success:\n        logging.info(f\"Success {status_msg}\")\n    else:\n        logging.warning(f\"Failure {status_msg}\")\n    ```", "```py\n    $ cd /path/to/checkpoint_dir\n    $ ./zero_to_fp32.py . pytorch_model.bin\n    Processing zero checkpoint at global_step1\n    Detected checkpoint of type zero stage 3, world_size: 2\n    Saving fp32 state dict to pytorch_model.bin (total_numel=60506624)\n    ```", "```py\n    from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\n    unwrapped_model = accelerator.unwrap_model(model)\n    fp32_model = load_state_dict_from_zero_checkpoint(unwrapped_model, checkpoint_dir)\n    ```", "```py\n    from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)\n    ```", "```py\nmodel, eval_dataloader = accelerator.prepare(model, eval_dataloader)\n```"]