# 示例

> 原始文本：[`huggingface.co/docs/trl/example_overview`](https://huggingface.co/docs/trl/example_overview)

## 介绍

这些示例应该在以下任何设置中运行（使用相同的脚本）：

+   单 GPU

+   多 GPU（使用 PyTorch 分布式模式）

+   多 GPU（使用 DeepSpeed ZeRO-Offload 阶段 1、2 和 3）

+   fp16（混合精度），fp32（正常精度）或 bf16（bfloat16 精度）

要在这些不同模式中运行它，请首先使用`accelerate config`初始化加速配置

**注意：要训练 4 位或 8 位模型，请运行**

```py
pip install --upgrade trl[quantization]
```

## 加速配置

对于所有示例，您需要生成一个🤗 Accelerate 配置文件：

```py
accelerate config # will prompt you to define the training configuration
```

然后，鼓励使用`accelerate launch`启动作业！

# 维护的示例

| 文件 | 描述 |
| --- | --- |
| [`examples/scripts/sft.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) | 此脚本展示了如何使用`SFTTrainer`来微调模型或适配器以适应目标数据集。 |
| [`examples/scripts/reward_modeling.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/reward_modeling.py) | 此脚本展示了如何使用`RewardTrainer`来在您自己的数据集上训练奖励模型。 |
| [`examples/scripts/ppo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py) | 此脚本展示了如何使用`PPOTrainer`来使用 IMDB 数据集微调情感分析模型 |
| [`examples/scripts/ppo_multi_adapter.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/ppo_multi_adapter.py) | 此脚本展示了如何使用`PPOTrainer`来训练一个带有多个适配器的单个基础模型。需要您先运行奖励模型训练的示例脚本。 |
| [`examples/scripts/stable_diffusion_tuning_example.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/stable_diffusion_tuning_example.py) | 此脚本展示了如何使用 DDPOTrainer 来使用强化学习微调稳定扩散模型。 |

这里还有一些更容易运行的 colab 笔记本，您可以用来开始使用 TRL：

| 文件 | 描述 |
| --- | --- |
| [`examples/notebooks/best_of_n.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/best_of_n.ipynb) | 该笔记本演示了在使用 PPO 微调模型时如何使用 TRL 的“最佳 N”抽样策略。 |
| [`examples/notebooks/gpt2-sentiment.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/gpt2-sentiment.ipynb) | 该笔记本演示了如何在 jupyter 笔记本上重现 GPT2 imdb 情感调整示例。 |
| [`examples/notebooks/gpt2-control.ipynb`](https://github.com/huggingface/trl/tree/main/examples/notebooks/gpt2-control.ipynb) | 该笔记本演示了如何在 jupyter 笔记本上重现 GPT2 情感控制示例。 |

我们还有一些其他示例，虽然维护较少，但可以用作参考：

1.  [research_projects](https://github.com/huggingface/trl/tree/main/examples/research_projects)：查看此文件夹以找到用于使用 TRL 的一些研究项目的脚本（LM 去毒化，Stack-Llama 等）

## 分布式训练

通过在调用`accelerate launch`时提供🤗 Accelerate 配置文件的路径，所有脚本都可以在多个 GPU 上运行。要在一个或多个 GPU 上启动其中一个，请运行以下命令（将`{NUM_GPUS}`替换为您机器上的 GPU 数量，将`--all_arguments_of_the_script`替换为您的参数。）

```py
accelerate launch --config_file=examples/accelerate_configs/multi_gpu.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script
```

您还可以调整🤗 Accelerate 配置文件的参数以满足您的需求（例如在混合精度下训练）。

### 使用 DeepSpeed 进行分布式训练

大多数脚本可以与 DeepSpeed ZeRO-{1,2,3}一起在多个 GPU 上运行，以实现优化器状态、梯度和模型权重的有效分片。要这样做，请运行以下命令（将`{NUM_GPUS}`替换为您机器上的 GPU 数量，将`--all_arguments_of_the_script`替换为您的参数，将`--deepspeed_config`替换为 DeepSpeed 配置文件的路径，例如`examples/deepspeed_configs/deepspeed_zero1.yaml`）：

```py
accelerate launch --config_file=examples/accelerate_configs/deepspeed_zero{1,2,3}.yaml --num_processes {NUM_GPUS} path_to_script.py --all_arguments_of_the_script
```
