- en: Pop2Piano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pop2piano)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/223.78c46da4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">[![Spaces](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/sweetcocoa/pop2piano)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover
    Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.'
  prefs: []
  type: TYPE_NORMAL
- en: Piano covers of pop music are widely enjoyed, but generating them from music
    is not a trivial task. It requires great expertise with playing piano as well
    as knowing different characteristics and melodies of a song. With Pop2Piano you
    can directly generate a cover from a songâ€™s audio waveform. It is the first model
    to directly generate a piano cover from pop audio without melody and chord extraction
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pop2Piano is an encoder-decoder Transformer model based on [T5](https://arxiv.org/pdf/1910.10683.pdf).
    The input audio is transformed to its waveform and passed to the encoder, which
    transforms it to a latent representation. The decoder uses these latent representations
    to generate token ids in an autoregressive way. Each token id corresponds to one
    of four different token types: time, velocity, note and â€˜specialâ€™. The token ids
    are then decoded to their equivalent MIDI file.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Piano covers of pop music are enjoyed by many people. However, the task of
    automatically generating piano covers of pop music is still understudied. This
    is partly due to the lack of synchronized {Pop, Piano Cover} data pairs, which
    made it challenging to apply the latest data-intensive deep learning-based methods.
    To leverage the power of the data-driven approach, we make a large amount of paired
    and synchronized {Pop, Piano Cover} data using an automated pipeline. In this
    paper, we present Pop2Piano, a Transformer network that generates piano covers
    given waveforms of pop music. To the best of our knowledge, this is the first
    model to generate a piano cover directly from pop audio without using melody and
    chord extraction modules. We show that Pop2Piano, trained with our dataset, is
    capable of producing plausible piano covers.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [Susnato Dhar](https://huggingface.co/susnato).
    The original code can be found [here](https://github.com/sweetcocoa/pop2piano).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use Pop2Piano, you will need to install the ðŸ¤— Transformers library, as well
    as the following third party modules:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Please note that you may need to restart your runtime after installation.
  prefs: []
  type: TYPE_NORMAL
- en: Pop2Piano is an Encoder-Decoder based model like T5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pop2Piano can be used to generate midi-audio files for a given audio sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing different composers in `Pop2PianoForConditionalGeneration.generate()`
    can lead to variety of different results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the sampling rate to 44.1 kHz when loading the audio file can give good
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though Pop2Piano was mainly trained on Korean Pop music, it also does pretty
    well on other Western Pop or Hip Hop songs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Example using HuggingFace Dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Example using your own audio file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of processing multiple audio files in batch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor`
    and `Pop2PianoTokenizer`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Pop2PianoConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Pop2PianoConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/configuration_pop2piano.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_size = 2400 composer_vocab_size = 21 d_model = 512 d_kv = 64 d_ff =
    2048 num_layers = 6 num_decoder_layers = None num_heads = 8 relative_attention_num_buckets
    = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon
    = 1e-06 initializer_factor = 1.0 feed_forward_proj = 'gated-gelu' is_encoder_decoder
    = True use_cache = True pad_token_id = 0 eos_token_id = 1 dense_act_fn = 'relu'
    **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 2400) â€” Vocabulary size of the
    `Pop2PianoForConditionalGeneration` model. Defines the number of different tokens
    that can be represented by the `inputs_ids` passed when calling [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**composer_vocab_size** (`int`, *optional*, defaults to 21) â€” Denotes the number
    of composers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d_model** (`int`, *optional*, defaults to 512) â€” Size of the encoder layers
    and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d_kv** (`int`, *optional*, defaults to 64) â€” Size of the key, query, value
    projections per attention head. The `inner_dim` of the projection layer will be
    defined as `num_heads * d_kv`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**d_ff** (`int`, *optional*, defaults to 2048) â€” Size of the intermediate feed
    forward layer in each `Pop2PianoBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_layers** (`int`, *optional*, defaults to 6) â€” Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_decoder_layers** (`int`, *optional*) â€” Number of hidden layers in the
    Transformer decoder. Will use the same value as `num_layers` if not set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_heads** (`int`, *optional*, defaults to 8) â€” Number of attention heads
    for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**relative_attention_num_buckets** (`int`, *optional*, defaults to 32) â€” The
    number of buckets to use for each attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**relative_attention_max_distance** (`int`, *optional*, defaults to 128) â€”
    The maximum distance of the longer sequences for the bucket separation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout_rate** (`float`, *optional*, defaults to 0.1) â€” The ratio for all
    dropout layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_epsilon** (`float`, *optional*, defaults to 1e-6) â€” The epsilon
    used by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_factor** (`float`, *optional*, defaults to 1.0) â€” A factor for
    initializing all weight matrices (should be kept to 1.0, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**feed_forward_proj** (`string`, *optional*, defaults to `"gated-gelu"`) â€”
    Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) â€” Whether or not the
    model should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dense_act_fn** (`string`, *optional*, defaults to `"relu"`) â€” Type of Activation
    Function to be used in `Pop2PianoDenseActDense` and in `Pop2PianoDenseGatedActDense`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration).
    It is used to instantiate a Pop2PianoForConditionalGeneration model according
    to the specified arguments, defining the model architecture. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the Pop2Piano
    [sweetcocoa/pop2piano](https://huggingface.co/sweetcocoa/pop2piano) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Pop2PianoFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Pop2PianoFeatureExtractor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L5)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Call self as a function.
  prefs: []
  type: TYPE_NORMAL
- en: Pop2PianoForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Pop2PianoForConditionalGeneration'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1009)'
  prefs: []
  type: TYPE_NORMAL
- en: '( config: Pop2PianoConfig )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig))
    â€” Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pop2Piano Model with a `language modeling` head on top. This model inherits
    from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1109)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids:
    Optional = None decoder_attention_mask: Optional = None head_mask: Optional =
    None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None
    encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds:
    Optional = None input_features: Optional = None decoder_inputs_embeds: Optional
    = None labels: Optional = None use_cache: Optional = None output_attentions: Optional
    = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’
    [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    â€” Indices of input sequence tokens in the vocabulary. Pop2Piano is a model with
    relative position embeddings so you should be able to pad the inputs on both the
    right and the left. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for detail. [What are input IDs?](../glossary#input-ids) To know more on how to
    prepare `input_ids` for pretraining take a look a [Pop2Pianp Training](./Pop2Piano#training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) â€” Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_input_ids** (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Indices of decoder input sequence tokens in the vocabulary. Indices
    can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are decoder input IDs?](../glossary#decoder-input-ids) Pop2Piano
    uses the `pad_token_id` as the starting token for `decoder_input_ids` generation.
    If `past_key_values` is used, optionally only the last `decoder_input_ids` have
    to be input (see `past_key_values`). To know more on how to prepare'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_attention_mask** (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) â€” Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules in the encoder. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the self-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attn_head_mask** (`torch.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) â€” Mask to nullify selected heads of the cross-attention
    modules in the decoder. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_outputs** (`tuple(tuple(torch.FloatTensor)`, *optional*) â€” Tuple
    consists of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a
    sequence of hidden states at the output of the last layer of the encoder. Used
    in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) â€” Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding. If `past_key_values`
    are used, the user can optionally input only the last `decoder_input_ids` (those
    that donâ€™t have their past key value states given to this model) of shape `(batch_size,
    1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the modelâ€™s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_features** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Does the same task as `inputs_embeds`. If `inputs_embeds`
    is not present but `input_features` is present then `input_features` will be considered
    as `inputs_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**decoder_inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) â€” Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the modelâ€™s internal
    embedding lookup matrix. If `decoder_input_ids` and `decoder_inputs_embeds` are
    both unset, `decoder_inputs_embeds` takes the value of `inputs_embeds`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*) â€” If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) â€” Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) â€” Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) â€” Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) â€” Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[-100, 0, ..., config.vocab_size - 1]`. All labels set to `-100` are ignored
    (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Pop2PianoConfig](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) â€” Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    â€” Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) â€” Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**decoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**cross_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoderâ€™s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**encoder_hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    â€” Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**encoder_attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) â€”
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Pop2PianoForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: '#### generate'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pop2piano/modeling_pop2piano.py#L1217)'
  prefs: []
  type: TYPE_NORMAL
- en: ( input_features attention_mask = None composer = 'composer1' generation_config
    = None **kwargs ) â†’ [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_features** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) â€” This is the featurized version of audio generated
    by `Pop2PianoFeatureExtractor`. attention_mask â€” For batched generation `input_features`
    are padded to have the same shape across all examples. `attention_mask` helps
    to determine which areas were padded and which were not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not padded**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **padded**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**composer** (`str`, *optional*, defaults to `"composer1"`) â€” This value is
    passed to `Pop2PianoConcatEmbeddingToMel` to generate different embeddings for
    each `"composer"`. Please make sure that the composet value is present in `composer_to_feature_token`
    in `generation_config`. For an example please see [https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json](https://huggingface.co/sweetcocoa/pop2piano/blob/main/generation_config.json)
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generation_config** (`~generation.GenerationConfig`, *optional*) â€” The generation
    configuration to be used as base parametrization for the generation call. `**kwargs`
    passed to generate matching the attributes of `generation_config` will override
    them. If `generation_config` is not provided, the default will be used, which
    had the following loading priority: 1) from the `generation_config.json` model
    file, if it exists; 2) from the model configuration. Please note that unspecified
    parameters will inherit [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)â€™s
    default values, whose documentation should be checked to parameterize generation.
    kwargs â€” Ad hoc parametrization of `generate_config` and/or additional model-specific
    kwargs that will be forwarded to the `forward` function of the model. If the model
    is an encoder-decoder model, encoder specific kwargs should not be prefixed and
    decoder specific kwargs should be prefixed with *decoder_*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    or `torch.LongTensor`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    (if `return_dict_in_generate=True` or when `config.return_dict_in_generate=True`)
    or a `torch.FloatTensor`. Since Pop2Piano is an encoder-decoder model (`model.config.is_encoder_decoder=True`),
    the possible [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    types are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[GenerateEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateEncoderDecoderOutput),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GenerateBeamEncoderDecoderOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateBeamEncoderDecoderOutput)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates token ids for midi outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Most generation-controlling parameters are set in `generation_config` which,
    if not passed, will be set to the modelâ€™s default generation configuration. You
    can override any `generation_config` by passing the corresponding parameters to
    generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`. For an overview
    of generation strategies and code examples, check out the [following guide](./generation_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: Pop2PianoTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Pop2PianoTokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L12)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Call self as a function.
  prefs: []
  type: TYPE_NORMAL
- en: Pop2PianoProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.Pop2PianoProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects.py#L19)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Call self as a function.
  prefs: []
  type: TYPE_NORMAL
