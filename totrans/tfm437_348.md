# IDEFICS

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/idefics](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/idefics)

## 概述

IDEFICS模型是由Hugo Laurençon、Lucile Saulnier、Léo Tronchon、Stas Bekman、Amanpreet Singh、Anton Lozhkov、Thomas Wang、Siddharth Karamcheti、Alexander M. Rush、Douwe Kiela、Matthieu Cord、Victor Sanh提出的。

论文摘要如下：

*在自然文档上训练的大型多模型，交替显示图像和文本，比在各种多模基准上训练的图像-文本对模型表现更好，这些基准需要对一个或多个图像进行推理以生成文本。然而，用于训练这些模型的数据集尚未发布，并且收集过程尚未完全指定。我们介绍了OBELICS数据集，这是一个包含来自Common Crawl的141亿个网页、3.53亿个相关图像和1150亿个文本标记的开放式网络规模过滤数据集。我们描述了数据集创建过程，提出了全面的过滤规则，并对数据集内容进行了分析。为了展示OBELISC的可行性，我们在数据集上训练了一个拥有800亿参数的视觉和语言模型，并在各种多模基准上获得了竞争性能。我们发布了用于重现数据集的代码以及数据集本身。*

此模型由[HuggingFaceM4](https://huggingface.co/HuggingFaceM4)贡献。原始代码可以在[这里](INSERT%20LINK%20TO%20GITHUB%20REPO%20HERE)找到。（TODO：目前没有公开链接）。

Transformers中的IDEFICS建模代码用于微调和推理预训练的IDEFICS模型。

要从头开始训练一个新的IDEFICS模型，请使用m4代码库（一旦公开，将提供链接）

## IdeficsConfig

### `class transformers.IdeficsConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/configuration_idefics.py#L161)

```py
( vocab_size = 32000 additional_vocab_size = 0 hidden_size = 4096 intermediate_size = 11008 num_hidden_layers = 32 num_attention_heads = 32 dropout = 0.0 hidden_act = 'silu' initializer_range = 0.02 alpha_initializer = 'zeros' alphas_initializer_range = 0.0 alpha_type = 'float' rms_norm_eps = 1e-06 use_cache = True pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 tie_word_embeddings = False cross_layer_interval = 1 qk_layer_norms = False freeze_text_layers = True freeze_text_module_exceptions = [] freeze_lm_head = False freeze_vision_layers = True freeze_vision_module_exceptions = [] use_resampler = False vision_config = None perceiver_config = None **kwargs )
```

参数

+   `additional_vocab_size` (`int`, *optional`, defaults to 0) — 模型的额外词汇量，通常用于特殊的“”标记。额外的词汇标记始终是可训练的，而常规词汇标记可以冻结或不冻结。

+   `vocab_size` (`int`, *optional*, defaults to 32000) — Idefics模型的词汇量。定义了在调用[~IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel)时可以表示的不同标记的数量。

+   `hidden_size` (`int`, *optional*, defaults to 4096) — 隐藏表示的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 11008) — MLP表示的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 32) — Transformer编码器中隐藏层的数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 32) — Transformer编码器中每个注意力层的注意力头数。

+   `dropout` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"silu"`) — 解码器中的非线性激活函数（函数或字符串）。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。

+   `alpha_initializer` (`str`, *optional*, defaults to `"zeros"`) — alphas的初始化类型。

+   `alphas_initializer_range` (`float`, *optional*, defaults to 0.0) — 初始化Gated Cross Attention中alphas的truncated_normal_initializer的标准差。

+   `alpha_type` (`str`, *optional*, defaults to `"float"`) — 门控alphas应该是向量还是单个浮点数。

+   `rms_norm_eps` (`float`, *optional*, defaults to 1e-6) — rms归一化层使用的epsilon。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在`config.is_decoder=True`时相关。

+   `pad_token_id` (`int`, *optional*, defaults to 0) — 填充标记id。

+   `bos_token_id` (`int`, *optional*, defaults to 1) — 流的开始标记id。

+   `eos_token_id` (`int`, *optional*, defaults to 2) — 流的结束标记id。

+   `tie_word_embeddings(bool,` *optional*, defaults to `False`) — 是否绑定权重嵌入

+   `cross_layer_interval` (`int`, *optional*, default to 1) — 交叉注意力（从文本到图像）层的间隔。

+   `qk_layer_norms` (`bool`, *optional*, defaults to `False`) — 是否在q和k之后添加层归一化

+   `freeze_text_layers` (`bool`, *optional*, defaults to `True`) — 是否冻结文本层

+   `freeze_text_module_exceptions` (`bool`, *optional*, defaults to `[]`) — 当`freeze_text_layers`为`True`时，冻结文本层的异常

+   `freeze_lm_head` (`bool`, *optional*, defaults to `False`) — 是否冻结lm头

+   `freeze_vision_layers` (`bool`, *optional*, defaults to `True`) — 是否冻结视觉层

+   `freeze_vision_module_exceptions` (`bool`, *optional*, defaults to `[]`) — 当`freeze_vision_layers`为`True`时，冻结视觉层的异常

+   `use_resampler` (`bool`, *optional*, defaults to `False`) — 是否使用重采样器

+   `vision_config` (`IdeficsVisionConfig`, *optional*) — 自定义视觉配置或字典

+   `perceiver_config` (`IdeficsPerceiverConfig`, *optional*) — 自定义感知器配置或字典

这是用于存储[IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel)配置的配置类。根据指定的参数实例化一个Idefics模型，定义模型架构。使用默认值实例化配置将产生类似于Idefics-9B的配置。

例如[HuggingFaceM4/idefics-9b](https://huggingface.co/HuggingFaceM4/idefics-9b)

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import IdeficsModel, IdeficsConfig

>>> # Initializing a Idefics idefics-9b style configuration
>>> configuration = IdeficsConfig()

>>> # Initializing a model from the idefics-9b style configuration
>>> model = IdeficsModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## IdeficsModel

### `class transformers.IdeficsModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1064)

```py
( config: IdeficsConfig )
```

参数

+   `config` ([IdeficsConfig](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。config — IdeficsConfig

裸的LLaMA模型输出原始隐藏状态，没有特定的头部。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

由`config.num_hidden_layers`层组成的Transformer解码器。每一层都是一个`IdeficsDecoderLayer`

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1145)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None pixel_values: Optional = None image_encoder_embeddings: Optional = None perceiver_embeddings: Optional = None image_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = False return_dict: Optional = None )
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将被忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记，为1。

    +   对于被`masked`的标记，为0。

    [什么是注意力掩码？](../glossary#attention-mask)

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    如果使用`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（请参阅`past_key_values`）。

    如果要更改填充行为，您应该阅读`modeling_opt._prepare_decoder_attention_mask`并根据您的需求进行修改。有关默认策略的更多信息，请参阅[论文中的图表1](https://arxiv.org/abs/1910.13461)。

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。[什么是位置ID？](../glossary#position-ids)

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可以用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*）— 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（请参阅`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

[IdeficsModel](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## IdeficsForVisionText2Text

### `class transformers.IdeficsForVisionText2Text`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1403)

```py
( config vision_model = None )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/modeling_idefics.py#L1461)

```py
( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None pixel_values: Optional = None image_encoder_embeddings: Optional = None perceiver_embeddings: Optional = None image_attention_mask: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = False return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被“掩码”的标记，

    +   0表示被“掩码”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    如果使用了`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

    如果要更改填充行为，应阅读`modeling_opt._prepare_decoder_attention_mask`并根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

    +   1表示头部未被“掩码”，

    +   0表示头部被“掩码”。

+   `position_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。[什么是位置ID？](../glossary#position-ids)

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（即没有将其过去键值状态提供给此模型的那些）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

    参数 — labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*): 用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有`[0, ..., config.vocab_size]`标签的标记。

返回值

`transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast`或`tuple(torch.FloatTensor)`

一个`transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`），包含根据配置（[IdeficsConfig](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsConfig)）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 一个长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 一个元组，包含形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 一个元组，包含形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `image_hidden_states` (`tuple(torch.FloatTensor)`, *optional*) — 一个元组，包含形状为`(batch_size, num_images, sequence_length, hidden_size)`的`torch.FloatTensor`（用于图像嵌入的输出）。

    由视觉编码器生成的模型的图像隐藏状态，以及可选的感知器

[IdeficsForVisionText2Text](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsForVisionText2Text)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例:

```py
>>> from transformers import AutoTokenizer, IdeficsForVisionText2Text

>>> model = IdeficsForVisionText2Text.from_pretrained("HuggingFaceM4/idefics-9b")
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceM4/idefics-9b")

>>> prompt = "Hey, are you consciours? Can you talk to me?"
>>> inputs = tokenizer(prompt, return_tensors="pt")

>>> # Generate
>>> generate_ids = model.generate(inputs.input_ids, max_length=30)
>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"Hey, are you consciours? Can you talk to me?\nI'm not consciours, but I can talk to you."
```

## IdeficsImageProcessor

### `class transformers.IdeficsImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/image_processing_idefics.py#L51)

```py
( image_size: int = 224 image_mean: Union = None image_std: Union = None image_num_channels: Optional = 3 **kwargs )
```

参数

+   `image_size` (`int`, *可选*, 默认为 224) — 调整到图像大小

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IDEFICS_STANDARD_MEAN`) — 用于归一化图像的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_mean`参数覆盖。可以通过`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IDEFICS_STANDARD_STD`) — 用于归一化图像的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_std`参数覆盖。可以通过`preprocess`方法中的`image_std`参数覆盖。

+   `image_num_channels` (`int`, *可选*, 默认为 3) — 图像通道数。

构建一个Idefics图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/image_processing_idefics.py#L87)

```py
( images: Union image_num_channels: Optional = 3 image_size: Optional = None image_mean: Union = None image_std: Union = None transform: Callable = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像列表。

+   `image_size` (`int`, *可选*, 默认为 `self.image_size`) — 调整到图像大小

+   `image_num_channels` (`int`, *可选*, 默认为 `self.image_num_channels`) — 图像通道数。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IDEFICS_STANDARD_MEAN`) — 用于归一化图像的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_mean`参数覆盖。可以通过`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IDEFICS_STANDARD_STD`) — 用于归一化图像的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_std`参数覆盖。可以通过`preprocess`方法中的`image_std`参数覆盖。

+   `transform` (`Callable`, *可选*, 默认为 `None`) — 可以传递一个接受单个图像的自定义转换函数进行训练。例如，可以使用`torchvision.Compose`来组合多个转换。如果为`None` - 则假定为推断模式 - 然后将应用一组预设的推断特定转换到图像。

预处理一批图像。

## IdeficsProcessor

### `class transformers.IdeficsProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/processing_idefics.py#L108)

```py
( image_processor tokenizer = None image_size = 224 add_end_of_utterance_token = None **kwargs )
```

参数

+   `image_processor` (`IdeficsImageProcessor`) — [IdeficsImageProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsImageProcessor)的一个实例。图像处理器是一个必需的输入。

+   `tokenizer` (`LlamaTokenizerFast`) — [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast)的一个实例。分词器是一个必需的输入。

+   `image_size` (`int`, *可选*, 默认为 224) — 图像大小（假设为正方形图像）

构建一个IDEFICS处理器，将LLama分词器和IDEFICS图像处理器封装成一个处理器。

[IdeficsProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsProcessor) 提供了 [IdeficsImageProcessor](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsImageProcessor) 和 [LlamaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/llama2#transformers.LlamaTokenizerFast) 的所有功能。查看 [**call**()](/docs/transformers/v4.37.2/en/model_doc/idefics#transformers.IdeficsProcessor.__call__) 和 `decode()` 的文档字符串以获取更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/idefics/processing_idefics.py#L149)

```py
( prompts: Union padding: Union = False truncation: Union = None max_length: Optional = None transform: Callable = None add_eos_token = False add_end_of_utterance_token = None debug = False return_tensors: Union = <TensorType.PYTORCH: 'pt'> ) → export const metadata = 'undefined';a dict with entries
```

参数

+   `prompts`（`Union[List[TextInput], [List[List[TextInput]]]]`）— 单个提示或批处理提示的列表 - 请查看参数文档部分结束后的详细描述。

+   `padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为 `False`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引）：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供了单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。

+   `truncation`（`bool`，*可选*）— 激活截断，将输入序列截断为比 `max_length` 更长的序列至 `max_length`。

+   `transform`（`Callable`，*可选*）— 可以传递一个接受单个图像的自定义转换函数进行训练。例如，可以使用 `torchvision.Compose` 来组合多个函数。如果为 `None`，则将应用预设的推理特定转换集合到图像上。

+   `add_eos_token`（`bool`，*可选*，默认为 `False`）— 如果为 True，则在最终提示的末尾添加 `eos_token`

+   `add_end_of_utterance_token`（`bool`，*可选*）— 是否在每个提示的文本输入后自动添加 `<end_of_utterance>`（除非后面跟着一个图像）。如果为 `None`，则将检查分词器，如果在 `additional_special_tokens` 中找到该标记，则值将为 `True`。

+   `debug`（`bool`，*可选*，默认为 `False`）— `True` 值将通过转储有用信息来帮助调试提示生成

+   `return_tensors`（`str` 或 `TensorType`，*可选*，默认为 `TensorType.PYTORCH`）— 要返回的张量类型。可以是以下之一：

    +   `TensorType.PYTORCH` 或 `'pt'`：返回类型为 `torch.Tensor` 的批处理。

返回

一个包含条目的字典

`input_ids`、`attention_mask`、`pixel_values`、`image_attention_mask` 可以直接传递给 `model.generate`

这个方法接受由文本和图像组成的批处理或非批处理提示，并将它们转换为模型训练的提示，并准备图像像素值供模型处理。

详细说明：

`prompts` 中的每个条目都是要原样传递的文本或将被处理的图像。

图像可以是图像对象（`PIL.Image`）或可以检索图像的url。

当处理器遇到图像时，它将在提示中注入 `<fake_token_around_image><image><fake_token_around_image>` 条目。

例子：

```py
checkpoint = "HuggingFaceM4/idefics-9b"
processor = AutoProcessor.from_pretrained(checkpoint)
url = "https://hips.hearstapps.com/hmg-prod/images/cute-photos-of-cats-in-grass-1593184777.jpg"
img = processor.image_processor.fetch_images([url])[0]

prompts = [
    "User:",
    img,
    "Describe this image.
t: An image of two kittens in grass.

    "User:",
    "https://hips.hearstapps.com/hmg-prod/images/dog-puns-1581708208.jpg",
    "Describe this image.
t:",
]

inputs = processor(prompts, return_tensors="pt")
generated_ids = model.generate(**inputs, max_length=100)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

在这个例子中，`prompts` 将被转换为：

```py
<s>User:<fake_token_around_image><image><fake_token_around_image>Describe this image.
Assistant: An image of two kittens in grass.
User:<fake_token_around_image><image><fake_token_around_image>Describe this image.
Assistant:'
```

两个图像将使用 [IdeficsImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__) 方法进行处理，并放置在返回值的 `pixel_values` 字典条目中。

这个例子还说明了图像可以作为对象或文本url传递。可以看到第一个图像作为对象传递，第二个图像作为url传递。

进行训练时：

```py
image_transform = transforms.Compose(
    [
        transforms.RandomResizedCrop(
            (w, h), scale=(0.9, 1.0), interpolation=transforms.InterpolationMode.BICUBIC
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=self.image_mean, std=self.image_std),
    ]
)
inputs = processor(prompts, transform=image_transform, return_tensors="pt")
```

为了帮助调试提示生成，启用`debug=True`，这将向您展示发生了什么。
