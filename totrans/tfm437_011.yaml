- en: Distributed training with 🤗 Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用🤗 Accelerate进行分布式训练
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/accelerate](https://huggingface.co/docs/transformers/v4.37.2/en/accelerate)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/accelerate](https://huggingface.co/docs/transformers/v4.37.2/en/accelerate)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: As models get bigger, parallelism has emerged as a strategy for training larger
    models on limited hardware and accelerating training speed by several orders of
    magnitude. At Hugging Face, we created the [🤗 Accelerate](https://huggingface.co/docs/accelerate)
    library to help users easily train a 🤗 Transformers model on any type of distributed
    setup, whether it is multiple GPU’s on one machine or multiple GPU’s across several
    machines. In this tutorial, learn how to customize your native PyTorch training
    loop to enable training in a distributed environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得更大，并行性已经成为在有限硬件上训练更大模型并通过几个数量级加速训练速度的策略。在Hugging Face，我们创建了[🤗 Accelerate](https://huggingface.co/docs/accelerate)库，以帮助用户轻松地在任何类型的分布式设置上训练🤗
    Transformers模型，无论是在一台机器上的多个GPU还是跨多台机器的多个GPU。在本教程中，了解如何自定义您的本地PyTorch训练循环以在分布式环境中进行训练。
- en: Setup
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'Get started by installing 🤗 Accelerate:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过安装🤗 Accelerate开始：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then import and create an [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    object. The [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    will automatically detect your type of distributed setup and initialize all the
    necessary components for training. You don’t need to explicitly place your model
    on a device.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后导入并创建一个[Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)对象。[Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)将自动检测您的分布式设置类型，并初始化所有必要的组件进行训练。您不需要明确将模型放在设备上。
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Prepare to accelerate
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备加速
- en: 'The next step is to pass all the relevant training objects to the [prepare](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method. This includes your training and evaluation DataLoaders, a model and an
    optimizer:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将所有相关的训练对象传递给[prepare](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.prepare)方法。这包括您的训练和评估DataLoaders，一个模型和一个优化器：
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Backward
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向后
- en: 'The last addition is to replace the typical `loss.backward()` in your training
    loop with 🤗 Accelerate’s [backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)method:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个补充是用🤗 Accelerate的[backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)方法替换训练循环中典型的`loss.backward()`：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the following code, you only need to add four additional lines
    of code to your training loop to enable distributed training!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，您只需要向训练循环中添加四行额外的代码即可启用分布式训练！
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Train
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: Once you’ve added the relevant lines of code, launch your training in a script
    or a notebook like Colaboratory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了相关代码行后，可以在脚本或类似Colaboratory的笔记本中启动训练。
- en: Train with a script
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用脚本进行训练
- en: 'If you are running your training from a script, run the following command to
    create and save a configuration file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从脚本中运行训练，请运行以下命令以创建并保存配置文件：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then launch your training with:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动您的训练：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Train with a notebook
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用笔记本进行训练
- en: '🤗 Accelerate can also run in a notebook if you’re planning on using Colaboratory’s
    TPUs. Wrap all the code responsible for training in a function, and pass it to
    [notebook_launcher](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/launchers#accelerate.notebook_launcher):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate也可以在笔记本中运行，如果您计划使用Colaboratory的TPU。将负责训练的所有代码包装在一个函数中，并将其传递给[notebook_launcher](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/launchers#accelerate.notebook_launcher)：
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For more information about 🤗 Accelerate and its rich features, refer to the
    [documentation](https://huggingface.co/docs/accelerate).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有关🤗 Accelerate及其丰富功能的更多信息，请参考[文档](https://huggingface.co/docs/accelerate)。
