- en: Distributed training with ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/accelerate](https://huggingface.co/docs/transformers/v4.37.2/en/accelerate)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/accelerate](https://huggingface.co/docs/transformers/v4.37.2/en/accelerate)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: As models get bigger, parallelism has emerged as a strategy for training larger
    models on limited hardware and accelerating training speed by several orders of
    magnitude. At Hugging Face, we created the [ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate)
    library to help users easily train a ğŸ¤— Transformers model on any type of distributed
    setup, whether it is multiple GPUâ€™s on one machine or multiple GPUâ€™s across several
    machines. In this tutorial, learn how to customize your native PyTorch training
    loop to enable training in a distributed environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ¨¡å‹å˜å¾—æ›´å¤§ï¼Œå¹¶è¡Œæ€§å·²ç»æˆä¸ºåœ¨æœ‰é™ç¡¬ä»¶ä¸Šè®­ç»ƒæ›´å¤§æ¨¡å‹å¹¶é€šè¿‡å‡ ä¸ªæ•°é‡çº§åŠ é€Ÿè®­ç»ƒé€Ÿåº¦çš„ç­–ç•¥ã€‚åœ¨Hugging Faceï¼Œæˆ‘ä»¬åˆ›å»ºäº†[ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate)åº“ï¼Œä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾åœ°åœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè®­ç»ƒğŸ¤—
    Transformersæ¨¡å‹ï¼Œæ— è®ºæ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šçš„å¤šä¸ªGPUè¿˜æ˜¯è·¨å¤šå°æœºå™¨çš„å¤šä¸ªGPUã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œäº†è§£å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„æœ¬åœ°PyTorchè®­ç»ƒå¾ªç¯ä»¥åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚
- en: Setup
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: 'Get started by installing ğŸ¤— Accelerate:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å®‰è£…ğŸ¤— Accelerateå¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then import and create an [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    object. The [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    will automatically detect your type of distributed setup and initialize all the
    necessary components for training. You donâ€™t need to explicitly place your model
    on a device.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¼å…¥å¹¶åˆ›å»ºä¸€ä¸ª[Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡ã€‚[Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)å°†è‡ªåŠ¨æ£€æµ‹æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®ç±»å‹ï¼Œå¹¶åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„ç»„ä»¶è¿›è¡Œè®­ç»ƒã€‚æ‚¨ä¸éœ€è¦æ˜ç¡®å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šã€‚
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Prepare to accelerate
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡åŠ é€Ÿ
- en: 'The next step is to pass all the relevant training objects to the [prepare](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method. This includes your training and evaluation DataLoaders, a model and an
    optimizer:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯å°†æ‰€æœ‰ç›¸å…³çš„è®­ç»ƒå¯¹è±¡ä¼ é€’ç»™[prepare](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.prepare)æ–¹æ³•ã€‚è¿™åŒ…æ‹¬æ‚¨çš„è®­ç»ƒå’Œè¯„ä¼°DataLoadersï¼Œä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªä¼˜åŒ–å™¨ï¼š
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Backward
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘å
- en: 'The last addition is to replace the typical `loss.backward()` in your training
    loop with ğŸ¤— Accelerateâ€™s [backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)method:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ä¸ªè¡¥å……æ˜¯ç”¨ğŸ¤— Accelerateçš„[backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)æ–¹æ³•æ›¿æ¢è®­ç»ƒå¾ªç¯ä¸­å…¸å‹çš„`loss.backward()`ï¼š
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the following code, you only need to add four additional lines
    of code to your training loop to enable distributed training!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºï¼Œæ‚¨åªéœ€è¦å‘è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å››è¡Œé¢å¤–çš„ä»£ç å³å¯å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Train
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: Once youâ€™ve added the relevant lines of code, launch your training in a script
    or a notebook like Colaboratory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ äº†ç›¸å…³ä»£ç è¡Œåï¼Œå¯ä»¥åœ¨è„šæœ¬æˆ–ç±»ä¼¼Colaboratoryçš„ç¬”è®°æœ¬ä¸­å¯åŠ¨è®­ç»ƒã€‚
- en: Train with a script
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è„šæœ¬è¿›è¡Œè®­ç»ƒ
- en: 'If you are running your training from a script, run the following command to
    create and save a configuration file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä»è„šæœ¬ä¸­è¿è¡Œè®­ç»ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥åˆ›å»ºå¹¶ä¿å­˜é…ç½®æ–‡ä»¶ï¼š
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then launch your training with:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯åŠ¨æ‚¨çš„è®­ç»ƒï¼š
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Train with a notebook
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç¬”è®°æœ¬è¿›è¡Œè®­ç»ƒ
- en: 'ğŸ¤— Accelerate can also run in a notebook if youâ€™re planning on using Colaboratoryâ€™s
    TPUs. Wrap all the code responsible for training in a function, and pass it to
    [notebook_launcher](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/launchers#accelerate.notebook_launcher):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateä¹Ÿå¯ä»¥åœ¨ç¬”è®°æœ¬ä¸­è¿è¡Œï¼Œå¦‚æœæ‚¨è®¡åˆ’ä½¿ç”¨Colaboratoryçš„TPUã€‚å°†è´Ÿè´£è®­ç»ƒçš„æ‰€æœ‰ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™[notebook_launcher](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/launchers#accelerate.notebook_launcher)ï¼š
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For more information about ğŸ¤— Accelerate and its rich features, refer to the
    [documentation](https://huggingface.co/docs/accelerate).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³ğŸ¤— AccelerateåŠå…¶ä¸°å¯ŒåŠŸèƒ½çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://huggingface.co/docs/accelerate)ã€‚
