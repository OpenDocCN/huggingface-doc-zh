- en: Distributed training with ðŸ¤— Accelerate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/accelerate](https://huggingface.co/docs/transformers/v4.37.2/en/accelerate)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: As models get bigger, parallelism has emerged as a strategy for training larger
    models on limited hardware and accelerating training speed by several orders of
    magnitude. At Hugging Face, we created the [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate)
    library to help users easily train a ðŸ¤— Transformers model on any type of distributed
    setup, whether it is multiple GPUâ€™s on one machine or multiple GPUâ€™s across several
    machines. In this tutorial, learn how to customize your native PyTorch training
    loop to enable training in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Get started by installing ðŸ¤— Accelerate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then import and create an [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    object. The [Accelerator](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator)
    will automatically detect your type of distributed setup and initialize all the
    necessary components for training. You donâ€™t need to explicitly place your model
    on a device.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Prepare to accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to pass all the relevant training objects to the [prepare](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.prepare)
    method. This includes your training and evaluation DataLoaders, a model and an
    optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Backward
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last addition is to replace the typical `loss.backward()` in your training
    loop with ðŸ¤— Accelerateâ€™s [backward](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.backward)method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the following code, you only need to add four additional lines
    of code to your training loop to enable distributed training!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once youâ€™ve added the relevant lines of code, launch your training in a script
    or a notebook like Colaboratory.
  prefs: []
  type: TYPE_NORMAL
- en: Train with a script
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are running your training from a script, run the following command to
    create and save a configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then launch your training with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Train with a notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ðŸ¤— Accelerate can also run in a notebook if youâ€™re planning on using Colaboratoryâ€™s
    TPUs. Wrap all the code responsible for training in a function, and pass it to
    [notebook_launcher](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/launchers#accelerate.notebook_launcher):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For more information about ðŸ¤— Accelerate and its rich features, refer to the
    [documentation](https://huggingface.co/docs/accelerate).
  prefs: []
  type: TYPE_NORMAL
