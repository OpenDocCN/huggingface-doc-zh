- en: Zero-shot object detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/zero_shot_object_detection](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/zero_shot_object_detection)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, models used for [object detection](object_detection) require
    labeled image datasets for training, and are limited to detecting the set of classes
    from the training data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit)
    model which uses a different approach. OWL-ViT is an open-vocabulary object detector.
    It means that it can detect objects in images based on free-text queries without
    the need to fine-tune the model on labeled datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: OWL-ViT leverages multi-modal representations to perform open-vocabulary detection.
    It combines [CLIP](../model_doc/clip) with lightweight object classification and
    localization heads. Open-vocabulary detection is achieved by embedding free-text
    queries with the text encoder of CLIP and using them as input to the object classification
    and localization heads. associate images and their corresponding textual descriptions,
    and ViT processes image patches as inputs. The authors of OWL-ViT first trained
    CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection
    datasets using a bipartite matching loss.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, the model can detect objects based on textual descriptions
    without prior training on labeled datasets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide, you will learn how to use OWL-ViT:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: to detect objects based on text prompts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for batch object detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: for image-guided object detection
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Zero-shot object detection pipeline
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest way to try out inference with OWL-ViT is to use it in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a pipeline for zero-shot object detection from a [checkpoint on the
    Hugging Face Hub](https://huggingface.co/models?other=owlvit):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, choose an image you’d like to detect objects in. Here we’ll use the image
    of astronaut Eileen Collins that is a part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html)
    Great Images dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Astronaut Eileen Collins](../Images/96b84ae6b3724360765220a32947ac55.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Pass the image and the candidate object labels to look for to the pipeline.
    Here we pass the image directly; other suitable options include a local path to
    an image or an image url. We also pass text descriptions for all items we want
    to query the image for.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s visualize the predictions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Visualized predictions on NASA image](../Images/feb704c325d7a61bdcec14c2946ccd78.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Text-prompted zero-shot object detection by hand
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve seen how to use the zero-shot object detection pipeline, let’s
    replicate the same result manually.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the model and associated processor from a [checkpoint on the
    Hugging Face Hub](https://huggingface.co/models?other=owlvit). Here we’ll use
    the same checkpoint as before:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s take a different image to switch things up.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Beach photo](../Images/113adca13e3c6860545da1a86b83790e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Use the processor to prepare the inputs for the model. The processor combines
    an image processor that prepares the image for the model by resizing and normalizing
    it, and a [CLIPTokenizer](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)
    that takes care of the text inputs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Pass the inputs through the model, post-process, and visualize the results.
    Since the image processor resized images before feeding them to the model, you
    need to use the [post_process_object_detection()](/docs/transformers/v4.37.2/en/model_doc/owlvit#transformers.OwlViTImageProcessor.post_process_object_detection)
    method to make sure the predicted bounding boxes have the correct coordinates
    relative to the original image:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Beach photo with detected objects](../Images/15bf65ba7808914dddfb0bc50e7be255.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Batch processing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can pass multiple sets of images and text queries to search for different
    (or same) objects in several images. Let’s use both an astronaut image and the
    beach image together. For batch processing, you should pass text queries as a
    nested list to the processor and images as lists of PIL images, PyTorch tensors,
    or NumPy arrays.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以传递多组图像和文本查询以在多个图像中搜索不同（或相同）的对象。让我们一起使用宇航员图像和海滩图像。对于批处理，您应该将文本查询作为嵌套列表传递给处理器，并将图像作为PIL图像、PyTorch张量或NumPy数组的列表。
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Previously for post-processing you passed the single image’s size as a tensor,
    but you can also pass a tuple, or, in case of several images, a list of tuples.
    Let’s create predictions for the two examples, and visualize the second one (`image_idx
    = 1`).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，用于后处理的是将单个图像的大小作为张量传递，但您也可以传递一个元组，或者在有多个图像的情况下，传递一个元组列表。让我们为这两个示例创建预测，并可视化第二个示例（`image_idx
    = 1`）。
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Beach photo with detected objects](../Images/15bf65ba7808914dddfb0bc50e7be255.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![带有检测到的对象的海滩照片](../Images/15bf65ba7808914dddfb0bc50e7be255.png)'
- en: Image-guided object detection
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像引导的对象检测
- en: In addition to zero-shot object detection with text queries, OWL-ViT offers
    image-guided object detection. This means you can use an image query to find similar
    objects in the target image. Unlike text queries, only a single example image
    is allowed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用文本查询进行零样本对象检测外，OWL-ViT还提供了图像引导的对象检测。这意味着您可以使用图像查询在目标图像中找到相似的对象。与文本查询不同，只允许一个示例图像。
- en: 'Let’s take an image with two cats on a couch as a target image, and an image
    of a single cat as a query:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一张沙发上有两只猫的图像作为目标图像，以一张单猫图像作为查询：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s take a quick look at the images:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看这些图像：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Cats](../Images/8494ff7169eabc8d8637dec8997e45c2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![猫](../Images/8494ff7169eabc8d8637dec8997e45c2.png)'
- en: 'In the preprocessing step, instead of text queries, you now need to use `query_images`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理步骤中，现在需要使用`query_images`而不是文本查询：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For predictions, instead of passing the inputs to the model, pass them to [image_guided_detection()](/docs/transformers/v4.37.2/en/model_doc/owlvit#transformers.OwlViTForObjectDetection.image_guided_detection).
    Draw the predictions as before except now there are no labels.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，不要将输入传递给模型，而是将它们传递给[image_guided_detection()](/docs/transformers/v4.37.2/en/model_doc/owlvit#transformers.OwlViTForObjectDetection.image_guided_detection)。除了现在没有标签之外，绘制预测与以前相同。
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Cats with bounding boxes](../Images/2894fbe4429cbea3d46c5b6776d179b0.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![带有边界框的猫](../Images/2894fbe4429cbea3d46c5b6776d179b0.png)'
