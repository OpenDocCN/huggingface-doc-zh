# 飞马

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pegasus](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/pegasus)

[![模型](../Images/b6ef9d25f66d4fe46ebef514edce9483.png)](https://huggingface.co/models?filter=pegasus) [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/pegasus_paraphrase)

## 概述

Pegasus模型是由Jingqing Zhang、Yao Zhao、Mohammad Saleh和Peter J. Liu于2019年12月18日提出的[PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf)。

根据摘要，

+   Pegasus的预训练任务故意与摘要相似：从输入文档中删除/屏蔽重要句子，并将其作为一个输出序列从剩余句子中生成，类似于提取式摘要。

+   Pegasus在所有12个下游任务上实现了SOTA摘要性能，由ROUGE和人类评估测量。

这个模型是由[sshleifer](https://huggingface.co/sshleifer)贡献的。作者的代码可以在[这里](https://github.com/google-research/pegasus)找到。

## 使用提示

+   具有与BART相同的编码器-解码器模型架构的序列到序列模型。Pegasus在两个自监督目标函数上联合预训练：掩码语言建模（MLM）和一种新颖的摘要特定预训练目标，称为Gap Sentence Generation（GSG）。

    +   MLM：编码器输入标记被随机替换为掩码标记，并且必须由编码器预测（就像在BERT中一样）

    +   GSG：整个编码器输入句子被第二个掩码标记替换并馈送到解码器，但是具有因果掩码以隐藏未来单词，就像常规的自回归变压器解码器一样。

+   不支持FP16（对此的帮助/想法赞赏！）。

+   推荐使用adafactor优化器进行pegasus微调。

## 检查点

所有[检查点](https://huggingface.co/models?search=pegasus)都经过微调以用于摘要，除了*pegasus-large*，其他检查点都经过微调：

+   每个检查点在磁盘上占用2.2 GB，参数为568M。

+   不支持FP16（对此的帮助/想法赞赏！）。

+   在v100 GPU上，默认参数下，使用fp32对xsum进行摘要大约需要400ms/样本。

+   完整的复制结果和正确预处理的数据可以在这个[Issue](https://github.com/huggingface/transformers/issues/6844#issue-689259666)中找到。

+   [蒸馏检查点](https://huggingface.co/models?search=distill-pegasus)在这篇[论文](https://arxiv.org/abs/2010.13002)中有描述。

## 实现说明

+   所有模型都是具有16层的变压器编码器-解码器。

+   实现完全继承自[BartForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/bart#transformers.BartForConditionalGeneration)

+   一些关键配置差异：

    +   静态，正弦位置嵌入

    +   模型从pad_token_id（其具有0个token_embedding）开始生成。

    +   使用更多的beam（`num_beams=8`）

+   所有预训练的pegasus检查点除了三个属性外都是相同的：`tokenizer.model_max_length`（最大输入大小），`max_length`（要生成的最大标记数）和`length_penalty`。

+   可以在`convert_pegasus_tf_to_pytorch.py`中找到将在作者的[repo](https://github.com/google-research/pegasus)中训练的检查点转换的代码。

## 使用示例

```py
>>> from transformers import PegasusForConditionalGeneration, PegasusTokenizer
>>> import torch

>>> src_text = [
...     """ PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."""
... ]

... model_name = "google/pegasus-xsum"
... device = "cuda" if torch.cuda.is_available() else "cpu"
... tokenizer = PegasusTokenizer.from_pretrained(model_name)
... model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)
... batch = tokenizer(src_text, truncation=True, padding="longest", return_tensors="pt").to(device)
... translated = model.generate(**batch)
... tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
... assert (
...     tgt_text[0]
...     == "California's largest electricity provider has turned off power to hundreds of thousands of customers."
... )
```

## 资源

+   [脚本](https://github.com/huggingface/transformers/tree/main/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh)用于在XSUM数据集上微调pegasus。数据下载说明在[examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md)。

+   [因果语言建模任务指南](../tasks/language_modeling)

+   [翻译任务指南](../tasks/translation)

+   [摘要任务指南](../tasks/summarization)

## PegasusConfig

### `class transformers.PegasusConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/configuration_pegasus.py#L29)

```py
( vocab_size = 50265 max_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 0 scale_embedding = False pad_token_id = 0 eos_token_id = 1 forced_eos_token_id = 1 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 50265) — PEGASUS模型的词汇表大小。定义了在调用[PegasusModel](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusModel)或[TFPegasusModel](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.TFPegasusModel)时可以表示的不同标记数量。

+   `d_model` (`int`, *optional*, defaults to 1024) — 层和池化器层的维度。

+   `encoder_layers` (`int`, *optional*, defaults to 12) — 编码器层数。

+   `decoder_layers` (`int`, *optional*, defaults to 12) — 解码器层数。

+   `encoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数。

+   `decoder_attention_heads` (`int`, *optional*, defaults to 16) — Transformer解码器中每个注意力层的注意力头数。

+   `decoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 解码器中“中间”（通常称为前馈）层的维度。

+   `encoder_ffn_dim` (`int`, *optional*, defaults to 4096) — 解码器中“中间”（通常称为前馈）层的维度。

+   `activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢弃比例。

+   `activation_dropout` (`float`, *optional*, defaults to 0.0) — 全连接层内激活的丢弃比例。

+   `max_position_embeddings` (`int`, *optional*, defaults to 1024) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。

+   `init_std` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](参见[https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。

+   `decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — 解码器的LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](参见[https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。

+   `scale_embedding` (`bool`, *optional*, defaults to `False`) — 通过除以sqrt(d_model)来缩放嵌入。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）

+   `forced_eos_token_id` (`int`, *optional*, defaults to 1) — 当达到`max_length`时，强制作为最后生成的标记的标记ID。通常设置为`eos_token_id`。

这是用于存储[PegasusModel](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusModel)配置的配置类。根据指定的参数实例化一个PEGASUS模型，定义模型架构。使用默认值实例化配置将产生类似于PEGASUS [google/pegasus-large](https://huggingface.co/google/pegasus-large)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import PegasusConfig, PegasusModel

>>> # Initializing a PEGASUS google/pegasus-large style configuration
>>> configuration = PegasusConfig()

>>> # Initializing a model (with random weights) from the google/pegasus-large style configuration
>>> model = PegasusModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## PegasusTokenizer

警告：`add_tokens` 目前不起作用。

### `class transformers.PegasusTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus.py#L42)

```py
( vocab_file pad_token = '<pad>' eos_token = '</s>' unk_token = '<unk>' mask_token = '<mask_2>' mask_token_sent = '<mask_1>' additional_special_tokens = None offset = 103 sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化分词器所需词汇表的[SentencePiece](https://github.com/google/sentencepiece)文件（通常具有 *.spm* 扩展名）。

+   `pad_token` (`str`, *optional*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `eos_token` (`str`, *optional*, 默认为 `"</s>"`) — 序列结束标记。

    在构建带有特殊标记的序列时，这不是用于序列结束的标记。使用的标记是 `sep_token`。

+   `unk_token` (`str`, *optional*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `mask_token` (`str`, *optional*, 默认为 `"<mask_2>"`) — 用于屏蔽单个标记值的标记。在使用屏蔽语言建模（MLM）训练此模型时使用的标记。这是PEGASUS编码器在预训练期间尝试预测的标记。它对应于[PEGASUS：用于抽象摘要的提取间隙句子预训练](https://arxiv.org/pdf/1912.08777.pdf)中的*[MASK2]*。

+   `mask_token_sent` (`str`, *optional*, 默认为 `"<mask_1>"`) — 用于屏蔽整个目标句子的标记。在使用间隙句子生成（GSG）训练此模型时使用的标记。这是PEGASUS解码器在预训练期间尝试预测的句子。它对应于[PEGASUS：用于抽象摘要的提取间隙句子预训练](https://arxiv.org/pdf/1912.08777.pdf)中的*[MASK1]*。

+   `additional_special_tokens` (`List[str]`, *optional*) — 分词器使用的额外特殊标记。如果未提供额外的特殊标记，则使用 <mask_2>和 <unk_2 unk_102="">作为额外的特殊标记，对应于[原始PEGASUS分词器](https://github.com/google-research/pegasus/blob/939830367bcf411193d2b5eca2f2f90f3f9260ca/pegasus/ops/pretrain_parsing_ops.cc#L66)仅在预训练中使用标记2 - 104</unk_2></mask_2>

+   `sp_model_kwargs` (`dict`, *optional*) — 将传递给 `SentencePieceProcessor.__init__()` 方法。[SentencePiece的Python包装器](https://github.com/google/sentencepiece/tree/master/python)可用于设置：

    +   `enable_sampling`: 启用子词正则化。

    +   `nbest_size`: 单字采样的采样参数。对于BPE-Dropout无效。

        +   `nbest_size = {0,1}`: 不执行采样。

        +   `nbest_size > 1`: 从nbest_size结果中抽样。

        +   `nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向采样算法从所有假设（格）中抽样。

    +   `alpha`: 用于单字采样的平滑参数，以及BPE-dropout合并操作的丢弃概率。

构建一个PEGASUS分词器。基于[SentencePiece](https://github.com/google/sentencepiece)。

此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus.py#L255)

```py
( token_ids_0 token_ids_1 = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的 ID 列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对应的 ID 列表。

返回

`List[int]`

带有适当特殊标记的 [输入 ID](../glossary#input-ids) 列表。

通过连接和添加特殊标记，从序列或序列对构建用于序列分类任务的模型输入。PEGASUS 序列具有以下格式，其中 `X` 表示序列：

+   单个序列：`X </s>`

+   序列对：`A B </s>`（非预期用例）

BOS 从不使用。序列对不是预期的用例，但它们将在没有分隔符的情况下处理。

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus.py#L220)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus.py#L244)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False )
```

获取列表，其中条目为 [1] 如果标记为 [eos] 或 [pad] 否则为 0。

#### `num_special_tokens_to_add`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus.py#L234)

```py
( pair = False )
```

只有 EOS

## PegasusTokenizerFast

### `class transformers.PegasusTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus_fast.py#L51)

```py
( vocab_file = None tokenizer_file = None pad_token = '<pad>' eos_token = '</s>' unk_token = '<unk>' mask_token = '<mask_2>' mask_token_sent = '<mask_1>' additional_special_tokens = None offset = 103 **kwargs )
```

参数

+   `vocab_file` (`str`) — [SentencePiece](https://github.com/google/sentencepiece) 文件（通常具有 *.spm* 扩展名），其中包含实例化分词器所需的词汇表。

+   `pad_token` (`str`, *可选*，默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `eos_token` (`str`, *可选*，默认为 `"</s>"`) — 序列结束标记。

    在使用特殊标记构建序列时，这不是用于序列结束的标记。使用的标记是 `sep_token`。

+   `unk_token` (`str`, *可选*，默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `mask_token` (`str`, *可选*，默认为 `"<mask_2>"`) — 用于屏蔽单个标记值的标记。这是在使用掩码语言建模（MLM）训练此模型时使用的标记。这是 PEGASUS 编码器在预训练期间尝试预测的标记。它对应于 [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf) 中的 *[MASK2]*。

+   `mask_token_sent` (`str`, *可选*，默认为 `"<mask_1>"`) — 用于屏蔽整个目标句子的标记。这是在使用间隙句子生成（GSG）训练此模型时使用的标记。这是 PEGASUS 解码器在预训练期间尝试预测的句子。它对应于 [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf) 中的 *[MASK1]*。

+   `additional_special_tokens` (`List[str]`, *可选*) — 分词器使用的额外特殊标记。如果未提供额外特殊标记，则使用 <mask_2> 和 <unk_2 unk_102=""> 作为额外特殊标记，对应于仅在预训练中使用标记 2 - 104 的 [原始 PEGASUS 分词器](https://github.com/google-research/pegasus/blob/939830367bcf411193d2b5eca2f2f90f3f9260ca/pegasus/ops/pretrain_parsing_ops.cc#L66)</unk_2></mask_2>

构建一个“快速” PEGASUS 分词器（由 HuggingFace 的 *tokenizers* 库支持）。基于 [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models)。

该分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus_fast.py#L192)

```py
( token_ids_0 token_ids_1 = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表

+   `token_ids_1` (`List[int]`, *optional*) — 可选的第二个序列对应的ID列表。

返回

`List[int]`

包含适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过在序列末尾添加 eos 来构建模型输入。不会在前面添加 bos 标记。

+   单个序列：`X </s>`

+   序列对：`A B </s>`（不是预期的用法）

#### `get_special_tokens_mask`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/tokenization_pegasus_fast.py#L181)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False )
```

获取列表，如果标记是[eos]或[pad]则为[1]，否则为0。

PytorchHide Pytorch content

## PegasusModel

### `class transformers.PegasusModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1098)

```py
( config: PegasusConfig )
```

参数

+   `config` ([PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的 PEGASUS 模型输出原始隐藏状态，没有特定的头部。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1154)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。选择在 `[0, 1]` 中的掩码值：

    +   对于未被掩码的标记，为1，

    +   对于被掩码的标记，为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 解码器输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

    Pegasus使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（请参见`past_key_values`）。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于在编码器中使注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`掩码`。

    +   0表示头部被`掩码`。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于在解码器中使注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`掩码`。

    +   0表示头部被`掩码`。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于在解码器中使交叉注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`掩码`。

    +   0表示头部被`掩码`。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包括（`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`）`last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可以使用（参见`past_key_values`输入）以加速顺序解码。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后的`decoder_inputs_embeds`（请参见`past_key_values`）。这将非常有用，如果您想要更多控制权，以便将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache` (`bool`，*optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)，而不是一个普通的元组。

返回

[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.Seq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的隐藏状态序列。

    如果使用了`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*optional*，当传递了`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递了`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每一层的输出）。

    解码器在每一层输出处的隐藏状态，以及可选的初始嵌入输出。

+   `decoder_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递了`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递了`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个，每层的输出为一个）。

    每层编码器的隐藏状态以及可选的初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[PegasusModel](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在之后调用`Module`实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, PegasusModel

>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")
>>> model = PegasusModel.from_pretrained("google/pegasus-large")

>>> inputs = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt")
>>> decoder_inputs = tokenizer("Studies show that", return_tensors="pt")
>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 4, 1024]
```

## PEGASUS用于条件生成

### `class transformers.PegasusForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1250)

```py
( config: PegasusConfig )
```

参数

+   `config`（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有语言建模头的PEGASUS模型。可用于摘要。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1316)

```py
( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 输入序列标记在词汇表中的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)以获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 解码器输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是decoder input IDs？](../glossary#decoder-input-ids)

    Pegasus使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，可以选择仅输入最后一个`decoder_input_ids`（请参见`past_key_values`）。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`torch.LongTensor`，*可选*）- 默认行为：生成一个忽略`decoder_input_ids`中填充标记的张量。因果掩码也将默认使用。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于使编码器中注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被`掩盖`，

    +   0表示头部被`掩盖`。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于使解码器中注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被`掩盖`，

    +   0表示头部被`掩盖`。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*）- 用于使解码器中交叉注意力模块的选定头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被`掩盖`，

    +   0表示头部被`掩盖`。

+   `encoder_outputs`（`tuple(tuple(torch.FloatTensor)`，*可选*）- 元组包含(`last_hidden_state`，*可选*：`hidden_states`，*可选*：`attentions`) `last_hidden_state`的形状为`(batch_size, sequence_length, hidden_size)`，*可选*）是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`tuple(torch.FloatTensor)`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入形状为`(batch_size, 1)`的最后一个`decoder_input_ids`（即那些没有将它们的过去键值状态提供给此模型的输入）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，可以直接传递嵌入表示而不是传递`decoder_input_ids`。如果使用了`past_key_values`，可以选择仅输入最后一个`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制如何将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果`decoder_input_ids`和`decoder_inputs_embeds`都未设置，则`decoder_inputs_embeds`取`inputs_embeds`的值。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`范围内，或者为-100（参见`input_ids`文档字符串）。将索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有`[0, ..., config.vocab_size]`标签的标记。

返回

[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码。

+   `decoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出加上每层的输出）。

    解码器在每层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*） — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每个层的输出）。

    编码器在每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[PegasusForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

摘要示例：

```py
>>> from transformers import AutoTokenizer, PegasusForConditionalGeneration

>>> model = PegasusForConditionalGeneration.from_pretrained("google/pegasus-xsum")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-xsum")

>>> ARTICLE_TO_SUMMARIZE = (
...     "PG&E stated it scheduled the blackouts in response to forecasts for high winds "
...     "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "
...     "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
... )
>>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors="pt")

>>> # Generate Summary
>>> summary_ids = model.generate(inputs["input_ids"])
>>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
"California's largest electricity provider has turned off power to hundreds of thousands of customers."
```

## PegasusForCausalLM

### `class transformers.PegasusForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1465)

```py
( config )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_pegasus.py#L1520)

```py
( input_ids: LongTensor = None attention_mask: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None head_mask: Optional = None cross_attn_head_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。默认情况下，如果提供，将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*） — 用于避免对填充标记索引执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `encoder_hidden_states`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*） — 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） — 用于避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。掩码值选择在`[0, 1]`之间：

+   `head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*） — 用于使注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部`未被掩盖`，

    +   0表示头部`被掩盖`。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`torch.Tensor`，*可选*） — 用于使交叉注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部`未被掩盖`，

    +   0表示头部`被掩盖`。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或当`config.use_cache=True`时返回）— 长度为`config.n_layers`的元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，以及2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。当模型用作序列到序列模型中的解码器时，只有这两个额外的张量是必需的。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`的张量，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`或-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（被`masked`），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出，则为一个，每层的输出为一个）。

    每层模型的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在交叉注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）- 长度为`config.n_layers`的`torch.FloatTensor`元组的元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态。仅在`config.is_decoder = True`时相关。

    包含预先计算的隐藏状态（注意力块中的键和值），可以使用（查看`past_key_values`输入）以加速顺序解码。

示例：

```py
>>> from transformers import AutoTokenizer, PegasusForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")
>>> model = PegasusForCausalLM.from_pretrained("google/pegasus-large", add_cross_attention=False)
>>> assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> logits = outputs.logits
>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]
>>> list(logits.shape) == expected_shape
True
```

TensorFlow隐藏TensorFlow内容

## TFPegasusModel

### `class transformers.TFPegasusModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_tf_pegasus.py#L1260)

```py
( config: PegasusConfig *inputs **kwargs )
```

参数

+   `config`（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

PEGASUS模型裸输出原始隐藏状态，没有任何特定的头部。这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（如PyTorch模型），或

+   将所有输入作为列表、元组或字典的第一个位置参数。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需传递您的输入和标签，以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量作为第一个位置参数：

+   仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   包含一个或多个输入张量的长度可变的列表，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您不需要担心这些，因为您可以像对待其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_tf_pegasus.py#L1276)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免对填充标记索引执行注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示标记`未被掩码`,

    +   0表示标记`被掩码`。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 词汇表中解码器输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

    Pegasus使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用`past_key_values`，可以选择仅输入最后的`decoder_input_ids`（参见`past_key_values`）。

+   `decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`, *optional*) — 将默认生成并忽略填充标记。不建议在大多数情况下设置此项。

+   `decoder_position_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个解码器输入序列标记在位置嵌入中的位置索引。选定范围为`[0, config.max_position_embeddings - 1]`。

+   `head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*) — 编码器中用于使注意力模块中的特定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部`未被掩码`,

    +   0表示头部`被掩码`。

+   `decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 解码器中用于使注意力模块中的特定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部`未被掩码`,

    +   0表示头部`被掩码`。

+   `cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*) — 交叉注意力模块中选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部`未被掩码`,

    +   0表示头部`被掩码`。

+   `encoder_outputs` (`tf.FloatTensor`, *optional*) — 编码器最后一层的隐藏状态输出。在解码器的交叉注意力中使用。形状为`(batch_size, sequence_length, hidden_size)`的序列是

+   `past_key_values` (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（即没有将其过去键值状态提供给此模型的那些）形状为`(batch_size, 1)`，而不是所有`decoder_input_ids`的形状为`(batch_size, sequence_length)`。

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。在训练期间设置为`False`，在生成期间设置为`True`。output_attentions (`bool`, *optional*): 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（一些模块，如dropout模块，在训练和评估之间具有不同的行为）。

返回

[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)或者`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)或者一个`tf.Tensor`元组（如果传递`return_dict=False`或者`config.return_dict=False`）包含根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入的不同元素。

+   `last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的输出的隐藏状态序列。

    如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`List[tf.Tensor]`, *optional*, 当传递`use_cache=True`或者`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含解码器的预计算隐藏状态（注意力块中的键和值），可以用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    解码器每层的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*） - 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） - 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入输出，一个用于每一层的输出）。

    编码器在每一层的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFPegasusModel](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.TFPegasusModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFPegasusModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")
>>> model = TFPegasusModel.from_pretrained("google/pegasus-large")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFPegasusForConditionalGeneration

### `class transformers.TFPegasusForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_tf_pegasus.py#L1373)

```py
( config *inputs **kwargs )
```

参数

+   `config`（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)） - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

带有语言建模头的PEGASUS模型。可用于摘要。此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

TensorFlow模型和`transformers`中的层接受两种格式作为输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有这种支持，当使用`model.fit()`等方法时，您应该可以“轻松使用” - 只需以`model.fit()`支持的任何格式传递输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量作为第一个位置参数：

+   只有`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个按照文档字符串中给定的顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_tf_pegasus.py#L1415)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None decoder_position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None cross_attn_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: Optional[TFBaseModelOutput] = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSeq2SeqLMOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`({0})`的`tf.Tensor`）— 输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [输入ID是什么？](../glossary#input-ids)

+   `attention_mask`（形状为`({0})`的`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [注意力掩码是什么？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`tf.Tensor`，*可选*）— 解码器输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [解码器输入ID是什么？](../glossary#decoder-input-ids)

    Pegasus使用`pad_token_id`作为`decoder_input_ids`生成的起始标记。如果使用了`past_key_values`，则只需输入最后的`decoder_input_ids`（参见`past_key_values`）。

+   `decoder_attention_mask`（形状为`(batch_size, target_sequence_length)`的`tf.Tensor`，*可选*）— 默认情况下将生成并忽略填充标记。不建议为大多数用例设置此选项。

+   `decoder_position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 每个解码器输入序列标记在位置嵌入中的位置索引。选定范围为`[0, config.max_position_embeddings - 1]`。

+   `head_mask`（形状为`(encoder_layers, encoder_attention_heads)`的`tf.Tensor`，*可选*）— 用于将编码器中注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：

    +   对于未被`masked`的头部为1。

    +   对于被`masked`的头部为0。

+   `decoder_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`tf.Tensor`，*可选*）- 用于在解码器中的注意力模块中使选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被“掩盖”，

    +   0表示头部被“掩盖”。

+   `cross_attn_head_mask`（形状为`(decoder_layers, decoder_attention_heads)`的`tf.Tensor`，*可选*）- 用于使交叉注意力模块中选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被“掩盖”，

    +   0表示头部被“掩盖”。

+   `encoder_outputs`（`tf.FloatTensor`，*可选*）- 编码器最后一层的输出的隐藏状态。用于解码器的交叉注意力。形状为`(batch_size, sequence_length, hidden_size)`是一个序列

+   `past_key_values`（长度为`config.n_layers`的`Tuple[Tuple[tf.Tensor]]`）- 包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用了`past_key_values`，用户可以选择仅输入最后一个`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）的形状为`(batch_size, 1)`，而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）- 可选地，可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `use_cache`（`bool`，*可选*，默认为`True`）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。在训练期间设置为`False`，在生成输出期间设置为`True`。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅可在急切模式中使用，在图模式中，将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅可在急切模式中使用，在图模式中，将使用配置中的值。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数仅可在急切模式中使用，在图模式中，该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）- 是否在训练模式中使用模型（某些模块，如丢弃模块，在训练和评估之间具有不同的行为）。

+   `labels`（形状为`(batch_size, sequence_length)`的`tf.tensor`，*可选*）- 用于计算掩盖语言建模损失的标签。索引应该在`[0, ..., config.vocab_size]`或-100（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩盖），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

返回

[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入不同元素。

+   `损失`（形状为`(n,)`的`tf.Tensor`，*可选*，当提供`标签`时返回，其中n是非掩码标签的数量）— 语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values`（`List[tf.Tensor]`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`tf.Tensor`列表，每个张量的形状为`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`。

    包含预先计算的解码器隐藏状态（注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的注意力权重，在注意力SoftMax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力SoftMax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）— 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    编码器的注意力权重，在注意力SoftMax之后，用于计算自注意力头中的加权平均值。

[TFPegasusForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

摘要示例：

```py
>>> from transformers import AutoTokenizer, TFPegasusForConditionalGeneration

>>> model = TFPegasusForConditionalGeneration.from_pretrained("google/pegasus-xsum")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-xsum")

>>> ARTICLE_TO_SUMMARIZE = (
...     "PG&E stated it scheduled the blackouts in response to forecasts for high winds "
...     "amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were "
...     "scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
... )
>>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors="tf")

>>> # Generate Summary
>>> summary_ids = model.generate(input_ids)
>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))
```

JAXHide JAX内容

## FlaxPegasusModel

### `class transformers.FlaxPegasusModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1219)

```py
( config: PegasusConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config` ([PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) 方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`，*可选*，默认为 `jax.numpy.float32`) — 计算的数据类型。可以是 `jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和 `jax.numpy.bfloat16`（在 TPU 上）之一。

    这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定了数据类型，所有计算将使用给定的 `dtype` 执行。

    请注意，这仅指定了计算的数据类型，不会影响模型参数的数据类型。

    如果希望更改模型参数的数据类型，请参阅 [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) 和 [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

裸 Pegasus 模型变压器输出原始隐藏状态，没有特定的头部。此模型继承自 [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) 的子类。将其用作常规的 Flax 模块，并参考 Flax 文档以获取有关一般用法和行为的所有信息。

最后，此模型支持内置的 JAX 功能，例如：

+   [即时 (JIT) 编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1158)

```py
( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`jnp.ndarray`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下会忽略填充。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`jnp.ndarray`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   对于未被“masked”掩盖的标记为1。

    +   对于被`masked`掩盖的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids` (`jnp.ndarray`，形状为 `(batch_size, target_sequence_length)`，*可选*) — 词汇表中解码器输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是decoder input IDs?](../glossary#decoder-input-ids)

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。默认情况下也将使用因果掩码。

    如果要更改填充行为，应根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。

+   `position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

+   `decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 每个解码器输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时），包含根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入而异的各种元素。

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) — 模型解码器最后一层的隐藏状态序列。

    如果使用`past_key_values`，则只输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（请参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    解码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `decoder_attentions` (`tuple(jnp.ndarray)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state` (`jnp.ndarray`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states` (`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组。

    编码器在每一层输出的隐藏状态加上初始嵌入输出。

+   `encoder_attentions` (`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

`FlaxPegasusPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxPegasusModel

>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")
>>> model = FlaxPegasusModel.from_pretrained("google/pegasus-large")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L981)

```py
( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：

    +   对于`未被掩码`的标记为1，

    +   对于`被掩码`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`numpy.ndarray`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`， *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput) 或 `tuple(torch.FloatTensor)`

[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包括根据配置 (`<class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>`) 和输入而异的各种元素。

+   `last_hidden_state` (`jnp.ndarray`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层输出的隐藏状态序列。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, max_length=1024, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)
```

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1044)

```py
( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `decoder_input_ids` (`jnp.ndarray`，形状为 `(batch_size, target_sequence_length)`) — 词汇表中解码器输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`) — 元组包括 (`last_hidden_state`, *可选*: `hidden_states`, *可选*: `attentions`) `last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`， *可选*) 是编码器最后一层输出的隐藏状态序列。用于解码器的交叉注意力。

+   `encoder_attention_mask` (`jnp.ndarray`，形状为 `(batch_size, sequence_length)`， *可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   对于未被掩盖的标记，为1，

    +   对于被掩盖的标记，为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_attention_mask` (`jnp.ndarray`，形状为 `(batch_size, target_sequence_length)`， *可选*) — 默认行为：生成一个忽略 `decoder_input_ids` 中填充标记的张量。因果掩码也将默认使用。

    如果要更改填充行为，应根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

+   `decoder_position_ids` (`numpy.ndarray`，形状为 `(batch_size, sequence_length)`， *可选*) — 每个解码器输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

+   `past_key_values`（`Dict[str, np.ndarray]`，*可选*，由`init_cache`返回或传递先前的`past_key_values`时返回） — 预先计算的隐藏状态的字典（注意力块中的键和值），可用于快速自回归解码。预先计算的键和值隐藏状态的形状为*[batch_size, max_length]*。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`） — 模型最后一层输出的隐藏状态序列。

    如果仅使用`past_key_values`，则输出形状为`(batch_size, 1, hidden_size)`的序列的最后一个隐藏状态。

+   `past_key_values`（`tuple(tuple(jnp.ndarray))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值以及可选地在交叉注意力块中，如果`config.is_encoder_decoder=True`）可用（请参见`past_key_values`输入）以加速顺序解码。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器的交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

示例：

```py
>>> import jax.numpy as jnp
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, max_length=1024, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)

>>> decoder_start_token_id = model.config.decoder_start_token_id
>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

>>> outputs = model.decode(decoder_input_ids, encoder_outputs)
>>> last_decoder_hidden_states = outputs.last_hidden_state
```

## FlaxPegasusForConditionalGeneration

### `class transformers.FlaxPegasusForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1305)

```py
( config: PegasusConfig input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）- 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）- 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可以用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的数据类型，不影响模型参数的数据类型。`

    如果您希望更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

具有语言建模头的PEGASUS模型。可用于摘要。此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

此模型还是Flax亚麻[flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。

最后，此模型支持JAX的固有特性，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1158)

```py
( input_ids: Array attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None position_ids: Optional = None decoder_position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）- 词汇表中输入序列标记的索引。默认情况下，如果提供，将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于`未被掩码`的标记为1，

    +   对于`被掩码`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_input_ids`（形状为`(batch_size, target_sequence_length)`的`jnp.ndarray`，*可选*）- 词汇表中解码器输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

+   `decoder_attention_mask` (`jnp.ndarray`，形状为`(batch_size, target_sequence_length)`，*optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

    如果要更改填充行为，应根据需要进行修改。有关默认策略的更多信息，请参见[论文](https://arxiv.org/abs/1910.13461)中的图表1。

+   `position_ids` (`numpy.ndarray`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

+   `decoder_position_ids` (`numpy.ndarray`，形状为`(batch_size, sequence_length)`，*optional*) — 每个解码器输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[PegasusConfig](/docs/transformers/v4.37.2/en/model_doc/pegasus#transformers.PegasusConfig)）和输入的不同元素。

+   `logits` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `past_key_values` (`tuple(tuple(jnp.ndarray))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(jnp.ndarray)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量和2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块和交叉注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `decoder_hidden_states` (`tuple(jnp.ndarray)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    每个层的解码器在每层输出的隐藏状态以及初始嵌入输出。

+   `decoder_attentions` (`tuple(jnp.ndarray)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每个层一个）。

    解码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    解码器交叉注意力层的注意力权重，在注意力softmax之后，用于计算交叉注意力头中的加权平均值。

+   `encoder_last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`，*可选*）— 模型编码器最后一层的隐藏状态序列。

+   `encoder_hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    编码器在每一层的输出处的隐藏状态加上初始嵌入输出。

+   `encoder_attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    编码器的注意力权重，在注意力softmax之后，用于计算自注意力头中的加权平均值。

`FlaxPegasusPreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

摘要示例：

```py
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')
>>> tokenizer = AutoTokenizer.from_pretrained('google/pegasus-large')

>>> ARTICLE_TO_SUMMARIZE = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='np')

>>> # Generate Summary
>>> summary_ids = model.generate(inputs['input_ids']).sequences
>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))
```

掩码填充示例：

```py
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")
>>> TXT = "My friends are <mask> but they eat too many carbs."

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
>>> input_ids = tokenizer([TXT], return_tensors="np")["input_ids"]
>>> logits = model(input_ids).logits

>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()
>>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)
>>> values, predictions = jax.lax.top_k(probs)

>>> tokenizer.decode(predictions).split()
```

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L981)

```py
( input_ids: Array attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None train: bool = False params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxBaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）— 词汇表中输入序列标记的索引。默认情况下，如果提供填充，则将被忽略。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`numpy.ndarray`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包括根据配置（`<class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列的输出。

+   `hidden_states` (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — 元组包括形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层的输出隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — 元组包括形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

示例：

```py
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, max_length=1024, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)
```

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/pegasus/modeling_flax_pegasus.py#L1312)

```py
( decoder_input_ids encoder_outputs encoder_attention_mask: Optional = None decoder_attention_mask: Optional = None decoder_position_ids: Optional = None past_key_values: dict = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None deterministic: bool = True params: dict = None dropout_rng: PRNGKey = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `decoder_input_ids` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`) — 词汇表中解码器输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是解码器输入ID？](../glossary#decoder-input-ids)

+   `encoder_outputs` (`tuple(tuple(jnp.ndarray)`) — 元组包括 (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`) `last_hidden_state` 的形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) 是编码器最后一层的隐藏状态序列。用于解码器的交叉注意力。

+   `encoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   对于未被`masked`的标记为1。

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `decoder_attention_mask` (`jnp.ndarray` of shape `(batch_size, target_sequence_length)`, *optional*) — 默认行为：生成一个张量，忽略`decoder_input_ids`中的填充标记。因果掩码也将默认使用。

    如果要更改填充行为，应根据需要进行修改。有关默认策略的更多信息，请参阅[论文](https://arxiv.org/abs/1910.13461)中的图表1。

+   `decoder_position_ids` (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*) — 每个解码器输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

+   `past_key_values` (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`) — 预先计算的隐藏状态的字典（在注意力块中的键和值），可用于快速自回归解码。预先计算的键和值隐藏状态的形状为 *[batch_size, max_length]*。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

返回

[transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions) 或 `tuple(torch.FloatTensor)`

[transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（`<class 'transformers.models.pegasus.configuration_pegasus.PegasusConfig'>`）和输入的不同元素。

+   `logits`（形状为 `(batch_size, sequence_length, config.vocab_size)` 的 `jnp.ndarray`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递了 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    每层模型输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递了 `output_attentions=True` 或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions`（`tuple(jnp.ndarray)`，*可选*，当传递了 `output_attentions=True` 或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每层一个）。

    在注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values`（`tuple(tuple(jnp.ndarray))`，*可选*，当传递了 `use_cache=True` 或当 `config.use_cache=True` 时返回）— 长度为 `config.n_layers` 的 `jnp.ndarray` 元组的元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型用于编码器-解码器设置，则相关。仅在 `config.is_decoder = True` 时相关。

    包含预先计算的隐藏状态（注意力块中的键和值），可用于加速顺序解码。

示例：

```py
>>> import jax.numpy as jnp
>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration

>>> model = FlaxPegasusForConditionalGeneration.from_pretrained("google/pegasus-large")
>>> tokenizer = AutoTokenizer.from_pretrained("google/pegasus-large")

>>> text = "My friends are cool but they eat too many carbs."
>>> inputs = tokenizer(text, max_length=1024, return_tensors="np")
>>> encoder_outputs = model.encode(**inputs)

>>> decoder_start_token_id = model.config.decoder_start_token_id
>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id

>>> outputs = model.decode(decoder_input_ids, encoder_outputs)
>>> logits = outputs.logits
```
