# T5v1.1

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/t5v1.1)

## 概述

T5v1.1 是由 Colin Raffel 等人在[google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)存储库中发布的。这是原始 T5 模型的改进版本。这个模型是由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。原始代码可以在[这里](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)找到。

## 使用提示

可以直接将 T5v1.1 的权重插入到 T5 模型中，如下所示：

```py
>>> from transformers import T5ForConditionalGeneration

>>> model = T5ForConditionalGeneration.from_pretrained("google/t5-v1_1-base")
```

T5 版本 1.1 相对于原始 T5 模型包括以下改进：

+   在前馈隐藏层中使用 GEGLU 激活，而不是 ReLU。参见[这篇论文](https://arxiv.org/abs/2002.05202)。

+   在预训练中关闭了 Dropout（质量优胜）。在微调期间应重新启用 Dropout。

+   仅在 C4 上进行预训练，没有混合下游任务。

+   嵌入层和分类器层之间没有参数共享。

+   “xl”和“xxl”取代了“3B”和“11B”。模型形状有些不同 - 更大的`d_model`和较小的`num_heads`和`d_ff`。

注意：T5 版本 1.1 仅在[C4](https://huggingface.co/datasets/c4)上进行了预训练，不包括任何监督训练。因此，这个模型在用于下游任务之前必须进行微调，与原始 T5 模型不同。由于 t5v1.1 是无监督预训练的，单任务微调时使用任务前缀并没有真正的优势。如果进行多任务微调，应该使用前缀。

Google 发布了以下变体：

+   [google/t5-v1_1-small](https://huggingface.co/google/t5-v1_1-small)

+   [google/t5-v1_1-base](https://huggingface.co/google/t5-v1_1-base)

+   [google/t5-v1_1-large](https://huggingface.co/google/t5-v1_1-large)

+   [google/t5-v1_1-xl](https://huggingface.co/google/t5-v1_1-xl)

+   [google/t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl)。

请参考 T5 的文档页面获取所有 API 参考、提示、代码示例和笔记本。
