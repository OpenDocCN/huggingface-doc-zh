["```py\n( vocab_size = None is_encoder_decoder = True prefix = None bos_token_id = None pad_token_id = None eos_token_id = None decoder_start_token_id = None title_sep = ' / ' doc_sep = ' // ' n_docs = 5 max_combined_length = 300 retrieval_vector_size = 768 retrieval_batch_size = 8 dataset = 'wiki_dpr' dataset_split = 'train' index_name = 'compressed' index_path = None passages_path = None use_dummy_dataset = False reduce_loss = False label_smoothing = 0.0 do_deduplication = True exclude_bos_score = False do_marginalize = False output_retrieved = False use_cache = True forced_eos_token_id = None **kwargs )\n```", "```py\n( question_encoder_config: PretrainedConfig generator_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';EncoderDecoderConfig\n```", "```py\n( question_encoder generator )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None doc_scores: FloatTensor = None past_key_values: Optional = None retrieved_doc_embeds: Optional = None retrieved_doc_ids: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None question_encoder_last_hidden_state: Optional = None question_enc_hidden_states: Optional = None question_enc_attentions: Optional = None generator_enc_last_hidden_state: Optional = None generator_enc_hidden_states: Optional = None generator_enc_attentions: Optional = None generator_dec_hidden_states: Optional = None generator_dec_attentions: Optional = None generator_cross_attentions: Optional = None )\n```", "```py\n( logits: FloatTensor = None doc_scores: FloatTensor = None past_key_values: Optional = None retrieved_doc_embeds: Optional = None retrieved_doc_ids: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None question_encoder_last_hidden_state: Optional = None question_enc_hidden_states: Optional = None question_enc_attentions: Optional = None generator_enc_last_hidden_state: Optional = None generator_enc_hidden_states: Optional = None generator_enc_attentions: Optional = None generator_dec_hidden_states: Optional = None generator_dec_attentions: Optional = None generator_cross_attentions: Optional = None )\n```", "```py\n( config question_encoder_tokenizer generator_tokenizer index = None init_retrieval = True )\n```", "```py\n>>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')\n>>> from transformers import RagRetriever\n\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n... )\n\n>>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py\n>>> from transformers import RagRetriever\n\n>>> dataset = (\n...     ...\n... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n>>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n\n>>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n>>> from transformers import RagRetriever\n\n>>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n>>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/dpr-ctx_encoder-single-nq-base\",\n...     index_name=\"custom\",\n...     passages_path=dataset_path,\n...     index_path=index_path,\n... )\n\n>>> # To load the legacy index built originally for Rag's paper\n>>> from transformers import RagRetriever\n\n>>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")\n```", "```py\n( )\n```", "```py\n( docs input_strings prefix n_docs return_tensors = None ) \u2192 export const metadata = 'undefined';tuple(tensors)\n```", "```py\n( question_hidden_states: ndarray n_docs: int ) \u2192 export const metadata = 'undefined';Tuple[np.ndarray, np.ndarray, List[dict]]\n```", "```py\n( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None doc_scores: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None n_docs: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> outputs = model(input_ids=inputs[\"input_ids\"])\n```", "```py\n( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None exclude_bos_score: Optional = None reduce_loss: Optional = None labels: Optional = None n_docs: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n>>> labels = targets[\"input_ids\"]\n>>> outputs = model(input_ids=input_ids, labels=labels)\n\n>>> # or use retriever separately\n>>> model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", use_dummy_dataset=True)\n>>> # 1\\. Encode\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n>>> doc_scores = torch.bmm(\n...     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n... ).squeeze(1)\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=labels,\n... )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None do_deduplication: Optional = None num_return_sequences: Optional = None num_beams: Optional = None n_docs: Optional = None **model_kwargs ) \u2192 export const metadata = 'undefined';torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)\n```", "```py\n( config: Optional = None question_encoder: Optional = None generator: Optional = None retriever: Optional = None **kwargs )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None encoder_outputs: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None past_key_values: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_retrieved: Optional = None do_marginalize: Optional = None reduce_loss: Optional = None labels: Optional = None n_docs: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_rag.RetrievAugLMMarginOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n>>> labels = targets[\"input_ids\"]\n>>> outputs = model(input_ids=input_ids, labels=labels)\n\n>>> # or use retriever separately\n>>> model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n>>> # 1\\. Encode\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n>>> doc_scores = torch.bmm(\n...     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n... ).squeeze(1)\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=labels,\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None context_input_ids: Optional = None context_attention_mask: Optional = None doc_scores: Optional = None n_docs: Optional = None generation_config: Optional = None prefix_allowed_tokens_fn: Callable = None logits_processor: Optional = [] stopping_criteria: Optional = [] **kwargs ) \u2192 export const metadata = 'undefined';torch.LongTensor of shape (batch_size * num_return_sequences, sequence_length)\n```", "```py\n( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None load_weight_prefix: Optional[str] = None **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[Union[np.ndarray, tf.Tensor]]] | None = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None output_retrieved: bool | None = None n_docs: int | None = None return_dict: bool | None = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever, from_pt=True)\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> input_ids = input_dict[\"input_ids\"]\n>>> outputs = model(input_ids)\n```", "```py\n( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None output_retrieved: Optional[bool] = None n_docs: Optional[int] = None exclude_bos_score: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None reduce_loss: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagSequenceForGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagSequenceForGeneration.from_pretrained(\n...     \"facebook/rag-sequence-nq\", retriever=retriever, from_pt=True\n... )\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> outputs = model(input_dict, output_retrieved=True)\n\n>>> # or use retriever separately\n>>> # 1\\. Encode\n>>> input_ids = input_dict[\"input_ids\"]\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors=\"tf\")\n>>> doc_scores = tf.squeeze(\n...     tf.matmul(\n...         tf.expand_dims(question_hidden_states, axis=1), docs_dict[\"retrieved_doc_embeds\"], transpose_b=True\n...     ),\n...     axis=1,\n... )\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     inputs=None,\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=input_dict[\"labels\"],\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None context_input_ids = None context_attention_mask = None doc_scores = None do_deduplication = None num_return_sequences = None num_beams = None n_docs = None **model_kwargs ) \u2192 export const metadata = 'undefined';tf.Tensor of shape (batch_size * num_return_sequences, sequence_length)\n```", "```py\n( config: Optional[PretrainedConfig] = None question_encoder: Optional[TFPreTrainedModel] = None generator: Optional[TFPreTrainedModel] = None retriever: Optional[RagRetriever] = None **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[Union[np.ndarray, tf.Tensor]]] | None = None doc_scores: np.ndarray | tf.Tensor | None = None context_input_ids: np.ndarray | tf.Tensor | None = None context_attention_mask: np.ndarray | tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None output_retrieved: bool | None = None n_docs: int | None = None do_marginalize: bool | None = None labels: np.ndarray | tf.Tensor | None = None reduce_loss: bool | None = None return_dict: bool | None = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.rag.modeling_tf_rag.TFRetrievAugLMMarginOutput or tuple(tf.Tensor)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagTokenForGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever, from_pt=True)\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> outputs = model(input_dict, output_retrieved=True)\n\n>>> # or use retriever separately\n>>> # 1\\. Encode\n>>> input_ids = input_dict[\"input_ids\"]\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors=\"tf\")\n>>> doc_scores = tf.squeeze(\n...     tf.matmul(\n...         tf.expand_dims(question_hidden_states, axis=1), docs_dict[\"retrieved_doc_embeds\"], transpose_b=True\n...     ),\n...     axis=1,\n... )\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     inputs=None,\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=input_dict[\"labels\"],\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None context_input_ids = None context_attention_mask = None doc_scores = None n_docs = None generation_config = None logits_processor = [] **kwargs ) \u2192 export const metadata = 'undefined';tf.Tensor of shape (batch_size * num_return_sequences, sequence_length)\n```"]