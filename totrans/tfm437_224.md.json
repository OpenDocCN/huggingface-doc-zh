["```py\n>>> # To load the default \"wiki_dpr\" dataset with 21M passages from wikipedia (index name is 'compressed' or 'exact')\n>>> from transformers import RagRetriever\n\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/dpr-ctx_encoder-single-nq-base\", dataset=\"wiki_dpr\", index_name=\"compressed\"\n... )\n\n>>> # To load your own indexed dataset built with the datasets library. More info on how to build the indexed dataset in examples/rag/use_own_knowledge_dataset.py\n>>> from transformers import RagRetriever\n\n>>> dataset = (\n...     ...\n... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n>>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n\n>>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n>>> from transformers import RagRetriever\n\n>>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n>>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/dpr-ctx_encoder-single-nq-base\",\n...     index_name=\"custom\",\n...     passages_path=dataset_path,\n...     index_path=index_path,\n... )\n\n>>> # To load the legacy index built originally for Rag's paper\n>>> from transformers import RagRetriever\n\n>>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", index_name=\"legacy\")\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> outputs = model(input_ids=inputs[\"input_ids\"])\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagSequenceForGeneration\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n>>> labels = targets[\"input_ids\"]\n>>> outputs = model(input_ids=input_ids, labels=labels)\n\n>>> # or use retriever separately\n>>> model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", use_dummy_dataset=True)\n>>> # 1\\. Encode\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n>>> doc_scores = torch.bmm(\n...     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n... ).squeeze(1)\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=labels,\n... )\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, RagTokenForGeneration\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever)\n\n>>> inputs = tokenizer(\"How many people live in Paris?\", return_tensors=\"pt\")\n>>> targets = tokenizer(text_target=\"In Paris, there are 10 million people.\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n>>> labels = targets[\"input_ids\"]\n>>> outputs = model(input_ids=input_ids, labels=labels)\n\n>>> # or use retriever separately\n>>> model = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", use_dummy_dataset=True)\n>>> # 1\\. Encode\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.detach().numpy(), return_tensors=\"pt\")\n>>> doc_scores = torch.bmm(\n...     question_hidden_states.unsqueeze(1), docs_dict[\"retrieved_doc_embeds\"].float().transpose(1, 2)\n... ).squeeze(1)\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=labels,\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-base\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-base\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagModel.from_pretrained(\"facebook/rag-token-base\", retriever=retriever, from_pt=True)\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> input_ids = input_dict[\"input_ids\"]\n>>> outputs = model(input_ids)\n```", "```py\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagSequenceForGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-sequence-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagSequenceForGeneration.from_pretrained(\n...     \"facebook/rag-sequence-nq\", retriever=retriever, from_pt=True\n... )\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> outputs = model(input_dict, output_retrieved=True)\n\n>>> # or use retriever separately\n>>> # 1\\. Encode\n>>> input_ids = input_dict[\"input_ids\"]\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors=\"tf\")\n>>> doc_scores = tf.squeeze(\n...     tf.matmul(\n...         tf.expand_dims(question_hidden_states, axis=1), docs_dict[\"retrieved_doc_embeds\"], transpose_b=True\n...     ),\n...     axis=1,\n... )\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     inputs=None,\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=input_dict[\"labels\"],\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```", "```py\n>>> import tensorflow as tf\n>>> from transformers import AutoTokenizer, RagRetriever, TFRagTokenForGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/rag-token-nq\")\n>>> retriever = RagRetriever.from_pretrained(\n...     \"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True\n... )\n>>> # initialize with RagRetriever to do everything in one forward call\n>>> model = TFRagTokenForGeneration.from_pretrained(\"facebook/rag-token-nq\", retriever=retriever, from_pt=True)\n\n>>> input_dict = tokenizer.prepare_seq2seq_batch(\n...     \"How many people live in Paris?\", \"In Paris, there are 10 million people.\", return_tensors=\"tf\"\n... )\n>>> outputs = model(input_dict, output_retrieved=True)\n\n>>> # or use retriever separately\n>>> # 1\\. Encode\n>>> input_ids = input_dict[\"input_ids\"]\n>>> question_hidden_states = model.question_encoder(input_ids)[0]\n>>> # 2\\. Retrieve\n>>> docs_dict = retriever(input_ids.numpy(), question_hidden_states.numpy(), return_tensors=\"tf\")\n>>> doc_scores = tf.squeeze(\n...     tf.matmul(\n...         tf.expand_dims(question_hidden_states, axis=1), docs_dict[\"retrieved_doc_embeds\"], transpose_b=True\n...     ),\n...     axis=1,\n... )\n>>> # 3\\. Forward to generator\n>>> outputs = model(\n...     inputs=None,\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n...     decoder_input_ids=input_dict[\"labels\"],\n... )\n\n>>> # or directly generate\n>>> generated = model.generate(\n...     context_input_ids=docs_dict[\"context_input_ids\"],\n...     context_attention_mask=docs_dict[\"context_attention_mask\"],\n...     doc_scores=doc_scores,\n... )\n>>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n```"]