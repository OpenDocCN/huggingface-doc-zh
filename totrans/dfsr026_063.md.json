["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/t2i_adapter\npip install -r requirements.txt\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_t2i_adapter_sdxl.py \\\n  ----gradient_accumulation_steps=4\n```", "```py\nconditioning_image_transforms = transforms.Compose(\n    [\n        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n        transforms.CenterCrop(args.resolution),\n        transforms.ToTensor(),\n    ]\n)\n```", "```py\nif args.adapter_model_name_or_path:\n    logger.info(\"Loading existing adapter weights.\")\n    t2iadapter = T2IAdapter.from_pretrained(args.adapter_model_name_or_path)\nelse:\n    logger.info(\"Initializing t2iadapter weights.\")\n    t2iadapter = T2IAdapter(\n        in_channels=3,\n        channels=(320, 640, 1280, 1280),\n        num_res_blocks=2,\n        downscale_factor=16,\n        adapter_type=\"full_adapter_xl\",\n    )\n```", "```py\nparams_to_optimize = t2iadapter.parameters()\noptimizer = optimizer_class(\n    params_to_optimize,\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```", "```py\nt2iadapter_image = batch[\"conditioning_pixel_values\"].to(dtype=weight_dtype)\ndown_block_additional_residuals = t2iadapter(t2iadapter_image)\ndown_block_additional_residuals = [\n    sample.to(dtype=weight_dtype) for sample in down_block_additional_residuals\n]\n\nmodel_pred = unet(\n    inp_noisy_latents,\n    timesteps,\n    encoder_hidden_states=batch[\"prompt_ids\"],\n    added_cond_kwargs=batch[\"unet_added_conditions\"],\n    down_block_additional_residuals=down_block_additional_residuals,\n).sample\n```", "```py\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png\n```", "```py\nexport MODEL_DIR=\"stabilityai/stable-diffusion-xl-base-1.0\"\nexport OUTPUT_DIR=\"path to save model\"\n\naccelerate launch train_t2i_adapter_sdxl.py \\\n --pretrained_model_name_or_path=$MODEL_DIR \\\n --output_dir=$OUTPUT_DIR \\\n --dataset_name=fusing/fill50k \\\n --mixed_precision=\"fp16\" \\\n --resolution=1024 \\\n --learning_rate=1e-5 \\\n --max_train_steps=15000 \\\n --validation_image \"./conditioning_image_1.png\" \"./conditioning_image_2.png\" \\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\n --validation_steps=100 \\\n --train_batch_size=1 \\\n --gradient_accumulation_steps=4 \\\n --report_to=\"wandb\" \\\n --seed=42 \\\n --push_to_hub\n```", "```py\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, EulerAncestralDiscreteSchedulerTest\nfrom diffusers.utils import load_image\nimport torch\n\nadapter = T2IAdapter.from_pretrained(\"path/to/adapter\", torch_dtype=torch.float16)\npipeline = StableDiffusionXLAdapterPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", adapter=adapter, torch_dtype=torch.float16\n)\n\npipeline.scheduler = EulerAncestralDiscreteSchedulerTest.from_config(pipe.scheduler.config)\npipeline.enable_xformers_memory_efficient_attention()\npipeline.enable_model_cpu_offload()\n\ncontrol_image = load_image(\"./conditioning_image_1.png\")\nprompt = \"pale golden rod circle with old lace background\"\n\ngenerator = torch.manual_seed(0)\nimage = pipeline(\n    prompt, image=control_image, generator=generator\n).images[0]\nimage.save(\"./output.png\")\n```"]