["```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( attention_op: Optional = None )\n```", "```py\n>>> import torch\n>>> from diffusers import UNet2DConditionModel\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> model = UNet2DConditionModel.from_pretrained(\n...     \"stabilityai/stable-diffusion-2-1\", subfolder=\"unet\", torch_dtype=torch.float16\n... )\n>>> model = model.to(\"cuda\")\n>>> model.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n```", "```py\n( pretrained_model_name_or_path: Union **kwargs )\n```", "```py\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n```", "```py\nSome weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\n( only_trainable: bool = False exclude_embeddings: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\nfrom diffusers import UNet2DConditionModel\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\nunet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\nunet.num_parameters(only_trainable=True)\n859520964\n```", "```py\n( save_directory: Union is_main_process: bool = True save_function: Optional = None safe_serialization: bool = True variant: Optional = None push_to_hub: bool = False **kwargs )\n```", "```py\n( )\n```", "```py\n( pretrained_model_name_or_path: Union dtype: dtype = <class 'jax.numpy.float32'> *model_args **kwargs )\n```", "```py\n>>> from diffusers import FlaxUNet2DConditionModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"./test/saved_model/\")\n```", "```py\nSome weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\n( save_directory: Union params: Union is_main_process: bool = True push_to_hub: bool = False **kwargs )\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from diffusers import FlaxUNet2DConditionModel\n\n>>> # load model\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n>>> params = model.to_bf16(params)\n>>> # If you don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> flat_params = traverse_util.flatten_dict(params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> params = model.to_bf16(params, mask)\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from diffusers import FlaxUNet2DConditionModel\n\n>>> # load model\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> # By default, the model params will be in fp32, to cast these to float16\n>>> params = model.to_fp16(params)\n>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> flat_params = traverse_util.flatten_dict(params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> params = model.to_fp16(params, mask)\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from diffusers import FlaxUNet2DConditionModel\n\n>>> # Download model and configuration from huggingface.co\n>>> model, params = FlaxUNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n>>> # By default, the model params will be in fp32, to illustrate the use of this method,\n>>> # we'll first cast to fp16 and back to fp32\n>>> params = model.to_f16(params)\n>>> # now cast back to fp32\n>>> params = model.to_fp32(params)\n```", "```py\n( )\n```", "```py\n( repo_id: str commit_message: Optional = None private: Optional = None token: Optional = None create_pr: bool = False safe_serialization: bool = True variant: Optional = None )\n```", "```py\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\")\n\n# Push the `unet` to your namespace with the name \"my-finetuned-unet\".\nunet.push_to_hub(\"my-finetuned-unet\")\n\n# Push the `unet` to an organization with the name \"my-finetuned-unet\".\nunet.push_to_hub(\"your-org/my-finetuned-unet\")\n```"]