- en: Build a custom container for TEI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-embeddings-inference/custom_container](https://huggingface.co/docs/text-embeddings-inference/custom_container)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-embeddings-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/start.f5781b4e.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/scheduler.b108d059.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/singletons.26f524d0.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/paths.e8cea87f.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/entry/app.ca5804ae.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/index.008de539.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/0.a44871a2.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/nodes/3.32656b5b.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/CodeBlock.3968c746.js">
    <link rel="modulepreload" href="/docs/text-embeddings-inference/main/en/_app/immutable/chunks/Heading.88bfeb84.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'You can build our own CPU or CUDA TEI container using Docker. To build a CPU
    container, run the following command in the directory containing your custom Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To build a CUDA container, it is essential to determine the compute capability
    (compute cap) of the GPU that will be used at runtime. This information is crucial
    for the proper configuration of the CUDA containers. The following are the examples
    of runtime compute capabilities for various GPU types:'
  prefs: []
  type: TYPE_NORMAL
- en: Turing (T4, RTX 2000 series, …) - `runtime_compute_cap=75`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A100 - `runtime_compute_cap=80`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A10 - `runtime_compute_cap=86`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ada Lovelace (RTX 4000 series, …) - `runtime_compute_cap=89`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H100 - `runtime_compute_cap=90`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have determined the compute capability is determined, set it as the
    `runtime_compute_cap` variable and build the container as shown in the example
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
