- en: IntelÂ® Extension for PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IntelÂ® PyTorchæ‰©å±•
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/ipex](https://huggingface.co/docs/accelerate/usage_guides/ipex)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://huggingface.co/docs/accelerate/usage_guides/ipex](https://huggingface.co/docs/accelerate/usage_guides/ipex)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[IPEX](https://github.com/intel/intel-extension-for-pytorch) is optimized for
    CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2\. So,
    it is expected to bring performance benefit for Intel CPU generations with AVX-512
    or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might
    result in a better performance under IPEX, but not guaranteed. IPEX provides performance
    optimizations for CPU training with both Float32 and BFloat16\. The usage of BFloat16
    is the main focus of the following sections.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[IPEX](https://github.com/intel/intel-extension-for-pytorch)ç»è¿‡ä¼˜åŒ–ï¼Œé€‚ç”¨äºå…·æœ‰AVX-512æˆ–æ›´é«˜ç‰ˆæœ¬çš„CPUï¼Œå¹¶ä¸”åœ¨ä»…å…·æœ‰AVX2çš„CPUä¸Šä¹Ÿå¯ä»¥æ­£å¸¸è¿è¡Œã€‚å› æ­¤ï¼Œé¢„è®¡åœ¨å…·æœ‰AVX-512æˆ–æ›´é«˜ç‰ˆæœ¬çš„Intel
    CPUä»£æ•°ä¸Šä¼šå¸¦æ¥æ€§èƒ½ä¼˜åŠ¿ï¼Œè€Œä»…å…·æœ‰AVX2çš„CPUï¼ˆä¾‹å¦‚AMD CPUæˆ–è¾ƒæ—§çš„Intel CPUï¼‰åœ¨IPEXä¸‹å¯èƒ½ä¼šè·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œä½†ä¸èƒ½ä¿è¯ã€‚IPEXä¸ºä½¿ç”¨Float32å’ŒBFloat16è¿›è¡ŒCPUè®­ç»ƒæä¾›äº†æ€§èƒ½ä¼˜åŒ–ã€‚ä»¥ä¸‹éƒ¨åˆ†ä¸»è¦å…³æ³¨BFloat16çš„ä½¿ç”¨ã€‚'
- en: Low precision data type BFloat16 has been natively supported on the 3rd Generation
    XeonÂ® Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will
    be supported on the next generation of IntelÂ® XeonÂ® Scalable Processors with IntelÂ®
    Advanced Matrix Extensions (IntelÂ® AMX) instruction set with further boosted performance.
    The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10\.
    At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and
    BFloat16 optimization of operators has been massively enabled in IntelÂ® Extension
    for PyTorch, and partially upstreamed to PyTorch master branch. Users can get
    better performance and user experience with IPEX Auto Mixed Precision.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ç²¾åº¦æ•°æ®ç±»å‹BFloat16å·²ç»åœ¨ç¬¬ä¸‰ä»£XeonÂ® Scalableå¤„ç†å™¨ï¼ˆä¹Ÿç§°ä¸ºCooper Lakeï¼‰ä¸Šæœ¬åœ°æ”¯æŒï¼Œå…·æœ‰AVX512æŒ‡ä»¤é›†ï¼Œå¹¶å°†åœ¨ä¸‹ä¸€ä»£IntelÂ®
    XeonÂ® Scalableå¤„ç†å™¨ä¸Šæ”¯æŒï¼Œå…·æœ‰IntelÂ® Advanced Matrix Extensionsï¼ˆIntelÂ® AMXï¼‰æŒ‡ä»¤é›†ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è‡ªPyTorch-1.10ä»¥æ¥ï¼ŒCPUåç«¯çš„è‡ªåŠ¨æ··åˆç²¾åº¦å·²ç»å¯ç”¨ã€‚åŒæ—¶ï¼Œåœ¨IntelÂ®
    PyTorchæ‰©å±•ä¸­ï¼Œå·²ç»å¤§è§„æ¨¡å¯ç”¨äº†CPUå’ŒBFloat16ä¼˜åŒ–è¿ç®—ç¬¦çš„è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œå¹¶éƒ¨åˆ†ä¸Šæ¸¸åˆ°PyTorchä¸»åˆ†æ”¯ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡IPEXè‡ªåŠ¨æ··åˆç²¾åº¦è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚
- en: 'IPEX installation:'
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPEXå®‰è£…ï¼š
- en: 'IPEX release is following PyTorch, to install via pip:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: IPEXå‘å¸ƒéµå¾ªPyTorchç‰ˆæœ¬ï¼Œå¯ä»¥é€šè¿‡pipè¿›è¡Œå®‰è£…ï¼š
- en: '| PyTorch Version | IPEX version |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| PyTorchç‰ˆæœ¬ | IPEXç‰ˆæœ¬ |'
- en: '| :-: | :-: |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| :-: | :-: |'
- en: '| 2.0 | 2.0.0 |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | 2.0.0 |'
- en: '| 1.13 | 1.13.0 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 1.13 | 1.13.0 |'
- en: '| 1.12 | 1.12.300 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 1.12 | 1.12.300 |'
- en: '| 1.11 | 1.11.200 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 1.11 | 1.11.200 |'
- en: '| 1.10 | 1.10.100 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 1.10 | 1.10.100 |'
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Check more approaches for [IPEX installation](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹æ›´å¤š[IPEXå®‰è£…æ–¹æ³•](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html)ã€‚
- en: How It Works For Training optimization in CPU
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPUè®­ç»ƒä¼˜åŒ–çš„å·¥ä½œåŸç†
- en: ğŸ¤— Accelerate has integrated [IPEX](https://github.com/intel/intel-extension-for-pytorch),
    all you need to do is enabling it through the config.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå·²ç»é›†æˆäº†[IPEX](https://github.com/intel/intel-extension-for-pytorch)ï¼Œæ‚¨åªéœ€è¦é€šè¿‡é…ç½®å¯ç”¨å®ƒã€‚
- en: '**Scenario 1**: Acceleration of No distributed CPU training'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœºæ™¯1**ï¼šæ— åˆ†å¸ƒå¼CPUè®­ç»ƒåŠ é€Ÿ'
- en: 'Run accelerate config on your machine:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œaccelerate configï¼š
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will generate a config file that will be used automatically to properly
    set the default options when doing
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå°†è‡ªåŠ¨ç”¨äºåœ¨è¿›è¡Œæ—¶æ­£ç¡®è®¾ç½®é»˜è®¤é€‰é¡¹
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For instance, here is how you would run the NLP example `examples/nlp_example.py`
    (from the root of the repo) with IPEX enabled. default_config.yaml that is generated
    after `accelerate config`
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨å¯ç”¨IPEXçš„æƒ…å†µä¸‹è¿è¡ŒNLPç¤ºä¾‹`examples/nlp_example.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰çš„æ–¹æ³•ã€‚åœ¨`accelerate
    config`ä¹‹åç”Ÿæˆçš„default_config.yaml
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Scenario 2**: Acceleration of distributed CPU training we use Intel oneCCL
    for communication, combined with IntelÂ® MPI library to deliver flexible, efficient,
    scalable cluster messaging on IntelÂ® architecture. you could refer the [here](https://huggingface.co/docs/transformers/perf_train_cpu_many)
    for the installation guide'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœºæ™¯2**ï¼šåˆ†å¸ƒå¼CPUè®­ç»ƒåŠ é€Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨Intel oneCCLè¿›è¡Œé€šä¿¡ï¼Œç»“åˆIntelÂ® MPIåº“åœ¨IntelÂ®æ¶æ„ä¸Šæä¾›çµæ´»ã€é«˜æ•ˆã€å¯æ‰©å±•çš„é›†ç¾¤æ¶ˆæ¯ä¼ é€’ã€‚æ‚¨å¯ä»¥å‚è€ƒ[è¿™é‡Œ](https://huggingface.co/docs/transformers/perf_train_cpu_many)è¿›è¡Œå®‰è£…æŒ‡å—'
- en: 'Run accelerate config on your machine(node0):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œaccelerate configï¼ˆnode0ï¼‰ï¼š
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For instance, here is how you would run the NLP example `examples/nlp_example.py`
    (from the root of the repo) with IPEX enabled for distributed CPU training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨å¯ç”¨IPEXçš„æƒ…å†µä¸‹è¿è¡ŒNLPç¤ºä¾‹`examples/nlp_example.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰è¿›è¡Œåˆ†å¸ƒå¼CPUè®­ç»ƒçš„æ–¹æ³•ã€‚
- en: default_config.yaml that is generated after `accelerate config`
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`accelerate config`ä¹‹åç”Ÿæˆçš„default_config.yaml
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Set following env and using intel MPI to launch the training
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå¹¶ä½¿ç”¨Intel MPIå¯åŠ¨è®­ç»ƒ
- en: In node0, you need to create a configuration file which contains the IP addresses
    of each node (for example hostfile) and pass that configuration file path as an
    argument.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨node0ä¸­ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåŒ…å«æ¯ä¸ªèŠ‚ç‚¹IPåœ°å€çš„é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚hostfileï¼‰ï¼Œå¹¶å°†è¯¥é…ç½®æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°ä¼ é€’ã€‚
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, run the following command in node0 and **16DDP** will be enabled in node0,node1,node2,node3
    with BF16 mixed precision:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œåœ¨node0ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå°†åœ¨node0ã€node1ã€node2ã€node3ä¸­å¯ç”¨**16DDP**ï¼Œå¹¶ä½¿ç”¨BF16æ··åˆç²¾åº¦ï¼š
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Related Resources
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸å…³èµ„æº
- en: '[Projectâ€™s github](https://github.com/intel/intel-extension-for-pytorch)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[é¡¹ç›®çš„github](https://github.com/intel/intel-extension-for-pytorch)'
- en: '[API docs](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/api_doc.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[APIæ–‡æ¡£](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/api_doc.html)'
- en: '[Tuning guide](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è°ƒä¼˜æŒ‡å—](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html)'
- en: '[Blogs & Publications](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/blogs_publications.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åšå®¢å’Œå‡ºç‰ˆç‰©](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/blogs_publications.html)'
