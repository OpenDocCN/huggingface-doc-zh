- en: Text-to-Image Generation with Adapter Conditioning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用适配器调节的文本到图像生成
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: '[T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image
    Diffusion Models](https://arxiv.org/abs/2302.08453) by Chong Mou, Xintao Wang,
    Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[T2I-Adapter：学习适配器以挖掘文本到图像扩散模型的更可控能力](https://arxiv.org/abs/2302.08453) 作者：Chong
    Mou，Xintao Wang，Liangbin Xie，Jian Zhang，Zhongang Qi，Ying Shan，Xiaohu Qie。'
- en: Using the pretrained models we can provide control images (for example, a depth
    map) to control Stable Diffusion text-to-image generation so that it follows the
    structure of the depth image and fills in the details.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型，我们可以提供控制图像（例如深度图）来控制稳定扩散文本到图像生成，使其遵循深度图的结构并填充细节。
- en: 'The abstract of the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*The incredible generative ability of large-scale text-to-image (T2I) models
    has demonstrated strong power of learning complex structures and meaningful semantics.
    However, relying solely on text prompts cannot fully take advantage of the knowledge
    learned by the model, especially when flexible and accurate controlling (e.g.,
    color and structure) is needed. In this paper, we aim to “dig out” the capabilities
    that T2I models have implicitly learned, and then explicitly use them to control
    the generation more granularly. Specifically, we propose to learn simple and lightweight
    T2I-Adapters to align internal knowledge in T2I models with external control signals,
    while freezing the original large T2I models. In this way, we can train various
    adapters according to different conditions, achieving rich control and editing
    effects in the color and structure of the generation results. Further, the proposed
    T2I-Adapters have attractive properties of practical value, such as composability
    and generalization ability. Extensive experiments demonstrate that our T2I-Adapter
    has promising generation quality and a wide range of applications.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*大规模文本到图像（T2I）模型的惊人生成能力展示了学习复杂结构和有意义语义的强大力量。然而，仅依赖文本提示无法充分利用模型隐式学习的知识，特别是在需要灵活和准确控制（例如颜色和结构）时。在本文中，我们旨在“挖掘”T2I模型隐式学习的能力，然后明确地使用它们来更加精细地控制生成。具体来说，我们提出学习简单轻量级的T2I-Adapter，将T2I模型内部知识与外部控制信号对齐，同时冻结原始大型T2I模型。通过这种方式，我们可以根据不同条件训练各种适配器，实现生成结果颜色和结构的丰富控制和编辑效果。此外，所提出的T2I-Adapter具有实用价值的吸引人特性，如可组合性和泛化能力。大量实验证明我们的T2I-Adapter具有有前途的生成质量和广泛的应用。*'
- en: This model was contributed by the community contributor [HimariO](https://github.com/HimariO)
    ❤️ .
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由社区贡献者[HimariO](https://github.com/HimariO) ❤️ 贡献的。
- en: 'Available Pipelines:'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用流水线：
- en: '| Pipeline | Tasks | Demo |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 流水线 | 任务 | 演示 |'
- en: '| --- | --- | :-: |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | :-: |'
- en: '| [StableDiffusionAdapterPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py)
    | *Text-to-Image Generation with T2I-Adapter Conditioning* | - |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| [StableDiffusionAdapterPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py)
    | *使用T2I-Adapter进行文本到图像生成* | - |'
- en: '| [StableDiffusionXLAdapterPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py)
    | *Text-to-Image Generation with T2I-Adapter Conditioning on StableDiffusion-XL*
    | - |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| [StableDiffusionXLAdapterPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py)
    | *在StableDiffusion-XL上使用T2I-Adapter进行文本到图像生成* | - |'
- en: Usage example with the base model of StableDiffusion-1.4/1.5
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用StableDiffusion-1.4/1.5基础模型的使用示例
- en: In the following we give a simple example of how to use a *T2I-Adapter* checkpoint
    with Diffusers for inference based on StableDiffusion-1.4/1.5. All adapters use
    the same pipeline.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们给出一个简单的例子，展示如何使用基于StableDiffusion-1.4/1.5的*T2I-Adapter*检查点进行推断。所有适配器使用相同的流水线。
- en: Images are first converted into the appropriate *control image* format.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将图像转换为适当的*控制图像*格式。
- en: The *control image* and *prompt* are passed to the [StableDiffusionAdapterPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionAdapterPipeline).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*控制图像*和*prompt*被传递给[StableDiffusionAdapterPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionAdapterPipeline)。'
- en: Let’s have a look at a simple example using the [Color Adapter](https://huggingface.co/TencentARC/t2iadapter_color_sd14v1).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，使用[Color Adapter](https://huggingface.co/TencentARC/t2iadapter_color_sd14v1)。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![img](../Images/2a901c2e8d29e8c7dc4cbfbfa08fc50c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/2a901c2e8d29e8c7dc4cbfbfa08fc50c.png)'
- en: Then we can create our color palette by simply resizing it to 8 by 8 pixels
    and then scaling it back to original size.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过简单地将其调整为8x8像素的调色板，然后将其缩放回原始大小来创建我们的调色板。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s take a look at the processed image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看处理后的图像。
- en: '![img](../Images/719bc889f51dd866789e50c89680e506.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/719bc889f51dd866789e50c89680e506.png)'
- en: Next, create the adapter pipeline
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建适配器流水线
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, pass the prompt and control image to the pipeline
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将提示和控制图像传递给流水线
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![img](../Images/b794d45ab3fd924aa913420d359cfa19.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/b794d45ab3fd924aa913420d359cfa19.png)'
- en: Usage example with the base model of StableDiffusion-XL
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用StableDiffusion-XL基础模型的使用示例
- en: In the following we give a simple example of how to use a *T2I-Adapter* checkpoint
    with Diffusers for inference based on StableDiffusion-XL. All adapters use the
    same pipeline.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们给出一个简单的例子，展示如何使用基于StableDiffusion-XL的* T2I-Adapter *检查点进行推断。所有适配器使用相同的流水线。
- en: Images are first downloaded into the appropriate *control image* format.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将图像下载到适当的*控制图像*格式中。
- en: The *control image* and *prompt* are passed to the [StableDiffusionXLAdapterPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionXLAdapterPipeline).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*控制图像* 和 *提示* 被传递给 [StableDiffusionXLAdapterPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/adapter#diffusers.StableDiffusionXLAdapterPipeline)。'
- en: Let’s have a look at a simple example using the [Sketch Adapter](https://huggingface.co/Adapter/t2iadapter/tree/main/sketch_sdxl_1.0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，使用 [Sketch Adapter](https://huggingface.co/Adapter/t2iadapter/tree/main/sketch_sdxl_1.0)。
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![img](../Images/9ea552d711091022ac1233d7acbc6d31.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/9ea552d711091022ac1233d7acbc6d31.png)'
- en: Then, create the adapter pipeline
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建适配器管道
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, pass the prompt and control image to the pipeline
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将提示和控制图像传递给管道
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![img](../Images/95e8c462f187f8abba972e5701bf6240.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/95e8c462f187f8abba972e5701bf6240.png)'
- en: Available checkpoints
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用的检查点
- en: Non-diffusers checkpoints can be found under [TencentARC/T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 非扩散检查点可以在[TencentARC/T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter/tree/main/models)下找到。
- en: T2I-Adapter with Stable Diffusion 1.4
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有稳定扩散1.4的T2I-Adapter
- en: '| Model Name | Control Image Overview | Control Image Example | Generated Image
    Example |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 控制图像概述 | 控制图像示例 | 生成的图像示例 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| [TencentARC/t2iadapter_color_sd14v1](https://huggingface.co/TencentARC/t2iadapter_color_sd14v1)
    *Trained with spatial color palette* | An image with 8x8 color palette. | [![](../Images/fa957b819779fa14820c846c95881191.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_sample_input.png)
    | [![](../Images/1ddd3dee6b7e73551bda407a91d49919.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_sample_output.png)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_color_sd14v1](https://huggingface.co/TencentARC/t2iadapter_color_sd14v1)
    *使用空间色彩调色板训练* | 一个带有8x8色彩调色板的图像。 | [![](../Images/fa957b819779fa14820c846c95881191.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_sample_input.png)
    | [![](../Images/1ddd3dee6b7e73551bda407a91d49919.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/color_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_canny_sd14v1](https://huggingface.co/TencentARC/t2iadapter_canny_sd14v1)
    *Trained with canny edge detection* | A monochrome image with white edges on a
    black background. | [![](../Images/8e298073b9d6c4ea9667e4fb3a332538.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/canny_sample_input.png)
    | [![](../Images/812c28716f8615d885ddafb387dbc5ad.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/canny_sample_output.png)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_canny_sd14v1](https://huggingface.co/TencentARC/t2iadapter_canny_sd14v1)
    *使用canny边缘检测训练* | 黑色背景上带有白色边缘的单色图像。 | [![](../Images/8e298073b9d6c4ea9667e4fb3a332538.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/canny_sample_input.png)
    | [![](../Images/812c28716f8615d885ddafb387dbc5ad.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/canny_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_sketch_sd14v1](https://huggingface.co/TencentARC/t2iadapter_sketch_sd14v1)
    *Trained with [PidiNet](https://github.com/zhuoinoulu/pidinet) edge detection*
    | A hand-drawn monochrome image with white outlines on a black background. | [![](../Images/1ed7b4a392584b8a42452feb64f516e4.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/sketch_sample_input.png)
    | [![](../Images/f40a8f0464bef455bc3620faff513e9a.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/sketch_sample_output.png)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_sketch_sd14v1](https://huggingface.co/TencentARC/t2iadapter_sketch_sd14v1)
    *使用[PidiNet](https://github.com/zhuoinoulu/pidinet)边缘检测训练* | 在黑色背景上带有白色轮廓的手绘单色图像。
    | [![](../Images/1ed7b4a392584b8a42452feb64f516e4.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/sketch_sample_input.png)
    | [![](../Images/f40a8f0464bef455bc3620faff513e9a.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/sketch_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_depth_sd14v1](https://huggingface.co/TencentARC/t2iadapter_depth_sd14v1)
    *Trained with Midas depth estimation* | A grayscale image with black representing
    deep areas and white representing shallow areas. | [![](../Images/ab0f67dc7a64595580e36a8c5a466d2f.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png)
    | [![](../Images/fa8d3fdd93d59c2650e9fe73d601f022.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_output.png)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_depth_sd14v1](https://huggingface.co/TencentARC/t2iadapter_depth_sd14v1)
    *使用Midas深度估计训练* | 灰度图像，黑色代表深区域，白色代表浅区域。 | [![](../Images/ab0f67dc7a64595580e36a8c5a466d2f.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_input.png)
    | [![](../Images/fa8d3fdd93d59c2650e9fe73d601f022.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/depth_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_openpose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_openpose_sd14v1)
    *Trained with OpenPose bone image* | A [OpenPose bone](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
    image. | [![](../Images/d6c1dd593a346b95c40790e5cbe0ab35.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/openpose_sample_input.png)
    | [![](../Images/1d009b15e04f4c1ece4146c450ea2793.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/openpose_sample_output.png)
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_openpose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_openpose_sd14v1)
    *使用OpenPose骨骼图像训练* | 一个 [OpenPose骨骼](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
    图像。 | [![](../Images/d6c1dd593a346b95c40790e5cbe0ab35.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/openpose_sample_input.png)
    | [![](../Images/1d009b15e04f4c1ece4146c450ea2793.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/openpose_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_keypose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_keypose_sd14v1)
    *Trained with mmpose skeleton image* | A [mmpose skeleton](https://github.com/open-mmlab/mmpose)
    image. | [![](../Images/59ff2153bf794816a617bc7ba523076c.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png)
    | [![](../Images/d80398d56ef1b3bfd91f70584d1d458c.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_output.png)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_keypose_sd14v1](https://huggingface.co/TencentARC/t2iadapter_keypose_sd14v1)
    *使用mmpose骨架图像训练* | 一个[mmpose骨架](https://github.com/open-mmlab/mmpose)图像。 | [![](../Images/59ff2153bf794816a617bc7ba523076c.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_input.png)
    | [![](../Images/d80398d56ef1b3bfd91f70584d1d458c.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/keypose_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_seg_sd14v1](https://huggingface.co/TencentARC/t2iadapter_seg_sd14v1)
    *Trained with semantic segmentation* | An [custom](https://github.com/TencentARC/T2I-Adapter/discussions/25)
    segmentation protocol image. | [![](../Images/9c7f950344c931f0e1fcd603e1abf405.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/seg_sample_input.png)
    | [![](../Images/946bdcddf7573de1109f79465b38dd65.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/seg_sample_output.png)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_seg_sd14v1](https://huggingface.co/TencentARC/t2iadapter_seg_sd14v1)
    *使用语义分割训练* | 一个[自定义](https://github.com/TencentARC/T2I-Adapter/discussions/25)分割协议图像。
    | [![](../Images/9c7f950344c931f0e1fcd603e1abf405.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/seg_sample_input.png)
    | [![](../Images/946bdcddf7573de1109f79465b38dd65.png)](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/t2i-adapter/seg_sample_output.png)
    |'
- en: '| [TencentARC/t2iadapter_canny_sd15v2](https://huggingface.co/TencentARC/t2iadapter_canny_sd15v2)
    |  |  |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_canny_sd15v2](https://huggingface.co/TencentARC/t2iadapter_canny_sd15v2)
    |  |  |  |'
- en: '| [TencentARC/t2iadapter_depth_sd15v2](https://huggingface.co/TencentARC/t2iadapter_depth_sd15v2)
    |  |  |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_depth_sd15v2](https://huggingface.co/TencentARC/t2iadapter_depth_sd15v2)
    |  |  |  |'
- en: '| [TencentARC/t2iadapter_sketch_sd15v2](https://huggingface.co/TencentARC/t2iadapter_sketch_sd15v2)
    |  |  |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_sketch_sd15v2](https://huggingface.co/TencentARC/t2iadapter_sketch_sd15v2)
    |  |  |  |'
- en: '| [TencentARC/t2iadapter_zoedepth_sd15v1](https://huggingface.co/TencentARC/t2iadapter_zoedepth_sd15v1)
    |  |  |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [TencentARC/t2iadapter_zoedepth_sd15v1](https://huggingface.co/TencentARC/t2iadapter_zoedepth_sd15v1)
    |  |  |  |'
- en: '| [Adapter/t2iadapter, subfolder=‘sketch_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/sketch_sdxl_1.0)
    |  |  |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [Adapter/t2iadapter, subfolder=‘sketch_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/sketch_sdxl_1.0)
    |  |  |  |'
- en: '| [Adapter/t2iadapter, subfolder=‘canny_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/canny_sdxl_1.0)
    |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [Adapter/t2iadapter, subfolder=‘canny_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/canny_sdxl_1.0)
    |  |  |  |'
- en: '| [Adapter/t2iadapter, subfolder=‘openpose_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/openpose_sdxl_1.0)
    |  |  |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [Adapter/t2iadapter, subfolder=‘openpose_sdxl_1.0’](https://huggingface.co/Adapter/t2iadapter/tree/main/openpose_sdxl_1.0)
    |  |  |  |'
- en: Combining multiple adapters
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合多个适配器
- en: '`MultiAdapter` can be used for applying multiple conditionings at once.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiAdapter`可用于同时应用多个调节。'
- en: Here we use the keypose adapter for the character posture and the depth adapter
    for creating the scene.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用关键姿势适配器来表示角色姿势，使用深度适配器来创建场景。
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The two control images look as such:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 两个控制图像如下所示：
- en: '![img](../Images/59ff2153bf794816a617bc7ba523076c.png) ![img](../Images/ab0f67dc7a64595580e36a8c5a466d2f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/59ff2153bf794816a617bc7ba523076c.png) ![img](../Images/ab0f67dc7a64595580e36a8c5a466d2f.png)'
- en: '`MultiAdapter` combines keypose and depth adapters.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiAdapter`结合了关键姿势和深度适配器。'
- en: '`adapter_conditioning_scale` balances the relative influence of the different
    adapters.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`adapter_conditioning_scale`平衡不同适配器的相对影响。'
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![img](../Images/12bbec650ea2bf1db0343ef0f09256cb.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![img](../Images/12bbec650ea2bf1db0343ef0f09256cb.png)'
- en: T2I-Adapter vs ControlNet
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T2I-Adapter vs ControlNet
- en: T2I-Adapter is similar to [ControlNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet).
    T2I-Adapter uses a smaller auxiliary network which is only run once for the entire
    diffusion process. However, T2I-Adapter performs slightly worse than ControlNet.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: T2I-Adapter类似于[ControlNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet)。T2I-Adapter使用一个较小的辅助网络，仅在整个扩散过程中运行一次。然而，T2I-Adapter的性能略逊于ControlNet。
- en: StableDiffusionAdapterPipeline
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稳定扩散适配器管道
- en: '### `class diffusers.StableDiffusionAdapterPipeline`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.StableDiffusionAdapterPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L166)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L166)'
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter` (`T2IAdapter` or `MultiAdapter` or `List[T2IAdapter]`) — Provides
    additional conditioning to the unet during the denoising process. If you set multiple
    Adapter as a list, the outputs from each Adapter are added together to create
    one combined additional conditioning.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter`（`T2IAdapter`或`MultiAdapter`或`List[T2IAdapter]`） - 在去噪过程中为unet提供额外的调节。如果将多个Adapter设置为列表，则每个Adapter的输出将相加以创建一个组合的额外调节。'
- en: '`adapter_weights` (`List[float]`, *optional*, defaults to None) — List of floats
    representing the weight which will be multiply to each adapter’s output before
    adding them together.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_weights`（`List[float]`，*可选*，默认为None） - 代表将乘以每个适配器输出的权重的浮点数列表，然后将它们相加。'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL））
    - 变分自动编码器（VAE）模型，用于对图像进行编码和解码，以及从潜在表示中。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder. Stable Diffusion uses
    the text portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel),
    specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
    variant.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_encoder`（`CLIPTextModel`） - 冻结的文本编码器。稳定扩散使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。'
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer` (`CLIPTokenizer`) — [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)类的分词器。'
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the encoded image latents.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — 用于去噪编码图像潜变量的条件U-Net架构。'
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — 用于与 `unet` 结合使用的调度器，以去噪编码图像潜变量。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。'
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) — Classification module that
    estimates whether generated images could be considered offensive or harmful. Please,
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for details.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成图像是否可能被视为具有攻击性或有害的分类模块。请参考[model
    card](https://huggingface.co/runwayml/stable-diffusion-v1-5)获取详细信息。'
- en: '`feature_extractor` (`CLIPFeatureExtractor`) — Model that extracts features
    from generated images to be used as inputs for the `safety_checker`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_extractor` (`CLIPFeatureExtractor`) — 从生成的图像中提取特征以用作 `safety_checker`
    的输入的模型。'
- en: Pipeline for text-to-image generation using Stable Diffusion augmented with
    T2I-Adapter [https://arxiv.org/abs/2302.08453](https://arxiv.org/abs/2302.08453)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稳定扩散增强的文本到图像生成管道，配备了T2I-Adapter [https://arxiv.org/abs/2302.08453](https://arxiv.org/abs/2302.08453)
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（如下载或保存、在特定设备上运行等）。
- en: '#### `__call__`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L699)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L699)'
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt` (`str` 或 `List[str]`, *optional*) — 用于引导图像生成的提示。如果未定义，则必须传递 `prompt_embeds`。'
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`
    or `List[PIL.Image.Image]` or `List[List[PIL.Image.Image]]`) — The Adapter input
    condition. Adapter uses this input condition to generate guidance to Unet. If
    the type is specified as `Torch.FloatTensor`, it is passed to Adapter as is. PIL.Image.Image`
    can also be accepted as an image. The control image is automatically resized to
    fit the output image.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`
    或 `List[PIL.Image.Image]` 或 `List[List[PIL.Image.Image]]`) — 适配器输入条件。适配器使用此输入条件生成对Unet的引导。如果类型指定为
    `Torch.FloatTensor`，则按原样传递给适配器。`PIL.Image.Image` 也可以作为图像接受。控制图像会自动调整大小以适应输出图像。'
- en: '`height` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The height in pixels of the generated image.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — 生成图像的像素高度。'
- en: '`width` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The width in pixels of the generated image.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — 生成图像的像素宽度。'
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_inference_steps` (`int`, *optional*, defaults to 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。'
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process with schedulers which support a `timesteps` argument in their `set_timesteps`
    method. If not defined, the default behavior when `num_inference_steps` is passed
    will be used. Must be in descending order.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps` (`List[int]`, *optional*) — 用于支持在其 `set_timesteps` 方法中具有 `timesteps`
    参数的调度器进行去噪过程的自定义时间步。如果未定义，则将使用传递 `num_inference_steps` 时的默认行为。必须按降序排列。'
- en: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale` (`float`, *optional*, defaults to 7.5) — 在[无分类器扩散引导](https://arxiv.org/abs/2207.12598)中定义的引导比例。`guidance_scale`
    定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的 `w`。通过设置 `guidance_scale
    > 1` 来启用引导比例。更高的引导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`.
    instead. If not defined, one has to pass `negative_prompt_embeds`. instead. Ignored
    when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt` (`str` or `List[str]`, *optional*) — 不指导图像生成的提示或提示。如果未定义，则必须传递
    `negative_prompt_embeds`。而不是。如果未定义，则必须传递 `negative_prompt_embeds`。而不是。在不使用指导时被忽略（即，如果
    `guidance_scale` 小于 `1`，则被忽略）。'
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的图像数量。'
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta` (`float`, *optional*, defaults to 0.0) — 对应于 DDIM 论文中的参数 eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于
    [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，对其他情况将被忽略。'
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 一个或多个
    [torch 生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。'
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents` (`torch.FloatTensor`, *optional*) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机
    `generator` 进行采样生成潜变量张量。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从
    `prompt` 输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从
    `negative_prompt` 输入参数生成负文本嵌入。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, defaults to `"pil"`) — 生成图像的输出格式。选择在 [PIL](https://pillow.readthedocs.io/en/stable/)
    之间：`PIL.Image.Image` 或 `np.array`。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` instead
    of a plain tuple.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回 `~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput`
    而不是普通的元组。'
- en: '`callback` (`Callable`, *optional*) — A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 一个函数，在推断过程中每 `callback_steps` 步调用一次。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, defaults to 1) — `callback` 函数将被调用的频率。如果未指定，回调将在每一步调用。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttnProcessor` as defined under `self.processor`
    in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 一个 kwargs 字典，如果指定，将传递给 `AttnProcessor`，如在
    [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中的 `self.processor` 中定义的那样。'
- en: '`adapter_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) — The outputs of the adapter are multiplied by `adapter_conditioning_scale`
    before they are added to the residual in the original unet. If multiple adapters
    are specified in init, you can set the corresponding scale as a list.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) — 适配器的输出在添加到原始 unet 中的残差之前将乘以 `adapter_conditioning_scale`。如果在初始化中指定了多个适配器，可以将相应的比例设置为列表。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 表示将使用预终层的输出来计算提示嵌入。'
- en: Returns
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` or `tuple`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` 或 `tuple`'
- en: '`~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` if `return_dict`
    is True, otherwise a `tuple. When returning a tuple, the first element is a list
    with the generated images, and the second element is a list of` bool`s denoting
    whether the corresponding generated image likely represents "not-safe-for-work"
    (nsfw) content, according to the` safety_checker`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 True，则为 `~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput`，否则为元组。当返回元组时，第一个元素是生成的图像列表，第二个元素是一个列表，其中包含表示相应生成图像是否可能代表“不适宜工作”（nsfw）内容的
    `bool`，根据 `safety_checker`。
- en: Function invoked when calling the pipeline for generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 调用管道生成时调用的函数。
- en: 'Examples:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `enable_attention_slicing`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_attention_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) — When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slice_size`（`str`或`int`，*可选*，默认为`"auto"`）— 当为`"auto"`时，将输入减半到注意力头部，因此注意力将分两步计算。如果为`"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim
    // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。'
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片注意力计算。当启用此选项时，注意力模块将输入张量分割成多个切片，以便在几个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低是有用的。
- en: ⚠️ Don’t enable attention slicing if you’re already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won’t need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 如果您已经在使用PyTorch 2.0或xFormers中的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常内存高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重的减速！
- en: 'Examples:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#### `disable_attention_slicing`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_attention_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将一次计算。
- en: '#### `enable_vae_slicing`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L252)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L252)'
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 启用切片VAE解码。当启用此选项时，VAE将把输入张量分割成多个切片，以便在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小是有用的。
- en: '#### `disable_vae_slicing`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_vae_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L260)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L260)'
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将回到一步计算解码。
- en: '#### `enable_xformers_memory_efficient_attention`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_xformers_memory_efficient_attention`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`attention_op` (`Callable`, *optional*) — Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_op`（`Callable`，*可选*）— 用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`操作符的覆盖。'
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 启用[xFormers](https://facebookresearch.github.io/xformers/)中的内存高效注意力。当启用此选项时，您应该观察到较低的GPU内存使用量，并且在推断期间可能会加速。训练期间的加速不被保证。
- en: ⚠️ When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。
- en: 'Examples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '#### `disable_xformers_memory_efficient_attention`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_xformers_memory_efficient_attention`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用[xFormers](https://facebookresearch.github.io/xformers/)中的内存高效注意力。
- en: '#### `disable_freeu`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L655)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L655)'
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用FreeU机制（如果已启用）。
- en: '#### `enable_freeu`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L632)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L632)'
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1`（`float`）— 用于减弱跳过特征的贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2`（`float`）— 用于减弱跳过特征的贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1`（`float`）—用于放大骨干特征贡献的第1阶段的缩放因子。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2`（`float`）—用于放大骨干特征贡献的第2阶段的缩放因子。'
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)。
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后缀表示它们被应用的阶段。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同管道（如Stable Diffusion
    v1、v2和Stable Diffusion XL）的值组合。
- en: '#### `encode_prompt`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode_prompt`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L301)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L301)'
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded device —
    (`torch.device`): torch device'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt`（`str`或`List[str]`，*可选*）—要编码的提示设备—（`torch.device`）：torch设备'
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_images_per_prompt`（`int`）—应为每个提示生成的图像数量'
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_classifier_free_guidance`（`bool`）—是否使用分类器自由指导'
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt`（`str`或`List[str]`，*可选*）—不指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。不使用指导时忽略（即，如果`guidance_scale`小于`1`，则忽略）。'
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，文本嵌入将从`prompt`输入参数生成。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的负文本嵌入。可用于轻松调整文本输入，*例如*提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。'
- en: '`lora_scale` (`float`, *optional*) — A LoRA scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_scale`（`float`，*可选*）—将应用于文本编码器的所有LoRA层的LoRA比例。'
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clip_skip`（`int`，*可选*）—在计算提示嵌入时要跳过的层数。值为1表示将使用前一层的输出来计算提示嵌入。'
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示编码为文本编码器隐藏状态。
- en: '#### `get_guidance_scale_embedding`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_guidance_scale_embedding`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L660)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py#L660)'
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`timesteps` (`torch.Tensor`) — generate embedding vectors at these timesteps'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps`（`torch.Tensor`）—在这些时间步生成嵌入向量'
- en: '`embedding_dim` (`int`, *optional*, defaults to 512) — dimension of the embeddings
    to generate dtype — data type of the generated embeddings'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_dim`（`int`，*可选*，默认为512）—要生成的嵌入的维度dtype—生成的嵌入的数据类型'
- en: Returns
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`torch.FloatTensor`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.FloatTensor`'
- en: Embedding vectors with shape `(len(timesteps), embedding_dim)`
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 形状为`(len(timesteps), embedding_dim)`的嵌入向量
- en: See [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)
- en: StableDiffusionXLAdapterPipeline
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StableDiffusionXLAdapterPipeline
- en: '### `class diffusers.StableDiffusionXLAdapterPipeline`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.StableDiffusionXLAdapterPipeline`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L182)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L182)'
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`adapter` (`T2IAdapter` or `MultiAdapter` or `List[T2IAdapter]`) — Provides
    additional conditioning to the unet during the denoising process. If you set multiple
    Adapter as a list, the outputs from each Adapter are added together to create
    one combined additional conditioning.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter`（`T2IAdapter`或`MultiAdapter`或`List[T2IAdapter]`）—在去噪过程中为unet提供额外的调节。如果将多个Adapter设置为列表，则每个Adapter的输出将相加以创建一个组合的额外调节。'
- en: '`adapter_weights` (`List[float]`, *optional*, defaults to None) — List of floats
    representing the weight which will be multiply to each adapter’s output before
    adding them together.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adapter_weights`（`List[float]`，*可选*，默认为None）—表示将乘以每个适配器输出的权重的浮点数列表。'
- en: '`vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL))
    — Variational Auto-Encoder (VAE) Model to encode and decode images to and from
    latent representations.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）—变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。'
- en: '`text_encoder` (`CLIPTextModel`) — Frozen text-encoder. Stable Diffusion uses
    the text portion of [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel),
    specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)
    variant.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`CLIPTokenizer`) — Tokenizer of class [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel))
    — Conditional U-Net architecture to denoise the encoded image latents.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin))
    — A scheduler to be used in combination with `unet` to denoise the encoded image
    latents. Can be one of [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    [LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler),
    or [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`safety_checker` (`StableDiffusionSafetyChecker`) — Classification module that
    estimates whether generated images could be considered offensive or harmful. Please,
    refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    for details.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_extractor` (`CLIPFeatureExtractor`) — Model that extracts features
    from generated images to be used as inputs for the `safety_checker`.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline for text-to-image generation using Stable Diffusion augmented with
    T2I-Adapter [https://arxiv.org/abs/2302.08453](https://arxiv.org/abs/2302.08453)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
    Check the superclass documentation for the generic methods the library implements
    for all the pipelines (such as downloading or saving, running on a particular
    device, etc.)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline also inherits the following loading methods:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)
    for loading textual inversion embeddings'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    for loading `.ckpt` files'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    for loading LoRA weights'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_lora_weights()` for saving LoRA weights'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter)
    for loading IP Adapters'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `__call__`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L846)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts to guide
    the image generation. If not defined, one has to pass `prompt_embeds`. instead.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts to be
    sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor`, `PIL.Image.Image`, `List[torch.FloatTensor]`
    or `List[PIL.Image.Image]` or `List[List[PIL.Image.Image]]`) — The Adapter input
    condition. Adapter uses this input condition to generate guidance to Unet. If
    the type is specified as `Torch.FloatTensor`, it is passed to Adapter as is. PIL.Image.Image`
    can also be accepted as an image. The control image is automatically resized to
    fit the output image.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The height in pixels of the generated image. Anything below 512 pixels won’t
    work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`width` (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor)
    — The width in pixels of the generated image. Anything below 512 pixels won’t
    work well for [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
    and checkpoints that are not specifically fine-tuned on low resolutions.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_inference_steps` (`int`, *optional*, defaults to 50) — The number of denoising
    steps. More denoising steps usually lead to a higher quality image at the expense
    of slower inference.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timesteps` (`List[int]`, *optional*) — Custom timesteps to use for the denoising
    process with schedulers which support a `timesteps` argument in their `set_timesteps`
    method. If not defined, the default behavior when `num_inference_steps` is passed
    will be used. Must be in descending order.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`denoising_end` (`float`, *optional*) — When specified, determines the fraction
    (between 0.0 and 1.0) of the total denoising process to be completed before it
    is intentionally prematurely terminated. As a result, the returned sample will
    still retain a substantial amount of noise as determined by the discrete timesteps
    selected by the scheduler. The denoising_end parameter should ideally be utilized
    when this pipeline forms a part of a “Mixture of Denoisers” multi-pipeline setup,
    as elaborated in [`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`, *optional*, defaults to 5.0) — Guidance scale as
    defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
    `guidance_scale` is defined as `w` of equation 2\. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).
    Guidance scale is enabled by setting `guidance_scale > 1`. Higher guidance scale
    encourages to generate images that are closely linked to the text `prompt`, usually
    at the expense of lower image quality.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`, *optional*, defaults to 1) — The number of
    images to generate per prompt.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta` (`float`, *optional*, defaults to 0.0) — Corresponds to parameter eta
    (η) in the DDIM paper: [https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502).
    Only applies to [schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler),
    will be ignored for others.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — One
    or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
    to make generation deterministic.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latents` (`torch.FloatTensor`, *optional*) — Pre-generated noisy latents,
    sampled from a Gaussian distribution, to be used as inputs for image generation.
    Can be used to tweak the same generation with different prompts. If not provided,
    a latents tensor will ge generated by sampling using the supplied random `generator`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。'
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负文本嵌入。'
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, pooled text embeddings will be generated from `prompt` input
    argument.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的汇总文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成汇总文本嵌入。'
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument. ip_adapter_image — (`PipelineImageInput`,
    *optional*): Optional image input to work with IP Adapters.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负汇总文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成负汇总文本嵌入。ip_adapter_image
    — (`PipelineImageInput`, *optional*): 可选的图像输入，用于与IP适配器一起使用。'
- en: '`output_type` (`str`, *optional*, defaults to `"pil"`) — The output format
    of the generate image. Choose between [PIL](https://pillow.readthedocs.io/en/stable/):
    `PIL.Image.Image` or `np.array`.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。可选择[PIL](https://pillow.readthedocs.io/en/stable/)：`PIL.Image.Image`或`np.array`。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~pipelines.stable_diffusion_xl.StableDiffusionAdapterPipelineOutput`
    instead of a plain tuple.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回`~pipelines.stable_diffusion_xl.StableDiffusionAdapterPipelineOutput`而不是普通元组。'
- en: '`callback` (`Callable`, *optional*) — A function that will be called every
    `callback_steps` steps during inference. The function will be called with the
    following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback` (`Callable`, *optional*) — 在推断过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step:
    int, timestep: int, latents: torch.FloatTensor)`。'
- en: '`callback_steps` (`int`, *optional*, defaults to 1) — The frequency at which
    the `callback` function will be called. If not specified, the callback will be
    called at every step.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callback_steps` (`int`, *optional*, 默认为1) — `callback`函数将被调用的频率。如果未指定，将在每一步调用回调。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined under `self.processor`
    in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *optional*) — 如果指定，将传递给`AttentionProcessor`的kwargs字典，如[diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中的`self.processor`中定义的那样。'
- en: '`guidance_rescale` (`float`, *optional*, defaults to 0.0) — Guidance rescale
    factor proposed by [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)
    `guidance_scale` is defined as `φ` in equation 16\. of [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).
    Guidance rescale factor should fix overexposure when using zero terminal SNR.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_rescale` (`float`, *optional*, 默认为0.0) — [Common Diffusion Noise
    Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)提出的指导重缩放因子。`guidance_scale`在[Common
    Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf)的方程式16中定义。指导重缩放因子应在使用零终端SNR时修复过曝光问题。'
- en: '`original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — If `original_size`
    is not the same as `target_size` the image will appear to be down- or upsampled.
    `original_size` defaults to `(height, width)` if not specified. Part of SDXL’s
    micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_size` (`Tuple[int]`, *optional*, 默认为(1024, 1024)) — 如果`original_size`与`target_size`不同，图像将呈现为缩小或放大。如果未指定，`original_size`默认为`(height,
    width)`。作为SDXL微调节的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节中所述。'
- en: '`crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0, 0)) — `crops_coords_top_left`
    can be used to generate an image that appears to be “cropped” from the position
    `crops_coords_top_left` downwards. Favorable, well-centered images are usually
    achieved by setting `crops_coords_top_left` to (0, 0). Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_coords_top_left` (`Tuple[int]`, *optional*, 默认为(0, 0)) — `crops_coords_top_left`可用于生成一个看起来被从位置`crops_coords_top_left`向下“裁剪”的图像。通常通过将`crops_coords_top_left`设置为(0,
    0)来实现有利的、居中的图像。作为SDXL微调节的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节中所述。'
- en: '`target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024)) — For most
    cases, `target_size` should be set to the desired height and width of the generated
    image. If not specified it will default to `(height, width)`. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_size` (`Tuple[int]`, *optional*, 默认为(1024, 1024)) — 对于大多数情况，`target_size`应设置为生成图像的期望高度和宽度。如果未指定，将默认为`(height,
    width)`。作为SDXL微调节的一部分，如[https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952)第2.2节中所述。'
- en: '`negative_original_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    — To negatively condition the generation process based on a specific image resolution.
    Part of SDXL’s micro-conditioning as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_crops_coords_top_left` (`Tuple[int]`, *optional*, defaults to (0,
    0)) — To negatively condition the generation process based on a specific crop
    coordinates. Part of SDXL’s micro-conditioning as explained in section 2.2 of
    [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_target_size` (`Tuple[int]`, *optional*, defaults to (1024, 1024))
    — To negatively condition the generation process based on a target image resolution.
    It should be as same as the `target_size` for most cases. Part of SDXL’s micro-conditioning
    as explained in section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).
    For more information, refer to this issue thread: [https://github.com/huggingface/diffusers/issues/4208](https://github.com/huggingface/diffusers/issues/4208).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_conditioning_scale` (`float` or `List[float]`, *optional*, defaults
    to 1.0) — The outputs of the adapter are multiplied by `adapter_conditioning_scale`
    before they are added to the residual in the original unet. If multiple adapters
    are specified in init, you can set the corresponding scale as a list.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adapter_conditioning_factor` (`float`, *optional*, defaults to 1.0) — The
    fraction of timesteps for which adapter should be applied. If `adapter_conditioning_factor`
    is `0.0`, adapter is not applied at all. If `adapter_conditioning_factor` is `1.0`,
    adapter is applied for all timesteps. If `adapter_conditioning_factor` is `0.5`,
    adapter is applied for half of the timesteps.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '`~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` or `tuple`'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '`~pipelines.stable_diffusion.StableDiffusionAdapterPipelineOutput` if `return_dict`
    is True, otherwise a `tuple`. When returning a tuple, the first element is a list
    with the generated images.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Function invoked when calling the pipeline for generation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#### `enable_attention_slicing`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`str` or `int`, *optional*, defaults to `"auto"`) — When `"auto"`,
    halves the input to the attention heads, so attention will be computed in two
    steps. If `"max"`, maximum amount of memory will be saved by running only one
    slice at a time. If a number is provided, uses as many slices as `attention_head_dim
    // slice_size`. In this case, `attention_head_dim` must be a multiple of `slice_size`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable sliced attention computation. When this option is enabled, the attention
    module splits the input tensor in slices to compute attention in several steps.
    For more than one attention head, the computation is performed sequentially over
    each head. This is useful to save some memory in exchange for a small speed decrease.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ Don’t enable attention slicing if you’re already using `scaled_dot_product_attention`
    (SDPA) from PyTorch 2.0 or xFormers. These attention computations are already
    very memory efficient so you won’t need to enable this function. If you enable
    attention slicing with SDPA or xFormers, it can lead to serious slow downs!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#### `disable_attention_slicing`'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Disable sliced attention computation. If `enable_attention_slicing` was previously
    called, attention is computed in one step.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_slicing`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L274)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_slicing`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L282)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled,
    this method will go back to computing decoding in one step.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_xformers_memory_efficient_attention`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*) — Override the default `None` operator
    for use as `op` argument to the [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)
    function of xFormers.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
    When this option is enabled, you should observe lower GPU memory usage and a potential
    speed up during inference. Speed up during training is not guaranteed.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ When memory efficient attention and sliced attention are both enabled, memory
    efficient attention takes precedent.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '#### `disable_xformers_memory_efficient_attention`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_freeu`'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L802)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Disables the FreeU mechanism if enabled.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_vae_tiling`'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L299)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_freeu`'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L779)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate “oversmoothing effect” in the enhanced
    denoising process.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables the FreeU mechanism as in [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The suffixes after the scaling factors represent the stages where they are being
    applied.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of the values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_vae_tiling`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L290)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '#### `encode_prompt`'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L307)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt` (`str` or `List[str]`, *optional*) — prompt to be encoded'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts to be
    sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is used
    in both text-encoders device — (`torch.device`): torch device'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_images_per_prompt` (`int`) — number of images that should be generated
    per prompt'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_classifier_free_guidance` (`bool`) — whether to use classifier free guidance
    or not'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation. If not defined, one has to pass `negative_prompt_embeds`
    instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is
    less than `1`).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_2` (`str` or `List[str]`, *optional*) — The prompt or prompts
    not to guide the image generation to be sent to `tokenizer_2` and `text_encoder_2`.
    If not defined, `negative_prompt` is used in both text-encoders'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated text embeddings.
    Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not provided,
    text embeddings will be generated from `prompt` input argument.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
    weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt`
    input argument.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated pooled
    text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.
    If not provided, pooled text embeddings will be generated from `prompt` input
    argument.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — Pre-generated
    negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.*
    prompt weighting. If not provided, pooled negative_prompt_embeds will be generated
    from `negative_prompt` input argument.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_scale` (`float`, *optional*) — A lora scale that will be applied to all
    LoRA layers of the text encoder if LoRA layers are loaded.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clip_skip` (`int`, *optional*) — Number of layers to be skipped from CLIP
    while computing the prompt embeddings. A value of 1 means that the output of the
    pre-final layer will be used for computing the prompt embeddings.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encodes the prompt into text encoder hidden states.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_guidance_scale_embedding`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L807)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '`timesteps` (`torch.Tensor`) — generate embedding vectors at these timesteps'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_dim` (`int`, *optional*, defaults to 512) — dimension of the embeddings
    to generate dtype — data type of the generated embeddings'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor`'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Embedding vectors with shape `(len(timesteps), embedding_dim)`
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: See [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
