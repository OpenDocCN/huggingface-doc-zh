- en: Hands-on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/hands-on](https://huggingface.co/learn/deep-rl-course/unit5/hands-on)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Ask
    a Question](../Images/255e59f8542cbd6d3f1c72646b2fff13.png)](http://hf.co/join/discord)
    [![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: We learned what ML-Agents is and how it works. We also studied the two environments
    we’re going to use. Now we’re ready to train our agents!
  prefs: []
  type: TYPE_NORMAL
- en: '![Environments](../Images/a536e3342e9e887aef01941b6e7dc2d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To validate this hands-on for the certification process, you **just need to
    push your trained models to the Hub.** There are **no minimum results to attain**
    in order to validate this Hands On. But if you want to get nice results, you can
    try to reach the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For [Pyramids](https://huggingface.co/spaces/unity/ML-Agents-Pyramids): Mean
    Reward = 1.75'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For [SnowballTarget](https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget):
    Mean Reward = 15 or 30 targets shoot in an episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: '**To start the hands-on, click on Open In Colab button** 👇 :'
  prefs: []
  type: TYPE_NORMAL
- en: '[![Open In Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit5/unit5.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: We strongly **recommend students use Google Colab for the hands-on exercises**
    instead of running them on their personal computers.
  prefs: []
  type: TYPE_NORMAL
- en: By using Google Colab, **you can focus on learning and experimenting without
    worrying about the technical aspects** of setting up your environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unit 5: An Introduction to ML-Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Thumbnail](../Images/de76e57f9c05ee846deb2b4e602f2f0b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this notebook, you’ll learn about ML-Agents and train two agents.
  prefs: []
  type: TYPE_NORMAL
- en: The first one will learn to **shoot snowballs onto spawning targets**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second needs to press a button to spawn a pyramid, then navigate to the
    pyramid, knock it over, **and move to the gold brick at the top**. To do that,
    it will need to explore its environment, and we will use a technique called curiosity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, you’ll be able **to watch your agents playing directly on your browser**.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)
  prefs: []
  type: TYPE_NORMAL
- en: ⬇️ Here is an example of what **you will achieve at the end of this unit.**
    ⬇️
  prefs: []
  type: TYPE_NORMAL
- en: '![Pyramids](../Images/8aed616d792e17cf89f7b4d3fac5559c.png) ![SnowballTarget](../Images/fb625fa1ee3e280912baaaa56548960f.png)'
  prefs: []
  type: TYPE_IMG
- en: '🎮 Environments:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Pyramids](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SnowballTarget
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '📚 RL-Library:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[ML-Agents](https://github.com/Unity-Technologies/ml-agents)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re constantly trying to improve our tutorials, so **if you find some issues
    in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).
  prefs: []
  type: TYPE_NORMAL
- en: Objectives of this notebook 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the end of the notebook, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how **ML-Agents** works and the environment library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be able to **train agents in Unity Environments**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites 🏗️
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving into the notebook, you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: 🔲 📚 **Study [what ML-Agents is and how it works by reading Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction)**
    🤗
  prefs: []
  type: TYPE_NORMAL
- en: Let’s train our agents 🚀
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Set the GPU 💪
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To **accelerate the agent’s training, we’ll use a GPU**. To do that, go to `Runtime
    > Change Runtime type`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 1](../Images/5378127c314cdd92729aa31b7e11ca44.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Hardware Accelerator > GPU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![GPU Step 2](../Images/e0fec252447f98378386ccca8e57a80a.png)'
  prefs: []
  type: TYPE_IMG
- en: Clone the repository and install the dependencies 🔽
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to clone the repository that **contains the experimental version of
    the library that allows you to push your trained agent to the Hub.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: SnowballTarget ⛄
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you need a refresher on how this environment works check this section 👉 [https://huggingface.co/deep-rl-course/unit5/snowball-target](https://huggingface.co/deep-rl-course/unit5/snowball-target)
  prefs: []
  type: TYPE_NORMAL
- en: Download and move the environment zip file in ./training-envs-executables/linux/
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our environment executable is in a zip file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to download it and place it to `./training-envs-executables/linux/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a linux executable because we use colab, and colab machines OS is Ubuntu
    (linux)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We downloaded the file SnowballTarget.zip from [https://github.com/huggingface/Snowball-Target](https://github.com/huggingface/Snowball-Target)
    using `wget`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We unzip the executable.zip file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Make sure your file is accessible
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Define the SnowballTarget config file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In ML-Agents, you define the **training hyperparameters in config.yaml files.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple hyperparameters. To understand them better, you should read
    the explanation for each one in [the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)
  prefs: []
  type: TYPE_NORMAL
- en: You need to create a `SnowballTarget.yaml` config file in ./content/ml-agents/config/ppo/
  prefs: []
  type: TYPE_NORMAL
- en: We’ll give you a preliminary version of this config (to copy and paste into
    your `SnowballTarget.yaml file`), **but you should modify it**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Config SnowballTarget](../Images/be3b76b22dafd564ace7c1b867395093.png) ![Config
    SnowballTarget](../Images/3188bf2d59c42f6f7bc7435114c69c75.png)'
  prefs: []
  type: TYPE_IMG
- en: As an experiment, try to modify some other hyperparameters. Unity provides very
    [good documentation explaining each of them here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve created the config file and understand what most hyperparameters
    do, we’re ready to train our agent 🔥.
  prefs: []
  type: TYPE_NORMAL
- en: Train the agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To train our agent, we need to **launch mlagents-learn and select the executable
    containing the environment.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We define four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mlagents-learn <config>`: the path where the hyperparameter config file is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--env`: where the environment executable is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--run_id`: the name you want to give to your training run id.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--no-graphics`: to not launch the visualization during the training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![MlAgents learn](../Images/031eb13548c8d61ed2dbe5b169712883.png)'
  prefs: []
  type: TYPE_IMG
- en: Train the model and use the `--resume` flag to continue training in case of
    interruption.
  prefs: []
  type: TYPE_NORMAL
- en: It will fail the first time if and when you use `--resume`. Try rerunning the
    block to bypass the error.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The training will take 10 to 35min depending on your config. Go take a ☕️ you
    deserve it 🤗.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Push the agent to the Hugging Face Hub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve trained our agent, we’re **ready to push it to the Hub and visualize
    it playing on your browser🔥.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be able to share your model with the community, there are three more steps
    to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ (If it’s not already done) create an account to HF ➡ [https://huggingface.co/join](https://huggingface.co/join)
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Sign in and store your authentication token from the Hugging Face website.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new token ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))
    **with write role**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Create HF Token](../Images/d21a97c736edaab9119d2d1c1da9deac.png)'
  prefs: []
  type: TYPE_IMG
- en: Copy the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the cell below and paste the token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to use Google Colab or a Jupyter Notebook, you need to use
    this command instead: `huggingface-cli login`'
  prefs: []
  type: TYPE_NORMAL
- en: Then we need to run `mlagents-push-to-hf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'And we define four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--run-id`: the name of the training run id.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--local-dir`: where the agent was saved, it’s results/<run_id name>, so in
    my case results/First Training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--repo-id`: the name of the Hugging Face repo you want to create or update.
    It’s always <your huggingface username>/<the repo name> If the repo does not exist
    **it will be created automatically**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--commit-message`: since HF repos are git repositories you need to give a
    commit message.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Push to Hub](../Images/e5f3533da18ced3904ea589dc1d88cec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mlagents-push-to-hf --run-id="SnowballTarget1" --local-dir="./results/SnowballTarget1"
    --repo-id="ThomasSimonini/ppo-SnowballTarget" --commit-message="First Push"`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything worked you should see this at the end of the process (but with
    a different url 😆) :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It’s the link to your model. It contains a model card that explains how to use
    it, your Tensorboard, and your config file. **What’s awesome is that it’s a git
    repository, which means you can have different commits, update your repository
    with a new push, etc.**
  prefs: []
  type: TYPE_NORMAL
- en: 'But now comes the best: **being able to visualize your agent online 👀.**'
  prefs: []
  type: TYPE_NORMAL
- en: Watch your agent playing 👀
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This step it’s simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Remember your repo-id
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go here: [https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget](https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the game and put it in full screen by clicking on the bottom right button
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Snowballtarget load](../Images/885269944cb24970c22e13dd919445f3.png)'
  prefs: []
  type: TYPE_IMG
- en: In step 1, choose your model repository, which is the model id (in my case ThomasSimonini/ppo-SnowballTarget).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In step 2, **choose what model you want to replay**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have multiple ones since we saved a model every 500000 timesteps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But if I want the more recent I choose `SnowballTarget.onnx`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 👉 It’s nice to **try different model stages to see the improvement of the agent.**
  prefs: []
  type: TYPE_NORMAL
- en: 'And don’t hesitate to share the best score your agent gets on discord in the
    #rl-i-made-this channel 🔥'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try a more challenging environment called Pyramids.
  prefs: []
  type: TYPE_NORMAL
- en: Pyramids 🏆
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download and move the environment zip file in ./training-envs-executables/linux/
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our environment executable is in a zip file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to download it and place it into `./training-envs-executables/linux/`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a linux executable because we’re using colab, and the colab machine’s
    OS is Ubuntu (linux)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the file Pyramids.zip from [https://drive.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H](https://drive.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H)
    using `wget`. Check out the full solution to download large files from GDrive
    [here](https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Unzip it
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Make sure your file is accessible
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Modify the PyramidsRND config file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contrary to the first environment, which was a custom one, **Pyramids was made
    by the Unity team**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So the PyramidsRND config file already exists and is in ./content/ml-agents/config/ppo/PyramidsRND.yaml
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might ask why “RND” is in PyramidsRND. RND stands for *random network distillation*
    it’s a way to generate curiosity rewards. If you want to know more about that,
    we wrote an article explaining this technique: [https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938](https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For this training, we’ll modify one thing:'
  prefs: []
  type: TYPE_NORMAL
- en: The total training steps hyperparameter is too high since we can hit the benchmark
    (mean reward = 1.75) in only 1M training steps. 👉 To do that, we go to config/ppo/PyramidsRND.yaml,**and
    change max_steps to 1000000.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Pyramids config](../Images/3a69b29716f0a6185c5dec0998d811f3.png)'
  prefs: []
  type: TYPE_IMG
- en: As an experiment, you should also try to modify some other hyperparameters.
    Unity provides very [good documentation explaining each of them here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready to train our agent 🔥.
  prefs: []
  type: TYPE_NORMAL
- en: Train the agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training will take 30 to 45min depending on your machine, go take a ☕️ you
    deserve it 🤗.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Push the agent to the Hugging Face Hub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we trained our agent, we’re **ready to push it to the Hub to be able
    to visualize it playing on your browser🔥.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Watch your agent playing 👀
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 👉 [https://huggingface.co/spaces/unity/ML-Agents-Pyramids](https://huggingface.co/spaces/unity/ML-Agents-Pyramids)
  prefs: []
  type: TYPE_NORMAL
- en: '🎁 Bonus: Why not train on another environment?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you know how to train an agent using MLAgents, **why not try another
    environment?**
  prefs: []
  type: TYPE_NORMAL
- en: MLAgents provides 17 different environments and we’re building some custom ones.
    The best way to learn is to try things on your own, have fun.
  prefs: []
  type: TYPE_NORMAL
- en: '![cover](../Images/9cfcb902144d3f31941072483268cb93.png)'
  prefs: []
  type: TYPE_IMG
- en: You have the full list of the one currently available environments on Hugging
    Face here 👉 [https://github.com/huggingface/ml-agents#the-environments](https://github.com/huggingface/ml-agents#the-environments)
  prefs: []
  type: TYPE_NORMAL
- en: For the demos to visualize your agent 👉 [https://huggingface.co/unity](https://huggingface.co/unity)
  prefs: []
  type: TYPE_NORMAL
- en: 'For now we have integrated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) demo where you teach
    a **worm to crawl**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) demo where you
    teach an agent **to walk towards a goal**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s all for today. Congrats on finishing this tutorial!
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn is to practice and try stuff. Why not try another environment?
    ML-Agents has 18 different environments, but you can also create your own. Check
    the documentation and have fun!
  prefs: []
  type: TYPE_NORMAL
- en: See you on Unit 6 🔥,
  prefs: []
  type: TYPE_NORMAL
- en: Keep Learning, Stay awesome 🤗
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
