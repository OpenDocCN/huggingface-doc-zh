# Falcon

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/falcon](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/falcon)

## 概述

Falcon 是由 [TII](https://www.tii.ae/) 构建的一类仅因果解码器模型。最大的 Falcon 检查点已经在 >=1T 个文本标记上进行了训练，特别强调了 [RefinedWeb](https://arxiv.org/abs/2306.01116) 语料库。它们在 Apache 2.0 许可下提供。

Falcon 的架构现代化且优化用于推断，具有多查询注意力和支持 `FlashAttention` 等高效注意力变体。既有仅作为因果语言模型训练的“基础”模型，也有接受进一步微调的“指导”模型可用。

Falcon 模型（截至 2023 年）是一些最大且最强大的开源语言模型，在 [OpenLLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) 中始终排名靠前。

## 转换自定义检查点

Falcon 模型最初作为自定义代码检查点添加到 Hugging Face Hub。但是，现在 Falcon 在 Transformers 库中得到了全面支持。如果您从自定义代码检查点微调了模型，我们建议将您的检查点转换为新的库格式，这应该显著提高稳定性和性能，特别是对于生成，同时消除了使用 `trust_remote_code=True` 的需要！

您可以使用位于 Transformers 库的 [Falcon 模型目录](https://github.com/huggingface/transformers/tree/main/src/transformers/models/falcon) 中的 `convert_custom_code_checkpoint.py` 脚本将自定义代码检查点转换为完整的 Transformers 检查点。要使用此脚本，只需调用 `python convert_custom_code_checkpoint.py --checkpoint_dir my_model`。这将原地转换您的检查点，然后您可以立即从目录中加载它，例如 `from_pretrained()`。如果您的模型尚未上传到 Hub，我们建议在尝试转换之前进行备份，以防万一！

## FalconConfig

### `class transformers.FalconConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/configuration_falcon.py#L28)

```py
( vocab_size = 65024 hidden_size = 4544 num_hidden_layers = 32 num_attention_heads = 71 layer_norm_epsilon = 1e-05 initializer_range = 0.02 use_cache = True hidden_dropout = 0.0 attention_dropout = 0.0 num_kv_heads = None alibi = False new_decoder_architecture = False multi_query = True parallel_attn = True bias = False max_position_embeddings = 2048 rope_theta = 10000.0 rope_scaling = None bos_token_id = 11 eos_token_id = 11 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 65024) — Falcon 模型的词汇量大小。定义了在调用 [FalconModel](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconModel) 时可以表示的不同 token 数量。

+   `hidden_size` (`int`, *optional*, defaults to 4544) — 隐藏表示的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 32) — Transformer 解码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 71) — Transformer 编码器中每个注意力层的注意力头数量。

+   `layer_norm_epsilon` (`float`, *optional*, defaults to 1e-05) — 层归一化层使用的 epsilon。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态分布初始化器的标准差。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在 `config.is_decoder=True` 时相关。

+   `hidden_dropout` (`float`, *optional*, defaults to 0.0) — MLP 层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力层的 dropout 概率。

+   `num_kv_heads` (`int`, *optional*) — 每个注意力层使用的键值头的数量。如果未设置，默认为与 `num_attention_heads` 相同的值。

+   `alibi` (`bool`, *optional*, defaults to `False`) — 是否在自注意力期间使用 ALiBi 位置偏差。

+   `new_decoder_architecture` (`bool`, *optional*, defaults to `False`) — 是否使用新的（Falcon-40B）解码器架构。如果为`True`，则`multi_query`和`parallel_attn`参数将被忽略，因为新的解码器总是使用并行注意力。

+   `multi_query` (`bool`, *optional*, defaults to `True`) — 是否在解码器中使用多查询注意力。当`new_decoder_architecture`为`True`时忽略。

+   `parallel_attn` (`bool`, *optional*, defaults to `True`) — 是否在前馈层中并行计算注意力。如果为False，则它们是连续的，就像原始Transformer架构中一样。当`new_decoder_architecture`为`True`时忽略。

+   `bias` (`bool`, *optional*, defaults to `False`) — 是否在线性层上使用偏置。

+   `max_position_embeddings` (`int`, *optional*, defaults to 2048) — 当`alibi`为`False`时，此模型可能使用的最大序列长度。支持RoPE的预训练Falcon模型最多支持2048个标记。

+   `rope_theta` (`float`, *optional*, defaults to 10000.0) — RoPE嵌入的基本周期。

+   `rope_scaling` (`Dict`, *optional*) — 包含RoPE嵌入的缩放配置的字典。目前支持两种缩放策略：线性和动态。它们的缩放因子必须是大于1的浮点数。期望的格式是`{"type": 策略名称, "factor": 缩放因子}`。当使用此标志时，不要更新`max_position_embeddings`到预期的新最大值。查看以下主题以了解这些缩放策略的行为更多信息：[https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/](https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/)。这是一个实验性功能，可能在未来版本中发生破坏性API更改。

+   `bos_token_id` (`int`, *optional*, defaults to 11) — “序列开始”标记的id。

+   `eos_token_id` (`int`, *optional*, defaults to 11) — “序列结束”标记的id。

这是一个配置类，用于存储[FalconModel](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconModel)的配置。根据指定的参数实例化Falcon模型，定义模型架构。使用默认值实例化配置将产生类似于[tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import FalconModel, FalconConfig

>>> # Initializing a small (2-layer) Falcon configuration
>>> configuration = FalconConfig(num_hidden_layers=2)

>>> # Initializing a model from the small configuration
>>> model = FalconModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FalconModel

### `class transformers.FalconModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L988)

```py
( config: FalconConfig )
```

参数

+   `config` ([FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的Falcon模型变压器输出原始隐藏状态，没有特定的头部。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1022)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, input_ids_length)`) — `input_ids_length` = `sequence_length`，如果`past_key_values`是`None`，否则`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了`past_key_values`，则应该只传递那些没有计算过去的`input_ids`作为`input_ids`。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.num_hidden_layers`) — 包含由模型计算的预计算隐藏状态（注意力块中的键和值）（请参见下面的`past_key_values`输出）。可用于加速顺序解码。已经计算的`input_ids`的过去不应该作为`input_ids`传递，因为它们已经被计算。

    `past_key_values`的每个元素都是一个元组(past_key, past_value)：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示标记是`未掩盖的`,

    +   0表示被`掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的特定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部是`未掩盖的`,

    +   0表示头部是`掩盖的`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

    如果使用了`past_key_values`，则可以选择仅输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或`torch.FloatTensor`元组。

一个[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）和输入不同的元素。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

    如果使用`past_key_values`，则仅输出形状为`(batch_size, 1, hidden_size)`的序列的最后隐藏状态。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回) — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量，如果`config.is_encoder_decoder=True`还有2个额外的形状为`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`的张量。

    包含预先计算的隐藏状态（自注意力块中的键和值，以及如果`config.is_encoder_decoder=True`则在交叉注意力块中）可用于加速顺序解码。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个和每层输出的一个）。

    模型每一层的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中用于计算加权平均值的注意力softmax之后的注意力权重。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`和`config.add_cross_attention=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在解码器的交叉注意力层中的注意力softmax之后的注意力权重，用于计算交叉注意力头中的加权平均值。

[FalconModel](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FalconModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
>>> model = FalconModel.from_pretrained("Rocketknight1/falcon-rw-1b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FalconForCausalLM

### `class transformers.FalconForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1205)

```py
( config: FalconConfig )
```

参数

+   `config`（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Falcon模型变压器在顶部带有语言建模头（线性层，其权重与输入嵌入绑定）。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

该模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1262)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— `input_ids_length` = `sequence_length`，如果`past_key_values`为`None`，否则为`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.num_hidden_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意力块中的键和值，参见下面的`past_key_values`输出）。可用于加速顺序解码。将其过去给定给该模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充令牌索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1表示未被`masked`的标记，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

    如果使用了`past_key_values`，则可能只需输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于语言建模的标签。请注意，模型内部的标签**已经移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都将被忽略（掩码），损失仅计算标签在`[0, ..., config.vocab_size]`中的标签。

返回

[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或`torch.FloatTensor`元组

一个[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包括根据配置（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`, *optional*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记预测）。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每个层的输出的一个）。

    每个层输出的模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的交叉注意力权重，用于计算交叉注意力头中的加权平均值。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, 当传递`use_cache=True`或`config.use_cache=True`时返回）— 长度为`config.n_layers`的`torch.FloatTensor`元组，每个元组包含自注意力和交叉注意力层的缓存键、值状态，如果模型用于编码器-解码器设置，则相关。仅在`config.is_decoder = True`时相关。

    包含可以用于加速顺序解码的预计算隐藏状态（注意力块中的键和值）。 

[FalconForCausalLM](/docs/transformers/v4.37.2/zh/model_doc/falcon#transformers.FalconForCausalLM)的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, FalconForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
>>> model = FalconForCausalLM.from_pretrained("Rocketknight1/falcon-rw-1b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## FalconForSequenceClassification

### `class transformers.FalconForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1356)

```py
( config: FalconConfig )
```

参数

+   `config`（[FalconConfig](/docs/transformers/v4.37.2/zh/model_doc/falcon#transformers.FalconConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Falcon模型变压器顶部带有序列分类头（线性层）。

[FalconForSequenceClassification](/docs/transformers/v4.37.2/zh/model_doc/falcon#transformers.FalconForSequenceClassification) 使用最后一个标记来进行分类，就像其他因果模型（例如GPT-1）一样。

由于它对最后一个标记进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则它会找到每行中不是填充标记的最后一个标记。如果没有定义`pad_token_id`，则它会简单地取每行批次中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此它会执行相同的操作（取每行批次中的最后一个值）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1381)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutputWithPast or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）— 如果`past_key_values`为`None`，则`input_ids_length` = `sequence_length`，否则为`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了`past_key_values`，则只应将尚未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/zh/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/zh/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.num_hidden_layers`的`Tuple[Tuple[torch.Tensor]]`）— 包含由模型计算的预计算隐藏状态（注意块中的键和值），如下面的`past_key_values`输出所示。可用于加速顺序解码。已经计算过其过去的`input_ids`不应作为`input_ids`传递给此模型，因为它们已经被计算过。

    `past_key_values`的每个元素都是一个元组（past_key, past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`范围内：

    +   1 表示标记是`未被掩盖`。

    +   0 表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.n_positions - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`范围内：

    +   1 表示头部是`未被掩盖`，

    +   0 表示头部是`被掩盖`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

    如果使用了`past_key_values`，则只需输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache` (`bool`，*可选*) — 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_outputs.SequenceClassifierOutputWithPast` 或 `tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.SequenceClassifierOutputWithPast`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）和输入的不同元素。

+   `损失` (`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，*可选*，当传递`use_cache=True`或`config.use_cache=True`时返回） — 长度为`config.n_layers`的`tuple(torch.FloatTensor)`元组，每个元组有2个形状为`(batch_size, num_heads, sequence_length, embed_size_per_head)`的张量）

    包含预先计算的隐藏状态（自注意力块中的键和值），可用于加速顺序解码（参见`past_key_values`输入）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出 + 每个层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[FalconForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

单标签分类的示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, FalconForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
>>> model = FalconForSequenceClassification.from_pretrained("Rocketknight1/falcon-rw-1b")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = FalconForSequenceClassification.from_pretrained("Rocketknight1/falcon-rw-1b", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

多标签分类的示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, FalconForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
>>> model = FalconForSequenceClassification.from_pretrained("Rocketknight1/falcon-rw-1b", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = FalconForSequenceClassification.from_pretrained(
...     "Rocketknight1/falcon-rw-1b", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

## FalconForTokenClassification

### `class transformers.FalconForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1483)

```py
( config: FalconConfig )
```

参数

+   `config`（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Falcon模型在顶部带有一个标记分类头（隐藏状态输出的线性层），例如用于命名实体识别（NER）任务。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1508)

```py
( input_ids: Optional = None past_key_values: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`）- 如果`past_key_values`为`None`，则`input_ids_length`=`sequence_length`，否则`input_ids_length`=`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.num_hidden_layers`的`Tuple[Tuple[torch.Tensor]]`）- 包含由模型计算的预计算隐藏状态（注意力块中的键和值），可以用于加速顺序解码。将其过去给定给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经被计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`掩码`的标记，

    +   0表示被`掩码`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`掩码`的标记，

    +   0表示头部被`掩码`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

    如果使用`past_key_values`，则可能只需输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`中。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或者`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)或者一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或者当`config.return_dict=False`时）包括根据配置（[FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 分类损失。

+   `logits`（形状为`(batch_size, sequence_length, config.num_labels)`的`torch.FloatTensor`）— 分类分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或者当`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的形状+每层的输出形状）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或者当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[FalconForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FalconForTokenClassification
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
>>> model = FalconForTokenClassification.from_pretrained("Rocketknight1/falcon-rw-1b")

>>> inputs = tokenizer(
...     "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
... )

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_token_class_ids = logits.argmax(-1)

>>> # Note that tokens are classified rather then input words which means that
>>> # there might be more predicted token classes than words.
>>> # Multiple token classes might account for the same word
>>> predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]

>>> labels = predicted_token_class_ids
>>> loss = model(**inputs, labels=labels).loss
```

## FalconForQuestionAnswering

### `class transformers.FalconForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1572)

```py
( config )
```

参数

+   `config` ([FalconConfig](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Falcon模型变压器，顶部带有用于提取问答任务的跨度分类头，如SQuAD（在隐藏状态输出顶部的线性层，用于计算`span start logits`和`span end logits`）。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库实现的通用方法（如下载或保存，调整输入嵌入等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/falcon/modeling_falcon.py#L1588)

```py
( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, input_ids_length)`的`torch.LongTensor`） — `input_ids_length` = `sequence_length`（如果`past_key_values`为`None`）否则`past_key_values[0][0].shape[2]`（输入过去键值状态的序列长度）。词汇表中输入序列标记的索引。

    如果使用了`past_key_values`，则只应将未计算其过去的`input_ids`作为`input_ids`传递。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `past_key_values`（长度为`config.num_hidden_layers`的`Tuple[Tuple[torch.Tensor]]`） — 包含由模型计算的预计算隐藏状态（注意力块中的键和值，如下面的`past_key_values`输出所示）。可用于加速顺序解码。将其过去给定给此模型的`input_ids`不应作为`input_ids`传递，因为它们已经计算过。

    `past_key_values`的每个元素都是一个元组（past_key，past_value）：

    +   past_key: [batch_size * num_heads, head_dim, kv_length]

    +   past_value: [batch_size * num_heads, kv_length, head_dim]

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） — 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   对于未被`masked`的标记为1的标记，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.n_positions - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被遮罩，

    +   0表示头部被遮罩。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

    如果使用`past_key_values`，可选地只需输入最后的`inputs_embeds`（参见`past_key_values`）。

+   `use_cache`（`bool`，*可选*）— 如果设置为`True`，则返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `start_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记范围起始位置的位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。

+   `end_positions`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算标记范围结束位置的位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度（`sequence_length`）。序列外的位置不会被考虑在内以计算损失。

[FalconForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/falcon#transformers.FalconForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。
