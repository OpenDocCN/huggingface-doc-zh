- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit4/quiz](https://huggingface.co/learn/deep-rl-course/unit4/quiz)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf)
    **is to test yourself.** This will help you to find **where you need to reinforce
    your knowledge**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: What are the advantages of policy-gradient over value-based methods? (Check
    all that apply)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Q2: What is the Policy Gradient Theorem?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <details data-svelte-h="svelte-nhcd05"><summary>Solution</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '*The Policy Gradient Theorem* is a formula that will help us to reformulate
    the objective function into a differentiable function that does not involve the
    differentiation of the state distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Gradient](../Images/5a9b6c1a3ee9cf5b0e888fb819446af5.png)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: What‚Äôs the difference between policy-based methods and policy-gradient
    methods? (Check all that apply)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Q4: Why do we use gradient ascent instead of gradient descent to optimize J(Œ∏)?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Congrats on finishing this Quiz ü•≥, if you missed some elements, take time to
    read the chapter again to reinforce (üòè) your knowledge.
  prefs: []
  type: TYPE_NORMAL
