- en: GPTSAN-japanese
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto
    (tanreinama).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: GPTSAN is a Japanese language model using Switch Transformer. It has the same
    structure as the model introduced as Prefix LM in the T5 paper, and support both
    Text Generation and Masked Language Modeling tasks. These basic tasks similarly
    can fine-tune for translation or summarization.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `generate()` method can be used to generate text using GPTSAN-Japanese model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: GPTSAN Features
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTSAN has some unique features. It has a model structure of Prefix-LM. It works
    as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs
    behave like normal generative models. The Spout vector is a GPTSAN specific input.
    Spout is pre-trained with random inputs, but you can specify a class of text or
    an arbitrary vector during fine-tuning. This allows you to indicate the tendency
    of the generated text. GPTSAN has a sparse Feed Forward based on Switch-Transformer.
    You can also add other layers and train them partially. See the original GPTSAN
    repository for details.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-LM Model
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The
    original GPTSAN repository calls it `hybrid`) In GPTSAN, the `Prefix` part of
    Prefix-LM, that is, the input position that can be referenced by both tokens,
    can be specified with any length. Arbitrary lengths can also be specified differently
    for each batch. This length applies to the text entered in `prefix_text` for the
    tokenizer. The tokenizer returns the mask of the `Prefix` part of Prefix-LM as
    `token_type_ids`. The model treats the part where `token_type_ids` is 1 as a `Prefix`
    part, that is, the input can refer to both tokens before and after.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Specifying the Prefix part is done with a mask passed to self-attention. When
    token_type_ids=None or all zero, it is equivalent to regular causal mask
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'for example:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'x_token = tokenizer(“ｱｲｳｴ”) input_ids: | SOT | SEG | ｱ | ｲ | ｳ | ｴ | token_type_ids:
    | 1 | 0 | 0 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 0 0 0 0 0 | SEG | 1 1 0 0 0
    0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 1 0 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1 1 |'
  id: totrans-16
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'x_token = tokenizer("", prefix_text=“ｱｲｳｴ”) input_ids: | SOT | ｱ | ｲ | ｳ |
    ｴ | SEG | token_type_ids: | 1 | 1 | 1 | 1 | 1 | 0 | prefix_lm_mask: SOT | 1 1
    1 1 1 0 | ｱ | 1 1 1 1 1 0 | ｲ | 1 1 1 1 1 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1
    0 | SEG | 1 1 1 1 1 1 |'
  id: totrans-17
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'x_token = tokenizer(“ｳｴ”, prefix_text=“ｱｲ”) input_ids: | SOT | ｱ | ｲ | SEG
    | ｳ | ｴ | token_type_ids: | 1 | 1 | 1 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 1
    1 0 0 0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 0 0 0 | SEG | 1 1 1 1 0 0 | ｳ | 1 1 1 1
    1 0 | ｴ | 1 1 1 1 1 1 |'
  id: totrans-18
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Spout Vector
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Spout Vector is a special vector for controlling text generation. This vector
    is treated as the first embedding in self-attention to bring extraneous attention
    to the generated tokens. In the pre-trained model published from `Tanrei/GPTSAN-japanese`,
    the Spout Vector is a 128-dimensional vector that passes through 8 fully connected
    layers in the model and is projected into the space acting as external attention.
    The Spout Vector projected by the fully connected layer is split to be passed
    to all self-attentions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: GPTSanJapaneseConfig
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSanJapaneseConfig`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/configuration_gptsan_japanese.py#L29)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 36000) — Vocabulary size of the
    GPTSANJapanese model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1280) — The maximum
    sequence length that this model might ever be used with. Defaults set this to
    1280.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, defaults to 1280) — 该模型可能使用的最大序列长度。默认设置为1280。'
- en: '`d_model` (`int`, *optional*, defaults to 1024) — Size of the encoder layers
    and the pooler layer.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, defaults to 1024) — 编码器层和池化层的大小。'
- en: '`d_ff` (`int`, *optional*, defaults to 8192) — Size of the intermediate feed
    forward layer in each `SwitchTransformersBlock`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ff` (`int`, *optional*, defaults to 8192) — 每个`SwitchTransformersBlock`中间级前馈层的大小。'
- en: '`d_ext` (`int`, *optional*, defaults to 4096) — Size of the intermediate feed
    forward layer in each Extra-layers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_ext` (`int`, *optional*, defaults to 4096) — 额外层中间前馈层的大小。'
- en: '`d_spout` (`int`, *optional*, defaults to 128) — Size of the `spout` vector.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_spout` (`int`, *optional*, defaults to 128) — `spout`向量的大小。'
- en: '`num_switch_layers` (`int`, *optional*, defaults to 10) — Number of layers
    in the Switch Transformer layer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_switch_layers` (`int`, *optional*, defaults to 10) — Switch Transformer层中的层数。'
- en: '`num_ext_layers` (`int`, *optional*, defaults to 0) — Number of layers in the
    Extra-layers.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_ext_layers` (`int`, *optional*, defaults to 0) — 额外层中的层数。'
- en: '`num_heads` (`int`, *optional*, defaults to 16) — Number of attention heads
    for each attention layer in the Transformer encoder.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_heads` (`int`, *optional*, defaults to 16) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`num_experts` (`int`, *optional*, defaults to 16) — Number of experts for each
    SwitchTransformer layer.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_experts` (`int`, *optional*, defaults to 16) — 每个SwitchTransformer层的专家数量。'
- en: '`expert_capacity` (`int`, *optional*, defaults to 128) — Number of tokens that
    can be stored in each expert. If set to 1, the model will behave like a regular
    Transformer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expert_capacity` (`int`, *optional*, defaults to 128) — 每个专家可以存储的令牌数量。如果设置为1，则模型将表现得像一个常规Transformer。'
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.0) — The ratio for all dropout
    layers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout_rate` (`float`, *optional*, defaults to 0.0) — 所有dropout层的比率。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon used
    by the layer normalization layers.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的epsilon。'
- en: '`router_bias` (`bool`, *optional*, defaults to `False`) — Whether to add a
    bias to the router.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_bias` (`bool`, *optional*, defaults to `False`) — 是否向路由器添加偏置。'
- en: '`router_jitter_noise` (`float`, *optional*, defaults to 0.0) — Amount of noise
    to add to the router. Set it to 0.0 during prediction or set small value (usually
    1e-2) during training.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_jitter_noise` (`float`, *optional*, defaults to 0.0) — 添加到路由器的噪声量。在预测期间将其设置为0.0，或者在训练期间设置一个小值（通常为1e-2）。'
- en: '`router_dtype` (`str`, *optional*, default to `"float32"`) — The `dtype` used
    for the routers. It is preferable to keep the `dtype` to `"float32"` as specified
    in the *selective precision* discussion in [the paper](https://arxiv.org/abs/2101.03961).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_dtype` (`str`, *optional*, default to `"float32"`) — 用于路由器的`dtype`。最好将`dtype`保持为在[论文](https://arxiv.org/abs/2101.03961)中指定的“float32”类型。'
- en: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    Whether to ignore padding tokens when routing.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    在路由时是否忽略填充标记。'
- en: '`output_hidden_states` (`bool`, *optional*, default to `False`) — Whether or
    not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more detail.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*, default to `False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。'
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。'
- en: '`initializer_factor` (`float`, *optional*, defaults to 0.002) — A factor for
    initializing all weight matrices.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_factor` (`float`, *optional*, defaults to 0.002) — 用于初始化所有权重矩阵的因子。'
- en: '`output_router_logits` (`bool`, *optional*, default to `False`) — Whether or
    not to return the router logits of all experts.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_router_logits` (`bool`, *optional*, default to `False`) — 是否返回所有专家的路由器logits。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: This is the configuration class to store the configuration of a [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel).
    It is used to instantiate a GPTSANJapanese model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the GPTSANJapanese [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
    architecture.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个配置类，用于存储[GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel)的配置。根据指定的参数实例化一个GPTSANJapanese模型，定义模型架构。使用默认值实例化配置将产生类似于GPTSANJapanese[Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: GPTSanJapaneseTokenizer
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTSanJapaneseTokenizer
- en: '### `class transformers.GPTSanJapaneseTokenizer`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GPTSanJapaneseTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74)'
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`emoji_file` (`str`) — File containing the emoji.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emoji_file` (`str`) — 包含表情符号的文件。'
- en: '`unk_token` (`str`, *optional*, defaults to `"<|nottoken|>"`) — The token used
    for unknown charactor'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`（`str`，*optional*，默认为`“<|nottoken|>”`）--用于未知字符的令牌'
- en: '`pad_token` (`str`, *optional*, defaults to `"<|separator|>"`) — The token
    used for padding'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`（`str`，*optional*，默认为`“<[UNK]分隔符[UNK]>”`）--用于填充的令牌'
- en: '`bos_token` (`str`, *optional*, defaults to `"<|startoftext|>"`) — The beginning
    of sequence token.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`（`str`，*optional*，默认为`“<|startoftext|>”`）--序列标记的开头。'
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`（`str`，*optional*，默认为`“<|endoftext|>”`）--序列结束标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"<|segmenter|>"`) — A special
    token to separate token to prefix part and general input part.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`（`str`，*optional*，默认为`“<|segmenter|>”`）--一个特殊的令牌，用于分隔前缀部分和一般输入部分的令牌。'
- en: '`do_clean_text` (`bool`, *optional*, defaults to `False`) — Whether or not
    to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_clean_text`（`bool`，*可选*，默认为`False`） - 是否清理URL、EMAIL、TEL、日语日期和日语价格的文本。'
- en: This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此标记器基于GPTNeoXJapaneseTokenizer，并具有以下修改
- en: Decoding byte0~byte255 tokens correctly
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确解码字节0~字节255的标记
- en: Added bagofword token handling
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了bagofword标记处理
- en: Return token_type_ids for Prefix-LM model The bagofword token represents a repetition
    of the previous token and is converted to 3 consecutive tokens when decoding In
    addition, the original Japanese special Sub-Word-Encoding has been released in
    this repository ([https://github.com/tanreinama/Japanese-BPEEncoder_V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2)).
    The token_type_ids is a mask indicating the prefix input position of the Prefix-LM
    model. To specify a prefix position, specify a prefix input for prefix_text, or
    specify a sentence of the prefix part and the part after it as a text pair of
    batch input.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Prefix-LM模型返回token_type_ids。 bagofword标记表示前一个标记的重复，并在解码时转换为3个连续的标记。此外，原始的日语特殊Sub-Word-Encoding已在此存储库中发布（[https://github.com/tanreinama/Japanese-BPEEncoder_V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2)）。
    token_type_ids是一个掩码，指示Prefix-LM模型的前缀输入位置。要指定前缀位置，请为prefix_text指定前缀输入，或将前缀部分和其后部分作为批量输入的文本对指定为前缀部分。
- en: 'Example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Example for Prefix-LM:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀-LM示例：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Example for batch encode:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 批量编码示例：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `convert_tokens_to_string`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219)'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列标记（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308)'
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The tokenizer returns token_type_ids as separators between the Prefix part and
    the rest. token_type_ids is 1 for the Prefix part and 0 for the rest of the token.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器返回token_type_ids作为前缀部分和其余部分之间的分隔符。 token_type_ids对于前缀部分为1，对于其余标记为0。
- en: 'Example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: GPTSanJapaneseModel
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTSanJapaneseModel
- en: '### `class transformers.GPTSanJapaneseModel`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GPTSanJapaneseModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855)'
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([GPTSanJapaneseConfig](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[GPTSanJapaneseConfig](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig)）
    - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The bare GPTSAN-japanese Model transformer outputting raw hidden-states without
    any specific head on top.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 裸GPTSAN-japanese模型变压器输出原始隐藏状态，没有特定的顶部头。
- en: The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed
    in General-purpose Swich transformer based Japanese language model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTSAN-japanese](https://github.com/tanreinama/GPTSAN)模型是基于通用Swich变压器的日语语言模型'
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。
- en: '#### `forward`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893)'
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model
    that generates sentence continuations or predicts tokens at mask positions. Special
    tokens required for inputs to the model are automatically appended.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） - 词汇表中输入序列标记的索引。GPTSAN-japanese是一个生成句子延续或预测掩码位置标记的模型。用于模型输入的特殊标记会自动附加。'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）
    - 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `not masked`,
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未掩码`的标记为1，
- en: 0 for tokens that are `masked`.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`masked`的标记为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — An input that masks the Prefix part in the Prefix-LM input. Mask
    values selected in `[0, 1]`:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）
    - 用于掩盖Prefix-LM输入中的前缀部分的输入。选择的掩码值为`[0, 1]`：'
- en: 1 for tokens that are `prefix` input,
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`prefix`输入的标记，
- en: 0 for tokens that are `not-prefix` input.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`not-prefix`输入的标记为0。
- en: '`spout` (`torch.Tensor` of shape `(batch_size, config.d_spout)`) — This vector
    is transformed through an 8-layer FFN and can be used instead of `past_key_values`.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spout`（形状为`(batch_size, config.d_spout)`的`torch.Tensor`） - 通过8层FFN转换的向量，可以用于替代`past_key_values`。'
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组有4个形状为`(batch_size,
    num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。'
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size,
    1)`的张量，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于将自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：'
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可以用于加速解码（参见`past_key_values`）。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length,
    hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制权，以便将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。'
- en: '`router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True`
    is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, sequence_length, num_experts)`. Router
    logits of the decoder model, useful to compute the auxiliary loss for Mixture
    of Experts models.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_logits` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回）
    — 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。解码器模型的路由器对数，有助于计算混合专家模型的辅助损失。'
- en: '`num_precontext` (`torch.LongTensor` of shape `(batch_size,1)`) — length of
    `hybrid` input tokens in the input. Tokens up to this length refer to both front
    and back like BERT, tokens after that refer only to front like GPT. see also:
    [https://github.com/tanreinama/GPTSAN/blob/main/report/model.md](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_precontext` (`torch.LongTensor`，形状为`(batch_size,1)`) — 输入中`hybrid`标记的长度。直到此长度的标记同时指向前后，类似于BERT，之后的标记只指向前，类似于GPT。另请参阅：[https://github.com/tanreinama/GPTSAN/blob/main/report/model.md](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)'
- en: The [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel)
    forward method, overrides the `__call__` special method.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。
- en: GPTSanJapaneseForConditionalGeneration
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPTSanJapaneseForConditionalGeneration
- en: '### `class transformers.GPTSanJapaneseForConditionalGeneration`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.GPTSanJapaneseForConditionalGeneration`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100)'
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([GPTSanJapaneseConfig](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare GPTSAN-japanese Model with a language modeling head.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed
    in General-purpose Swich transformer based Japanese language model
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1115)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model
    that generates sentence continuations or predicts tokens at mask positions. Special
    tokens required for inputs to the model are automatically appended.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — An input that masks the Prefix part in the Prefix-LM input. Mask
    values selected in `[0, 1]`:'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `prefix` input,
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not-prefix` input.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spout` (`torch.Tensor` of shape `(batch_size, config.d_spout)`) — This vector
    is transformed through an 8-layer FFN and can be used instead of `past_key_values`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    而不是一个普通元组。'
- en: '`router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True`
    is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, sequence_length, num_experts)`. Router
    logits of the decoder model, useful to compute the auxiliary loss for Mixture
    of Experts models.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`router_logits` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_router_logits=True`
    或 `config.add_router_probs=True` 时返回) — 形状为 `(batch_size, sequence_length, num_experts)`
    的 `torch.FloatTensor` 元组（每层一个）。解码器模型的路由器对数，用于计算混合专家模型的辅助损失。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification loss. Indices should be in `[-100, 0,
    ..., config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the
    loss is only computed for labels in `[0, ..., config.vocab_size]`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算序列分类损失的标签。索引应在
    `[-100, 0, ..., config.vocab_size - 1]` 中。所有设置为 `-100` 的标签都被忽略（掩码），损失仅计算 `[0,
    ..., config.vocab_size]` 中的标签'
- en: The [GPTSanJapaneseForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTSanJapaneseForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration)
    的前向方法，覆盖了 `__call__` 特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: Text Generation with regular LM Model
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 带有常规LM模型的文本生成
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Text Generation with Prefix-LM Model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 带有前缀-LM模型的文本生成
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Simultaneously Text Generation And Masked Language Model
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同时进行文本生成和掩码语言模型
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
