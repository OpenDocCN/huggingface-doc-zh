- en: GPTSAN-japanese
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto
    (tanreinama).
  prefs: []
  type: TYPE_NORMAL
- en: GPTSAN is a Japanese language model using Switch Transformer. It has the same
    structure as the model introduced as Prefix LM in the T5 paper, and support both
    Text Generation and Masked Language Modeling tasks. These basic tasks similarly
    can fine-tune for translation or summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `generate()` method can be used to generate text using GPTSAN-Japanese model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: GPTSAN Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPTSAN has some unique features. It has a model structure of Prefix-LM. It works
    as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs
    behave like normal generative models. The Spout vector is a GPTSAN specific input.
    Spout is pre-trained with random inputs, but you can specify a class of text or
    an arbitrary vector during fine-tuning. This allows you to indicate the tendency
    of the generated text. GPTSAN has a sparse Feed Forward based on Switch-Transformer.
    You can also add other layers and train them partially. See the original GPTSAN
    repository for details.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix-LM Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPTSAN has the structure of the model named Prefix-LM in the `T5` paper. (The
    original GPTSAN repository calls it `hybrid`) In GPTSAN, the `Prefix` part of
    Prefix-LM, that is, the input position that can be referenced by both tokens,
    can be specified with any length. Arbitrary lengths can also be specified differently
    for each batch. This length applies to the text entered in `prefix_text` for the
    tokenizer. The tokenizer returns the mask of the `Prefix` part of Prefix-LM as
    `token_type_ids`. The model treats the part where `token_type_ids` is 1 as a `Prefix`
    part, that is, the input can refer to both tokens before and after.
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Specifying the Prefix part is done with a mask passed to self-attention. When
    token_type_ids=None or all zero, it is equivalent to regular causal mask
  prefs: []
  type: TYPE_NORMAL
- en: 'for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'x_token = tokenizer(“ｱｲｳｴ”) input_ids: | SOT | SEG | ｱ | ｲ | ｳ | ｴ | token_type_ids:
    | 1 | 0 | 0 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 0 0 0 0 0 | SEG | 1 1 0 0 0
    0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 1 0 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1 1 |'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'x_token = tokenizer("", prefix_text=“ｱｲｳｴ”) input_ids: | SOT | ｱ | ｲ | ｳ |
    ｴ | SEG | token_type_ids: | 1 | 1 | 1 | 1 | 1 | 0 | prefix_lm_mask: SOT | 1 1
    1 1 1 0 | ｱ | 1 1 1 1 1 0 | ｲ | 1 1 1 1 1 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1
    0 | SEG | 1 1 1 1 1 1 |'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'x_token = tokenizer(“ｳｴ”, prefix_text=“ｱｲ”) input_ids: | SOT | ｱ | ｲ | SEG
    | ｳ | ｴ | token_type_ids: | 1 | 1 | 1 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 1
    1 0 0 0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 0 0 0 | SEG | 1 1 1 1 0 0 | ｳ | 1 1 1 1
    1 0 | ｴ | 1 1 1 1 1 1 |'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Spout Vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Spout Vector is a special vector for controlling text generation. This vector
    is treated as the first embedding in self-attention to bring extraneous attention
    to the generated tokens. In the pre-trained model published from `Tanrei/GPTSAN-japanese`,
    the Spout Vector is a 128-dimensional vector that passes through 8 fully connected
    layers in the model and is projected into the space acting as external attention.
    The Spout Vector projected by the fully connected layer is split to be passed
    to all self-attentions.
  prefs: []
  type: TYPE_NORMAL
- en: GPTSanJapaneseConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSanJapaneseConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/configuration_gptsan_japanese.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 36000) — Vocabulary size of the
    GPTSANJapanese model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1280) — The maximum
    sequence length that this model might ever be used with. Defaults set this to
    1280.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_model` (`int`, *optional*, defaults to 1024) — Size of the encoder layers
    and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_ff` (`int`, *optional*, defaults to 8192) — Size of the intermediate feed
    forward layer in each `SwitchTransformersBlock`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_ext` (`int`, *optional*, defaults to 4096) — Size of the intermediate feed
    forward layer in each Extra-layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_spout` (`int`, *optional*, defaults to 128) — Size of the `spout` vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_switch_layers` (`int`, *optional*, defaults to 10) — Number of layers
    in the Switch Transformer layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_ext_layers` (`int`, *optional*, defaults to 0) — Number of layers in the
    Extra-layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_heads` (`int`, *optional*, defaults to 16) — Number of attention heads
    for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_experts` (`int`, *optional*, defaults to 16) — Number of experts for each
    SwitchTransformer layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expert_capacity` (`int`, *optional*, defaults to 128) — Number of tokens that
    can be stored in each expert. If set to 1, the model will behave like a regular
    Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout_rate` (`float`, *optional*, defaults to 0.0) — The ratio for all dropout
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_bias` (`bool`, *optional*, defaults to `False`) — Whether to add a
    bias to the router.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_jitter_noise` (`float`, *optional*, defaults to 0.0) — Amount of noise
    to add to the router. Set it to 0.0 during prediction or set small value (usually
    1e-2) during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_dtype` (`str`, *optional*, default to `"float32"`) — The `dtype` used
    for the routers. It is preferable to keep the `dtype` to `"float32"` as specified
    in the *selective precision* discussion in [the paper](https://arxiv.org/abs/2101.03961).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) —
    Whether to ignore padding tokens when routing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*, default to `False`) — Whether or
    not to return the hidden states of all layers. See `hidden_states` under returned
    tensors for more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*, defaults to `False`) — Whether or
    not to return the attentions tensors of all attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_factor` (`float`, *optional*, defaults to 0.002) — A factor for
    initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_router_logits` (`bool`, *optional*, default to `False`) — Whether or
    not to return the router logits of all experts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel).
    It is used to instantiate a GPTSANJapanese model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the GPTSANJapanese [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: GPTSanJapaneseTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSanJapaneseTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emoji_file` (`str`) — File containing the emoji.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<|nottoken|>"`) — The token used
    for unknown charactor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<|separator|>"`) — The token
    used for padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<|startoftext|>"`) — The beginning
    of sequence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"<|endoftext|>"`) — The end of
    sequence token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"<|segmenter|>"`) — A special
    token to separate token to prefix part and general input part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_clean_text` (`bool`, *optional*, defaults to `False`) — Whether or not
    to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications
  prefs: []
  type: TYPE_NORMAL
- en: Decoding byte0~byte255 tokens correctly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Added bagofword token handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return token_type_ids for Prefix-LM model The bagofword token represents a repetition
    of the previous token and is converted to 3 consecutive tokens when decoding In
    addition, the original Japanese special Sub-Word-Encoding has been released in
    this repository ([https://github.com/tanreinama/Japanese-BPEEncoder_V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2)).
    The token_type_ids is a mask indicating the prefix input position of the Prefix-LM
    model. To specify a prefix position, specify a prefix input for prefix_text, or
    specify a sentence of the prefix part and the part after it as a text pair of
    batch input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Example for Prefix-LM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Example for batch encode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#### `convert_tokens_to_string`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Converts a sequence of tokens (string) in a single string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_token_type_ids_from_sequences`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The tokenizer returns token_type_ids as separators between the Prefix part and
    the rest. token_type_ids is 1 for the Prefix part and 0 for the rest of the token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: GPTSanJapaneseModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSanJapaneseModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([GPTSanJapaneseConfig](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare GPTSAN-japanese Model transformer outputting raw hidden-states without
    any specific head on top.
  prefs: []
  type: TYPE_NORMAL
- en: The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed
    in General-purpose Swich transformer based Japanese language model
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model
    that generates sentence continuations or predicts tokens at mask positions. Special
    tokens required for inputs to the model are automatically appended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — An input that masks the Prefix part in the Prefix-LM input. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `prefix` input,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not-prefix` input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spout` (`torch.Tensor` of shape `(batch_size, config.d_spout)`) — This vector
    is transformed through an 8-layer FFN and can be used instead of `past_key_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True`
    is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, sequence_length, num_experts)`. Router
    logits of the decoder model, useful to compute the auxiliary loss for Mixture
    of Experts models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_precontext` (`torch.LongTensor` of shape `(batch_size,1)`) — length of
    `hybrid` input tokens in the input. Tokens up to this length refer to both front
    and back like BERT, tokens after that refer only to front like GPT. see also:
    [https://github.com/tanreinama/GPTSAN/blob/main/report/model.md](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [GPTSanJapaneseModel](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: GPTSanJapaneseForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.GPTSanJapaneseForConditionalGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([GPTSanJapaneseConfig](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare GPTSAN-japanese Model with a language modeling head.
  prefs: []
  type: TYPE_NORMAL
- en: The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed
    in General-purpose Swich transformer based Japanese language model
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model
    that generates sentence continuations or predicts tokens at mask positions. Special
    tokens required for inputs to the model are automatically appended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — An input that masks the Prefix part in the Prefix-LM input. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `prefix` input,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not-prefix` input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spout` (`torch.Tensor` of shape `(batch_size, config.d_spout)`) — This vector
    is transformed through an 8-layer FFN and can be used instead of `past_key_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers`
    with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length
    - 1, embed_size_per_head)`) — Contains precomputed key and value hidden states
    of the attention blocks. Can be used to speed up decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`router_logits` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True`
    is passed or when `config.add_router_probs=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, sequence_length, num_experts)`. Router
    logits of the decoder model, useful to compute the auxiliary loss for Mixture
    of Experts models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification loss. Indices should be in `[-100, 0,
    ..., config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the
    loss is only computed for labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [GPTSanJapaneseForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation with regular LM Model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Text Generation with Prefix-LM Model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Simultaneously Text Generation And Masked Language Model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
