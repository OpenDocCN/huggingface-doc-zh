# AltCLIP

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/altclip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/altclip)

## 概述

AltCLIP模型是由陈忠志、刘光、张博文、叶福龙、杨庆红、吴乐德在[AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679v2)中提出的。AltCLIP（改变CLIP中的语言编码器）是一个神经网络，训练于各种图像文本和文本文本对。通过用预训练的多语言文本编码器XLM-R替换CLIP的文本编码器，我们可以在几乎所有任务上获得与CLIP非常接近的性能，并扩展原始CLIP的能力，如多语言理解。

论文摘要如下：

*在这项工作中，我们提出了一个概念简单且有效的方法来训练强大的双语多模态表示模型。从OpenAI发布的预训练多模态表示模型CLIP开始，我们将其文本编码器替换为预训练的多语言文本编码器XLM-R，并通过包含教师学习和对比学习的两阶段训练模式来对齐两种语言和图像表示。我们通过对各种任务的评估验证了我们的方法。我们在一系列任务中取得了新的最先进表现，包括ImageNet-CN、Flicker30k-CN和COCO-CN。此外，我们在几乎所有任务上与CLIP获得了非常接近的性能，表明可以简单地改变CLIP中的文本编码器以获得扩展能力，如多语言理解。*

该模型由[jongjyh](https://huggingface.co/jongjyh)贡献。

## 使用提示和示例

AltCLIP的使用与CLIP非常相似，区别在于文本编码器。请注意，我们使用双向注意力而不是单向注意力，并且我们使用XLM-R中的[CLS]标记来表示文本嵌入。

AltCLIP是一个多模态视觉和语言模型。它可用于图像文本相似度和零样本图像分类。AltCLIP使用类似ViT的变压器来获取视觉特征，并使用双向语言模型来获取文本特征。然后将文本和视觉特征投影到具有相同维度的潜在空间中。然后使用投影图像和文本特征之间的点积作为相似分数。

为了将图像馈送到变压器编码器中，每个图像被分割成一系列固定大小且不重叠的补丁，然后进行线性嵌入。添加一个[CLS]标记作为整个图像的表示。作者还添加了绝对位置嵌入，并将结果向量序列馈送到标准变压器编码器。[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)可用于调整（或重新缩放）和规范化模型的图像。

[AltCLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPProcessor)将[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)和[XLMRobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer)封装成一个单一实例，用于对文本进行编码和准备图像。以下示例展示了如何使用[AltCLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPProcessor)和[AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel)获取图像文本相似度分数。

```py
>>> from PIL import Image
>>> import requests

>>> from transformers import AltCLIPModel, AltCLIPProcessor

>>> model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AltCLIPProcessor.from_pretrained("BAAI/AltCLIP")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

该模型基于`CLIPModel`，使用方式与原始的[CLIP](clip)相同。

## AltCLIPConfig

### `class transformers.AltCLIPConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/configuration_altclip.py#L259)

```py
( text_config = None vision_config = None projection_dim = 768 logit_scale_init_value = 2.6592 **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化 [AltCLIPTextConfig](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextConfig) 的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化 [AltCLIPVisionConfig](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPVisionConfig) 的配置选项字典。

+   `projection_dim` (`int`, *optional*, defaults to 768) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`, *optional*, defaults to 2.6592) — *logit_scale* 参数的初始值。默认值根据原始的 CLIP 实现使用。

+   `kwargs` (*optional*) — 关键字参数的字典。

这是一个配置类，用于存储 [AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel) 的配置。根据指定的参数实例化一个 AltCLIP 模型，定义模型架构。使用默认值实例化配置将产生类似于 AltCLIP [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import AltCLIPConfig, AltCLIPModel

>>> # Initializing a AltCLIPConfig with BAAI/AltCLIP style configuration
>>> configuration = AltCLIPConfig()

>>> # Initializing a AltCLIPModel (with random weights) from the BAAI/AltCLIP style configuration
>>> model = AltCLIPModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a AltCLIPConfig from a AltCLIPTextConfig and a AltCLIPVisionConfig

>>> # Initializing a AltCLIPText and AltCLIPVision configuration
>>> config_text = AltCLIPTextConfig()
>>> config_vision = AltCLIPVisionConfig()

>>> config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/configuration_altclip.py#L394)

```py
( text_config: AltCLIPTextConfig vision_config: AltCLIPVisionConfig **kwargs ) → export const metadata = 'undefined';AltCLIPConfig
```

返回

[AltCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPConfig)

一个配置对象的实例

从 altclip 文本模型配置和 altclip 视觉模型配置实例化一个 [AltCLIPConfig](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPConfig)（或派生类）。

## AltCLIPTextConfig

### `class transformers.AltCLIPTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/configuration_altclip.py#L31)

```py
( vocab_size = 250002 hidden_size = 1024 num_hidden_layers = 24 num_attention_heads = 16 intermediate_size = 4096 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 initializer_range = 0.02 initializer_factor = 0.02 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True project_dim = 768 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 250002) — AltCLIP 模型的词汇表大小。定义了在调用 [AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel) 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 1024) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 24) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 16) — Transformer 编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, defaults to 4096) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢失比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 514) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 1) — 在调用 [AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel) 时传递的 `token_type_ids` 的词汇表大小。

+   `initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-05) — 层归一化层使用的epsilon。

+   `pad_token_id` (`int`, *optional*, 默认为1) — *填充* 标记的id。

+   `bos_token_id` (`int`, *optional*, 默认为0) — *序列开始* 标记的id。

+   `eos_token_id` (`Union[int, List[int]]`, *optional*, 默认为2) — *序列结束* 标记的id。可选择使用列表设置多个 *序列结束* 标记。

+   `position_embedding_type` (`str`, *optional*, 默认为`"absolute"`) — 位置嵌入的类型。选择 `"absolute"`, `"relative_key"`, `"relative_key_query"` 中的一个。对于位置嵌入，请使用 `"absolute"`。有关 `"relative_key"` 的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关 `"relative_key_query"` 的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658) 中的 *Method 4*。

+   `use_cache` (`bool`, *optional*, 默认为`True`) — 模型是否应返回最后的键/值注意力（不是所有模型都使用）。仅在 `config.is_decoder=True` 时相关。

+   `project_dim` (`int`, *optional*, 默认为768) — 映射层之前教师模型的维度。

这是用于存储[AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel)配置的配置类。根据指定的参数实例化一个AltCLIP文本模型的配置，定义模型架构。使用默认值实例化配置将产生类似于AltCLIP [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import AltCLIPTextModel, AltCLIPTextConfig

>>> # Initializing a AltCLIPTextConfig with BAAI/AltCLIP style configuration
>>> configuration = AltCLIPTextConfig()

>>> # Initializing a AltCLIPTextModel (with random weights) from the BAAI/AltCLIP style configuration
>>> model = AltCLIPTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AltCLIPVisionConfig

### `class transformers.AltCLIPVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/configuration_altclip.py#L149)

```py
( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `projection_dim` (`int`, *optional*, 默认为512) — 文本和视觉投影层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。

+   `num_channels` (`int`, *optional*, 默认为3) — 输入通道数。

+   `image_size` (`int`, *optional*, 默认为224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, 默认为32) — 每个patch的大小（分辨率）。

+   `hidden_act` (`str` 或 `function`, *optional*, 默认为`"quick_gelu"`) — 编码器和池化层中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"` 以及 `"quick_gelu"`。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-05) — 层归一化层使用的epsilon。

+   `attention_dropout`（`float`，*可选*，默认为0.0）— 注意力概率的dropout比率。

+   `initializer_range`（`float`，*可选*，默认为0.02）— 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `initializer_factor`（`float`，*可选*，默认为1.0）— 用于初始化所有权重矩阵的因子（应保持为1，用于内部初始化测试）。

这是用于存储[AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel)配置的配置类。它用于根据指定的参数实例化AltCLIP模型，定义模型架构。使用默认值实例化配置将产生类似于AltCLIP [BAAI/AltCLIP](https://huggingface.co/BAAI/AltCLIP)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import AltCLIPVisionConfig, AltCLIPVisionModel

>>> # Initializing a AltCLIPVisionConfig with BAAI/AltCLIP style configuration
>>> configuration = AltCLIPVisionConfig()

>>> # Initializing a AltCLIPVisionModel (with random weights) from the BAAI/AltCLIP style configuration
>>> model = AltCLIPVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## AltCLIPProcessor

### `class transformers.AltCLIPProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/processing_altclip.py#L24)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)，*可选*）— 图像处理器是必需的。

+   `tokenizer`（[XLMRobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast)，*可选*）— 分词器是必需的。

构建一个AltCLIP处理器，将CLIP图像处理器和XLM-Roberta分词器包装成一个单一处理器。

[AltCLIPProcessor](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPProcessor)提供了[CLIPImageProcessor](/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)和[XLMRobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast)的所有功能。查看`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPProcessor.decode)以获取更多信息。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/processing_altclip.py#L114)

```py
( *args **kwargs )
```

此方法将其所有参数转发给XLMRobertaTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/processing_altclip.py#L121)

```py
( *args **kwargs )
```

此方法将其所有参数转发给XLMRobertaTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参阅此方法的文档字符串。

## AltCLIPModel

### `class transformers.AltCLIPModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1461)

```py
( config: AltCLIPConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1588)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None token_type_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.altclip.modeling_altclip.AltCLIPOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   对于`未被掩码`的标记为1，

    +   对于`被掩码`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`, *可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.altclip.modeling_altclip.AltCLIPOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.altclip.modeling_altclip.AltCLIPOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时），包括根据配置(`<class 'transformers.models.altclip.configuration_altclip.AltCLIPConfig'>`)和输入的各种元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当`return_loss`为`True`时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(torch.FloatTensor`，形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(torch.FloatTensor`，形状为`(text_batch_size, image_batch_size)`) — `text_embeds`和`image_embeds`之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel)的汇聚输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor`，形状为`(batch_size, output_dim`) — 通过将投影层应用于[AltCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPVisionModel)的汇聚输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel)的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [AltCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPVisionModel)的输出。

[AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AltCLIPModel

>>> model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
... )
>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1495)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None token_type_ids = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入ID？

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）- 避免在填充令牌索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   对于未被屏蔽的标记，为1，

    +   对于被屏蔽的标记，为0。

    什么是注意力掩码？

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    什么是位置ID？

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

text_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`）

通过将投影层应用于[AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel)的汇总输出获得的文本嵌入。

[AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AltCLIPModel

>>> model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1542)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（布尔值，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features（形状为（batch_size，output_dim）的torch.FloatTensor）

通过将投影层应用于[AltCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPVisionModel)的汇聚输出获得的图像嵌入。

[AltCLIPModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AltCLIPModel

>>> model = AltCLIPModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")
>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> inputs = processor(images=image, return_tensors="pt")
>>> image_features = model.get_image_features(**inputs)
```

## AltCLIPTextModel

### `class transformers.AltCLIPTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1369)

```py
( config )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1388)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None output_attentions: Optional = None return_dict: Optional = None output_hidden_states: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为（batch_size，sequence_length）的torch.LongTensor）— 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为（batch_size，sequence_length）的torch.Tensor，*可选*）— 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示“未被掩盖”的标记，

    +   0表示“被掩盖”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为（batch_size，sequence_length）的torch.LongTensor，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions`（布尔值，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（布尔值，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（布尔值，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection`或`tuple(torch.FloatTensor)`

一个`transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（`<class 'transformers.models.altclip.configuration_altclip.AltCLIPTextConfig'>`）和输入。

+   `last_hidden_state`（形状为（batch_size，sequence_length，hidden_size）的torch.FloatTensor）— 模型最后一层的隐藏状态序列输出。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后，序列中第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出一个 + 每一层的输出一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `projection_state` (`tuple(torch.FloatTensor)`，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size,config.project_dim)`的`torch.FloatTensor`元组。

    投影层之前的文本嵌入，用于模仿教师编码器的最后隐藏状态。

[AltCLIPTextModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPTextModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, AltCLIPTextModel

>>> model = AltCLIPTextModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")

>>> texts = ["it's a cat", "it's a dog"]

>>> inputs = processor(text=texts, padding=True, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```

## AltCLIPVisionModel

### `class transformers.AltCLIPVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1138)

```py
( config: AltCLIPVisionConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/altclip/modeling_altclip.py#L1151)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[CLIPImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（`<class 'transformers.models.altclip.configuration_altclip.AltCLIPVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) — 经过用于辅助预训练任务的层进一步处理后，序列中第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是从下一个句子预测（分类）目标在预训练期间训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[AltCLIPVisionModel](/docs/transformers/v4.37.2/en/model_doc/altclip#transformers.AltCLIPVisionModel)的前向方法覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, AltCLIPVisionModel

>>> model = AltCLIPVisionModel.from_pretrained("BAAI/AltCLIP")
>>> processor = AutoProcessor.from_pretrained("BAAI/AltCLIP")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> outputs = model(**inputs)
>>> last_hidden_state = outputs.last_hidden_state
>>> pooled_output = outputs.pooler_output  # pooled CLS states
```
