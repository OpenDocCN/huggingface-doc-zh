["```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)\n>>> print(next(iter(dataset)))\n{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...\n```", "```py\n>>> from datasets import load_dataset\n>>> data_files = {'train': 'path/to/OSCAR-2201/compressed/en_meta/*.jsonl.gz'}\n>>> dataset = load_dataset('json', data_files=data_files, split='train', streaming=True)\n>>> print(next(iter(dataset)))\n{'id': 0, 'text': 'Founded in 2015, Golden Bees is a leading programmatic recruitment platform dedicated to employers, HR agencies and job boards. The company has developed unique HR-custom technologies and predictive algorithms to identify and attract the best candidates for a job opportunity.', ...\n```", "```py\n>>> from datasets import load_dataset\n\n# faster \ud83d\udc07\n>>> dataset = load_dataset(\"food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset()\n\n# slower \ud83d\udc22\n>>> iterable_dataset = load_dataset(\"food101\", streaming=True)\n```", "```py\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"food101\")\n>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=64) # shard the dataset\n>>> iterable_dataset = iterable_dataset.shuffle(buffer_size=10_000)  # shuffles the shards order and use a shuffle buffer when you start iterating\ndataloader = torch.utils.data.DataLoader(iterable_dataset, num_workers=4)  # assigns 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n>>> shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n```", "```py\n>>> for epoch in range(epochs):\n...     shuffled_dataset.set_epoch(epoch)\n...     for example in shuffled_dataset:\n...         ...\n```", "```py\n>>> dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n>>> dataset_head = dataset.take(2)\n>>> list(dataset_head)\n[{'id': 0, 'text': 'Mtendere Village was...'}, {'id': 1, 'text': 'Lily James cannot fight the music...'}]\n```", "```py\n>>> train_dataset = shuffled_dataset.skip(1000)\n```", "```py\n>>> from datasets import interleave_datasets\n>>> en_dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True, trust_remote_code=True)\n>>> fr_dataset = load_dataset('oscar', \"unshuffled_deduplicated_fr\", split='train', streaming=True, trust_remote_code=True)\n\n>>> multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])\n>>> list(multilingual_dataset.take(2))\n[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': \"M\u00e9dia de d\u00e9bat d'id\u00e9es, de culture et de litt\u00e9rature...\"}]\n```", "```py\n>>> multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)\n>>> list(multilingual_dataset_with_oversampling.take(2))\n[{'text': 'Mtendere Village was inspired by the vision...'}, {'text': 'Lily James cannot fight the music...'}]\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)\n>>> dataset = dataset.rename_column(\"text\", \"content\")\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('mc4', 'en', streaming=True, split='train', trust_remote_code=True)\n>>> dataset = dataset.remove_columns('timestamp')\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('glue', 'mrpc', split='train', streaming=True)\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n'idx': Value(dtype='int32', id=None)}\n\n>>> from datasets import ClassLabel, Value\n>>> new_features = dataset.features.copy()\n>>> new_features[\"label\"] = ClassLabel(names=['negative', 'positive'])\n>>> new_features[\"idx\"] = Value('int64')\n>>> dataset = dataset.cast(new_features)\n>>> dataset.features\n{'sentence1': Value(dtype='string', id=None),\n'sentence2': Value(dtype='string', id=None),\n'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),\n'idx': Value(dtype='int64', id=None)}\n```", "```py\n>>> dataset.features\n{'audio': Audio(sampling_rate=44100, mono=True, id=None)}\n\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> dataset.features\n{'audio': Audio(sampling_rate=16000, mono=True, id=None)}\n```", "```py\n>>> def add_prefix(example):\n...     example['text'] = 'My text: ' + example['text']\n...     return example\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)\n>>> updated_dataset = dataset.map(add_prefix)\n>>> list(updated_dataset.take(3))\n[{'id': 0, 'text': 'My text: Mtendere Village was inspired by...'},\n {'id': 1, 'text': 'My text: Lily James cannot fight the music...'},\n {'id': 2, 'text': 'My text: \"I\\'d love to help kickstart...'}]\n```", "```py\n>>> updated_dataset = dataset.map(add_prefix, remove_columns=[\"id\"])\n>>> list(updated_dataset.take(3))\n[{'text': 'My text: Mtendere Village was inspired by...'},\n {'text': 'My text: Lily James cannot fight the music...'},\n {'text': 'My text: \"I\\'d love to help kickstart...'}]\n```", "```py\n>>> from datasets import load_dataset\n>>> from transformers import AutoTokenizer\n>>> dataset = load_dataset(\"mc4\", \"en\", streaming=True, split=\"train\", trust_remote_code=True)\n>>> tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n>>> def encode(examples):\n...     return tokenizer(examples['text'], truncation=True, padding='max_length')\n>>> dataset = dataset.map(encode, batched=True, remove_columns=[\"text\", \"timestamp\", \"url\"])\n>>> next(iter(dataset))\n{'input_ids': [101, 8466, 1018, 1010, 4029, 2475, 2062, 18558, 3100, 2061, ...,1106, 3739, 102],\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1]}\n```", "```py\n>>> from datasets import load_dataset\n>>> dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train', trust_remote_code=True)\n>>> start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))\n>>> next(iter(start_with_ar))\n{'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)?...'}\n```", "```py\n>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)\n>>> list(even_dataset.take(3))\n[{'id': 0, 'text': 'Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...'},\n {'id': 2, 'text': '\"I\\'d love to help kickstart continued development! And 0 EUR/month...'},\n {'id': 4, 'text': 'Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...'}]\n```", "```py\n>>> seed, buffer_size = 42, 10_000\n>>> dataset = dataset.shuffle(seed, buffer_size=buffer_size)\n```", "```py\n>>> import torch\n>>> from torch.utils.data import DataLoader\n>>> from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling\n>>> from tqdm import tqdm\n>>> dataset = dataset.with_format(\"torch\")\n>>> dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))\n>>> device = 'cuda' if torch.cuda.is_available() else 'cpu' \n>>> model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n>>> model.train().to(device)\n>>> optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n>>> for epoch in range(3):\n...     dataset.set_epoch(epoch)\n...     for i, batch in enumerate(tqdm(dataloader, total=5)):\n...         if i == 5:\n...             break\n...         batch = {k: v.to(device) for k, v in batch.items()}\n...         outputs = model(**batch)\n...         loss = outputs[0]\n...         loss.backward()\n...         optimizer.step()\n...         optimizer.zero_grad()\n...         if i % 10 == 0:\n...             print(f\"loss: {loss}\")\n```"]