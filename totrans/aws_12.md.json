["```py\npip install sentence-transformers\n```", "```py\nfrom optimum.neuron import NeuronModelForSentenceTransformers\n\n# Sentence Transformers model from HuggingFace\nmodel_id = \"BAAI/bge-small-en-v1.5\"\ninput_shapes = {\"batch_size\": 1, \"sequence_length\": 384}  # mandatory shapes\n\n# Load Transformers model and export it to AWS Inferentia2\nmodel = NeuronModelForSentenceTransformers.from_pretrained(model_id, export=True, **input_shapes)\n\n# Save model to disk\nmodel.save_pretrained(\"bge_emb_inf2/\")\n```", "```py\noptimum-cli export neuron -m BAAI/bge-small-en-v1.5 --library-name sentence_transformers --sequence_length 384 --batch_size 1 --task feature-extraction bge_emb_inf2/\n```", "```py\nfrom optimum.neuron import NeuronModelForSentenceTransformers\nfrom transformers import AutoTokenizer\n\nmodel_id_or_path = \"bge_emb_inf2/\"\ntokenizer_id = \"BAAI/bge-small-en-v1.5\"\n\n# Load model and tokenizer\nmodel = NeuronModelForSentenceTransformers.from_pretrained(model_id_or_path)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n\n# Run inference\nprompt = \"I like to eat apples\"\nencoded_input = tokenizer(prompt, return_tensors='pt')\noutputs = model(**encoded_input)\n\ntoken_embeddings = outputs.token_embeddings\nsentence_embedding = outputs.sentence_embedding\n\nprint(f\"token embeddings: {token_embeddings.shape}\") # torch.Size([1, 7, 384])\nprint(f\"sentence_embedding: {sentence_embedding.shape}\") # torch.Size([1, 384])\n```"]