- en: TRL - Transformer Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRL - Transformer Reinforcement Learning
- en: 'Original text: [https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)
- en: '![](../Images/4b294f1850c2a0865587ccc36b23d404.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/4b294f1850c2a0865587ccc36b23d404.png)'
- en: TRL is a full stack library where we provide a set of tools to train transformer
    language models with Reinforcement Learning, from the Supervised Fine-tuning step
    (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.
    The library is integrated with 🤗 [transformers](https://github.com/huggingface/transformers).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: TRL是一个全栈库，我们提供一套工具来使用强化学习训练变压器语言模型，从监督微调步骤（SFT）、奖励建模步骤（RM）到近端策略优化（PPO）步骤。该库与🤗
    [transformers](https://github.com/huggingface/transformers)集成。
- en: '![](../Images/6bff4454a0be1455f1b3b09f74640ec7.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bff4454a0be1455f1b3b09f74640ec7.png)'
- en: 'Check the appropriate sections of the documentation depending on your needs:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的需求查看文档中的适当部分：
- en: API documentation
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API文档
- en: '[Model Classes](models): *A brief overview of what each public model class
    does.*'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模型类](models): *每个公共模型类的简要概述*'
- en: '[`SFTTrainer`](sft_trainer): *Supervise Fine-tune your model easily with `SFTTrainer`*'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`SFTTrainer`](sft_trainer): *使用`SFTTrainer`轻松监督微调您的模型*'
- en: '[`RewardTrainer`](reward_trainer): *Train easily your reward model using `RewardTrainer`.*'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`RewardTrainer`](reward_trainer): *使用`RewardTrainer`轻松训练您的奖励模型*'
- en: '[`PPOTrainer`](ppo_trainer): *Further fine-tune the supervised fine-tuned model
    using PPO algorithm*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`PPOTrainer`](ppo_trainer): *使用PPO算法进一步微调监督微调的模型*'
- en: '[Best-of-N Sampling](best-of-n): *Use best of n sampling as an alternative
    way to sample predictions from your active model*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最佳N采样](best-of-n): *使用最佳N采样作为从活跃模型中采样预测的替代方式*'
- en: '[`DPOTrainer`](dpo_trainer): *Direct Preference Optimization training using
    `DPOTrainer`.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`DPOTrainer`](dpo_trainer): *使用`DPOTrainer`进行直接偏好优化训练*'
- en: '[`TextEnvironment`](text_environment): *Text environment to train your model
    using tools with RL.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`TextEnvironment`](text_environment): *使用RL工具训练您的模型的文本环境*'
- en: Examples
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: '[Sentiment Tuning](sentiment_tuning): *Fine tune your model to generate positive
    movie contents*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[情感微调](sentiment_tuning): *微调您的模型以生成积极的电影内容*'
- en: '[Training with PEFT](lora_tuning_peft): *Memory efficient RLHF training using
    adapters with PEFT*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PEFT进行训练](lora_tuning_peft): *使用PEFT和适配器进行内存高效的RLHF训练*'
- en: '[Detoxifying LLMs](detoxifying_a_lm): *Detoxify your language model through
    RLHF*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[去毒化LLMs](detoxifying_a_lm): *通过RLHF去毒化您的语言模型*'
- en: '[StackLlama](using_llama_models): *End-to-end RLHF training of a Llama model
    on Stack exchange dataset*'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Llama模型](using_llama_models): *在Stack交换数据集上进行端到端的RLHF训练*'
- en: '[Learning with Tools](learning_tools): *Walkthrough of using `TextEnvironments`*'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用工具进行学习](learning_tools): *使用`TextEnvironments`的演练*'
- en: '[Multi-Adapter Training](multi_adapter_rl): *Use a single base model and multiple
    adapters for memory efficient end-to-end training*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多适配器训练](multi_adapter_rl): *使用单个基础模型和多个适配器进行内存高效的端到端训练*'
- en: Blog posts
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 博客文章
- en: '[![thumbnail](../Images/6a5d903a4cb2b18948ab6f3db912185c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![缩略图](../Images/6a5d903a4cb2b18948ab6f3db912185c.png)'
- en: Illustrating Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf)
    [![thumbnail](../Images/22aa00200f429b7d9ce649b675e3f3b1.png)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类反馈中阐释强化学习](https://huggingface.co/blog/rlhf) [![缩略图](../Images/22aa00200f429b7d9ce649b675e3f3b1.png)
- en: Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft)
    [![thumbnail](../Images/c83a44cfb05cf6785db903a8231e5d59.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在24GB消费级GPU上使用RLHF对20B LLM进行微调](https://huggingface.co/blog/trl-peft) [![缩略图](../Images/c83a44cfb05cf6785db903a8231e5d59.png)
- en: 'StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)
    [![thumbnail](../Images/aca25811c595ffdf3eabce54f97a87d5.png)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: StackLLaMA：使用RLHF进行LLaMA的实践指南](https://huggingface.co/blog/stackllama) [![缩略图](../Images/aca25811c595ffdf3eabce54f97a87d5.png)
- en: Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) [![thumbnail](../Images/3aa727b1aa5caba1090533892ec9e378.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DPO对Llama 2进行微调](https://huggingface.co/blog/dpo-trl) [![缩略图](../Images/3aa727b1aa5caba1090533892ec9e378.png)
- en: Finetune Stable Diffusion Models with DDPO via TRL](https://huggingface.co/blog/trl-ddpo)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TRL通过DDPO微调稳定扩散模型](https://huggingface.co/blog/trl-ddpo)
