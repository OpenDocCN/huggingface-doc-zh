- en: TRL - Transformer Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4b294f1850c2a0865587ccc36b23d404.png)'
  prefs: []
  type: TYPE_NORMAL
- en: TRL is a full stack library where we provide a set of tools to train transformer
    language models with Reinforcement Learning, from the Supervised Fine-tuning step
    (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step.
    The library is integrated with ðŸ¤— [transformers](https://github.com/huggingface/transformers).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bff4454a0be1455f1b3b09f74640ec7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Check the appropriate sections of the documentation depending on your needs:'
  prefs: []
  type: TYPE_NORMAL
- en: API documentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Model Classes](models): *A brief overview of what each public model class
    does.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`SFTTrainer`](sft_trainer): *Supervise Fine-tune your model easily with `SFTTrainer`*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`RewardTrainer`](reward_trainer): *Train easily your reward model using `RewardTrainer`.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`PPOTrainer`](ppo_trainer): *Further fine-tune the supervised fine-tuned model
    using PPO algorithm*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best-of-N Sampling](best-of-n): *Use best of n sampling as an alternative
    way to sample predictions from your active model*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`DPOTrainer`](dpo_trainer): *Direct Preference Optimization training using
    `DPOTrainer`.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[`TextEnvironment`](text_environment): *Text environment to train your model
    using tools with RL.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Sentiment Tuning](sentiment_tuning): *Fine tune your model to generate positive
    movie contents*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Training with PEFT](lora_tuning_peft): *Memory efficient RLHF training using
    adapters with PEFT*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Detoxifying LLMs](detoxifying_a_lm): *Detoxify your language model through
    RLHF*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[StackLlama](using_llama_models): *End-to-end RLHF training of a Llama model
    on Stack exchange dataset*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learning with Tools](learning_tools): *Walkthrough of using `TextEnvironments`*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-Adapter Training](multi_adapter_rl): *Use a single base model and multiple
    adapters for memory efficient end-to-end training*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blog posts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[![thumbnail](../Images/6a5d903a4cb2b18948ab6f3db912185c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Illustrating Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf)
    [![thumbnail](../Images/22aa00200f429b7d9ce649b675e3f3b1.png)
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft)
    [![thumbnail](../Images/c83a44cfb05cf6785db903a8231e5d59.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)
    [![thumbnail](../Images/aca25811c595ffdf3eabce54f97a87d5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) [![thumbnail](../Images/3aa727b1aa5caba1090533892ec9e378.png)
  prefs: []
  type: TYPE_NORMAL
- en: Finetune Stable Diffusion Models with DDPO via TRL](https://huggingface.co/blog/trl-ddpo)
  prefs: []
  type: TYPE_NORMAL
