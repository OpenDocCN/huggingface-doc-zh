- en: The inference API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/inference](https://huggingface.co/docs/accelerate/package_reference/inference)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: These docs refer to the [PiPPy](https://github.com/PyTorch/PiPPy) integration.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `accelerate.prepare_pippy`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/inference.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`model` (`torch.nn.Module`) — A model we want to split for pipeline-parallel
    inference'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split_points` (`str` or `List[str]`, defaults to ‘auto’) — How to generate
    the split points and chunk the model across each GPU. ‘auto’ will find the best
    balanced split given any model. Should be a list of layer names in the model to
    split by otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_split_module_classes` (`List[str]`) — A list of class names for layers
    we don’t want to be split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`example_args` (tuple of model inputs) — The expected inputs for the model
    that uses order-based inputs. Recommended to use this method if possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`example_kwargs` (dict of model inputs) — The expected inputs for the model
    that uses dictionary-based inputs. This is a *highly* limiting structure that
    requires the same keys be present at *all* inference calls. Not recommended unless
    the prior condition is true for all cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_chunks` (`int`, defaults to the number of available GPUs) — The number
    of different stages the Pipeline will have. By default it will assign one chunk
    per GPU, but this can be tuned and played with. In general one should have num_chunks
    >= num_gpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gather_output` (`bool`, defaults to `False`) — If `True`, the output from
    the last GPU (which holds the true outputs) is sent across to all GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wraps `model` for pipeline parallel inference.
  prefs: []
  type: TYPE_NORMAL
