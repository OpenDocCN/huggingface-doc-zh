- en: The inference API
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理API
- en: 'Original text: [https://huggingface.co/docs/accelerate/package_reference/inference](https://huggingface.co/docs/accelerate/package_reference/inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/package_reference/inference](https://huggingface.co/docs/accelerate/package_reference/inference)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: These docs refer to the [PiPPy](https://github.com/PyTorch/PiPPy) integration.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文档涉及[PiPPy](https://github.com/PyTorch/PiPPy)集成。
- en: '#### `accelerate.prepare_pippy`'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '`accelerate.prepare_pippy`'
- en: '[< source >](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/inference.py#L111)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/accelerate/blob/v0.27.2/src/accelerate/inference.py#L111)'
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`model` (`torch.nn.Module`) — A model we want to split for pipeline-parallel
    inference'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`（`torch.nn.Module`）- 我们要为管道并行推理拆分的模型'
- en: '`split_points` (`str` or `List[str]`, defaults to ‘auto’) — How to generate
    the split points and chunk the model across each GPU. ‘auto’ will find the best
    balanced split given any model. Should be a list of layer names in the model to
    split by otherwise.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_points`（`str`或`List[str]`，默认为''auto''）- 如何生成分割点并在每个GPU上分块模型。''auto''将根据任何模型找到最佳平衡的分割。否则应该是要按照分割的模型中的层名称列表。'
- en: '`no_split_module_classes` (`List[str]`) — A list of class names for layers
    we don’t want to be split.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_split_module_classes`（`List[str]`）- 我们不希望拆分的层的类名列表。'
- en: '`example_args` (tuple of model inputs) — The expected inputs for the model
    that uses order-based inputs. Recommended to use this method if possible.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`example_args`（模型输入的元组）- 该模型预期使用基于顺序的输入。如果可能的话，建议使用这种方法。'
- en: '`example_kwargs` (dict of model inputs) — The expected inputs for the model
    that uses dictionary-based inputs. This is a *highly* limiting structure that
    requires the same keys be present at *all* inference calls. Not recommended unless
    the prior condition is true for all cases.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`example_kwargs`（模型输入的字典）- 该模型预期使用基于字典的输入。这是一个*非常*限制性的结构，需要在*所有*推理调用中存在相同的键。除非先前条件对所有情况都成立，否则不建议使用。'
- en: '`num_chunks` (`int`, defaults to the number of available GPUs) — The number
    of different stages the Pipeline will have. By default it will assign one chunk
    per GPU, but this can be tuned and played with. In general one should have num_chunks
    >= num_gpus.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_chunks`（`int`，默认为可用GPU的数量）- 管道将具有的不同阶段的数量。默认情况下，它将为每个GPU分配一个块，但可以进行调整和调试。一般来说，应该有num_chunks
    >= num_gpus。'
- en: '`gather_output` (`bool`, defaults to `False`) — If `True`, the output from
    the last GPU (which holds the true outputs) is sent across to all GPUs.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gather_output`（`bool`，默认为`False`）- 如果为`True`，则来自最后一个GPU（保存真实输出的GPU）的输出将发送到所有GPU。'
- en: Wraps `model` for pipeline parallel inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为管道并行推理包装`model`。
