# LeViT

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/levit](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/levit)

## æ¦‚è¿°

LeViTæ¨¡å‹æ˜¯ç”±Ben Grahamï¼ŒAlaaeldin El-Noubyï¼ŒHugo Touvronï¼ŒPierre Stockï¼ŒArmand Joulinï¼ŒHervÃ© JÃ©gouï¼ŒMatthijs Douzeåœ¨[LeViT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2104.01136)ä¸­æå‡ºçš„ã€‚LeViTé€šè¿‡ä¸€äº›æ¶æ„ä¸Šçš„å·®å¼‚æ¥æé«˜[è§†è§‰Transformerï¼ˆViTï¼‰](vit)çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä¾‹å¦‚åœ¨Transformerä¸­ä½¿ç”¨åˆ†è¾¨ç‡é€’å‡çš„æ¿€æ´»å›¾ä»¥åŠå¼•å…¥æ³¨æ„åç½®ä»¥æ•´åˆä½ç½®ä¿¡æ¯ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬è®¾è®¡äº†ä¸€ç³»åˆ—å›¾åƒåˆ†ç±»æ¶æ„ï¼Œä¼˜åŒ–äº†åœ¨é«˜é€Ÿç¯å¢ƒä¸­å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„å·¥ä½œåˆ©ç”¨äº†æœ€è¿‘åœ¨åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ä¸­çš„å‘ç°ï¼Œè¿™äº›æ¶æ„åœ¨é«˜åº¦å¹¶è¡Œå¤„ç†ç¡¬ä»¶ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†äº†å·ç§¯ç¥ç»ç½‘ç»œçš„å¹¿æ³›æ–‡çŒ®ä¸­çš„åŸåˆ™ï¼Œå°†å®ƒä»¬åº”ç”¨äºTransformerï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨åˆ†è¾¨ç‡é€’å‡çš„æ¿€æ´»å›¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ³¨æ„åç½®ï¼Œä¸€ç§å°†ä½ç½®ä¿¡æ¯æ•´åˆåˆ°è§†è§‰Transformerä¸­çš„æ–°æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LeVITï¼šä¸€ä¸ªç”¨äºå¿«é€Ÿæ¨ç†å›¾åƒåˆ†ç±»çš„æ··åˆç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬è€ƒè™‘åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šçš„ä¸åŒæ•ˆç‡æªæ–½ï¼Œä»¥æœ€å¥½åœ°åæ˜ å„ç§åº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æŠ€æœ¯é€‰æ‹©ï¼Œå¹¶è¡¨æ˜å®ƒä»¬é€‚ç”¨äºå¤§å¤šæ•°æ¶æ„ã€‚æ€»ä½“è€Œè¨€ï¼ŒLeViTåœ¨é€Ÿåº¦/å‡†ç¡®æ€§æƒè¡¡æ–¹é¢æ˜æ˜¾ä¼˜äºç°æœ‰çš„å·ç§¯ç½‘ç»œå’Œè§†è§‰Transformerã€‚ä¾‹å¦‚ï¼Œåœ¨80%çš„ImageNet top-1å‡†ç¡®ç‡ä¸‹ï¼ŒLeViTæ¯”EfficientNetåœ¨CPUä¸Šå¿«5å€ã€‚*

![drawing](../Images/79a4bb02ada009fb7b657999872ea7e3.png) LeViTæ¶æ„ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡](https://arxiv.org/abs/2104.01136)ã€‚

è¿™ä¸ªæ¨¡å‹ç”±[anugunj](https://huggingface.co/anugunj)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/facebookresearch/LeViT)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   ä¸ViTç›¸æ¯”ï¼ŒLeViTæ¨¡å‹ä½¿ç”¨é¢å¤–çš„è’¸é¦å¤´æ¥æœ‰æ•ˆåœ°ä»æ•™å¸ˆï¼ˆåœ¨LeViTè®ºæ–‡ä¸­æ˜¯ç±»ä¼¼ResNetçš„æ¨¡å‹ï¼‰ä¸­å­¦ä¹ ã€‚è’¸é¦å¤´é€šè¿‡åœ¨ç±»ä¼¼ResNetçš„æ¨¡å‹çš„ç›‘ç£ä¸‹è¿›è¡Œåå‘ä¼ æ’­æ¥å­¦ä¹ ã€‚ä»–ä»¬è¿˜ä»å·ç§¯ç¥ç»ç½‘ç»œä¸­æ±²å–çµæ„Ÿï¼Œä½¿ç”¨åˆ†è¾¨ç‡é€’å‡çš„æ¿€æ´»å›¾æ¥æé«˜æ•ˆç‡ã€‚

+   å¯¹äºç²¾ç‚¼æ¨¡å‹ï¼Œæœ‰ä¸¤ç§å¾®è°ƒæ–¹æ³•ï¼Œè¦ä¹ˆï¼ˆ1ï¼‰ä»¥ç»å…¸æ–¹å¼ï¼Œåªåœ¨æœ€ç»ˆéšè—çŠ¶æ€çš„é¡¶éƒ¨æ”¾ç½®ä¸€ä¸ªé¢„æµ‹å¤´ï¼Œä¸ä½¿ç”¨è’¸é¦å¤´ï¼Œè¦ä¹ˆï¼ˆ2ï¼‰åœ¨æœ€ç»ˆéšè—çŠ¶æ€çš„é¡¶éƒ¨æ”¾ç½®é¢„æµ‹å¤´å’Œè’¸é¦å¤´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„æµ‹å¤´ä½¿ç”¨æ­£å¸¸çš„äº¤å‰ç†µè®­ç»ƒï¼Œé¢„æµ‹å¤´å’Œåœ°é¢çœŸå®æ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µï¼Œè€Œè’¸é¦é¢„æµ‹å¤´ä½¿ç”¨ç¡¬è’¸é¦ï¼ˆè’¸é¦å¤´çš„é¢„æµ‹å’Œæ•™å¸ˆé¢„æµ‹çš„æ ‡ç­¾ä¹‹é—´çš„äº¤å‰ç†µï¼‰ã€‚åœ¨æ¨ç†æ—¶ï¼Œå°†ä¸¤ä¸ªå¤´ä¹‹é—´çš„å¹³å‡é¢„æµ‹ä½œä¸ºæœ€ç»ˆé¢„æµ‹ã€‚ï¼ˆ2ï¼‰ä¹Ÿè¢«ç§°ä¸ºâ€œè’¸é¦å¾®è°ƒâ€ï¼Œå› ä¸ºä¾èµ–äºå·²ç»åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒçš„æ•™å¸ˆã€‚åœ¨æ¨¡å‹æ–¹é¢ï¼Œï¼ˆ1ï¼‰å¯¹åº”äº[LevitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassification)ï¼Œï¼ˆ2ï¼‰å¯¹åº”äº[LevitForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher)ã€‚

+   æ‰€æœ‰å‘å¸ƒçš„æ£€æŸ¥ç‚¹éƒ½æ˜¯åœ¨ [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒå’Œå¾®è°ƒï¼ˆä¹Ÿç§°ä¸º ILSVRC 2012ï¼ŒåŒ…å« 130 ä¸‡å¼ å›¾åƒå’Œ 1,000 ä¸ªç±»åˆ«ï¼‰ã€‚æ²¡æœ‰ä½¿ç”¨å¤–éƒ¨æ•°æ®ã€‚è¿™ä¸åŸå§‹çš„ ViT æ¨¡å‹ç›¸åï¼Œåè€…åœ¨é¢„è®­ç»ƒä¸­ä½¿ç”¨äº†å¤–éƒ¨æ•°æ®ï¼Œå¦‚ JFT-300M æ•°æ®é›†/Imagenet-21kã€‚

+   LeViT çš„ä½œè€…å‘å¸ƒäº† 5 ä¸ªç»è¿‡è®­ç»ƒçš„ LeViT æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ç›´æ¥æ’å…¥ [LevitModel](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitModel) æˆ– [LevitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassification)ã€‚ä¸ºäº†æ¨¡æ‹Ÿåœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆä»…ä½¿ç”¨ ImageNet-1k è¿›è¡Œé¢„è®­ç»ƒï¼‰ï¼Œä½¿ç”¨äº†æ•°æ®å¢å¼ºã€ä¼˜åŒ–å’Œæ­£åˆ™åŒ–ç­‰æŠ€æœ¯ã€‚å¯ç”¨çš„ 5 ä¸ªå˜ä½“æ˜¯ï¼ˆéƒ½åœ¨å¤§å°ä¸º 224x224 çš„å›¾åƒä¸Šè®­ç»ƒï¼‰ï¼š*facebook/levit-128S*ã€*facebook/levit-128*ã€*facebook/levit-192*ã€*facebook/levit-256* å’Œ *facebook/levit-384*ã€‚è¯·æ³¨æ„ï¼Œåº”è¯¥ä½¿ç”¨ [LevitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitImageProcessor) æ¥å‡†å¤‡æ¨¡å‹çš„å›¾åƒã€‚

+   [LevitForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher) ç›®å‰ä»…æ”¯æŒæ¨ç†ï¼Œä¸æ”¯æŒè®­ç»ƒæˆ–å¾®è°ƒã€‚

+   æ‚¨å¯ä»¥æŸ¥çœ‹å…³äºæ¨ç†ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬ [è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)ï¼ˆæ‚¨åªéœ€å°† [ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor) æ›¿æ¢ä¸º [LevitImageProcessor](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitImageProcessor)ï¼Œå¹¶å°† [ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification) æ›¿æ¢ä¸º [LevitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassification) æˆ– [LevitForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher)ï¼‰ã€‚

## èµ„æº

ä¸€ä¸ªå®˜æ–¹ Hugging Face å’Œç¤¾åŒºï¼ˆç”± ğŸŒ è¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ LeViTã€‚

å›¾åƒåˆ†ç±»

+   [LevitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassification) å—åˆ°è¿™ä¸ª [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) å’Œ [ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb) çš„æ”¯æŒã€‚

+   å‚è§ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ª Pull Requestï¼Œæˆ‘ä»¬å°†å¯¹å…¶è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°ä¸œè¥¿ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## LevitConfig

### `class transformers.LevitConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/configuration_levit.py#L35)

```py
( image_size = 224 num_channels = 3 kernel_size = 3 stride = 2 padding = 1 patch_size = 16 hidden_sizes = [128, 256, 384] num_attention_heads = [4, 8, 12] depths = [4, 4, 4] key_dim = [16, 16, 16] drop_path_rate = 0 mlp_ratio = [2, 2, 2] attention_ratio = [2, 2, 2] initializer_range = 0.02 **kwargs )
```

å‚æ•°

+   `image_size` (`int`, *optional*, defaults to 224) â€” è¾“å…¥å›¾åƒçš„å¤§å°ã€‚

+   `num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥å›¾åƒä¸­çš„é€šé“æ•°ã€‚

+   `kernel_size` (`int`, *optional*, defaults to 3) â€” è¡¥ä¸åµŒå…¥çš„åˆå§‹å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚

+   `stride` (`int`, *optional*, defaults to 2) â€” è¡¥ä¸åµŒå…¥çš„åˆå§‹å·ç§¯å±‚çš„æ­¥å¹…å¤§å°ã€‚

+   `padding` (`int`, *optional*, defaults to 1) â€” è¡¥ä¸åµŒå…¥çš„åˆå§‹å·ç§¯å±‚çš„å¡«å……å¤§å°ã€‚

+   `patch_size` (`int`, *optional*, defaults to 16) â€” åµŒå…¥çš„è¡¥ä¸å¤§å°ã€‚

+   `hidden_sizes` (`List[int]`, *optional*, defaults to `[128, 256, 384]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„ç»´åº¦ã€‚

+   `num_attention_heads` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[4, 8, 12]`) â€” Transformerç¼–ç å™¨æ¯ä¸ªå—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `depths` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[4, 4, 4]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­çš„å±‚æ•°ã€‚

+   `key_dim` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[16, 16, 16]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­é”®çš„å¤§å°ã€‚

+   `drop_path_rate` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” ç”¨äºéšæœºæ·±åº¦çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œç”¨äºTransformerç¼–ç å™¨å—ä¸­çš„å—ã€‚

+   `mlp_ratios` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[2, 2, 2]`) â€” ç¼–ç å™¨å—ä¸­Mix FFNsçš„éšè—å±‚å¤§å°ä¸è¾“å…¥å±‚å¤§å°çš„æ¯”ç‡ã€‚

+   `attention_ratios` (`List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `[2, 2, 2]`) â€” æ³¨æ„åŠ›å±‚çš„è¾“å‡ºç»´åº¦ä¸è¾“å…¥ç»´åº¦çš„æ¯”ç‡ã€‚

+   `initializer_range` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨[LevitModel](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitModel)çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ªLeViTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºLeViT [facebook/levit-128S](https://huggingface.co/facebook/levit-128S)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import LevitConfig, LevitModel

>>> # Initializing a LeViT levit-128S style configuration
>>> configuration = LevitConfig()

>>> # Initializing a model (with random weights) from the levit-128S style configuration
>>> model = LevitModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## LevitFeatureExtractor

### `class transformers.LevitFeatureExtractor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/feature_extraction_levit.py#L26)

```py
( *args **kwargs )
```

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)

```py
( images **kwargs )
```

é¢„å¤„ç†å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## LevitImageProcessor

### `class transformers.LevitImageProcessor`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/image_processing_levit.py#L45)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = [0.485, 0.456, 0.406] image_std: Union = [0.229, 0.224, 0.225] **kwargs )
```

å‚æ•°

+   `do_resize` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†è¾“å…¥çš„æœ€çŸ­è¾¹è°ƒæ•´ä¸ºint(256/224 *`size`)ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_resize`å‚æ•°è¦†ç›–ã€‚

+   `size` (`Dict[str, int]`, *å¯é€‰*, é»˜è®¤ä¸º `{"shortest_edge" -- 224}`): è°ƒæ•´å¤§å°åçš„è¾“å‡ºå›¾åƒå¤§å°ã€‚å¦‚æœsizeæ˜¯ä¸€ä¸ªå¸¦æœ‰â€œwidthâ€å’Œâ€œheightâ€é”®çš„å­—å…¸ï¼Œåˆ™å›¾åƒå°†è¢«è°ƒæ•´ä¸º`(size["height"], size["width"])`ã€‚å¦‚æœsizeæ˜¯ä¸€ä¸ªå¸¦æœ‰â€œshortest_edgeâ€é”®çš„å­—å…¸ï¼Œåˆ™æœ€çŸ­è¾¹å€¼`c`å°†è¢«é‡æ–°ç¼©æ”¾ä¸º`int(c * (256/224))`ã€‚å›¾åƒçš„è¾ƒå°è¾¹å°†åŒ¹é…åˆ°è¿™ä¸ªå€¼ï¼Œå³å¦‚æœé«˜åº¦>å®½åº¦ï¼Œåˆ™å›¾åƒå°†è¢«é‡æ–°ç¼©æ”¾ä¸º`(size["shortest_egde"] * height / width, size["shortest_egde"])`ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`size`å‚æ•°è¦†ç›–ã€‚

+   `resample` (`PILImageResampling`, *å¯é€‰*, é»˜è®¤ä¸º `Resampling.BICUBIC`) â€” å¦‚æœè°ƒæ•´å›¾åƒå¤§å°ï¼Œåˆ™è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤é•œã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`resample`å‚æ•°è¦†ç›–ã€‚

+   `do_center_crop` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œä¸­å¿ƒè£å‰ªä¸º`(crop_size["height"], crop_size["width"])`ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_center_crop`å‚æ•°è¦†ç›–ã€‚

+   `crop_size` (`Dict`, *å¯é€‰*, é»˜è®¤ä¸º `{"height" -- 224, "width": 224}`): `center_crop`åçš„æœŸæœ›å›¾åƒå¤§å°ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`crop_size`å‚æ•°è¦†ç›–ã€‚

+   `do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ§åˆ¶æ˜¯å¦é€šè¿‡æŒ‡å®šçš„æ¯”ä¾‹`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_rescale`å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor`ï¼ˆ`int`æˆ–`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`1/255`ï¼‰â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ¯”ä¾‹å› å­ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`rescale_factor`å‚æ•°è¦†ç›–ã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ§åˆ¶æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`do_normalize`å‚æ•°è¦†ç›–ã€‚

+   `image_mean`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.485, 0.456, 0.406]`ï¼‰â€” å¦‚æœè¦å½’ä¸€åŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_mean`å‚æ•°è¦†ç›–ã€‚

+   `image_std`ï¼ˆ`List[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`[0.229, 0.224, 0.225]`ï¼‰â€” å¦‚æœè¦å½’ä¸€åŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒé€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢«`preprocess`æ–¹æ³•ä¸­çš„`image_std`å‚æ•°è¦†ç›–ã€‚

æ„å»ºä¸€ä¸ªLeViTå›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/image_processing_levit.py#L173)

```py
( images: Union do_resize: Optional = None size: Optional = None resample: Resampling = None do_center_crop: Optional = None crop_size: Optional = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Optional = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images`ï¼ˆ`ImageInput`ï¼‰â€” è¦é¢„å¤„ç†çš„å›¾åƒæˆ–å›¾åƒæ‰¹å¤„ç†ã€‚æœŸæœ›å•ä¸ªæˆ–æ‰¹å¤„ç†çš„å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä»0åˆ°255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨0åˆ°1ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½®`do_rescale=False`ã€‚

+   `do_resize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.do_resize`ï¼‰â€” æ˜¯å¦è°ƒæ•´å›¾åƒå¤§å°ã€‚

+   `size`ï¼ˆ`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.size`ï¼‰â€” è°ƒæ•´å¤§å°åçš„è¾“å‡ºå›¾åƒå¤§å°ã€‚å¦‚æœå¤§å°æ˜¯ä¸€ä¸ªå¸¦æœ‰â€œå®½åº¦â€å’Œâ€œé«˜åº¦â€é”®çš„å­—å…¸ï¼Œåˆ™å›¾åƒå°†è¢«è°ƒæ•´ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚å¦‚æœå¤§å°æ˜¯ä¸€ä¸ªå¸¦æœ‰â€œæœ€çŸ­è¾¹â€é”®çš„å­—å…¸ï¼Œåˆ™æœ€çŸ­è¾¹å€¼`c`å°†è¢«é‡æ–°ç¼©æ”¾ä¸ºintï¼ˆ`c` *ï¼ˆ256/224ï¼‰ï¼‰ã€‚å›¾åƒçš„è¾ƒå°è¾¹å°†ä¸æ­¤å€¼åŒ¹é…ï¼Œå³ï¼Œå¦‚æœé«˜åº¦>å®½åº¦ï¼Œåˆ™å›¾åƒå°†è¢«é‡æ–°ç¼©æ”¾ä¸ºï¼ˆå¤§å°*é«˜åº¦/å®½åº¦ï¼Œå¤§å°ï¼‰ã€‚

+   `resample`ï¼ˆ`PILImageResampling`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`PILImageResampling.BICUBIC`ï¼‰â€” è°ƒæ•´å›¾åƒå¤§å°æ—¶è¦ä½¿ç”¨çš„é‡é‡‡æ ·æ»¤æ³¢å™¨ã€‚

+   `do_center_crop`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.do_center_crop`ï¼‰â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œä¸­å¿ƒè£å‰ªã€‚

+   `crop_size`ï¼ˆ`Dict[str, int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.crop_size`ï¼‰â€” ä¸­å¿ƒè£å‰ªåçš„è¾“å‡ºå›¾åƒå¤§å°ã€‚å°†å›¾åƒè£å‰ªä¸ºï¼ˆcrop_size[â€œheightâ€]ï¼Œcrop_size[â€œwidthâ€]ï¼‰ã€‚

+   `do_rescale`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.do_rescale`ï¼‰â€” æ˜¯å¦é€šè¿‡æŒ‡å®šçš„æ¯”ä¾‹`rescale_factor`é‡æ–°ç¼©æ”¾å›¾åƒåƒç´ å€¼-é€šå¸¸ä¸º0åˆ°1ä¹‹é—´çš„å€¼ã€‚

+   `rescale_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.rescale_factor`ï¼‰â€” ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒåƒç´ å€¼çš„å› å­ã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.do_normalize`ï¼‰â€” æ˜¯å¦é€šè¿‡`image_mean`å’Œ`image_std`å¯¹å›¾åƒåƒç´ å€¼è¿›è¡Œå½’ä¸€åŒ–ã€‚

+   `image_mean`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.image_mean`ï¼‰â€” ç”¨äºå½’ä¸€åŒ–å›¾åƒåƒç´ å€¼çš„å‡å€¼ã€‚

+   `image_std`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.image_std`ï¼‰â€” ç”¨äºå½’ä¸€åŒ–å›¾åƒåƒç´ å€¼çš„æ ‡å‡†å·®ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–`TensorType`ï¼Œ*å¯é€‰*ï¼‰â€” è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   å–æ¶ˆè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW`æˆ–`'tf'`ï¼šè¿”å›ç±»å‹ä¸º`tf.Tensor`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.PYTORCH`æˆ–`'pt'`ï¼šè¿”å›ç±»å‹ä¸º`torch.Tensor`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.NUMPY`æˆ–`'np'`ï¼šè¿”å›ç±»å‹ä¸º`np.ndarray`çš„æ‰¹å¤„ç†ã€‚

    +   `TensorType.JAX`æˆ–`'jax'`ï¼šè¿”å›ç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹å¤„ç†ã€‚

+   `data_format`ï¼ˆ`str`æˆ–`ChannelDimension`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`ChannelDimension.FIRST`ï¼‰â€” è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥(num_channels, height, width)æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥(height, width, num_channels)æ ¼å¼ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥(num_channels, height, width)æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥(height, width, num_channels)æ ¼å¼ã€‚

    +   `"none"`æˆ–`ChannelDimension.NONE`ï¼šå›¾åƒä»¥(height, width)æ ¼å¼ã€‚

å¯¹å›¾åƒæˆ–å›¾åƒæ‰¹æ¬¡è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ç”¨ä½œLeViTæ¨¡å‹çš„è¾“å…¥ã€‚

## LevitModel

### `class transformers.LevitModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L535)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„Levitæ¨¡å‹è¾“å‡ºåŸå§‹ç‰¹å¾ï¼Œæ²¡æœ‰ä»»ä½•ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L548)

```py
( pixel_values: FloatTensor = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LevitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `pooler_output`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, hidden_size)`çš„`torch.FloatTensor`ï¼‰â€” åœ¨ç©ºé—´ç»´åº¦ä¸Šè¿›è¡Œæ± åŒ–æ“ä½œåçš„æœ€åä¸€å±‚éšè—çŠ¶æ€ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡ºå’Œæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

[LevitModel](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitModel)å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, LevitModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/levit-128S")
>>> model = LevitModel.from_pretrained("facebook/levit-128S")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 16, 384]
```

## LevitForImageClassification

### `class transformers.LevitForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L592)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Levitæ¨¡å‹ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨æ± åŒ–ç‰¹å¾çš„é¡¶éƒ¨æœ‰ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L616)

```py
( pixel_values: FloatTensor = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰ â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LevitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œè¿™å–å†³äºé…ç½®ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

[LevitForImageClassification](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è°ƒç”¨æ­¤å‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, LevitForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/levit-128S")
>>> model = LevitForImageClassification.from_pretrained("facebook/levit-128S")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

## LevitForImageClassificationWithTeacher

### `class transformers.LevitForImageClassificationWithTeacher`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L677)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

LeViTæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆæœ€ç»ˆéšè—çŠ¶æ€é¡¶éƒ¨çš„çº¿æ€§å±‚å’Œè’¸é¦ä»¤ç‰Œæœ€ç»ˆéšè—çŠ¶æ€é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚.. è­¦å‘Š:: æ­¤æ¨¡å‹ä»…æ”¯æŒæ¨æ–­ã€‚å°šä¸æ”¯æŒä½¿ç”¨è’¸é¦ï¼ˆå³ä½¿ç”¨æ•™å¸ˆï¼‰è¿›è¡Œå¾®è°ƒã€‚

æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/levit/modeling_levit.py#L708)

```py
( pixel_values: FloatTensor = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.levit.modeling_levit.LevitForImageClassificationWithTeacherOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LevitImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.models.levit.modeling_levit.LevitForImageClassificationWithTeacherOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.levit.modeling_levit.LevitForImageClassificationWithTeacherOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[LevitConfig](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰â€” é¢„æµ‹åˆ†æ•°ï¼Œä½œä¸º`cls_logits`å’Œ`distillation_logits`çš„å¹³å‡å€¼ã€‚

+   `cls_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰â€” åˆ†ç±»å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆå³ç±»ä»¤ç‰Œæœ€ç»ˆéšè—çŠ¶æ€é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ã€‚

+   `distillation_logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰â€” è’¸é¦å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆå³è’¸é¦ä»¤ç‰Œæœ€ç»ˆéšè—çŠ¶æ€é¡¶éƒ¨çš„çº¿æ€§å±‚ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºçš„éšè—çŠ¶æ€ã€‚

[LevitForImageClassificationWithTeacher](/docs/transformers/v4.37.2/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, LevitForImageClassificationWithTeacher
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/levit-128S")
>>> model = LevitForImageClassificationWithTeacher.from_pretrained("facebook/levit-128S")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```
