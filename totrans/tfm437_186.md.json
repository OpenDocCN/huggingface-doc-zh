["```py\n( vocab_size = 50265 max_encoder_position_embeddings = 16384 max_decoder_position_embeddings = 1024 encoder_layers = 12 encoder_ffn_dim = 4096 encoder_attention_heads = 16 decoder_layers = 12 decoder_ffn_dim = 4096 decoder_attention_heads = 16 encoder_layerdrop = 0.0 decoder_layerdrop = 0.0 use_cache = True is_encoder_decoder = True activation_function = 'gelu' d_model = 1024 dropout = 0.1 attention_dropout = 0.0 activation_dropout = 0.0 init_std = 0.02 decoder_start_token_id = 2 classifier_dropout = 0.0 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 attention_window: Union = 512 **kwargs )\n```", "```py\n>>> from transformers import LEDModel, LEDConfig\n\n>>> # Initializing a LED allenai/led-base-16384 style configuration\n>>> configuration = LEDConfig()\n\n>>> # Initializing a model from the allenai/led-base-16384 style configuration\n>>> model = LEDModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file merges_file errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False **kwargs )\n```", "```py\n>>> from transformers import LEDTokenizer\n\n>>> tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( save_directory: str filename_prefix: Optional = None )\n```", "```py\n( vocab_file = None merges_file = None tokenizer_file = None errors = 'replace' bos_token = '<s>' eos_token = '</s>' sep_token = '</s>' cls_token = '<s>' unk_token = '<unk>' pad_token = '<pad>' mask_token = '<mask>' add_prefix_space = False trim_offsets = True **kwargs )\n```", "```py\n>>> from transformers import LEDTokenizerFast\n\n>>> tokenizer = LEDTokenizerFast.from_pretrained(\"allenai/led-base-16384\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( last_hidden_state: FloatTensor hidden_states: Optional = None attentions: Optional = None global_attentions: Optional = None )\n```", "```py\n( last_hidden_state: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None encoder_global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None encoder_global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None logits: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None encoder_global_attentions: Optional = None )\n```", "```py\n( loss: Optional = None start_logits: FloatTensor = None end_logits: FloatTensor = None past_key_values: Optional = None decoder_hidden_states: Optional = None decoder_attentions: Optional = None cross_attentions: Optional = None encoder_last_hidden_state: Optional = None encoder_hidden_states: Optional = None encoder_attentions: Optional = None encoder_global_attentions: Optional = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( last_hidden_state: tf.Tensor = None past_key_values: List[tf.Tensor] | None = None decoder_hidden_states: Tuple[tf.Tensor] | None = None decoder_attentions: Tuple[tf.Tensor] | None = None cross_attentions: Tuple[tf.Tensor] | None = None encoder_last_hidden_state: tf.Tensor | None = None encoder_hidden_states: Tuple[tf.Tensor] | None = None encoder_attentions: Tuple[tf.Tensor] | None = None encoder_global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( loss: tf.Tensor | None = None logits: tf.Tensor = None past_key_values: List[tf.Tensor] | None = None decoder_hidden_states: Tuple[tf.Tensor] | None = None decoder_attentions: Tuple[tf.Tensor] | None = None cross_attentions: Tuple[tf.Tensor] | None = None encoder_last_hidden_state: tf.Tensor | None = None encoder_hidden_states: Tuple[tf.Tensor] | None = None encoder_attentions: Tuple[tf.Tensor] | None = None encoder_global_attentions: Tuple[tf.Tensor] | None = None )\n```", "```py\n( config: LEDConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None global_attention_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LEDModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDModel.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: LEDConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None global_attention_mask: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LEDForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n\n>>> prediction = model.generate(input_ids)[0]\n>>> print(tokenizer.decode(prediction, skip_special_tokens=True))\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForConditionalGeneration\n\n>>> model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n\n>>> ARTICLE_TO_SUMMARIZE = '''Transformers (Vaswani et al., 2017) have achieved state-of-the-art\n...     results in a wide range of natural language tasks including generative language modeling\n...     (Dai et al., 2019; Radford et al., 2019) and discriminative ... language understanding (Devlin et al., 2019).\n...     This success is partly due to the self-attention component which enables the network to capture contextual\n...     information from the entire sequence. While powerful, the memory and computational requirements of\n...     self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to\n...     process long sequences. To address this limitation, we present Longformer, a modified Transformer\n...     architecture with a self-attention operation that scales linearly with the sequence length, making it\n...     versatile for processing long documents (Fig 1). This is an advantage for natural language tasks such as\n...     long document classification, question answering (QA), and coreference resolution, where existing approaches\n...     partition or shorten the long context into smaller sequences that fall within the typical 512 token limit\n...     of BERT-style pretrained models. Such partitioning could potentially result in loss of important\n...     cross-partition information, and to mitigate this problem, existing methods often rely on complex\n...     architectures to address such interactions. On the other hand, our proposed Longformer is able to build\n...     contextual representations of the entire context using multiple layers of attention, reducing the need for\n...     task-specific architectures.'''\n>>> inputs = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\")\n\n>>> # Global attention on the first token (cf. Beltagy et al. 2020)\n>>> global_attention_mask = torch.zeros_like(inputs)\n>>> global_attention_mask[:, 0] = 1\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask, num_beams=3, max_length=32)\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n```", "```py\n( config: LEDConfig **kwargs )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None global_attention_mask: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LEDForSequenceClassification.from_pretrained(\n...     \"allenai/led-base-16384\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None global_attention_mask: Optional = None start_positions: Optional = None end_positions: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, LEDForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForQuestionAnswering.from_pretrained(\"allenai/led-base-16384\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None head_mask: tf.Tensor | None = None decoder_head_mask: tf.Tensor | None = None encoder_outputs: tf.Tensor | None = None global_attention_mask: tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None inputs_embeds: tf.Tensor | None = None decoder_inputs_embeds: tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False **kwargs ) \u2192 export const metadata = 'undefined';transformers.models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLEDModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = TFLEDModel.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None decoder_input_ids: np.ndarray | tf.Tensor | None = None decoder_attention_mask: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None decoder_head_mask: np.ndarray | tf.Tensor | None = None encoder_outputs: TFLEDEncoderBaseModelOutput | None = None global_attention_mask: np.ndarray | tf.Tensor | None = None past_key_values: Tuple[Tuple[Union[np.ndarray, tf.Tensor]]] | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: tf.Tensor | None = None training: bool = False ) \u2192 export const metadata = 'undefined';transformers.models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput or tuple(tf.Tensor)\n```", "```py\n>>> from transformers import AutoTokenizer, TFLEDForConditionalGeneration\n>>> import tensorflow as tf\n\n>>> mname = \"allenai/led-base-16384\"\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n>>> model = TFLEDForConditionalGeneration.from_pretrained(mname)\n>>> batch = tokenizer([TXT], return_tensors=\"tf\")\n>>> logits = model(inputs=batch.input_ids).logits\n>>> probs = tf.nn.softmax(logits[0])\n>>> # probs[5] is associated with the mask token\n```"]