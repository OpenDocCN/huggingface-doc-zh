["```py\n>>> from transformers import LEDModel, LEDConfig\n\n>>> # Initializing a LED allenai/led-base-16384 style configuration\n>>> configuration = LEDConfig()\n\n>>> # Initializing a model from the allenai/led-base-16384 style configuration\n>>> model = LEDModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import LEDTokenizer\n\n>>> tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import LEDTokenizerFast\n\n>>> tokenizer = LEDTokenizerFast.from_pretrained(\"allenai/led-base-16384\")\n>>> tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n>>> tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```", "```py\n>>> from transformers import AutoTokenizer, LEDModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDModel.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, LEDForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n\n>>> prediction = model.generate(input_ids)[0]\n>>> print(tokenizer.decode(prediction, skip_special_tokens=True))\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForConditionalGeneration\n\n>>> model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n\n>>> ARTICLE_TO_SUMMARIZE = '''Transformers (Vaswani et al., 2017) have achieved state-of-the-art\n...     results in a wide range of natural language tasks including generative language modeling\n...     (Dai et al., 2019; Radford et al., 2019) and discriminative ... language understanding (Devlin et al., 2019).\n...     This success is partly due to the self-attention component which enables the network to capture contextual\n...     information from the entire sequence. While powerful, the memory and computational requirements of\n...     self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to\n...     process long sequences. To address this limitation, we present Longformer, a modified Transformer\n...     architecture with a self-attention operation that scales linearly with the sequence length, making it\n...     versatile for processing long documents (Fig 1). This is an advantage for natural language tasks such as\n...     long document classification, question answering (QA), and coreference resolution, where existing approaches\n...     partition or shorten the long context into smaller sequences that fall within the typical 512 token limit\n...     of BERT-style pretrained models. Such partitioning could potentially result in loss of important\n...     cross-partition information, and to mitigate this problem, existing methods often rely on complex\n...     architectures to address such interactions. On the other hand, our proposed Longformer is able to build\n...     contextual representations of the entire context using multiple layers of attention, reducing the need for\n...     task-specific architectures.'''\n>>> inputs = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\")\n\n>>> # Global attention on the first token (cf. Beltagy et al. 2020)\n>>> global_attention_mask = torch.zeros_like(inputs)\n>>> global_attention_mask[:, 0] = 1\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask, num_beams=3, max_length=32)\n>>> print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_id = logits.argmax().item()\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\", num_labels=num_labels)\n\n>>> labels = torch.tensor([1])\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, LEDForSequenceClassification\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForSequenceClassification.from_pretrained(\"allenai/led-base-16384\", problem_type=\"multi_label_classification\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n\n>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n>>> num_labels = len(model.config.id2label)\n>>> model = LEDForSequenceClassification.from_pretrained(\n...     \"allenai/led-base-16384\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n... )\n\n>>> labels = torch.sum(\n...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n... ).to(torch.float)\n>>> loss = model(**inputs, labels=labels).loss\n```", "```py\n>>> from transformers import AutoTokenizer, LEDForQuestionAnswering\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = LEDForQuestionAnswering.from_pretrained(\"allenai/led-base-16384\")\n\n>>> question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\n>>> inputs = tokenizer(question, text, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> answer_start_index = outputs.start_logits.argmax()\n>>> answer_end_index = outputs.end_logits.argmax()\n\n>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n\n>>> # target is \"nice puppet\"\n>>> target_start_index = torch.tensor([14])\n>>> target_end_index = torch.tensor([15])\n\n>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n>>> loss = outputs.loss\n```", "```py\n>>> from transformers import AutoTokenizer, TFLEDModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n>>> model = TFLEDModel.from_pretrained(\"allenai/led-base-16384\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFLEDForConditionalGeneration\n>>> import tensorflow as tf\n\n>>> mname = \"allenai/led-base-16384\"\n>>> tokenizer = AutoTokenizer.from_pretrained(mname)\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n>>> model = TFLEDForConditionalGeneration.from_pretrained(mname)\n>>> batch = tokenizer([TXT], return_tensors=\"tf\")\n>>> logits = model(inputs=batch.input_ids).logits\n>>> probs = tf.nn.softmax(logits[0])\n>>> # probs[5] is associated with the mask token\n```"]