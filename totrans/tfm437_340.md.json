["```py\n>>> from transformers import CLIPSegConfig, CLIPSegModel\n\n>>> # Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegConfig()\n\n>>> # Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig\n\n>>> # Initializing a CLIPSegText and CLIPSegVision configuration\n>>> config_text = CLIPSegTextConfig()\n>>> config_vision = CLIPSegVisionConfig()\n\n>>> config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n>>> from transformers import CLIPSegTextConfig, CLIPSegTextModel\n\n>>> # Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegTextConfig()\n\n>>> # Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import CLIPSegVisionConfig, CLIPSegVisionModel\n\n>>> # Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegVisionConfig()\n\n>>> # Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPSegModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPSegTextModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegTextModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegVisionModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n>>> from transformers import AutoProcessor, CLIPSegForImageSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"a cat\", \"a remote\", \"a blanket\"]\n>>> inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> print(logits.shape)\ntorch.Size([3, 352, 352])\n```"]