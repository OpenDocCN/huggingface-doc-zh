["```py\n( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 extract_layers = [3, 6, 9] reduce_dim = 64 decoder_num_attention_heads = 4 decoder_attention_dropout = 0.0 decoder_hidden_act = 'quick_gelu' decoder_intermediate_size = 2048 conditional_layer = 0 use_complex_transposed_convolution = False **kwargs )\n```", "```py\n>>> from transformers import CLIPSegConfig, CLIPSegModel\n\n>>> # Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegConfig()\n\n>>> # Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n\n>>> # We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig\n\n>>> # Initializing a CLIPSegText and CLIPSegVision configuration\n>>> config_text = CLIPSegTextConfig()\n>>> config_vision = CLIPSegVisionConfig()\n\n>>> config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)\n```", "```py\n( text_config: CLIPSegTextConfig vision_config: CLIPSegVisionConfig **kwargs ) \u2192 export const metadata = 'undefined';CLIPSegConfig\n```", "```py\n( vocab_size = 49408 hidden_size = 512 intermediate_size = 2048 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 77 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 pad_token_id = 1 bos_token_id = 49406 eos_token_id = 49407 **kwargs )\n```", "```py\n>>> from transformers import CLIPSegTextConfig, CLIPSegTextModel\n\n>>> # Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegTextConfig()\n\n>>> # Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegTextModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 num_channels = 3 image_size = 224 patch_size = 32 hidden_act = 'quick_gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 0.02 initializer_factor = 1.0 **kwargs )\n```", "```py\n>>> from transformers import CLIPSegVisionConfig, CLIPSegVisionModel\n\n>>> # Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration\n>>> configuration = CLIPSegVisionConfig()\n\n>>> # Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration\n>>> model = CLIPSegVisionModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: CLIPSegConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clipseg.modeling_clipseg.CLIPSegOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(\n...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n... )\n\n>>> outputs = model(**inputs)\n>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPSegModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n>>> text_features = model.get_text_features(**inputs)\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> image_features = model.get_image_features(**inputs)\n```", "```py\n( config: CLIPSegTextConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, CLIPSegTextModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegTextModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n```", "```py\n( config: CLIPSegVisionConfig )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import AutoProcessor, CLIPSegVisionModel\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegVisionModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> inputs = processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n>>> last_hidden_state = outputs.last_hidden_state\n>>> pooled_output = outputs.pooler_output  # pooled CLS states\n```", "```py\n( config: CLIPSegConfig )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None conditional_pixel_values: Optional = None conditional_embeddings: Optional = None attention_mask: Optional = None position_ids: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, CLIPSegForImageSegmentation\n>>> from PIL import Image\n>>> import requests\n\n>>> processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n>>> model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"a cat\", \"a remote\", \"a blanket\"]\n>>> inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> print(logits.shape)\ntorch.Size([3, 352, 352])\n```"]