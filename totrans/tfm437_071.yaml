- en: Fully Sharded Data Parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全分片数据并行
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/fsdp](https://huggingface.co/docs/transformers/v4.37.2/en/fsdp)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/fsdp](https://huggingface.co/docs/transformers/v4.37.2/en/fsdp)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
    is a data parallel method that shards a model’s parameters, gradients and optimizer
    states across the number of available GPUs (also called workers or *rank*). Unlike
    [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),
    FSDP reduces memory-usage because a model is replicated on each GPU. This improves
    GPU memory-efficiency and allows you to train much larger models on fewer GPUs.
    FSDP is integrated with the Accelerate, a library for easily managing training
    in distributed environments, which means it is available for use from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[完全分片数据并行（FSDP）](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)是一种数据并行方法，它将模型的参数、梯度和优化器状态分片到可用GPU（也称为工作器或*rank*）的数量上。与[分布式数据并行（DDP）](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)不同，FSDP减少了内存使用，因为模型在每个GPU上都有副本。这提高了GPU内存效率，并允许您在较少的GPU上训练更大的模型。FSDP与Accelerate集成，Accelerate是一个用于轻松管理分布式环境中训练的库，这意味着可以从[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类中使用它。'
- en: Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0
    or newer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保已安装Accelerate，并且至少安装了PyTorch 2.1.0或更新版本。
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: FSDP configuration
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FSDP配置
- en: To start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)
    command to create a configuration file for your training environment. Accelerate
    uses this configuration file to automatically setup the correct training environment
    based on your selected training options in `accelerate config`.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，运行[`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)命令，为您的训练环境创建一个配置文件。Accelerate使用此配置文件根据您在`accelerate
    config`中选择的训练选项自动设置正确的训练环境。
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you run `accelerate config`, you’ll be prompted with a series of options
    to configure your training environment. This section covers some of the most important
    FSDP options. To learn more about the other available FSDP options, take a look
    at the [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config)
    parameters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行`accelerate config`时，您将被提示一系列选项来配置您的训练环境。本节涵盖了一些最重要的FSDP选项。要了解更多关于其他可用的FSDP选项，请查看[fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config)参数。
- en: Sharding strategy
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分片策略
- en: 'FSDP offers a number of sharding strategies to select from:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP提供了许多分片策略可供选择：
- en: '`FULL_SHARD` - shards model parameters, gradients and optimizer states across
    workers; select `1` for this option'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FULL_SHARD` - 在工作器之间对模型参数、梯度和优化器状态进行分片；选择`1`作为此选项'
- en: '`SHARD_GRAD_OP`- shard gradients and optimizer states across workers; select
    `2` for this option'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SHARD_GRAD_OP`- 在工作器之间对梯度和优化器状态进行分片；选择`2`作为此选项'
- en: '`NO_SHARD` - don’t shard anything (this is equivalent to DDP); select `3` for
    this option'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO_SHARD` - 不对任何内容进行分片（相当于DDP）；选择`3`作为此选项'
- en: '`HYBRID_SHARD` - shard model parameters, gradients and optimizer states within
    each worker where each worker also has a full copy; select `4` for this option'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HYBRID_SHARD` - 在每个工作器内对模型参数、梯度和优化器状态进行分片，每个工作器也有完整副本；选择`4`作为此选项'
- en: '`HYBRID_SHARD_ZERO2` - shard gradients and optimizer states within each worker
    where each worker also has a full copy; select `5` for this option'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HYBRID_SHARD_ZERO2` - 在每个工作器内对梯度和优化器状态进行分片，每个工作器也有完整副本；选择`5`作为此选项'
- en: This is enabled by the `fsdp_sharding_strategy` flag.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过`fsdp_sharding_strategy`标志启用的。
- en: CPU offload
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU卸载
- en: 'You could also offload parameters and gradients when they are not in use to
    the CPU to save even more GPU memory and help you fit large models where even
    FSDP may not be sufficient. This is enabled by setting `fsdp_offload_params: true`
    when running `accelerate config`.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '当参数和梯度不在使用时，您还可以将它们卸载到CPU以节省更多GPU内存，并帮助您适应大型模型，即使FSDP可能不足。这可以通过在运行`accelerate
    config`时设置`fsdp_offload_params: true`来启用。'
- en: Wrapping policy
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包装策略
- en: 'FSDP is applied by wrapping each layer in the network. The wrapping is usually
    applied in a nested way where the full weights are discarded after each forward
    pass to save memory for use in the next layer. The *auto wrapping* policy is the
    simplest way to implement this and you don’t need to change any code. You should
    select `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer
    and `fsdp_transformer_layer_cls_to_wrap` to specify which layer to wrap (for example
    `BertLayer`).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 'FSDP通过包装网络中的每一层来应用。包装通常以嵌套方式应用，其中完整权重在每次前向传递后被丢弃，以节省内存供下一层使用。*自动包装*策略是实现这一点的最简单方法，您无需更改任何代码。您应该选择`fsdp_auto_wrap_policy:
    TRANSFORMER_BASED_WRAP`来包装一个Transformer层，并选择`fsdp_transformer_layer_cls_to_wrap`来指定要包装的层（例如`BertLayer`）。'
- en: 'Otherwise, you can choose a size-based wrapping policy where FSDP is applied
    to a layer if it exceeds a certain number of parameters. This is enabled by setting
    `fsdp_wrap_policy: SIZE_BASED_WRAP` and `min_num_param` to the desired size threshold.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '否则，您可以选择基于大小的包装策略，如果某一层的参数超过一定数量，则应用FSDP。这可以通过将`fsdp_wrap_policy: SIZE_BASED_WRAP`和`min_num_param`设置为所需的大小阈值来启用。'
- en: Checkpointing
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查点
- en: 'Intermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT`
    because saving the full state dict with CPU offloading on rank 0 takes a lot of
    time and often results in `NCCL Timeout` errors due to indefinite hanging during
    broadcasting. You can resume training with the sharded state dicts with the [load_state](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.load_state)`
    method.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '中间检查点应该使用`fsdp_state_dict_type: SHARDED_STATE_DICT`保存，因为在rank 0上使用CPU卸载保存完整状态字典需要很长时间，并且经常由于广播期间的无限挂起而导致`NCCL
    Timeout`错误。您可以使用[load_state](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.load_state)`方法恢复训练。'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, when training ends, you want to save the full state dict because sharded
    state dict is only compatible with FSDP.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当训练结束时，您希望保存完整状态字典，因为分片状态字典仅与FSDP兼容。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TPU
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPU
- en: '[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) supports FSDP
    training for TPUs and it can be enabled by modifying the FSDP configuration file
    generated by `accelerate config`. In addition to the sharding strategies and wrapping
    options specified above, you can add the parameters shown below to the file.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html)支持TPU的FSDP训练，可以通过修改`accelerate
    config`生成的FSDP配置文件来启用。除了上面指定的分片策略和包装选项之外，您还可以向文件中添加下面显示的参数。'
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The [`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)
    allow you to configure additional XLA-specific parameters for FSDP.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)允许您配置FSDP的额外XLA特定参数。'
- en: Launch training
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动训练
- en: 'An example FSDP configuration file may look like:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的FSDP配置文件可能如下所示：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To launch training, run the [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)
    command and it’ll automatically use the configuration file you previously created
    with `accelerate config`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练，请运行[`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)命令，它将自动使用您之前使用`accelerate
    config`创建的配置文件。
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next steps
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'FSDP can be a powerful tool for training really large models and you have access
    to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient
    states, and even offloading them to the CPU when they’re inactive, FSDP can reduce
    the high cost of large-scale training. If you’re interested in learning more,
    the following may be helpful:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: FSDP可以是训练非常大模型的强大工具，如果您有多个GPU或TPU可用。通过对模型参数、优化器和梯度状态进行分片，甚至在它们不活动时将它们卸载到CPU上，FSDP可以减少大规模训练的高成本。如果您有兴趣了解更多，以下内容可能有所帮助：
- en: Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照更详细的[FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)加速指南进行操作。
- en: Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
    blog post.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读[介绍PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)博文。
- en: Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)
    blog post.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读[使用FSDP在Cloud TPU上扩展PyTorch模型](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)博文。
