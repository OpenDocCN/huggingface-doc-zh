- en: Fully Sharded Data Parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/fsdp](https://huggingface.co/docs/transformers/v4.37.2/en/fsdp)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/19.b0691c8e.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
    is a data parallel method that shards a model’s parameters, gradients and optimizer
    states across the number of available GPUs (also called workers or *rank*). Unlike
    [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html),
    FSDP reduces memory-usage because a model is replicated on each GPU. This improves
    GPU memory-efficiency and allows you to train much larger models on fewer GPUs.
    FSDP is integrated with the Accelerate, a library for easily managing training
    in distributed environments, which means it is available for use from the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class.'
  prefs: []
  type: TYPE_NORMAL
- en: Before you start, make sure Accelerate is installed and at least PyTorch 2.1.0
    or newer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: FSDP configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, run the [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)
    command to create a configuration file for your training environment. Accelerate
    uses this configuration file to automatically setup the correct training environment
    based on your selected training options in `accelerate config`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you run `accelerate config`, you’ll be prompted with a series of options
    to configure your training environment. This section covers some of the most important
    FSDP options. To learn more about the other available FSDP options, take a look
    at the [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config)
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Sharding strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FSDP offers a number of sharding strategies to select from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FULL_SHARD` - shards model parameters, gradients and optimizer states across
    workers; select `1` for this option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHARD_GRAD_OP`- shard gradients and optimizer states across workers; select
    `2` for this option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NO_SHARD` - don’t shard anything (this is equivalent to DDP); select `3` for
    this option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYBRID_SHARD` - shard model parameters, gradients and optimizer states within
    each worker where each worker also has a full copy; select `4` for this option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYBRID_SHARD_ZERO2` - shard gradients and optimizer states within each worker
    where each worker also has a full copy; select `5` for this option'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is enabled by the `fsdp_sharding_strategy` flag.
  prefs: []
  type: TYPE_NORMAL
- en: CPU offload
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You could also offload parameters and gradients when they are not in use to
    the CPU to save even more GPU memory and help you fit large models where even
    FSDP may not be sufficient. This is enabled by setting `fsdp_offload_params: true`
    when running `accelerate config`.'
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'FSDP is applied by wrapping each layer in the network. The wrapping is usually
    applied in a nested way where the full weights are discarded after each forward
    pass to save memory for use in the next layer. The *auto wrapping* policy is the
    simplest way to implement this and you don’t need to change any code. You should
    select `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` to wrap a Transformer layer
    and `fsdp_transformer_layer_cls_to_wrap` to specify which layer to wrap (for example
    `BertLayer`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, you can choose a size-based wrapping policy where FSDP is applied
    to a layer if it exceeds a certain number of parameters. This is enabled by setting
    `fsdp_wrap_policy: SIZE_BASED_WRAP` and `min_num_param` to the desired size threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Intermediate checkpoints should be saved with `fsdp_state_dict_type: SHARDED_STATE_DICT`
    because saving the full state dict with CPU offloading on rank 0 takes a lot of
    time and often results in `NCCL Timeout` errors due to indefinite hanging during
    broadcasting. You can resume training with the sharded state dicts with the [load_state](https://huggingface.co/docs/accelerate/v0.26.1/en/package_reference/accelerator#accelerate.Accelerator.load_state)`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, when training ends, you want to save the full state dict because sharded
    state dict is only compatible with FSDP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: TPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) supports FSDP
    training for TPUs and it can be enabled by modifying the FSDP configuration file
    generated by `accelerate config`. In addition to the sharding strategies and wrapping
    options specified above, you can add the parameters shown below to the file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The [`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)
    allow you to configure additional XLA-specific parameters for FSDP.
  prefs: []
  type: TYPE_NORMAL
- en: Launch training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An example FSDP configuration file may look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To launch training, run the [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)
    command and it’ll automatically use the configuration file you previously created
    with `accelerate config`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FSDP can be a powerful tool for training really large models and you have access
    to more than one GPU or TPU. By sharding the model parameters, optimizer and gradient
    states, and even offloading them to the CPU when they’re inactive, FSDP can reduce
    the high cost of large-scale training. If you’re interested in learning more,
    the following may be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: Follow along with the more in-depth Accelerate guide for [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
    blog post.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the [Scaling PyTorch models on Cloud TPUs with FSDP](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)
    blog post.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
