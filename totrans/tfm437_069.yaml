- en: Methods and tools for efficient training on a single GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_one](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_gpu_one)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This guide demonstrates practical techniques that you can use to increase the
    efficiency of your modelâ€™s training by optimizing memory utilization, speeding
    up the training, or both. If youâ€™d like to understand how GPU is utilized during
    training, please refer to the [Model training anatomy](model_memory_anatomy) conceptual
    guide first. This guide focuses on practical techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: If you have access to a machine with multiple GPUs, these approaches are still
    valid, plus you can leverage additional methods outlined in the [multi-GPU section](perf_train_gpu_many).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'When training large models, there are two aspects that should be considered
    at the same time:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Data throughput/training time
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximizing the throughput (samples/second) leads to lower training cost. This
    is generally achieved by utilizing the GPU as much as possible and thus filling
    GPU memory to its limit. If the desired batch size exceeds the limits of the GPU
    memory, the memory optimization techniques, such as gradient accumulation, can
    help.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: However, if the preferred batch size fits into memory, thereâ€™s no reason to
    apply memory-optimizing techniques because they can slow down the training. Just
    because one can use a large batch size, does not necessarily mean they should.
    As part of hyperparameter tuning, you should determine which batch size yields
    the best results and then optimize resources accordingly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods and tools covered in this guide can be classified based on the
    effect they have on the training process:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '| Method/tool | Improves training speed | Optimizes memory utilization |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| [Batch size choice](#batch-size-choice) | Yes | Yes |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| [Gradient accumulation](#gradient-accumulation) | No | Yes |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| [Gradient checkpointing](#gradient-checkpointing) | No | Yes |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| [Mixed precision training](#mixed-precision-training) | Yes | (No) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: '| [Optimizer choice](#optimizer-choice) | Yes | Yes |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
- en: '| [Data preloading](#data-preloading) | Yes | No |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| [DeepSpeed Zero](#deepspeed-zero) | No | Yes |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| [torch.compile](#using-torchcompile) | Yes | No |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| [Parameter-Efficient Fine Tuning (PEFT)](#peft) | No | Yes |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: 'Note: when using mixed precision with a small model and a large batch size,
    there will be some memory savings but with a large model and a small batch size,
    the memory use will be larger.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: You can combine the above methods to get a cumulative effect. These techniques
    are available to you whether you are training your model with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    or writing a pure PyTorch loop, in which case you can [configure these optimizations
    with ğŸ¤— Accelerate](#using-accelerate).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'If these methods do not result in sufficient gains, you can explore the following
    options:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[Look into building your own custom Docker container with efficient softare
    prebuilds](#efficient-software-prebuilds)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Consider a model that uses Mixture of Experts (MoE)](#mixture-of-experts)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Convert your model to BetterTransformer to leverage PyTorch native attention](#using-pytorch-native-attention)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, if all of the above is still not enough, even after switching to a
    server-grade GPU like A100, consider moving to a multi-GPU setup. All these approaches
    are still valid in a multi-GPU setup, plus you can leverage additional parallelism
    techniques outlined in the [multi-GPU section](perf_train_gpu_many).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Batch size choice
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To achieve optimal performance, start by identifying the appropriate batch size.
    It is recommended to use batch sizes and input/output neuron counts that are of
    size 2^N. Often itâ€™s a multiple of 8, but it can be higher depending on the hardware
    being used and the modelâ€™s dtype.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: For reference, check out NVIDIAâ€™s recommendation for [input/output neuron counts](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features)
    and [batch size](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size)
    for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å‚è€ƒï¼Œè¯·æŸ¥çœ‹NVIDIAå…³äºå…¨è¿æ¥å±‚ï¼ˆæ¶‰åŠGEMMsï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ï¼‰çš„[è¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features)å’Œ[æ‰¹æ¬¡å¤§å°](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size)çš„å»ºè®®ã€‚
- en: '[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)
    define the multiplier based on the dtype and the hardware. For instance, for fp16
    data type a multiple of 8 is recommended, unless itâ€™s an A100 GPU, in which case
    use multiples of 64.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tensor Coreè¦æ±‚](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)æ ¹æ®æ•°æ®ç±»å‹å’Œç¡¬ä»¶å®šä¹‰ä¹˜æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äºfp16æ•°æ®ç±»å‹ï¼Œæ¨èä½¿ç”¨8çš„å€æ•°ï¼Œé™¤éæ˜¯A100
    GPUï¼Œæ­¤æ—¶ä½¿ç”¨64çš„å€æ•°ã€‚'
- en: For parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization).
    This is where tiling happens and the right multiplier can have a significant speedup.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾ƒå°çš„å‚æ•°ï¼Œè¿˜è¦è€ƒè™‘[ç»´åº¦é‡åŒ–æ•ˆåº”](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization)ã€‚è¿™æ˜¯ç“¦ç‰‡åŒ–å‘ç”Ÿçš„åœ°æ–¹ï¼Œæ­£ç¡®çš„ä¹˜æ•°å¯ä»¥æ˜¾è‘—åŠ å¿«é€Ÿåº¦ã€‚
- en: Gradient Accumulation
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯
- en: The **gradient accumulation** method aims to calculate gradients in smaller
    increments instead of computing them for the entire batch at once. This approach
    involves iteratively calculating gradients in smaller batches by performing forward
    and backward passes through the model and accumulating the gradients during the
    process. Once a sufficient number of gradients have been accumulated, the modelâ€™s
    optimization step is executed. By employing gradient accumulation, it becomes
    possible to increase the **effective batch size** beyond the limitations imposed
    by the GPUâ€™s memory capacity. However, it is important to note that the additional
    forward and backward passes introduced by gradient accumulation can slow down
    the training process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦ç´¯ç§¯**æ–¹æ³•æ—¨åœ¨ä»¥è¾ƒå°çš„å¢é‡è®¡ç®—æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡ä¸ºæ•´ä¸ªæ‰¹æ¬¡è®¡ç®—æ¢¯åº¦ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠé€šè¿‡æ¨¡å‹æ‰§è¡Œå‰å‘å’Œåå‘ä¼ é€’ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­ç´¯ç§¯æ¢¯åº¦æ¥è¿­ä»£è®¡ç®—è¾ƒå°æ‰¹æ¬¡çš„æ¢¯åº¦ã€‚ä¸€æ—¦ç§¯ç´¯äº†è¶³å¤Ÿæ•°é‡çš„æ¢¯åº¦ï¼Œå°±ä¼šæ‰§è¡Œæ¨¡å‹çš„ä¼˜åŒ–æ­¥éª¤ã€‚é€šè¿‡ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥å°†**æœ‰æ•ˆæ‰¹æ¬¡å¤§å°**å¢åŠ åˆ°GPUå†…å­˜å®¹é‡æ‰€æ–½åŠ çš„é™åˆ¶ä¹‹å¤–ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¢¯åº¦ç´¯ç§¯å¼•å…¥çš„é¢å¤–å‰å‘å’Œåå‘ä¼ é€’å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚'
- en: 'You can enable gradient accumulation by adding the `gradient_accumulation_steps`
    argument to [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å‘[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)æ·»åŠ `gradient_accumulation_steps`å‚æ•°æ¥å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the above example, your effective batch size becomes 4.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œæ‚¨çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å˜ä¸º4ã€‚
- en: Alternatively, use ğŸ¤— Accelerate to gain full control over the training loop.
    Find the ğŸ¤— Accelerate example [further down in this guide](#using-accelerate).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œä½¿ç”¨ğŸ¤— Accelerateæ¥å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ã€‚åœ¨æœ¬æŒ‡å—çš„[åé¢](#using-accelerate)æ‰¾åˆ°ğŸ¤— Accelerateç¤ºä¾‹ã€‚
- en: While it is advised to max out GPU usage as much as possible, a high number
    of gradient accumulation steps can result in a more pronounced training slowdown.
    Consider the following example. Letâ€™s say, the `per_device_train_batch_size=4`
    without gradient accumulation hits the GPUâ€™s limit. If you would like to train
    with batches of size 64, do not set the `per_device_train_batch_size` to 1 and
    `gradient_accumulation_steps` to 64\. Instead, keep `per_device_train_batch_size=4`
    and set `gradient_accumulation_steps=16`. This results in the same effective batch
    size while making better use of the available GPU resources.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°½å¯èƒ½åœ°æœ€å¤§åŒ–GPUä½¿ç”¨ç‡æ˜¯å»ºè®®çš„ï¼Œä½†æ˜¯é«˜æ•°é‡çš„æ¢¯åº¦ç´¯ç§¯æ­¥éª¤å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒå‡é€Ÿæ›´åŠ æ˜æ˜¾ã€‚è€ƒè™‘ä»¥ä¸‹ç¤ºä¾‹ã€‚å‡è®¾`per_device_train_batch_size=4`ï¼Œæ²¡æœ‰æ¢¯åº¦ç´¯ç§¯è¾¾åˆ°äº†GPUçš„æé™ã€‚å¦‚æœæ‚¨æƒ³è¦ä½¿ç”¨å¤§å°ä¸º64çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒï¼Œè¯·ä¸è¦å°†`per_device_train_batch_size`è®¾ç½®ä¸º1ï¼Œå¹¶å°†`gradient_accumulation_steps`è®¾ç½®ä¸º64ã€‚ç›¸åï¼Œä¿æŒ`per_device_train_batch_size=4`ï¼Œå¹¶è®¾ç½®`gradient_accumulation_steps=16`ã€‚è¿™æ ·å¯ä»¥å®ç°ç›¸åŒçš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼ŒåŒæ—¶æ›´å¥½åœ°åˆ©ç”¨å¯ç”¨çš„GPUèµ„æºã€‚
- en: For additional information, please refer to batch size and gradient accumulation
    benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)
    and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)å’Œ[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957)çš„æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯åŸºå‡†ã€‚
- en: Gradient Checkpointing
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦æ£€æŸ¥ç‚¹
- en: Some large models may still face memory issues even when the batch size is set
    to 1 and gradient accumulation is used. This is because there are other components
    that also require memory storage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿å°†æ‰¹æ¬¡å¤§å°è®¾ç½®ä¸º1å¹¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œä¸€äº›å¤§å‹æ¨¡å‹ä»å¯èƒ½é¢ä¸´å†…å­˜é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºè¿˜æœ‰å…¶ä»–ç»„ä»¶ä¹Ÿéœ€è¦å†…å­˜å­˜å‚¨ã€‚
- en: Saving all activations from the forward pass in order to compute the gradients
    during the backward pass can result in significant memory overhead. The alternative
    approach of discarding the activations and recalculating them when needed during
    the backward pass, would introduce a considerable computational overhead and slow
    down the training process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜å‰å‘ä¼ é€’ä¸­çš„æ‰€æœ‰æ¿€æ´»ä»¥ä¾¿åœ¨åå‘ä¼ é€’æœŸé—´è®¡ç®—æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´æ˜¾ç€çš„å†…å­˜å¼€é”€ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨åå‘ä¼ é€’æœŸé—´ä¸¢å¼ƒæ¿€æ´»å¹¶åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å®ƒä»¬ï¼Œè¿™å°†å¼•å…¥ç›¸å½“å¤§çš„è®¡ç®—å¼€é”€å¹¶å‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚
- en: '**Gradient checkpointing** offers a compromise between these two approaches
    and saves strategically selected activations throughout the computational graph
    so only a fraction of the activations need to be re-computed for the gradients.
    For an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¢¯åº¦æ£€æŸ¥ç‚¹**æä¾›äº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„æŠ˜è¡·æ–¹æ¡ˆï¼Œå¹¶åœ¨è®¡ç®—å›¾ä¸­ä¿å­˜äº†ç­–ç•¥æ€§é€‰æ‹©çš„æ¿€æ´»ï¼Œå› æ­¤åªéœ€é‡æ–°è®¡ç®—ä¸€å°éƒ¨åˆ†æ¿€æ´»ä»¥è·å¾—æ¢¯åº¦ã€‚æœ‰å…³æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ·±å…¥è§£é‡Šï¼Œè¯·å‚é˜…[è¿™ç¯‡å¾ˆæ£’çš„æ–‡ç« ](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)ã€‚'
- en: 'To enable gradient checkpointing in the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    pass the corresponding a flag to [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¸­å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œè¯·å°†ç›¸åº”çš„æ ‡å¿—ä¼ é€’ç»™[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ï¼š
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Alternatively, use ğŸ¤— Accelerate - find the ğŸ¤— Accelerate example [further in
    this guide](#using-accelerate).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œä½¿ç”¨ğŸ¤— Accelerate-åœ¨æœ¬æŒ‡å—ä¸­æ‰¾åˆ°ğŸ¤— Accelerateç¤ºä¾‹[ï¼ˆ#ä½¿ç”¨åŠ é€Ÿï¼‰](#using-accelerate)ã€‚
- en: While gradient checkpointing may improve memory efficiency, it slows training
    by approximately 20%.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ¢¯åº¦æ£€æŸ¥ç‚¹å¯èƒ½æé«˜å†…å­˜æ•ˆç‡ï¼Œä½†ä¼šä½¿è®­ç»ƒé€Ÿåº¦å‡æ…¢çº¦20%ã€‚
- en: Mixed precision training
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒ
- en: '**Mixed precision training** is a technique that aims to optimize the computational
    efficiency of training models by utilizing lower-precision numerical formats for
    certain variables. Traditionally, most models use 32-bit floating point precision
    (fp32 or float32) to represent and process variables. However, not all variables
    require this high precision level to achieve accurate results. By reducing the
    precision of certain variables to lower numerical formats like 16-bit floating
    point (fp16 or float16), we can speed up the computations. Because in this approach
    some computations are performed in half-precision, while some are still in full
    precision, the approach is called mixed precision training.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ··åˆç²¾åº¦è®­ç»ƒ**æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡åˆ©ç”¨è¾ƒä½ç²¾åº¦æ•°å€¼æ ¼å¼æ¥å¤„ç†æŸäº›å˜é‡æ¥ä¼˜åŒ–è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æ•ˆç‡çš„æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œå¤§å¤šæ•°æ¨¡å‹ä½¿ç”¨32ä½æµ®ç‚¹ç²¾åº¦ï¼ˆfp32æˆ–float32ï¼‰æ¥è¡¨ç¤ºå’Œå¤„ç†å˜é‡ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å˜é‡éƒ½éœ€è¦è¿™ç§é«˜ç²¾åº¦çº§åˆ«æ‰èƒ½è·å¾—å‡†ç¡®çš„ç»“æœã€‚é€šè¿‡å°†æŸäº›å˜é‡çš„ç²¾åº¦é™ä½åˆ°è¾ƒä½çš„æ•°å€¼æ ¼å¼ï¼Œå¦‚16ä½æµ®ç‚¹ï¼ˆfp16æˆ–float16ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚å› ä¸ºåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œä¸€äº›è®¡ç®—æ˜¯ä»¥åŠç²¾åº¦è¿›è¡Œçš„ï¼Œè€Œä¸€äº›ä»ç„¶æ˜¯ä»¥å…¨ç²¾åº¦è¿›è¡Œçš„ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•è¢«ç§°ä¸ºæ··åˆç²¾åº¦è®­ç»ƒã€‚'
- en: Most commonly mixed precision training is achieved by using fp16 (float16) data
    types, however, some GPU architectures (such as the Ampere architecture) offer
    bf16 and tf32 (CUDA internal data type) data types. Check out the [NVIDIA Blog](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)
    to learn more about the differences between these data types.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¸¸è§çš„æ··åˆç²¾åº¦è®­ç»ƒæ˜¯é€šè¿‡ä½¿ç”¨fp16ï¼ˆfloat16ï¼‰æ•°æ®ç±»å‹æ¥å®ç°çš„ï¼Œä½†æ˜¯ä¸€äº›GPUæ¶æ„ï¼ˆä¾‹å¦‚Ampereæ¶æ„ï¼‰æä¾›äº†bf16å’Œtf32ï¼ˆCUDAå†…éƒ¨æ•°æ®ç±»å‹ï¼‰æ•°æ®ç±»å‹ã€‚æŸ¥çœ‹[NVIDIAåšå®¢](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)ä»¥äº†è§£è¿™äº›æ•°æ®ç±»å‹ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: fp16
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: fp16
- en: The main advantage of mixed precision training comes from saving the activations
    in half precision (fp16). Although the gradients are also computed in half precision
    they are converted back to full precision for the optimization step so no memory
    is saved here. While mixed precision training results in faster computations,
    it can also lead to more GPU memory being utilized, especially for small batch
    sizes. This is because the model is now present on the GPU in both 16-bit and
    32-bit precision (1.5x the original model on the GPU).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸»è¦ä¼˜åŠ¿æ¥è‡ªäºå°†æ¿€æ´»ä¿å­˜åœ¨åŠç²¾åº¦ï¼ˆfp16ï¼‰ä¸­ã€‚å°½ç®¡æ¢¯åº¦ä¹Ÿæ˜¯ä»¥åŠç²¾åº¦è®¡ç®—çš„ï¼Œä½†å®ƒä»¬åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è¢«è½¬æ¢å›å…¨ç²¾åº¦ï¼Œå› æ­¤åœ¨è¿™é‡Œæ²¡æœ‰èŠ‚çœå†…å­˜ã€‚è™½ç„¶æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ›´å¤šçš„GPUå†…å­˜è¢«åˆ©ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°æ‰¹é‡å¤§å°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹ç°åœ¨ä»¥16ä½å’Œ32ä½ç²¾åº¦ï¼ˆGPUä¸ŠåŸå§‹æ¨¡å‹çš„1.5å€ï¼‰å­˜åœ¨äºGPUä¸Šã€‚
- en: 'To enable mixed precision training, set the `fp16` flag to `True`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¯·å°†`fp16`æ ‡å¿—è®¾ç½®ä¸º`True`ï¼š
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you prefer to use ğŸ¤— Accelerate, find the ğŸ¤— Accelerate example [further in
    this guide](#using-accelerate).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ğŸ¤— Accelerateï¼Œè¯·åœ¨æœ¬æŒ‡å—ä¸­æ‰¾åˆ°ğŸ¤— Accelerateç¤ºä¾‹[ï¼ˆ#ä½¿ç”¨åŠ é€Ÿï¼‰](#using-accelerate)ã€‚
- en: BF16
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BF16
- en: If you have access to an Ampere or newer hardware you can use bf16 for mixed
    precision training and evaluation. While bf16 has a worse precision than fp16,
    it has a much bigger dynamic range. In fp16 the biggest number you can have is
    `65535` and any number above that will result in an overflow. A bf16 number can
    be as large as `3.39e+38` (!) which is about the same as fp32 - because both have
    8-bits used for the numerical range.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å¯ä»¥è®¿é—®Ampereæˆ–æ›´æ–°çš„ç¡¬ä»¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨bf16è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒå’Œè¯„ä¼°ã€‚è™½ç„¶bf16çš„ç²¾åº¦æ¯”fp16å·®ï¼Œä½†å®ƒå…·æœ‰æ›´å¤§çš„åŠ¨æ€èŒƒå›´ã€‚åœ¨fp16ä¸­ï¼Œæ‚¨å¯ä»¥æ‹¥æœ‰çš„æœ€å¤§æ•°å­—æ˜¯`65535`ï¼Œä»»ä½•è¶…è¿‡è¿™ä¸ªæ•°å­—çš„æ•°å­—éƒ½ä¼šå¯¼è‡´æº¢å‡ºã€‚bf16çš„æ•°å­—å¯ä»¥è¾¾åˆ°`3.39e+38`ï¼ˆï¼ï¼‰ï¼Œè¿™å¤§çº¦ä¸fp32ç›¸åŒ-å› ä¸ºä¸¤è€…éƒ½ä½¿ç”¨äº†8ä½ç”¨äºæ•°å€¼èŒƒå›´ã€‚
- en: 'You can enable BF16 in the ğŸ¤— Trainer with:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ğŸ¤— Trainerä¸­å¯ç”¨BF16ï¼š
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TF32
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF32
- en: 'The Ampere hardware uses a magical data type called tf32\. It has the same
    numerical range as fp32 (8-bits), but instead of 23 bits precision it has only
    10 bits (same as fp16) and uses only 19 bits in total. Itâ€™s â€œmagicalâ€ in the sense
    that you can use the normal fp32 training and/or inference code and by enabling
    tf32 support you can get up to 3x throughput improvement. All you need to do is
    to add the following to your code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Ampereç¡¬ä»¶ä½¿ç”¨ä¸€ç§åä¸ºtf32çš„ç¥å¥‡æ•°æ®ç±»å‹ã€‚å®ƒå…·æœ‰ä¸fp32ç›¸åŒçš„æ•°å€¼èŒƒå›´ï¼ˆ8ä½ï¼‰ï¼Œä½†æ˜¯ç²¾åº¦åªæœ‰10ä½ï¼ˆä¸fp16ç›¸åŒï¼‰ï¼Œæ€»å…±åªä½¿ç”¨äº†19ä½ã€‚å®ƒåœ¨è¿™ç§æ„ä¹‰ä¸Šæ˜¯â€œç¥å¥‡çš„â€ï¼Œå³æ‚¨å¯ä»¥ä½¿ç”¨æ­£å¸¸çš„fp32è®­ç»ƒå’Œ/æˆ–æ¨ç†ä»£ç ï¼Œå¹¶é€šè¿‡å¯ç”¨tf32æ”¯æŒï¼Œæ‚¨å¯ä»¥è·å¾—é«˜è¾¾3å€çš„ååé‡æ”¹è¿›ã€‚æ‚¨åªéœ€è¦å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­ï¼š
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: CUDA will automatically switch to using tf32 instead of fp32 where possible,
    assuming that the used GPU is from the Ampere series.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CUDAå°†åœ¨å¯èƒ½çš„æƒ…å†µä¸‹è‡ªåŠ¨åˆ‡æ¢åˆ°ä½¿ç”¨tf32è€Œä¸æ˜¯fp32ï¼Œå‡è®¾æ‰€ä½¿ç”¨çš„GPUæ¥è‡ªAmpereç³»åˆ—ã€‚
- en: According to [NVIDIA research](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/),
    the majority of machine learning training workloads show the same perplexity and
    convergence with tf32 training as with fp32. If youâ€™re already using fp16 or bf16
    mixed precision it may help with the throughput as well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®[NVIDIAç ”ç©¶](https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/)ï¼Œå¤§å¤šæ•°æœºå™¨å­¦ä¹ è®­ç»ƒå·¥ä½œè´Ÿè½½æ˜¾ç¤ºå‡ºä¸fp32ç›¸åŒçš„å›°æƒ‘åº¦å’Œæ”¶æ•›æ€§ã€‚å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨fp16æˆ–bf16æ··åˆç²¾åº¦ï¼Œè¿™ä¹Ÿå¯èƒ½æœ‰åŠ©äºæé«˜ååé‡ã€‚
- en: 'You can enable this mode in the ğŸ¤— Trainer:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨ğŸ¤— Trainerä¸­å¯ç”¨æ­¤æ¨¡å¼ï¼š
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: tf32 canâ€™t be accessed directly via `tensor.to(dtype=torch.tf32)` because it
    is an internal CUDA data type. You need `torch>=1.7` to use tf32 data types.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: tf32æ— æ³•é€šè¿‡`tensor.to(dtype=torch.tf32)`ç›´æ¥è®¿é—®ï¼Œå› ä¸ºå®ƒæ˜¯å†…éƒ¨CUDAæ•°æ®ç±»å‹ã€‚æ‚¨éœ€è¦`torch>=1.7`æ‰èƒ½ä½¿ç”¨tf32æ•°æ®ç±»å‹ã€‚
- en: 'For additional information on tf32 vs other precisions, please refer to the
    following benchmarks: [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803)
    and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³tf32ä¸å…¶ä»–ç²¾åº¦çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä»¥ä¸‹åŸºå‡†æµ‹è¯•ï¼š[RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004390803)å’Œ[A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1004543189)ã€‚
- en: Flash Attention 2
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flash Attention 2
- en: You can speedup the training throughput by using Flash Attention 2 integration
    in transformers. Check out the appropriate section in the [single GPU section](./perf_infer_gpu_one#Flash-Attention-2)
    to learn more about how to load a model with Flash Attention 2 modules.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡åœ¨transformersä¸­ä½¿ç”¨Flash Attention 2é›†æˆæ¥åŠ å¿«è®­ç»ƒååé‡ã€‚æŸ¥çœ‹[å•GPUéƒ¨åˆ†](./perf_infer_gpu_one#Flash-Attention-2)ä¸­çš„é€‚å½“éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•åŠ è½½å¸¦æœ‰Flash
    Attention 2æ¨¡å—çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ã€‚
- en: Optimizer choice
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å™¨é€‰æ‹©
- en: The most common optimizer used to train transformer models is Adam or AdamW
    (Adam with weight decay). Adam achieves good convergence by storing the rolling
    average of the previous gradients; however, it adds an additional memory footprint
    of the order of the number of model parameters. To remedy this, you can use an
    alternative optimizer. For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex)
    installed for NVIDIA GPUs, or [ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex)
    for AMD GPUs, `adamw_apex_fused` will give you the fastest training experience
    among all supported AdamW optimizers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè®­ç»ƒå˜å‹å™¨æ¨¡å‹çš„æœ€å¸¸ç”¨ä¼˜åŒ–å™¨æ˜¯Adamæˆ–AdamWï¼ˆå¸¦æœ‰æƒé‡è¡°å‡çš„Adamï¼‰ã€‚Adamé€šè¿‡å­˜å‚¨å…ˆå‰æ¢¯åº¦çš„æ»šåŠ¨å¹³å‡å€¼å®ç°è‰¯å¥½çš„æ”¶æ•›ï¼›ç„¶è€Œï¼Œå®ƒä¼šå¢åŠ ä¸æ¨¡å‹å‚æ•°æ•°é‡ç›¸åŒæ•°é‡çº§çš„é¢å¤–å†…å­˜å ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¦ä¸€ç§ä¼˜åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨åœ¨NVIDIA
    GPUä¸Šå®‰è£…äº†[NVIDIA/apex](https://github.com/NVIDIA/apex)ï¼Œæˆ–è€…åœ¨AMD GPUä¸Šå®‰è£…äº†[ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex)ï¼Œ`adamw_apex_fused`å°†ä¸ºæ‚¨æä¾›æ‰€æœ‰æ”¯æŒçš„AdamWä¼˜åŒ–å™¨ä¸­æœ€å¿«çš„è®­ç»ƒä½“éªŒã€‚
- en: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`,
    `adamw_torch_fused`, `adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or
    `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)é›†æˆäº†å„ç§å¯ç«‹å³ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼š`adamw_hf`ã€`adamw_torch`ã€`adamw_torch_fused`ã€`adamw_apex_fused`ã€`adamw_anyprecision`ã€`adafactor`æˆ–`adamw_bnb_8bit`ã€‚æ›´å¤šä¼˜åŒ–å™¨å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å®ç°æ’å…¥ã€‚'
- en: 'Letâ€™s take a closer look at two alternatives to AdamW optimizer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹çœ‹ä¸¤ç§æ›¿ä»£AdamWä¼˜åŒ–å™¨ï¼š
- en: '`adafactor` which is available in [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`adafactor`å¯åœ¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ä¸­ä½¿ç”¨'
- en: '`adamw_bnb_8bit` is also available in Trainer, but a third-party integration
    is provided below for demonstration.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`adamw_bnb_8bit`ä¹Ÿå¯åœ¨Trainerä¸­ä½¿ç”¨ï¼Œä½†ä»¥ä¸‹æä¾›äº†ç¬¬ä¸‰æ–¹é›†æˆä»¥ä¾›æ¼”ç¤ºã€‚'
- en: 'For comparison, for a 3B-parameter model, like â€œt5-3bâ€:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥3Bå‚æ•°æ¨¡å‹â€œt5-3bâ€ä¸ºä¾‹è¿›è¡Œæ¯”è¾ƒï¼š
- en: A standard AdamW optimizer will need 24GB of GPU memory because it uses 8 bytes
    for each parameter (8*3 => 24GB)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡å‡†çš„AdamWä¼˜åŒ–å™¨å°†éœ€è¦24GBçš„GPUå†…å­˜ï¼Œå› ä¸ºå®ƒä¸ºæ¯ä¸ªå‚æ•°ä½¿ç”¨8å­—èŠ‚ï¼ˆ8*3 => 24GBï¼‰
- en: Adafactor optimizer will need more than 12GB. It uses slightly more than 4 bytes
    for each parameter, so 4*3 and then some extra.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adafactorä¼˜åŒ–å™¨å°†éœ€è¦è¶…è¿‡12GBã€‚å®ƒä¸ºæ¯ä¸ªå‚æ•°ä½¿ç”¨ç•¥å¤šäº4å­—èŠ‚ï¼Œå› æ­¤4*3ï¼Œç„¶åå†åŠ ä¸€äº›ã€‚
- en: 8bit BNB quantized optimizer will use only (2*3) 6GB if all optimizer states
    are quantized.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 8ä½BNBé‡åŒ–ä¼˜åŒ–å™¨å°†ä»…ä½¿ç”¨ï¼ˆ2*3ï¼‰6GBï¼Œå¦‚æœæ‰€æœ‰ä¼˜åŒ–å™¨çŠ¶æ€éƒ½è¢«é‡åŒ–ã€‚
- en: Adafactor
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adafactor
- en: Adafactor doesnâ€™t store rolling averages for each element in weight matrices.
    Instead, it keeps aggregated information (sums of rolling averages row- and column-wise),
    significantly reducing its footprint. However, compared to Adam, Adafactor may
    have slower convergence in certain cases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Adafactorä¸ä¼šä¸ºæƒé‡çŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ å­˜å‚¨æ»šåŠ¨å¹³å‡å€¼ã€‚ç›¸åï¼Œå®ƒä¿ç•™èšåˆä¿¡æ¯ï¼ˆæŒ‰è¡Œå’Œåˆ—çš„æ»šåŠ¨å¹³å‡å’Œï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ã€‚ç„¶è€Œï¼Œä¸Adamç›¸æ¯”ï¼ŒAdafactoråœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æ”¶æ•›è¾ƒæ…¢ã€‚
- en: 'You can switch to Adafactor by setting `optim="adafactor"` in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­è®¾ç½®`optim="adafactor"`æ¥åˆ‡æ¢åˆ°Adafactorï¼š
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Combined with other approaches (gradient accumulation, gradient checkpointing,
    and mixed precision training) you can notice up to 3x improvement while maintaining
    the throughput! However, as mentioned before, the convergence of Adafactor can
    be worse than Adam.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆå…¶ä»–æ–¹æ³•ï¼ˆæ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼‰ï¼Œæ‚¨å¯ä»¥åœ¨ä¿æŒååé‡çš„åŒæ—¶å®ç°é«˜è¾¾3å€çš„æ”¹è¿›ï¼ç„¶è€Œï¼Œå¦‚å‰æ‰€è¿°ï¼ŒAdafactorçš„æ”¶æ•›æ€§å¯èƒ½æ¯”Adamæ›´å·®ã€‚
- en: 8-bit Adam
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8ä½Adam
- en: Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the
    full state and quantizes it. Quantization means that it stores the state with
    lower precision and dequantizes it only for the optimization. This is similar
    to the idea behind mixed precision training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Adafactorç­‰èšåˆä¼˜åŒ–å™¨çŠ¶æ€ä¸åŒï¼Œ8ä½Adamä¿ç•™å®Œæ•´çŠ¶æ€å¹¶å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚é‡åŒ–æ„å‘³ç€ä»¥è¾ƒä½ç²¾åº¦å­˜å‚¨çŠ¶æ€ï¼Œå¹¶ä»…åœ¨ä¼˜åŒ–æ—¶å¯¹å…¶è¿›è¡Œåé‡åŒ–ã€‚è¿™ç±»ä¼¼äºæ··åˆç²¾åº¦è®­ç»ƒçš„æ€æƒ³ã€‚
- en: 'To use `adamw_bnb_8bit`, you simply need to set `optim="adamw_bnb_8bit"` in
    [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨`adamw_bnb_8bit`ï¼Œæ‚¨åªéœ€åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­è®¾ç½®`optim="adamw_bnb_8bit"`ï¼š
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, we can also use a third-party implementation of the 8-bit optimizer
    for demonstration purposes to see how that can be integrated.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ç¬¬ä¸‰æ–¹å®ç°çš„8ä½ä¼˜åŒ–å™¨è¿›è¡Œæ¼”ç¤ºï¼Œä»¥äº†è§£å¦‚ä½•é›†æˆã€‚
- en: First, follow the installation guide in the GitHub [repo](https://github.com/TimDettmers/bitsandbytes)
    to install the `bitsandbytes` library that implements the 8-bit Adam optimizer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒæŒ‰ç…§GitHub [repo](https://github.com/TimDettmers/bitsandbytes)ä¸­çš„å®‰è£…æŒ‡å—å®‰è£…å®ç°8ä½Adamä¼˜åŒ–å™¨çš„`bitsandbytes`åº“ã€‚
- en: 'Next you need to initialize the optimizer. This involves two steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥éœ€è¦åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚è¿™æ¶‰åŠä¸¤ä¸ªæ­¥éª¤ï¼š
- en: First, group the modelâ€™s parameters into two groups - one where weight decay
    should be applied, and the other one where it should not. Usually, biases and
    layer norm parameters are not weight decayed.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå°†æ¨¡å‹çš„å‚æ•°åˆ†ä¸ºä¸¤ç»„ - ä¸€ç»„åº”ç”¨æƒé‡è¡°å‡ï¼Œå¦ä¸€ç»„ä¸åº”ç”¨ã€‚é€šå¸¸ï¼Œåç½®å’Œå±‚å½’ä¸€åŒ–å‚æ•°ä¸ä¼šè¢«æƒé‡è¡°å‡ã€‚
- en: Then do some argument housekeeping to use the same parameters as the previously
    used AdamW optimizer.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åè¿›è¡Œä¸€äº›å‚æ•°æ•´ç†ï¼Œä»¥ä½¿ç”¨ä¸å…ˆå‰ä½¿ç”¨çš„AdamWä¼˜åŒ–å™¨ç›¸åŒçš„å‚æ•°ã€‚
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, pass the custom optimizer as an argument to the `Trainer`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå°†è‡ªå®šä¹‰ä¼˜åŒ–å™¨ä½œä¸ºå‚æ•°ä¼ é€’ç»™`Trainer`ï¼š
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Combined with other approaches (gradient accumulation, gradient checkpointing,
    and mixed precision training), you can expect to get about a 3x memory improvement
    and even slightly higher throughput as using Adafactor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆå…¶ä»–æ–¹æ³•ï¼ˆæ¢¯åº¦ç´¯ç§¯ã€æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼‰ï¼Œæ‚¨å¯ä»¥æœŸæœ›è·å¾—å¤§çº¦3å€çš„å†…å­˜æ”¹è¿›ï¼Œç”šè‡³æ¯”ä½¿ç”¨Adafactoræ—¶çš„ååé‡ç¨é«˜ã€‚
- en: multi_tensor
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: multi_tensor
- en: pytorch-nightly introduced `torch.optim._multi_tensor` which should significantly
    speed up the optimizers for situations with lots of small feature tensors. It
    should eventually become the default, but if you want to experiment with it sooner,
    take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: pytorch-nightlyå¼•å…¥äº†`torch.optim._multi_tensor`ï¼Œåº”è¯¥æ˜¾ç€åŠ å¿«å…·æœ‰å¤§é‡å°ç‰¹å¾å¼ é‡çš„ä¼˜åŒ–å™¨çš„é€Ÿåº¦ã€‚æœ€ç»ˆåº”è¯¥æˆä¸ºé»˜è®¤è®¾ç½®ï¼Œä½†å¦‚æœæ‚¨æƒ³æ›´æ—©å°è¯•å®ƒï¼Œè¯·æŸ¥çœ‹è¿™ä¸ªGitHub
    [é—®é¢˜](https://github.com/huggingface/transformers/issues/9965)ã€‚
- en: Data preloading
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®é¢„åŠ è½½
- en: 'One of the important requirements to reach great training speed is the ability
    to feed the GPU at the maximum speed it can handle. By default, everything happens
    in the main process, and it might not be able to read the data from disk fast
    enough, and thus create a bottleneck, leading to GPU under-utilization. Configure
    the following arguments to reduce the bottleneck:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¾åˆ°è‰¯å¥½è®­ç»ƒé€Ÿåº¦çš„ä¸€ä¸ªé‡è¦è¦æ±‚æ˜¯èƒ½å¤Ÿä»¥GPUèƒ½å¤Ÿå¤„ç†çš„æœ€å¤§é€Ÿåº¦æä¾›æ•°æ®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ“ä½œéƒ½åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼Œå¯èƒ½æ— æ³•å¿«é€Ÿä»ç£ç›˜è¯»å–æ•°æ®ï¼Œä»è€Œå¯¼è‡´ç“¶é¢ˆï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä¸è¶³ã€‚é…ç½®ä»¥ä¸‹å‚æ•°ä»¥å‡å°‘ç“¶é¢ˆï¼š
- en: '`DataLoader(pin_memory=True, ...)` - ensures the data gets preloaded into the
    pinned memory on CPU and typically leads to much faster transfers from CPU to
    GPU memory.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataLoader(pin_memory=True, ...)` - ç¡®ä¿æ•°æ®é¢„åŠ è½½åˆ°CPUä¸Šçš„å›ºå®šå†…å­˜ä¸­ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä»CPUåˆ°GPUå†…å­˜çš„ä¼ è¾“é€Ÿåº¦æ›´å¿«ã€‚'
- en: '`DataLoader(num_workers=4, ...)` - spawn several workers to preload data faster.
    During training, watch the GPU utilization stats; if itâ€™s far from 100%, experiment
    with increasing the number of workers. Of course, the problem could be elsewhere,
    so many workers wonâ€™t necessarily lead to better performance.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataLoader(num_workers=4, ...)` - ç”Ÿæˆå‡ ä¸ªå·¥ä½œè¿›ç¨‹ä»¥æ›´å¿«åœ°é¢„åŠ è½½æ•°æ®ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè§‚å¯ŸGPUåˆ©ç”¨ç‡ç»Ÿè®¡æ•°æ®ï¼›å¦‚æœè¿œç¦»100ï¼…ï¼Œå°è¯•å¢åŠ å·¥ä½œè¿›ç¨‹çš„æ•°é‡ã€‚å½“ç„¶ï¼Œé—®é¢˜å¯èƒ½å‡ºåœ¨å…¶ä»–åœ°æ–¹ï¼Œå› æ­¤è®¸å¤šå·¥ä½œè¿›ç¨‹ä¸ä¸€å®šä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚'
- en: 'When using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    the corresponding [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    are: `dataloader_pin_memory` (`True` by default), and `dataloader_num_workers`
    (defaults to `0`).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)æ—¶ï¼Œç›¸åº”çš„[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)æ˜¯ï¼š`dataloader_pin_memory`ï¼ˆé»˜è®¤ä¸º`True`ï¼‰ï¼Œå’Œ`dataloader_num_workers`ï¼ˆé»˜è®¤ä¸º`0`ï¼‰ã€‚
- en: DeepSpeed ZeRO
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepSpeed ZeRO
- en: DeepSpeed is an open-source deep learning optimization library that is integrated
    with ğŸ¤— Transformers and ğŸ¤— Accelerate. It provides a wide range of features and
    optimizations designed to improve the efficiency and scalability of large-scale
    deep learning training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeedæ˜¯ä¸€ä¸ªå¼€æºçš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œä¸ğŸ¤— Transformerså’ŒğŸ¤— Accelerateé›†æˆã€‚å®ƒæä¾›äº†å„ç§åŠŸèƒ½å’Œä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
- en: 'If your model fits onto a single GPU and you have enough space to fit a small
    batch size, you donâ€™t need to use DeepSpeed as itâ€™ll only slow things down. However,
    if the model doesnâ€™t fit onto a single GPU or you canâ€™t fit a small batch, you
    can leverage DeepSpeed ZeRO + CPU Offload, or NVMe Offload for much larger models.
    In this case, you need to separately [install the library](main_classes/deepspeed#installation),
    then follow one of the guides to create a configuration file and launch DeepSpeed:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„æ¨¡å‹é€‚åˆå•ä¸ªGPUå¹¶ä¸”æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥å®¹çº³å°æ‰¹é‡å¤§å°ï¼Œåˆ™ä¸éœ€è¦ä½¿ç”¨DeepSpeedï¼Œå› ä¸ºå®ƒåªä¼šå‡æ…¢é€Ÿåº¦ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ¨¡å‹æ— æ³•é€‚åº”å•ä¸ªGPUæˆ–æ— æ³•å®¹çº³å°æ‰¹é‡ï¼Œåˆ™å¯ä»¥åˆ©ç”¨DeepSpeed
    ZeRO + CPU Offloadï¼Œæˆ–NVMe Offloadæ¥å¤„ç†æ›´å¤§çš„æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦å•ç‹¬[å®‰è£…è¯¥åº“](main_classes/deepspeed#installation)ï¼Œç„¶åæŒ‰ç…§æŒ‡å—ä¹‹ä¸€åˆ›å»ºé…ç½®æ–‡ä»¶å¹¶å¯åŠ¨DeepSpeedï¼š
- en: For an in-depth guide on DeepSpeed integration with [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    review [the corresponding documentation](main_classes/deepspeed), specifically
    the [section for a single GPU](main_classes/deepspeed#deployment-with-one-gpu).
    Some adjustments are required to use DeepSpeed in a notebook; please take a look
    at the [corresponding guide](main_classes/deepspeed#deployment-in-notebooks).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰å…³DeepSpeedä¸[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)é›†æˆçš„è¯¦ç»†æŒ‡å—ï¼Œè¯·æŸ¥çœ‹[ç›¸åº”æ–‡æ¡£](main_classes/deepspeed)ï¼Œç‰¹åˆ«æ˜¯[å•ä¸ªGPUéƒ¨ç½²](main_classes/deepspeed#deployment-with-one-gpu)éƒ¨åˆ†ã€‚åœ¨ç¬”è®°æœ¬ä¸­ä½¿ç”¨DeepSpeedéœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ï¼›è¯·æŸ¥çœ‹[ç›¸åº”æŒ‡å—](main_classes/deepspeed#deployment-in-notebooks)ã€‚
- en: If you prefer to use ğŸ¤— Accelerate, refer to [ğŸ¤— Accelerate DeepSpeed guide](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ›´å–œæ¬¢ä½¿ç”¨ğŸ¤— Accelerateï¼Œè¯·å‚è€ƒ[ğŸ¤— Accelerate DeepSpeedæŒ‡å—](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed)ã€‚
- en: Using torch.compile
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨torch.compile
- en: 'PyTorch 2.0 introduced a new compile function that doesnâ€™t require any modification
    to existing PyTorch code but can optimize your code by adding a single line of
    code: `model = torch.compile(model)`.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2.0å¼•å…¥äº†ä¸€ä¸ªæ–°çš„ç¼–è¯‘å‡½æ•°ï¼Œä¸éœ€è¦å¯¹ç°æœ‰çš„PyTorchä»£ç è¿›è¡Œä»»ä½•ä¿®æ”¹ï¼Œåªéœ€æ·»åŠ ä¸€è¡Œä»£ç å³å¯ä¼˜åŒ–æ‚¨çš„ä»£ç ï¼š`model = torch.compile(model)`ã€‚
- en: 'If using [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    you only need `to` pass the `torch_compile` option in the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œæ‚¨åªéœ€è¦åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­ä¼ é€’`torch_compile`é€‰é¡¹ï¼š
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`torch.compile` uses Pythonâ€™s frame evaluation API to automatically create
    a graph from existing PyTorch programs. After capturing the graph, different backends
    can be deployed to lower the graph to an optimized engine. You can find more details
    and benchmarks in [PyTorch documentation](https://pytorch.org/get-started/pytorch-2.0/).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.compile`ä½¿ç”¨Pythonçš„å¸§è¯„ä¼°APIè‡ªåŠ¨ä»ç°æœ‰çš„PyTorchç¨‹åºåˆ›å»ºå›¾ã€‚åœ¨æ•è·å›¾ä¹‹åï¼Œå¯ä»¥éƒ¨ç½²ä¸åŒçš„åç«¯ä»¥å°†å›¾é™ä½åˆ°ä¼˜åŒ–å¼•æ“ã€‚æ‚¨å¯ä»¥åœ¨[PyTorchæ–‡æ¡£](https://pytorch.org/get-started/pytorch-2.0/)ä¸­æ‰¾åˆ°æ›´å¤šè¯¦ç»†ä¿¡æ¯å’ŒåŸºå‡†æµ‹è¯•ã€‚'
- en: '`torch.compile` has a growing list of backends, which can be found in by calling
    `torchdynamo.list_backends()`, each of which with its optional dependencies.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.compile`æœ‰ä¸€ä¸ªä¸æ–­å¢é•¿çš„åç«¯åˆ—è¡¨ï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨`torchdynamo.list_backends()`æ‰¾åˆ°ï¼Œæ¯ä¸ªåç«¯éƒ½æœ‰å…¶å¯é€‰ä¾èµ–é¡¹ã€‚'
- en: 'Choose which backend to use by specifying it via `torch_compile_backend` in
    the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    Some of the most commonly used backends are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­æŒ‡å®š`torch_compile_backend`æ¥é€‰æ‹©è¦ä½¿ç”¨çš„åç«¯ã€‚ä¸€äº›æœ€å¸¸ç”¨çš„åç«¯åŒ…æ‹¬ï¼š
- en: '**Debugging backends**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**è°ƒè¯•åç«¯**ï¼š'
- en: '`dynamo.optimize("eager")` - Uses PyTorch to run the extracted GraphModule.
    This is quite useful in debugging TorchDynamo issues.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("eager")` - ä½¿ç”¨PyTorchè¿è¡Œæå–çš„GraphModuleã€‚è¿™åœ¨è°ƒè¯•TorchDynamoé—®é¢˜æ—¶éå¸¸æœ‰ç”¨ã€‚'
- en: '`dynamo.optimize("aot_eager")` - Uses AotAutograd with no compiler, i.e, just
    using PyTorch eager for the AotAutogradâ€™s extracted forward and backward graphs.
    This is useful for debugging, and unlikely to give speedups.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("aot_eager")` - ä½¿ç”¨AotAutogradè€Œä¸ä½¿ç”¨ç¼–è¯‘å™¨ï¼Œå³ä»…ä½¿ç”¨PyTorch eagerè¿›è¡ŒAotAutogradçš„æå–å‰å‘å’Œåå‘å›¾ã€‚è¿™å¯¹è°ƒè¯•å¾ˆæœ‰ç”¨ï¼Œä½†ä¸å¤ªå¯èƒ½æä¾›åŠ é€Ÿã€‚'
- en: '**Training & inference backends**:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå’Œæ¨ç†åç«¯**ï¼š'
- en: '`dynamo.optimize("inductor")` - Uses TorchInductor backend with AotAutograd
    and cudagraphs by leveraging codegened Triton kernels [Read more](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("inductor")` - ä½¿ç”¨TorchInductoråç«¯ï¼Œé€šè¿‡åˆ©ç”¨codegened Tritonå†…æ ¸å®ç°AotAutogradå’Œcudagraphsã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)'
- en: '`dynamo.optimize("nvfuser")` - nvFuser with TorchScript. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("nvfuser")` - nvFuserä¸TorchScriptã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
- en: '`dynamo.optimize("aot_nvfuser")` - nvFuser with AotAutograd. [Read more](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("aot_nvfuser")` - nvFuserä¸AotAutogradã€‚[é˜…è¯»æ›´å¤š](https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593)'
- en: '`dynamo.optimize("aot_cudagraphs")` - cudagraphs with AotAutograd. [Read more](https://github.com/pytorch/torchdynamo/pull/757)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("aot_cudagraphs")` - ä½¿ç”¨AotAutogradçš„cudagraphsã€‚[é˜…è¯»æ›´å¤š](https://github.com/pytorch/torchdynamo/pull/757)'
- en: '**Inference-only backend**s:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»…æ¨ç†åç«¯**ï¼š'
- en: '`dynamo.optimize("ofi")` - Uses Torchscript optimize_for_inference. [Read more](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("ofi")` - ä½¿ç”¨Torchscriptçš„optimize_for_inferenceã€‚[é˜…è¯»æ›´å¤š](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html)'
- en: '`dynamo.optimize("fx2trt")` - Uses NVIDIA TensorRT for inference optimizations.
    [Read more](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("fx2trt")` - ä½¿ç”¨NVIDIA TensorRTè¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚[é˜…è¯»æ›´å¤š](https://pytorch.org/TensorRT/tutorials/getting_started_with_fx_path.html)'
- en: '`dynamo.optimize("onnxrt")` - Uses ONNXRT for inference on CPU/GPU. [Read more](https://onnxruntime.ai/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("onnxrt")` - ä½¿ç”¨ONNXRTè¿›è¡ŒCPU/GPUæ¨ç†ã€‚[é˜…è¯»æ›´å¤š](https://onnxruntime.ai/)'
- en: '`dynamo.optimize("ipex")` - Uses IPEX for inference on CPU. [Read more](https://github.com/intel/intel-extension-for-pytorch)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dynamo.optimize("ipex")` - ä½¿ç”¨IPEXè¿›è¡ŒCPUæ¨ç†ã€‚[é˜…è¯»æ›´å¤š](https://github.com/intel/intel-extension-for-pytorch)'
- en: For an example of using `torch.compile` with ğŸ¤— Transformers, check out this
    [blog post on fine-tuning a BERT model for Text Classification using the newest
    PyTorch 2.0 features](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`torch.compile`ä¸ğŸ¤— Transformersçš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡[åšæ–‡ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨æœ€æ–°çš„PyTorch 2.0åŠŸèƒ½å¾®è°ƒBERTæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»](https://www.philschmid.de/getting-started-pytorch-2-0-transformers)
- en: Using ğŸ¤— PEFT
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— PEFT
- en: '[Parameter-Efficient Fine Tuning (PEFT)](https://huggingface.co/blog/peft)
    methods freeze the pretrained model parameters during fine-tuning and add a small
    number of trainable parameters (the adapters) on top of it.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰](https://huggingface.co/blog/peft)æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å™¨ï¼‰ã€‚'
- en: As a result the [memory associated to the optimizer states and gradients](https://huggingface.co/docs/transformers/model_memory_anatomy#anatomy-of-models-memory)
    are greatly reduced.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œ[ä¸ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ç›¸å…³çš„å†…å­˜](https://huggingface.co/docs/transformers/model_memory_anatomy#anatomy-of-models-memory)å¤§å¤§å‡å°‘ã€‚
- en: 'For example with a vanilla AdamW, the memory requirement for the optimizer
    state would be:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯¹äºæ™®é€šçš„AdamWï¼Œä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚å°†æ˜¯ï¼š
- en: 'fp32 copy of parameters: 4 bytes/param'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fp32å‚æ•°çš„å‰¯æœ¬ï¼š4å­—èŠ‚/å‚æ•°
- en: 'Momentum: 4 bytes/param'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨é‡ï¼š4å­—èŠ‚/å‚æ•°
- en: 'Variance: 4 bytes/param'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–¹å·®ï¼š4å­—èŠ‚/å‚æ•°
- en: Suppose a model with 7B parameters and 200 millions parameters injected with
    [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä¸€ä¸ªå…·æœ‰70äº¿å‚æ•°å’Œ2äº¿å‚æ•°æ³¨å…¥[ä½ç§©é€‚é…å™¨](https://huggingface.co/docs/peft/conceptual_guides/lora)çš„æ¨¡å‹ã€‚
- en: The memory requirement for the optimizer state of the plain model would be 12
    * 7 = 84 GB (assuming 7B trainable parameters).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æ™®é€šæ¨¡å‹çš„ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚å°†ä¸º12 * 7 = 84 GBï¼ˆå‡è®¾æœ‰7Bå¯è®­ç»ƒå‚æ•°ï¼‰ã€‚
- en: Adding Lora increases slightly the memory associated to the model weights and
    substantially decreases memory requirement for the optimizer state to 12 * 0.2
    = 2.4GB.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ Loraä¼šç•¥å¾®å¢åŠ ä¸æ¨¡å‹æƒé‡ç›¸å…³çš„å†…å­˜ï¼Œå¹¶å¤§å¹…å‡å°‘ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚è‡³12 * 0.2 = 2.4GBã€‚
- en: Read more about PEFT and its detailed usage in [the PEFT documentation](https://huggingface.co/docs/peft/)
    or [PEFT repository](https://github.com/huggingface/peft).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[PEFTæ–‡æ¡£](https://huggingface.co/docs/peft/)æˆ–[PEFTå­˜å‚¨åº“](https://github.com/huggingface/peft)ä¸­è¯¦ç»†äº†è§£PEFTåŠå…¶è¯¦ç»†ç”¨æ³•ã€‚
- en: Using ğŸ¤— Accelerate
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerate
- en: With [ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) you can use
    the above methods while gaining full control over the training loop and can essentially
    write the loop in pure PyTorch with some minor modifications.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index)å¯ä»¥åœ¨å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯çš„åŒæ—¶ä½¿ç”¨ä¸Šè¿°æ–¹æ³•ï¼Œå¹¶ä¸”åŸºæœ¬ä¸Šå¯ä»¥ä½¿ç”¨çº¯PyTorchç¼–å†™å¾ªç¯å¹¶è¿›è¡Œä¸€äº›å¾®å°ä¿®æ”¹ã€‚
- en: 'Suppose you have combined the methods in the [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    like so:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ‚¨å·²å°†[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)ä¸­çš„æ–¹æ³•ç»„åˆå¦‚ä¸‹ï¼š
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The full example training loop with ğŸ¤— Accelerate is only a handful of lines
    of code long:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateçš„å®Œæ•´ç¤ºä¾‹è®­ç»ƒå¾ªç¯åªæœ‰å‡ è¡Œä»£ç ï¼š
- en: '[PRE12]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).
    Then we can enable gradient checkpointing by calling the modelâ€™s [gradient_checkpointing_enable()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.gradient_checkpointing_enable)
    method. When we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator)
    we can specify if we want to use mixed precision training and it will take care
    of it for us in the `prepare` call. During the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare)
    call the dataloader will also be distributed across workers should we use multiple
    GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åŒ…è£…åœ¨[`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)ä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨æ¨¡å‹çš„[gradient_checkpointing_enable()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.gradient_checkpointing_enable)æ–¹æ³•å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ã€‚å½“æˆ‘ä»¬åˆå§‹åŒ–[`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator)æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ˜¯å¦è¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶ä¸”å®ƒå°†åœ¨`prepare`è°ƒç”¨ä¸­ä¸ºæˆ‘ä»¬å¤„ç†ã€‚åœ¨[`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare)è°ƒç”¨æœŸé—´ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨å¤šä¸ªGPUï¼Œæ•°æ®åŠ è½½å™¨ä¹Ÿå°†åˆ†å¸ƒåœ¨å·¥ä½œè¿›ç¨‹ä¹‹é—´ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ä¹‹å‰ç¤ºä¾‹ç›¸åŒçš„[8ä½ä¼˜åŒ–å™¨](#8-bit-adam)ã€‚
- en: 'Finally, we can add the main training loop. Note that the `backward` call is
    handled by ğŸ¤— Accelerate. We can also see how gradient accumulation works: we normalize
    the loss, so we get the average at the end of accumulation and once we have enough
    steps we run the optimization.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ ä¸»è¦çš„è®­ç»ƒå¾ªç¯ã€‚è¯·æ³¨æ„ï¼Œ`backward`è°ƒç”¨ç”±ğŸ¤— Accelerateå¤„ç†ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°æ¢¯åº¦ç´¯ç§¯çš„å·¥ä½œåŸç†ï¼šæˆ‘ä»¬è§„èŒƒåŒ–æŸå¤±ï¼Œå› æ­¤åœ¨ç´¯ç§¯ç»“æŸæ—¶è·å¾—å¹³å‡å€¼ï¼Œä¸€æ—¦æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ­¥éª¤ï¼Œæˆ‘ä»¬å°±è¿è¡Œä¼˜åŒ–ã€‚
- en: Implementing these optimization techniques with ğŸ¤— Accelerate only takes a handful
    of lines of code and comes with the benefit of more flexibility in the training
    loop. For a full documentation of all features have a look at the [Accelerate
    documentation](https://huggingface.co/docs/accelerate/index).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateå®ç°è¿™äº›ä¼˜åŒ–æŠ€æœ¯åªéœ€è¦å‡ è¡Œä»£ç ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå¾ªç¯ä¸­å…·æœ‰æ›´å¤§çš„çµæ´»æ€§ã€‚è¦æŸ¥çœ‹æ‰€æœ‰åŠŸèƒ½çš„å®Œæ•´æ–‡æ¡£ï¼Œè¯·æŸ¥çœ‹[Accelerateæ–‡æ¡£](https://huggingface.co/docs/accelerate/index)ã€‚
- en: Efficient Software Prebuilds
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ•ˆçš„è½¯ä»¶é¢„æ„å»º
- en: PyTorchâ€™s [pip and conda builds](https://pytorch.org/get-started/locally/#start-locally)
    come prebuilt with the cuda toolkit which is enough to run PyTorch, but it is
    insufficient if you need to build cuda extensions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchçš„[pipå’Œcondaæ„å»º](https://pytorch.org/get-started/locally/#start-locally)é¢„å…ˆæ„å»ºäº†cudaå·¥å…·åŒ…ï¼Œè¶³ä»¥è¿è¡ŒPyTorchï¼Œä½†å¦‚æœéœ€è¦æ„å»ºcudaæ‰©å±•ï¼Œåˆ™ä¸è¶³ã€‚
- en: At times, additional efforts may be required to pre-build some components. For
    instance, if youâ€™re using libraries like `apex` that donâ€™t come pre-compiled.
    In other situations figuring out how to install the right cuda toolkit system-wide
    can be complicated. To address these scenarios PyTorch and NVIDIA released a new
    version of NGC docker container which already comes with everything prebuilt.
    You just need to install your programs on it, and it will run out of the box.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„åŠªåŠ›æ¥é¢„æ„å»ºä¸€äº›ç»„ä»¶ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯æœªç»é¢„ç¼–è¯‘çš„åº“ï¼Œå¦‚`apex`ã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œå¼„æ¸…æ¥šå¦‚ä½•åœ¨ç³»ç»ŸèŒƒå›´å†…å®‰è£…æ­£ç¡®çš„cudaå·¥å…·åŒ…å¯èƒ½ä¼šå¾ˆå¤æ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æƒ…å†µï¼ŒPyTorchå’ŒNVIDIAå‘å¸ƒäº†ä¸€ä¸ªæ–°ç‰ˆæœ¬çš„NGC
    dockerå®¹å™¨ï¼Œå…¶ä¸­å·²ç»é¢„å…ˆæ„å»ºäº†ä¸€åˆ‡ã€‚æ‚¨åªéœ€åœ¨å…¶ä¸­å®‰è£…æ‚¨çš„ç¨‹åºï¼Œå®ƒå°±å¯ä»¥ç«‹å³è¿è¡Œã€‚
- en: This approach is also useful if you want to tweak the pytorch source and/or
    make a new customized build. To find the docker image version you want start [with
    PyTorch release notes](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/),
    choose one of the latest monthly releases. Go into the releaseâ€™s notes for the
    desired release, check that the environmentâ€™s components are matching your needs
    (including NVIDIA Driver requirements!) and then at the very top of that document
    go to the corresponding NGC page. If for some reason you get lost, here is [the
    index of all PyTorch NGC images](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•åœ¨æ‚¨æƒ³è¦è°ƒæ•´pytorchæºä»£ç å’Œ/æˆ–åˆ¶ä½œæ–°çš„å®šåˆ¶æ„å»ºæ—¶ä¹Ÿå¾ˆæœ‰ç”¨ã€‚è¦æ‰¾åˆ°æ‚¨æƒ³è¦çš„dockeré•œåƒç‰ˆæœ¬ï¼Œè¯·ä»[PyTorchå‘å¸ƒè¯´æ˜](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/)å¼€å§‹ï¼Œé€‰æ‹©æœ€æ–°çš„ä¸€ä¸ªæœˆå‘å¸ƒä¹‹ä¸€ã€‚è¿›å…¥æ‰€éœ€å‘å¸ƒçš„å‘å¸ƒè¯´æ˜ï¼Œæ£€æŸ¥ç¯å¢ƒçš„ç»„ä»¶æ˜¯å¦ç¬¦åˆæ‚¨çš„éœ€æ±‚ï¼ˆåŒ…æ‹¬NVIDIAé©±åŠ¨ç¨‹åºè¦æ±‚ï¼ï¼‰ï¼Œç„¶ååœ¨è¯¥æ–‡æ¡£çš„é¡¶éƒ¨è½¬åˆ°ç›¸åº”çš„NGCé¡µé¢ã€‚å¦‚æœç”±äºæŸç§åŸå› æ‚¨è¿·å¤±äº†æ–¹å‘ï¼Œè¿™é‡Œæ˜¯[æ‰€æœ‰PyTorch
    NGCé•œåƒçš„ç´¢å¼•](https://ngc.nvidia.com/catalog/containers/nvidia:pytorch)ã€‚
- en: Next follow the instructions to download and deploy the docker image.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æŒ‰ç…§è¯´æ˜ä¸‹è½½å’Œéƒ¨ç½²dockeré•œåƒã€‚
- en: Mixture of Experts
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸“å®¶æ··åˆ
- en: Some recent papers reported a 4-5x training speedup and a faster inference by
    integrating Mixture of Experts (MoE) into the Transformer models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æœ€è¿‘çš„è®ºæ–‡æŠ¥å‘Šäº†4-5å€çš„è®­ç»ƒåŠ é€Ÿå’Œå°†Mixture of Expertsï¼ˆMoEï¼‰é›†æˆåˆ°Transformeræ¨¡å‹ä¸­ä»¥å®ç°æ›´å¿«çš„æ¨ç†ã€‚
- en: Since it has been discovered that more parameters lead to better performance,
    this technique allows to increase the number of parameters by an order of magnitude
    without increasing training costs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå‘ç°æ›´å¤šçš„å‚æ•°ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ï¼Œè¿™ç§æŠ€æœ¯å…è®¸å°†å‚æ•°æ•°é‡å¢åŠ ä¸€ä¸ªæ•°é‡çº§ï¼Œè€Œä¸å¢åŠ è®­ç»ƒæˆæœ¬ã€‚
- en: In this approach every other FFN layer is replaced with a MoE Layer which consists
    of many experts, with a gated function that trains each expert in a balanced way
    depending on the input tokenâ€™s position in a sequence.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæ¯ä¸ªå…¶ä»–çš„FFNå±‚éƒ½è¢«ä¸€ä¸ªMoEå±‚æ›¿æ¢ï¼Œè¯¥å±‚ç”±è®¸å¤šä¸“å®¶ç»„æˆï¼Œå…·æœ‰ä¸€ä¸ªé—¨æ§å‡½æ•°ï¼Œæ ¹æ®è¾“å…¥ä»¤ç‰Œåœ¨åºåˆ—ä¸­çš„ä½ç½®å¹³è¡¡åœ°è®­ç»ƒæ¯ä¸ªä¸“å®¶ã€‚
- en: '![MoE Transformer 2x block](../Images/582afa9e1fb35b75b07f45d236dc9d35.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![MoE Transformer 2x block](../Images/582afa9e1fb35b75b07f45d236dc9d35.png)'
- en: '(source: [GLAM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html))'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (æ¥æºï¼š[GLAM](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html))
- en: You can find exhaustive details and comparison tables in the papers listed at
    the end of this section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨æœ¬èŠ‚æœ«å°¾åˆ—å‡ºçš„è®ºæ–‡ä¸­æ‰¾åˆ°è¯¦å°½çš„ç»†èŠ‚å’Œæ¯”è¾ƒè¡¨ã€‚
- en: The main drawback of this approach is that it requires staggering amounts of
    GPU memory - almost an order of magnitude larger than its dense equivalent. Various
    distillation and approaches are proposed to how to overcome the much higher memory
    requirements.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¸»è¦ç¼ºç‚¹æ˜¯å®ƒéœ€è¦å¤§é‡çš„GPUå†…å­˜ - å‡ ä¹æ¯”å…¶å¯†é›†ç­‰ä»·ç‰©å¤§ä¸€ä¸ªæ•°é‡çº§ã€‚æå‡ºäº†å„ç§è’¸é¦å’Œæ–¹æ³•ï¼Œä»¥å…‹æœæ›´é«˜çš„å†…å­˜éœ€æ±‚ã€‚
- en: There is direct trade-off though, you can use just a few experts with a 2-3x
    smaller base model instead of dozens or hundreds experts leading to a 5x smaller
    model and thus increase the training speed moderately while increasing the memory
    requirements moderately as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå­˜åœ¨ç›´æ¥çš„æƒè¡¡ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å°‘é‡ä¸“å®¶å’Œ2-3å€è¾ƒå°çš„åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯æ•°åæˆ–æ•°ç™¾ä¸ªä¸“å®¶ï¼Œä»è€Œå¯¼è‡´5å€è¾ƒå°çš„æ¨¡å‹ï¼Œå› æ­¤é€‚åº¦å¢åŠ è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶é€‚åº¦å¢åŠ å†…å­˜éœ€æ±‚ã€‚
- en: 'Most related papers and implementations are built around Tensorflow/TPUs:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç›¸å…³è®ºæ–‡å’Œå®ç°éƒ½æ˜¯å›´ç»•Tensorflow/TPUsæ„å»ºçš„ï¼š
- en: '[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GShard: ä½¿ç”¨æ¡ä»¶è®¡ç®—å’Œè‡ªåŠ¨åˆ†ç‰‡æ‰©å±•å·¨å‹æ¨¡å‹](https://arxiv.org/abs/2006.16668)'
- en: '[Switch Transformers: Scaling to Trillion Parameter Models with Simple and
    Efficient Sparsity](https://arxiv.org/abs/2101.03961)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Switch Transformers: Scaling to Trillion Parameter Models with Simple and
    Efficient Sparsity](https://arxiv.org/abs/2101.03961)'
- en: '[GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GLaM: Generalist Language Model (GLaM)](https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html)'
- en: 'And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing
    Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale](https://arxiv.org/abs/2201.05596),
    [Mixture of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) -
    blog posts: [1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/),
    [2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/)
    and specific deployment with large transformer-based natural language generation
    models: [blog post](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html),
    [Megatron-Deepspeed branch](https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºPytorchï¼ŒDeepSpeedä¹Ÿæ„å»ºäº†ä¸€ä¸ªï¼š[DeepSpeed-MoE: æ¨è¿›æ··åˆä¸“å®¶æ¨ç†å’Œè®­ç»ƒä»¥æ”¯æŒä¸‹ä¸€ä»£AIè§„æ¨¡](https://arxiv.org/abs/2201.05596)ï¼Œ[Mixture
    of Experts](https://www.deepspeed.ai/tutorials/mixture-of-experts/) - åšæ–‡ï¼š[1](https://www.microsoft.com/en-us/research/blog/deepspeed-powers-8x-larger-moe-model-training-with-high-performance/)ï¼Œ[2](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-moe-training-for-multitask-multilingual-models/)ä»¥åŠå¤§å‹åŸºäºTransformerçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„ç‰¹å®šéƒ¨ç½²ï¼š[åšæ–‡](https://www.deepspeed.ai/2021/12/09/deepspeed-moe-nlg.html)ï¼Œ[Megatron-Deepspeedåˆ†æ”¯](https://github.com/microsoft/Megatron-DeepSpeed/tree/moe-training)ã€‚'
- en: Using PyTorch native attention and Flash Attention
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PyTorchåŸç”Ÿæ³¨æ„åŠ›å’ŒFlash Attention
- en: PyTorch 2.0 released a native [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)
    (SDPA), that allows using fused GPU kernels such as [memory-efficient attention](https://arxiv.org/abs/2112.05682)
    and [flash attention](https://arxiv.org/abs/2205.14135).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2.0å‘å¸ƒäº†ä¸€ä¸ªåŸç”Ÿçš„[`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)ï¼ˆSDPAï¼‰ï¼Œå…è®¸ä½¿ç”¨èåˆçš„GPUå†…æ ¸ï¼Œå¦‚[å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›](https://arxiv.org/abs/2112.05682)å’Œ[é—ªå­˜æ³¨æ„åŠ›](https://arxiv.org/abs/2205.14135)ã€‚
- en: 'After installing the [`optimum`](https://github.com/huggingface/optimum) package,
    the relevant internal modules can be replaced to use PyTorchâ€™s native attention
    with:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å®‰è£…[`optimum`](https://github.com/huggingface/optimum)åŒ…åï¼Œå¯ä»¥æ›¿æ¢ç›¸å…³çš„å†…éƒ¨æ¨¡å—ä»¥ä½¿ç”¨PyTorchçš„åŸç”Ÿæ³¨æ„åŠ›ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Once converted, train the model as usual.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è½¬æ¢åï¼Œåƒå¾€å¸¸ä¸€æ ·è®­ç»ƒæ¨¡å‹ã€‚
- en: The PyTorch-native `scaled_dot_product_attention` operator can only dispatch
    to Flash Attention if no `attention_mask` is provided.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchåŸç”Ÿçš„`scaled_dot_product_attention`æ“ä½œç¬¦åªæœ‰åœ¨æ²¡æœ‰æä¾›`attention_mask`æ—¶æ‰èƒ½åˆ†æ´¾åˆ°Flash
    Attentionã€‚
- en: By default, in training mode, the BetterTransformer integration **drops the
    mask support and can only be used for training that does not require a padding
    mask for batched training**. This is the case, for example, during masked language
    modeling or causal language modeling. BetterTransformer is not suited for fine-tuning
    models on tasks that require a padding mask.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼ŒBetterTransformeré›†æˆ**å–æ¶ˆäº†æ©ç æ”¯æŒï¼Œåªèƒ½ç”¨äºä¸éœ€è¦å¡«å……æ©ç çš„æ‰¹é‡è®­ç»ƒ**ã€‚ä¾‹å¦‚ï¼Œåœ¨æ©ç è¯­è¨€å»ºæ¨¡æˆ–å› æœè¯­è¨€å»ºæ¨¡æœŸé—´ã€‚BetterTransformerä¸é€‚ç”¨äºéœ€è¦å¡«å……æ©ç çš„ä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹ã€‚
- en: Check out this [blogpost](https://pytorch.org/blog/out-of-the-box-acceleration/)
    to learn more about acceleration and memory-savings with SDPA.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹è¿™ç¯‡[åšæ–‡](https://pytorch.org/blog/out-of-the-box-acceleration/)ï¼Œäº†è§£æœ‰å…³SDPAåŠ é€Ÿå’ŒèŠ‚çœå†…å­˜çš„æ›´å¤šä¿¡æ¯ã€‚
