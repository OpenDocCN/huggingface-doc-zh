["```py\n>>> from transformers import MCTCTConfig, MCTCTModel\n\n>>> # Initializing a M-CTC-T mctct-large style configuration\n>>> configuration = MCTCTConfig()\n\n>>> # Initializing a model (with random weights) from the mctct-large style configuration\n>>> model = MCTCTModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoProcessor, MCTCTModel\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"speechbrain/m-ctc-t-large\")\n>>> model = MCTCTModel.from_pretrained(\"speechbrain/m-ctc-t-large\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n[1, 195, 1536]\n\n>>> inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n```", "```py\n>>> from transformers import AutoProcessor, MCTCTForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"speechbrain/m-ctc-t-large\")\n>>> model = MCTCTForCTC.from_pretrained(\"speechbrain/m-ctc-t-large\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n\"Mr. Quilter is the apostle of the middle classes, and we're glad to welcome his gospel.\"\n\n>>> inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n1885.65\n```"]