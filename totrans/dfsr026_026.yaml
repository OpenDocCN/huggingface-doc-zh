- en: Text-to-image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到图像
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: When you think of diffusion models, text-to-image is usually one of the first
    things that come to mind. Text-to-image generates an image from a text description
    (for example, “Astronaut in a jungle, cold color palette, muted colors, detailed,
    8k”) which is also known as a *prompt*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到扩散模型时，文本到图像通常是首先想到的。文本到图像从文本描述（例如，“太空人在丛林中，冷色调，柔和的颜色，详细，8k”）生成图像，这也被称为*提示*。
- en: From a very high level, a diffusion model takes a prompt and some random initial
    noise, and iteratively removes the noise to construct an image. The *denoising*
    process is guided by the prompt, and once the denoising process ends after a predetermined
    number of time steps, the image representation is decoded into an image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个非常高的层面来看，扩散模型接受一个提示和一些随机的初始噪声，并迭代地去除噪声以构建图像。*去噪*过程由提示引导，一旦去噪过程在预定数量的时间步骤之后结束，图像表示将被解码成图像。
- en: Read the [How does Stable Diffusion work?](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)
    blog post to learn more about how a latent diffusion model works.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读[稳定扩散是如何工作的？](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)博文，了解潜在扩散模型的工作原理。
- en: 'You can generate images from a prompt in 🤗 Diffusers in two steps:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在🤗 Diffusers中通过两个步骤从提示生成图像：
- en: 'Load a checkpoint into the [AutoPipelineForText2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image)
    class, which automatically detects the appropriate pipeline class to use based
    on the checkpoint:'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将检查点加载到[AutoPipelineForText2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image)类中，该类会根据检查点自动检测要使用的适当管道类：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Pass a prompt to the pipeline to generate an image:'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提示传递给管道以生成图像：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/3d5a1c2477f576fbe2f04b0abeeb5cae.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d5a1c2477f576fbe2f04b0abeeb5cae.png)'
- en: Popular models
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 热门模型
- en: The most common text-to-image models are [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5),
    [Stable Diffusion XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0),
    and [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder).
    There are also ControlNet models or adapters that can be used with text-to-image
    models for more direct control in generating images. The results from each model
    are slightly different because of their architecture and training process, but
    no matter which model you choose, their usage is more or less the same. Let’s
    use the same prompt for each model and compare their results.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的文本到图像模型是[稳定扩散 v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)、[稳定扩散
    XL（SDXL）](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和[坎丁斯基
    2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)。还有ControlNet模型或适配器，可以与文本到图像模型一起使用，以更直接地控制生成图像。由于它们的架构和训练过程不同，每个模型的结果略有不同，但无论选择哪个模型，它们的使用方式基本相同。让我们为每个模型使用相同的提示并比较它们的结果。
- en: Stable Diffusion v1.5
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稳定扩散 v1.5
- en: '[Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    is a latent diffusion model initialized from [Stable Diffusion v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4),
    and finetuned for 595K steps on 512x512 images from the LAION-Aesthetics V2 dataset.
    You can use this model like:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[稳定扩散 v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)是一个从[稳定扩散
    v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)初始化的潜在扩散模型，并在LAION-Aesthetics
    V2数据集的512x512图像上进行了595K步的微调。您可以像这样使用这个模型：'
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Stable Diffusion XL
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稳定扩散 XL
- en: 'SDXL is a much larger version of the previous Stable Diffusion models, and
    involves a two-stage model process that adds even more details to an image. It
    also includes some additional *micro-conditionings* to generate high-quality images
    centered subjects. Take a look at the more comprehensive [SDXL](sdxl) guide to
    learn more about how to use it. In general, you can use SDXL like:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL是之前稳定扩散模型的一个更大版本，涉及一个两阶段模型过程，为图像添加了更多细节。它还包括一些额外的*微调*来生成以中心主题为中心的高质量图像。查看更全面的[SDXL](sdxl)指南，了解如何使用它。一般来说，您可以像这样使用SDXL：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Kandinsky 2.2
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 坎丁斯基 2.2
- en: The Kandinsky model is a bit different from the Stable Diffusion models because
    it also uses an image prior model to create embeddings that are used to better
    align text and images in the diffusion model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 坎丁斯基模型与稳定扩散模型有些不同，因为它还使用图像先验模型来创建嵌入，这些嵌入用于更好地对齐扩散模型中的文本和图像。
- en: 'The easiest way to use Kandinsky 2.2 is:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用坎丁斯基 2.2的最简单方法是：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ControlNet
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ControlNet
- en: ControlNet models are auxiliary models or adapters that are finetuned on top
    of text-to-image models, such as [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5).
    Using ControlNet models in combination with text-to-image models offers diverse
    options for more explicit control over how to generate an image. With ControlNet,
    you add an additional conditioning input image to the model. For example, if you
    provide an image of a human pose (usually represented as multiple keypoints that
    are connected into a skeleton) as a conditioning input, the model generates an
    image that follows the pose of the image. Check out the more in-depth [ControlNet](controlnet)
    guide to learn more about other conditioning inputs and how to use them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet模型是辅助模型或适配器，它们在文本到图像模型的基础上进行微调，例如[稳定扩散 v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)。将ControlNet模型与文本到图像模型结合使用，可以提供更多明确控制生成图像的选项。使用ControlNet，您可以向模型添加额外的条件输入图像。例如，如果您提供一个人体姿势的图像（通常表示为连接成骨架的多个关键点）作为条件输入，模型将生成遵循图像姿势的图像。查看更详细的[ControlNet](controlnet)指南，了解其他条件输入以及如何使用它们。
- en: 'In this example, let’s condition the ControlNet with a human pose estimation
    image. Load the ControlNet model pretrained on human pose estimations:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Pass the `controlnet` to the [AutoPipelineForText2Image](/docs/diffusers/v0.26.3/en/api/pipelines/auto_pipeline#diffusers.AutoPipelineForText2Image),
    and provide the prompt and pose estimation image:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/d2b7e6b0b8516e613654db5d9e482041.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Stable Diffusion v1.5
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47eb848e4e09fd8ca2c920b1f98d089d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Stable Diffusion XL
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c09e0df13fba3dbf95d5232315d36eb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Kandinsky 2.2
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3657c31e95dc399792fc8b0ee81679f4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: ControlNet (pose conditioning)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Configure pipeline parameters
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of parameters that can be configured in the pipeline that
    affect how an image is generated. You can change the image’s output size, specify
    a negative prompt to improve image quality, and more. This section dives deeper
    into how to use these parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Height and width
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `height` and `width` parameters control the height and width (in pixels)
    of the generated image. By default, the Stable Diffusion v1.5 model outputs 512x512
    images, but you can change this to any size that is a multiple of 8\. For example,
    to create a rectangular image:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/84e161b40900b68ec614371d04cb4eb1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Other models may have different default image sizes depending on the image sizes
    in the training dataset. For example, SDXL’s default image size is 1024x1024 and
    using lower `height` and `width` values may result in lower quality images. Make
    sure you check the model’s API reference first!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Guidance scale
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `guidance_scale` parameter affects how much the prompt influences image
    generation. A lower value gives the model “creativity” to generate images that
    are more loosely related to the prompt. Higher `guidance_scale` values push the
    model to follow the prompt more closely, and if this value is too high, you may
    observe some artifacts in the generated image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/d4e1ac912506a0937e8776bcaf804434.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 2.5
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd6081df4d5f8e0649e3668778fa3eab.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 7.5
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3741374b77d721ec460507012c586ea0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: guidance_scale = 10.5
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Negative prompt
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just like how a prompt guides generation, a *negative prompt* steers the model
    away from things you don’t want the model to generate. This is commonly used to
    improve overall image quality by removing poor or bad image features such as “low
    resolution” or “bad details”. You can also use a negative prompt to remove or
    modify the content and style of an image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/841a0429e6f0e2536bdabb5aa8e25a23.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9695ff0c8decf5141f5f125a58a6ec4c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: negative_prompt = "astronaut"
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html#generator)
    object enables reproducibility in a pipeline by setting a manual seed. You can
    use a `Generator` to generate batches of images and iteratively improve on an
    image generated from a seed as detailed in the [Improve image quality with deterministic
    generation](reusing_seeds) guide.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: You can set a seed and `Generator` as shown below. Creating an image with a
    `Generator` should return the same result each time instead of randomly generating
    a new image.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Control image generation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several ways to exert more control over how an image is generated
    outside of configuring a pipeline’s parameters, such as prompt weighting and ControlNet
    models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Prompt weighting
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt weighting is a technique for increasing or decreasing the importance
    of concepts in a prompt to emphasize or minimize certain features in an image.
    We recommend using the [Compel](https://github.com/damian0815/compel) library
    to help you generate the weighted prompt embeddings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to create the prompt embeddings in the [Prompt weighting](weighted_prompts)
    guide. This example focuses on how to use the prompt embeddings in the pipeline.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve created the embeddings, you can pass them to the `prompt_embeds`
    (and `negative_prompt_embeds` if you’re using a negative prompt) parameter in
    the pipeline.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建嵌入后，您可以将它们传递给管道中的`prompt_embeds`（如果您使用负提示，则还有`negative_prompt_embeds`参数）。
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ControlNet
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ControlNet
- en: As you saw in the [ControlNet](#controlnet) section, these models offer a more
    flexible and accurate way to generate images by incorporating an additional conditioning
    image input. Each ControlNet model is pretrained on a particular type of conditioning
    image to generate new images that resemble it. For example, if you take a ControlNet
    model pretrained on depth maps, you can give the model a depth map as a conditioning
    input and it’ll generate an image that preserves the spatial information in it.
    This is quicker and easier than specifying the depth information in a prompt.
    You can even combine multiple conditioning inputs with a [MultiControlNet](controlnet#multicontrolnet)!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ControlNet](#controlnet)部分中，正如您所看到的，这些模型通过引入额外的条件图像输入，提供了一种更灵活和准确的生成图像的方式。每个ControlNet模型都是在特定类型的条件图像上预训练的，以生成类似的新图像。例如，如果您使用一个在深度图上预训练的ControlNet模型，您可以将深度图作为条件输入给模型，它将生成保留其中空间信息的图像。这比在提示中指定深度信息更快更容易。您甚至可以将多个条件输入与[MultiControlNet](controlnet#multicontrolnet)结合使用！
- en: There are many types of conditioning inputs you can use, and 🤗 Diffusers supports
    ControlNet for Stable Diffusion and SDXL models. Take a look at the more comprehensive
    [ControlNet](controlnet) guide to learn how you can use these models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用许多类型的条件输入，并且🤗 Diffusers支持Stable Diffusion和SDXL模型的ControlNet。查看更全面的[ControlNet](controlnet)指南，了解如何使用这些模型。
- en: Optimize
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: Diffusion models are large, and the iterative nature of denoising an image is
    computationally expensive and intensive. But this doesn’t mean you need access
    to powerful - or even many - GPUs to use them. There are many optimization techniques
    for running diffusion models on consumer and free-tier resources. For example,
    you can load model weights in half-precision to save GPU memory and increase speed
    or offload the entire model to the GPU to save even more memory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型很大，对图像进行去噪的迭代性质在计算上是昂贵且密集的。但这并不意味着您需要访问强大的 - 或者甚至很多 - GPU来使用它们。有许多优化技术可用于在消费者和免费资源上运行扩散模型。例如，您可以将模型权重加载为半精度以节省GPU内存并提高速度，或者将整个模型卸载到GPU以节省更多内存。
- en: 'PyTorch 2.0 also supports a more memory-efficient attention mechanism called
    [*scaled dot product attention*](../optimization/torch2.0#scaled-dot-product-attention)
    that is automatically enabled if you’re using PyTorch 2.0\. You can combine this
    with [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    to speed your code up even more:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 2.0还支持一种更节省内存的注意机制，称为[*缩放点积注意*](../optimization/torch2.0#scaled-dot-product-attention)，如果您使用PyTorch
    2.0，则会自动启用。您可以将其与[`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)结合使用，以进一步加快代码速度：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For more tips on how to optimize your code to save memory and speed up inference,
    read the [Memory and speed](../optimization/fp16) and [Torch 2.0](../optimization/torch2.0)
    guides.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何优化代码以节省内存和加快推理速度的更多提示，请阅读[内存和速度](../optimization/fp16)和[Torch 2.0](../optimization/torch2.0)指南。
