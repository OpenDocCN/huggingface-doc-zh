- en: Quicktour
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨
- en: 'Original text: [https://huggingface.co/docs/diffusers/quicktour](https://huggingface.co/docs/diffusers/quicktour)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/quicktour](https://huggingface.co/docs/diffusers/quicktour)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models are trained to denoise random Gaussian noise step-by-step to
    generate a sample of interest, such as an image or audio. This has sparked a tremendous
    amount of interest in generative AI, and you have probably seen examples of diffusion
    generated images on the internet. ğŸ§¨ Diffusers is a library aimed at making diffusion
    models widely accessible to everyone.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹è¢«è®­ç»ƒä¸ºé€æ­¥å»å™ªéšæœºé«˜æ–¯å™ªå£°ï¼Œä»¥ç”Ÿæˆæ„Ÿå…´è¶£çš„æ ·æœ¬ï¼Œå¦‚å›¾åƒæˆ–éŸ³é¢‘ã€‚è¿™å¼•å‘äº†å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æå¤§å…´è¶£ï¼Œæ‚¨å¯èƒ½åœ¨äº’è”ç½‘ä¸Šçœ‹åˆ°è¿‡æ‰©æ•£ç”Ÿæˆçš„å›¾åƒç¤ºä¾‹ã€‚ğŸ§¨
    Diffusers æ˜¯ä¸€ä¸ªæ—¨åœ¨ä½¿æ‰©æ•£æ¨¡å‹æ™®éå¯è®¿é—®çš„åº“ã€‚
- en: 'Whether youâ€™re a developer or an everyday user, this quicktour will introduce
    you to ğŸ§¨ Diffusers and help you get up and generating quickly! There are three
    main components of the library to know about:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºæ‚¨æ˜¯å¼€å‘äººå‘˜è¿˜æ˜¯æ—¥å¸¸ç”¨æˆ·ï¼Œè¿™ä¸ªå¿«é€Ÿå…¥é—¨å°†å‘æ‚¨ä»‹ç» ğŸ§¨ Diffusers å¹¶å¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ç”Ÿæˆï¼äº†è§£è¯¥åº“çš„ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
- en: The [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    is a high-level end-to-end class designed to rapidly generate samples from pretrained
    diffusion models for inference.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    æ˜¯ä¸€ä¸ªé«˜çº§ç«¯åˆ°ç«¯ç±»ï¼Œæ—¨åœ¨å¿«é€Ÿä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆæ ·æœ¬ä»¥è¿›è¡Œæ¨æ–­ã€‚'
- en: Popular pretrained [model](./api/models) architectures and modules that can
    be used as building blocks for creating diffusion systems.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æµè¡Œçš„é¢„è®­ç»ƒ [æ¨¡å‹](./api/models) æ¶æ„å’Œæ¨¡å—å¯ç”¨ä½œåˆ›å»ºæ‰©æ•£ç³»ç»Ÿçš„æ„å»ºå—ã€‚
- en: Many different [schedulers](./api/schedulers/overview) - algorithms that control
    how noise is added for training, and how to generate denoised images during inference.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¸å¤šä¸åŒçš„ [è°ƒåº¦ç¨‹åº](./api/schedulers/overview) - æ§åˆ¶è®­ç»ƒæ—¶å¦‚ä½•æ·»åŠ å™ªå£°ä»¥åŠæ¨æ–­æ—¶å¦‚ä½•ç”Ÿæˆå»å™ªå›¾åƒçš„ç®—æ³•ã€‚
- en: The quicktour will show you how to use the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    for inference, and then walk you through how to combine a model and scheduler
    to replicate whatâ€™s happening inside the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    è¿›è¡Œæ¨æ–­ï¼Œç„¶åæŒ‡å¯¼æ‚¨å¦‚ä½•ç»„åˆæ¨¡å‹å’Œè°ƒåº¦ç¨‹åºä»¥å¤åˆ¶ [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    å†…éƒ¨å‘ç”Ÿçš„æƒ…å†µã€‚
- en: The quicktour is a simplified version of the introductory ğŸ§¨ Diffusers [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)
    to help you get started quickly. If you want to learn more about ğŸ§¨ Diffusersâ€™
    goal, design philosophy, and additional details about its core API, check out
    the notebook!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¿«é€Ÿå…¥é—¨æ˜¯ç®€åŒ–çš„ ğŸ§¨ Diffusers å…¥é—¨ç¬”è®°æœ¬ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿå…¥é—¨ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äº ğŸ§¨ Diffusers çš„ç›®æ ‡ã€è®¾è®¡ç†å¿µä»¥åŠæœ‰å…³å…¶æ ¸å¿ƒ API
    çš„å…¶ä»–è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ç¬”è®°æœ¬ï¼
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) speeds up model
    loading for inference and training.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) å¯åŠ å¿«æ¨¡å‹åŠ è½½çš„é€Ÿåº¦ï¼Œç”¨äºæ¨æ–­å’Œè®­ç»ƒã€‚'
- en: '[ğŸ¤— Transformers](https://huggingface.co/docs/transformers/index) is required
    to run the most popular diffusion models, such as [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¦è¿è¡Œæœ€å—æ¬¢è¿çš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚ [ç¨³å®šæ‰©æ•£](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)ï¼Œéœ€è¦
    [ğŸ¤— Transformers](https://huggingface.co/docs/transformers/index)ã€‚
- en: DiffusionPipeline
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DiffusionPipeline
- en: The [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    is the easiest way to use a pretrained diffusion system for inference. It is an
    end-to-end system containing the model and the scheduler. You can use the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    out-of-the-box for many tasks. Take a look at the table below for some supported
    tasks, and for a complete list of supported tasks, check out the [ğŸ§¨ Diffusers
    Summary](./api/pipelines/overview#diffusers-summary) table.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£ç³»ç»Ÿè¿›è¡Œæ¨æ–­çš„æœ€ç®€å•æ–¹å¼ã€‚å®ƒæ˜¯ä¸€ä¸ªåŒ…å«æ¨¡å‹å’Œè°ƒåº¦ç¨‹åºçš„ç«¯åˆ°ç«¯ç³»ç»Ÿã€‚æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    æ¥æ‰§è¡Œè®¸å¤šä»»åŠ¡ã€‚æŸ¥çœ‹ä¸‹é¢çš„è¡¨æ ¼ä»¥äº†è§£ä¸€äº›æ”¯æŒçš„ä»»åŠ¡ï¼Œä»¥åŠå®Œæ•´æ”¯æŒä»»åŠ¡åˆ—è¡¨ï¼Œè¯·æŸ¥çœ‹ [ğŸ§¨ Diffusers æ¦‚è¦](./api/pipelines/overview#diffusers-summary)
    è¡¨ã€‚'
- en: '| **Task** | **Description** | **Pipeline** |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **ä»»åŠ¡** | **æè¿°** | **ç®¡é“** |'
- en: '| --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Unconditional Image Generation | generate an image from Gaussian noise |
    [unconditional_image_generation](./using-diffusers/unconditional_image_generation)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| æ— æ¡ä»¶å›¾åƒç”Ÿæˆ | ä»é«˜æ–¯å™ªå£°ç”Ÿæˆå›¾åƒ | [æ— æ¡ä»¶å›¾åƒç”Ÿæˆ](./using-diffusers/unconditional_image_generation)
    |'
- en: '| Text-Guided Image Generation | generate an image given a text prompt | [conditional_image_generation](./using-diffusers/conditional_image_generation)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆ | æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒ | [æœ‰æ¡ä»¶å›¾åƒç”Ÿæˆ](./using-diffusers/conditional_image_generation)
    |'
- en: '| Text-Guided Image-to-Image Translation | adapt an image guided by a text
    prompt | [img2img](./using-diffusers/img2img) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒè½¬æ¢ | æ ¹æ®æ–‡æœ¬æç¤ºè°ƒæ•´å›¾åƒ | [img2img](./using-diffusers/img2img) |'
- en: '| Text-Guided Image-Inpainting | fill the masked part of an image given the
    image, the mask and a text prompt | [inpaint](./using-diffusers/inpaint) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æœ¬å¼•å¯¼çš„å›¾åƒä¿®å¤ | æ ¹æ®å›¾åƒã€è’™ç‰ˆå’Œæ–‡æœ¬æç¤ºå¡«å……å›¾åƒçš„é®ç½©éƒ¨åˆ† | [inpaint](./using-diffusers/inpaint)
    |'
- en: '| Text-Guided Depth-to-Image Translation | adapt parts of an image guided by
    a text prompt while preserving structure via depth estimation | [depth2img](./using-diffusers/depth2img)
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| æ–‡æœ¬å¼•å¯¼çš„æ·±åº¦åˆ°å›¾åƒè½¬æ¢ | åœ¨ä¿ç•™ç»“æ„çš„åŒæ—¶ï¼Œé€šè¿‡æ·±åº¦ä¼°è®¡è°ƒæ•´å›¾åƒçš„éƒ¨åˆ†ï¼Œç”±æ–‡æœ¬æç¤ºå¼•å¯¼ | [depth2img](./using-diffusers/depth2img)
    |'
- en: Start by creating an instance of a [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    and specify which pipeline checkpoint you would like to download. You can use
    the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    for any [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads)
    stored on the Hugging Face Hub. In this quicktour, youâ€™ll load the [`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5)
    checkpoint for text-to-image generation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: For [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion) models,
    please carefully read the [license](https://huggingface.co/spaces/CompVis/stable-diffusion-license)
    first before running the model. ğŸ§¨ Diffusers implements a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py)
    to prevent offensive or harmful content, but the modelâ€™s improved image generation
    capabilities can still produce potentially harmful content.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the model with the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained)
    method:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    downloads and caches all modeling, tokenization, and scheduling components. Youâ€™ll
    see that the Stable Diffusion pipeline is composed of the [UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)
    and [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    among other things:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We strongly recommend running the pipeline on a GPU because the model consists
    of roughly 1.4 billion parameters. You can move the generator object to a GPU,
    just like you would in PyTorch:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now you can pass a text prompt to the `pipeline` to generate an image, and then
    access the denoised image. By default, the image output is wrapped in a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class)
    object.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/020c32038fcd5d69660e0f7072208150.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: 'Save the image by calling `save`:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Local pipeline
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can also use the pipeline locally. The only difference is you need to download
    the weights first:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then load the saved weights into the pipeline:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, you can run the pipeline as you would in the section above.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Swapping schedulers
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different schedulers come with different denoising speeds and quality trade-offs.
    The best way to find out which one works best for you is to try them out! One
    of the main features of ğŸ§¨ Diffusers is to allow you to easily switch between schedulers.
    For example, to replace the default [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)
    with the [EulerDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/euler#diffusers.EulerDiscreteScheduler),
    load it with the [from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)
    method:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Try generating an image with the new scheduler and see if you notice a difference!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, youâ€™ll take a closer look at the components - the model
    and scheduler - that make up the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    and learn how to use these components to generate an image of a cat.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Models
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most models take a noisy sample, and at each timestep it predicts the *noise
    residual* (other models learn to predict the previous sample directly or the velocity
    or [`v-prediction`](https://github.com/huggingface/diffusers/blob/5e5ce13e2f89ac45a0066cb3f369462a3cf1d9ef/src/diffusers/schedulers/scheduling_ddim.py#L110)),
    the difference between a less noisy image and the input image. You can mix and
    match models to create other diffusion systems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Models are initiated with the [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)
    method which also locally caches the model weights so it is faster the next time
    you load the model. For the quicktour, youâ€™ll load the [UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel),
    a basic unconditional image generation model with a checkpoint trained on cat
    images:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ˜¯ä½¿ç”¨[from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)æ–¹æ³•åˆå§‹åŒ–çš„ï¼Œè¯¥æ–¹æ³•è¿˜ä¼šæœ¬åœ°ç¼“å­˜æ¨¡å‹æƒé‡ï¼Œå› æ­¤ä¸‹æ¬¡åŠ è½½æ¨¡å‹æ—¶ä¼šæ›´å¿«ã€‚å¯¹äºå¿«é€Ÿå…¥é—¨ï¼Œæ‚¨å°†åŠ è½½[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¸¦æœ‰åœ¨çŒ«å›¾åƒä¸Šè®­ç»ƒçš„æ£€æŸ¥ç‚¹ï¼š
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To access the model parameters, call `model.config`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¿é—®æ¨¡å‹å‚æ•°ï¼Œè¯·è°ƒç”¨`model.config`ï¼š
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The model configuration is a ğŸ§Š frozen ğŸ§Š dictionary, which means those parameters
    canâ€™t be changed after the model is created. This is intentional and ensures that
    the parameters used to define the model architecture at the start remain the same,
    while other parameters can still be adjusted during inference.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹é…ç½®æ˜¯ä¸€ä¸ªğŸ§Šå†»ç»“ğŸ§Šå­—å…¸ï¼Œè¿™æ„å‘³ç€åœ¨åˆ›å»ºæ¨¡å‹åæ— æ³•æ›´æ”¹è¿™äº›å‚æ•°ã€‚è¿™æ˜¯æœ‰æ„ä¸ºä¹‹çš„ï¼Œå¯ä»¥ç¡®ä¿åœ¨å¼€å§‹å®šä¹‰æ¨¡å‹æ¶æ„æ—¶ä½¿ç”¨çš„å‚æ•°ä¿æŒä¸å˜ï¼Œè€Œå…¶ä»–å‚æ•°ä»ç„¶å¯ä»¥åœ¨æ¨æ–­è¿‡ç¨‹ä¸­è¿›è¡Œè°ƒæ•´ã€‚
- en: 'Some of the most important parameters are:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æœ€é‡è¦çš„å‚æ•°æ˜¯ï¼š
- en: '`sample_size`: the height and width dimension of the input sample.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`ï¼šè¾“å…¥æ ·æœ¬çš„é«˜åº¦å’Œå®½åº¦ç»´åº¦ã€‚'
- en: '`in_channels`: the number of input channels of the input sample.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`ï¼šè¾“å…¥æ ·æœ¬çš„è¾“å…¥é€šé“æ•°ã€‚'
- en: '`down_block_types` and `up_block_types`: the type of down- and upsampling blocks
    used to create the UNet architecture.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types`å’Œ`up_block_types`ï¼šç”¨äºåˆ›å»ºUNetæ¶æ„çš„ä¸‹é‡‡æ ·å’Œä¸Šé‡‡æ ·å—çš„ç±»å‹ã€‚'
- en: '`block_out_channels`: the number of output channels of the downsampling blocks;
    also used in reverse order for the number of input channels of the upsampling
    blocks.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels`ï¼šä¸‹é‡‡æ ·å—çš„è¾“å‡ºé€šé“æ•°ï¼›ä¹Ÿä»¥ç›¸åé¡ºåºç”¨äºä¸Šé‡‡æ ·å—çš„è¾“å…¥é€šé“æ•°ã€‚'
- en: '`layers_per_block`: the number of ResNet blocks present in each UNet block.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_block`ï¼šæ¯ä¸ªUNetå—ä¸­å­˜åœ¨çš„ResNetå—çš„æ•°é‡ã€‚'
- en: 'To use the model for inference, create the image shape with random Gaussian
    noise. It should have a `batch` axis because the model can receive multiple random
    noises, a `channel` axis corresponding to the number of input channels, and a
    `sample_size` axis for the height and width of the image:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†æ¨¡å‹ç”¨äºæ¨æ–­ï¼Œè¯·ä½¿ç”¨éšæœºé«˜æ–¯å™ªå£°åˆ›å»ºå›¾åƒå½¢çŠ¶ã€‚å®ƒåº”è¯¥æœ‰ä¸€ä¸ª`batch`è½´ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥æ¥æ”¶å¤šä¸ªéšæœºå™ªå£°ï¼Œä¸€ä¸ª`channel`è½´å¯¹åº”äºè¾“å…¥é€šé“çš„æ•°é‡ï¼Œä»¥åŠä¸€ä¸ª`sample_size`è½´ç”¨äºå›¾åƒçš„é«˜åº¦å’Œå®½åº¦ï¼š
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For inference, pass the noisy image and a `timestep` to the model. The `timestep`
    indicates how noisy the input image is, with more noise at the beginning and less
    at the end. This helps the model determine its position in the diffusion process,
    whether it is closer to the start or the end. Use the `sample` method to get the
    model output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¨æ–­ï¼Œå°†å˜ˆæ‚å›¾åƒå’Œä¸€ä¸ª`timestep`ä¼ é€’ç»™æ¨¡å‹ã€‚`timestep`æŒ‡ç¤ºè¾“å…¥å›¾åƒæœ‰å¤šå˜ˆæ‚ï¼Œå¼€å§‹æ—¶æ›´å¤šå™ªå£°ï¼Œç»“æŸæ—¶æ›´å°‘å™ªå£°ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹ç¡®å®šå…¶åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä½ç½®ï¼Œæ— è®ºæ˜¯é è¿‘å¼€å§‹è¿˜æ˜¯ç»“æŸã€‚ä½¿ç”¨`sample`æ–¹æ³•è·å–æ¨¡å‹è¾“å‡ºï¼š
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To generate actual examples though, youâ€™ll need a scheduler to guide the denoising
    process. In the next section, youâ€™ll learn how to couple a model with a scheduler.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¦ç”Ÿæˆå®é™…ç¤ºä¾‹ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªè°ƒåº¦å™¨æ¥æŒ‡å¯¼å»å™ªè¿‡ç¨‹ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•å°†æ¨¡å‹ä¸è°ƒåº¦å™¨é…å¯¹ã€‚
- en: Schedulers
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨
- en: Schedulers manage going from a noisy sample to a less noisy sample given the
    model output - in this case, it is the `noisy_residual`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒåº¦å™¨ç®¡ç†ä»å˜ˆæ‚çš„æ ·æœ¬åˆ°è¾ƒå°‘å˜ˆæ‚çš„æ ·æœ¬çš„è½¬å˜ï¼Œç»™å®šæ¨¡å‹è¾“å‡º - åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒæ˜¯`noisy_residual`ã€‚
- en: ğŸ§¨ Diffusers is a toolbox for building diffusion systems. While the [DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)
    is a convenient way to get started with a pre-built diffusion system, you can
    also choose your own model and scheduler components separately to build a custom
    diffusion system.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ§¨ Diffusersæ˜¯ä¸€ä¸ªæ„å»ºæ‰©æ•£ç³»ç»Ÿçš„å·¥å…·ç®±ã€‚è™½ç„¶[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„æ–¹å¼æ¥å¼€å§‹ä½¿ç”¨é¢„æ„å»ºçš„æ‰©æ•£ç³»ç»Ÿï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹å’Œè°ƒåº¦å™¨ç»„ä»¶åˆ†åˆ«æ„å»ºè‡ªå®šä¹‰æ‰©æ•£ç³»ç»Ÿã€‚
- en: 'For the quicktour, youâ€™ll instantiate the [DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)
    with its [from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)
    method:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¿«é€Ÿå…¥é—¨ï¼Œæ‚¨å°†ä½¿ç”¨å…¶[from_config()](/docs/diffusers/v0.26.3/en/api/configuration#diffusers.ConfigMixin.from_config)æ–¹æ³•å®ä¾‹åŒ–[DDPMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler)ï¼š
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ğŸ’¡ Unlike a model, a scheduler does not have trainable weights and is parameter-free!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä¸æ¨¡å‹ä¸åŒï¼Œè°ƒåº¦å™¨æ²¡æœ‰å¯è®­ç»ƒçš„æƒé‡ï¼Œä¹Ÿæ²¡æœ‰å‚æ•°ï¼
- en: 'Some of the most important parameters are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›æœ€é‡è¦çš„å‚æ•°æ˜¯ï¼š
- en: '`num_train_timesteps`: the length of the denoising process or, in other words,
    the number of timesteps required to process random Gaussian noise into a data
    sample.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_train_timesteps`ï¼šå»å™ªè¿‡ç¨‹çš„é•¿åº¦ï¼Œæˆ–è€…æ¢å¥è¯è¯´ï¼Œå¤„ç†éšæœºé«˜æ–¯å™ªå£°æˆä¸ºæ•°æ®æ ·æœ¬æ‰€éœ€çš„æ—¶é—´æ­¥æ•°ã€‚'
- en: '`beta_schedule`: the type of noise schedule to use for inference and training.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta_schedule`ï¼šç”¨äºæ¨æ–­å’Œè®­ç»ƒçš„å™ªå£°è®¡åˆ’ç±»å‹ã€‚'
- en: '`beta_start` and `beta_end`: the start and end noise values for the noise schedule.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta_start`å’Œ`beta_end`ï¼šå™ªå£°è®¡åˆ’çš„èµ·å§‹å’Œç»“æŸå™ªå£°å€¼ã€‚'
- en: 'To predict a slightly less noisy image, pass the following to the schedulerâ€™s
    [step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)
    method: model output, `timestep`, and current `sample`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é¢„æµ‹ä¸€ä¸ªç¨å¾®ä¸é‚£ä¹ˆå˜ˆæ‚çš„å›¾åƒï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹ä¼ é€’ç»™è°ƒåº¦å™¨çš„[step()](/docs/diffusers/v0.26.3/en/api/schedulers/ddpm#diffusers.DDPMScheduler.step)æ–¹æ³•ï¼šæ¨¡å‹è¾“å‡ºï¼Œ`timestep`å’Œå½“å‰`sample`ã€‚
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `less_noisy_sample` can be passed to the next `timestep` where itâ€™ll get
    even less noisy! Letâ€™s bring it all together now and visualize the entire denoising
    process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`less_noisy_sample`å¯ä»¥ä¼ é€’åˆ°ä¸‹ä¸€ä¸ª`timestep`ï¼Œåœ¨é‚£é‡Œå®ƒä¼šå˜å¾—æ›´å°‘å˜ˆæ‚ï¼ç°åœ¨è®©æˆ‘ä»¬æŠŠæ‰€æœ‰å†…å®¹æ±‡æ€»èµ·æ¥ï¼Œå¯è§†åŒ–æ•´ä¸ªå»å™ªè¿‡ç¨‹ã€‚'
- en: 'First, create a function that postprocesses and displays the denoised image
    as a `PIL.Image`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªåå¤„ç†å’Œæ˜¾ç¤ºå»å™ªå›¾åƒçš„å‡½æ•°ä½œä¸º`PIL.Image`ï¼š
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To speed up the denoising process, move the input and model to a GPU:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºåŠ å¿«å»å™ªè¿‡ç¨‹ï¼Œè¯·å°†è¾“å…¥å’Œæ¨¡å‹ç§»è‡³GPUï¼š
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now create a denoising loop that predicts the residual of the less noisy sample,
    and computes the less noisy sample with the scheduler:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åˆ›å»ºä¸€ä¸ªå»å™ªå¾ªç¯ï¼Œé¢„æµ‹è¾ƒå°‘å™ªéŸ³æ ·æœ¬çš„æ®‹å·®ï¼Œå¹¶ä½¿ç”¨è°ƒåº¦å™¨è®¡ç®—è¾ƒå°‘å™ªéŸ³çš„æ ·æœ¬ï¼š
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Sit back and watch as a cat is generated from nothing but noise! ğŸ˜»
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åä¸‹æ¥çœ‹ç€ä¸€åªçŒ«ä»ä¸€æ— æ‰€æœ‰çš„å™ªéŸ³ä¸­ç”Ÿæˆï¼ğŸ˜»
- en: '![](../Images/e6bad1468d511bff5946306330accda8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6bad1468d511bff5946306330accda8.png)'
- en: Next steps
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥
- en: 'Hopefully, you generated some cool images with ğŸ§¨ Diffusers in this quicktour!
    For your next steps, you can:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æ‚¨åœ¨è¿™æ¬¡å¿«é€Ÿæµè§ˆä¸­ä½¿ç”¨ğŸ§¨ Diffusers ç”Ÿæˆäº†ä¸€äº›é…·ç‚«çš„å›¾åƒï¼æ¥ä¸‹æ¥ï¼Œæ‚¨å¯ä»¥ï¼š
- en: Train or finetune a model to generate your own images in the [training](./tutorials/basic_training)
    tutorial.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[è®­ç»ƒ](./tutorials/basic_training)æ•™ç¨‹ä¸­è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ä»¥ç”Ÿæˆæ‚¨è‡ªå·±çš„å›¾åƒã€‚
- en: See example official and community [training or finetuning scripts](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples)
    for a variety of use cases.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹å®˜æ–¹å’Œç¤¾åŒºçš„[è®­ç»ƒæˆ–å¾®è°ƒè„šæœ¬ç¤ºä¾‹](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples)ï¼Œé€‚ç”¨äºå„ç§ç”¨ä¾‹ã€‚
- en: Learn more about loading, accessing, changing, and comparing schedulers in the
    [Using different Schedulers](./using-diffusers/schedulers) guide.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[ä½¿ç”¨ä¸åŒè°ƒåº¦å™¨](./using-diffusers/schedulers)æŒ‡å—ä¸­äº†è§£æ›´å¤šå…³äºåŠ è½½ã€è®¿é—®ã€æ›´æ”¹å’Œæ¯”è¾ƒè°ƒåº¦å™¨çš„ä¿¡æ¯ã€‚
- en: Explore prompt engineering, speed and memory optimizations, and tips and tricks
    for generating higher-quality images with the [Stable Diffusion](./stable_diffusion)
    guide.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢å¿«é€Ÿå·¥ç¨‹ã€é€Ÿåº¦å’Œå†…å­˜ä¼˜åŒ–ï¼Œä»¥åŠä½¿ç”¨[ç¨³å®šæ‰©æ•£](./stable_diffusion)æŒ‡å—ç”Ÿæˆæ›´é«˜è´¨é‡å›¾åƒçš„æŠ€å·§å’Œçªé—¨ã€‚
- en: Dive deeper into speeding up ğŸ§¨ Diffusers with guides on [optimized PyTorch on
    a GPU](./optimization/fp16), and inference guides for running [Stable Diffusion
    on Apple Silicon (M1/M2)](./optimization/mps) and [ONNX Runtime](./optimization/onnx).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·±å…¥äº†è§£å¦‚ä½•é€šè¿‡[ä¼˜åŒ–PyTorchåœ¨GPUä¸Š](./optimization/fp16)çš„æŒ‡å—ä»¥åŠåœ¨Apple Siliconï¼ˆM1/M2ï¼‰ä¸Šè¿è¡Œ[ç¨³å®šæ‰©æ•£](./optimization/mps)å’Œ[ONNX
    Runtime](./optimization/onnx)çš„æ¨ç†æŒ‡å—æ¥åŠ å¿«ğŸ§¨ Diffusersçš„é€Ÿåº¦ã€‚
