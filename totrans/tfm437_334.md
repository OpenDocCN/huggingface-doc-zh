# BLIP

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/blip)

## 概述

BLIP模型是由Junnan Li、Dongxu Li、Caiming Xiong和Steven Hoi在[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)中提出的。

BLIP是一个能够执行各种多模态任务的模型，包括：

+   视觉问答

+   图像-文本检索（图像-文本匹配）

+   图像字幕生成

论文摘要如下：

*视觉-语言预训练（VLP）已经提高了许多视觉-语言任务的性能。然而，大多数现有的预训练模型只擅长于理解型任务或生成型任务。此外，性能的提升主要是通过扩大数据集规模，使用从网络收集的带有噪声的图像-文本对来实现的，这是一种次优的监督来源。在本文中，我们提出了BLIP，一种新的VLP框架，可以灵活地转移到视觉-语言理解和生成任务。BLIP通过引导标题来有效利用嘈杂的网络数据，其中一个标题生成器生成合成标题，一个过滤器去除噪声标题。我们在各种视觉-语言任务上取得了最先进的结果，如图像-文本检索（平均召回率@1提高了2.7%）、图像字幕生成（CIDEr提高了2.8%）和VQA（VQA分数提高了1.6%）。BLIP还展示了在直接转移到视频-语言任务时的强大泛化能力。代码、模型和数据集已发布。*

![BLIP.gif](../Images/1467bbd9717edbf9c1a760c470789799.png)

该模型由[ybelkada](https://huggingface.co/ybelkada)贡献。原始代码可在[此处](https://github.com/salesforce/BLIP)找到。

## 资源

+   [Jupyter笔记本](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)，介绍如何在自定义数据集上对BLIP进行图像字幕微调

## BlipConfig

### `class transformers.BlipConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/configuration_blip.py#L280)

```py
( text_config = None vision_config = None projection_dim = 512 logit_scale_init_value = 2.6592 image_text_hidden_size = 256 **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化[BlipTextConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextConfig)的配置选项字典。

+   `vision_config` (`dict`, *optional*) — 用于初始化[BlipVisionConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionConfig)的配置选项字典。

+   `projection_dim` (`int`, *optional*, 默认为512) — 文本和视觉投影层的维度。

+   `logit_scale_init_value` (`float`, *optional*, 默认为2.6592) — *logit_scale* 参数的初始值。默认值与原始BLIP实现相同。

+   `image_text_hidden_size` (`int`, *optional*, 默认为256) — 图像文本融合层隐藏状态的维度。

+   `kwargs` (*optional*) — 关键字参数的字典。

[BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)是存储[BlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipModel)配置的配置类。它用于根据指定的参数实例化一个BLIP模型，定义文本模型和视觉模型配置。使用默认值实例化配置将产生类似于BLIP-base [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import BlipConfig, BlipModel

>>> # Initializing a BlipConfig with Salesforce/blip-vqa-base style configuration
>>> configuration = BlipConfig()

>>> # Initializing a BlipPModel (with random weights) from the Salesforce/blip-vqa-base style configuration
>>> model = BlipModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config

>>> # We can also initialize a BlipConfig from a BlipTextConfig and a BlipVisionConfig

>>> # Initializing a BLIPText and BLIPVision configuration
>>> config_text = BlipTextConfig()
>>> config_vision = BlipVisionConfig()

>>> config = BlipConfig.from_text_vision_configs(config_text, config_vision)
```

#### `from_text_vision_configs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/configuration_blip.py#L359)

```py
( text_config: BlipTextConfig vision_config: BlipVisionConfig **kwargs ) → export const metadata = 'undefined';BlipConfig
```

返回

[BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)

配置对象的一个实例

从 blip 文本模型配置和 blip 视觉模型配置实例化一个 [BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)（或派生类）。

## BlipTextConfig

### `class transformers.BlipTextConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/configuration_blip.py#L46)

```py
( vocab_size = 30524 hidden_size = 768 encoder_hidden_size = 768 intermediate_size = 3072 projection_dim = 768 num_hidden_layers = 12 num_attention_heads = 8 max_position_embeddings = 512 hidden_act = 'gelu' layer_norm_eps = 1e-12 hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 bos_token_id = 30522 eos_token_id = 2 pad_token_id = 0 sep_token_id = 102 is_decoder = True use_cache = True **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, 默认为30524) — `Blip` 文本模型的词汇量。定义了在调用 [BlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipModel) 时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化器层的维度。

+   `encoder_hidden_size` (`int`, *optional*, 默认为768) — 来自视觉模型的编码器层的维度。

+   `intermediate_size` (`int`, *optional*, 默认为3072) — Transformer 编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer 编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, 默认为8) — Transformer 编码器中每个注意力层的注意力头数量。

+   `max_position_embeddings` (`int`, *optional*, 默认为512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024 或 2048）。

+   `hidden_act` (`str` 或 `function`, *optional*, 默认为 `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"selu"` 和 `"gelu_new"` `"gelu"`。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的 epsilon。

+   `hidden_dropout_prob` (`float`, *optional*, 默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的丢弃概率。

+   `attention_dropout` (`float`, *optional*, 默认为0.0) — 注意力概率的丢弃比率。

+   `initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `bos_token_id` (`int`, *optional*, 默认为30522) — `beginning-of-sequence` 标记的 id。

+   `eos_token_id` (`int`, *optional*, 默认为2) — `end-of-sequence` 标记的 id。

+   `pad_token_id` (`int`, *optional*, 默认为0) — `padding` 标记的 id。

+   `sep_token_id` (`int`, *optional*, 默认为102) — `separator` 标记的 id。

+   `is_decoder` (`bool`, *optional*, 默认为 `True`) — 模型是否用作解码器。

+   `use_cache` (`bool`, *optional*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

这是用于存储 [BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel) 配置的配置类。它用于根据指定的参数实例化一个 BLIP 文本模型，定义模型架构。使用默认值实例化配置将产生类似于由 [base architectures](https://huggingface.co/Salesforce/blip-vqa-base) 使用的 `BlipText` 的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import BlipTextConfig, BlipTextModel

>>> # Initializing a BlipTextConfig with Salesforce/blip-vqa-base style configuration
>>> configuration = BlipTextConfig()

>>> # Initializing a BlipTextModel (with random weights) from the Salesforce/blip-vqa-base style configuration
>>> model = BlipTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## BlipVisionConfig

### `class transformers.BlipVisionConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/configuration_blip.py#L181)

```py
( hidden_size = 768 intermediate_size = 3072 projection_dim = 512 num_hidden_layers = 12 num_attention_heads = 12 image_size = 384 patch_size = 16 hidden_act = 'gelu' layer_norm_eps = 1e-05 attention_dropout = 0.0 initializer_range = 1e-10 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化层的维度。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — 每个注意力层中的注意力头数。

+   `image_size` (`int`, *optional*, defaults to 384) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 16) — 每个patch的大小（分辨率）。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的epsilon。

+   `attention_dropout` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 1e-10) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

这是用于存储[BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel)配置的配置类。它用于根据指定的参数实例化BLIP视觉模型，定义模型架构。实例化默认配置将产生类似于Blip-base [Salesforce/blip-vqa-base](https://huggingface.co/Salesforce/blip-vqa-base)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import BlipVisionConfig, BlipVisionModel

>>> # Initializing a BlipVisionConfig with Salesforce/blip-vqa-base style configuration
>>> configuration = BlipVisionConfig()

>>> # Initializing a BlipVisionModel (with random weights) from the Salesforce/blip-vqa-base style configuration
>>> model = BlipVisionModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## BlipProcessor

### `class transformers.BlipProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/processing_blip.py#L27)

```py
( image_processor tokenizer )
```

参数

+   `image_processor` (`BlipImageProcessor`) — 一个[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)的实例。图像处理器是必需的输入。

+   `tokenizer` (`BertTokenizerFast`) — 一个[‘BertTokenizerFast`]的实例。此tokenizer是必需的输入。

构建一个BLIP处理器，将BERT tokenizer和BLIP图像处理器包装成单个处理器。

[BlipProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor) 提供了[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)和[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipProcessor.decode)的文档字符串。

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/processing_blip.py#L132)

```py
( *args **kwargs )
```

此方法将其所有参数转发给BertTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参阅此方法的文档字符串。

#### `decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/processing_blip.py#L139)

```py
( *args **kwargs )
```

此方法将所有参数转发给BertTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。请参考此方法的文档字符串以获取更多信息。

## BlipImageProcessor

### `class transformers.BlipImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/image_processing_blip.py#L45)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_convert_rgb: bool = True **kwargs )
```

参数

+   `do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`参数覆盖。

+   `size` (`dict`, *可选*, 默认为 `{"height" -- 384, "width": 384}`): 调整大小后的输出图像尺寸。可以被`preprocess`方法中的`size`参数覆盖。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BICUBIC`) — 如果调整图像大小，则要使用的重采样滤波器。仅在`do_resize`设置为`True`时才有效。可以被`preprocess`方法中的`resample`参数覆盖。

+   `do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按指定比例`rescale_factor`对图像进行重新缩放。可以被`preprocess`方法中的`do_rescale`参数覆盖。

+   `rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则要使用的比例因子。仅在`do_rescale`设置为`True`时才有效。可以被`preprocess`方法中的`rescale_factor`参数覆盖。

+   `do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`参数覆盖。可以被`preprocess`方法中的`do_normalize`参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_MEAN`) — 在归一化图像时使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_STD`) — 在归一化图像时使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_std`参数覆盖。可以被`preprocess`方法中的`image_std`参数覆盖。

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为RGB。

构建一个BLIP图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/image_processing_blip.py#L158)

```py
( images: Union do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_tensors: Union = None do_convert_rgb: bool = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望传入像素值范围为0到255的单个图像或图像批次。如果传入像素值在0到1之间的图像，请设置`do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 控制`resize`后图像的大小。图像的最短边被调整为`size["shortest_edge"]`，同时保持纵横比。如果此调整后图像的最长边 > `int(size["shortest_edge"] * (1333 / 800))`，则再次调整图像大小，使最长边等于`int(size["shortest_edge"] * (1333 / 800))`。

+   `resample` (`PILImageResampling`, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。仅在`do_resize`设置为`True`时才有效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放在[0 - 1]之间。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果`do_rescale`设置为`True`，则要按照此比例因子重新缩放图像。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 如果 `do_normalize` 设置为 `True`，用于归一化图像的图像均值。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 如果 `do_normalize` 设置为 `True`，用于归一化图像的图像标准差。

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `self.do_convert_rgb`) — 是否将图像转换为 RGB。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回一个类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回一个类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`: 返回一个类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`: 返回一个类型为 `jax.numpy.ndarray` 的批次。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像格式为 (通道数, 高度, 宽度)。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像格式为 (高度, 宽度, 通道数)。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像格式为 (高度, 宽度)。

预处理图像或图像批次。

Pytorch隐藏 Pytorch 内容

## BlipModel

### `class transformers.BlipModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L715)

```py
( config: BlipConfig )
```

参数

+   `config` ([BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L826)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None position_ids: Optional = None return_loss: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下，提供填充将被忽略。

    可以使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) 获取索引。有关详细信息，请参阅 `BlipProcessor.__call__()`。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。选择在 `[0, 1]` 中的掩码值：

    +   1 代表未被“掩码”的标记，

    +   0 代表被“掩码”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    [什么是位置 ID？](../glossary#position-ids)

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。默认情况下会忽略填充。可以使用 [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor) 获取像素值。详细信息请参见 [BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss` (`bool`，*可选*) — 是否返回对比损失。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

`transformers.models.blip.modeling_blip.BlipOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.blip.modeling_blip.BlipOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置 (`<class 'transformers.models.blip.configuration_blip.BlipConfig'>`) 和输入而异的各种元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当 `return_loss` 为 `True` 时返回) — 图像-文本相似性的对比损失。

+   `logits_per_image:(torch.FloatTensor`，形状为 `(image_batch_size, text_batch_size)`) — `image_embeds` 和 `text_embeds` 之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(torch.FloatTensor`，形状为 `(text_batch_size, image_batch_size)`) — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表文本-图像相似性分数。

+   `text_embeds(torch.FloatTensor`，形状为 `(batch_size, output_dim`) — 通过将投影层应用于 [BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel) 的池化输出获得的文本嵌入。

+   `image_embeds(torch.FloatTensor`，形状为 `(batch_size, output_dim`) — 通过将投影层应用于 [BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel) 的池化输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel) 的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel) 的输出。

[BlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, BlipModel

>>> model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L751)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';text_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下会忽略填充。 

    可以使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)获取索引。有关详细信息，请参阅`BlipProcessor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1表示`未被掩盖`的标记，

    +   0表示`被掩盖`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

text_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`

通过将投影层应用于[BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel)的池化输出获得的文本嵌入。

[BlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, BlipModel

>>> model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
>>> text_features = model.get_text_features(**inputs)
```

`get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L789)

```py
( pixel_values: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';image_features (torch.FloatTensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features（形状为`(batch_size, output_dim)`的`torch.FloatTensor`

通过将投影层应用于[BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel)的池化输出获得的图像嵌入。

[BlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, BlipModel

>>> model = BlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="pt")

>>> image_features = model.get_image_features(**inputs)
```

## BlipTextModel

### `class transformers.BlipTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip_text.py#L567)

```py
( config add_pooling_layer = True )
```

该模型可以作为编码器（仅具有自注意力）以及解码器运行，此时在自注意力层之间添加了一层交叉注意力，遵循[Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，Lukasz Kaiser和Illia Polosukhin所描述的架构](https://arxiv.org/abs/1706.03762)。参数和`is_decoder`设置为`True`；然后预期将`encoder_hidden_states`作为输入传递给前向传递。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip_text.py#L666)

```py
( input_ids: Optional = None attention_mask: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None is_decoder: Optional = False )
```

encoder_hidden_states（`torch.FloatTensor`，*可选*）：编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。encoder_attention_mask（`torch.FloatTensor`，*可选*）：避免对编码器输入的填充标记索引执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。掩码值选择在`[0, 1]`中：

+   对于未被`masked`的标记为1，

+   对于被`masked`的标记为0。past_key_values（`tuple(tuple(torch.FloatTensor))`，*可选*）：包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（这些没有将其过去的键值状态提供给此模型的）形状为`(batch_size, 1)`的而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。use_cache（`bool`，*可选*）：如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

## BlipVisionModel

### `class transformers.BlipVisionModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L649)

```py
( config: BlipVisionConfig )
```

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L664)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`）和输入的不同元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态序列。

+   `pooler_output` (`torch.FloatTensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记（分类标记）的最后一层隐藏状态，经过用于辅助预训练任务的层进一步处理后的结果。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

## BlipForConditionalGeneration

### `class transformers.BlipForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L920)

```py
( config: BlipConfig )
```

参数

+   `config` ([BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

用于图像字幕的BLIP模型。该模型由视觉编码器和文本解码器组成。可以选择向模型传递`input_ids`，作为文本提示，以使文本解码器继续提示。否则，解码器将从[BOS]（序列开始）标记开始生成文本。将从文本输入开始生成标题。如果未提供文本输入，则解码器将仅从[BOS]标记开始。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L950)

```py
( pixel_values: FloatTensor input_ids: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipForConditionalGenerationModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

`transformers.models.blip.modeling_blip.BlipForConditionalGenerationModelOutput` 或 `torch.FloatTensor` 元组

一个 `transformers.models.blip.modeling_blip.BlipForConditionalGenerationModelOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`）和输入的不同元素。

+   `loss` (`torch.FloatTensor`, *optional*, 当提供 `labels` 时返回，形状为 `(1,)` 的 `torch.FloatTensor`) — 文本解码器的语言建模损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.vocab_size)`，*optional*) — 文本解码器模型的语言建模头的预测分数。

+   `image_embeds` (`torch.FloatTensor`，形状为 `(batch_size, output_dim)`，*optional*) — 将 Vision Transformer 模型应用于输入图像后获得的图像嵌入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*optional*) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`) — `torch.FloatTensor` 元组（如果模型有嵌入层，则为嵌入的输出 + 每个层的输出）的形状为 `(batch_size, sequence_length, hidden_size)`。

    模型在每个层的输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True` 时返回) — `torch.FloatTensor` 元组（每个层一个）的形状为 `(batch_size, num_heads, sequence_length, sequence_length)`。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

[BlipForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipForConditionalGeneration) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, BlipForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
>>> model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "A picture of"

>>> inputs = processor(images=image, text=text, return_tensors="pt")

>>> outputs = model(**inputs)
```

## BlipForImageTextRetrieval

### `class transformers.BlipForImageTextRetrieval`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L1315)

```py
( config: BlipConfig )
```

参数

+   `config` ([BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

带有视觉和文本投影仪以及顶部分类头的 BLIP 模型。该模型用于图像文本检索的上下文。给定一张图像和一段文本，模型返回文本与图像相关的概率。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

`forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L1359)

```py
( input_ids: LongTensor pixel_values: FloatTensor use_itm_head: Optional = True attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipTextVisionModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height, width)`） — 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参见[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput` 或 `tuple(torch.FloatTensor)`

一个`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*optional*，当提供`labels`时返回） — 文本解码器的语言建模损失。

+   `image_embeds` (`torch.FloatTensor`，形状为`(batch_size, output_dim)` *optional*，当模型使用`with_projection=True`初始化时返回） — 通过将投影层应用于pooler_output获得的图像嵌入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`） — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[BlipForImageTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipForImageTextRetrieval)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前和运行后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, BlipForImageTextRetrieval

>>> model = BlipForImageTextRetrieval.from_pretrained("Salesforce/blip-itm-base-coco")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-itm-base-coco")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "an image of a cat"

>>> inputs = processor(images=image, text=text, return_tensors="pt")
>>> outputs = model(**inputs)
```

## BlipForQuestionAnswering

### `class transformers.BlipForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L1093)

```py
( config: BlipConfig )
```

参数

+   `config`（[BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

用于视觉问答的BLIP模型。该模型包括一个视觉编码器、一个文本编码器以及一个文本解码器。视觉编码器将对输入图像进行编码，文本编码器将对输入问题进行编码，并与图像的编码一起进行编码，文本解码器将输出问题的答案。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_blip.py#L1123)

```py
( input_ids: LongTensor pixel_values: FloatTensor decoder_input_ids: Optional = None decoder_attention_mask: Optional = None attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_blip.BlipTextVisionModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.blip.modeling_blip.BlipTextVisionModelOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`）和输入的不同元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）- 来自文本解码器的语言建模损失。

+   `image_embeds`（形状为`(batch_size, output_dim)`的`torch.FloatTensor` *可选*，当使用`with_projection=True`初始化模型时返回）- 通过将投影层应用于pooler_output获得的图像嵌入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层，则为嵌入的输出+每个层的输出）。

    模型在每个层的输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[BlipForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, BlipForQuestionAnswering

>>> model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # training
>>> text = "How many cats are in the picture?"
>>> label = "2"
>>> inputs = processor(images=image, text=text, return_tensors="pt")
>>> labels = processor(text=label, return_tensors="pt").input_ids

>>> inputs["labels"] = labels
>>> outputs = model(**inputs)
>>> loss = outputs.loss
>>> loss.backward()

>>> # inference
>>> text = "How many cats are in the picture?"
>>> inputs = processor(images=image, text=text, return_tensors="pt")
>>> outputs = model.generate(**inputs)
>>> print(processor.decode(outputs[0], skip_special_tokens=True))
2
```

TensorFlow隐藏TensorFlow内容

## TFBlipModel

### `class transformers.TFBlipModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L941)

```py
( config: BlipConfig *inputs **kwargs )
```

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L959)

```py
( input_ids: tf.Tensor | None = None pixel_values: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_loss: Optional[bool] = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`） — 输入序列标记在词汇表中的索引。默认情况下，如果提供了填充，将忽略填充。

    可以使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)获取索引。有关详细信息，请参阅`BlipProcessor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*） — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   1表示`未被掩码`的标记，

    +   0表示`被掩码`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*） — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`） — 像素值。默认情况下，如果提供了填充，将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `return_loss`（`bool`，*可选*） — 是否返回对比损失。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

返回

`transformers.models.blip.modeling_tf_blip.TFBlipOutput`或`tuple(tf.Tensor)`

一个`transformers.models.blip.modeling_tf_blip.TFBlipOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipConfig'>`）和输入的各种元素。

+   `损失`（形状为`(1,)`的`tf.Tensor`，*可选*，当`return_loss`为`True`时返回） — 图像-文本相似性的对比损失。

+   `logits_per_image:(tf.Tensor`形状为`(image_batch_size, text_batch_size)`) — `image_embeds`和`text_embeds`之间的缩放点积分数。这代表图像-文本相似性分数。

+   `logits_per_text:(tf.Tensor` 的形状为 `(text_batch_size, image_batch_size)`) — `text_embeds` 和 `image_embeds` 之间的缩放点积分数。这代表了文本-图像相似性分数。

+   `text_embeds(tf.Tensor` 的形状为 `(batch_size, output_dim`) — 通过将投影层应用于[BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel)的汇聚输出获得的文本嵌入。

+   `image_embeds(tf.Tensor` 的形状为 `(batch_size, output_dim`) — 通过将投影层应用于[BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel)的汇聚输出获得的图像嵌入。

+   `text_model_output(BaseModelOutputWithPooling):` [BlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipTextModel) 的输出。

+   `vision_model_output(BaseModelOutputWithPooling):` [BlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipVisionModel) 的输出。

[TFBlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFBlipModel

>>> model = TFBlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(
...     text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="tf", padding=True
... )

>>> outputs = model(**inputs)
>>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
>>> probs = tf.nn.softmax(logits_per_image, axis=1)  # we can take the softmax to get the label probabilities
```

#### `get_text_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1011)

```py
( input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None return_dict: Optional[bool] = None ) → export const metadata = 'undefined';text_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `input_ids` (`tf.Tensor` 的形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。默认情况下将忽略填充。

    可以使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)获得索引。有关详细信息，请参阅 `BlipProcessor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` 的形状为 `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 代表未被“masked”的标记，

    +   对于被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`tf.Tensor` 的形状为 `(batch_size, sequence_length)`, *可选*) — 每个输入序列标记的位置在位置嵌入中的索引。选在范围 `[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

text_features (`tf.Tensor` 的形状为 `(batch_size, output_dim`)

通过将投影层应用于[TFBlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipTextModel)的汇聚输出获得的文本嵌入。

[TFBlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, TFBlipModel

>>> model = TFBlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> inputs = processor(text=["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="tf")
>>> text_features = model.get_text_features(**inputs)
```

#### `get_image_features`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1049)

```py
( pixel_values: tf.Tensor | None = None return_dict: Optional[bool] = None ) → export const metadata = 'undefined';image_features (tf.Tensor of shape (batch_size, output_dim)
```

参数

+   `pixel_values` (`tf.Tensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

image_features (`tf.Tensor`，形状为`(batch_size, output_dim`)

通过将[TFBlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipVisionModel)的池化输出应用于投影层获得的图像嵌入。

[TFBlipModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipModel) 前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFBlipModel

>>> model = TFBlipModel.from_pretrained("Salesforce/blip-image-captioning-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(images=image, return_tensors="tf")

>>> image_features = model.get_image_features(**inputs)
```

## TFBlipTextModel

### `class transformers.TFBlipTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip_text.py#L728)

```py
( config add_pooling_layer = True name = None **kwargs )
```

该模型可以作为编码器（仅具有自注意力）以及解码器运行，此时在自注意力层之间添加了一层交叉注意力，遵循[Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser和Illia Polosukhin在《Attention is all you need》](https://arxiv.org/abs/1706.03762)中描述的架构。参数`is_decoder`设置为`True`；然后期望输入到前向传递的`encoder_hidden_states`。

`调用`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip_text.py#L816)

```py
( input_ids: TFModelInputType | None = None attention_mask: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None encoder_embeds: tf.Tensor | None = None encoder_hidden_states: tf.Tensor | None = None encoder_attention_mask: tf.Tensor | None = None past_key_values: Tuple[Tuple[tf.Tensor]] | None = None use_cache: bool | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None is_decoder: bool = False training: bool = False )
```

参数

+   `input_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。默认情况下将忽略填充。

    可以使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)获取索引。有关详细信息，请参阅`BlipProcessor.__call__()`。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `position_ids` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `encoder_hidden_states` (`tf.Tensor`, *optional*) — 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask` (`tf.Tensor`, *optional*) — 遮罩，用于避免在编码器输入的填充令牌索引上执行注意力。如果模型配置为解码器，则在交叉注意力中使用此遮罩。遮罩值选在 `[0, 1]` 之间：

    +   1 表示未被遮罩的令牌，

    +   0 表示被遮罩的令牌。

+   `past_key_values` (`tuple(tuple(tf.Tensor))`, *optional*) — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。如果使用了 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）形状为 `(batch_size, 1)`，而不是所有 `decoder_input_ids` 的形状为 `(batch_size, sequence_length)`。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，将返回 `past_key_values` 键值状态，可用于加速解码（参见 `past_key_values`）。

[TFBlipTextModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipTextModel) 前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前处理和后处理步骤，而后者会默默地忽略它们。

## TFBlipVisionModel

### `class transformers.TFBlipVisionModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L707)

```py
( config: BlipVisionConfig *args **kwargs )
```

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L731)

```py
( pixel_values: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling or tuple(tf.Tensor)
```

参数

+   `pixel_values` (`tf.Tensor`，形状为 `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充，如果提供的话。可以使用 [BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor) 获取像素值。有关详细信息，请参阅 [BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling) 或者 `tuple(tf.Tensor)`

一个 [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling) 或者一个 `tf.Tensor` 元组（如果传递了 `return_dict=False` 或者 `config.return_dict=False` 时）包含根据配置 (`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`) 和输入不同元素。

+   `last_hidden_state` (`tf.Tensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `pooler_output` (`tf.Tensor`，形状为 `(batch_size, hidden_size)`) — 序列中第一个令牌（分类令牌）的最后一层隐藏状态，经过线性层和 Tanh 激活函数进一步处理。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

    通常，此输出通常*不是*输入语义内容的良好摘要，您通常最好对整个输入序列的隐藏状态序列进行平均或池化。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。

    模型在每个层的输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFBlipVisionModel](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipVisionModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## TFBlipForConditionalGeneration

### `class transformers.TFBlipForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1095)

```py
( config: BlipConfig *args **kwargs )
```

参数

+   `config` ([BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

用于图像字幕的BLIP模型。该模型由视觉编码器和文本解码器组成。可以选择向模型传递`input_ids`，这些作为文本提示，以使文本解码器继续提示。否则，解码器将从[BOS]（序列开始）标记开始生成文本。将从文本输入开始生成标题。如果未提供文本输入，则解码器将仅从[BOS]标记开始。

该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1122)

```py
( pixel_values: tf.Tensor input_ids: tf.Tensor | None = None attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None labels: tf.Tensor | None = None return_dict: Optional[bool] = None training: Optional[bool] = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipForConditionalGenerationModelOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip.modeling_tf_blip.TFBlipForConditionalGenerationModelOutput` 或 `tuple(tf.Tensor)`

一个`transformers.models.blip.modeling_tf_blip.TFBlipForConditionalGenerationModelOutput`或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipConfig'>`）和输入的不同元素。

+   `loss`（`tf.Tensor`，*可选*，当提供`labels`时返回，形状为`(1,)`的`tf.Tensor`）- 来自文本解码器的语言建模损失。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`，*可选*）- 文本解码器模型语言建模头的预测分数。

+   `image_embeds`（形状为`(batch_size, output_dim)`的`tf.Tensor`，*可选*）- 在将输入图像应用于Vision Transformer模型后获得的图像嵌入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`，*可选*）- 模型最后一层输出的隐藏状态序列。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当`output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每层输出的一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。`

[TFBlipForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipForConditionalGeneration)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFBlipForConditionalGeneration

>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
>>> model = TFBlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "A picture of"

>>> inputs = processor(images=image, text=text, return_tensors="tf")

>>> outputs = model(**inputs)
```

## TFBlipForImageTextRetrieval

### `class transformers.TFBlipForImageTextRetrieval`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1534)

```py
( config: BlipConfig *args **kwargs )
```

参数

+   `config`（[BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

带有视觉和文本投影器以及顶部分类头的BLIP模型。该模型用于图像文本检索的上下文。给定一张图像和一段文本，模型返回文本与图像相关的概率。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1586)

```py
( input_ids: tf.Tensor pixel_values: tf.Tensor | None = None use_itm_head: Optional[bool] = True attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipImageTextMatchingModelOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`tf.Tensor`）— 像素值。默认情况下将忽略填充。如果提供填充，可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip.modeling_tf_blip.TFBlipImageTextMatchingModelOutput`或`tuple(tf.Tensor)`

一个`transformers.models.blip.modeling_tf_blip.TFBlipImageTextMatchingModelOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`）和输入的不同元素。

+   `itm_score`（`tf.Tensor`）— 图像文本相似性分数。

+   `loss`（形状为`(1,)`的`tf.Tensor`，*可选*，在提供`labels`时返回）— 文本解码器的语言建模损失。

+   `image_embeds`（形状为`(batch_size, output_dim)`的`tf.Tensor`，*可选*，在使用`with_projection=True`初始化模型时返回）— 通过将pooler_output应用到投影层获得的图像嵌入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `vision_pooler_output`（形状为`(batch_size, hidden_size)`的`tf.Tensor`，*可选*）— 模型视觉分支的最后一层隐藏状态。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `question_embeds`（`tf.Tensor`）— 由文本投影层获得的问题嵌入。

[TFBlipForImageTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.TFBlipForImageTextRetrieval)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFBlipForImageTextRetrieval

>>> model = TFBlipForImageTextRetrieval.from_pretrained("Salesforce/blip-itm-base-coco")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-itm-base-coco")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> text = "an image of a cat"

>>> inputs = processor(images=image, text=text, return_tensors="tf")
>>> outputs = model(**inputs)
```

## TFBlipForQuestionAnswering

### `class transformers.TFBlipForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1279)

```py
( config: BlipConfig *args **kwargs )
```

参数

+   `config` ([BlipConfig](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

用于视觉问答的BLIP模型。该模型包括一个视觉编码器、一个文本编码器以及一个文本解码器。视觉编码器将对输入图像进行编码，文本编码器将对输入问题以及图像的编码进行编码，文本解码器将输出问题的答案。

该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/blip/modeling_tf_blip.py#L1330)

```py
( input_ids: tf.Tensor pixel_values: tf.Tensor | None = None decoder_input_ids: tf.Tensor | None = None decoder_attention_mask: tf.Tensor | None = None attention_mask: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None labels: tf.Tensor | None = None return_dict: Optional[bool] = None training: Optional[bool] = None ) → export const metadata = 'undefined';transformers.models.blip.modeling_tf_blip.TFBlipTextVisionModelOutput or tuple(tf.Tensor)
```

参数

+   `pixel_values` (`tf.Tensor`，形状为`(batch_size, num_channels, height, width)`) — 像素值。默认情况下将忽略填充。可以使用[BlipImageProcessor](/docs/transformers/v4.37.2/en/model_doc/blip#transformers.BlipImageProcessor)获取像素值。有关详细信息，请参阅[BlipImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.blip.modeling_tf_blip.TFBlipTextVisionModelOutput`或`tuple(tf.Tensor)`

一个`transformers.models.blip.modeling_tf_blip.TFBlipTextVisionModelOutput`或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置(`<class 'transformers.models.blip.configuration_blip.BlipVisionConfig'>`)和输入的各种元素。

+   `loss` (`tf.Tensor`，形状为`(1,)`，*optional*，当提供`labels`时返回) — 来自文本解码器的语言建模损失。

+   `image_embeds` (`tf.Tensor`，形状为`(batch_size, output_dim)` *optional*，当模型使用`with_projection=True`初始化时返回) — 通过将投影层应用于pooler_output获得的图像嵌入。

+   `last_hidden_state` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出，如果模型有嵌入层，+ 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递 `output_attentions=True` 或者当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    在注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

[TFBlipForQuestionAnswering](/docs/transformers/v4.37.2/zh/model_doc/blip#transformers.TFBlipForQuestionAnswering) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例而不是这个函数，因为前者会处理运行前后的处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, TFBlipForQuestionAnswering

>>> model = TFBlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
>>> processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> # training
>>> text = "How many cats are in the picture?"
>>> label = "2"
>>> inputs = processor(images=image, text=text, return_tensors="tf")
>>> labels = processor(text=label, return_tensors="tf").input_ids

>>> inputs["labels"] = labels
>>> outputs = model(**inputs)
>>> loss = outputs.loss

>>> # inference
>>> text = "How many cats are in the picture?"
>>> inputs = processor(images=image, text=text, return_tensors="tf")
>>> outputs = model.generate(**inputs)
>>> print(processor.decode(outputs[0], skip_special_tokens=True))
2
```
