# 加载文本数据

> 原始文本：[`huggingface.co/docs/datasets/nlp_load`](https://huggingface.co/docs/datasets/nlp_load)

本指南向您展示如何加载文本数据集。要了解如何加载任何类型的数据集，请查看通用加载指南。

文本文件是存储数据集的最常见文件类型之一。默认情况下，🤗数据集逐行抽样文本文件以构建数据集。

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("text", data_files={"train": ["my_text_1.txt", "my_text_2.txt"], "test": "my_test_file.txt"})

# Load from a directory
>>> dataset = load_dataset("text", data_dir="path/to/text/dataset")
```

要按段落或整个文档对文本文件进行抽样，请使用`sample_by`参数：

```py
# Sample by paragraph
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="paragraph")

# Sample by document
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="document")
```

您还可以使用 grep 模式来加载特定文件：

```py
>>> from datasets import load_dataset
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")
```

通过 HTTP 加载远程文本文件，传递 URL 即可：

```py
>>> dataset = load_dataset("text", data_files="https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt")
```
