# åŠ è½½æ–‡æœ¬æ•°æ®

> åŽŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/datasets/nlp_load`](https://huggingface.co/docs/datasets/nlp_load)

æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•åŠ è½½æ–‡æœ¬æ•°æ®é›†ã€‚è¦äº†è§£å¦‚ä½•åŠ è½½ä»»ä½•ç±»åž‹çš„æ•°æ®é›†ï¼Œè¯·æŸ¥çœ‹é€šç”¨åŠ è½½æŒ‡å—ã€‚

æ–‡æœ¬æ–‡ä»¶æ˜¯å­˜å‚¨æ•°æ®é›†çš„æœ€å¸¸è§æ–‡ä»¶ç±»åž‹ä¹‹ä¸€ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒðŸ¤—æ•°æ®é›†é€è¡ŒæŠ½æ ·æ–‡æœ¬æ–‡ä»¶ä»¥æž„å»ºæ•°æ®é›†ã€‚

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("text", data_files={"train": ["my_text_1.txt", "my_text_2.txt"], "test": "my_test_file.txt"})

# Load from a directory
>>> dataset = load_dataset("text", data_dir="path/to/text/dataset")
```

è¦æŒ‰æ®µè½æˆ–æ•´ä¸ªæ–‡æ¡£å¯¹æ–‡æœ¬æ–‡ä»¶è¿›è¡ŒæŠ½æ ·ï¼Œè¯·ä½¿ç”¨`sample_by`å‚æ•°ï¼š

```py
# Sample by paragraph
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="paragraph")

# Sample by document
>>> dataset = load_dataset("text", data_files={"train": "my_train_file.txt", "test": "my_test_file.txt"}, sample_by="document")
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ grep æ¨¡å¼æ¥åŠ è½½ç‰¹å®šæ–‡ä»¶ï¼š

```py
>>> from datasets import load_dataset
>>> c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")
```

é€šè¿‡ HTTP åŠ è½½è¿œç¨‹æ–‡æœ¬æ–‡ä»¶ï¼Œä¼ é€’ URL å³å¯ï¼š

```py
>>> dataset = load_dataset("text", data_files="https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt")
```
