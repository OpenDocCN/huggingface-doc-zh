# Graphormer

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/graphormer)

## 概述

Graphormer 模型是由 Chengxuan Ying、Tianle Cai、Shengjie Luo、Shuxin Zheng、Guolin Ke、Di He、Yanming Shen 和 Tie-Yan Liu 在[Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234)中提出的。这是一个图形变换器模型，经过修改以允许在图形上进行计算，而不是文本序列，通过在预处理和整理过程中生成感兴趣的嵌入和特征，然后使用修改后的注意力。

论文摘要如下：

*变压器架构已经成为许多领域的主要选择，如自然语言处理和计算机视觉。然而，与主流 GNN 变体相比，它在流行的图级预测排行榜上并没有取得竞争性表现。因此，变压器如何在图形表示学习中表现良好仍然是一个谜。在本文中，我们通过提出 Graphormer 来解决这个谜团，它建立在标准 Transformer 架构之上，并且在广泛的图形表示学习任务中取得了出色的结果，特别是在最近的 OGB 大规模挑战赛上。我们利用 Transformer 在图中的关键见解是有效地将图的结构信息编码到模型中。为此，我们提出了几种简单而有效的结构编码方法，以帮助 Graphormer 更好地建模图结构化数据。此外，我们数学地刻画了 Graphormer 的表达能力，并展示了通过我们的方式对图的结构信息进行编码，许多流行的 GNN 变体可以被覆盖为 Graphormer 的特殊情况。*

该模型由[clefourrier](https://huggingface.co/clefourrier)贡献。原始代码可以在[这里](https://github.com/microsoft/Graphormer)找到。

## 使用提示

这个模型在大型图上（超过 100 个节点/边）效果不佳，因为会导致内存爆炸。您可以减小批量大小，增加 RAM，或者减小 algos_graphormer.pyx 中的`UNREACHABLE_NODE_DISTANCE`参数，但很难超过 700 个节点/边。

该模型不使用分词器，而是在训练过程中使用特殊的整理器。

## GraphormerConfig

### `class transformers.GraphormerConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/configuration_graphormer.py#L30)

```py
( num_classes: int = 1 num_atoms: int = 4608 num_edges: int = 1536 num_in_degree: int = 512 num_out_degree: int = 512 num_spatial: int = 512 num_edge_dis: int = 128 multi_hop_max_dist: int = 5 spatial_pos_max: int = 1024 edge_type: str = 'multi_hop' max_nodes: int = 512 share_input_output_embed: bool = False num_hidden_layers: int = 12 embedding_dim: int = 768 ffn_embedding_dim: int = 768 num_attention_heads: int = 32 dropout: float = 0.1 attention_dropout: float = 0.1 activation_dropout: float = 0.1 layerdrop: float = 0.0 encoder_normalize_before: bool = False pre_layernorm: bool = False apply_graphormer_init: bool = False activation_fn: str = 'gelu' embed_scale: float = None freeze_embeddings: bool = False num_trans_layers_to_freeze: int = 0 traceable: bool = False q_noise: float = 0.0 qn_block_size: int = 8 kdim: int = None vdim: int = None bias: bool = True self_attention: bool = True pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 **kwargs )
```

参数

+   `num_classes` (`int`, *optional*, defaults to 1) — 目标类别或标签的数量，设置为 n 用于 n 个任务的二元分类。

+   `num_atoms` (`int`, *optional*, defaults to 512*9) — 图中节点类型的数量。

+   `num_edges` (`int`, *optional*, defaults to 512*3) — 图中边的类型数量。

+   `num_in_degree` (`int`, *optional*, defaults to 512) — 输入图中的入度类型数量。

+   `num_out_degree` (`int`, *optional*, defaults to 512) — 输入图中的出度类型数量。

+   `num_edge_dis` (`int`, *optional*, defaults to 128) — 输入图中的边缘 dis 数量。

+   `multi_hop_max_dist` (`int`, *optional*, defaults to 20) — 两个节点之间多跳边的最大距离。

+   `spatial_pos_max` (`int`, *optional*, defaults to 1024) — 图注意力偏置矩阵中节点之间的最大距离，在预处理和整理过程中使用。

+   `edge_type` (`str`, *optional*, defaults to multihop) — 选择的边关系类型。

+   `max_nodes` (`int`, *optional*, defaults to 512) — 可以解析的输入图中的最大节点数。

+   `share_input_output_embed` (`bool`, *optional*, defaults to `False`) — 在编码器和解码器之间共享嵌入层 - 注意，True 未实现。

+   `num_layers` (`int`, *optional*, defaults to 12) — 层数。

+   `embedding_dim` (`int`, *optional*, defaults to 768) — 编码器中嵌入层的维度。

+   `ffn_embedding_dim` (`int`, *optional*, defaults to 768) — 编码器中“中间”（通常称为前馈）层的维度。

+   `num_attention_heads` (`int`, *optional*, defaults to 32) — 编码器中的注意力头数。

+   `self_attention` (`bool`, *optional*, defaults to `True`) — 模型是自注意的（False 未实现）。

+   `activation_function` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"silu"`和`"gelu_new"`。

+   `dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的 dropout 概率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力权重的 dropout 概率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) — 线性变换器层激活的 dropout 概率。

+   `layerdrop` (`float`, *optional*, defaults to 0.0) — 编码器的 LayerDrop 概率。有关更多详细信息，请参阅 LayerDrop paper)。

+   `bias` (`bool`, *optional*, defaults to `True`) — 在注意力模块中使用偏置 - 目前不支持。

+   `embed_scale(float,` *optional*, defaults to None) — 节点嵌入的缩放因子。

+   `num_trans_layers_to_freeze` (`int`, *optional*, defaults to 0) — 要冻结的 Transformer 层数。

+   `encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — 在对图进行编码之前对特征进行归一化。

+   `pre_layernorm` (`bool`, *optional*, defaults to `False`) — 在自注意力和前馈网络之前应用层归一化。如果没有这个，将使用后层归一化。

+   `apply_graphormer_init` (`bool`, *optional*, defaults to `False`) — 在训练之前对模型应用自定义的 graphormer 初始化。

+   `freeze_embeddings` (`bool`, *optional*, defaults to `False`) — 冻结嵌入层，或者与模型一起训练。

+   `encoder_normalize_before` (`bool`, *optional*, defaults to `False`) — 在每个编码器块之前应用层归一化。

+   `q_noise` (`float`, *optional*, defaults to 0.0) — 量化噪声的量（参见“使用量化噪声进行极端模型压缩”）。 （更多细节，请参阅 fairseq 关于 quant_noise 的文档）。

+   `qn_block_size` (`int`, *optional*, defaults to 8) — 用于后续 iPQ 量化的块的大小（参见 q_noise）。

+   `kdim` (`int`, *optional*, defaults to None) — 注意力中键的维度，如果与其他值不同。

+   `vdim` (`int`, *optional*, defaults to None) — 注意力中值的维度，如果与其他值不同。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

+   `traceable` (`bool`, *optional*, defaults to `False`) — 将编码器的 inner_state 的返回值更改为堆叠的张量。

    示例 —

这是一个配置类，用于存储~GraphormerModel 的配置。它用于根据指定的参数实例化一个 Graphormer 模型，定义模型架构。使用默认值实例化配置将产生类似于 Graphormer [graphormer-base-pcqm4mv1](https://huggingface.co/graphormer-base-pcqm4mv1)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

## GraphormerModel

### `class transformers.GraphormerModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L775)

```py
( config: GraphormerConfig )
```

Graphormer 模型是一个图编码器模型。

它将一个图转换为其表示。如果您想将模型用于下游分类任务，请改用 GraphormerForGraphClassification。对于任何其他下游任务，请随意添加一个新类，或将此模型与您选择的下游模型结合，按照 GraphormerForGraphClassification 中的示例进行操作。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L804)

```py
( input_nodes: LongTensor input_edges: LongTensor attn_bias: Tensor in_degree: LongTensor out_degree: LongTensor spatial_pos: LongTensor attn_edge_type: LongTensor perturb: Optional = None masked_tokens: None = None return_dict: Optional = None **unused )
```

## GraphormerForGraphClassification

### `class transformers.GraphormerForGraphClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L846)

```py
( config: GraphormerConfig )
```

这个模型可以用于图级分类或回归任务。

可以在以下链接上进行训练

+   回归（通过将 config.num_classes 设置为 1）；每个图应有一个浮点类型标签

+   单任务分类（通过将 config.num_classes 设置为类别数）；每个图应有一个整数标签

+   二元多任务分类（通过将 config.num_classes 设置为标签数）；每个图应有一个整数标签列表。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/graphormer/modeling_graphormer.py#L869)

```py
( input_nodes: LongTensor input_edges: LongTensor attn_bias: Tensor in_degree: LongTensor out_degree: LongTensor spatial_pos: LongTensor attn_edge_type: LongTensor labels: Optional = None return_dict: Optional = None **unused )
```
