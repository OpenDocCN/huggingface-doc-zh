["```py\n# uncomment to install the necessary libraries in Colab\n#!pip install -q diffusers transformers accelerate opencv-python\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport cv2\nimport numpy as np\n\noriginal_image = load_image(\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n)\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```", "```py\noutput = pipe(\n    \"the mona lisa\", image=canny_image\n).images[0]\nmake_image_grid([original_image, canny_image, output], rows=1, cols=3)\n```", "```py\nimport torch\nimport numpy as np\n\nfrom transformers import pipeline\nfrom diffusers.utils import load_image, make_image_grid\n\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg\"\n)\n\ndef get_depth_map(image, depth_estimator):\n    image = depth_estimator(image)[\"depth\"]\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    detected_map = torch.from_numpy(image).float() / 255.0\n    depth_map = detected_map.permute(2, 0, 1)\n    return depth_map\n\ndepth_estimator = pipeline(\"depth-estimation\")\ndepth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to(\"cuda\")\n```", "```py\nfrom diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11f1p_sd15_depth\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```", "```py\noutput = pipe(\n    \"lego batman and robin\", image=image, control_image=depth_map,\n).images[0]\nmake_image_grid([image, output], rows=1, cols=2)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\n\ninit_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg\"\n)\ninit_image = init_image.resize((512, 512))\n\nmask_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg\"\n)\nmask_image = mask_image.resize((512, 512))\nmake_image_grid([init_image, mask_image], rows=1, cols=2)\n```", "```py\nimport numpy as np\nimport torch\n\ndef make_inpaint_condition(image, image_mask):\n    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n\n    assert image.shape[0:1] == image_mask.shape[0:1]\n    image[image_mask > 0.5] = -1.0  # set as masked pixel\n    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    return image\n\ncontrol_image = make_inpaint_condition(init_image, mask_image)\n```", "```py\nfrom diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True\n)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```", "```py\noutput = pipe(\n    \"corgi face with large ears, detailed, pixar, animated, disney\",\n    num_inference_steps=20,\n    eta=1.0,\n    image=init_image,\n    mask_image=mask_image,\n    control_image=control_image,\n).images[0]\nmake_image_grid([init_image, mask_image, output], rows=1, cols=3)\n```", "```py\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\nfrom diffusers.utils import load_image, make_image_grid\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport cv2\n\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\n\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\n```", "```py\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport torch\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n)\n\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\nmake_image_grid([original_image, canny_image], rows=1, cols=2)\n```", "```py\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\",\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    controlnet=controlnet,\n    vae=vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True\n)\npipe.enable_model_cpu_offload()\n```", "```py\nprompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\nnegative_prompt = 'low quality, bad quality, sketches'\n\nimage = pipe(\n    prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    controlnet_conditioning_scale=0.5,\n).images[0]\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\n```", "```py\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL\nfrom diffusers.utils import load_image, make_image_grid\nimport numpy as np\nimport torch\nimport cv2\nfrom PIL import Image\n\nprompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\nnegative_prompt = \"low quality, bad quality, sketches\"\n\noriginal_image = load_image(\n    \"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\"\n)\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n)\npipe.enable_model_cpu_offload()\n\nimage = np.array(original_image)\nimage = cv2.Canny(image, 100, 200)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\nimage = pipe(\n    prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,\n).images[0]\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\n```", "```py\nfrom diffusers.utils import load_image, make_image_grid\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png\"\n)\nimage = np.array(original_image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\n\n# zero out middle columns of image where pose will be overlaid\nzero_start = image.shape[1] // 4\nzero_end = zero_start + image.shape[1] // 2\nimage[:, zero_start:zero_end] = 0\n\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\nmake_image_grid([original_image, canny_image], rows=1, cols=2)\n```", "```py\n# uncomment to install the necessary library in Colab\n#!pip install -q controlnet-aux\n```", "```py\nfrom controlnet_aux import OpenposeDetector\n\nopenpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\noriginal_image = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png\"\n)\nopenpose_image = openpose(original_image)\nmake_image_grid([original_image, openpose_image], rows=1, cols=2)\n```", "```py\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler\nimport torch\n\ncontrolnets = [\n    ControlNetModel.from_pretrained(\n        \"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16\n    ),\n    ControlNetModel.from_pretrained(\n        \"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True\n    ),\n]\n\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n```", "```py\nprompt = \"a giant standing in a fantasy landscape, best quality\"\nnegative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\ngenerator = torch.manual_seed(1)\n\nimages = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]\n\nimages = pipe(\n    prompt,\n    image=images,\n    num_inference_steps=25,\n    generator=generator,\n    negative_prompt=negative_prompt,\n    num_images_per_prompt=3,\n    controlnet_conditioning_scale=[1.0, 0.8],\n).images\nmake_image_grid([original_image, canny_image, openpose_image,\n                images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)\n```"]