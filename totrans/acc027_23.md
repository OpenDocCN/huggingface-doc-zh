# 低精度训练方法

> 原始文本：[`huggingface.co/docs/accelerate/usage_guides/low_precision_training`](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training)

🤗 Accelerate 提供了集成，可以使用指定支持的硬件通过`TransformersEngine`和`MS-AMP`软件包进行低精度方法的训练。本文档将帮助您了解支持的硬件是什么，如何配置您的加速器以利用低精度方法，以及在训练时可以期望什么。

## 在 FP8 上进行训练意味着什么

要深入了解使用 PyTorch 和🤗 Accelerate 在 FP8 中进行训练的细节，请查看概念指南，了解为什么这可能会很困难。但基本上，与在 BF16 中训练不同，训练模型的某些（或全部）方面可以使用 8 位而不是 16 位来执行。挑战在于在不降低最终性能的情况下这样做。

这仅在特定的 NVIDIA 硬件上启用，即：

+   3000 系列消费级显卡之后的任何硬件（例如 4090）

+   基于 Hopper 的 GPU 架构（例如`H100`和`H200`）

这将导致一些内存使用上的增益（因为我们已经将某些训练部分所需的内存减少了一半），并且对于可以用 FP8 启用的某些层替换的较大模型，也应该会看到吞吐量的增加。

## 配置加速器

目前支持两种不同的 FP8 后端（`TransformersEngine`和`MS-AMP`），每个都具有不同的功能和配置。

要使用任何一个，都使用相同的核心 API。只需将`mixed_precision="fp8"`传递给 Accelerator 之一，在`accelerate config`期间，当询问混合精度时，或作为`config.yaml`文件中的`mixed_precision`键的一部分：

```py
from accelerate import Accelerator
accelerator = Accelerator(mixed_precision="fp8")
```

默认情况下，如果您的环境中有`MS-AMP`，🤗 Accelerate 将自动将其用作后端。要自行指定它（并自定义 FP8 混合精度设置的其他部分），您可以使用 utils.FP8RecipeKwargs：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="msamp")]
# Or to specify the backend as `TransformersEngine` even if MS-AMP is installed
# kwargs = [FP8RecipeKwargs(backend="te")]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 配置 MS-AMP

在这两者中，`MS-AMP`传统上更容易配置，因为只有一个参数：优化级别。

目前🤗 Accelerate 集成支持两个优化级别，`"O1"`和`"O2"`（使用字母‘o’，而不是零）。

+   `"O1"`将将权重梯度和`all_reduce`通信转换为 8 位，而其余部分将以 16 位完成。这减少了一般 GPU 内存使用量并加快了通信带宽。

+   `"O2"`还将一阶优化器状态转换为 8 位，而二阶状态为 FP16（目前仅支持`Adam`优化器）。这尽最大努力最小化最终准确性的降级，并将节省最高潜在内存。

要指定优化级别，请将其传递给`FP8KwargsHandler`，设置`optimization_level`参数：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="msamp", optimization_level="O2")]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 配置 TransformersEngine

TransformersEngine 有更多可用于自定义如何以及何时执行 FP8 计算的内容。支持的参数列表和它们的含义在[NVIDIA 文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)中都有，但是它们也作为`FP8KwargsHandler`的 docstring 的一部分重申，以方便您查看。

🤗 Accelerate 尝试设置合理的默认值，但是自己探索和调整各种参数可能会导致更好的性能。

要使用它，请指定`backend="te"`并修改您想要作为 kwarg 处理程序的任何参数：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="te", ...)]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 进一步阅读

要了解更多关于在 FP8 中训练的信息，请查看以下资源：

+   我们的概念指南详细介绍了 TransformersEngine 和 MS-AMP。

+   [transformers-engine 文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)

+   [MS-AMP 文档](https://azure.github.io/MS-AMP/docs/)
