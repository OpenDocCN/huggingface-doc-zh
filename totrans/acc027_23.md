# 低精度训练方法

> 原始文本：[https://huggingface.co/docs/accelerate/usage_guides/low_precision_training](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training)

🤗 Accelerate提供了集成，可以使用指定支持的硬件通过`TransformersEngine`和`MS-AMP`软件包进行低精度方法的训练。本文档将帮助您了解支持的硬件是什么，如何配置您的[加速器](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)以利用低精度方法，以及在训练时可以期望什么。

## 在FP8上进行训练意味着什么

要深入了解使用PyTorch和🤗 Accelerate在FP8中进行训练的细节，请查看[概念指南](../concept_guides/low_precision_training.md)，了解为什么这可能会很困难。但基本上，与在BF16中训练不同，训练模型的某些（或全部）方面可以使用8位而不是16位来执行。挑战在于在不降低最终性能的情况下这样做。

这仅在特定的NVIDIA硬件上启用，即：

+   3000系列消费级显卡之后的任何硬件（例如4090）

+   基于Hopper的GPU架构（例如`H100`和`H200`）

这将导致一些内存使用上的增益（因为我们已经将某些训练部分所需的内存减少了一半），并且对于可以用FP8启用的某些层替换的较大模型，也应该会看到吞吐量的增加。

## 配置加速器

目前支持两种不同的FP8后端（`TransformersEngine`和`MS-AMP`），每个都具有不同的功能和配置。

要使用任何一个，都使用相同的核心API。只需将`mixed_precision="fp8"`传递给[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)之一，在`accelerate config`期间，当询问混合精度时，或作为`config.yaml`文件中的`mixed_precision`键的一部分：

```py
from accelerate import Accelerator
accelerator = Accelerator(mixed_precision="fp8")
```

默认情况下，如果您的环境中有`MS-AMP`，🤗 Accelerate将自动将其用作后端。要自行指定它（并自定义FP8混合精度设置的其他部分），您可以使用[utils.FP8RecipeKwargs](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.FP8RecipeKwargs)：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="msamp")]
# Or to specify the backend as `TransformersEngine` even if MS-AMP is installed
# kwargs = [FP8RecipeKwargs(backend="te")]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 配置MS-AMP

在这两者中，`MS-AMP`传统上更容易配置，因为只有一个参数：优化级别。

目前🤗 Accelerate集成支持两个优化级别，`"O1"`和`"O2"`（使用字母‘o’，而不是零）。

+   `"O1"`将将权重梯度和`all_reduce`通信转换为8位，而其余部分将以16位完成。这减少了一般GPU内存使用量并加快了通信带宽。

+   `"O2"`还将一阶优化器状态转换为8位，而二阶状态为FP16（目前仅支持`Adam`优化器）。这尽最大努力最小化最终准确性的降级，并将节省最高潜在内存。

要指定优化级别，请将其传递给`FP8KwargsHandler`，设置`optimization_level`参数：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="msamp", optimization_level="O2")]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 配置TransformersEngine

TransformersEngine有更多可用于自定义如何以及何时执行FP8计算的内容。支持的参数列表和它们的含义在[NVIDIA文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)中都有，但是它们也作为`FP8KwargsHandler`的docstring的一部分重申，以方便您查看。

🤗 Accelerate尝试设置合理的默认值，但是自己探索和调整各种参数可能会导致更好的性能。

要使用它，请指定`backend="te"`并修改您想要作为kwarg处理程序的任何参数：

```py
from accelerate import Accelerator
from accelerate.utils import FP8RecipeKwargs
kwargs = [FP8RecipeKwargs(backend="te", ...)]
accelerator = Accelerator(mixed_precision="fp8", kwarg_handlers=kwargs)
```

## 进一步阅读

要了解更多关于在FP8中训练的信息，请查看以下资源：

+   [我们的概念指南](../concept_guides/low_precision_training.md)详细介绍了TransformersEngine和MS-AMP。

+   [transformers-engine文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/common.html)

+   [MS-AMP文档](https://azure.github.io/MS-AMP/docs/)
