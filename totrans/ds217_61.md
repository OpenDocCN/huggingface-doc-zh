# æ„å»ºå™¨ç±»

> åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/datasets/package_reference/builder_classes](https://huggingface.co/docs/datasets/package_reference/builder_classes)

## æ„å»ºå™¨

ğŸ¤— æ•°æ®é›†åœ¨æ•°æ®é›†æ„å»ºè¿‡ç¨‹ä¸­ä¾èµ–äºä¸¤ä¸ªä¸»è¦ç±»ï¼š[DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder) å’Œ [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)ã€‚

### `class datasets.DatasetBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L214)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

å‚æ•°

+   `cache_dir` (`str`, *å¯é€‰*) â€” ç¼“å­˜æ•°æ®çš„ç›®å½•ã€‚é»˜è®¤ä¸º `"~/.cache/huggingface/datasets"`ã€‚

+   `dataset_name` (`str`, *å¯é€‰*) â€” æ•°æ®é›†çš„åç§°ï¼Œå¦‚æœä¸æ„å»ºå™¨åç§°ä¸åŒã€‚å¯¹äºæ‰“åŒ…çš„æ„å»ºå™¨ï¼ˆå¦‚ csvã€imagefolderã€audiofolder ç­‰ï¼‰ï¼Œç”¨äºåæ˜ ä½¿ç”¨ç›¸åŒæ‰“åŒ…æ„å»ºå™¨çš„æ•°æ®é›†ä¹‹é—´çš„å·®å¼‚ã€‚

+   `config_name` (`str`, *å¯é€‰*) â€” æ•°æ®é›†é…ç½®çš„åç§°ã€‚å®ƒä¼šå½±å“ç£ç›˜ä¸Šç”Ÿæˆçš„æ•°æ®ã€‚ä¸åŒçš„é…ç½®å°†æœ‰è‡ªå·±çš„å­ç›®å½•å’Œç‰ˆæœ¬ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

    åœ¨ 2.3.0 ä¸­æ·»åŠ 

    å‚æ•° `name` å·²é‡å‘½åä¸º `config_name`ã€‚

+   `hash` (`str`, *å¯é€‰*) â€” ç‰¹å®šäºæ•°æ®é›†ä»£ç çš„å“ˆå¸Œå€¼ã€‚ç”¨äºåœ¨æ•°æ®é›†åŠ è½½è„šæœ¬ä»£ç æ›´æ–°æ—¶æ›´æ–°ç¼“å­˜ç›®å½•ï¼ˆä»¥é¿å…é‡ç”¨æ—§æ•°æ®ï¼‰ã€‚å…¸å‹çš„ç¼“å­˜ç›®å½•ï¼ˆåœ¨ `self._relative_data_dir` ä¸­å®šä¹‰ï¼‰æ˜¯ `name/version/hash/`ã€‚

+   `base_path` (`str`, *å¯é€‰*) â€” ç”¨äºä¸‹è½½æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„çš„åŸºæœ¬è·¯å¾„ã€‚è¿™å¯ä»¥æ˜¯è¿œç¨‹ URLã€‚

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *å¯é€‰*) â€” ç”¨äºæ­¤æ•°æ®é›†çš„ç‰¹å¾ç±»å‹ã€‚å®ƒå¯ä»¥ç”¨äºæ›´æ”¹æ•°æ®é›†çš„ [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features) ç±»å‹ï¼Œä¾‹å¦‚ã€‚

+   `token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œæ•°æ®é›†ä¸­è¿œç¨‹æ–‡ä»¶çš„ Bearer token çš„å­—ç¬¦ä¸²æˆ–å¸ƒå°”å€¼ã€‚å¦‚æœä¸º `True`ï¼Œå°†ä» `"~/.huggingface"` è·å–ä»¤ç‰Œã€‚

+   `repo_id` (`str`, *å¯é€‰*) â€” æ•°æ®é›†å­˜å‚¨åº“çš„ IDã€‚ç”¨äºåŒºåˆ†å…·æœ‰ç›¸åŒåç§°ä½†ä¸æ¥è‡ªç›¸åŒå‘½åç©ºé—´çš„æ„å»ºå™¨ï¼Œä¾‹å¦‚â€œsquadâ€å’Œâ€œlhoestq/squadâ€å­˜å‚¨åº“ IDã€‚åœ¨åè€…ä¸­ï¼Œæ„å»ºå™¨åç§°å°†æ˜¯â€œlhoestq___squadâ€ã€‚

+   `data_files` (`str` æˆ– `Sequence` æˆ– `Mapping`, *å¯é€‰*) â€” æºæ•°æ®æ–‡ä»¶çš„è·¯å¾„ã€‚å¯¹äºåƒâ€œcsvâ€æˆ–â€œjsonâ€è¿™æ ·éœ€è¦ç”¨æˆ·æŒ‡å®šæ•°æ®æ–‡ä»¶çš„æ„å»ºå™¨ã€‚å®ƒä»¬å¯ä»¥æ˜¯æœ¬åœ°æ–‡ä»¶æˆ–è¿œç¨‹æ–‡ä»¶ã€‚ä¸ºæ–¹ä¾¿èµ·è§ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `DataFilesDict`ã€‚

+   `data_dir` (`str`, *å¯é€‰*) â€” åŒ…å«æºæ•°æ®æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚ä»…åœ¨æœªä¼ é€’ `data_files` çš„æƒ…å†µä¸‹ä½¿ç”¨ï¼Œæ­¤æ—¶å®ƒç­‰æ•ˆäºå°† `os.path.join(data_dir, "**")` ä½œä¸º `data_files`ã€‚å¯¹äºéœ€è¦æ‰‹åŠ¨ä¸‹è½½çš„æ„å»ºå™¨ï¼Œå®ƒå¿…é¡»æ˜¯åŒ…å«æ‰‹åŠ¨ä¸‹è½½æ•°æ®çš„æœ¬åœ°ç›®å½•çš„è·¯å¾„ã€‚

+   `storage_options` (`dict`, *å¯é€‰*) â€” è¦ä¼ é€’ç»™æ•°æ®é›†æ–‡ä»¶ç³»ç»Ÿåç«¯çš„é”®/å€¼å¯¹ï¼Œå¦‚æœæœ‰çš„è¯ã€‚

+   `writer_batch_size` (`int`, *å¯é€‰*) â€” ArrowWriter ä½¿ç”¨çš„æ‰¹å¤„ç†å¤§å°ã€‚å®ƒå®šä¹‰äº†åœ¨å†™å…¥ä¹‹å‰åœ¨å†…å­˜ä¸­ä¿ç•™çš„æ ·æœ¬æ•°é‡ï¼Œä¹Ÿå®šä¹‰äº†ç®­å¤´å—çš„é•¿åº¦ã€‚None è¡¨ç¤º ArrowWriter å°†ä½¿ç”¨å…¶é»˜è®¤å€¼ã€‚

+   `name` (`str`) â€” æ•°æ®é›†çš„é…ç½®åç§°ã€‚

    åœ¨ 2.3.0 ä¸­å·²å¼ƒç”¨

    è¯·ä½¿ç”¨ `config_name`ã€‚

+   *`*config_kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼‰ â€” è¦ä¼ é€’ç»™ç›¸åº”æ„å»ºå™¨é…ç½®ç±»çš„å…³é”®å­—å‚æ•°ï¼Œè®¾ç½®åœ¨ç±»å±æ€§ [DatasetBuilder.BUILDER_CONFIG_CLASS](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig) ä¸Šã€‚æ„å»ºå™¨é…ç½®ç±»æ˜¯ [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig) æˆ–å…¶å­ç±»ã€‚

æ‰€æœ‰æ•°æ®é›†çš„æŠ½è±¡åŸºç±»ã€‚

`DatasetBuilder` æœ‰ 3 ä¸ªå…³é”®æ–¹æ³•ï¼š

+   `DatasetBuilder.info`ï¼šè®°å½•æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç‰¹å¾åç§°ã€ç±»å‹ã€å½¢çŠ¶ã€ç‰ˆæœ¬ã€æ‹†åˆ†ã€å¼•ç”¨ç­‰ã€‚

+   [DatasetBuilder.download_and_prepare()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare): ä¸‹è½½æºæ•°æ®å¹¶å°†å…¶å†™å…¥ç£ç›˜ã€‚

+   [DatasetBuilder.as_dataset()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset): ç”Ÿæˆä¸€ä¸ª[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)ã€‚

ä¸€äº›`DatasetBuilder`é€šè¿‡å®šä¹‰ä¸€ä¸ª[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)å­ç±»å¹¶åœ¨æ„é€ æ—¶æ¥å—ä¸€ä¸ªé…ç½®å¯¹è±¡ï¼ˆæˆ–åç§°ï¼‰æ¥å…¬å¼€æ•°æ®é›†çš„å¤šä¸ªå˜ä½“ã€‚å¯é…ç½®çš„æ•°æ®é›†åœ¨`DatasetBuilder.builder_configs()`ä¸­å…¬å¼€äº†ä¸€ç»„é¢„å®šä¹‰çš„é…ç½®ã€‚

#### `as_dataset`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1166)

```py
( split: Optional = None run_post_process = True verification_mode: Union = None ignore_verifications = 'deprecated' in_memory = False )
```

å‚æ•°

+   `split` (`datasets.Split`) â€” è¦è¿”å›çš„æ•°æ®çš„å“ªä¸ªå­é›†ã€‚

+   `run_post_process` (`bool`ï¼Œé»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿è¡Œåå¤„ç†æ•°æ®é›†è½¬æ¢å’Œ/æˆ–æ·»åŠ ç´¢å¼•ã€‚

+   `verification_mode` ([VerificationMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.VerificationMode)æˆ–`str`ï¼Œé»˜è®¤ä¸º`BASIC_CHECKS`) â€” ç¡®å®šè¦åœ¨ä¸‹è½½/å¤„ç†çš„æ•°æ®é›†ä¿¡æ¯ä¸Šè¿è¡Œçš„æ£€æŸ¥çš„éªŒè¯æ¨¡å¼ï¼ˆæ ¡éªŒå’Œ/å¤§å°/æ‹†åˆ†/...ï¼‰ã€‚

    åœ¨2.9.1ä¸­æ·»åŠ 

+   `ignore_verifications` (`bool`ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¿½ç•¥ä¸‹è½½/å¤„ç†çš„æ•°æ®é›†ä¿¡æ¯çš„éªŒè¯ï¼ˆæ ¡éªŒå’Œ/å¤§å°/æ‹†åˆ†/...ï¼‰ã€‚

    åœ¨2.9.1ä¸­å¼ƒç”¨

    `ignore_verifications`åœ¨ç‰ˆæœ¬2.9.1ä¸­å·²å¼ƒç”¨ï¼Œå°†åœ¨3.0.0ä¸­åˆ é™¤ã€‚è¯·æ”¹ç”¨`verification_mode`ã€‚

+   `in_memory` (`bool`ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦å°†æ•°æ®å¤åˆ¶åˆ°å†…å­˜ä¸­ã€‚

è¿”å›æŒ‡å®šæ‹†åˆ†çš„æ•°æ®é›†ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder('rotten_tomatoes')
>>> builder.download_and_prepare()
>>> ds = builder.as_dataset(split='train')
>>> ds
Dataset({
    features: ['text', 'label'],
    num_rows: 8530
})
```

#### `download_and_prepare`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L744)

```py
( output_dir: Optional = None download_config: Optional = None download_mode: Union = None verification_mode: Union = None ignore_verifications = 'deprecated' try_from_hf_gcs: bool = True dl_manager: Optional = None base_path: Optional = None use_auth_token = 'deprecated' file_format: str = 'arrow' max_shard_size: Union = None num_proc: Optional = None storage_options: Optional = None **download_and_prepare_kwargs )
```

å‚æ•°

+   `output_dir` (`str`ï¼Œ*å¯é€‰çš„*) â€” æ•°æ®é›†çš„è¾“å‡ºç›®å½•ã€‚é»˜è®¤ä¸ºæ­¤æ„å»ºå™¨çš„`cache_dir`ï¼Œé»˜è®¤æƒ…å†µä¸‹ä½äº`~/.cache/huggingface/datasets`å†…ã€‚

    åœ¨2.5.0ä¸­æ·»åŠ 

+   `download_config` (`DownloadConfig`ï¼Œ*å¯é€‰çš„*) â€” ç‰¹å®šçš„ä¸‹è½½é…ç½®å‚æ•°ã€‚

+   `download_mode` ([DownloadMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadMode)æˆ–`str`ï¼Œ*å¯é€‰çš„*) â€” é€‰æ‹©ä¸‹è½½/ç”Ÿæˆæ¨¡å¼ï¼Œé»˜è®¤ä¸º`REUSE_DATASET_IF_EXISTS`ã€‚

+   `verification_mode` ([VerificationMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.VerificationMode)æˆ–`str`ï¼Œé»˜è®¤ä¸º`BASIC_CHECKS`) â€” ç¡®å®šè¦åœ¨ä¸‹è½½/å¤„ç†çš„æ•°æ®é›†ä¿¡æ¯ä¸Šè¿è¡Œçš„æ£€æŸ¥çš„éªŒè¯æ¨¡å¼ï¼ˆæ ¡éªŒå’Œ/å¤§å°/æ‹†åˆ†/...ï¼‰ã€‚

    åœ¨2.9.1ä¸­æ·»åŠ 

+   `ignore_verifications` (`bool`ï¼Œé»˜è®¤ä¸º`False`) â€” å¿½ç•¥ä¸‹è½½/å¤„ç†çš„æ•°æ®é›†ä¿¡æ¯çš„éªŒè¯ï¼ˆæ ¡éªŒå’Œ/å¤§å°/æ‹†åˆ†/...ï¼‰ã€‚

    åœ¨2.9.1ä¸­å¼ƒç”¨

    `ignore_verifications`åœ¨ç‰ˆæœ¬2.9.1ä¸­å·²å¼ƒç”¨ï¼Œå°†åœ¨3.0.0ä¸­åˆ é™¤ã€‚è¯·æ”¹ç”¨`verification_mode`ã€‚

+   `try_from_hf_gcs` (`bool`) â€” å¦‚æœä¸º`True`ï¼Œå°†å°è¯•ä»HF Googleäº‘å­˜å‚¨ä¸­ä¸‹è½½å·²å‡†å¤‡å¥½çš„æ•°æ®é›†ã€‚

+   `dl_manager` (`DownloadManager`ï¼Œ*å¯é€‰çš„*) â€” è¦ä½¿ç”¨çš„ç‰¹å®š`DownloadManger`ã€‚

+   `base_path` (`str`ï¼Œ*å¯é€‰çš„*) â€” ç”¨äºä¸‹è½½æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„çš„åŸºæœ¬è·¯å¾„ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªè¿œç¨‹urlã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`base_path`å±æ€§ï¼ˆ`self.base_path`ï¼‰çš„å€¼ã€‚

+   `use_auth_token` (`Union[str, bool]`ï¼Œ*å¯é€‰çš„*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶åœ¨æ•°æ®é›†ä¸­å¿ƒçš„Bearerä»¤ç‰Œçš„å¯é€‰å­—ç¬¦ä¸²æˆ–å¸ƒå°”å€¼ã€‚å¦‚æœä¸ºTrueï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä»~/.huggingfaceè·å–ä»¤ç‰Œã€‚

    åœ¨2.7.1ä¸­å¼ƒç”¨

    å°†`use_auth_token`ä¼ é€’ç»™`load_dataset_builder`ã€‚

+   `file_format` (`str`, *å¯é€‰*) â€” æ•°æ®æ–‡ä»¶çš„æ ¼å¼ï¼Œæ•°æ®é›†å°†è¢«å†™å…¥å…¶ä¸­ã€‚æ”¯æŒçš„æ ¼å¼: â€œarrowâ€, â€œparquetâ€ã€‚é»˜è®¤ä¸ºâ€œarrowâ€æ ¼å¼ã€‚å¦‚æœæ ¼å¼ä¸ºâ€œparquetâ€ï¼Œåˆ™å›¾åƒå’ŒéŸ³é¢‘æ•°æ®å°†åµŒå…¥åˆ°Parquetæ–‡ä»¶ä¸­ï¼Œè€Œä¸æ˜¯æŒ‡å‘æœ¬åœ°æ–‡ä»¶ã€‚

    åœ¨2.5.0ä¸­æ·»åŠ 

+   `max_shard_size` (`Union[str, int]`, *å¯é€‰*) â€” æ¯ä¸ªåˆ†ç‰‡å†™å…¥çš„æœ€å¤§å­—èŠ‚æ•°ï¼Œé»˜è®¤ä¸ºâ€œ500MBâ€ã€‚è¯¥å¤§å°åŸºäºæœªå‹ç¼©æ•°æ®å¤§å°ï¼Œå› æ­¤å®é™…ä¸Šï¼Œç”±äºParquetå‹ç¼©ï¼Œæ‚¨çš„åˆ†ç‰‡æ–‡ä»¶å¯èƒ½æ¯”`max_shard_size`å°ã€‚

    åœ¨2.5.0ä¸­æ·»åŠ 

+   `num_proc` (`int`, *å¯é€‰*, é»˜è®¤ä¸º `None`) â€” ä¸‹è½½å’Œæœ¬åœ°ç”Ÿæˆæ•°æ®é›†æ—¶çš„è¿›ç¨‹æ•°ã€‚é»˜è®¤æƒ…å†µä¸‹ç¦ç”¨å¤šè¿›ç¨‹ã€‚

    åœ¨2.7.0ä¸­æ·»åŠ 

+   `storage_options` (`dict`, *å¯é€‰*) â€” è¦ä¼ é€’ç»™ç¼“å­˜æ–‡ä»¶ç³»ç»Ÿåç«¯çš„é”®/å€¼å¯¹ã€‚

    åœ¨2.5.0ä¸­æ·»åŠ 

+   *`*download_and_prepare_kwargs` (é¢å¤–çš„å…³é”®å­—å‚æ•°) â€” å…³é”®å­—å‚æ•°ã€‚

ä¸‹è½½å¹¶å‡†å¤‡ç”¨äºè¯»å–çš„æ•°æ®é›†ã€‚

ç¤ºä¾‹ï¼š

ä¸‹è½½å¹¶å‡†å¤‡å¯ä»¥ä½¿ç”¨`builder.as_dataset()`åŠ è½½ä¸ºæ•°æ®é›†çš„Arrowæ–‡ä»¶ï¼š

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare()
```

ä¸‹è½½å¹¶å‡†å¤‡æœ¬åœ°ä½œä¸ºåˆ†ç‰‡Parquetæ–‡ä»¶çš„æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare("./output_dir", file_format="parquet")
```

ä¸‹è½½å¹¶å‡†å¤‡ä½œä¸ºåˆ†ç‰‡Parquetæ–‡ä»¶åœ¨äº‘å­˜å‚¨ä¸­çš„æ•°æ®é›†ï¼š

```py
>>> from datasets import load_dataset_builder
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare("s3://my-bucket/my_rotten_tomatoes", storage_options=storage_options, file_format="parquet")
```

#### `get_all_exported_dataset_infos`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L530)

```py
( )
```

å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä¸ºç©ºå­—å…¸

ç¤ºä¾‹ï¼š

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder('rotten_tomatoes')
>>> ds_builder.get_all_exported_dataset_infos()
{'default': DatasetInfo(description="Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, ``Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.'', Proceedings of the
5.
ion='@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}
```

#### `get_exported_dataset_info`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L545)

```py
( )
```

å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä¸ºç©º`DatasetInfo`

ç¤ºä¾‹ï¼š

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder('rotten_tomatoes')
>>> ds_builder.get_exported_dataset_info()
DatasetInfo(description="Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, ``Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.'', Proceedings of the
5.
ion='@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)
```

#### `get_imported_module_dir`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L736)

```py
( )
```

è¿”å›æ­¤ç±»æˆ–å­ç±»çš„æ¨¡å—è·¯å¾„ã€‚

### `class datasets.GeneratorBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1514)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

åŸºäºå­—å…¸ç”Ÿæˆå™¨çš„æ•°æ®ç”Ÿæˆçš„æ•°æ®é›†çš„åŸºç±»ã€‚

`GeneratorBasedBuilder`æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„ç±»ï¼Œå®ƒæŠ½è±¡äº†`DatasetBuilder`çš„è®¸å¤šæ•°æ®å†™å…¥å’Œè¯»å–ã€‚å®ƒæœŸæœ›å­ç±»å®ç°æ•°æ®é›†åˆ†å‰²ä¸­ç‰¹å¾å­—å…¸çš„ç”Ÿæˆå™¨ï¼ˆ`_split_generators`ï¼‰ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

### `class datasets.BeamBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L2028)

```py
( *args beam_runner = None beam_options = None **kwargs )
```

åŸºäºBeamçš„Builderã€‚

### `class datasets.ArrowBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1779)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

åŸºäºArrowåŠ è½½å‡½æ•°ï¼ˆCSV/JSON/Parquetï¼‰çš„æ•°æ®ç”Ÿæˆçš„æ•°æ®é›†çš„åŸºç±»ã€‚

### `class datasets.BuilderConfig`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L100)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None )
```

å‚æ•°

+   `name` (`str`, é»˜è®¤ä¸º `default`) â€” é…ç½®çš„åç§°ã€‚

+   `version` (`Version` æˆ– `str`, é»˜è®¤ä¸º `0.0.0`) â€” é…ç½®çš„ç‰ˆæœ¬ã€‚

+   `data_dir` (`str`, *å¯é€‰*) â€” åŒ…å«æºæ•°æ®çš„ç›®å½•çš„è·¯å¾„ã€‚

+   `data_files` (`str` æˆ– `Sequence` æˆ– `Mapping`, *å¯é€‰*) â€” æºæ•°æ®æ–‡ä»¶çš„è·¯å¾„ã€‚

+   `description` (`str`, *å¯é€‰*) â€” é…ç½®çš„äººç±»æè¿°ã€‚

`DatasetBuilder`æ•°æ®é…ç½®çš„åŸºç±»ã€‚

å¸¦æœ‰æ•°æ®é…ç½®é€‰é¡¹çš„`DatasetBuilder`å­ç±»åº”è¯¥ç»§æ‰¿`BuilderConfig`å¹¶æ·»åŠ è‡ªå·±çš„å±æ€§ã€‚

#### `create_config_id`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L144)

```py
( config_kwargs: dict custom_features: Optional = None )
```

é…ç½®idç”¨äºæ„å»ºç¼“å­˜ç›®å½•ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒç­‰äºé…ç½®åç§°ã€‚ä½†æ˜¯ï¼Œé…ç½®çš„åç§°ä¸è¶³ä»¥ä¸ºç”Ÿæˆçš„æ•°æ®é›†æä¾›å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œå› ä¸ºå®ƒæ²¡æœ‰è€ƒè™‘åˆ°ï¼š

+   å¯ä»¥ç”¨æ¥è¦†ç›–å±æ€§çš„é…ç½®kwargs

+   ç”¨äºç¼–å†™æ•°æ®é›†çš„è‡ªå®šä¹‰ç‰¹å¾

+   json/text/csv/pandasæ•°æ®é›†çš„æ•°æ®æ–‡ä»¶

å› æ­¤ï¼Œé…ç½®IDåªæ˜¯é…ç½®åç§°ï¼Œæ ¹æ®è¿™äº›ï¼Œå¯ä»¥é€‰æ‹©æ·»åŠ åç¼€ã€‚

## ä¸‹è½½

### `class datasets.DownloadManager`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L263)

```py
( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None record_checksums = True )
```

#### `download`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L406)

```py
( url_or_urls ) â†’ export const metadata = 'undefined';str or list or dict
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ - è¦ä¸‹è½½çš„URLæˆ–`list`æˆ–`dict`ã€‚æ¯ä¸ªURLéƒ½æ˜¯`str`ã€‚

è¿”å›

`str`æˆ–`list`æˆ–`dict`

ä¸‹è½½çš„è·¯å¾„ä¸ç»™å®šçš„è¾“å…¥`url_or_urls`åŒ¹é…ã€‚

ä¸‹è½½ç»™å®šçš„URLã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªè¿›ç¨‹è¿›è¡Œä¸‹è½½ã€‚ä¼ é€’è‡ªå®šä¹‰çš„`download_config.num_proc`ä»¥æ›´æ”¹æ­¤è¡Œä¸ºã€‚

ç¤ºä¾‹ï¼š

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
```

#### `download_and_extract`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L554)

```py
( url_or_urls ) â†’ export const metadata = 'undefined';extracted_path(s)
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ - è¦ä¸‹è½½å’Œæå–çš„URLæˆ–`list`æˆ–`dict`ã€‚æ¯ä¸ªURLéƒ½æ˜¯`str`ã€‚

è¿”å›

æå–çš„è·¯å¾„

`str`ï¼Œæå–ç»™å®šURLçš„è·¯å¾„ã€‚

ä¸‹è½½å¹¶æå–ç»™å®šçš„`url_or_urls`ã€‚

å¤§è‡´ç›¸å½“äºï¼š

```py
extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))
```

#### `download_custom`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L359)

```py
( url_or_urls custom_download ) â†’ export const metadata = 'undefined';downloaded_path(s)
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ - è¦ä¸‹è½½å’Œæå–çš„URLæˆ–`list`æˆ–`dict`ã€‚æ¯ä¸ªURLéƒ½æ˜¯`str`ã€‚

+   `custom_download`ï¼ˆ`Callable[src_url, dst_path]`ï¼‰ - æºURLå’Œç›®æ ‡è·¯å¾„ã€‚ä¾‹å¦‚`tf.io.gfile.copy`ï¼Œå¯è®©æ‚¨ä»Googleå­˜å‚¨ä¸‹è½½ã€‚

è¿”å›

ä¸‹è½½çš„è·¯å¾„

`str`ï¼ŒåŒ¹é…ç»™å®šè¾“å…¥`url_or_urls`çš„ä¸‹è½½è·¯å¾„ã€‚

é€šè¿‡è°ƒç”¨`custom_download`ä¸‹è½½ç»™å®šçš„URLã€‚

ç¤ºä¾‹ï¼š

```py
>>> downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)
```

#### `extract`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L508)

```py
( path_or_paths num_proc = 'deprecated' ) â†’ export const metadata = 'undefined';extracted_path(s)
```

å‚æ•°

+   `path_or_paths`ï¼ˆè·¯å¾„æˆ–`list`æˆ–`dict`ï¼‰ - è¦æå–çš„æ–‡ä»¶è·¯å¾„ã€‚æ¯ä¸ªè·¯å¾„éƒ½æ˜¯`str`ã€‚

+   `num_proc`ï¼ˆ`int`ï¼‰ - å¦‚æœ`num_proc` > 1ä¸”`path_or_paths`çš„é•¿åº¦å¤§äº`num_proc`ï¼Œåˆ™ä½¿ç”¨å¤šè¿›ç¨‹ã€‚

    åœ¨2.6.2ä¸­å·²å¼ƒç”¨

    å°†`DownloadConfig(num_proc=<num_proc>)`ä¼ é€’ç»™åˆå§‹åŒ–ç¨‹åºã€‚

è¿”å›

æå–çš„è·¯å¾„

`str`ï¼ŒåŒ¹é…ç»™å®šè¾“å…¥`path_or_paths`çš„æå–è·¯å¾„ã€‚

æå–ç»™å®šçš„è·¯å¾„ã€‚

ç¤ºä¾‹ï¼š

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> extracted_files = dl_manager.extract(downloaded_files)
```

#### `iter_archive`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L464)

```py
( path_or_buf: Union ) â†’ export const metadata = 'undefined';tuple[str, io.BufferedReader]
```

å‚æ•°

+   `path_or_buf`ï¼ˆ`str`æˆ–`io.BufferedReader`ï¼‰ - å­˜æ¡£è·¯å¾„æˆ–å­˜æ¡£äºŒè¿›åˆ¶æ–‡ä»¶å¯¹è±¡ã€‚

äº§å‡º

`tuple[str, io.BufferedReader]`

è¿­ä»£å½’æ¡£ä¸­çš„æ–‡ä»¶ã€‚

ç¤ºä¾‹ï¼š

```py
>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> files = dl_manager.iter_archive(archive)
```

#### `iter_files`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L489)

```py
( paths: Union ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `paths`ï¼ˆ`str`æˆ–`str`çš„`list`ï¼‰ - æ ¹è·¯å¾„ã€‚

äº§å‡º

`str`

è¿­ä»£æ–‡ä»¶è·¯å¾„ã€‚

ç¤ºä¾‹ï¼š

```py
>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
>>> files = dl_manager.iter_files(files)
```

#### `ship_files_with_pipeline`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L310)

```py
( downloaded_path_or_paths pipeline )
```

å‚æ•°

+   `downloaded_path_or_paths`ï¼ˆ`str`æˆ–`list[str]`æˆ–`dict[str, str]ï¼‰ - åŒ…å«ä¸‹è½½è·¯å¾„çš„åµŒå¥—ç»“æ„ã€‚

+   `pipeline`ï¼ˆ`utils.beam_utils.BeamPipeline`ï¼‰ - Apache Beam Pipelineã€‚

ä½¿ç”¨Beam FileSystemså°†æ–‡ä»¶ä¼ é€åˆ°ç®¡é“ä¸´æ—¶ç›®å½•ã€‚

### `class datasets.StreamingDownloadManager`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L969)

```py
( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None )
```

ä½¿ç”¨â€::â€åˆ†éš”ç¬¦å¯¼èˆªï¼ˆå¯èƒ½æ˜¯è¿œç¨‹ï¼‰å‹ç¼©å­˜æ¡£çš„ä¸‹è½½ç®¡ç†å™¨ã€‚ä¸å¸¸è§„`DownloadManager`ç›¸åï¼Œ`download`å’Œ`extract`æ–¹æ³•å®é™…ä¸Šä¸ä¼šä¸‹è½½æˆ–æå–æ•°æ®ï¼Œè€Œæ˜¯è¿”å›å¯ä»¥ä½¿ç”¨`xopen`å‡½æ•°æ‰“å¼€çš„è·¯å¾„æˆ–URLï¼Œè¯¥å‡½æ•°æ‰©å±•äº†å†…ç½®çš„`open`å‡½æ•°ä»¥ä»è¿œç¨‹æ–‡ä»¶æµå¼ä¼ è¾“æ•°æ®ã€‚

#### `download`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L995)

```py
( url_or_urls ) â†’ export const metadata = 'undefined';url(s)
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰â€”è¦ä»ä¸­æµå¼ä¼ è¾“æ•°æ®çš„æ–‡ä»¶çš„URL(s)ã€‚æ¯ä¸ªURLéƒ½æ˜¯ä¸€ä¸ª`str`ã€‚

è¿”å›

url(s)

ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ï¼ŒURL(s)ä»¥åŒ¹é…ç»™å®šè¾“å…¥`url_or_urls`çš„æ•°æ®æµã€‚

è§„èŒƒåŒ–è¦ä»ä¸­æµå¼ä¼ è¾“æ•°æ®çš„æ–‡ä»¶çš„URL(s)ã€‚è¿™æ˜¯ç”¨äºæµå¼ä¼ è¾“çš„`DownloadManager.download`çš„å»¶è¿Ÿç‰ˆæœ¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
```

#### `download_and_extract`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1071)

```py
( url_or_urls ) â†’ export const metadata = 'undefined';url(s)
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰â€”è¦ä»ä¸­æµå¼ä¼ è¾“æ•°æ®çš„URL(s)ã€‚æ¯ä¸ªURLéƒ½æ˜¯ä¸€ä¸ª`str`ã€‚

è¿”å›

url(s)

ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ï¼ŒURL(s)ä»¥åŒ¹é…ç»™å®šè¾“å…¥`url_or_urls`çš„æ•°æ®æµã€‚

å‡†å¤‡å¥½ç”¨äºæµå¼ä¼ è¾“çš„`url_or_urls`ï¼ˆæ·»åŠ æå–åè®®ï¼‰ã€‚

è¿™æ˜¯ç”¨äºæµå¼ä¼ è¾“çš„`DownloadManager.download_and_extract`çš„å»¶è¿Ÿç‰ˆæœ¬ã€‚

ç­‰åŒäºï¼š

```py
urls = dl_manager.extract(dl_manager.download(url_or_urls))
```

#### `extract`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1022)

```py
( url_or_urls ) â†’ export const metadata = 'undefined';url(s)
```

å‚æ•°

+   `url_or_urls`ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰â€”è¦ä»ä¸­æµå¼ä¼ è¾“æ•°æ®çš„æ–‡ä»¶çš„URL(s)ã€‚æ¯ä¸ªURLéƒ½æ˜¯ä¸€ä¸ª`str`ã€‚

è¿”å›

url(s)

ï¼ˆ`str`æˆ–`list`æˆ–`dict`ï¼‰ï¼ŒURL(s)ä»¥åŒ¹é…ç»™å®šè¾“å…¥`url_or_urls`çš„æ•°æ®æµã€‚

ä¸ºç»™å®šçš„URL(s)æ·»åŠ æå–åè®®ä»¥è¿›è¡Œæµå¼ä¼ è¾“ã€‚

è¿™æ˜¯ç”¨äºæµå¼ä¼ è¾“çš„`DownloadManager.extract`çš„å»¶è¿Ÿç‰ˆæœ¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> extracted_files = dl_manager.extract(downloaded_files)
```

#### `iter_archive`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1091)

```py
( urlpath_or_buf: Union ) â†’ export const metadata = 'undefined';tuple[str, io.BufferedReader]
```

å‚æ•°

+   `urlpath_or_buf`ï¼ˆ`str`æˆ–`io.BufferedReader`ï¼‰â€”å­˜æ¡£è·¯å¾„æˆ–å­˜æ¡£äºŒè¿›åˆ¶æ–‡ä»¶å¯¹è±¡ã€‚

äº§é‡

`tuple[str, io.BufferedReader]`

åœ¨å­˜æ¡£ä¸­éå†æ–‡ä»¶ã€‚

ç¤ºä¾‹ï¼š

```py
>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> files = dl_manager.iter_archive(archive)
```

#### `iter_files`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1116)

```py
( urlpaths: Union ) â†’ export const metadata = 'undefined';str
```

å‚æ•°

+   `urlpaths`ï¼ˆ`str`æˆ–`str`åˆ—è¡¨ï¼‰â€”æ ¹è·¯å¾„ã€‚

äº§é‡

str

éå†æ–‡ä»¶ã€‚

ç¤ºä¾‹ï¼š

```py
>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
>>> files = dl_manager.iter_files(files)
```

### `class datasets.DownloadConfig`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_config.py#L10)

```py
( cache_dir: Union = None force_download: bool = False resume_download: bool = False local_files_only: bool = False proxies: Optional = None user_agent: Optional = None extract_compressed_file: bool = False force_extract: bool = False delete_extracted: bool = False use_etag: bool = True num_proc: Optional = None max_retries: int = 1 token: Union = None use_auth_token: dataclasses.InitVar[typing.Union[str, bool, NoneType]] = 'deprecated' ignore_url_params: bool = False storage_options: Dict = <factory> download_desc: Optional = None )
```

å‚æ•°

+   `cache_dir`ï¼ˆ`str`æˆ–`Path`ï¼Œ*å¯é€‰*ï¼‰â€”æŒ‡å®šä¸€ä¸ªç¼“å­˜ç›®å½•ä»¥ä¿å­˜æ–‡ä»¶ï¼ˆè¦†ç›–é»˜è®¤ç¼“å­˜ç›®å½•ï¼‰ã€‚

+   `force_download`ï¼ˆé»˜è®¤ä¸º`False`çš„`bool`ï¼‰â€”å¦‚æœä¸º`True`ï¼Œå³ä½¿æ–‡ä»¶å·²ç»ç¼“å­˜åœ¨ç¼“å­˜ç›®å½•ä¸­ï¼Œä¹Ÿä¼šé‡æ–°ä¸‹è½½æ–‡ä»¶ã€‚

+   `resume_download`ï¼ˆé»˜è®¤ä¸º`False`çš„`bool`ï¼‰â€”å¦‚æœä¸º`True`ï¼Œåˆ™åœ¨å‘ç°æœªå®Œå…¨æ¥æ”¶çš„æ–‡ä»¶æ—¶æ¢å¤ä¸‹è½½ã€‚

+   `proxies`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰â€”

+   `user_agent`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€”å°†é™„åŠ åˆ°è¿œç¨‹è¯·æ±‚çš„ç”¨æˆ·ä»£ç†çš„å¯é€‰å­—ç¬¦ä¸²æˆ–å­—å…¸ã€‚

+   `extract_compressed_file`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”å¦‚æœä¸º`True`ä¸”è·¯å¾„æŒ‡å‘zipæˆ–taræ–‡ä»¶ï¼Œåˆ™åœ¨å­˜æ¡£æ—è¾¹æå–å‹ç¼©æ–‡ä»¶ã€‚

+   `force_extract`ï¼ˆé»˜è®¤ä¸º`False`çš„`bool`ï¼‰â€”å¦‚æœ`extract_compressed_file`ä¸º`True`ä¸”å­˜æ¡£å·²ç»è¢«æå–ï¼Œåˆ™é‡æ–°æå–å­˜æ¡£å¹¶è¦†ç›–æå–å­˜æ¡£çš„æ–‡ä»¶å¤¹ã€‚

+   `delete_extracted`ï¼ˆé»˜è®¤ä¸º`False`çš„`bool`ï¼‰â€”æ˜¯å¦åˆ é™¤ï¼ˆæˆ–ä¿ç•™ï¼‰æå–çš„æ–‡ä»¶ã€‚

+   `use_etag`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”æ˜¯å¦ä½¿ç”¨ETag HTTPå“åº”å¤´æ¥éªŒè¯ç¼“å­˜æ–‡ä»¶ã€‚

+   `num_proc`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”è¦å¹¶è¡Œä¸‹è½½æ–‡ä»¶çš„è¿›ç¨‹æ•°ã€‚

+   `max_retries`ï¼ˆé»˜è®¤ä¸º`1`çš„`int`ï¼‰â€”å¦‚æœHTTPè¯·æ±‚å¤±è´¥ï¼Œåˆ™é‡è¯•çš„æ¬¡æ•°ã€‚

+   `token`ï¼ˆ`str`æˆ–`bool`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨ä½œDatasets Hubä¸Šè¿œç¨‹æ–‡ä»¶çš„Bearerä»¤ç‰Œçš„å¯é€‰å­—ç¬¦ä¸²æˆ–å¸ƒå°”å€¼ã€‚å¦‚æœä¸º`True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä»`~/.huggingface`è·å–ä»¤ç‰Œã€‚

+   `use_auth_token`ï¼ˆ`str`æˆ–`bool`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨ä½œDatasets Hubä¸Šè¿œç¨‹æ–‡ä»¶çš„Bearerä»¤ç‰Œçš„å¯é€‰å­—ç¬¦ä¸²æˆ–å¸ƒå°”å€¼ã€‚å¦‚æœä¸º`True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä»`~/.huggingface`è·å–ä»¤ç‰Œã€‚

    åœ¨2.14.0ä¸­å·²å¼ƒç”¨

    `use_auth_token`åœ¨ç‰ˆæœ¬2.14.0ä¸­å·²å¼ƒç”¨ï¼Œæ”¹ç”¨`token`ï¼Œå¹¶å°†åœ¨3.0.0ä¸­åˆ é™¤ã€‚

+   `ignore_url_params`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”åœ¨å°†å…¶ç”¨äºç¼“å­˜æ–‡ä»¶ä¹‹å‰ï¼Œæ˜¯å¦å‰¥ç¦»ä¸‹è½½URLä¸­çš„æ‰€æœ‰æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µã€‚

+   `storage_options`ï¼ˆ`dict`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ä¼ é€’ç»™æ•°æ®é›†æ–‡ä»¶ç³»ç»Ÿåç«¯çš„é”®/å€¼å¯¹ã€‚

+   `download_desc`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨ä¸‹è½½æ–‡ä»¶æ—¶æ˜¾ç¤ºåœ¨è¿›åº¦æ¡æ—è¾¹çš„æè¿°ã€‚

æˆ‘ä»¬çš„ç¼“å­˜è·¯å¾„ç®¡ç†å™¨çš„é…ç½®ã€‚

### `class datasets.DownloadMode`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L85)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

`Enum`ç”¨äºå¤„ç†é¢„å…ˆå­˜åœ¨çš„ä¸‹è½½å’Œæ•°æ®ã€‚

é»˜è®¤æ¨¡å¼æ˜¯`REUSE_DATASET_IF_EXISTS`ï¼Œå¦‚æœå­˜åœ¨åŸå§‹ä¸‹è½½å’Œå‡†å¤‡å¥½çš„æ•°æ®é›†ï¼Œåˆ™å°†é‡ç”¨å®ƒä»¬ã€‚

ç”Ÿæˆæ¨¡å¼ï¼š

|  | ä¸‹è½½ | æ•°æ®é›† |
| --- | --- | --- |
| `REUSE_DATASET_IF_EXISTS`ï¼ˆé»˜è®¤ï¼‰ | é‡ç”¨ | é‡ç”¨ |
| `REUSE_CACHE_IF_EXISTS` | é‡ç”¨ | æ–°é²œ |
| `FORCE_REDOWNLOAD` | æ–°é²œ | æ–°é²œ |

## éªŒè¯

### `class datasets.VerificationMode`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/utils/info_utils.py#L14)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

æŒ‡å®šè¦è¿è¡Œå“ªäº›éªŒè¯æ£€æŸ¥çš„`Enum`ã€‚

é»˜è®¤æ¨¡å¼æ˜¯`BASIC_CHECKS`ï¼Œä»…æ‰§è¡ŒåŸºæœ¬æ£€æŸ¥ï¼Œä»¥é¿å…åœ¨é¦–æ¬¡ç”Ÿæˆ/ä¸‹è½½æ•°æ®é›†æ—¶å‡æ…¢é€Ÿåº¦ã€‚

éªŒè¯æ¨¡å¼ï¼š

|  | éªŒè¯æ£€æŸ¥ |
| --- | --- |
| `ALL_CHECKS` | æ‹†åˆ†æ£€æŸ¥ï¼ŒGeneratorBuilderä¸­ç”Ÿæˆçš„é”®çš„å”¯ä¸€æ€§ |
|  | ä»¥åŠä¸‹è½½æ–‡ä»¶çš„æœ‰æ•ˆæ€§ï¼ˆæ–‡ä»¶æ•°é‡ï¼Œæ ¡éªŒå’Œç­‰ï¼‰ |
| `BASIC_CHECKS`ï¼ˆé»˜è®¤ï¼‰ | ä¸`ALL_CHECKS`ç›¸åŒï¼Œä½†ä¸æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶ |
| `NO_CHECKS` | æ—  |

## æ‹†åˆ†

### `class datasets.SplitGenerator`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L604)

```py
( name: str gen_kwargs: Dict = <factory> )
```

å‚æ•°

+   `name`ï¼ˆ`str`ï¼‰â€”ç”Ÿæˆå™¨å°†ä¸ºå…¶åˆ›å»ºç¤ºä¾‹çš„`Split`çš„åç§°ã€‚

+   *`*gen_kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼‰â€”è¦è½¬å‘åˆ°æ„å»ºå™¨çš„`DatasetBuilder._generate_examples`æ–¹æ³•çš„å…³é”®å­—å‚æ•°ã€‚

ä¸ºç”Ÿæˆå™¨å®šä¹‰æ‹†åˆ†ä¿¡æ¯ã€‚

åº”å°†æ­¤ç”¨ä½œ`GeneratorBasedBuilder._split_generators`çš„è¿”å›å€¼ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯å’Œç”¨æ³•ç¤ºä¾‹ï¼Œè¯·å‚é˜…`GeneratorBasedBuilder._split_generators`ã€‚

ç¤ºä¾‹ï¼š

```py
>>> datasets.SplitGenerator(
...     name=datasets.Split.TRAIN,
...     gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
... )
```

### `class datasets.Split`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L407)

```py
( name )
```

æ•°æ®é›†æ‹†åˆ†çš„`Enum`ã€‚

æ•°æ®é›†é€šå¸¸è¢«æ‹†åˆ†ä¸ºä¸åŒçš„å­é›†ï¼Œä»¥åœ¨è®­ç»ƒå’Œè¯„ä¼°çš„å„ä¸ªé˜¶æ®µä½¿ç”¨ã€‚

+   `TRAIN`ï¼šè®­ç»ƒæ•°æ®ã€‚

+   `VALIDATION`ï¼šéªŒè¯æ•°æ®ã€‚å¦‚æœå­˜åœ¨ï¼Œé€šå¸¸åœ¨è¿­ä»£æ¨¡å‹æ—¶ç”¨ä½œè¯„ä¼°æ•°æ®ï¼ˆä¾‹å¦‚æ›´æ”¹è¶…å‚æ•°ï¼Œæ¨¡å‹æ¶æ„ç­‰ï¼‰ã€‚

+   `TEST`ï¼šæµ‹è¯•æ•°æ®ã€‚è¿™æ˜¯è¦æŠ¥å‘ŠæŒ‡æ ‡çš„æ•°æ®ã€‚é€šå¸¸åœ¨æ¨¡å‹è¿­ä»£è¿‡ç¨‹ä¸­ä¸å¸Œæœ›ä½¿ç”¨æ­¤æ•°æ®ï¼Œå› ä¸ºå¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚

+   `ALL`ï¼šæ‰€æœ‰å®šä¹‰çš„æ•°æ®é›†æ‹†åˆ†çš„å¹¶é›†ã€‚

æ‰€æœ‰æ‹†åˆ†ï¼ŒåŒ…æ‹¬ç»„åˆï¼Œéƒ½ç»§æ‰¿è‡ª`datasets.SplitBase`ã€‚

æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æŒ‡å—](../load_hub#splits)ã€‚

ç¤ºä¾‹ï¼š

```py
>>> datasets.SplitGenerator(
...     name=datasets.Split.TRAIN,
...     gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
... ),
... datasets.SplitGenerator(
...     name=datasets.Split.VALIDATION,
...     gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
... ),
... datasets.SplitGenerator(
...     name=datasets.Split.TEST,
...     gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
... )
```

### `class datasets.NamedSplit`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L315)

```py
( name )
```

ä¸å‘½åæ‹†åˆ†ï¼ˆè®­ç»ƒï¼Œæµ‹è¯•ç­‰ï¼‰å¯¹åº”çš„æè¿°ç¬¦ã€‚

ç¤ºä¾‹ï¼š

æ¯ä¸ªæè¿°ç¬¦éƒ½å¯ä»¥ä½¿ç”¨åŠ æ³•æˆ–åˆ‡ç‰‡ä¸å…¶ä»–æè¿°ç¬¦ç»„åˆï¼š

```py
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST
```

ç”Ÿæˆçš„æ‹†åˆ†å°†å¯¹åº”äºè®­ç»ƒæ‹†åˆ†çš„25%ä¸æµ‹è¯•æ‹†åˆ†çš„100%åˆå¹¶ã€‚

æ‹†åˆ†ä¸èƒ½æ·»åŠ ä¸¤æ¬¡ï¼Œå› æ­¤ä»¥ä¸‹å°†å¤±è´¥ï¼š

```py
split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error
```

åˆ‡ç‰‡åªèƒ½åº”ç”¨ä¸€æ¬¡ã€‚å› æ­¤ä»¥ä¸‹æ˜¯æœ‰æ•ˆçš„ï¼š

```py
split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])
```

ä½†è¿™æ˜¯æ— æ•ˆçš„ï¼š

```py
train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])
```

### `class datasets.NamedSplitAll`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L392)

```py
( )
```

å°†æ‰€æœ‰å®šä¹‰çš„æ•°æ®é›†æ‹†åˆ†çš„å¹¶é›†æ‹†åˆ†ã€‚

### `class datasets.ReadInstruction`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L495)

```py
( split_name rounding = None from_ = None to = None unit = None )
```

æ•°æ®é›†çš„è¯»å–æŒ‡ä»¤ã€‚

ç¤ºä¾‹ï¼š

```py
# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])
```

#### `from_spec`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L575)

```py
( spec )
```

å‚æ•°

+   `spec`ï¼ˆ`str`ï¼‰â€” æ‹†åˆ† + å¯é€‰åˆ‡ç‰‡ + å¦‚æœç™¾åˆ†æ¯”ç”¨ä½œåˆ‡ç‰‡å•ä½ï¼Œåˆ™å¯é€‰å››èˆäº”å…¥ã€‚å¯ä»¥æŒ‡å®šä¸€ä¸ªåˆ‡ç‰‡ï¼Œä½¿ç”¨ç»å¯¹æ•°å­—ï¼ˆ`int`ï¼‰æˆ–ç™¾åˆ†æ¯”ï¼ˆ`int`ï¼‰ã€‚

ä»å­—ç¬¦ä¸²è§„èŒƒåˆ›å»ºä¸€ä¸ª`ReadInstruction`å®ä¾‹ã€‚

ç¤ºä¾‹ï¼š

```py
test: test split.
test + validation: test split + validation split.
test[10:]: test split, minus its first 10 records.
test[:10%]: first 10% records of test split.
test[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.
test[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.
```

#### `to_absolute`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L647)

```py
( name2len )
```

å‚æ•°

+   `name2len`ï¼ˆ`dict`ï¼‰â€” å°†æ‹†åˆ†åç§°ä¸ç¤ºä¾‹æ•°é‡å…³è”èµ·æ¥ã€‚

å°†æŒ‡ä»¤ç¿»è¯‘æˆç»å¯¹æŒ‡ä»¤åˆ—è¡¨ã€‚

ç„¶åå°†è¿™äº›ç»å¯¹æŒ‡ä»¤ç›¸åŠ ã€‚

## ç‰ˆæœ¬

### `class datasets.Version`

[<æ¥æº>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/utils/version.py#L28)

```py
( version_str: str description: Optional = None major: Union = None minor: Union = None patch: Union = None )
```

å‚æ•°

+   `version_str`ï¼ˆ`str`ï¼‰â€” æ•°æ®é›†ç‰ˆæœ¬ã€‚

+   `description`ï¼ˆ`str`ï¼‰â€” è¿™ä¸ªç‰ˆæœ¬ä¸­çš„æ–°å†…å®¹æè¿°ã€‚

+   `major`ï¼ˆ`str`ï¼‰â€”

+   `minor`ï¼ˆ`str`ï¼‰â€”

+   `patch`ï¼ˆ`str`ï¼‰â€”

æ•°æ®é›†ç‰ˆæœ¬ `MAJOR.MINOR.PATCH`ã€‚

ç¤ºä¾‹ï¼š

```py
>>> VERSION = datasets.Version("1.0.0")
```
