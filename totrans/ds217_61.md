# 构建器类

> 原文链接: [https://huggingface.co/docs/datasets/package_reference/builder_classes](https://huggingface.co/docs/datasets/package_reference/builder_classes)

## 构建器

🤗 数据集在数据集构建过程中依赖于两个主要类：[DatasetBuilder](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder) 和 [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)。

### `class datasets.DatasetBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L214)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

参数

+   `cache_dir` (`str`, *可选*) — 缓存数据的目录。默认为 `"~/.cache/huggingface/datasets"`。

+   `dataset_name` (`str`, *可选*) — 数据集的名称，如果与构建器名称不同。对于打包的构建器（如 csv、imagefolder、audiofolder 等），用于反映使用相同打包构建器的数据集之间的差异。

+   `config_name` (`str`, *可选*) — 数据集配置的名称。它会影响磁盘上生成的数据。不同的配置将有自己的子目录和版本。如果未提供，则使用默认配置（如果存在）。

    在 2.3.0 中添加

    参数 `name` 已重命名为 `config_name`。

+   `hash` (`str`, *可选*) — 特定于数据集代码的哈希值。用于在数据集加载脚本代码更新时更新缓存目录（以避免重用旧数据）。典型的缓存目录（在 `self._relative_data_dir` 中定义）是 `name/version/hash/`。

+   `base_path` (`str`, *可选*) — 用于下载文件的相对路径的基本路径。这可以是远程 URL。

+   `features` ([Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features), *可选*) — 用于此数据集的特征类型。它可以用于更改数据集的 [Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features) 类型，例如。

+   `token` (`str` 或 `bool`, *可选*) — 用作数据集中远程文件的 Bearer token 的字符串或布尔值。如果为 `True`，将从 `"~/.huggingface"` 获取令牌。

+   `repo_id` (`str`, *可选*) — 数据集存储库的 ID。用于区分具有相同名称但不来自相同命名空间的构建器，例如“squad”和“lhoestq/squad”存储库 ID。在后者中，构建器名称将是“lhoestq___squad”。

+   `data_files` (`str` 或 `Sequence` 或 `Mapping`, *可选*) — 源数据文件的路径。对于像“csv”或“json”这样需要用户指定数据文件的构建器。它们可以是本地文件或远程文件。为方便起见，您可以使用 `DataFilesDict`。

+   `data_dir` (`str`, *可选*) — 包含源数据文件的目录路径。仅在未传递 `data_files` 的情况下使用，此时它等效于将 `os.path.join(data_dir, "**")` 作为 `data_files`。对于需要手动下载的构建器，它必须是包含手动下载数据的本地目录的路径。

+   `storage_options` (`dict`, *可选*) — 要传递给数据集文件系统后端的键/值对，如果有的话。

+   `writer_batch_size` (`int`, *可选*) — ArrowWriter 使用的批处理大小。它定义了在写入之前在内存中保留的样本数量，也定义了箭头块的长度。None 表示 ArrowWriter 将使用其默认值。

+   `name` (`str`) — 数据集的配置名称。

    在 2.3.0 中已弃用

    请使用 `config_name`。

+   *`*config_kwargs`（额外的关键字参数） — 要传递给相应构建器配置类的关键字参数，设置在类属性 [DatasetBuilder.BUILDER_CONFIG_CLASS](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig) 上。构建器配置类是 [BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig) 或其子类。

所有数据集的抽象基类。

`DatasetBuilder` 有 3 个关键方法：

+   `DatasetBuilder.info`：记录数据集，包括特征名称、类型、形状、版本、拆分、引用等。

+   [DatasetBuilder.download_and_prepare()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare): 下载源数据并将其写入磁盘。

+   [DatasetBuilder.as_dataset()](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset): 生成一个[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)。

一些`DatasetBuilder`通过定义一个[BuilderConfig](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.BuilderConfig)子类并在构造时接受一个配置对象（或名称）来公开数据集的多个变体。可配置的数据集在`DatasetBuilder.builder_configs()`中公开了一组预定义的配置。

#### `as_dataset`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1166)

```py
( split: Optional = None run_post_process = True verification_mode: Union = None ignore_verifications = 'deprecated' in_memory = False )
```

参数

+   `split` (`datasets.Split`) — 要返回的数据的哪个子集。

+   `run_post_process` (`bool`，默认为`True`) — 是否运行后处理数据集转换和/或添加索引。

+   `verification_mode` ([VerificationMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.VerificationMode)或`str`，默认为`BASIC_CHECKS`) — 确定要在下载/处理的数据集信息上运行的检查的验证模式（校验和/大小/拆分/...）。

    在2.9.1中添加

+   `ignore_verifications` (`bool`，默认为`False`) — 是否忽略下载/处理的数据集信息的验证（校验和/大小/拆分/...）。

    在2.9.1中弃用

    `ignore_verifications`在版本2.9.1中已弃用，将在3.0.0中删除。请改用`verification_mode`。

+   `in_memory` (`bool`，默认为`False`) — 是否将数据复制到内存中。

返回指定拆分的数据集。

示例：

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder('rotten_tomatoes')
>>> builder.download_and_prepare()
>>> ds = builder.as_dataset(split='train')
>>> ds
Dataset({
    features: ['text', 'label'],
    num_rows: 8530
})
```

#### `download_and_prepare`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L744)

```py
( output_dir: Optional = None download_config: Optional = None download_mode: Union = None verification_mode: Union = None ignore_verifications = 'deprecated' try_from_hf_gcs: bool = True dl_manager: Optional = None base_path: Optional = None use_auth_token = 'deprecated' file_format: str = 'arrow' max_shard_size: Union = None num_proc: Optional = None storage_options: Optional = None **download_and_prepare_kwargs )
```

参数

+   `output_dir` (`str`，*可选的*) — 数据集的输出目录。默认为此构建器的`cache_dir`，默认情况下位于`~/.cache/huggingface/datasets`内。

    在2.5.0中添加

+   `download_config` (`DownloadConfig`，*可选的*) — 特定的下载配置参数。

+   `download_mode` ([DownloadMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.DownloadMode)或`str`，*可选的*) — 选择下载/生成模式，默认为`REUSE_DATASET_IF_EXISTS`。

+   `verification_mode` ([VerificationMode](/docs/datasets/v2.17.0/en/package_reference/builder_classes#datasets.VerificationMode)或`str`，默认为`BASIC_CHECKS`) — 确定要在下载/处理的数据集信息上运行的检查的验证模式（校验和/大小/拆分/...）。

    在2.9.1中添加

+   `ignore_verifications` (`bool`，默认为`False`) — 忽略下载/处理的数据集信息的验证（校验和/大小/拆分/...）。

    在2.9.1中弃用

    `ignore_verifications`在版本2.9.1中已弃用，将在3.0.0中删除。请改用`verification_mode`。

+   `try_from_hf_gcs` (`bool`) — 如果为`True`，将尝试从HF Google云存储中下载已准备好的数据集。

+   `dl_manager` (`DownloadManager`，*可选的*) — 要使用的特定`DownloadManger`。

+   `base_path` (`str`，*可选的*) — 用于下载文件的相对路径的基本路径。这可以是一个远程url。如果未指定，将使用`base_path`属性（`self.base_path`）的值。

+   `use_auth_token` (`Union[str, bool]`，*可选的*) — 用作远程文件在数据集中心的Bearer令牌的可选字符串或布尔值。如果为True，或未指定，将从~/.huggingface获取令牌。

    在2.7.1中弃用

    将`use_auth_token`传递给`load_dataset_builder`。

+   `file_format` (`str`, *可选*) — 数据文件的格式，数据集将被写入其中。支持的格式: “arrow”, “parquet”。默认为“arrow”格式。如果格式为“parquet”，则图像和音频数据将嵌入到Parquet文件中，而不是指向本地文件。

    在2.5.0中添加

+   `max_shard_size` (`Union[str, int]`, *可选*) — 每个分片写入的最大字节数，默认为“500MB”。该大小基于未压缩数据大小，因此实际上，由于Parquet压缩，您的分片文件可能比`max_shard_size`小。

    在2.5.0中添加

+   `num_proc` (`int`, *可选*, 默认为 `None`) — 下载和本地生成数据集时的进程数。默认情况下禁用多进程。

    在2.7.0中添加

+   `storage_options` (`dict`, *可选*) — 要传递给缓存文件系统后端的键/值对。

    在2.5.0中添加

+   *`*download_and_prepare_kwargs` (额外的关键字参数) — 关键字参数。

下载并准备用于读取的数据集。

示例：

下载并准备可以使用`builder.as_dataset()`加载为数据集的Arrow文件：

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare()
```

下载并准备本地作为分片Parquet文件的数据集：

```py
>>> from datasets import load_dataset_builder
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare("./output_dir", file_format="parquet")
```

下载并准备作为分片Parquet文件在云存储中的数据集：

```py
>>> from datasets import load_dataset_builder
>>> storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}
>>> builder = load_dataset_builder("rotten_tomatoes")
>>> builder.download_and_prepare("s3://my-bucket/my_rotten_tomatoes", storage_options=storage_options, file_format="parquet")
```

#### `get_all_exported_dataset_infos`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L530)

```py
( )
```

如果不存在，则为空字典

示例：

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder('rotten_tomatoes')
>>> ds_builder.get_all_exported_dataset_infos()
{'default': DatasetInfo(description="Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, ``Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.'', Proceedings of the
5.
ion='@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}
```

#### `get_exported_dataset_info`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L545)

```py
( )
```

如果不存在，则为空`DatasetInfo`

示例：

```py
>>> from datasets import load_dataset_builder
>>> ds_builder = load_dataset_builder('rotten_tomatoes')
>>> ds_builder.get_exported_dataset_info()
DatasetInfo(description="Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, ``Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.'', Proceedings of the
5.
ion='@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age='http://www.cs.cornell.edu/people/pabo/movie-review-data/', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input='', output=''), task_templates=[TextClassification(task='text-classification', text_column='text', label_column='label')], builder_name='rotten_tomatoes_movie_review', config_name='default', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=1074810, num_examples=8530, dataset_name='rotten_tomatoes_movie_review'), 'validation': SplitInfo(name='validation', num_bytes=134679, num_examples=1066, dataset_name='rotten_tomatoes_movie_review'), 'test': SplitInfo(name='test', num_bytes=135972, num_examples=1066, dataset_name='rotten_tomatoes_movie_review')}, download_checksums={'https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz': {'num_bytes': 487770, 'checksum': 'a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9'}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)
```

#### `get_imported_module_dir`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L736)

```py
( )
```

返回此类或子类的模块路径。

### `class datasets.GeneratorBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1514)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

基于字典生成器的数据生成的数据集的基类。

`GeneratorBasedBuilder`是一个方便的类，它抽象了`DatasetBuilder`的许多数据写入和读取。它期望子类实现数据集分割中特征字典的生成器（`_split_generators`）。有关详细信息，请参阅方法文档字符串。

### `class datasets.BeamBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L2028)

```py
( *args beam_runner = None beam_options = None **kwargs )
```

基于Beam的Builder。

### `class datasets.ArrowBasedBuilder`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L1779)

```py
( cache_dir: Optional = None dataset_name: Optional = None config_name: Optional = None hash: Optional = None base_path: Optional = None info: Optional = None features: Optional = None token: Union = None use_auth_token = 'deprecated' repo_id: Optional = None data_files: Union = None data_dir: Optional = None storage_options: Optional = None writer_batch_size: Optional = None name = 'deprecated' **config_kwargs )
```

基于Arrow加载函数（CSV/JSON/Parquet）的数据生成的数据集的基类。

### `class datasets.BuilderConfig`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L100)

```py
( name: str = 'default' version: Union = 0.0.0 data_dir: Optional = None data_files: Union = None description: Optional = None )
```

参数

+   `name` (`str`, 默认为 `default`) — 配置的名称。

+   `version` (`Version` 或 `str`, 默认为 `0.0.0`) — 配置的版本。

+   `data_dir` (`str`, *可选*) — 包含源数据的目录的路径。

+   `data_files` (`str` 或 `Sequence` 或 `Mapping`, *可选*) — 源数据文件的路径。

+   `description` (`str`, *可选*) — 配置的人类描述。

`DatasetBuilder`数据配置的基类。

带有数据配置选项的`DatasetBuilder`子类应该继承`BuilderConfig`并添加自己的属性。

#### `create_config_id`

[< source >](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/builder.py#L144)

```py
( config_kwargs: dict custom_features: Optional = None )
```

配置id用于构建缓存目录。默认情况下，它等于配置名称。但是，配置的名称不足以为生成的数据集提供唯一标识符，因为它没有考虑到：

+   可以用来覆盖属性的配置kwargs

+   用于编写数据集的自定义特征

+   json/text/csv/pandas数据集的数据文件

因此，配置ID只是配置名称，根据这些，可以选择添加后缀。

## 下载

### `class datasets.DownloadManager`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L263)

```py
( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None record_checksums = True )
```

#### `download`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L406)

```py
( url_or_urls ) → export const metadata = 'undefined';str or list or dict
```

参数

+   `url_or_urls`（`str`或`list`或`dict`） - 要下载的URL或`list`或`dict`。每个URL都是`str`。

返回

`str`或`list`或`dict`

下载的路径与给定的输入`url_or_urls`匹配。

下载给定的URL。

默认情况下，仅使用一个进程进行下载。传递自定义的`download_config.num_proc`以更改此行为。

示例：

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
```

#### `download_and_extract`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L554)

```py
( url_or_urls ) → export const metadata = 'undefined';extracted_path(s)
```

参数

+   `url_or_urls`（`str`或`list`或`dict`） - 要下载和提取的URL或`list`或`dict`。每个URL都是`str`。

返回

提取的路径

`str`，提取给定URL的路径。

下载并提取给定的`url_or_urls`。

大致相当于：

```py
extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))
```

#### `download_custom`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L359)

```py
( url_or_urls custom_download ) → export const metadata = 'undefined';downloaded_path(s)
```

参数

+   `url_or_urls`（`str`或`list`或`dict`） - 要下载和提取的URL或`list`或`dict`。每个URL都是`str`。

+   `custom_download`（`Callable[src_url, dst_path]`） - 源URL和目标路径。例如`tf.io.gfile.copy`，可让您从Google存储下载。

返回

下载的路径

`str`，匹配给定输入`url_or_urls`的下载路径。

通过调用`custom_download`下载给定的URL。

示例：

```py
>>> downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)
```

#### `extract`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L508)

```py
( path_or_paths num_proc = 'deprecated' ) → export const metadata = 'undefined';extracted_path(s)
```

参数

+   `path_or_paths`（路径或`list`或`dict`） - 要提取的文件路径。每个路径都是`str`。

+   `num_proc`（`int`） - 如果`num_proc` > 1且`path_or_paths`的长度大于`num_proc`，则使用多进程。

    在2.6.2中已弃用

    将`DownloadConfig(num_proc=<num_proc>)`传递给初始化程序。

返回

提取的路径

`str`，匹配给定输入`path_or_paths`的提取路径。

提取给定的路径。

示例：

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> extracted_files = dl_manager.extract(downloaded_files)
```

#### `iter_archive`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L464)

```py
( path_or_buf: Union ) → export const metadata = 'undefined';tuple[str, io.BufferedReader]
```

参数

+   `path_or_buf`（`str`或`io.BufferedReader`） - 存档路径或存档二进制文件对象。

产出

`tuple[str, io.BufferedReader]`

迭代归档中的文件。

示例：

```py
>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> files = dl_manager.iter_archive(archive)
```

#### `iter_files`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L489)

```py
( paths: Union ) → export const metadata = 'undefined';str
```

参数

+   `paths`（`str`或`str`的`list`） - 根路径。

产出

`str`

迭代文件路径。

示例：

```py
>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
>>> files = dl_manager.iter_files(files)
```

#### `ship_files_with_pipeline`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L310)

```py
( downloaded_path_or_paths pipeline )
```

参数

+   `downloaded_path_or_paths`（`str`或`list[str]`或`dict[str, str]） - 包含下载路径的嵌套结构。

+   `pipeline`（`utils.beam_utils.BeamPipeline`） - Apache Beam Pipeline。

使用Beam FileSystems将文件传送到管道临时目录。

### `class datasets.StreamingDownloadManager`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L969)

```py
( dataset_name: Optional = None data_dir: Optional = None download_config: Optional = None base_path: Optional = None )
```

使用”::”分隔符导航（可能是远程）压缩存档的下载管理器。与常规`DownloadManager`相反，`download`和`extract`方法实际上不会下载或提取数据，而是返回可以使用`xopen`函数打开的路径或URL，该函数扩展了内置的`open`函数以从远程文件流式传输数据。

#### `download`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L995)

```py
( url_or_urls ) → export const metadata = 'undefined';url(s)
```

参数

+   `url_or_urls`（`str`或`list`或`dict`）—要从中流式传输数据的文件的URL(s)。每个URL都是一个`str`。

返回

url(s)

（`str`或`list`或`dict`），URL(s)以匹配给定输入`url_or_urls`的数据流。

规范化要从中流式传输数据的文件的URL(s)。这是用于流式传输的`DownloadManager.download`的延迟版本。

示例：

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
```

#### `download_and_extract`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1071)

```py
( url_or_urls ) → export const metadata = 'undefined';url(s)
```

参数

+   `url_or_urls`（`str`或`list`或`dict`）—要从中流式传输数据的URL(s)。每个URL都是一个`str`。

返回

url(s)

（`str`或`list`或`dict`），URL(s)以匹配给定输入`url_or_urls`的数据流。

准备好用于流式传输的`url_or_urls`（添加提取协议）。

这是用于流式传输的`DownloadManager.download_and_extract`的延迟版本。

等同于：

```py
urls = dl_manager.extract(dl_manager.download(url_or_urls))
```

#### `extract`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1022)

```py
( url_or_urls ) → export const metadata = 'undefined';url(s)
```

参数

+   `url_or_urls`（`str`或`list`或`dict`）—要从中流式传输数据的文件的URL(s)。每个URL都是一个`str`。

返回

url(s)

（`str`或`list`或`dict`），URL(s)以匹配给定输入`url_or_urls`的数据流。

为给定的URL(s)添加提取协议以进行流式传输。

这是用于流式传输的`DownloadManager.extract`的延迟版本。

示例：

```py
>>> downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> extracted_files = dl_manager.extract(downloaded_files)
```

#### `iter_archive`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1091)

```py
( urlpath_or_buf: Union ) → export const metadata = 'undefined';tuple[str, io.BufferedReader]
```

参数

+   `urlpath_or_buf`（`str`或`io.BufferedReader`）—存档路径或存档二进制文件对象。

产量

`tuple[str, io.BufferedReader]`

在存档中遍历文件。

示例：

```py
>>> archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
>>> files = dl_manager.iter_archive(archive)
```

#### `iter_files`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/streaming_download_manager.py#L1116)

```py
( urlpaths: Union ) → export const metadata = 'undefined';str
```

参数

+   `urlpaths`（`str`或`str`列表）—根路径。

产量

str

遍历文件。

示例：

```py
>>> files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
>>> files = dl_manager.iter_files(files)
```

### `class datasets.DownloadConfig`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_config.py#L10)

```py
( cache_dir: Union = None force_download: bool = False resume_download: bool = False local_files_only: bool = False proxies: Optional = None user_agent: Optional = None extract_compressed_file: bool = False force_extract: bool = False delete_extracted: bool = False use_etag: bool = True num_proc: Optional = None max_retries: int = 1 token: Union = None use_auth_token: dataclasses.InitVar[typing.Union[str, bool, NoneType]] = 'deprecated' ignore_url_params: bool = False storage_options: Dict = <factory> download_desc: Optional = None )
```

参数

+   `cache_dir`（`str`或`Path`，*可选*）—指定一个缓存目录以保存文件（覆盖默认缓存目录）。

+   `force_download`（默认为`False`的`bool`）—如果为`True`，即使文件已经缓存在缓存目录中，也会重新下载文件。

+   `resume_download`（默认为`False`的`bool`）—如果为`True`，则在发现未完全接收的文件时恢复下载。

+   `proxies`（`dict`，*可选*）—

+   `user_agent`（`str`，*可选*）—将附加到远程请求的用户代理的可选字符串或字典。

+   `extract_compressed_file`（`bool`，默认为`False`）—如果为`True`且路径指向zip或tar文件，则在存档旁边提取压缩文件。

+   `force_extract`（默认为`False`的`bool`）—如果`extract_compressed_file`为`True`且存档已经被提取，则重新提取存档并覆盖提取存档的文件夹。

+   `delete_extracted`（默认为`False`的`bool`）—是否删除（或保留）提取的文件。

+   `use_etag`（`bool`，默认为`True`）—是否使用ETag HTTP响应头来验证缓存文件。

+   `num_proc`（`int`，*可选*）—要并行下载文件的进程数。

+   `max_retries`（默认为`1`的`int`）—如果HTTP请求失败，则重试的次数。

+   `token`（`str`或`bool`，*可选*）—用作Datasets Hub上远程文件的Bearer令牌的可选字符串或布尔值。如果为`True`，或未指定，将从`~/.huggingface`获取令牌。

+   `use_auth_token`（`str`或`bool`，*可选*）—用作Datasets Hub上远程文件的Bearer令牌的可选字符串或布尔值。如果为`True`，或未指定，将从`~/.huggingface`获取令牌。

    在2.14.0中已弃用

    `use_auth_token`在版本2.14.0中已弃用，改用`token`，并将在3.0.0中删除。

+   `ignore_url_params`（`bool`，默认为`False`）—在将其用于缓存文件之前，是否剥离下载URL中的所有查询参数和片段。

+   `storage_options`（`dict`，*可选*）—要传递给数据集文件系统后端的键/值对。

+   `download_desc`（`str`，*可选*）—在下载文件时显示在进度条旁边的描述。

我们的缓存路径管理器的配置。

### `class datasets.DownloadMode`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/download/download_manager.py#L85)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

`Enum`用于处理预先存在的下载和数据。

默认模式是`REUSE_DATASET_IF_EXISTS`，如果存在原始下载和准备好的数据集，则将重用它们。

生成模式：

|  | 下载 | 数据集 |
| --- | --- | --- |
| `REUSE_DATASET_IF_EXISTS`（默认） | 重用 | 重用 |
| `REUSE_CACHE_IF_EXISTS` | 重用 | 新鲜 |
| `FORCE_REDOWNLOAD` | 新鲜 | 新鲜 |

## 验证

### `class datasets.VerificationMode`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/utils/info_utils.py#L14)

```py
( value names = None module = None qualname = None type = None start = 1 )
```

指定要运行哪些验证检查的`Enum`。

默认模式是`BASIC_CHECKS`，仅执行基本检查，以避免在首次生成/下载数据集时减慢速度。

验证模式：

|  | 验证检查 |
| --- | --- |
| `ALL_CHECKS` | 拆分检查，GeneratorBuilder中生成的键的唯一性 |
|  | 以及下载文件的有效性（文件数量，校验和等） |
| `BASIC_CHECKS`（默认） | 与`ALL_CHECKS`相同，但不检查下载的文件 |
| `NO_CHECKS` | 无 |

## 拆分

### `class datasets.SplitGenerator`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L604)

```py
( name: str gen_kwargs: Dict = <factory> )
```

参数

+   `name`（`str`）—生成器将为其创建示例的`Split`的名称。

+   *`*gen_kwargs`（额外的关键字参数）—要转发到构建器的`DatasetBuilder._generate_examples`方法的关键字参数。

为生成器定义拆分信息。

应将此用作`GeneratorBasedBuilder._split_generators`的返回值。有关更多信息和用法示例，请参阅`GeneratorBasedBuilder._split_generators`。

示例：

```py
>>> datasets.SplitGenerator(
...     name=datasets.Split.TRAIN,
...     gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
... )
```

### `class datasets.Split`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L407)

```py
( name )
```

数据集拆分的`Enum`。

数据集通常被拆分为不同的子集，以在训练和评估的各个阶段使用。

+   `TRAIN`：训练数据。

+   `VALIDATION`：验证数据。如果存在，通常在迭代模型时用作评估数据（例如更改超参数，模型架构等）。

+   `TEST`：测试数据。这是要报告指标的数据。通常在模型迭代过程中不希望使用此数据，因为可能会过拟合。

+   `ALL`：所有定义的数据集拆分的并集。

所有拆分，包括组合，都继承自`datasets.SplitBase`。

有关更多信息，请参阅[指南](../load_hub#splits)。

示例：

```py
>>> datasets.SplitGenerator(
...     name=datasets.Split.TRAIN,
...     gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
... ),
... datasets.SplitGenerator(
...     name=datasets.Split.VALIDATION,
...     gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
... ),
... datasets.SplitGenerator(
...     name=datasets.Split.TEST,
...     gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
... )
```

### `class datasets.NamedSplit`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L315)

```py
( name )
```

与命名拆分（训练，测试等）对应的描述符。

示例：

每个描述符都可以使用加法或切片与其他描述符组合：

```py
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST
```

生成的拆分将对应于训练拆分的25%与测试拆分的100%合并。

拆分不能添加两次，因此以下将失败：

```py
split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error
```

切片只能应用一次。因此以下是有效的：

```py
split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])
```

但这是无效的：

```py
train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])
```

### `class datasets.NamedSplitAll`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/splits.py#L392)

```py
( )
```

将所有定义的数据集拆分的并集拆分。

### `class datasets.ReadInstruction`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L495)

```py
( split_name rounding = None from_ = None to = None unit = None )
```

数据集的读取指令。

示例：

```py
# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])
```

#### `from_spec`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L575)

```py
( spec )
```

参数

+   `spec`（`str`）— 拆分 + 可选切片 + 如果百分比用作切片单位，则可选四舍五入。可以指定一个切片，使用绝对数字（`int`）或百分比（`int`）。

从字符串规范创建一个`ReadInstruction`实例。

示例：

```py
test: test split.
test + validation: test split + validation split.
test[10:]: test split, minus its first 10 records.
test[:10%]: first 10% records of test split.
test[:20%](pct1_dropremainder): first 10% records, rounded with the pct1_dropremainder rounding.
test[:-5%]+train[40%:60%]: first 95% of test + middle 20% of train.
```

#### `to_absolute`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/arrow_reader.py#L647)

```py
( name2len )
```

参数

+   `name2len`（`dict`）— 将拆分名称与示例数量关联起来。

将指令翻译成绝对指令列表。

然后将这些绝对指令相加。

## 版本

### `class datasets.Version`

[<来源>](https://github.com/huggingface/datasets/blob/2.17.0/src/datasets/utils/version.py#L28)

```py
( version_str: str description: Optional = None major: Union = None minor: Union = None patch: Union = None )
```

参数

+   `version_str`（`str`）— 数据集版本。

+   `description`（`str`）— 这个版本中的新内容描述。

+   `major`（`str`）—

+   `minor`（`str`）—

+   `patch`（`str`）—

数据集版本 `MAJOR.MINOR.PATCH`。

示例：

```py
>>> VERSION = datasets.Version("1.0.0")
```
