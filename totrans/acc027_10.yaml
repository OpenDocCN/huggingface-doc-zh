- en: Troubleshooting guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•…éšœæ’é™¤æŒ‡å—
- en: 'Original text: [https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting](https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting](https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This guide aims to provide you the tools and knowledge required to navigate
    some common issues. However, as ğŸ¤— Accelerate continuously evolves and the use
    cases and setups are diverse, you might encounter an issue not covered in this
    guide. If the suggestions listed in this guide do not cover your such situation,
    please refer to the final section of the guide, [Asking for Help](#ask-for-help),
    to learn where to find help with your specific issue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—æ—¨åœ¨ä¸ºæ‚¨æä¾›å¯¼èˆªä¸€äº›å¸¸è§é—®é¢˜æ‰€éœ€çš„å·¥å…·å’ŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œç”±äºğŸ¤— Accelerateä¸æ–­å‘å±•ï¼Œç”¨ä¾‹å’Œè®¾ç½®å¤šç§å¤šæ ·ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°æœ¬æŒ‡å—æœªæ¶µç›–çš„é—®é¢˜ã€‚å¦‚æœæœ¬æŒ‡å—ä¸­åˆ—å‡ºçš„å»ºè®®æœªæ¶µç›–æ‚¨çš„æƒ…å†µï¼Œè¯·å‚è€ƒæŒ‡å—çš„æœ€åä¸€éƒ¨åˆ†[å¯»æ±‚å¸®åŠ©](#ask-for-help)ï¼Œäº†è§£å¦‚ä½•åœ¨ç‰¹å®šé—®é¢˜ä¸Šå¯»æ±‚å¸®åŠ©ã€‚
- en: Logging
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ—¥å¿—è®°å½•
- en: When facing an error, logging can help narrow down where it is coming from.
    In a distributed setup with multiple processes, logging can be a challenge, but
    ğŸ¤— Accelerate provides a utility that streamlines the logging process and ensures
    that logs are synchronized and managed effectively across the distributed setup.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: é¢å¯¹é”™è¯¯æ—¶ï¼Œæ—¥å¿—è®°å½•å¯ä»¥å¸®åŠ©ç¼©å°é”™è¯¯æ¥æºã€‚åœ¨å…·æœ‰å¤šä¸ªè¿›ç¨‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œæ—¥å¿—è®°å½•å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†ğŸ¤— Accelerateæä¾›äº†ä¸€ä¸ªå·¥å…·ï¼Œç®€åŒ–äº†æ—¥å¿—è®°å½•è¿‡ç¨‹ï¼Œå¹¶ç¡®ä¿æ—¥å¿—åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­åŒæ­¥å’Œæœ‰æ•ˆç®¡ç†ã€‚
- en: 'To troubleshoot an issue, use `accelerate.logging` instead of the standard
    Python `logging` module:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è§£å†³é—®é¢˜ï¼Œè¯·ä½¿ç”¨`accelerate.logging`è€Œä¸æ˜¯æ ‡å‡†çš„Python `logging`æ¨¡å—ï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To set the log level (`INFO`, `DEBUG`, `WARNING`, `ERROR`, `CRITICAL`), export
    it as the `ACCELERATE_LOG_LEVEL` environment, or pass as `log_level` to `get_logger`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¾ç½®æ—¥å¿—çº§åˆ«ï¼ˆ`INFO`ï¼Œ`DEBUG`ï¼Œ`WARNING`ï¼Œ`ERROR`ï¼Œ`CRITICAL`ï¼‰ï¼Œè¯·å°†å…¶å¯¼å‡ºä¸º`ACCELERATE_LOG_LEVEL`ç¯å¢ƒï¼Œæˆ–å°†å…¶ä½œä¸º`log_level`ä¼ é€’ç»™`get_logger`ï¼š
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By default, the log is called on main processes only. To call it on all processes,
    pass `main_process_only=False`. If a log should be called on all processes and
    in order, also pass `in_order=True`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œæ—¥å¿—ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè°ƒç”¨ã€‚è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨å®ƒï¼Œè¯·ä¼ é€’`main_process_only=False`ã€‚å¦‚æœæ—¥å¿—åº”åœ¨æ‰€æœ‰è¿›ç¨‹ä¸ŠæŒ‰é¡ºåºè°ƒç”¨ï¼Œè¿˜è¦ä¼ é€’`in_order=True`ã€‚
- en: Hanging code and timeout errors
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒ‚èµ·ä»£ç å’Œè¶…æ—¶é”™è¯¯
- en: Mismatched tensor shapes
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸åŒ¹é…çš„å¼ é‡å½¢çŠ¶
- en: If your code seems to be hanging for a significant amount time on a distributed
    setup, a common cause is mismatched shapes of tensors on different devices.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„ä»£ç åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸ŠæŒ‚èµ·äº†å¾ˆé•¿æ—¶é—´ï¼Œä¸€ä¸ªå¸¸è§åŸå› æ˜¯ä¸åŒè®¾å¤‡ä¸Šå¼ é‡çš„å½¢çŠ¶ä¸åŒ¹é…ã€‚
- en: When running scripts in a distributed fashion, functions such as [Accelerator.gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)
    and [Accelerator.reduce()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.reduce)
    are necessary to grab tensors across devices to perform operations on them collectively.
    These (and other) functions rely on `torch.distributed` performing a `gather`
    operation, which requires that tensors have the **exact same shape** across all
    processes. When the tensor shapes donâ€™t match, you will experience handing code,
    and eventually hit a timeout exception.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥åˆ†å¸ƒå¼æ–¹å¼è¿è¡Œè„šæœ¬æ—¶ï¼Œè¯¸å¦‚[Accelerator.gather()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.gather)å’Œ[Accelerator.reduce()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.reduce)ç­‰å‡½æ•°æ˜¯å¿…è¦çš„ï¼Œä»¥è·¨è®¾å¤‡è·å–å¼ é‡ä»¥é›†ä½“æ‰§è¡Œæ“ä½œã€‚è¿™äº›ï¼ˆä»¥åŠå…¶ä»–ï¼‰å‡½æ•°ä¾èµ–äº`torch.distributed`æ‰§è¡Œ`gather`æ“ä½œï¼Œè¿™è¦æ±‚å¼ é‡åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­å…·æœ‰**å®Œå…¨ç›¸åŒçš„å½¢çŠ¶**ã€‚å½“å¼ é‡å½¢çŠ¶ä¸åŒ¹é…æ—¶ï¼Œæ‚¨å°†é‡åˆ°æŒ‚èµ·ä»£ç ï¼Œå¹¶æœ€ç»ˆè§¦å‘è¶…æ—¶å¼‚å¸¸ã€‚
- en: If you suspect this to be the case, use Accelerateâ€™s operational debug mode
    to immediately catch the issue.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ€€ç–‘è¿™ç§æƒ…å†µï¼Œè¯·ä½¿ç”¨Accelerateçš„æ“ä½œè°ƒè¯•æ¨¡å¼ç«‹å³æ•æ‰é—®é¢˜ã€‚
- en: 'The recommended way to enable Accelerateâ€™s operational debug mode is during
    `accelerate config` setup. Alternative ways to enable debug mode are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨Accelerateçš„æ“ä½œè°ƒè¯•æ¨¡å¼çš„æ¨èæ–¹æ³•æ˜¯åœ¨`accelerate config`è®¾ç½®æœŸé—´ã€‚å¯ç”¨è°ƒè¯•æ¨¡å¼çš„æ›¿ä»£æ–¹æ³•åŒ…æ‹¬ï¼š
- en: 'From the CLI:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»CLIï¼š
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As an environmental variable (which avoids the need for `accelerate launch`):'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¯å¢ƒå˜é‡ï¼ˆé¿å…éœ€è¦`accelerate launch`ï¼‰ï¼š
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Manually changing the `config.yaml` file:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨æ›´æ”¹`config.yaml`æ–‡ä»¶ï¼š
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once you enable the debug mode, you should get a similar traceback that points
    to the tensor shape mismatch issue:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‚¨åº”è¯¥è·å¾—ç±»ä¼¼çš„å›æº¯ï¼ŒæŒ‡å‘å¼ é‡å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜ï¼š
- en: '[PRE5]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Early stopping leads to hanging
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ—©åœæ­¢å¯¼è‡´æŒ‚èµ·
- en: When doing early stopping in distributed training, if each process has a specific
    stopping condition (e.g. validation loss), it may not be synchronized across all
    of them. As a result, a break can happen on process 0 but not on process 1. This
    will cause the code to hang indefinitely until a timeout occurs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è¿›è¡Œæ—©åœæ­¢æ—¶ï¼Œå¦‚æœæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ç‰¹å®šçš„åœæ­¢æ¡ä»¶ï¼ˆä¾‹å¦‚éªŒè¯æŸå¤±ï¼‰ï¼Œå¯èƒ½ä¸ä¼šåœ¨æ‰€æœ‰è¿›ç¨‹ä¹‹é—´åŒæ­¥ã€‚ç»“æœï¼Œè¿›ç¨‹0ä¸Šå¯èƒ½å‘ç”Ÿä¸­æ–­ï¼Œä½†è¿›ç¨‹1ä¸Šå¯èƒ½æ²¡æœ‰ã€‚è¿™å°†å¯¼è‡´ä»£ç æ— é™æœŸæŒ‚èµ·ï¼Œç›´åˆ°è¶…æ—¶å‘ç”Ÿã€‚
- en: 'If you have early stopping conditionals, use `set_breakpoint` and `check_breakpoint`
    methods to make sure all the processes are ended correctly:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰æ—©åœæ­¢æ¡ä»¶ï¼Œè¯·ä½¿ç”¨`set_breakpoint`å’Œ`check_breakpoint`æ–¹æ³•ç¡®ä¿æ‰€æœ‰è¿›ç¨‹æ­£ç¡®ç»“æŸï¼š
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hanging on low kernel versions on Linux
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åœ¨Linuxä½å†…æ ¸ç‰ˆæœ¬ä¸ŠæŒ‚èµ·
- en: This is a known issue. On Linux with kernel version < 5.5, hanging processes
    have been reported. To avoid encountering this problem, we recommend upgrading
    your system to a later kernel version.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå·²çŸ¥é—®é¢˜ã€‚åœ¨Linuxä¸Šï¼Œå†…æ ¸ç‰ˆæœ¬<5.5ï¼Œå·²ç»æŠ¥å‘Šäº†æŒ‚èµ·è¿›ç¨‹ã€‚ä¸ºäº†é¿å…é‡åˆ°è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®å°†ç³»ç»Ÿå‡çº§åˆ°æ›´é«˜ç‰ˆæœ¬çš„å†…æ ¸ã€‚
- en: CUDA out of memory
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDAå†…å­˜ä¸è¶³
- en: One of the most frustrating errors when it comes to running training scripts
    is hitting â€œCUDA Out-of-Memoryâ€, as the entire script needs to be restarted, progress
    is lost, and typically a developer would want to simply start their script and
    let it run.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿è¡Œè®­ç»ƒè„šæœ¬æ—¶ï¼Œæœ€ä»¤äººæ²®ä¸§çš„é”™è¯¯ä¹‹ä¸€æ˜¯é‡åˆ°â€œCUDAå†…å­˜ä¸è¶³â€ï¼Œå› ä¸ºæ•´ä¸ªè„šæœ¬éœ€è¦é‡æ–°å¯åŠ¨ï¼Œè¿›åº¦ä¸¢å¤±ï¼Œé€šå¸¸å¼€å‘äººå‘˜å¸Œæœ›ç®€å•åœ°å¯åŠ¨ä»–ä»¬çš„è„šæœ¬å¹¶è®©å…¶è¿è¡Œã€‚
- en: To address this problem, `Accelerate` offers a utility `find_executable_batch_size`
    that is heavily based on [toma](https://github.com/BlackHC/toma). The utility
    retries code that fails due to OOM (out-of-memory) conditions and lowers batch
    sizes automatically.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œ`Accelerate`æä¾›äº†ä¸€ä¸ªå®ç”¨ç¨‹åº`find_executable_batch_size`ï¼Œå®ƒåœ¨å¾ˆå¤§ç¨‹åº¦ä¸ŠåŸºäº[toma](https://github.com/BlackHC/toma)ã€‚è¯¥å®ç”¨ç¨‹åºä¼šé‡è¯•ç”±äºOOMï¼ˆå†…å­˜ä¸è¶³ï¼‰æ¡ä»¶è€Œå¤±è´¥çš„ä»£ç ï¼Œå¹¶è‡ªåŠ¨é™ä½æ‰¹é‡å¤§å°ã€‚
- en: find_executable_batch_size
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: find_executable_batch_size
- en: This algorithm operates with exponential decay, decreasing the batch size in
    half after each failed run on some training script. To use it, restructure your
    training function to include an inner function that includes this wrapper, and
    build your dataloaders inside it. At a minimum, this could look like 4 new lines
    of code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼Œåœ¨æŸäº›è®­ç»ƒè„šæœ¬çš„æ¯æ¬¡å¤±è´¥è¿è¡Œåå°†æ‰¹é‡å¤§å°å‡åŠã€‚è¦ä½¿ç”¨å®ƒï¼Œé‡æ„æ‚¨çš„è®­ç»ƒå‡½æ•°ä»¥åŒ…å«ä¸€ä¸ªåŒ…å«æ­¤åŒ…è£…å™¨çš„å†…éƒ¨å‡½æ•°ï¼Œå¹¶åœ¨å…¶ä¸­æ„å»ºæ‚¨çš„æ•°æ®åŠ è½½å™¨ã€‚è‡³å°‘ï¼Œè¿™å¯èƒ½çœ‹èµ·æ¥åƒ4è¡Œæ–°ä»£ç ã€‚
- en: The inner function *must* take in the batch size as the first parameter, but
    we do not pass one to it when called. The wrapper handles this for us.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å†…éƒ¨å‡½æ•°*å¿…é¡»*å°†æ‰¹é‡å¤§å°ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥ï¼Œä½†åœ¨è°ƒç”¨æ—¶æˆ‘ä»¬ä¸ä¼šä¼ é€’ä¸€ä¸ªã€‚åŒ…è£…å™¨ä¼šä¸ºæˆ‘ä»¬å¤„ç†è¿™ä¸ªã€‚
- en: It should also be noted that anything which will consume CUDA memory and passed
    to the `accelerator` **must** be declared inside the inner function, such as models
    and optimizers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜åº”æ³¨æ„ï¼Œä»»ä½•å°†æ¶ˆè€—CUDAå†…å­˜å¹¶ä¼ é€’ç»™`accelerator`çš„å†…å®¹**å¿…é¡»**åœ¨å†…éƒ¨å‡½æ•°ä¸­å£°æ˜ï¼Œä¾‹å¦‚æ¨¡å‹å’Œä¼˜åŒ–å™¨ã€‚
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To find out more, check the documentation [here](../package_reference/utilities#accelerate.find_executable_batch_size).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šï¼Œè¯·æŸ¥çœ‹æ­¤å¤„çš„æ–‡æ¡£[here](../package_reference/utilities#accelerate.find_executable_batch_size)ã€‚
- en: Non-reproducible results between device setups
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾å¤‡è®¾ç½®ä¹‹é—´çš„ä¸å¯é‡ç°ç»“æœ
- en: If you have changed the device setup and are observing different model performance,
    this is likely due to the fact that you have not updated your script when moving
    from one setup to another. The same script with the same batch size across TPU,
    multi-GPU, and single-GPU with Accelerate will have different results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å·²æ›´æ”¹è®¾å¤‡è®¾ç½®å¹¶è§‚å¯Ÿåˆ°ä¸åŒçš„æ¨¡å‹æ€§èƒ½ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯å› ä¸ºåœ¨ä»ä¸€ä¸ªè®¾ç½®è½¬ç§»åˆ°å¦ä¸€ä¸ªè®¾ç½®æ—¶ï¼Œæ‚¨æ²¡æœ‰æ›´æ–°è„šæœ¬ã€‚åœ¨TPUã€å¤šGPUå’Œå•GPUä¸Accelerateä¸‹ï¼Œç›¸åŒæ‰¹é‡å¤§å°çš„ç›¸åŒè„šæœ¬å°†äº§ç”Ÿä¸åŒçš„ç»“æœã€‚
- en: For example, if you were previously training on a single GPU with a batch size
    of 16, when moving to two GPU setup, you need to change the batch size to 8 to
    have the same effective batch size. This is because when training with Accelerate,
    the batch size passed to the dataloader is the **batch size per GPU**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä»¥å‰åœ¨å•GPUä¸Šè®­ç»ƒï¼Œæ‰¹é‡å¤§å°ä¸º16ï¼Œå½“è½¬ç§»åˆ°ä¸¤ä¸ªGPUè®¾ç½®æ—¶ï¼Œæ‚¨éœ€è¦å°†æ‰¹é‡å¤§å°æ›´æ”¹ä¸º8ï¼Œä»¥è·å¾—ç›¸åŒçš„æœ‰æ•ˆæ‰¹é‡å¤§å°ã€‚è¿™æ˜¯å› ä¸ºåœ¨ä½¿ç”¨Accelerateè¿›è¡Œè®­ç»ƒæ—¶ï¼Œä¼ é€’ç»™æ•°æ®åŠ è½½å™¨çš„æ‰¹é‡å¤§å°æ˜¯**æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°**ã€‚
- en: To make sure you can reproduce the results between the setups, make sure to
    use the same seed, adjust the batch size accordingly, consider scaling the learning
    rate.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç¡®ä¿æ‚¨å¯ä»¥åœ¨ä¸åŒè®¾ç½®ä¹‹é—´é‡ç°ç»“æœï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ç§å­ï¼Œæ ¹æ®æƒ…å†µè°ƒæ•´æ‰¹é‡å¤§å°ï¼Œè€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡ã€‚
- en: For more details and a quick reference for batch sizes, check out the [Comparing
    performance between different device setups](../concept_guides/performance) guide.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ‰¹é‡å¤§å°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯å’Œå¿«é€Ÿå‚è€ƒï¼Œè¯·æŸ¥çœ‹[æ¯”è¾ƒä¸åŒè®¾å¤‡è®¾ç½®ä¹‹é—´æ€§èƒ½](../concept_guides/performance)æŒ‡å—ã€‚
- en: Performance issues on different GPUs
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸åŒGPUä¸Šçš„æ€§èƒ½é—®é¢˜
- en: 'If your multi-GPU setup consists of different GPUs, you may hit some limitations:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨çš„å¤šGPUè®¾ç½®åŒ…å«ä¸åŒçš„GPUï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é™åˆ¶ï¼š
- en: There may be an imbalance in GPU memory between the GPUs. In this case, the
    GPU with smaller memory will limit the batch size or the size of the model that
    can be loaded onto the GPUs.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPUä¹‹é—´çš„æ˜¾å­˜å¯èƒ½å­˜åœ¨ä¸å¹³è¡¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¾å­˜è¾ƒå°çš„GPUå°†é™åˆ¶æ‰¹é‡å¤§å°æˆ–å¯ä»¥åŠ è½½åˆ°GPUä¸Šçš„æ¨¡å‹å¤§å°ã€‚
- en: If you are using GPUs with different performance profiles, the performance will
    be driven by the slowest GPU that you are using as the other GPUs will have to
    wait for it to complete its workload.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä½¿ç”¨å…·æœ‰ä¸åŒæ€§èƒ½é…ç½®æ–‡ä»¶çš„GPUï¼Œæ€§èƒ½å°†ç”±æ‚¨ä½¿ç”¨çš„æœ€æ…¢çš„GPUé©±åŠ¨ï¼Œå› ä¸ºå…¶ä»–GPUå°†ä¸å¾—ä¸ç­‰å¾…å…¶å®Œæˆå·¥ä½œè´Ÿè½½ã€‚
- en: Vastly different GPUs within the same setup can lead to performance bottlenecks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åŒä¸€è®¾ç½®ä¸­æä¸åŒçš„GPUå¯èƒ½å¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
- en: Ask for help
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯»æ±‚å¸®åŠ©
- en: If the above troubleshooting tools and advice did not help you resolve your
    issue, reach out for help to the community and the team.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸Šè¿°æ•…éšœæ’é™¤å·¥å…·å’Œå»ºè®®æ— æ³•å¸®åŠ©æ‚¨è§£å†³é—®é¢˜ï¼Œè¯·å‘ç¤¾åŒºå’Œå›¢é˜Ÿå¯»æ±‚å¸®åŠ©ã€‚
- en: Forums
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®ºå›
- en: Ask for help on the Hugging Face forums - post your question in the [ğŸ¤—Accelerate
    category](https://discuss.huggingface.co/c/accelerate/18) Make sure to write a
    descriptive post with relevant context about your setup and reproducible code
    to maximize the likelihood that your problem is solved!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Hugging Faceè®ºå›ä¸Šå¯»æ±‚å¸®åŠ©-åœ¨[ğŸ¤—Accelerateç±»åˆ«](https://discuss.huggingface.co/c/accelerate/18)ä¸­å‘å¸ƒæ‚¨çš„é—®é¢˜ã€‚ç¡®ä¿å†™ä¸€ä¸ªæè¿°æ€§çš„å¸–å­ï¼Œæä¾›æœ‰å…³æ‚¨çš„è®¾ç½®å’Œå¯é‡ç°ä»£ç çš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°æé«˜è§£å†³é—®é¢˜çš„å¯èƒ½æ€§ï¼
- en: Discord
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Discord
- en: Post a question on [Discord](http://hf.co/join/discord), and let the team and
    the community help you.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[Discord](http://hf.co/join/discord)ä¸Šæé—®ï¼Œè®©å›¢é˜Ÿå’Œç¤¾åŒºå¸®åŠ©æ‚¨ã€‚
- en: GitHub Issues
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHubé—®é¢˜
- en: Create an Issue on the ğŸ¤— Accelerate [GitHub repository](https://github.com/huggingface/accelerate/issues)
    if you suspect to have found a bug related to the library. Include context regarding
    the bug and details about your distributed setup to help us better figure out
    whatâ€™s wrong and how we can fix it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ€€ç–‘å‘ç°ä¸åº“ç›¸å…³çš„é”™è¯¯ï¼Œè¯·åœ¨ğŸ¤— Accelerate [GitHubå­˜å‚¨åº“](https://github.com/huggingface/accelerate/issues)ä¸Šåˆ›å»ºä¸€ä¸ªé—®é¢˜ã€‚åŒ…æ‹¬æœ‰å…³é”™è¯¯çš„ä¸Šä¸‹æ–‡å’Œæœ‰å…³æ‚¨çš„åˆ†å¸ƒå¼è®¾ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°æ‰¾å‡ºé—®é¢˜æ‰€åœ¨ä»¥åŠå¦‚ä½•è§£å†³å®ƒã€‚
