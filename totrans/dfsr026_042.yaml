- en: Shap-E
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/using-diffusers/shap-e](https://huggingface.co/docs/diffusers/using-diffusers/shap-e)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'Shap-E is a conditional model for generating 3D assets which could be used
    for video game development, interior design, and architecture. It is trained on
    a large dataset of 3D assets, and post-processed to render more views of each
    object and produce 16K instead of 4K point clouds. The Shap-E model is trained
    in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: an encoder accepts the point clouds and rendered views of a 3D asset and outputs
    the parameters of implicit functions that represent the asset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a diffusion model is trained on the latents produced by the encoder to generate
    either neural radiance fields (NeRFs) or a textured 3D mesh, making it easier
    to render and use the 3D asset in downstream applications
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This guide will show you how to use Shap-E to start generating your own 3D assets!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have the following libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Text-to-3D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate a gif of a 3D object, pass a text prompt to the [ShapEPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/shap_e#diffusers.ShapEPipeline).
    The pipeline generates a list of image frames which are used to create the 3D
    object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now use the [export_to_gif()](/docs/diffusers/v0.26.3/en/api/utilities#diffusers.utils.export_to_gif)
    function to turn the list of image frames into a gif of the 3D object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1731548e3bafd24f5633f1aa2dbba775.png)'
  prefs: []
  type: TYPE_IMG
- en: prompt = "A firecracker"
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e99b86ebe2171b5eec061e465ece01d.png)'
  prefs: []
  type: TYPE_IMG
- en: prompt = "A birthday cupcake"
  prefs: []
  type: TYPE_NORMAL
- en: Image-to-3D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate a 3D object from another image, use the [ShapEImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/shap_e#diffusers.ShapEImg2ImgPipeline).
    You can use an existing image or generate an entirely new one. Letâ€™s use the [Kandinsky
    2.1](../api/pipelines/kandinsky) model to generate a new image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Pass the cheeseburger to the [ShapEImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/shap_e#diffusers.ShapEImg2ImgPipeline)
    to generate a 3D representation of it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3bd8f6cd764240ee2dfdbf3d1d82155f.png)'
  prefs: []
  type: TYPE_IMG
- en: cheeseburger
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13d040413cdeea82e213c9a300bb2e18.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D cheeseburger
  prefs: []
  type: TYPE_NORMAL
- en: Generate mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shap-E is a flexible model that can also generate textured mesh outputs to be
    rendered for downstream applications. In this example, youâ€™ll convert the output
    into a `glb` file because the ðŸ¤— Datasets library supports mesh visualization of
    `glb` files which can be rendered by the [Dataset viewer](https://huggingface.co/docs/hub/datasets-viewer#dataset-preview).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can generate mesh outputs for both the [ShapEPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/shap_e#diffusers.ShapEPipeline)
    and [ShapEImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/shap_e#diffusers.ShapEImg2ImgPipeline)
    by specifying the `output_type` parameter as `"mesh"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `export_to_ply()` function to save the mesh output as a `ply` file:'
  prefs: []
  type: TYPE_NORMAL
- en: You can optionally save the mesh output as an `obj` file with the `export_to_obj()`
    function. The ability to save the mesh output in a variety of formats makes it
    more flexible for downstream usage!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can convert the `ply` file to a `glb` file with the trimesh library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the mesh output is focused from the bottom viewpoint but you can
    change the default viewpoint by applying a rotation transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Upload the mesh file to your dataset repository to visualize it with the Dataset
    viewer!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0289665c1f710f01cb60d1ad8a76f38c.png)'
  prefs: []
  type: TYPE_IMG
