# æ·±åº¦åˆ°å›¾åƒ

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img)

ç¨³å®šæ‰©æ•£æ¨¡å‹è¿˜å¯ä»¥åŸºäºå›¾åƒæ¨æ–­æ·±åº¦ï¼Œä½¿ç”¨[MiDaS](https://github.com/isl-org/MiDaS)ã€‚è¿™å…è®¸æ‚¨ä¼ é€’æ–‡æœ¬æç¤ºå’Œåˆå§‹å›¾åƒæ¥è°ƒèŠ‚ç”Ÿæˆæ–°å›¾åƒçš„è¿‡ç¨‹ï¼Œä»¥åŠä¸€ä¸ª`depth_map`æ¥ä¿ç•™å›¾åƒç»“æ„ã€‚

ç¡®ä¿æŸ¥çœ‹ç¨³å®šæ‰©æ•£ Tips éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆåœ°é‡ç”¨ç®¡é“ç»„ä»¶ï¼

å¦‚æœæ‚¨æœ‰å…´è¶£ä½¿ç”¨å®˜æ–¹æ£€æŸ¥ç‚¹æ¥æ‰§è¡Œä»»åŠ¡ï¼Œè¯·æ¢ç´¢[CompVis](https://huggingface.co/CompVis)ã€[Runway](https://huggingface.co/runwayml)å’Œ[Stability AI](https://huggingface.co/stabilityai) Hub ç»„ç»‡ï¼

## StableDiffusionDepth2ImgPipeline

`diffusers.StableDiffusionDepth2ImgPipeline`ç±»

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L77)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers depth_estimator: DPTForDepthEstimation feature_extractor: DPTFeatureExtractor )
```

å‚æ•°

+   `vae` (AutoencoderKL) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç å’Œè§£ç ä¸ºæ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚

+   `unet` (UNet2DConditionModel) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„`UNet2DConditionModel`ã€‚

+   `scheduler` (SchedulerMixin) â€” ä¸`unet`ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œåœ¨ç‰¹å¾çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯ DDIMSchedulerã€LMSDiscreteScheduler æˆ– PNDMScheduler ä¹‹ä¸€ã€‚

ç”¨äºä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„åŸºäºæ·±åº¦çš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª DiffusionPipelineã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥ç®¡é“è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   load_textual_inversion() ç”¨äºåŠ è½½æ–‡æœ¬åæ¼”åµŒå…¥

+   load_lora_weights() ç”¨äºåŠ è½½ LoRA æƒé‡

+   save_lora_weights() ç”¨äºä¿å­˜ LoRA æƒé‡

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L599)

```py
( prompt: Union = None image: Union = None depth_map: Optional = None strength: float = 0.8 num_inference_steps: Optional = 50 guidance_scale: Optional = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: Optional = 0.0 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, æˆ– `List[np.ndarray]`) â€” ç”¨ä½œèµ·å§‹ç‚¹çš„å›¾åƒæ‰¹æ¬¡çš„`Image`æˆ–å¼ é‡ã€‚åªæœ‰åœ¨`depth_map`ä¸ä¸º`None`æ—¶æ‰èƒ½æ¥å—å›¾åƒæ½œåœ¨ç‰¹å¾ä½œä¸º`image`ã€‚

+   `depth_map` (`torch.FloatTensor`, *å¯é€‰*) â€” æ·±åº¦é¢„æµ‹ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆè¿‡ç¨‹çš„é¢å¤–æ¡ä»¶ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™ä¼šä½¿ç”¨ `self.depth_estimator` è‡ªåŠ¨é¢„æµ‹æ·±åº¦ã€‚

+   `strength` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.8) â€” æŒ‡ç¤ºè¦è½¬æ¢å‚è€ƒ `image` çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚`image` ç”¨ä½œèµ·ç‚¹ï¼Œ`strength` è¶Šé«˜ï¼Œæ·»åŠ çš„å™ªéŸ³å°±è¶Šå¤šã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“ `strength` ä¸º 1 æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³æœ€å¤§ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„ `num_inference_steps` æ¬¡è¿­ä»£ã€‚å€¼ä¸º 1 å®è´¨ä¸Šå¿½ç•¥äº† `image`ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 50) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨æ–­é€Ÿåº¦ã€‚è¯¥å‚æ•°å— `strength` è°ƒèŠ‚ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 7.5) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬ `prompt` å¯†åˆ‡ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“ `guidance_scale > 1` æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’ `negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨å¼•å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰ä¼šè¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” å¯¹åº”äº [DDIM](https://arxiv.org/abs/2010.02502) è®ºæ–‡ä¸­çš„å‚æ•° eta (Î·)ã€‚ä»…é€‚ç”¨äº DDIMSchedulerï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­ä¼šè¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *å¯é€‰*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä» `prompt` è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *å¯é€‰*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä» `negative_prompt` è¾“å…¥å‚æ•°ç”Ÿæˆ `negative_prompt_embeds`ã€‚

+   `output_type` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹© `PIL.Image` æˆ– `np.array` ä¹‹é—´çš„ä¸€ä¸ªã€‚

+   `return_dict` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦è¿”å› StableDiffusionPipelineOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `cross_attention_kwargs` (`dict`, *å¯é€‰*) â€” å¦‚æœæŒ‡å®šï¼Œå°†ä¼ é€’ç»™ [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) ä¸­å®šä¹‰çš„ `AttentionProcessor` çš„ kwargs å­—å…¸ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦è·³è¿‡çš„ CLIP å±‚çš„æ•°é‡ã€‚å€¼ä¸º 1 è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end` (`Callable`, *å¯é€‰*) â€” åœ¨æ¨æ–­æœŸé—´æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs` å°†åŒ…æ‹¬ç”± `callback_on_step_end_tensor_inputs` æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs`ï¼ˆ`List`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äº `callback_on_step_end` å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º `callback_kwargs` å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„ `._callback_tensor_inputs` å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

StableDiffusionPipelineOutput æˆ– `tuple`

å¦‚æœ `return_dict` ä¸º `True`ï¼Œåˆ™è¿”å› StableDiffusionPipelineOutputï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª `tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯åŒ…å«ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> import requests
>>> from PIL import Image

>>> from diffusers import StableDiffusionDepth2ImgPipeline

>>> pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-2-depth",
...     torch_dtype=torch.float16,
... )
>>> pipe.to("cuda")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> init_image = Image.open(requests.get(url, stream=True).raw)
>>> prompt = "two tigers"
>>> n_propmt = "bad, deformed, ugly, bad anotomy"
>>> image = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size`ï¼ˆ`str` æˆ– `int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `"auto"`ï¼‰â€” å½“ä¸º `"auto"` æ—¶ï¼Œå°†è¾“å…¥å‡åŠåˆ°æ³¨æ„åŠ›å¤´éƒ¨ï¼Œå› æ­¤æ³¨æ„åŠ›å°†åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­è®¡ç®—ã€‚å¦‚æœä¸º `"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥ä¿å­˜æœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›äº†ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨ `attention_head_dim // slice_size` ä¸ªåˆ‡ç‰‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim` å¿…é¡»æ˜¯ `slice_size` çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªæ­¥éª¤è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºåœ¨èŠ‚çœä¸€äº›å†…å­˜ä»¥æ¢å–ä¸€ç‚¹é€Ÿåº¦é™ä½å¾ˆæœ‰ç”¨ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨ PyTorch 2.0 æˆ– xFormers ä¸­çš„ `scaled_dot_product_attention`ï¼ˆSDPAï¼‰ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸å†…å­˜é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨ SDPA æˆ– xFormers ä¸­å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº† `enable_attention_slicing`ï¼Œåˆ™æ³¨æ„åŠ›å°†åœ¨ä¸€æ­¥ä¸­è®¡ç®—ã€‚

#### `enable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œ xFormers çš„ [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention) å‡½æ•°çš„ `op` å‚æ•°çš„é»˜è®¤ `None` æ“ä½œç¬¦çš„è¦†ç›–ã€‚

å¯ç”¨æ¥è‡ª [xFormers](https://facebookresearch.github.io/xformers/) çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°æ›´ä½çš„ GPU å†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šåŠ é€Ÿã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ é€Ÿä¸è¢«ä¿è¯ã€‚

âš ï¸ å½“åŒæ—¶å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ç¦ç”¨æ¥è‡ª [xFormers](https://facebookresearch.github.io/xformers/) çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

#### `load_textual_inversion`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str` æˆ– `os.PathLike` æˆ– `List[str or os.PathLike]` æˆ– `Dict` æˆ– `List[Dict]`ï¼‰â€” å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å…¶ä¸­çš„åˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨ Hub ä¸Šçš„ *æ¨¡å‹ id*ï¼ˆä¾‹å¦‚ `sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ã€‚

    +   ä¸€ä¸ªæŒ‡å‘åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*ç›®å½•*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversion_directory/`ï¼‰ã€‚

    +   ä¸€ä¸ªæŒ‡å‘åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*æ–‡ä»¶*çš„è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversions.pt`ï¼‰ã€‚

    +   ä¸€ä¸ª[torch çŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ`pretrained_model_name_or_path`æ˜¯åˆ—è¡¨ï¼Œåˆ™`token`ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder`ï¼ˆ[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)ï¼Œ*å¯é€‰*ï¼‰â€”å†»ç»“æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨ self.tokenizerã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºæ ‡è®°æ–‡æœ¬çš„`CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨ self.tokenizerã€‚

+   `weight_name`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€”è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚åº”åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶æ˜¯ğŸ¤— Diffusers æ ¼å¼ï¼Œä½†æ˜¯ä¿å­˜åœ¨ç‰¹å®šæƒé‡åç§°ä¸‹ï¼Œä¾‹å¦‚`text_inv.bin`ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶æ˜¯ Automatic1111 æ ¼å¼ã€‚

+   `cache_dir`ï¼ˆ`Union[str, os.PathLike]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœæœªä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚

+   `force_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶å°†è¢«åˆ é™¤ã€‚

+   `proxies`ï¼ˆ`Dict[str, str]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼ŒæŒ‰åè®®æˆ–ç«¯ç‚¹ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸Šä½¿ç”¨ã€‚

+   `local_files_only`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œæ¨¡å‹å°†ä¸ä¼šä» Hub ä¸‹è½½ã€‚

+   `token`ï¼ˆ`str`æˆ–*bool*ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ï¼‰ã€‚

+   `revision`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"main"`ï¼‰â€”è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤ ID æˆ– Git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`""`ï¼‰â€”æ¨¡å‹æ–‡ä»¶åœ¨ Hub æˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶è§£å†³å¯è®¿é—®æ€§é—®é¢˜çš„é•œåƒæºã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åè½¬åµŒå…¥åŠ è½½åˆ° StableDiffusionPipeline çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒğŸ¤— Diffusers å’Œ Automatic1111 æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦åŠ è½½ğŸ¤— Diffusers æ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦åŠ è½½ Automatic1111 æ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä»[civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `load_lora_weights`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path_or_dict`ï¼ˆ`str`æˆ–`os.PathLike`æˆ–`dict`ï¼‰â€”å‚è§ lora_state_dict()ã€‚

+   `kwargs` (`dict`, *optional*) â€” æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ lora_state_dict()ã€‚

+   `adapter_name` (`str`, *optional*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­ i æ˜¯è¦åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚

å°†åœ¨`pretrained_model_name_or_path_or_dict`ä¸­æŒ‡å®šçš„ LoRA æƒé‡åŠ è½½åˆ°`self.unet`å’Œ`self.text_encoder`ä¸­ã€‚

æ‰€æœ‰ kwargs éƒ½å°†è½¬å‘åˆ°`self.lora_state_dict`ã€‚

æœ‰å…³å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ lora_state_dict()ã€‚

æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.unet`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ load_lora_into_unet()ã€‚

æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.text_encoder`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ load_lora_into_text_encoder()ã€‚

#### `save_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

å‚æ•°

+   `save_directory` (`str` or `os.PathLike`) â€” ä¿å­˜ LoRA å‚æ•°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•ã€‚

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`) â€” ä¸`unet`å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`) â€” ä¸`text_encoder`å¯¹åº”çš„ LoRA å±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨ LoRA çŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ªğŸ¤— Transformersã€‚

+   `is_main_process` (`bool`, *optional*, defaults to `True`) â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å¾ˆæœ‰ç”¨ï¼Œå½“æ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°æ—¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½®`is_main_process=True`ä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚

+   `save_function` (`Callable`) â€” ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´ï¼Œå½“æ‚¨éœ€è¦ç”¨å¦ä¸€ç§æ–¹æ³•æ›¿æ¢`torch.save`æ—¶å¾ˆæœ‰ç”¨ã€‚å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`DIFFUSERS_SAVE_MODE`è¿›è¡Œé…ç½®ã€‚

+   `safe_serialization` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦ä½¿ç”¨`safetensors`æˆ–ä¼ ç»Ÿçš„ PyTorch æ–¹å¼ä½¿ç”¨`pickle`ä¿å­˜æ¨¡å‹ã€‚

ä¿å­˜å¯¹åº”äº UNet å’Œæ–‡æœ¬ç¼–ç å™¨çš„ LoRA å‚æ•°ã€‚

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L185)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt` (`str` or `List[str]`, *optional*) â€” è¦ç¼–ç çš„æç¤º

+   `num_images_per_prompt` (`int`) â€” æ¯ä¸ªæç¤ºåº”ç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance` (`bool`) â€” æ˜¯å¦ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ã€‚

+   `negative_prompt` (`str` or `List[str]`, *optional*) â€” ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³ï¼Œå¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ`negative_prompt_embeds`ã€‚

+   `lora_scale` (`float`, *optional*) â€” å°†åº”ç”¨äºåŠ è½½ LoRA å±‚çš„æ‰€æœ‰æ–‡æœ¬ç¼–ç å™¨ LoRA å±‚çš„ LoRA æ¯”ä¾‹ã€‚

+   `clip_skip` (`int`, *å¯é€‰*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä» CLIP è·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º 1 æ„å‘³ç€å°†ä½¿ç”¨å‰ä¸€å±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

å‚æ•°

+   `images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º`batch_size`çš„å»å™ª PIL å›¾åƒåˆ—è¡¨ï¼Œæˆ–å½¢çŠ¶ä¸º`(batch_size, height, width, num_channels)`çš„ NumPy æ•°ç»„ã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨ï¼ŒæŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰ï¼Œå¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º`None`ã€‚

ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚
