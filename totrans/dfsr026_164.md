# 深度到图像

> 原始文本：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img)

稳定扩散模型还可以基于图像推断深度，使用[MiDaS](https://github.com/isl-org/MiDaS)。这允许您传递文本提示和初始图像来调节生成新图像的过程，以及一个`depth_map`来保留图像结构。

确保查看稳定扩散 Tips 部分，了解如何探索调度器速度和质量之间的权衡，以及如何高效地重用管道组件！

如果您有兴趣使用官方检查点来执行任务，请探索[CompVis](https://huggingface.co/CompVis)、[Runway](https://huggingface.co/runwayml)和[Stability AI](https://huggingface.co/stabilityai) Hub 组织！

## StableDiffusionDepth2ImgPipeline

`diffusers.StableDiffusionDepth2ImgPipeline`类

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L77)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers depth_estimator: DPTForDepthEstimation feature_extractor: DPTFeatureExtractor )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的`CLIPTokenizer`。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜在特征的`UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 与`unet`结合使用以去噪编码图像潜在特征的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 之一。

用于使用稳定扩散进行文本引导的基于深度的图像到图像生成的管道。

该模型继承自 DiffusionPipeline。查看超类文档以了解为所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

该管道还继承了以下加载方法：

+   load_textual_inversion() 用于加载文本反演嵌入

+   load_lora_weights() 用于加载 LoRA 权重

+   save_lora_weights() 用于保存 LoRA 权重

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L599)

```py
( prompt: Union = None image: Union = None depth_map: Optional = None strength: float = 0.8 num_inference_steps: Optional = 50 guidance_scale: Optional = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: Optional = 0.0 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: Optional = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 用作起始点的图像批次的`Image`或张量。只有在`depth_map`不为`None`时才能接受图像潜在特征作为`image`。

+   `depth_map` (`torch.FloatTensor`, *可选*) — 深度预测，用作图像生成过程的额外条件。如果未定义，则会使用 `self.depth_estimator` 自动预测深度。

+   `strength` (`float`, *可选*, 默认为 0.8) — 指示要转换参考 `image` 的程度。必须在 0 和 1 之间。`image` 用作起点，`strength` 越高，添加的噪音就越多。去噪步骤的数量取决于最初添加的噪音量。当 `strength` 为 1 时，添加的噪音最大，去噪过程将运行指定的 `num_inference_steps` 次迭代。值为 1 实质上忽略了 `image`。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推断速度。该参数受 `strength` 调节。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的图像，但会降低图像质量。当 `guidance_scale > 1` 时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成中不包括的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）会被忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于 [DDIM](https://arxiv.org/abs/2010.02502) 论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中会被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `prompt` 输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择 `PIL.Image` 或 `np.array` 之间的一个。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通元组。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给 [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中定义的 `AttentionProcessor` 的 kwargs 字典。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要跳过的 CLIP 层的数量。值为 1 表示将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *可选*) — 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs` 将包括由 `callback_on_step_end_tensor_inputs` 指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）— 用于 `callback_on_step_end` 函数的张量输入列表。列表中指定的张量将作为 `callback_kwargs` 参数传递。您只能包含在管道类的 `._callback_tensor_inputs` 属性中列出的变量。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果 `return_dict` 为 `True`，则返回 StableDiffusionPipelineOutput，否则返回一个 `tuple`，其中第一个元素是包含生成图像的列表。

用于生成的管道的调用函数。

示例：

```py
>>> import torch
>>> import requests
>>> from PIL import Image

>>> from diffusers import StableDiffusionDepth2ImgPipeline

>>> pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(
...     "stabilityai/stable-diffusion-2-depth",
...     torch_dtype=torch.float16,
... )
>>> pipe.to("cuda")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> init_image = Image.open(requests.get(url, stream=True).raw)
>>> prompt = "two tigers"
>>> n_propmt = "bad, deformed, ugly, bad anotomy"
>>> image = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size`（`str` 或 `int`，*可选*，默认为 `"auto"`）— 当为 `"auto"` 时，将输入减半到注意力头部，因此注意力将在两个步骤中计算。如果为 `"max"`，将通过一次只运行一个切片来保存最大内存量。如果提供了一个数字，则使用 `attention_head_dim // slice_size` 个切片。在这种情况下，`attention_head_dim` 必须是 `slice_size` 的倍数。

启用切片注意力计算。当启用此选项时，注意力模块将输入张量分割成多个步骤计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于在节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用 PyTorch 2.0 或 xFormers 中的 `scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常内存高效，因此您不需要启用此功能。如果您在 SDPA 或 xFormers 中启用了注意力切片，可能会导致严重的减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了 `enable_attention_slicing`，则注意力将在一步中计算。

#### `enable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）— 用作 xFormers 的 [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention) 函数的 `op` 参数的默认 `None` 操作符的覆盖。

启用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。当启用此选项时，您应该观察到更低的 GPU 内存使用量，并且在推理过程中可能会加速。训练过程中的加速不被保证。

⚠️ 当同时启用内存高效注意力和切片注意力时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用来自 [xFormers](https://facebookresearch.github.io/xformers/) 的内存高效注意力。

#### `load_textual_inversion`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str` 或 `os.PathLike` 或 `List[str or os.PathLike]` 或 `Dict` 或 `List[Dict]`）— 可以是以下之一或其中的列表：

    +   一个字符串，预训练模型在 Hub 上的 *模型 id*（例如 `sd-concepts-library/low-poly-hd-logos-icons`）。

    +   一个指向包含文本反转权重的*目录*的路径（例如`./my_text_inversion_directory/`）。

    +   一个指向包含文本反转权重的*文件*的路径（例如`./my_text_inversions.pt`）。

    +   一个[torch 状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token`（`str`或`List[str]`，*可选*）—覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是列表，则`token`也必须是相同长度的列表。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)，*可选*）—冻结文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用 self.tokenizer。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，*可选*）—用于标记文本的`CLIPTokenizer`。如果未指定，函数将使用 self.tokenizer。

+   `weight_name`（`str`，*可选*）—自定义权重文件的名称。应在以下情况下使用：

    +   保存的文本反转文件是🤗 Diffusers 格式，但是保存在特定权重名称下，例如`text_inv.bin`。

    +   保存的文本反转文件是 Automatic1111 格式。

+   `cache_dir`（`Union[str, os.PathLike]`，*可选*）—下载预训练模型配置的目录路径，如果未使用标准缓存。

+   `force_download`（`bool`，*可选*，默认为`False`）—是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download`（`bool`，*可选*，默认为`False`）—是否恢复下载模型权重和配置文件。如果设置为`False`，任何未完全下载的文件将被删除。

+   `proxies`（`Dict[str, str]`，*可选*）—要使用的代理服务器字典，按协议或端点，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理在每个请求上使用。

+   `local_files_only`（`bool`，*可选*，默认为`False`）—是否仅加载本地模型权重和配置文件。如果设置为`True`，模型将不会从 Hub 下载。

+   `token`（`str`或*bool*，*可选*）—用作远程文件的 HTTP bearer 授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision`（`str`，*可选*，默认为`"main"`）—要使用的特定模型版本。可以是分支名称、标签名称、提交 ID 或 Git 允许的任何标识符。

+   `subfolder`（`str`，*可选*，默认为`""`）—模型文件在 Hub 或本地较大模型存储库中的子文件夹位置。

+   `mirror`（`str`，*可选*）—在中国下载模型时解决可访问性问题的镜像源。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到 StableDiffusionPipeline 的文本编码器中（支持🤗 Diffusers 和 Automatic1111 格式）。

示例：

要加载🤗 Diffusers 格式的文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载 Automatic1111 格式的文本反转嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)）然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `load_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict`（`str`或`os.PathLike`或`dict`）—参见 lora_state_dict()。

+   `kwargs` (`dict`, *optional*) — 有关如何将状态字典加载的更多详细信息，请参见 lora_state_dict()。

+   `adapter_name` (`str`, *optional*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中 i 是要加载的适配器总数。

将在`pretrained_model_name_or_path_or_dict`中指定的 LoRA 权重加载到`self.unet`和`self.text_encoder`中。

所有 kwargs 都将转发到`self.lora_state_dict`。

有关如何加载状态字典的更多详细信息，请参见 lora_state_dict()。

有关如何将状态字典加载到`self.unet`中的更多详细信息，请参见 load_lora_into_unet()。

有关如何将状态字典加载到`self.text_encoder`中的更多详细信息，请参见 load_lora_into_text_encoder()。

#### `save_lora_weights`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory` (`str` or `os.PathLike`) — 保存 LoRA 参数的目录。如果不存在，将创建该目录。

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`) — 与`unet`对应的 LoRA 层的状态字典。

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`) — 与`text_encoder`对应的 LoRA 层的状态字典。必须显式传递文本编码器 LoRA 状态字典，因为它来自🤗 Transformers。

+   `is_main_process` (`bool`, *optional*, defaults to `True`) — 调用此函数的进程是否为主进程。在分布式训练中很有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`以避免竞争条件。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间，当您需要用另一种方法替换`torch.save`时很有用。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。

+   `safe_serialization` (`bool`, *optional*, defaults to `True`) — 是否使用`safetensors`或传统的 PyTorch 方式使用`pickle`保存模型。

保存对应于 UNet 和文本编码器的 LoRA 参数。

#### `encode_prompt`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py#L185)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示

+   `num_images_per_prompt` (`int`) — 每个提示应生成的图像数量

+   `do_classifier_free_guidance` (`bool`) — 是否使用分类器自由指导。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成`negative_prompt_embeds`。

+   `lora_scale` (`float`, *optional*) — 将应用于加载 LoRA 层的所有文本编码器 LoRA 层的 LoRA 比例。

+   `clip_skip` (`int`, *可选*) — 在计算提示嵌入时要从 CLIP 跳过的层数。值为 1 意味着将使用前一层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为`batch_size`的去噪 PIL 图像列表，或形状为`(batch_size, height, width, num_channels)`的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表，指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为`None`。

稳定扩散管道的输出类。
