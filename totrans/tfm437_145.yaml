- en: BORT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BORT
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bort)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, we do not accept any new PRs changing
    its code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型目前处于维护模式，我们不接受任何修改其代码的新PR。
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.30.0. You can do so by running the following command:
    `pip install -U transformers==4.30.0`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在运行此模型时遇到任何问题，请重新安装支持此模型的最后一个版本：v4.30.0。您可以通过运行以下命令来执行：`pip install -U transformers==4.30.0`。
- en: Overview
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://arxiv.org/abs/2010.10499)
    by Adrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural
    parameters for the BERT, which the authors refer to as “Bort”.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: BORT模型是由Adrian de Wynter和Daniel J. Perry在[为BERT提取最佳子架构](https://arxiv.org/abs/2010.10499)中提出的。这是BERT的一组最佳架构参数，作者称之为“Bort”。
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We extract an optimal subset of architectural parameters for the BERT architecture
    from Devlin et al. (2018) by applying recent breakthroughs in algorithms for neural
    architecture search. This optimal subset, which we refer to as “Bort”, is demonstrably
    smaller, having an effective (that is, not counting the embedding layer) size
    of 5.5% the original BERT-large architecture, and 16% of the net size. Bort is
    also able to be pretrained in 288 GPU hours, which is 1.2% of the time required
    to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large
    (Liu et al., 2019), and about 33% of that of the world-record, in GPU hours, required
    to train BERT-large on the same hardware. It is also 7.9x faster on a CPU, as
    well as being better performing than other compressed variants of the architecture,
    and some of the non-compressed variants: it obtains performance improvements of
    between 0.3% and 31%, absolute, with respect to BERT-large, on multiple public
    natural language understanding (NLU) benchmarks.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们从Devlin等人（2018）的BERT架构中提取了一组最佳的架构参数，通过应用神经架构搜索算法的最新突破。这个最佳子集，我们称之为“Bort”，明显更小，有效（即不计算嵌入层）大小为原始BERT-large架构的5.5%，以及净尺寸的16%。Bort还能在288个GPU小时内进行预训练，这相当于预训练最高性能的BERT参数化架构变体RoBERTa-large（Liu等人，2019）所需时间的1.2%，以及在相同硬件上训练BERT-large所需的GPU小时的世界纪录的约33%。它在CPU上也快了7.9倍，比架构的其他压缩变体以及一些非压缩变体表现更好：在多个公共自然语言理解（NLU）基准测试中，相对于BERT-large，它获得了0.3%至31%的性能改进，绝对值。*'
- en: This model was contributed by [stefan-it](https://huggingface.co/stefan-it).
    The original code can be found [here](https://github.com/alexa/bort/).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[stefan-it](https://huggingface.co/stefan-it)贡献。原始代码可以在[这里](https://github.com/alexa/bort/)找到。
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: BORT’s model architecture is based on BERT, refer to [BERT’s documentation page](bert)
    for the model’s API reference as well as usage examples.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BORT的模型架构基于BERT，参考[BERT的文档页面](bert)获取模型的API参考以及使用示例。
- en: BORT uses the RoBERTa tokenizer instead of the BERT tokenizer, refer to [RoBERTa’s
    documentation page](roberta) for the tokenizer’s API reference as well as usage
    examples.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BORT使用RoBERTa分词器而不是BERT分词器，参考[RoBERTa的文档页面](roberta)获取分词器的API参考以及使用示例。
- en: BORT requires a specific fine-tuning algorithm, called [Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology)
    , that is sadly not open-sourced yet. It would be very useful for the community,
    if someone tries to implement the algorithm to make BORT fine-tuning work.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BORT需要一种特定的微调算法，称为[Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology)，遗憾的是目前尚未开源。如果有人尝试实现该算法以使BORT微调工作，对社区将非常有用。
