["```py\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\")\nmodel.to_bettertransformer()\n```", "```py\npython run_qa.py \\\n--model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n--dataset_name squad \\\n--do_eval \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/ \\\n--no_cuda \\\n--jit_mode_eval\n```", "```py\npip install intel_extension_for_pytorch\n```", "```py\npython run_qa.py \\\n--model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n--dataset_name squad \\\n--do_eval \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/ \\\n--no_cuda \\\n--use_ipex \\\n--jit_mode_eval\n```", "```py\nfrom transformers import AutoTokenizer, pipeline\nfrom optimum.onnxruntime import ORTModelForQuestionAnswering\n\nmodel = ORTModelForQuestionAnswering.from_pretrained(\"optimum/roberta-base-squad2\")\ntokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n\nonnx_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n\nquestion = \"What's my name?\"\ncontext = \"My name is Philipp and I live in Nuremberg.\"\npred = onnx_qa(question, context)\n```"]