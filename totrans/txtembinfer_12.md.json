["```py\ntext-embeddings-router --help\n```", "```py\nUsage: text-embeddings-router [OPTIONS]\n\nOptions:\n      --model-id <MODEL_ID>\n          The name of the model to load. Can be a MODEL_ID as listed on <https://hf.co/models> like `thenlper/gte-base`. \n          Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of \n          transformers\n\n          [env: MODEL_ID=]\n          [default: thenlper/gte-base]\n\n      --revision <REVISION>\n          The actual revision of the model if you're referring to a model on the hub. You can use a specific commit id \n          or a branch like `refs/pr/2`\n\n          [env: REVISION=]\n\n      --tokenization-workers <TOKENIZATION_WORKERS>\n          Optionally control the number of tokenizer workers used for payload tokenization, validation and truncation. \n          Default to the number of CPU cores on the machine\n\n          [env: TOKENIZATION_WORKERS=]\n\n      --dtype <DTYPE>\n          The dtype to be forced upon the model\n\n          [env: DTYPE=]\n          [possible values: float16, float32]\n\n      --pooling <POOLING>\n          Optionally control the pooling method for embedding models.\n\n          If `pooling` is not set, the pooling configuration will be parsed from the model `1_Pooling/config.json` \n          configuration.\n\n          If `pooling` is set, it will override the model pooling configuration\n\n          [env: POOLING=]\n          [possible values: cls, mean]\n\n      --max-concurrent-requests <MAX_CONCURRENT_REQUESTS>\n          The maximum amount of concurrent requests for this particular deployment. \n          Having a low limit will refuse clients requests instead of having them wait for too long and is usually good \n          to handle backpressure correctly\n\n          [env: MAX_CONCURRENT_REQUESTS=]\n          [default: 512]\n\n      --max-batch-tokens <MAX_BATCH_TOKENS>\n          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.\n\n          This represents the total amount of potential tokens within a batch.\n\n          For `max_batch_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.\n\n          Overall this number should be the largest possible until the model is compute bound. Since the actual memory \n          overhead depends on the model implementation, text-embeddings-inference cannot infer this number automatically.\n\n          [env: MAX_BATCH_TOKENS=]\n          [default: 16384]\n\n      --max-batch-requests <MAX_BATCH_REQUESTS>\n          Optionally control the maximum number of individual requests in a batch\n\n          [env: MAX_BATCH_REQUESTS=]\n\n      --max-client-batch-size <MAX_CLIENT_BATCH_SIZE>\n          Control the maximum number of inputs that a client can send in a single request\n\n          [env: MAX_CLIENT_BATCH_SIZE=]\n          [default: 32]\n\n      --hf-api-token <HF_API_TOKEN>\n          Your HuggingFace hub token\n\n          [env: HF_API_TOKEN=]\n\n      --hostname <HOSTNAME>\n          The IP address to listen on\n\n          [env: HOSTNAME=]\n          [default: 0.0.0.0]\n\n  -p, --port <PORT>\n          The port to listen on\n\n          [env: PORT=]\n          [default: 3000]\n\n      --uds-path <UDS_PATH>\n          The name of the unix socket some text-embeddings-inference backends will use as they communicate internally \n          with gRPC\n\n          [env: UDS_PATH=]\n          [default: /tmp/text-embeddings-inference-server]\n\n      --huggingface-hub-cache <HUGGINGFACE_HUB_CACHE>\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk \n          for instance\n\n          [env: HUGGINGFACE_HUB_CACHE=/data]\n\n      --json-output\n          Outputs the logs in JSON format (useful for telemetry)\n\n          [env: JSON_OUTPUT=]\n\n      --otlp-endpoint <OTLP_ENDPOINT>\n          [env: OTLP_ENDPOINT=]\n\n      --cors-allow-origin <CORS_ALLOW_ORIGIN>\n          [env: CORS_ALLOW_ORIGIN=]\n```"]