["```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( enabled: bool = True cache_enabled: bool = None )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import AutocastKwargs\n\nkwargs = AutocastKwargs(cache_enabled=True)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( dim: int = 0 broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False check_reduction: bool = False gradient_as_bucket_view: bool = False static_graph: bool = False )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import DistributedDataParallelKwargs\n\nkwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( backend: Literal = 'MSAMP' opt_level: Literal = 'O2' margin: int = 0 interval: int = 1 fp8_format: Literal = 'E4M3' amax_history_len: int = 1 amax_compute_algo: Literal = 'most_recent' override_linear_precision: Tuple = (False, False, False) )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import FP8RecipeKwargs\n\nkwargs = FP8RecipeKwargs(backend=\"te\", fp8_format=\"HYBRID\")\naccelerator = Accelerator(mixed_precision=\"fp8\", kwargs_handlers=[kwargs])\n```", "```py\nkwargs = FP8RecipeKwargs(backend=\"msamp\", optimization_level=\"02\")\n```", "```py\n( init_scale: float = 65536.0 growth_factor: float = 2.0 backoff_factor: float = 0.5 growth_interval: int = 2000 enabled: bool = True )\n```", "```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import GradScalerKwargs\n\nkwargs = GradScalerKwargs(backoff_filter=0.25)\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( backend: Optional = 'nccl' init_method: Optional = None timeout: timedelta = datetime.timedelta(seconds=1800) )\n```", "```py\nfrom datetime import timedelta\nfrom accelerate import Accelerator\nfrom accelerate.utils import InitProcessGroupKwargs\n\nkwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=800))\naccelerator = Accelerator(kwargs_handlers=[kwargs])\n```", "```py\n( hf_ds_config: Any = None gradient_accumulation_steps: int = None gradient_clipping: float = None zero_stage: int = None is_train_batch_min: str = True offload_optimizer_device: bool = None offload_param_device: bool = None offload_optimizer_nvme_path: str = None offload_param_nvme_path: str = None zero3_init_flag: bool = None zero3_save_16bit_model: bool = None )\n```", "```py\n( prefix = '' mismatches = None config = None must_match = True **kwargs )\n```", "```py\n( sharding_strategy: typing.Any = None backward_prefetch: typing.Any = None mixed_precision_policy: typing.Any = None auto_wrap_policy: Optional = None cpu_offload: typing.Any = None ignored_modules: Optional = None state_dict_type: typing.Any = None state_dict_config: typing.Any = None optim_state_dict_config: typing.Any = None limit_all_gathers: bool = True use_orig_params: bool = True param_init_fn: Optional = None sync_module_states: bool = True forward_prefetch: bool = False activation_checkpointing: bool = False )\n```", "```py\n( module name )\n```", "```py\n( num_steps: int = None adjust_scheduler: bool = True sync_with_dataloader: bool = True )\n```", "```py\n( tp_degree: int = None pp_degree: int = None num_micro_batches: int = None gradient_clipping: float = None sequence_parallelism: bool = None recompute_activations: bool = None use_distributed_optimizer: bool = None pipeline_model_parallel_split_rank: int = None num_layers_per_virtual_pipeline_stage: int = None is_train_batch_min: str = True train_iters: int = None train_samples: int = None weight_decay_incr_style: str = 'constant' start_weight_decay: float = None end_weight_decay: float = None lr_decay_style: str = 'linear' lr_decay_iters: int = None lr_decay_samples: int = None lr_warmup_iters: int = None lr_warmup_samples: int = None lr_warmup_fraction: float = None min_lr: float = 0 consumed_samples: List = None no_wd_decay_cond: Optional = None scale_lr_cond: Optional = None lr_mult: float = 1.0 megatron_dataset_flag: bool = False seq_length: int = None encoder_seq_length: int = None decoder_seq_length: int = None tensorboard_dir: str = None set_all_logging_options: bool = False eval_iters: int = 100 eval_interval: int = 1000 return_logits: bool = False custom_train_step_class: Optional = None custom_train_step_kwargs: Optional = None custom_model_provider_function: Optional = None custom_prepare_model_function: Optional = None other_megatron_args: Optional = None )\n```", "```py\n( backend: DynamoBackend = None mode: str = None fullgraph: bool = None dynamic: bool = None options: Any = None disable: bool = False )\n```", "```py\n( load_in_8bit: bool = False llm_int8_threshold: float = 6.0 load_in_4bit: bool = False bnb_4bit_quant_type: str = 'fp4' bnb_4bit_use_double_quant: bool = False bnb_4bit_compute_dtype: bool = 'fp16' torch_dtype: dtype = None skip_modules: List = None keep_in_fp32_modules: List = None )\n```", "```py\n( project_dir: str = None logging_dir: str = None automatic_checkpoint_naming: bool = False total_limit: int = None iteration: int = 0 save_on_each_node: bool = False )\n```", "```py\n( project_dir: str = None )\n```", "```py\n( tensor from_process: int = 0 )\n```", "```py\n( object_list from_process: int = 0 )\n```", "```py\n( data dim = 0 )\n```", "```py\n( model_forward )\n```", "```py\n( tensor )\n```", "```py\n( tensor )\n```", "```py\n( object: Any )\n```", "```py\n( data )\n```", "```py\n( tensor dim = 0 pad_index = 0 pad_first = False )\n```", "```py\n( func data *args test_type = <function is_torch_tensor at 0x7f2b4f4c7d00> error_on_other_type = False **kwargs )\n```", "```py\n( tensor reduction = 'mean' scale = 1.0 )\n```", "```py\n( tensor device non_blocking = False skip_keys = None )\n```", "```py\n( data tensor_slice process_index = None num_processes = None )\n```", "```py\n( ignore_tpu = False )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( check_device = False )\n```", "```py\n( operation: str version: str )\n```", "```py\n( check_device = True )\n```", "```py\n( check_device = False )\n```", "```py\n( **kwargs )\n```", "```py\n>>> import os\n>>> from accelerate.utils import patch_environment\n\n>>> with patch_environment(FOO=\"bar\"):\n...     print(os.environ[\"FOO\"])  # prints \"bar\"\n>>> print(os.environ[\"FOO\"])  # raises KeyError\n```", "```py\n( )\n```", "```py\n>>> import os\n>>> from accelerate.utils import clear_environment\n\n>>> os.environ[\"FOO\"] = \"bar\"\n>>> with clear_environment():\n...     print(os.environ)\n...     os.environ[\"FOO\"] = \"new_bar\"\n...     print(os.environ[\"FOO\"])\n{}\nnew_bar\n\n>>> print(os.environ[\"FOO\"])\nbar\n```", "```py\n( mixed_precision = 'no' save_location: str = '/github/home/.cache/huggingface/accelerate/default_config.yaml' use_xpu: bool = False )\n```", "```py\n( function: callable = None starting_batch_size: int = 128 )\n```", "```py\n>>> from accelerate.utils import find_executable_batch_size\n\n>>> @find_executable_batch_size(starting_batch_size=128)\n... def train(batch_size, model, optimizer):\n...     ...\n\n>>> train(model, optimizer)\n```", "```py\n( model: Module )\n```", "```py\n( model: Module dtype: Union = None special_dtypes: Optional = None )\n```", "```py\n( model keep_fp32_wrapper: bool = True ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None low_zero: bool = False )\n```", "```py\n( modules: List module_sizes: Dict no_split_module_classes: List ) \u2192 export const metadata = 'undefined';Tuple[int, List[str]]\n```", "```py\n( model: Module max_memory: Optional = None no_split_module_classes: Optional = None dtype: Union = None special_dtypes: Optional = None verbose: bool = False clean_result: bool = True )\n```", "```py\n( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )\n```", "```py\n( model index offload_folder )\n```", "```py\n( checkpoint_file device_map = None )\n```", "```py\n( save_dir: Union state_dict: Dict )\n```", "```py\n( model tied_params )\n```", "```py\n( module: Module tensor_name: str device: Union value: Optional = None dtype: Union = None fp16_statistics: Optional = None tied_params_map: Optional = None )\n```", "```py\n( state_dict: Dict max_shard_size: Union = '10GB' weights_name: str = 'pytorch_model.bin' )\n```", "```py\n( model keep_fp32_wrapper: bool = True ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```", "```py\n( obj f save_on_each_node: bool = False safe_serialization: bool = False )\n```", "```py\n( )\n```", "```py\n( seed: int device_specific: bool = False )\n```", "```py\n( rng_type: Optional = None generator: Optional = None )\n```", "```py\n( rng_types: List generator: Optional = None )\n```", "```py\n( upgrade: bool = False )\n```", "```py\n>>> from accelerate.utils import install_xla\n\n>>> install_xla(upgrade=True)\n```", "```py\n( model: Module checkpoint: Union device_map: Optional = None offload_folder: Union = None dtype: Union = None offload_state_dict: bool = False offload_buffers: bool = False keep_in_fp32_modules: List = None offload_8bit_bnb: bool = False )\n```", "```py\n( model: Module bnb_quantization_config: BnbQuantizationConfig weights_location: Union = None device_map: Optional = None no_split_module_classes: Optional = None max_memory: Optional = None offload_folder: Union = None offload_state_dict: bool = False ) \u2192 export const metadata = 'undefined';torch.nn.Module\n```"]