- en: Considerations for model evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/considerations](https://huggingface.co/docs/evaluate/considerations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Developing an ML model is rarely a one-shot deal: it often involves multiple
    stages of defining the model architecture and tuning hyper-parameters before converging
    on a final set. Responsible model evaluation is a key part of this process, and
    ğŸ¤— Evaluate is here to help!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some things to keep in mind when evaluating your model using the ğŸ¤—
    Evaluate library:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Properly splitting your data
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Good evaluation generally requires three splits of your dataset:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**train**: this is used for training your model.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validation**: this is used for validating the model hyperparameters.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test**: this is used for evaluating your model.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of the datasets on the ğŸ¤— Hub are separated into 2 splits: `train` and
    `validation`; others are split into 3 splits (`train`, `validation` and `test`)
    â€” make sure to use the right split for the right purpose!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets on the ğŸ¤— Hub are already separated into these three splits. However,
    there are also many that only have a train/validation or only train split.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset youâ€™re using doesnâ€™t have a predefined train-test split, it is
    up to you to define which part of the dataset you want to use for training your
    model and which you want to use for hyperparameter tuning or final evaluation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating on the same split can misrepresent your results! If
    you overfit on your training data the evaluation results on that split will look
    great but the model will perform poorly on new data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the size of the dataset, you can keep anywhere from 10-30% for
    evaluation and the rest for training, while aiming to set up the test set to reflect
    the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090)
    for a more in-depth discussion of dataset splitting!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The impact of class imbalance
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb)
    of movie reviews, are perfectly balanced, most real-world datasets are not. In
    machine learning a *balanced dataset* corresponds to a datasets where all labels
    are represented equally. In the case of the IMDb dataset this means that there
    are as many positive as negative reviews in the dataset. In an imbalanced dataset
    this is not the case: in fraud detection for example there are usually many more
    non-fraud cases than fraud cases in the dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Having an imbalanced dataset can skew the results of your metrics. Imagine a
    dataset with 99 â€œnon-fraudâ€ cases and 1 â€œfraudâ€ case. A simple model that always
    predicts â€œnon-fraudâ€ cases would give yield a 99% accuracy which might sound good
    at first until you realize that you will never catch a fraud case.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Often, using more than one metric can help get a better idea of your modelâ€™s
    performance from different points of view. For instance, metrics like **[recall](https://huggingface.co/metrics/recall)**
    and **[precision](https://huggingface.co/metrics/precision)** can be used together,
    and the **[f1 score](https://huggingface.co/metrics/f1)** is actually the harmonic
    mean of the two.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where a dataset is balanced, using [accuracy](https://huggingface.co/metrics/accuracy)
    can reflect the overall model performance:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Balanced Labels](../Images/3806f661f4ca701df162a69f4b08f9cb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: In cases where there is an imbalance, using [F1 score](https://huggingface.co/metrics/f1)
    can be a better representation of performance, given that it encompasses both
    precision and recall.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Imbalanced Labels](../Images/da6bb8aa7c358d094b409c9168313526.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Using accuracy in an imbalanced setting is less ideal, since it is not sensitive
    to minority classes and will not faithfully reflect model performance on them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Offline vs. online model evaluation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple ways to evaluate models, and an important distinction is
    offline versus online evaluation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ¨¡å‹æœ‰å¤šç§æ–¹å¼ï¼Œä¸€ä¸ªé‡è¦çš„åŒºåˆ«æ˜¯ç¦»çº¿è¯„ä¼°å’Œåœ¨çº¿è¯„ä¼°ï¼š
- en: '**Offline evaluation** is done before deploying a model or using insights generated
    from a model, using static datasets and metrics.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¦»çº¿è¯„ä¼°**æ˜¯åœ¨éƒ¨ç½²æ¨¡å‹æˆ–ä½¿ç”¨æ¨¡å‹ç”Ÿæˆçš„è§è§£ä¹‹å‰è¿›è¡Œçš„ï¼Œä½¿ç”¨é™æ€æ•°æ®é›†å’ŒæŒ‡æ ‡ã€‚'
- en: '**Online evaluation** means evaluating how a model is performing after deployment
    and during its use in production.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨çº¿è¯„ä¼°**æ„å‘³ç€åœ¨éƒ¨ç½²åä»¥åŠåœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ¨¡å‹æ—¶è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚'
- en: These two types of evaluation can use different metrics and measure different
    aspects of model performance. For example, offline evaluation can compare a model
    to other models based on their performance on common benchmarks, whereas online
    evaluation will evaluate aspects such as latency and accuracy of the model based
    on production data (for example, the number of user queries that it was able to
    address).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§è¯„ä¼°ç±»å‹å¯ä»¥ä½¿ç”¨ä¸åŒçš„æŒ‡æ ‡ï¼Œå¹¶è¡¡é‡æ¨¡å‹æ€§èƒ½çš„ä¸åŒæ–¹é¢ã€‚ä¾‹å¦‚ï¼Œç¦»çº¿è¯„ä¼°å¯ä»¥æ ¹æ®å¸¸è§åŸºå‡†æµ‹è¯•æ¯”è¾ƒæ¨¡å‹ä¸å…¶ä»–æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œåœ¨çº¿è¯„ä¼°å°†æ ¹æ®ç”Ÿäº§æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œèƒ½å¤Ÿå¤„ç†çš„ç”¨æˆ·æŸ¥è¯¢æ•°é‡ï¼‰è¯„ä¼°æ¨¡å‹çš„å»¶è¿Ÿå’Œå‡†ç¡®æ€§ç­‰æ–¹é¢ã€‚
- en: Trade-offs in model evaluation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¯„ä¼°ä¸­çš„æƒè¡¡
- en: 'When evaluating models in practice, there are often trade-offs that have to
    be made between different aspects of model performance: for instance, choosing
    a model that is slightly less accurate but that has a faster inference time, compared
    to a high-accuracy that has a higher memory footprint and requires access to more
    GPUs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®è·µä¸­è¯„ä¼°æ¨¡å‹æ—¶ï¼Œé€šå¸¸éœ€è¦åœ¨æ¨¡å‹æ€§èƒ½çš„ä¸åŒæ–¹é¢ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼šä¾‹å¦‚ï¼Œé€‰æ‹©ä¸€ä¸ªç•¥å¾®å‡†ç¡®æ€§è¾ƒä½ä½†æ¨ç†æ—¶é—´æ›´å¿«çš„æ¨¡å‹ï¼Œä¸ä¸€ä¸ªå‡†ç¡®æ€§è¾ƒé«˜ä½†å†…å­˜å ç”¨é‡æ›´å¤§ä¸”éœ€è¦æ›´å¤šGPUè®¿é—®çš„æ¨¡å‹ç›¸æ¯”ã€‚
- en: 'Here are other aspects of model performance to consider during evaluation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­è€ƒè™‘çš„æ¨¡å‹æ€§èƒ½çš„å…¶ä»–æ–¹é¢ï¼š
- en: Interpretability
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é‡Šæ€§
- en: When evaluating models, **interpretability** (i.e. the ability to *interpret*
    results) can be very important, especially when deploying models in production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯„ä¼°æ¨¡å‹æ—¶ï¼Œ**è§£é‡Šæ€§**ï¼ˆå³*è§£é‡Šç»“æœçš„èƒ½åŠ›*ï¼‰å¯èƒ½éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿäº§ä¸­éƒ¨ç½²æ¨¡å‹æ—¶ã€‚
- en: 'For instance, metrics such as [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match)
    have a set range (between 0 and 1, or 0% and 100%) and are easily understandable
    to users: for a pair of strings, the exact match score is 1 if the two strings
    are the exact same, and 0 otherwise.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåƒ[exact match](https://huggingface.co/spaces/evaluate-metric/exact_match)è¿™æ ·çš„æŒ‡æ ‡å…·æœ‰ä¸€å®šèŒƒå›´ï¼ˆåœ¨0å’Œ1ä¹‹é—´ï¼Œæˆ–è€…0%å’Œ100%ä¹‹é—´ï¼‰ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·æ¥è¯´å¾ˆå®¹æ˜“ç†è§£ï¼šå¯¹äºä¸€å¯¹å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸¤ä¸ªå­—ç¬¦ä¸²å®Œå…¨ç›¸åŒï¼Œåˆ™ç²¾ç¡®åŒ¹é…åˆ†æ•°ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
- en: 'Other metrics, such as [BLEU](https://huggingface.co/spaces/evaluate-metric/exact_match)
    are harder to interpret: while they also range between 0 and 1, they can vary
    greatly depending on which parameters are used to generate the scores, especially
    when different tokenization and normalization techniques are used (see the [metric
    card](https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md)
    for more information about BLEU limitations). This means that it is difficult
    to interpret a BLEU score without having more information about the procedure
    used for obtaining it.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–æŒ‡æ ‡ï¼Œå¦‚[BLEU](https://huggingface.co/spaces/evaluate-metric/exact_match)æ›´éš¾è§£é‡Šï¼šè™½ç„¶å®ƒä»¬ä¹Ÿåœ¨0å’Œ1ä¹‹é—´å˜åŒ–ï¼Œä½†æ ¹æ®ç”¨äºç”Ÿæˆåˆ†æ•°çš„å‚æ•°ä¸åŒï¼Œå®ƒä»¬å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨ä¸åŒçš„æ ‡è®°åŒ–å’Œè§„èŒƒåŒ–æŠ€æœ¯æ—¶ï¼ˆæœ‰å…³BLEUé™åˆ¶çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æŒ‡æ ‡å¡](https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md)ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨æ²¡æœ‰æ›´å¤šå…³äºè·å¾—BLEUåˆ†æ•°çš„è¿‡ç¨‹çš„ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¾ˆéš¾è§£é‡ŠBLEUåˆ†æ•°ã€‚
- en: Interpretability can be more or less important depending on the evaluation use
    case, but it is a useful aspect of model evaluation to keep in mind, since communicating
    and comparing model evaluations is an important part of responsible machine learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é‡Šæ€§å¯èƒ½åœ¨è¯„ä¼°ç”¨ä¾‹ä¸­æ›´æˆ–æ›´ä¸é‡è¦ï¼Œä½†å®ƒæ˜¯æ¨¡å‹è¯„ä¼°çš„ä¸€ä¸ªæœ‰ç”¨æ–¹é¢ï¼Œå› ä¸ºæ²Ÿé€šå’Œæ¯”è¾ƒæ¨¡å‹è¯„ä¼°æ˜¯è´Ÿè´£ä»»çš„æœºå™¨å­¦ä¹ çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚
- en: Inference speed and memory footprint
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ¨ç†é€Ÿåº¦å’Œå†…å­˜å ç”¨é‡
- en: While recent years have seen increasingly large ML models achieve high performance
    on a large variety of tasks and benchmarks, deploying these multi-billion parameter
    models in practice can be a challenge in itself, and many organizations lack the
    resources for this. This is why considering the **inference speed** and **memory
    footprint** of models is important, especially when doing online model evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤§çš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å„ç§ä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é«˜æ€§èƒ½ï¼Œä½†åœ¨å®è·µä¸­éƒ¨ç½²è¿™äº›æ•°åäº¿å‚æ•°çš„æ¨¡å‹æœ¬èº«å°±æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè®¸å¤šç»„ç»‡ç¼ºä¹è¿™æ–¹é¢çš„èµ„æºã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨è¿›è¡Œåœ¨çº¿æ¨¡å‹è¯„ä¼°æ—¶è€ƒè™‘æ¨¡å‹çš„**æ¨ç†é€Ÿåº¦**å’Œ**å†…å­˜å ç”¨é‡**æ˜¯é‡è¦çš„åŸå› ã€‚
- en: Inference speed refers to the time that it takes for a model to make a prediction
    â€” this will vary depending on the hardware used and the way in which models are
    queried, e.g. in real time via an API or in batch jobs that run once a day.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨ç†é€Ÿåº¦æŒ‡çš„æ˜¯æ¨¡å‹è¿›è¡Œé¢„æµ‹æ‰€éœ€çš„æ—¶é—´ â€” è¿™å–å†³äºæ‰€ä½¿ç”¨çš„ç¡¬ä»¶ä»¥åŠæŸ¥è¯¢æ¨¡å‹çš„æ–¹å¼ï¼Œä¾‹å¦‚é€šè¿‡APIå®æ—¶æŸ¥è¯¢æˆ–æ¯å¤©è¿è¡Œä¸€æ¬¡çš„æ‰¹å¤„ç†ä½œä¸šã€‚
- en: Memory footprint refers to the size of the model weights and how much hardware
    memory they occupy. If a model is too large to fit on a single GPU or CPU, then
    it has to be split over multiple ones, which can be more or less difficult depending
    on the model architecture and the deployment method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜å ç”¨é‡æŒ‡çš„æ˜¯æ¨¡å‹æƒé‡çš„å¤§å°ä»¥åŠå®ƒä»¬å ç”¨çš„ç¡¬ä»¶å†…å­˜é‡ã€‚å¦‚æœä¸€ä¸ªæ¨¡å‹å¤ªå¤§ï¼Œæ— æ³•æ”¾å…¥å•ä¸ªGPUæˆ–CPUä¸­ï¼Œé‚£ä¹ˆå°±å¿…é¡»å°†å…¶åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œè¿™å–å†³äºæ¨¡å‹æ¶æ„å’Œéƒ¨ç½²æ–¹æ³•çš„å¤æ‚ç¨‹åº¦ã€‚
- en: When doing online model evaluation, there is often a trade-off to be done between
    inference speed and accuracy or precision, whereas this is less the case for offline
    evaluation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨çº¿æ¨¡å‹è¯„ä¼°æ—¶ï¼Œé€šå¸¸éœ€è¦åœ¨æ¨ç†é€Ÿåº¦å’Œå‡†ç¡®æ€§æˆ–ç²¾åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œè€Œå¯¹äºç¦»çº¿è¯„ä¼°æ¥è¯´ï¼Œæƒ…å†µå°±ä¸é‚£ä¹ˆæ˜æ˜¾äº†ã€‚
- en: Limitations and bias
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é™åˆ¶å’Œåè§
- en: All models and all metrics have their limitations and biases, which depend on
    the way in which they were trained, the data that was used, and their intended
    uses. It is important to measure and communicate these limitations clearly to
    prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt)
    which document the training and evaluation process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ¨¡å‹å’Œæ‰€æœ‰æŒ‡æ ‡éƒ½æœ‰å…¶å±€é™æ€§å’Œåè§ï¼Œè¿™å–å†³äºå®ƒä»¬çš„è®­ç»ƒæ–¹å¼ã€ä½¿ç”¨çš„æ•°æ®ä»¥åŠé¢„æœŸçš„ç”¨é€”ã€‚é‡è¦çš„æ˜¯è¦æ¸…æ¥šåœ°æµ‹é‡å’Œä¼ è¾¾è¿™äº›é™åˆ¶ï¼Œä»¥é˜²æ­¢è¯¯ç”¨å’Œæ„å¤–å½±å“ï¼Œä¾‹å¦‚é€šè¿‡[æ¨¡å‹å¡ç‰‡](https://huggingface.co/course/chapter4/4?fw=pt)æ¥è®°å½•è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ã€‚
- en: Measuring biases can be done by evaluating models on datasets such as [Wino
    Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias),
    and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis)
    to try to identify which subsets of the evaluation dataset a model performs poorly
    on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨è¯¸å¦‚[Wino Bias](https://huggingface.co/datasets/wino_bias)æˆ–[MD Gender Bias](https://huggingface.co/datasets/md_gender_bias)ç­‰æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶é€šè¿‡è¿›è¡Œ[äº¤äº’å¼é”™è¯¯åˆ†æ](https://huggingface.co/spaces/nazneen/error-analysis)æ¥æµ‹é‡åè§ã€‚å°è¯•è¯†åˆ«æ¨¡å‹åœ¨è¯„ä¼°æ•°æ®é›†çš„å“ªäº›å­é›†ä¸Šè¡¨ç°ä¸ä½³ã€‚
- en: We are currently working on additional measurements that can be used to quantify
    different dimensions of bias in both models and datasets â€” stay tuned for more
    documentation on this topic!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç›®å‰æ­£åœ¨åŠªåŠ›å¼€å±•é¢å¤–çš„æµ‹é‡å·¥ä½œï¼Œä»¥é‡åŒ–æ¨¡å‹å’Œæ•°æ®é›†ä¸­ä¸åŒç»´åº¦çš„åè§ - æ•¬è¯·å…³æ³¨æœ‰å…³æ­¤ä¸»é¢˜çš„æ›´å¤šæ–‡æ¡£ï¼
