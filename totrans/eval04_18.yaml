- en: Considerations for model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/evaluate/considerations](https://huggingface.co/docs/evaluate/considerations)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/start-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/vendor-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/paths-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/__layout.svelte-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/pages/considerations.mdx-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/Tip-hf-doc-builder.js">
    <link rel="modulepreload" href="/docs/evaluate/v0.4.0/en/_app/chunks/IconCopyLink-hf-doc-builder.js">
  prefs: []
  type: TYPE_NORMAL
- en: 'Developing an ML model is rarely a one-shot deal: it often involves multiple
    stages of defining the model architecture and tuning hyper-parameters before converging
    on a final set. Responsible model evaluation is a key part of this process, and
    ü§ó Evaluate is here to help!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some things to keep in mind when evaluating your model using the ü§ó
    Evaluate library:'
  prefs: []
  type: TYPE_NORMAL
- en: Properly splitting your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Good evaluation generally requires three splits of your dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**train**: this is used for training your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validation**: this is used for validating the model hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test**: this is used for evaluating your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of the datasets on the ü§ó Hub are separated into 2 splits: `train` and
    `validation`; others are split into 3 splits (`train`, `validation` and `test`)
    ‚Äî make sure to use the right split for the right purpose!'
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets on the ü§ó Hub are already separated into these three splits. However,
    there are also many that only have a train/validation or only train split.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset you‚Äôre using doesn‚Äôt have a predefined train-test split, it is
    up to you to define which part of the dataset you want to use for training your
    model and which you want to use for hyperparameter tuning or final evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating on the same split can misrepresent your results! If
    you overfit on your training data the evaluation results on that split will look
    great but the model will perform poorly on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the size of the dataset, you can keep anywhere from 10-30% for
    evaluation and the rest for training, while aiming to set up the test set to reflect
    the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090)
    for a more in-depth discussion of dataset splitting!
  prefs: []
  type: TYPE_NORMAL
- en: The impact of class imbalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb)
    of movie reviews, are perfectly balanced, most real-world datasets are not. In
    machine learning a *balanced dataset* corresponds to a datasets where all labels
    are represented equally. In the case of the IMDb dataset this means that there
    are as many positive as negative reviews in the dataset. In an imbalanced dataset
    this is not the case: in fraud detection for example there are usually many more
    non-fraud cases than fraud cases in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Having an imbalanced dataset can skew the results of your metrics. Imagine a
    dataset with 99 ‚Äúnon-fraud‚Äù cases and 1 ‚Äúfraud‚Äù case. A simple model that always
    predicts ‚Äúnon-fraud‚Äù cases would give yield a 99% accuracy which might sound good
    at first until you realize that you will never catch a fraud case.
  prefs: []
  type: TYPE_NORMAL
- en: Often, using more than one metric can help get a better idea of your model‚Äôs
    performance from different points of view. For instance, metrics like **[recall](https://huggingface.co/metrics/recall)**
    and **[precision](https://huggingface.co/metrics/precision)** can be used together,
    and the **[f1 score](https://huggingface.co/metrics/f1)** is actually the harmonic
    mean of the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where a dataset is balanced, using [accuracy](https://huggingface.co/metrics/accuracy)
    can reflect the overall model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Balanced Labels](../Images/3806f661f4ca701df162a69f4b08f9cb.png)'
  prefs: []
  type: TYPE_IMG
- en: In cases where there is an imbalance, using [F1 score](https://huggingface.co/metrics/f1)
    can be a better representation of performance, given that it encompasses both
    precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: '![Imbalanced Labels](../Images/da6bb8aa7c358d094b409c9168313526.png)'
  prefs: []
  type: TYPE_IMG
- en: Using accuracy in an imbalanced setting is less ideal, since it is not sensitive
    to minority classes and will not faithfully reflect model performance on them.
  prefs: []
  type: TYPE_NORMAL
- en: Offline vs. online model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple ways to evaluate models, and an important distinction is
    offline versus online evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Offline evaluation** is done before deploying a model or using insights generated
    from a model, using static datasets and metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Online evaluation** means evaluating how a model is performing after deployment
    and during its use in production.'
  prefs: []
  type: TYPE_NORMAL
- en: These two types of evaluation can use different metrics and measure different
    aspects of model performance. For example, offline evaluation can compare a model
    to other models based on their performance on common benchmarks, whereas online
    evaluation will evaluate aspects such as latency and accuracy of the model based
    on production data (for example, the number of user queries that it was able to
    address).
  prefs: []
  type: TYPE_NORMAL
- en: Trade-offs in model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When evaluating models in practice, there are often trade-offs that have to
    be made between different aspects of model performance: for instance, choosing
    a model that is slightly less accurate but that has a faster inference time, compared
    to a high-accuracy that has a higher memory footprint and requires access to more
    GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are other aspects of model performance to consider during evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When evaluating models, **interpretability** (i.e. the ability to *interpret*
    results) can be very important, especially when deploying models in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, metrics such as [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match)
    have a set range (between 0 and 1, or 0% and 100%) and are easily understandable
    to users: for a pair of strings, the exact match score is 1 if the two strings
    are the exact same, and 0 otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other metrics, such as [BLEU](https://huggingface.co/spaces/evaluate-metric/exact_match)
    are harder to interpret: while they also range between 0 and 1, they can vary
    greatly depending on which parameters are used to generate the scores, especially
    when different tokenization and normalization techniques are used (see the [metric
    card](https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md)
    for more information about BLEU limitations). This means that it is difficult
    to interpret a BLEU score without having more information about the procedure
    used for obtaining it.'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability can be more or less important depending on the evaluation use
    case, but it is a useful aspect of model evaluation to keep in mind, since communicating
    and comparing model evaluations is an important part of responsible machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Inference speed and memory footprint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While recent years have seen increasingly large ML models achieve high performance
    on a large variety of tasks and benchmarks, deploying these multi-billion parameter
    models in practice can be a challenge in itself, and many organizations lack the
    resources for this. This is why considering the **inference speed** and **memory
    footprint** of models is important, especially when doing online model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Inference speed refers to the time that it takes for a model to make a prediction
    ‚Äî this will vary depending on the hardware used and the way in which models are
    queried, e.g. in real time via an API or in batch jobs that run once a day.
  prefs: []
  type: TYPE_NORMAL
- en: Memory footprint refers to the size of the model weights and how much hardware
    memory they occupy. If a model is too large to fit on a single GPU or CPU, then
    it has to be split over multiple ones, which can be more or less difficult depending
    on the model architecture and the deployment method.
  prefs: []
  type: TYPE_NORMAL
- en: When doing online model evaluation, there is often a trade-off to be done between
    inference speed and accuracy or precision, whereas this is less the case for offline
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All models and all metrics have their limitations and biases, which depend on
    the way in which they were trained, the data that was used, and their intended
    uses. It is important to measure and communicate these limitations clearly to
    prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt)
    which document the training and evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring biases can be done by evaluating models on datasets such as [Wino
    Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias),
    and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis)
    to try to identify which subsets of the evaluation dataset a model performs poorly
    on.
  prefs: []
  type: TYPE_NORMAL
- en: We are currently working on additional measurements that can be used to quantify
    different dimensions of bias in both models and datasets ‚Äî stay tuned for more
    documentation on this topic!
  prefs: []
  type: TYPE_NORMAL
