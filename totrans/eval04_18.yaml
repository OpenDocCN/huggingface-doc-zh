- en: Considerations for model evaluation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/considerations](https://huggingface.co/docs/evaluate/considerations)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Developing an ML model is rarely a one-shot deal: it often involves multiple
    stages of defining the model architecture and tuning hyper-parameters before converging
    on a final set. Responsible model evaluation is a key part of this process, and
    🤗 Evaluate is here to help!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some things to keep in mind when evaluating your model using the 🤗
    Evaluate library:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Properly splitting your data
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Good evaluation generally requires three splits of your dataset:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**train**: this is used for training your model.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**validation**: this is used for validating the model hyperparameters.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test**: this is used for evaluating your model.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of the datasets on the 🤗 Hub are separated into 2 splits: `train` and
    `validation`; others are split into 3 splits (`train`, `validation` and `test`)
    — make sure to use the right split for the right purpose!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Some datasets on the 🤗 Hub are already separated into these three splits. However,
    there are also many that only have a train/validation or only train split.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset you’re using doesn’t have a predefined train-test split, it is
    up to you to define which part of the dataset you want to use for training your
    model and which you want to use for hyperparameter tuning or final evaluation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating on the same split can misrepresent your results! If
    you overfit on your training data the evaluation results on that split will look
    great but the model will perform poorly on new data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the size of the dataset, you can keep anywhere from 10-30% for
    evaluation and the rest for training, while aiming to set up the test set to reflect
    the production data as close as possible. Check out [this thread](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090)
    for a more in-depth discussion of dataset splitting!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The impact of class imbalance
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While many academic datasets, such as the [IMDb dataset](https://huggingface.co/datasets/imdb)
    of movie reviews, are perfectly balanced, most real-world datasets are not. In
    machine learning a *balanced dataset* corresponds to a datasets where all labels
    are represented equally. In the case of the IMDb dataset this means that there
    are as many positive as negative reviews in the dataset. In an imbalanced dataset
    this is not the case: in fraud detection for example there are usually many more
    non-fraud cases than fraud cases in the dataset.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Having an imbalanced dataset can skew the results of your metrics. Imagine a
    dataset with 99 “non-fraud” cases and 1 “fraud” case. A simple model that always
    predicts “non-fraud” cases would give yield a 99% accuracy which might sound good
    at first until you realize that you will never catch a fraud case.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Often, using more than one metric can help get a better idea of your model’s
    performance from different points of view. For instance, metrics like **[recall](https://huggingface.co/metrics/recall)**
    and **[precision](https://huggingface.co/metrics/precision)** can be used together,
    and the **[f1 score](https://huggingface.co/metrics/f1)** is actually the harmonic
    mean of the two.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases where a dataset is balanced, using [accuracy](https://huggingface.co/metrics/accuracy)
    can reflect the overall model performance:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Balanced Labels](../Images/3806f661f4ca701df162a69f4b08f9cb.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: In cases where there is an imbalance, using [F1 score](https://huggingface.co/metrics/f1)
    can be a better representation of performance, given that it encompasses both
    precision and recall.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![Imbalanced Labels](../Images/da6bb8aa7c358d094b409c9168313526.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Using accuracy in an imbalanced setting is less ideal, since it is not sensitive
    to minority classes and will not faithfully reflect model performance on them.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Offline vs. online model evaluation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple ways to evaluate models, and an important distinction is
    offline versus online evaluation:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型有多种方式，一个重要的区别是离线评估和在线评估：
- en: '**Offline evaluation** is done before deploying a model or using insights generated
    from a model, using static datasets and metrics.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**离线评估**是在部署模型或使用模型生成的见解之前进行的，使用静态数据集和指标。'
- en: '**Online evaluation** means evaluating how a model is performing after deployment
    and during its use in production.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线评估**意味着在部署后以及在生产中使用模型时评估模型的性能。'
- en: These two types of evaluation can use different metrics and measure different
    aspects of model performance. For example, offline evaluation can compare a model
    to other models based on their performance on common benchmarks, whereas online
    evaluation will evaluate aspects such as latency and accuracy of the model based
    on production data (for example, the number of user queries that it was able to
    address).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种评估类型可以使用不同的指标，并衡量模型性能的不同方面。例如，离线评估可以根据常见基准测试比较模型与其他模型的性能，而在线评估将根据生产数据（例如，能够处理的用户查询数量）评估模型的延迟和准确性等方面。
- en: Trade-offs in model evaluation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估中的权衡
- en: 'When evaluating models in practice, there are often trade-offs that have to
    be made between different aspects of model performance: for instance, choosing
    a model that is slightly less accurate but that has a faster inference time, compared
    to a high-accuracy that has a higher memory footprint and requires access to more
    GPUs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中评估模型时，通常需要在模型性能的不同方面之间进行权衡：例如，选择一个略微准确性较低但推理时间更快的模型，与一个准确性较高但内存占用量更大且需要更多GPU访问的模型相比。
- en: 'Here are other aspects of model performance to consider during evaluation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在评估过程中考虑的模型性能的其他方面：
- en: Interpretability
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释性
- en: When evaluating models, **interpretability** (i.e. the ability to *interpret*
    results) can be very important, especially when deploying models in production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型时，**解释性**（即*解释结果的能力*）可能非常重要，特别是在生产中部署模型时。
- en: 'For instance, metrics such as [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match)
    have a set range (between 0 and 1, or 0% and 100%) and are easily understandable
    to users: for a pair of strings, the exact match score is 1 if the two strings
    are the exact same, and 0 otherwise.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像[exact match](https://huggingface.co/spaces/evaluate-metric/exact_match)这样的指标具有一定范围（在0和1之间，或者0%和100%之间），并且对用户来说很容易理解：对于一对字符串，如果两个字符串完全相同，则精确匹配分数为1，否则为0。
- en: 'Other metrics, such as [BLEU](https://huggingface.co/spaces/evaluate-metric/exact_match)
    are harder to interpret: while they also range between 0 and 1, they can vary
    greatly depending on which parameters are used to generate the scores, especially
    when different tokenization and normalization techniques are used (see the [metric
    card](https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md)
    for more information about BLEU limitations). This means that it is difficult
    to interpret a BLEU score without having more information about the procedure
    used for obtaining it.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其他指标，如[BLEU](https://huggingface.co/spaces/evaluate-metric/exact_match)更难解释：虽然它们也在0和1之间变化，但根据用于生成分数的参数不同，它们可能会有很大的变化，特别是当使用不同的标记化和规范化技术时（有关BLEU限制的更多信息，请参阅[指标卡](https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md)）。这意味着在没有更多关于获得BLEU分数的过程的信息的情况下，很难解释BLEU分数。
- en: Interpretability can be more or less important depending on the evaluation use
    case, but it is a useful aspect of model evaluation to keep in mind, since communicating
    and comparing model evaluations is an important part of responsible machine learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性可能在评估用例中更或更不重要，但它是模型评估的一个有用方面，因为沟通和比较模型评估是负责任的机器学习的重要组成部分。
- en: Inference speed and memory footprint
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理速度和内存占用量
- en: While recent years have seen increasingly large ML models achieve high performance
    on a large variety of tasks and benchmarks, deploying these multi-billion parameter
    models in practice can be a challenge in itself, and many organizations lack the
    resources for this. This is why considering the **inference speed** and **memory
    footprint** of models is important, especially when doing online model evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，越来越大的机器学习模型在各种任务和基准测试中取得了高性能，但在实践中部署这些数十亿参数的模型本身就是一个挑战，许多组织缺乏这方面的资源。这就是为什么在进行在线模型评估时考虑模型的**推理速度**和**内存占用量**是重要的原因。
- en: Inference speed refers to the time that it takes for a model to make a prediction
    — this will vary depending on the hardware used and the way in which models are
    queried, e.g. in real time via an API or in batch jobs that run once a day.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 推理速度指的是模型进行预测所需的时间 — 这取决于所使用的硬件以及查询模型的方式，例如通过API实时查询或每天运行一次的批处理作业。
- en: Memory footprint refers to the size of the model weights and how much hardware
    memory they occupy. If a model is too large to fit on a single GPU or CPU, then
    it has to be split over multiple ones, which can be more or less difficult depending
    on the model architecture and the deployment method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 内存占用量指的是模型权重的大小以及它们占用的硬件内存量。如果一个模型太大，无法放入单个GPU或CPU中，那么就必须将其分割到多个设备上，这取决于模型架构和部署方法的复杂程度。
- en: When doing online model evaluation, there is often a trade-off to be done between
    inference speed and accuracy or precision, whereas this is less the case for offline
    evaluation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在线模型评估时，通常需要在推理速度和准确性或精度之间进行权衡，而对于离线评估来说，情况就不那么明显了。
- en: Limitations and bias
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制和偏见
- en: All models and all metrics have their limitations and biases, which depend on
    the way in which they were trained, the data that was used, and their intended
    uses. It is important to measure and communicate these limitations clearly to
    prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt)
    which document the training and evaluation process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型和所有指标都有其局限性和偏见，这取决于它们的训练方式、使用的数据以及预期的用途。重要的是要清楚地测量和传达这些限制，以防止误用和意外影响，例如通过[模型卡片](https://huggingface.co/course/chapter4/4?fw=pt)来记录训练和评估过程。
- en: Measuring biases can be done by evaluating models on datasets such as [Wino
    Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias),
    and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis)
    to try to identify which subsets of the evaluation dataset a model performs poorly
    on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在诸如[Wino Bias](https://huggingface.co/datasets/wino_bias)或[MD Gender Bias](https://huggingface.co/datasets/md_gender_bias)等数据集上评估模型，并通过进行[交互式错误分析](https://huggingface.co/spaces/nazneen/error-analysis)来测量偏见。尝试识别模型在评估数据集的哪些子集上表现不佳。
- en: We are currently working on additional measurements that can be used to quantify
    different dimensions of bias in both models and datasets — stay tuned for more
    documentation on this topic!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在努力开展额外的测量工作，以量化模型和数据集中不同维度的偏见 - 敬请关注有关此主题的更多文档！
