["```py\npip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade\n```", "```py\n%%capture\nimport IPython\n!conda install -c conda-forge ipywidgets -y\nIPython.Application.instance().kernel.do_shutdown(True)\n```", "```py\nimport sagemaker\n\nsess = sagemaker.Session()\nsagemaker_session_bucket = None\nif sagemaker_session_bucket is None and sess is not None:\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n```", "```py\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# load dataset\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# create tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n\n# tokenize train and test datasets\ntrain_dataset = train_dataset.map(tokenize, batched=True)\ntest_dataset = test_dataset.map(tokenize, batched=True)\n\n# set dataset format for PyTorch\ntrain_dataset =  train_dataset.rename_column(\"label\", \"labels\")\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset = test_dataset.rename_column(\"label\", \"labels\")\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n```", "```py\n# save train_dataset to s3\ntraining_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\ntrain_dataset.save_to_disk(training_input_path)\n\n# save test_dataset to s3\ntest_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\ntest_dataset.save_to_disk(test_input_path)\n```", "```py\nfrom sagemaker.huggingface import HuggingFace\n\nhyperparameters={\n    \"epochs\": 1,                            # number of training epochs\n    \"train_batch_size\": 32,                 # training batch size\n    \"model_name\":\"distilbert-base-uncased\"  # name of pretrained model\n}\n\nhuggingface_estimator = HuggingFace(\n    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n    source_dir=\"./scripts\",                 # directory where fine-tuning script is stored\n    instance_type=\"ml.p3.2xlarge\",          # instance type\n    instance_count=1,                       # number of instances\n    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n    transformers_version=\"4.26\",             # Transformers version\n    pytorch_version=\"1.13\",                  # PyTorch version\n    py_version=\"py39\",                      # Python version\n    hyperparameters=hyperparameters         # hyperparameters to use in training job\n)\n```", "```py\nhuggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})\n```", "```py\npredictor = huggingface_estimator.deploy(initial_instance_count=1,\"ml.g4dn.xlarge\")\n```", "```py\nsentiment_input = {\"inputs\": \"It feels like a curtain closing...there was an elegance in the way they moved toward conclusion. No fan is going to watch and feel short-changed.\"}\n\npredictor.predict(sentiment_input)\n```", "```py\npredictor.delete_endpoint()\n```"]