# æ¶ˆè´¹æ–‡æœ¬ç”Ÿæˆæ¨ç†

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi](https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi)

æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨ç†æœåŠ¡å™¨ã€‚å¯åŠ¨åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`/generate`è·¯ç”±å¹¶å‘å‡º`POST`è¯·æ±‚ä»¥ä»æœåŠ¡å™¨è·å–ç»“æœã€‚å¦‚æœå¸Œæœ›TGIè¿”å›æ ‡è®°æµï¼Œåˆ™è¿˜å¯ä»¥ä½¿ç”¨`/generate_stream`è·¯ç”±ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„å·¥å…·ï¼ˆå¦‚curlã€Pythonæˆ–TypeScrptï¼‰å‘å‡ºè¯·æ±‚ã€‚ä¸ºäº†è·å¾—æœ€ç»ˆçš„ç«¯åˆ°ç«¯ä½“éªŒï¼Œæˆ‘ä»¬è¿˜å¼€æºäº†ChatUIï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¼€æºæ¨¡å‹çš„èŠå¤©ç•Œé¢ã€‚

## curl

å¯åŠ¨åï¼Œå¯ä»¥ä½¿ç”¨`/generate`æˆ–`/generate_stream`è·¯ç”±æŸ¥è¯¢æ¨¡å‹ï¼š

```py
curl 127.0.0.1:8080/generate \
    -X POST \
    -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":20}}' \
    -H 'Content-Type: application/json'
```

## æ¨ç†å®¢æˆ·ç«¯

[`huggingface-hub`](https://huggingface.co/docs/huggingface_hub/main/en/index)æ˜¯ä¸€ä¸ªç”¨äºä¸Hugging Face Hubäº¤äº’çš„Pythonåº“ï¼ŒåŒ…æ‹¬å…¶ç«¯ç‚¹ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„é«˜çº§ç±»[`~huggingface_hub.InferenceClient`]ï¼Œä½¿å¾—è°ƒç”¨TGIç«¯ç‚¹å˜å¾—å®¹æ˜“ã€‚`InferenceClient`è¿˜è´Ÿè´£å‚æ•°éªŒè¯å¹¶æä¾›ä¸€ä¸ªç®€å•æ˜“ç”¨çš„æ¥å£ã€‚æ‚¨å¯ä»¥é€šè¿‡pipç®€å•å®‰è£…`huggingface-hub`åŒ…ã€‚

```py
pip install huggingface-hub
```

ä¸€æ—¦å¯åŠ¨TGIæœåŠ¡å™¨ï¼Œè¯·ä½¿ç”¨æä¾›æ¨¡å‹çš„ç«¯ç‚¹çš„URLå®ä¾‹åŒ–`InferenceClient()`ã€‚ç„¶åå¯ä»¥é€šè¿‡Pythonè°ƒç”¨`text_generation()`æ¥è®¿é—®ç«¯ç‚¹ã€‚

```py
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://127.0.0.1:8080")
client.text_generation(prompt="Write a code for snake game")
```

é€šè¿‡ä¼ é€’`stream=True`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`InferenceClient`è¿›è¡Œæµå¼å¤„ç†ã€‚æµå¼å¤„ç†å°†åœ¨æœåŠ¡å™¨ç”Ÿæˆæ ‡è®°æ—¶è¿”å›æ ‡è®°ã€‚è¦ä½¿ç”¨æµå¼å¤„ç†ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

```py
for token in client.text_generation("How do you make cheese?", max_new_tokens=12, stream=True):
    print(token)
```

æ‚¨å¯ä»¥åœ¨TGIåç«¯ä¸­ä½¿ç”¨çš„å¦ä¸€ä¸ªå‚æ•°æ˜¯`details`ã€‚é€šè¿‡å°†`details`è®¾ç½®ä¸º`True`ï¼Œå¯ä»¥è·å–æœ‰å…³ç”Ÿæˆï¼ˆæ ‡è®°ã€æ¦‚ç‡ç­‰ï¼‰çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚å½“æŒ‡å®šæ—¶ï¼ŒTGIå°†è¿”å›`TextGenerationResponse`æˆ–`TextGenerationStreamResponse`è€Œä¸æ˜¯å­—ç¬¦ä¸²æˆ–æµã€‚

```py
output = client.text_generation(prompt="Meaning of life is", details=True)
print(output)

# TextGenerationResponse(generated_text=' a complex concept that is not always clear to the individual. It is a concept that is not always', details=Details(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=20, seed=None, prefill=[], tokens=[Token(id=267, text=' a', logprob=-2.0723474, special=False), Token(id=11235, text=' complex', logprob=-3.1272552, special=False), Token(id=17908, text=' concept', logprob=-1.3632495, special=False),..))
```

æ‚¨å¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„æµå¼å¤„ç†æ–¹å¼ã€‚

```py
output = client.text_generation(prompt="Meaning of life is", stream=True, details=True)
print(next(iter(output)))

# TextGenerationStreamResponse(token=Token(id=267, text=' a', logprob=-2.0723474, special=False), generated_text=None, details=None)
```

æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation)æŸ¥çœ‹è¯¥å‡½æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚è¿˜æœ‰ä¸€ä¸ªåŸºäº`asyncio`å’Œ`aiohttp`çš„å®¢æˆ·ç«¯çš„å¼‚æ­¥ç‰ˆæœ¬`AsyncInferenceClient`ã€‚æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)æ‰¾åˆ°å…¶æ–‡æ¡£ã€‚

## ChatUI

ChatUIæ˜¯ä¸ºLLMæœåŠ¡æ„å»ºçš„å¼€æºç•Œé¢ã€‚å®ƒæä¾›è®¸å¤šè‡ªå®šä¹‰é€‰é¡¹ï¼Œä¾‹å¦‚ä½¿ç”¨SERP APIè¿›è¡Œç½‘ç»œæœç´¢ç­‰ã€‚ChatUIå¯ä»¥è‡ªåŠ¨æ¶ˆè´¹TGIæœåŠ¡å™¨ï¼Œç”šè‡³æä¾›åœ¨ä¸åŒTGIç«¯ç‚¹ä¹‹é—´åˆ‡æ¢çš„é€‰é¡¹ã€‚æ‚¨å¯ä»¥åœ¨[Hugging Chat](https://huggingface.co/chat/)å°è¯•å®ƒï¼Œæˆ–ä½¿ç”¨[ChatUI Docker Space](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)éƒ¨ç½²è‡ªå·±çš„Hugging Chatåˆ°Spacesã€‚

è¦åœ¨åŒä¸€ç¯å¢ƒä¸­åŒæ—¶æä¾›ChatUIå’ŒTGIæœåŠ¡ï¼Œåªéœ€å°†æ‚¨è‡ªå·±çš„ç«¯ç‚¹æ·»åŠ åˆ°`chat-ui`å­˜å‚¨åº“ä¸­çš„`.env.local`æ–‡ä»¶ä¸­çš„`MODELS`å˜é‡ä¸­ã€‚æä¾›æŒ‡å‘TGIæœåŠ¡ä½ç½®çš„ç«¯ç‚¹ã€‚

```py
{
// rest of the model config here
"endpoints": [{"url": "https://HOST:PORT/generate_stream"}]
}
```

![ChatUI](../Images/ce268cbdae69aa7842502b673821bebb.png)

## Gradio

Gradioæ˜¯ä¸€ä¸ªPythonåº“ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ä½¿ç”¨å‡ è¡Œä»£ç ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æ„å»ºWebåº”ç”¨ç¨‹åºã€‚å®ƒå…·æœ‰ä¸€ä¸ª`ChatInterface`åŒ…è£…å™¨ï¼Œå¯å¸®åŠ©åˆ›å»ºèŠå¤©æœºå™¨äººçš„æ•´æ´UIã€‚è®©æˆ‘ä»¬å…ˆå®‰è£…Gradioå’ŒHub Pythonåº“ã€‚

```py
pip install huggingface-hub gradio
```

å‡è®¾æ‚¨åœ¨ç«¯å£8080ä¸Šæä¾›æ¨¡å‹ï¼Œæˆ‘ä»¬å°†é€šè¿‡[InferenceClient](consuming_tgi#inference-client)è¿›è¡ŒæŸ¥è¯¢ã€‚

```py
import gradio as gr
from huggingface_hub import InferenceClient

client = InferenceClient(model="http://127.0.0.1:8080")

def inference(message, history):
    partial_message = ""
    for token in client.text_generation(message, max_new_tokens=20, stream=True):
        partial_message += token
        yield partial_message

gr.ChatInterface(
    inference,
    chatbot=gr.Chatbot(height=300),
    textbox=gr.Textbox(placeholder="Chat with me!", container=False, scale=7),
    description="This is the demo for Gradio UI consuming TGI endpoint with LLaMA 7B-Chat model.",
    title="Gradio ğŸ¤ TGI",
    examples=["Are tomatoes vegetables?"],
    retry_btn="Retry",
    undo_btn="Undo",
    clear_btn="Clear",
).queue().launch()
```

UIå¦‚ä¸‹ğŸ‘‡

![](../Images/145df95e03868e6e18c9588c0878b82a.png) ![](../Images/af0024df3a67614ca6a101e75f4cdb9c.png)

æ‚¨å¯ä»¥ç›´æ¥åœ¨æ­¤å¤„å°è¯•æ¼”ç¤ºğŸ‘‡

[https://merve-gradio-tgi-2.hf.space?__theme=light](https://merve-gradio-tgi-2.hf.space?__theme=light)

[https://merve-gradio-tgi-2.hf.space?__theme=dark](https://merve-gradio-tgi-2.hf.space?__theme=dark)

æ‚¨å¯ä»¥åœ¨æ¨æ–­å‡½æ•°ä¸­ä½¿ç”¨`return`è€Œä¸æ˜¯`yield`æ¥ç¦ç”¨æµå¼æ¨¡å¼ï¼Œå°±åƒä¸‹é¢è¿™æ ·ã€‚

```py
def inference(message, history):
    return client.text_generation(message, max_new_tokens=20)
```

æ‚¨å¯ä»¥é˜…è¯»æœ‰å…³å¦‚ä½•è‡ªå®šä¹‰`ChatInterface`çš„æ›´å¤šä¿¡æ¯[è¿™é‡Œ](https://www.gradio.app/guides/creating-a-chatbot-fast)ã€‚

## APIæ–‡æ¡£

æ‚¨å¯ä»¥ä½¿ç”¨`/docs`è·¯ç”±æŸ¥é˜…`text-generation-inference` REST APIçš„OpenAPIæ–‡æ¡£ã€‚Swagger UIä¹Ÿå¯åœ¨[è¿™é‡Œ](https://huggingface.github.io/text-generation-inference)æ‰¾åˆ°ã€‚
