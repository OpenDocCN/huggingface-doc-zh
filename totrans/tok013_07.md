# ä»å†…å­˜ä¸­è®­ç»ƒ

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/tokenizers/training_from_memory`](https://huggingface.co/docs/tokenizers/training_from_memory)

åœ¨å¿«é€Ÿå…¥é—¨ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ„å»ºå’Œè®­ç»ƒåˆ†è¯å™¨ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½• Python è¿­ä»£å™¨ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å‡ ç§ä¸åŒçš„è®­ç»ƒåˆ†è¯å™¨çš„æ–¹æ³•ã€‚

å¯¹äºä¸‹é¢åˆ—å‡ºçš„æ‰€æœ‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ Tokenizer å’Œ`Trainer`ï¼Œæ„å»ºå¦‚ä¸‹ï¼š

```py
from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, trainers
tokenizer = Tokenizer(models.Unigram())
tokenizer.normalizer = normalizers.NFKC()
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
tokenizer.decoder = decoders.ByteLevel()
trainer = trainers.UnigramTrainer(
    vocab_size=20000,
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),
    special_tokens=["<PAD>", "<BOS>", "<EOS>"],
)
```

è¿™ä¸ªåˆ†è¯å™¨åŸºäº Unigram æ¨¡å‹ã€‚å®ƒé€šè¿‡ NFKC Unicode è§„èŒƒåŒ–æ–¹æ³•å¯¹è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œå¹¶ä½¿ç”¨å…·æœ‰ç›¸åº”è§£ç å™¨çš„ ByteLevel é¢„åˆ†è¯å™¨ã€‚

æœ‰å…³æ­¤å¤„ä½¿ç”¨çš„ç»„ä»¶çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„æŸ¥çœ‹ã€‚

## æœ€åŸºæœ¬çš„æ–¹å¼

æ‚¨å¯èƒ½å·²ç»çŒœåˆ°äº†ï¼Œè®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨`List`{.interpreted-text role=â€œobjâ€}ï¼š

```py
# First few lines of the "Zen of Python" https://www.python.org/dev/peps/pep-0020/
data = [
    "Beautiful is better than ugly."
    "Explicit is better than implicit."
    "Simple is better than complex."
    "Complex is better than complicated."
    "Flat is better than nested."
    "Sparse is better than dense."
    "Readability counts."
]
tokenizer.train_from_iterator(data, trainer=trainer)
```

ç®€å•ï¼Œå¯¹å§ï¼Ÿæ‚¨å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ä»»ä½•ä½œä¸ºè¿­ä»£å™¨çš„ä¸œè¥¿ï¼Œæ— è®ºæ˜¯`List`{.interpreted-text role=â€œobjâ€}ã€`Tuple`{.interpreted-text role=â€œobjâ€}è¿˜æ˜¯`np.Array`{.interpreted-text role=â€œobjâ€}ã€‚åªè¦æä¾›å­—ç¬¦ä¸²ï¼Œä»»ä½•ä¸œè¥¿éƒ½å¯ä»¥ä½¿ç”¨ã€‚

## ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“

è®¿é—®ä¼—å¤šç°æœ‰æ•°æ®é›†ä¹‹ä¸€çš„ç»ä½³æ–¹å¼æ˜¯ä½¿ç”¨ğŸ¤—æ•°æ®é›†åº“ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[æ­¤å¤„çš„å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/datasets/)ã€‚

è®©æˆ‘ä»¬ä»åŠ è½½æ•°æ®é›†å¼€å§‹ï¼š

```py
import datasets
dataset = datasets.load_dataset("wikitext", "wikitext-103-raw-v1", split="train+test+validation")
```

ä¸‹ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šçš„è¿­ä»£å™¨ã€‚å¯èƒ½æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨ç”Ÿæˆå™¨ï¼š

```py
def batch_iterator(batch_size=1000):
    for i in range(0, len(dataset), batch_size):
        yield dataset[i : i + batch_size]["text"]
```

æ­£å¦‚æ‚¨åœ¨è¿™é‡Œæ‰€çœ‹åˆ°çš„ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥æä¾›ä¸€æ‰¹ç”¨äºè®­ç»ƒçš„ç¤ºä¾‹ï¼Œè€Œä¸æ˜¯é€ä¸ªè¿­ä»£å®ƒä»¬ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›è·å¾—ä¸ç›´æ¥ä»æ–‡ä»¶è®­ç»ƒæ—¶ç›¸ä¼¼çš„æ€§èƒ½ã€‚

æœ‰äº†å‡†å¤‡å¥½çš„è¿­ä»£å™¨ï¼Œæˆ‘ä»¬åªéœ€è¦å¯åŠ¨è®­ç»ƒã€‚ä¸ºäº†æ”¹å–„è¿›åº¦æ¡çš„å¤–è§‚ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šæ•°æ®é›†çš„æ€»é•¿åº¦ï¼š

```py
tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))
```

å°±æ˜¯è¿™æ ·ï¼

## ä½¿ç”¨ gzip æ–‡ä»¶

ç”±äº Python ä¸­çš„ gzip æ–‡ä»¶å¯ä»¥ç”¨ä½œè¿­ä»£å™¨ï¼Œå› æ­¤åœ¨è¿™äº›æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒéå¸¸ç®€å•ï¼š

```py
import gzip
with gzip.open("data/my-file.0.gz", "rt") as f:
    tokenizer.train_from_iterator(f, trainer=trainer)
```

ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ä»å¤šä¸ª gzip æ–‡ä»¶è¿›è¡Œè®­ç»ƒï¼Œé‚£ä¹Ÿä¸ä¼šæ›´éš¾ï¼š

```py
files = ["data/my-file.0.gz", "data/my-file.1.gz", "data/my-file.2.gz"]
def gzip_iterator():
    for path in files:
        with gzip.open(path, "rt") as f:
            for line in f:
                yield line
tokenizer.train_from_iterator(gzip_iterator(), trainer=trainer)
```

ç„¶åå°±å®Œæˆäº†ï¼
