# OpenAI GPT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/openai-gpt)

[![模型](../Images/65c2999b59581465a08a2c29063c6d24.png)](https://huggingface.co/models?filter=openai-gpt) [![空间](../Images/61b85edcfdd50048184e2646e3f80d91.png)](https://huggingface.co/spaces/docs-demos/openai-gpt)

## 概述

OpenAI GPT模型是由Alec Radford、Karthik Narasimhan、Tim Salimans和Ilya Sutskever在[通过生成预训练改进语言理解](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)中提出的。它是一个使用语言建模在大型语料库上进行预训练的因果（单向）变压器，具有长距离依赖性，多伦多图书语料库。

论文摘要如下：

*自然语言理解包括各种不同的任务，如文本蕴涵、问题回答、语义相似性评估和文档分类。尽管大量未标记的文本语料库丰富，但用于学习这些特定任务的标记数据稀缺，这使得经过区分训练的模型难以表现出色。我们证明，通过在多样化的未标记文本语料库上对语言模型进行生成预训练，然后在每个特定任务上进行区分微调，可以实现这些任务的大幅提升。与以往方法相反，我们在微调过程中利用任务感知的输入转换，实现有效的迁移，同时对模型架构进行最小的更改。我们在自然语言理解的广泛基准上展示了我们方法的有效性。我们的通用任务不可知模型在9个研究的12个任务中优于使用专门为每个任务精心设计的架构的经过区分训练的模型，显著改进了技术水平。*

[使用Transformer写作](https://transformer.huggingface.co/doc/gpt)是由Hugging Face创建和托管的网络应用程序，展示了几种模型的生成能力。GPT是其中之一。

此模型由[thomwolf](https://huggingface.co/thomwolf)贡献。原始代码可以在[此处](https://github.com/openai/finetune-transformer-lm)找到。

## 使用提示

+   GPT是一个具有绝对位置嵌入的模型，因此通常建议在右侧而不是左侧填充输入。

+   GPT是通过因果语言建模（CLM）目标进行训练的，因此在预测序列中的下一个标记时非常强大。利用这一特性使GPT-2能够生成句法连贯的文本，可以在*run_generation.py*示例脚本中观察到。

注意：

如果您想复制*OpenAI GPT*论文的原始标记化过程，您需要安装`ftfy`和`SpaCy`：

```py
pip install spacy ftfy==4.4.3
python -m spacy download en
```

如果您没有安装`ftfy`和`SpaCy`，[OpenAIGPTTokenizer](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer)将默认使用BERT的`BasicTokenizer`进行标记化，然后是字节对编码（对于大多数用途来说应该没问题，不用担心）。

## 资源

官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用OpenAI GPT。如果您有兴趣提交资源以包含在此处，请随时打开Pull Request，我们将进行审查！资源应该理想地展示一些新东西，而不是重复现有资源。

文本分类

+   一篇关于[使用SetFit在文本分类中胜过OpenAI GPT-3的博客文章](https://www.philschmid.de/getting-started-setfit)。

+   另请参阅：[文本分类任务指南](../tasks/sequence_classification)

文本生成

+   关于如何[使用Hugging Face对非英语GPT-2模型进行微调](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)的博客。

+   一篇关于如何使用 GPT-2 进行文本生成的博客：[使用不同解码方法进行语言生成与 Transformers](https://huggingface.co/blog/how-to-generate)。

+   一篇关于从头开始训练 [CodeParrot 🦜 的博客](https://huggingface.co/blog/codeparrot)，一个大型的 GPT-2 模型。

+   一篇关于如何使用 GPT-2 进行 [更快的文本生成与 TensorFlow 和 XLA](https://huggingface.co/blog/tf-xla-generate) 的博客。

+   一篇关于如何使用 GPT-2 模型 [训练语言模型与 Megatron-LM](https://huggingface.co/blog/megatron-training) 的博客。

+   一篇关于如何 [微调 GPT2 以生成您最喜爱艺术家风格的歌词](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) 的笔记本。🌎

+   一篇关于如何 [微调 GPT2 以生成您最喜爱 Twitter 用户风格的推文](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) 的笔记本。🌎

+   🤗 Hugging Face 课程的 [因果语言建模](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) 章节。

+   [OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel) 在这个 [因果语言建模示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling)、[文本生成示例脚本](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) 和 [笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) 中得到支持。

+   [TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel) 在这个 [因果语言建模示例脚本](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) 和 [笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb) 中得到支持。

+   另请参阅：[因果语言建模任务指南](../tasks/language_modeling)

标记分类

+   有关 [字节对编码标记化](https://huggingface.co/course/en/chapter6/5) 的课程材料。

## OpenAIGPTConfig

### `class transformers.OpenAIGPTConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/configuration_openai.py#L27)

```py
( vocab_size = 40478 n_positions = 512 n_embd = 768 n_layer = 12 n_head = 12 afn = 'gelu' resid_pdrop = 0.1 embd_pdrop = 0.1 attn_pdrop = 0.1 layer_norm_epsilon = 1e-05 initializer_range = 0.02 summary_type = 'cls_index' summary_use_proj = True summary_activation = None summary_proj_to_labels = True summary_first_dropout = 0.1 **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为 40478) — GPT-2 模型的词汇大小。定义了在调用 [OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel) 或 [TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel) 时可以表示的不同标记数量。

+   `n_positions` (`int`, *可选*, 默认为 512) — 此模型可能会使用的最大序列长度。通常设置为较大的值以防万一（例如，512、1024 或 2048）。

+   `n_embd` (`int`, *可选*, 默认为 768) — 嵌入和隐藏状态的维度。

+   `n_layer` (`int`, *可选*, 默认为 12) — Transformer 编码器中的隐藏层数。

+   `n_head` (`int`, *可选*, 默认为 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `afn` (`str` 或 `Callable`, *可选*, 默认为 `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。

+   `resid_pdrop` (`float`, *可选*, 默认为 0.1) — 嵌入、编码器和池化器中所有全连接层的丢弃概率。

+   `embd_pdrop` (`int`, *可选*, 默认为 0.1) — 嵌入的丢弃比率。

+   `attn_pdrop` (`float`, *可选*, 默认为 0.1) — 注意力的丢弃比率。

+   `layer_norm_epsilon` (`float`, *可选*, 默认为1e-05) — 在层归一化层中使用的 epsilon

+   `initializer_range` (`float`, *可选*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `summary_type` (`str`, *可选*, 默认为 `"cls_index"`) — 在进行序列摘要时使用的参数，在模型[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)和[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)中使用。

    必须是以下选项之一：

    +   `"last"`: 获取最后一个标记的隐藏状态（类似于XLNet）。

    +   `"first"`: 获取第一个标记的隐藏状态（类似于BERT）。

    +   `"mean"`: 获取所有标记的隐藏状态的平均值。

    +   `"cls_index"`: 提供一个分类标记位置的张量（类似于GPT/GPT-2）。

    +   `"attn"`: 目前未实现，使用多头注意力。

+   `summary_use_proj` (`bool`, *可选*, 默认为 `True`) — 在进行序列摘要时使用的参数，在模型[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)和[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)中使用。

    是否在向量提取后添加投影。

+   `summary_activation` (`str`, *可选*) — 在进行序列摘要时使用的参数，在模型[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)和[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)中使用。

    将 `"tanh"` 传递给输出以获得双曲正切激活，其他任何值都将导致无激活。

+   `summary_proj_to_labels` (`bool`, *可选*, 默认为 `True`) — 在进行序列摘要时使用的参数，在模型[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)和[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)中使用。

    投影输出应具有 `config.num_labels` 或 `config.hidden_size` 类。

+   `summary_first_dropout` (`float`, *可选*, 默认为0.1) — 在进行序列摘要时使用的参数，在模型[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)和[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)中使用。

    在投影和激活之后使用的辍学比例。

这是用于存储[OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)或[TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel)配置的配置类。它用于根据指定的参数实例化一个GPT模型，定义模型架构。使用默认值实例化配置将产生类似于OpenAI的GPT [openai-gpt](https://huggingface.co/openai-gpt)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import OpenAIGPTConfig, OpenAIGPTModel

>>> # Initializing a GPT configuration
>>> configuration = OpenAIGPTConfig()

>>> # Initializing a model (with random weights) from the configuration
>>> model = OpenAIGPTModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## OpenAIGPTTokenizer

### `class transformers.OpenAIGPTTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L245)

```py
( vocab_file merges_file unk_token = '<unk>' **kwargs )
```

参数

+   `vocab_file` (`str`) — 词汇文件的路径。

+   `merges_file` (`str`) — 合并文件的路径。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）- 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

构建一个GPT分词器。基于字节对编码，具有以下特点：

+   将所有输入转换为小写，

+   如果安装了`SpaCy`分词器和`ftfy`，则使用它们进行BPE标记化，否则回退到BERT的`BasicTokenizer`。

此分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai.py#L378)

```py
( save_directory: str filename_prefix: Optional = None )
```

## OpenAIGPTTokenizerFast

### `class transformers.OpenAIGPTTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/tokenization_openai_fast.py#L40)

```py
( vocab_file = None merges_file = None tokenizer_file = None unk_token = '<unk>' **kwargs )
```

参数

+   `vocab_file`（`str`）- 词汇表文件的路径。

+   `merges_file`（`str`）- 合并文件的路径。

+   `unk_token`（`str`，*可选*，默认为`"<unk>"`）- 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

构建一个“快速”GPT分词器（由HuggingFace的*tokenizers*库支持）。基于字节对编码，具有以下特点：

+   将所有输入转换为小写

+   使用BERT的BasicTokenizer进行BPE标记化

此分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

## OpenAI特定输出

### `class transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L297)

```py
( loss: Optional = None mc_loss: Optional = None logits: FloatTensor = None mc_logits: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）- 语言建模损失。

+   `mc_loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`mc_labels`时返回）- 多项选择分类损失。

+   `logits`（形状为`(batch_size, num_choices, sequence_length, config.vocab_size)`的`torch.FloatTensor`）- 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `mc_logits`（形状为`(batch_size, num_choices)`的`torch.FloatTensor`）- 多项选择分类头的预测分数（SoftMax之前每个选择的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

用于预测两个句子是否连续的模型输出的基类。

### `class transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L413)

```py
( logits: tf.Tensor = None mc_logits: tf.Tensor = None hidden_states: Tuple[tf.Tensor] | None = None attentions: Tuple[tf.Tensor] | None = None )
```

参数

+   `logits`（形状为`(batch_size, num_choices, sequence_length, config.vocab_size)`的`tf.Tensor`）- 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `mc_logits` (`tf.Tensor`，形状为 `(batch_size, num_choices)`) — 多项选择分类头的预测分数（SoftMax 之前每个选择的分数）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组（每层一个）。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

模型输出的基类，用于预测两个句子是否连续。

PytorchHide Pytorch 内容

## OpenAIGPTModel

### `class transformers.OpenAIGPTModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L398)

```py
( config )
```

参数

+   `config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

裸的 OpenAI GPT 变压器模型输出原始隐藏状态，没有特定的头部。

此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以了解一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L428)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   对于未被 `masked` 的标记为 1，

    +   对于被 `masked` 的标记为 0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 指示输入的第一部分和第二部分的段标记索引。索引在 `[0, 1]` 中选择：

    +   0 对应于一个 *句子 A* 标记，

    +   1 对应于一个 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为关联向量，而不是使用模型的内部嵌入查找矩阵，这将很有用。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含各种元素，取决于配置([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))和输入。

+   `last_hidden_state` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 模型最后一层输出的隐藏状态序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    每层输出的模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[OpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, OpenAIGPTModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = OpenAIGPTModel.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## OpenAIGPTLMHeadModel

### `class transformers.OpenAIGPTLMHeadModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L526)

```py
( config )
```

参数

+   `config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

OpenAI GPT模型变压器，顶部带有语言建模头（线性层，其权重与输入嵌入绑定）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库实现的所有模型的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L550)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列令牌的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 段令牌索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*令牌，

    +   1对应于*句子B*令牌。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 每个输入序列令牌在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于语言建模的标签。请注意，模型内部的标签**已经被移位**，即您可以设置`labels = input_ids`。索引在`[-100, 0, ..., config.vocab_size]`中选择。所有设置为`-100`的标签都被忽略（掩码），损失仅计算在`[0, ..., config.vocab_size]`中的标签。

返回

[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回）— 语言建模损失（用于下一个标记预测）。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    每层模型的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[OpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, OpenAIGPTLMHeadModel

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = OpenAIGPTLMHeadModel.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs, labels=inputs["input_ids"])
>>> loss = outputs.loss
>>> logits = outputs.logits
```

## OpenAIGPTDoubleHeadsModel

### `class transformers.OpenAIGPTDoubleHeadsModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L615)

```py
( config )
```

参数

+   `config`（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

OpenAI GPT模型变压器，顶部带有语言建模和多选分类头，例如用于RocStories/SWAG任务。这两个头是两个线性层。语言建模头的权重与输入嵌入绑定，分类头以指定的分类标记索引的输入作为输入序列的输入）。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L644)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None mc_token_ids: Optional = None labels: Optional = None mc_labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被掩盖的标记，

    +   0表示被掩盖的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选择在`[0, 1]`中：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部无效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `mc_token_ids` (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*, 默认为输入的最后一个标记的索引) — 每个输入序列中分类标记的索引。选择范围为`[0, input_ids.size(-1) - 1]`。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于语言建模的标签。请注意，模型内部的标签**已经移位**，即您可以设置`labels = input_ids`。索引选择在`[-1, 0, ..., config.vocab_size]`中。所有设置为`-100`的标签都被忽略（掩盖），损失仅计算在`[0, ..., config.vocab_size]`中的标签。

+   `mc_labels` (`torch.LongTensor` of shape `(batch_size)`, *optional*) — 用于计算多项选择分类损失的标签。索引应在`[0, ..., num_choices]`中，其中*num_choices*是输入张量第二维的大小。（参见上面的*input_ids*）

返回

[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入不同元素。

+   `loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`labels`时返回）— 语言建模损失。

+   `mc_loss`（`torch.FloatTensor`，形状为`(1,)`，*可选*，在提供`mc_labels`时返回）— 多项选择分类损失。

+   `logits`（`torch.FloatTensor`，形状为`(batch_size, num_choices, sequence_length, config.vocab_size)`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `mc_logits`（`torch.FloatTensor`，形状为`(batch_size, num_choices)`）— 多项选择分类头的预测分数（SoftMax之前每个选择的分数）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[OpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTDoubleHeadsModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, OpenAIGPTDoubleHeadsModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = OpenAIGPTDoubleHeadsModel.from_pretrained("openai-gpt")
>>> tokenizer.add_special_tokens(
...     {"cls_token": "[CLS]"}
... )  # Add a [CLS] to the vocabulary (we should train it also!)
>>> model.resize_token_embeddings(len(tokenizer))

>>> choices = ["Hello, my dog is cute [CLS]", "Hello, my cat is cute [CLS]"]
>>> input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices
>>> mc_token_ids = torch.tensor([input_ids.size(-1) - 1, input_ids.size(-1) - 1]).unsqueeze(0)  # Batch size 1

>>> outputs = model(input_ids, mc_token_ids=mc_token_ids)
>>> lm_logits = outputs.logits
>>> mc_logits = outputs.mc_logits
```

## OpenAIGPTForSequenceClassification

### `class transformers.OpenAIGPTForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L740)

```py
( config )
```

参数

+   `config`（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

原始的OpenAI GPT模型变压器，顶部带有序列分类头（线性层）。[OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)使用最后一个标记进行分类，就像其他因果模型（例如GPT-2）一样。由于它在最后一个标记上进行分类，因此需要知道最后一个标记的位置。如果在配置中定义了`pad_token_id`，则找到每行中不是填充标记的最后一个标记。如果未定义`pad_token_id`，则简单地取批次中每行的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充标记，因此它执行相同操作（取批次中每行的最后一个值）。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_openai.py#L762)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)来获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） — 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示“未被掩盖”的标记，

    +   0表示“被掩盖”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*） — 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记。

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*） — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*） — 用于使自注意力模块中选择的头部失效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部“未被掩盖”，

    +   0表示头部“被掩盖”。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*） — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*） — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`中。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig))和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`, *可选的*, 当提供`labels`时返回) — 分类（如果config.num_labels==1则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果config.num_labels==1则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的输出+每层的输出）。

    模型在每一层输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[OpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

单标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = OpenAIGPTForSequenceClassification.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_id = logits.argmax().item()

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = OpenAIGPTForSequenceClassification.from_pretrained("openai-gpt", num_labels=num_labels)

>>> labels = torch.tensor([1])
>>> loss = model(**inputs, labels=labels).loss
```

多标签分类示例：

```py
>>> import torch
>>> from transformers import AutoTokenizer, OpenAIGPTForSequenceClassification

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = OpenAIGPTForSequenceClassification.from_pretrained("openai-gpt", problem_type="multi_label_classification")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]

>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = OpenAIGPTForSequenceClassification.from_pretrained(
...     "openai-gpt", num_labels=num_labels, problem_type="multi_label_classification"
... )

>>> labels = torch.sum(
...     torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1
... ).to(torch.float)
>>> loss = model(**inputs, labels=labels).loss
```

TensorFlow隐藏TensorFlow内容

## TFOpenAIGPTModel

### `class transformers.TFOpenAIGPTModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L540)

```py
( config *inputs **kwargs )
```

参数

+   `config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法来加载模型权重。

裸的OpenAI GPT变换器模型输出原始隐藏状态，没有特定的顶部头。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

第二种格式得到支持的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，在使用诸如`model.fit()`之类的方法时，应该对您“只需工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量在第一个位置参数中：

+   只有一个包含`input_ids`的张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像将输入传递给任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L549)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy array`或`tf.Tensor`） - 词汇表中输入序列令牌的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*） - 用于避免在填充令牌索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*） - 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*令牌，

    +   1对应于*句子B*令牌。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy array`，*可选*） - 每个输入序列令牌在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`或`Numpy array`，*可选*） - 用于使自注意力模块的选定头部无效的掩码。掩码值在`[0, 1]`中选择：

    +   1表示头部未被`masked`，

    +   0表示头部是`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`或`Numpy array`，*可选*） - 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵。

+   `output_attentions`（`bool`，*可选*） - 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。此参数仅在 eager 模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。这个参数可以在 eager 模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`, *可选*，默认为 `False`) — 是否在训练模式下使用模型（一些模块如 dropout 模块在训练和评估之间有不同的行为）。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput) 或 `tuple(tf.Tensor)`

一个 [transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput) 或一个 `tf.Tensor` 元组（如果传入 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入的各种元素。

+   `last_hidden_state` (`tf.Tensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 模型最后一层输出的隐藏状态序列。

+   `hidden_states` (`tuple(tf.FloatTensor)`，*可选*，当传入 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, sequence_length, hidden_size)` 的 `tf.Tensor` 元组。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传入 `output_attentions=True` 或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `tf.Tensor` 元组。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

[TFOpenAIGPTModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFOpenAIGPTModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = TFOpenAIGPTModel.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFOpenAIGPTLMHeadModel

`transformers.TFOpenAIGPTLMHeadModel` 类

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L592)

```py
( config *inputs **kwargs )
```

参数

+   `config` ([OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

带有语言建模头的 OpenAI GPT 模型变换器（线性层，其权重与输入嵌入相关联）。

此模型继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有事项。

`transformers`中的TensorFlow模型和层接受两种输入格式：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras `Functional` API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入Tensor：

+   仅具有`input_ids`的单个Tensor，没有其他内容：`model(input_ids)`

+   一个具有不同长度的列表，其中包含一个或多个按照文档字符串中给定的顺序的输入Tensor：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入Tensor：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L612)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy`数组或`tf.Tensor`）- 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）- 避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示`not masked`的标记，

    +   0表示`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`Numpy`数组，*可选*）- 指示输入的第一部分和第二部分的段标记索引。索引在`[0, 1]`中选择：

    +   0对应于*句子A*的标记，

    +   1对应于*句子B*的标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（`tf.Tensor`或形状为`(batch_size, sequence_length)`的`Numpy`数组，*可选*）- 每个输入序列标记在位置嵌入中的位置的索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`tf.Tensor`或`Numpy`数组，*可选*）- 用于使自注意力模块的选定头部无效的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示头部是`not masked`，

    +   0表示头部是`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`或`Numpy array`，*可选*）— 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为关联向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式中使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

+   `labels`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）— 用于计算交叉熵分类损失的标签。索引应在`[0, ..., config.vocab_size - 1]`范围内。

返回

[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`，则根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入包含各种元素。

+   `loss`（形状为`(n,)`的`tf.Tensor`，*可选*，当提供`labels`时返回，其中n是未屏蔽标签的数量）— 语言建模损失（用于下一个标记预测）。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`）— 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出处的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[TFOpenAIGPTLMHeadModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFOpenAIGPTLMHeadModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = TFOpenAIGPTLMHeadModel.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)
>>> logits = outputs.logits
```

## TFOpenAIGPTDoubleHeadsModel

### `class transformers.TFOpenAIGPTDoubleHeadsModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L685)

```py
( config *inputs **kwargs )
```

参数

+   `config`（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

OpenAI GPT模型变压器，具有语言建模和顶部的多选分类头，例如用于RocStories/SWAG任务。这两个头是两个线性层。语言建模头的权重与输入嵌入绑定，分类头以输入序列中指定分类标记索引的输入为输入）。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)的子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，应该“只需工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras `Functional` API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度不同的列表，其中包含一个或多个输入张量，按照文档字符串中给定的顺序：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像将输入传递给任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L703)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None mc_token_ids: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`Numpy array`或`tf.Tensor`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（`tf.Tensor`或形状为`(batch_size, sequence_length)`的`Numpy array`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`tf.Tensor`或`Numpy array`，形状为`(batch_size, sequence_length)`，*optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于 *sentence A* 标记，

    +   1 对应于 *sentence B* 标记。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `position_ids` (`tf.Tensor`或`Numpy array`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是position IDs？](../glossary#position-ids)

+   `head_mask` (`tf.Tensor`或`Numpy array`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部无效的掩码。选择的掩码值在`[0, 1]`中：

    +   1 表示头部未被`masked`，

    +   0 表示头部被`masked`。

+   `inputs_embeds` (`tf.Tensor`或`Numpy array`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training` (`bool`, *optional*, defaults to `False`) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

+   `mc_token_ids` (`tf.Tensor`或`Numpy array`，形状为`(batch_size, num_choices)`，*optional*，默认为输入序列的最后一个标记的索引) — 每个输入序列中分类标记的索引。在范围`[0, input_ids.size(-1) - 1]`中选择。

返回

[transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput) 或 `tuple(tf.Tensor)`

一个[transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入的不同元素。

+   `logits` (`tf.Tensor`，形状为`(batch_size, num_choices, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `mc_logits` (`tf.Tensor`，形状为`(batch_size, num_choices)`) — 多项选择分类头的预测分数（SoftMax之前每个选择的分数）。

+   `hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFOpenAIGPTDoubleHeadsModel](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTDoubleHeadsModel)的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import tensorflow as tf
>>> from transformers import AutoTokenizer, TFOpenAIGPTDoubleHeadsModel

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = TFOpenAIGPTDoubleHeadsModel.from_pretrained("openai-gpt")

>>> # Add a [CLS] to the vocabulary (we should train it also!)
>>> tokenizer.add_special_tokens({"cls_token": "[CLS]"})
>>> model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size
>>> print(tokenizer.cls_token_id, len(tokenizer))  # The newly token the last token of the vocabulary

>>> choices = ["Hello, my dog is cute [CLS]", "Hello, my cat is cute [CLS]"]
>>> encoding = tokenizer(choices, return_tensors="tf")
>>> inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
>>> inputs["mc_token_ids"] = tf.constant(
...     [inputs["input_ids"].shape[-1] - 1, inputs["input_ids"].shape[-1] - 1]
... )[
...     None, :
... ]  # Batch size 1
>>> outputs = model(inputs)
>>> lm_prediction_scores, mc_prediction_scores = outputs[:2]
```

## TFOpenAIGPTForSequenceClassification

### `class transformers.TFOpenAIGPTForSequenceClassification`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L817)

```py
( config *inputs **kwargs )
```

参数

+   `config`（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）- 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部带有序列分类头的OpenAI GPT模型变压器（线性层）。

[TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification)使用最后一个令牌来进行分类，就像其他因果模型（例如GPT-2）一样。

由于它对最后一个令牌进行分类，因此需要知道最后一个令牌的位置。如果在配置中定义了`pad_token_id`，则它会找到每行中不是填充令牌的最后一个令牌。如果没有定义`pad_token_id`，则它会简单地取每行批次中的最后一个值。由于在传递`inputs_embeds`而不是`input_ids`时无法猜测填充令牌，因此它会执行相同的操作（取每行批次中的最后一个值）。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于这种支持，当使用`model.fit()`等方法时，您应该可以“只需传递”您的输入和标签，以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：

+   只有`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度可变的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_ids, attention_mask])` 或 `model([input_ids, attention_mask, token_type_ids])`

+   一个字典，其中包含与文档字符串中给定的输入名称相关联的一个或多个输入张量：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用 [子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) 创建模型和层时，您无需担心这些内容，因为您可以像对待其他 Python 函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/openai/modeling_tf_openai.py#L845)

```py
( input_ids: TFModelInputType | None = None attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None position_ids: np.ndarray | tf.Tensor | None = None head_mask: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None labels: np.ndarray | tf.Tensor | None = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFSequenceClassifierOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`Numpy array` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入 ID？](../glossary#input-ids)

+   `attention_mask` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在 `[0, 1]`：

    +   1 表示未被 `masked` 的标记，

    +   0 表示被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*) — 段标记索引，指示输入的第一部分和第二部分。索引选定在 `[0, 1]`：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length)`，*可选*) — 每个输入序列标记在位置嵌入中的位置索引。选定范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`tf.Tensor` 或 `Numpy array`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在 `[0, 1]`：

    +   1 表示头部未被 `masked`，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`tf.Tensor` 或 `Numpy array`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的 `attentions`。此参数仅可在急切模式下使用，在图模式中将使用配置中的值。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的 `hidden_states`。此参数仅可在急切模式下使用，在图模式中将使用配置中的值。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。此参数可在急切模式下使用，在图模式中该值将始终设置为 True。

+   `training` (`bool`，*可选*，默认为 `False`) — 是否在训练模式下使用模型（一些模块，如 dropout 模块，在训练和评估之间有不同的行为）。

+   `labels` (`tf.Tensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于计算交叉熵分类损失的标签。索引应在`[0, ..., config.vocab_size - 1]`范围内。

返回

`transformers.modeling_tf_outputs.TFSequenceClassifierOutput` 或 `tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[OpenAIGPTConfig](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig)）和输入而异的各种元素。

+   `loss` (`tf.Tensor`，形状为`(batch_size, )`，*可选*，当提供`labels`时返回） — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`tf.Tensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出以及初始嵌入输出的隐藏状态。

+   `attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFOpenAIGPTForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification) 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFOpenAIGPTForSequenceClassification
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("openai-gpt")
>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained("openai-gpt")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

>>> logits = model(**inputs).logits

>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
```

```py
>>> # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`
>>> num_labels = len(model.config.id2label)
>>> model = TFOpenAIGPTForSequenceClassification.from_pretrained("openai-gpt", num_labels=num_labels)

>>> labels = tf.constant(1)
>>> loss = model(**inputs, labels=labels).loss
```
