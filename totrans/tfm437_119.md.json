["```py\n( params: Iterable lr: float = 0.001 betas: Tuple = (0.9, 0.999) eps: float = 1e-06 weight_decay: float = 0.0 correct_bias: bool = True no_deprecation_warning: bool = False )\n```", "```py\n( closure: Callable = None )\n```", "```py\n( params lr = None eps = (1e-30, 0.001) clip_threshold = 1.0 decay_rate = -0.8 beta1 = None weight_decay = 0.0 scale_parameter = True relative_step = True warmup_init = False )\n```", "```py\nAdafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)\n```", "```py\nAdafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n```", "```py\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\noptimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\nlr_scheduler = AdafactorSchedule(optimizer)\ntrainer = Trainer(..., optimizers=(optimizer, lr_scheduler))\n```", "```py\n# replace AdamW with Adafactor\noptimizer = Adafactor(\n    model.parameters(),\n    lr=1e-3,\n    eps=(1e-30, 1e-3),\n    clip_threshold=1.0,\n    decay_rate=-0.8,\n    beta1=None,\n    weight_decay=0.0,\n    relative_step=False,\n    scale_parameter=False,\n    warmup_init=False,\n)\n```", "```py\n( closure = None )\n```", "```py\n( learning_rate: Union = 0.001 beta_1: float = 0.9 beta_2: float = 0.999 epsilon: float = 1e-07 amsgrad: bool = False weight_decay_rate: float = 0.0 include_in_weight_decay: Optional = None exclude_from_weight_decay: Optional = None name: str = 'AdamWeightDecay' **kwargs )\n```", "```py\n( config )\n```", "```py\n( init_lr: float num_train_steps: int num_warmup_steps: int min_lr_ratio: float = 0.0 adam_beta1: float = 0.9 adam_beta2: float = 0.999 adam_epsilon: float = 1e-08 adam_clipnorm: Optional = None adam_global_clipnorm: Optional = None weight_decay_rate: float = 0.0 power: float = 1.0 include_in_weight_decay: Optional = None )\n```", "```py\n( value names = None module = None qualname = None type = None start = 1 )\n```", "```py\n( name: Union optimizer: Optimizer num_warmup_steps: Optional = None num_training_steps: Optional = None scheduler_specific_kwargs: Optional = None )\n```", "```py\n( optimizer: Optimizer last_epoch: int = -1 )\n```", "```py\n( optimizer: Optimizer num_warmup_steps: int last_epoch: int = -1 )\n```", "```py\n( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles: float = 0.5 last_epoch: int = -1 )\n```", "```py\n( optimizer: Optimizer num_warmup_steps: int num_training_steps: int num_cycles: int = 1 last_epoch: int = -1 )\n```", "```py\n( optimizer num_warmup_steps num_training_steps last_epoch = -1 )\n```", "```py\n( optimizer num_warmup_steps num_training_steps lr_end = 1e-07 power = 1.0 last_epoch = -1 )\n```", "```py\n( optimizer: Optimizer num_warmup_steps: int timescale: int = None last_epoch: int = -1 )\n```", "```py\n( initial_learning_rate: float decay_schedule_fn: Callable warmup_steps: int power: float = 1.0 name: str = None )\n```", "```py\n( )\n```", "```py\n( )\n```"]