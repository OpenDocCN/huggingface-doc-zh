- en: Working with Keras and Tensorflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/keras_integrations](https://huggingface.co/docs/evaluate/keras_integrations)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate can be easily intergrated into your Keras and Tensorflow workflow.
    We’ll demonstrate two ways of incorporating Evaluate into model training, using
    the Fashion MNIST example dataset. We’ll train a standard classifier to predict
    two classes from this dataset, and show how to use a metric as a callback during
    training or afterwards for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we want to keep track of model metrics while a model is training. We
    can use a Callback in order to calculate this metric during training, after an
    epoch ends.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll define a callback here that will take a metric name and our training data,
    and have it calculate a metric after the epoch ends.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can pass this class to the `callbacks` keyword-argument to use it during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using an Evaluate Metric for... Evaluation!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use the same metric after model training! Here, we show how to
    check accuracy of the model after training on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
