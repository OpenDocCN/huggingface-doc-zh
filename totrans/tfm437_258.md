# 卷积视觉Transformer（CvT）

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)

## 概述

CvT模型由吴海平、肖斌、诺尔·科代拉、刘梦晨、戴希扬、袁璐和张磊在[CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)中提出。卷积视觉Transformer（CvT）通过将卷积引入ViT中，提高了[视觉Transformer（ViT）](vit)的性能和效率，以获得这两种设计的最佳效果。

论文摘要如下：

*我们在本文中提出了一种名为卷积视觉Transformer（CvT）的新架构，通过将卷积引入ViT中，提高了ViT的性能和效率，以获得这两种设计的最佳效果。这通过两个主要修改实现：包含新的卷积标记嵌入的Transformer层次结构，以及利用卷积投影的卷积Transformer块。这些改变将卷积神经网络（CNN）的理想特性引入ViT架构（即平移、缩放和失真不变性），同时保持Transformer的优点（即动态注意力、全局上下文和更好的泛化）。我们通过进行广泛实验验证了CvT，显示这种方法在ImageNet-1k上实现了其他视觉Transformer和ResNet的最新性能，参数更少，FLOPs更低。此外，当在更大的数据集（例如ImageNet-22k）上进行预训练并微调到下游任务时，性能增益得以保持。在ImageNet-22k上预训练，我们的CvT-W24在ImageNet-1k验证集上获得了87.7\%的top-1准确率。最后，我们的结果表明，位置编码，现有视觉Transformer中的关键组件，可以在我们的模型中安全地移除，简化了更高分辨率视觉任务的设计。*

此模型由[anugunj](https://huggingface.co/anugunj)贡献。原始代码可以在[这里](https://github.com/microsoft/CvT)找到。

## 使用提示

+   CvT模型是常规的视觉Transformer，但是经过卷积训练。当在ImageNet-1K和CIFAR-100上进行微调时，它们的性能优于[原始模型（ViT）](vit)。

+   您可以查看关于推理以及在自定义数据上进行微调的演示笔记本[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)（您只需将[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)替换为[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)，将[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)替换为[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)）。

+   可用的检查点要么（1）仅在[ImageNet-22k](http://www.image-net.org/)（包含1400万图像和22k类别）上进行预训练，要么（2）在ImageNet-22k上进行微调，要么（3）在[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)（也称为ILSVRC 2012，包含130万图像和1000类别）上进行微调。

## 资源

一份官方Hugging Face和社区（由🌎表示）资源列表，帮助您开始使用CvT。

图像分类

+   [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)支持。

+   另请参阅：[图像分类任务指南](../tasks/image_classification)

如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将进行审查！资源应该展示一些新内容，而不是重复现有资源。

## CvtConfig

### `class transformers.CvtConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)

```py
( num_channels = 3 patch_sizes = [7, 3, 3] patch_stride = [4, 2, 2] patch_padding = [2, 1, 1] embed_dim = [64, 192, 384] num_heads = [1, 3, 6] depth = [1, 2, 10] mlp_ratio = [4.0, 4.0, 4.0] attention_drop_rate = [0.0, 0.0, 0.0] drop_rate = [0.0, 0.0, 0.0] drop_path_rate = [0.0, 0.0, 0.1] qkv_bias = [True, True, True] cls_token = [False, False, True] qkv_projection_method = ['dw_bn', 'dw_bn', 'dw_bn'] kernel_qkv = [3, 3, 3] padding_kv = [1, 1, 1] stride_kv = [2, 2, 2] padding_q = [1, 1, 1] stride_q = [1, 1, 1] initializer_range = 0.02 layer_norm_eps = 1e-12 **kwargs )
```

参数

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道数

+   `patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) — 每个编码器块的补丁嵌入的内核大小

+   `patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) — 每个编码器块的补丁嵌入的步幅大小

+   `patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — 每个编码器块的补丁嵌入的填充大小

+   `embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) — 每个编码器块的维度

+   `num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) — 每个Transformer编码器块中每个注意力层的注意力头数。

+   `depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) — 每个编码器块中的层数

+   `mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`) — 在编码器块中Mix FFN的隐藏层大小与输入层大小的比率

+   `attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) — 注意力概率的丢弃比率

+   `drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) — 补丁嵌入概率的丢弃比率

+   `drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`) — 随机深度的丢弃概率，用于Transformer编码器块中

+   `qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) — 查询、键和值的注意力中的偏置布尔值

+   `cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`) — 是否向每个最后3个阶段的输出添加分类令牌

+   `qkv_projection_method` (`List[string]`, *optional*, defaults to [“dw_bn”, “dw_bn”, “dw_bn”]`) — 查询、键和值的投影方法，默认为深度卷积和批量归一化。使用“avg”进行线性投影。

+   `kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) — 注意力层中查询、键和值的内核大小

+   `padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 注意力层中键和值的填充大小

+   `stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — 注意力层中键和值的步幅大小

+   `padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 注意力层中查询的填充大小

+   `stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) — 查询在注意力层中的步幅大小

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-6) — 层归一化层使用的epsilon

这是用于存储[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)配置的配置类。它用于根据指定的参数实例化CvT模型，定义模型架构。使用默认值实例化配置将产生类似于CvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import CvtConfig, CvtModel

>>> # Initializing a Cvt msft/cvt style configuration
>>> configuration = CvtConfig()

>>> # Initializing a model (with random weights) from the msft/cvt style configuration
>>> model = CvtModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorch内容

## CvtModel

### `class transformers.CvtModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)

```py
( config add_pooling_layer = True )
```

参数

+   `config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸Cvt模型变压器输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅`CvtImageProcessor.__call__`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`或`tuple(torch.FloatTensor)`

一个`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的隐藏状态序列。

+   `cls_token_value`（形状为`(batch_size, 1, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的分类令牌。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每一层的输出的隐藏状态 + 初始嵌入输出。

[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, CvtModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = CvtModel.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 384, 14, 14]
```

## CvtForImageClassification

### `class transformers.CvtForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)

```py
( config )
```

参数

+   `config` ([CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

带有图像分类头部的 Cvt 模型变压器（在 [CLS] 标记的最终隐藏状态之上的线性层），例如用于 ImageNet。

此模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `前进`

[< 源代码 >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

参数

+   `pixel_values` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`） — 像素值。可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoImageProcessor) 获取像素值。有关详细信息，请参阅 `CvtImageProcessor.__call__`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算图像分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回结果

[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回) — 分类（如果 config.num_labels==1 则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, config.num_labels)`） — 分类（如果 config.num_labels==1 则为回归）分数（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递了 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, num_channels, height, width)` 的 `torch.FloatTensor` 元组。模型在每个阶段输出的隐藏状态（也称为特征图）。

[CvtForImageClassification](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtForImageClassification) 前进方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, CvtForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = CvtForImageClassification.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

TensorFlow 隐藏 TensorFlow 内容

## TFCvtModel

### `类 transformers.TFCvtModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)

```py
( config: CvtConfig *inputs **kwargs )
```

参数

+   `config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) 方法以加载模型权重。

裸 Cvt 模型变压器输出原始隐藏状态，没有特定的顶部头。

此模型继承自 [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个 [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 子类。将其用作常规的 TF 2.0 Keras 模型，并参考 TF 2.0 文档以获取有关一般用法和行为的所有相关信息。

TF 2.0 模型接受两种格式的输入：

+   将所有输入作为关键字参数（如 PyTorch 模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

当使用 `tf.keras.Model.fit` 方法时，第二个选项很有用，该方法当前要求在模型调用函数的第一个参数中具有所有张量：`model(inputs)`。

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)

```py
( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]` 或 `Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用 [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) 获取。有关详细信息，请参阅 `CvtImageProcessor.__call__`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（某些模块如丢弃模块在训练和评估之间有不同的行为）。

返回

`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` 或 `tuple(tf.Tensor)`

一个 `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` 或一个 `tf.Tensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 模型最后一层的隐藏状态序列。

+   `cls_token_value`（形状为`(batch_size, 1, hidden_size)`的`tf.Tensor`）— 模型最后一层的分类标记。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的 `tf.Tensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。模型在每一层的输出隐藏状态加上初始嵌入输出。

[TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFCvtModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = TFCvtModel.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(images=image, return_tensors="tf")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## TFCvtForImageClassification

### `class transformers.TFCvtForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)

```py
( config: CvtConfig *inputs **kwargs )
```

参数

+   `config`（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)方法以加载模型权重。

Cvt模型变压器，顶部带有图像分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于ImageNet。

此模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有事项。

TF 2.0模型接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

当使用`tf.keras.Model.fit`方法时，第二个选项很有用，该方法当前要求在模型调用函数的第一个参数中具有所有张量：`model(inputs)`。

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)

```py
( pixel_values: tf.Tensor | None = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)
```

参数

+   `pixel_values`（`np.ndarray`、`tf.Tensor`、`List[tf.Tensor]`、`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`(batch_size, num_channels, height, width)`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参见`CvtImageProcessor.__call__`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`）— 是否在训练模式下使用模型（某些模块如dropout模块在训练和评估之间具有不同的行为）。

+   `labels`（形状为`(batch_size,)`的`tf.Tensor`或`np.ndarray`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`或`tuple(tf.Tensor)`

`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` 或者一个 `tf.Tensor` 元组（如果传入了 `return_dict=False` 或者 `config.return_dict=False`）包含不同的元素，取决于配置（[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)）和输入。

+   `loss`（形状为 `(1,)` 的 `tf.Tensor`，*可选*，当提供了 `labels` 时返回）— 分类（如果 `config.num_labels==1` 则为回归）损失。

+   `logits`（形状为 `(batch_size, config.num_labels)` 的 `tf.Tensor`）— 分类（如果 `config.num_labels==1` 则为回归）得分（SoftMax 之前）。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传入 `output_hidden_states=True` 或者 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, num_channels, height, width)` 的 `tf.Tensor` 元组。模型在每个阶段输出的隐藏状态（也称为特征图）。

[TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification) 的前向方法，重写了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个函数，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, TFCvtForImageClassification
>>> import tensorflow as tf
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = TFCvtForImageClassification.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(images=image, return_tensors="tf")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]
>>> print("Predicted class:", model.config.id2label[int(predicted_class_idx)])
```
