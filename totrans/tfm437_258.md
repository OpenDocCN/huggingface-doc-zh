# å·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/cvt)

## æ¦‚è¿°

CvTæ¨¡å‹ç”±å´æµ·å¹³ã€è‚–æ–Œã€è¯ºå°”Â·ç§‘ä»£æ‹‰ã€åˆ˜æ¢¦æ™¨ã€æˆ´å¸Œæ‰¬ã€è¢ç’å’Œå¼ ç£Šåœ¨[CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808)ä¸­æå‡ºã€‚å·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰é€šè¿‡å°†å·ç§¯å¼•å…¥ViTä¸­ï¼Œæé«˜äº†[è§†è§‰Transformerï¼ˆViTï¼‰](vit)çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥è·å¾—è¿™ä¸¤ç§è®¾è®¡çš„æœ€ä½³æ•ˆæœã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§åä¸ºå·ç§¯è§†è§‰Transformerï¼ˆCvTï¼‰çš„æ–°æ¶æ„ï¼Œé€šè¿‡å°†å·ç§¯å¼•å…¥ViTä¸­ï¼Œæé«˜äº†ViTçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥è·å¾—è¿™ä¸¤ç§è®¾è®¡çš„æœ€ä½³æ•ˆæœã€‚è¿™é€šè¿‡ä¸¤ä¸ªä¸»è¦ä¿®æ”¹å®ç°ï¼šåŒ…å«æ–°çš„å·ç§¯æ ‡è®°åµŒå…¥çš„Transformerå±‚æ¬¡ç»“æ„ï¼Œä»¥åŠåˆ©ç”¨å·ç§¯æŠ•å½±çš„å·ç§¯Transformerå—ã€‚è¿™äº›æ”¹å˜å°†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ç†æƒ³ç‰¹æ€§å¼•å…¥ViTæ¶æ„ï¼ˆå³å¹³ç§»ã€ç¼©æ”¾å’Œå¤±çœŸä¸å˜æ€§ï¼‰ï¼ŒåŒæ—¶ä¿æŒTransformerçš„ä¼˜ç‚¹ï¼ˆå³åŠ¨æ€æ³¨æ„åŠ›ã€å…¨å±€ä¸Šä¸‹æ–‡å’Œæ›´å¥½çš„æ³›åŒ–ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡è¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯äº†CvTï¼Œæ˜¾ç¤ºè¿™ç§æ–¹æ³•åœ¨ImageNet-1kä¸Šå®ç°äº†å…¶ä»–è§†è§‰Transformerå’ŒResNetçš„æœ€æ–°æ€§èƒ½ï¼Œå‚æ•°æ›´å°‘ï¼ŒFLOPsæ›´ä½ã€‚æ­¤å¤–ï¼Œå½“åœ¨æ›´å¤§çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ImageNet-22kï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒå¹¶å¾®è°ƒåˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œæ€§èƒ½å¢ç›Šå¾—ä»¥ä¿æŒã€‚åœ¨ImageNet-22kä¸Šé¢„è®­ç»ƒï¼Œæˆ‘ä»¬çš„CvT-W24åœ¨ImageNet-1kéªŒè¯é›†ä¸Šè·å¾—äº†87.7\%çš„top-1å‡†ç¡®ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½ç½®ç¼–ç ï¼Œç°æœ‰è§†è§‰Transformerä¸­çš„å…³é”®ç»„ä»¶ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­å®‰å…¨åœ°ç§»é™¤ï¼Œç®€åŒ–äº†æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ä»»åŠ¡çš„è®¾è®¡ã€‚*

æ­¤æ¨¡å‹ç”±[anugunj](https://huggingface.co/anugunj)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/microsoft/CvT)æ‰¾åˆ°ã€‚

## ä½¿ç”¨æç¤º

+   CvTæ¨¡å‹æ˜¯å¸¸è§„çš„è§†è§‰Transformerï¼Œä½†æ˜¯ç»è¿‡å·ç§¯è®­ç»ƒã€‚å½“åœ¨ImageNet-1Kå’ŒCIFAR-100ä¸Šè¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼˜äº[åŸå§‹æ¨¡å‹ï¼ˆViTï¼‰](vit)ã€‚

+   æ‚¨å¯ä»¥æŸ¥çœ‹å…³äºæ¨ç†ä»¥åŠåœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„æ¼”ç¤ºç¬”è®°æœ¬[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)ï¼ˆæ‚¨åªéœ€å°†[ViTFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTFeatureExtractor)æ›¿æ¢ä¸º[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)ï¼Œå°†[ViTForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit#transformers.ViTForImageClassification)æ›¿æ¢ä¸º[CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)ï¼‰ã€‚

+   å¯ç”¨çš„æ£€æŸ¥ç‚¹è¦ä¹ˆï¼ˆ1ï¼‰ä»…åœ¨[ImageNet-22k](http://www.image-net.org/)ï¼ˆåŒ…å«1400ä¸‡å›¾åƒå’Œ22kç±»åˆ«ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¦ä¹ˆï¼ˆ2ï¼‰åœ¨ImageNet-22kä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¦ä¹ˆï¼ˆ3ï¼‰åœ¨[ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/)ï¼ˆä¹Ÿç§°ä¸ºILSVRC 2012ï¼ŒåŒ…å«130ä¸‡å›¾åƒå’Œ1000ç±»åˆ«ï¼‰ä¸Šè¿›è¡Œå¾®è°ƒã€‚

## èµ„æº

ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨CvTã€‚

å›¾åƒåˆ†ç±»

+   [CvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtForImageClassification)ç”±è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)æ”¯æŒã€‚

+   å¦è¯·å‚é˜…ï¼š[å›¾åƒåˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/image_classification)

å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€ä¸€ä¸ªPull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

## CvtConfig

### `class transformers.CvtConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/configuration_cvt.py#L29)

```py
( num_channels = 3 patch_sizes = [7, 3, 3] patch_stride = [4, 2, 2] patch_padding = [2, 1, 1] embed_dim = [64, 192, 384] num_heads = [1, 3, 6] depth = [1, 2, 10] mlp_ratio = [4.0, 4.0, 4.0] attention_drop_rate = [0.0, 0.0, 0.0] drop_rate = [0.0, 0.0, 0.0] drop_path_rate = [0.0, 0.0, 0.1] qkv_bias = [True, True, True] cls_token = [False, False, True] qkv_projection_method = ['dw_bn', 'dw_bn', 'dw_bn'] kernel_qkv = [3, 3, 3] padding_kv = [1, 1, 1] stride_kv = [2, 2, 2] padding_q = [1, 1, 1] stride_q = [1, 1, 1] initializer_range = 0.02 layer_norm_eps = 1e-12 **kwargs )
```

å‚æ•°

+   `num_channels` (`int`, *optional*, defaults to 3) â€” è¾“å…¥é€šé“æ•°

+   `patch_sizes` (`List[int]`, *optional*, defaults to `[7, 3, 3]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„å†…æ ¸å¤§å°

+   `patch_stride` (`List[int]`, *optional*, defaults to `[4, 2, 2]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„æ­¥å¹…å¤§å°

+   `patch_padding` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„è¡¥ä¸åµŒå…¥çš„å¡«å……å¤§å°

+   `embed_dim` (`List[int]`, *optional*, defaults to `[64, 192, 384]`) â€” æ¯ä¸ªç¼–ç å™¨å—çš„ç»´åº¦

+   `num_heads` (`List[int]`, *optional*, defaults to `[1, 3, 6]`) â€” æ¯ä¸ªTransformerç¼–ç å™¨å—ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `depth` (`List[int]`, *optional*, defaults to `[1, 2, 10]`) â€” æ¯ä¸ªç¼–ç å™¨å—ä¸­çš„å±‚æ•°

+   `mlp_ratios` (`List[float]`, *optional*, defaults to `[4.0, 4.0, 4.0, 4.0]`) â€” åœ¨ç¼–ç å™¨å—ä¸­Mix FFNçš„éšè—å±‚å¤§å°ä¸è¾“å…¥å±‚å¤§å°çš„æ¯”ç‡

+   `attention_drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡

+   `drop_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.0]`) â€” è¡¥ä¸åµŒå…¥æ¦‚ç‡çš„ä¸¢å¼ƒæ¯”ç‡

+   `drop_path_rate` (`List[float]`, *optional*, defaults to `[0.0, 0.0, 0.1]`) â€” éšæœºæ·±åº¦çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œç”¨äºTransformerç¼–ç å™¨å—ä¸­

+   `qkv_bias` (`List[bool]`, *optional*, defaults to `[True, True, True]`) â€” æŸ¥è¯¢ã€é”®å’Œå€¼çš„æ³¨æ„åŠ›ä¸­çš„åç½®å¸ƒå°”å€¼

+   `cls_token` (`List[bool]`, *optional*, defaults to `[False, False, True]`) â€” æ˜¯å¦å‘æ¯ä¸ªæœ€å3ä¸ªé˜¶æ®µçš„è¾“å‡ºæ·»åŠ åˆ†ç±»ä»¤ç‰Œ

+   `qkv_projection_method` (`List[string]`, *optional*, defaults to [â€œdw_bnâ€, â€œdw_bnâ€, â€œdw_bnâ€]`) â€” æŸ¥è¯¢ã€é”®å’Œå€¼çš„æŠ•å½±æ–¹æ³•ï¼Œé»˜è®¤ä¸ºæ·±åº¦å·ç§¯å’Œæ‰¹é‡å½’ä¸€åŒ–ã€‚ä½¿ç”¨â€œavgâ€è¿›è¡Œçº¿æ€§æŠ•å½±ã€‚

+   `kernel_qkv` (`List[int]`, *optional*, defaults to `[3, 3, 3]`) â€” æ³¨æ„åŠ›å±‚ä¸­æŸ¥è¯¢ã€é”®å’Œå€¼çš„å†…æ ¸å¤§å°

+   `padding_kv` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æ³¨æ„åŠ›å±‚ä¸­é”®å’Œå€¼çš„å¡«å……å¤§å°

+   `stride_kv` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) â€” æ³¨æ„åŠ›å±‚ä¸­é”®å’Œå€¼çš„æ­¥å¹…å¤§å°

+   `padding_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æ³¨æ„åŠ›å±‚ä¸­æŸ¥è¯¢çš„å¡«å……å¤§å°

+   `stride_q` (`List[int]`, *optional*, defaults to `[1, 1, 1]`) â€” æŸ¥è¯¢åœ¨æ³¨æ„åŠ›å±‚ä¸­çš„æ­¥å¹…å¤§å°

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-6) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilon

è¿™æ˜¯ç”¨äºå­˜å‚¨[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–CvTæ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äºCvT [microsoft/cvt-13](https://huggingface.co/microsoft/cvt-13)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import CvtConfig, CvtModel

>>> # Initializing a Cvt msft/cvt style configuration
>>> configuration = CvtConfig()

>>> # Initializing a model (with random weights) from the msft/cvt style configuration
>>> model = CvtModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

PytorchHide Pytorchå†…å®¹

## CvtModel

### `class transformers.CvtModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L586)

```py
( config add_pooling_layer = True )
```

å‚æ•°

+   `config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸Cvtæ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L605)

```py
( pixel_values: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰- åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…`CvtImageProcessor.__call__`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `cls_token_value`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„`torch.FloatTensor`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„åˆ†ç±»ä»¤ç‰Œã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€ + åˆå§‹åµŒå…¥è¾“å‡ºã€‚

[CvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, CvtModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = CvtModel.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 384, 14, 14]
```

## CvtForImageClassification

### `class transformers.CvtForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L644)

```py
( config )
```

å‚æ•°

+   `config` ([CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´éƒ¨çš„ Cvt æ¨¡å‹å˜å‹å™¨ï¼ˆåœ¨ [CLS] æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äº ImageNetã€‚

æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `å‰è¿›`

[< æºä»£ç  >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_cvt.py#L666)

```py
( pixel_values: Optional = None labels: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)`ï¼‰ â€” åƒç´ å€¼ã€‚å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/zh/model_doc/auto#transformers.AutoImageProcessor) è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… `CvtImageProcessor.__call__`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›ç»“æœ

[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.ImageClassifierOutputWithNoAttention](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.num_labels)`ï¼‰ â€” åˆ†ç±»ï¼ˆå¦‚æœ config.num_labels==1 åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’äº† `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

[CvtForImageClassification](/docs/transformers/v4.37.2/zh/model_doc/cvt#transformers.CvtForImageClassification) å‰è¿›æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, CvtForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = CvtForImageClassification.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```

TensorFlow éšè— TensorFlow å†…å®¹

## TFCvtModel

### `ç±» transformers.TFCvtModel`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L926)

```py
( config: CvtConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ Cvt æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ TF 2.0 Keras æ¨¡å‹ï¼Œå¹¶å‚è€ƒ TF 2.0 æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

TF 2.0 æ¨¡å‹æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆå¦‚ PyTorch æ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

å½“ä½¿ç”¨ `tf.keras.Model.fit` æ–¹æ³•æ—¶ï¼Œç¬¬äºŒä¸ªé€‰é¡¹å¾ˆæœ‰ç”¨ï¼Œè¯¥æ–¹æ³•å½“å‰è¦æ±‚åœ¨æ¨¡å‹è°ƒç”¨å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸­å…·æœ‰æ‰€æœ‰å¼ é‡ï¼š`model(inputs)`ã€‚

#### `call`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L936)

```py
( pixel_values: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken or tuple(tf.Tensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]` æˆ– `Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨ [AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor) è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… `CvtImageProcessor.__call__`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›ä¸€ä¸ª [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯ä»¥åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸º Trueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—å¦‚ä¸¢å¼ƒæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

è¿”å›

`transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª `transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken` æˆ–ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `cls_token_value`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, 1, hidden_size)`çš„`tf.Tensor`ï¼‰â€” æ¨¡å‹æœ€åä¸€å±‚çš„åˆ†ç±»æ ‡è®°ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„ `tf.Tensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºéšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

[TFCvtModel](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtModel)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, TFCvtModel
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = TFCvtModel.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(images=image, return_tensors="tf")
>>> outputs = model(**inputs)
>>> last_hidden_states = outputs.last_hidden_state
```

## TFCvtForImageClassification

### `class transformers.TFCvtForImageClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L995)

```py
( config: CvtConfig *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Cvtæ¨¡å‹å˜å‹å™¨ï¼Œé¡¶éƒ¨å¸¦æœ‰å›¾åƒåˆ†ç±»å¤´ï¼ˆåœ¨[CLS]æ ‡è®°çš„æœ€ç»ˆéšè—çŠ¶æ€ä¹‹ä¸Šçš„çº¿æ€§å±‚ï¼‰ï¼Œä¾‹å¦‚ç”¨äºImageNetã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF 2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

TF 2.0æ¨¡å‹æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

å½“ä½¿ç”¨`tf.keras.Model.fit`æ–¹æ³•æ—¶ï¼Œç¬¬äºŒä¸ªé€‰é¡¹å¾ˆæœ‰ç”¨ï¼Œè¯¥æ–¹æ³•å½“å‰è¦æ±‚åœ¨æ¨¡å‹è°ƒç”¨å‡½æ•°çš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸­å…·æœ‰æ‰€æœ‰å¼ é‡ï¼š`model(inputs)`ã€‚

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/cvt/modeling_tf_cvt.py#L1021)

```py
( pixel_values: tf.Tensor | None = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆ`np.ndarray`ã€`tf.Tensor`ã€`List[tf.Tensor]`ã€`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`(batch_size, num_channels, height, width)`ï¼‰â€” åƒç´ å€¼ã€‚åƒç´ å€¼å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§`CvtImageProcessor.__call__`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—å›¾åƒåˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention`æˆ–`tuple(tf.Tensor)`

`transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention` æˆ–è€…ä¸€ä¸ª `tf.Tensor` å…ƒç»„ï¼ˆå¦‚æœä¼ å…¥äº† `return_dict=False` æˆ–è€… `config.return_dict=False`ï¼‰åŒ…å«ä¸åŒçš„å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ[CvtConfig](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.CvtConfig)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º `(1,)` çš„ `tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›äº† `labels` æ—¶è¿”å›ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, config.num_labels)` çš„ `tf.Tensor`ï¼‰â€” åˆ†ç±»ï¼ˆå¦‚æœ `config.num_labels==1` åˆ™ä¸ºå›å½’ï¼‰å¾—åˆ†ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ å…¥ `output_hidden_states=True` æˆ–è€… `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_channels, height, width)` çš„ `tf.Tensor` å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

[TFCvtForImageClassification](/docs/transformers/v4.37.2/en/model_doc/cvt#transformers.TFCvtForImageClassification) çš„å‰å‘æ–¹æ³•ï¼Œé‡å†™äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoImageProcessor, TFCvtForImageClassification
>>> import tensorflow as tf
>>> from PIL import Image
>>> import requests

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/cvt-13")
>>> model = TFCvtForImageClassification.from_pretrained("microsoft/cvt-13")

>>> inputs = image_processor(images=image, return_tensors="tf")
>>> outputs = model(**inputs)
>>> logits = outputs.logits
>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_class_idx = tf.math.argmax(logits, axis=-1)[0]
>>> print("Predicted class:", model.config.id2label[int(predicted_class_idx)])
```
