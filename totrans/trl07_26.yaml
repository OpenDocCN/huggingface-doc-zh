- en: Learning Tools (Experimental 🧪)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习工具（实验性 🧪）
- en: 'Original text: [https://huggingface.co/docs/trl/learning_tools](https://huggingface.co/docs/trl/learning_tools)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/trl/learning_tools](https://huggingface.co/docs/trl/learning_tools)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Using Large Language Models (LLMs) with tools has been a popular topic recently
    with awesome works such as [ToolFormer](https://arxiv.org/abs/2302.04761) and
    [ToolBench](https://arxiv.org/pdf/2305.16504.pdf). In TRL, we provide a simple
    example of how to teach LLM to use tools with reinforcement learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，使用大型语言模型（LLMs）与工具的话题备受关注，如[ToolFormer](https://arxiv.org/abs/2302.04761)和[ToolBench](https://arxiv.org/pdf/2305.16504.pdf)等出色的作品。在TRL中，我们提供了一个简单的示例，展示如何使用强化学习教授LLM使用工具。
- en: 'Here’s an overview of the scripts in the [trl repository](https://github.com/lvwerra/trl/tree/main/examples/research_projects/tools):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是[trl存储库](https://github.com/lvwerra/trl/tree/main/examples/research_projects/tools)中脚本的概述：
- en: '| File | Description |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 文件 | 描述 |'
- en: '| --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [`calculator.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/calculator.py)
    | Script to train LLM to use a calculator with reinforcement learning. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| [`calculator.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/calculator.py)
    | 用于训练LLM使用强化学习使用计算器的脚本。 |'
- en: '| [`triviaqa.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/triviaqa.py)
    | Script to train LLM to use a wiki tool to answer questions. |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| [`triviaqa.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/triviaqa.py)
    | 用于训练LLM使用维基工具回答问题的脚本。 |'
- en: '| [`python_interpreter.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/python_interpreter.py)
    | Script to train LLM to use python interpreter to solve math puzzles. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| [`python_interpreter.py`](https://github.com/lvwerra/trl/blob/main/examples/research_projects/tools/python_interpreter.py)
    | 用于训练LLM使用Python解释器解决数学难题的脚本。 |'
- en: Note that the scripts above rely heavily on the `TextEnvironment` API which
    is still under active development. The API may change in the future. Please see
    [`TextEnvironment`](text_environment) for the related docs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述脚本严重依赖于仍在积极开发中的`TextEnvironment` API。API可能会在未来发生变化。请参阅[`TextEnvironment`](text_environment)以获取相关文档。
- en: Learning to Use a Calculator
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习使用计算器
- en: 'The rough idea is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大致想法如下：
- en: 'Load a tool such as [ybelkada/simple-calculator](https://huggingface.co/spaces/ybelkada/simple-calculator)
    that parse a text calculation like `"14 + 34"` and return the calulated number:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个工具，比如[ybelkada/simple-calculator](https://huggingface.co/spaces/ybelkada/simple-calculator)，解析文本计算如`"14
    + 34"`并返回计算后的数字：
- en: '[PRE0]'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define a reward function that returns a positive reward if the tool returns
    the correct answer. In the script we create a dummy reward function like `reward_fn
    = lambda x: 1`, but we override the rewards directly later.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '定义一个奖励函数，如果工具返回正确答案则返回正奖励。在脚本中，我们创建了一个虚拟的奖励函数，如`reward_fn = lambda x: 1`，但我们稍后直接覆盖了奖励。'
- en: Create a prompt on how to use the tools
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建如何使用工具的提示
- en: '[PRE1]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Create a `trl.TextEnvironment` with the model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型创建一个`trl.TextEnvironment`
- en: '[PRE2]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then generate some data such as `tasks = ["\n\nWhat is 13.1-3?", "\n\nWhat is
    4*3?"]` and run the environment with `queries, responses, masks, rewards, histories
    = env.run(tasks)`. The environment will look for the `<call>` token in the prompt
    and append the tool output to the response; it will also return the mask associated
    with the response. You can further use the `histories` to visualize the interaction
    between the model and the tool; `histories[0].show_text()` will show the text
    with color-coded tool output and `histories[0].show_tokens(tokenizer)` will show
    visualize the tokens. ![](../Images/e9b6e7b4e250b332fcbd9626573e3f25.png)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后生成一些数据，如`tasks = ["\n\nWhat is 13.1-3?", "\n\nWhat is 4*3?"]`，并使用`queries,
    responses, masks, rewards, histories = env.run(tasks)`运行环境。环境将查找提示中的`<call>`标记，并将工具输出附加到响应中；它还将返回与响应相关的掩码。您可以进一步使用`histories`来可视化模型和工具之间的交互；`histories[0].show_text()`将显示带有颜色编码工具输出的文本，`histories[0].show_tokens(tokenizer)`将显示可视化的标记。![](../Images/e9b6e7b4e250b332fcbd9626573e3f25.png)
- en: Finally, we can train the model with `train_stats = ppo_trainer.step(queries,
    responses, rewards, masks)`. The trainer will use the mask to ignore the tool
    output when computing the loss, make sure to pass that argument to `step`.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`train_stats = ppo_trainer.step(queries, responses, rewards, masks)`来训练模型。训练器将使用掩码在计算损失时忽略工具输出，请确保将该参数传递给`step`。
- en: Experiment results
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验结果
- en: We trained a model with the above script for 10 random seeds. You can reproduce
    the run with the following command. Feel free to remove the `--slurm-*` arguments
    if you don’t have access to a slurm cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述脚本对模型进行了10个随机种子的训练。您可以使用以下命令重现运行。如果您无法访问slurm集群，请随时删除`--slurm-*`参数。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can then use [`openrlbenchmark`](https://github.com/openrlbenchmark/openrlbenchmark)
    which generates the following plot.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用[`openrlbenchmark`](https://github.com/openrlbenchmark/openrlbenchmark)生成以下图表。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/6cbe96fa603fe61845da0a4c0fbf4641.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cbe96fa603fe61845da0a4c0fbf4641.png)'
- en: As we can see, while 1-2 experiments crashed for some reason, most of the runs
    obtained near perfect proficiency in the calculator task.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，虽然有1-2次实验由于某种原因崩溃，但大多数运行在计算器任务中获得了接近完美的熟练度。
- en: '(Early Experiments 🧪): learning to use a wiki tool for question answering'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: （早期实验 🧪）：学习使用维基工具回答问题
- en: In the [ToolFormer](https://arxiv.org/abs/2302.04761) paper, it shows an interesting
    use case that utilizes a Wikipedia Search tool to help answer questions. In this
    section, we attempt to perform similar experiments but uses RL instead to teach
    the model to use a wiki tool on the [TriviaQA](https://nlp.cs.washington.edu/triviaqa/)
    dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ToolFormer](https://arxiv.org/abs/2302.04761)论文中，展示了一个有趣的用例，利用维基百科搜索工具来帮助回答问题。在这一部分，我们尝试进行类似的实验，但是使用RL来教授模型如何在[TriviaQA](https://nlp.cs.washington.edu/triviaqa/)数据集上使用维基工具。
- en: '**Note that many settings are different so the results are not directly comparable.**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**请注意，许多设置不同，因此结果不能直接进行比较。**'
- en: Building a search index
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建搜索索引
- en: Since [ToolFormer](https://arxiv.org/abs/2302.04761) did not open source, we
    needed to first replicate the search index. It is mentioned in their paper that
    the authors built the search index using a BM25 retriever that indexes the Wikipedia
    dump from [KILT](https://github.com/facebookresearch/KILT)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[ToolFormer](https://arxiv.org/abs/2302.04761)没有开源，我们需要首先复制搜索索引。在他们的论文中提到，作者使用BM25检索器构建了搜索索引，该检索器从[KILT](https://github.com/facebookresearch/KILT)的维基百科转储中检索
- en: Fortunately, [`pyserini`](https://github.com/castorini/pyserini) already implements
    the BM25 retriever and provides a prebuilt index for the KILT Wikipedia dump.
    We can use the following code to search the index.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，[`pyserini`](https://github.com/castorini/pyserini)已经实现了BM25检索器，并为KILT维基百科转储提供了预构建的索引。我们可以使用以下代码搜索索引。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We then basically deployed this snippet as a Hugging Face space [here](https://huggingface.co/spaces/vwxyzjn/pyserini-wikipedia-kilt-doc),
    so that we can use the space as a `transformers.Tool` later.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们基本上将这个片段部署为Hugging Face空间[这里](https://huggingface.co/spaces/vwxyzjn/pyserini-wikipedia-kilt-doc)，以便稍后可以将空间用作`transformers.Tool`。
- en: '![](../Images/99bd431ca68ab971be553352b4f02e67.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99bd431ca68ab971be553352b4f02e67.png)'
- en: Experiment settings
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'We use the following settings:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下设置：
- en: use the `bigcode/starcoderbase` model as the base model
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`bigcode/starcoderbase`模型作为基础模型
- en: use the `pyserini-wikipedia-kilt-doc` space as the wiki tool and only uses the
    first paragrahs of the search result, allowing the `TextEnvironment` to obtain
    at most `max_tool_reponse=400` response tokens from the tool.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pyserini-wikipedia-kilt-doc`空间作为维基工具，并仅使用搜索结果的第一段，允许`TextEnvironment`从工具中最多获得`max_tool_reponse=400`个响应标记。
- en: test if the response contain the answer string, if so, give a reward of 1, otherwise,
    give a reward of 0.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试回复是否包含答案字符串，如果是，则奖励1，否则，奖励0。
- en: notice this is a simplified evaluation criteria. In [ToolFormer](https://arxiv.org/abs/2302.04761),
    the authors checks if the first 20 words of the response contain the correct answer.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，这是一个简化的评估标准。在[ToolFormer](https://arxiv.org/abs/2302.04761)中，作者检查回复的前20个单词是否包含正确答案。
- en: used the following prompt that demonstrates the usage of the wiki tool.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下演示了维基工具的用法的提示。
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Result and Discussion
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果和讨论
- en: Our experiments show that the agent can learn to use the wiki tool to answer
    questions. The learning curves would go up mostly, but one of the experiment did
    crash.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，代理可以学会使用维基工具来回答问题。学习曲线大多会上升，但其中一个实验崩溃了。
- en: '![](../Images/a85eaae9f29d7074402801be154c2d57.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a85eaae9f29d7074402801be154c2d57.png)'
- en: Wandb report is [here](https://wandb.ai/costa-huang/cleanRL/reports/TriviaQA-Final-Experiments--Vmlldzo1MjY0ODk5)
    for further inspection.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Wandb报告在[这里](https://wandb.ai/costa-huang/cleanRL/reports/TriviaQA-Final-Experiments--Vmlldzo1MjY0ODk5)进行进一步检查。
- en: 'Note that the correct rate of the trained model is on the low end, which could
    be due to the following reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练模型的正确率较低，可能是由于以下原因：
- en: '**incorrect searches:** When given the question `"What is Bruce Willis'' real
    first name?"` if the model searches for `Bruce Willis`, our wiki tool returns
    “Patrick Poivey (born 18 February 1948) is a French actor. He is especially known
    for his voice: he is the French dub voice of Bruce Willis since 1988.`But a correct
    search should be`Walter Bruce Willis (born March 19, 1955) is an American former
    actor. He achieved fame with a leading role on the comedy-drama series Moonlighting
    (1985–1989) and appeared in over a hundred films, gaining recognition as an action
    hero after his portrayal of John McClane in the Die Hard franchise (1988–2013)
    and other roles.[1][2]”'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误的搜索:** 当给出问题`“布鲁斯·威利斯的真实名字是什么？”`时，如果模型搜索`布鲁斯·威利斯`，我们的维基工具返回“帕特里克·普瓦（出生于1948年2月18日）是一位法国演员。他以他的声音而闻名：自1988年以来，他一直是布鲁斯·威利斯的法语配音。”但正确的搜索应该是`沃尔特·布鲁斯·威利斯（出生于1955年3月19日）是一位美国前演员。他因在喜剧剧集《月光光》（1985-1989）中担任主角而成名，并在一百多部电影中出演，因在《虎胆龙威》系列（1988-2013）中扮演约翰·麦克莱恩而被认可为动作英雄以及其他角色而获得认可。`'
- en: '![](../Images/36b96862e37367132a174f261df0cb60.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36b96862e37367132a174f261df0cb60.png)'
- en: '**unnecessarily long response**: The wiki tool by default sometimes output
    very long sequences. E.g., when the wiki tool searches for “Brown Act”'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不必要的长回复:** 默认情况下，维基工具有时会输出非常长的序列。例如，当维基工具搜索“布朗法案”时'
- en: Our wiki tool returns “The Ralph M. Brown Act, located at California Government
    Code 54950 “et seq.”, is an act of the California State Legislature, authored
    by Assemblymember Ralph M. Brown and passed in 1953, that guarantees the public’s
    right to attend and participate in meetings of local legislative bodies.”
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的维基工具返回“拉尔夫·M·布朗法案，位于加利福尼亚政府法典54950“等等”，是加利福尼亚州议会的一项法案，由议员拉尔夫·M·布朗撰写并于1953年通过，保证了公众有权参加和参与地方立法机构的会议。”
- en: '[ToolFormer](https://arxiv.org/abs/2302.04761)’s wiki tool returns “The Ralph
    M. Brown Act is an act of the California State Legislature that guarantees the
    public’s right to attend and participate in meetings of local legislative bodies.”
    which is more succinct.'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ToolFormer](https://arxiv.org/abs/2302.04761)的维基工具返回“拉尔夫·M·布朗法案是加利福尼亚州议会的一项法案，保证了公众有权参加和参与地方立法机构的会议。”更为简洁。'
- en: '![](../Images/03829057729a612fe5243cc49785765b.png)'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/03829057729a612fe5243cc49785765b.png)'
- en: '(Early Experiments 🧪): solving math puzzles with python interpreter'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: （早期实验🧪）：使用Python解释器解决数学难题
- en: 'In this section, we attempt to teach the model to use a python interpreter
    to solve math puzzles. The rough idea is to give the agent a prompt like the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们尝试教导模型使用Python解释器来解决数学难题。大致的想法是给予代理一个类似以下的提示：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Training experiment can be found at [https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y](https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练实验可以在[https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y](https://wandb.ai/lvwerra/trl-gsm8k/runs/a5odv01y)找到
- en: '![](../Images/07e24d84156093ad57f614545aaa5ab2.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e24d84156093ad57f614545aaa5ab2.png)'
