- en: Two main approaches for solving RL problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/learn/deep-rl-course/unit1/two-methods](https://huggingface.co/learn/deep-rl-course/unit1/two-methods)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/deep-rl-course/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/start.c0547f01.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/scheduler.37c15a92.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/singletons.b4cd11ef.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.18351ede.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/paths.3cd722f3.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/entry/app.41e0adab.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/index.7cb9c9b8.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/0.b906e680.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/nodes/19.b1e5e12c.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Tip.d10b3fc9.js">
    <link rel="modulepreload" href="/docs/deep-rl-course/main/en/_app/immutable/chunks/Heading.d3928e2a.js">Now
    that we learned the RL framework, how do we solve the RL problem?
  prefs: []
  type: TYPE_NORMAL
- en: In other words, how do we build an RL agent that can **select the actions that maximize
    its expected cumulative reward?**
  prefs: []
  type: TYPE_NORMAL
- en: 'The Policy π: the agent’s brain'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Policy **π** is the **brain of our Agent**, it’s the function that tells
    us what **action to take given the state we are in.** So it **defines the agent’s
    behavior** at a given time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy](../Images/83518e23a957f171ab1fe3fa7a6bbe35.png)'
  prefs: []
  type: TYPE_IMG
- en: Think of policy as the brain of our agent, the function that will tell us the
    action to take given a state
  prefs: []
  type: TYPE_NORMAL
- en: This Policy **is the function we want to learn**, our goal is to find the optimal
    policy π*, the policy that **maximizes expected return** when the agent acts according
    to it. We find this π* **through training.**
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two approaches to train our agent to find this optimal policy π*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Directly,** by teaching the agent to learn which **action to take,** given
    the current state: **Policy-Based Methods.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Indirectly, **teach the agent to learn which state is more valuable** and then
    take the action that **leads to the more valuable states**: Value-Based Methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy-Based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Policy-Based methods, **we learn a policy function directly.**
  prefs: []
  type: TYPE_NORMAL
- en: This function will define a mapping from each state to the best corresponding
    action. Alternatively, it could define **a probability distribution over the set
    of possible actions at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy](../Images/6758c67029516191953f67721d370c3e.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see here, the policy (deterministic) **directly indicates the action
    to take for each step.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two types of policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deterministic*: a policy at a given state **will always return the same action.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Policy](../Images/ee7b2c571f3d32864167d58a4063e9d3.png)'
  prefs: []
  type: TYPE_IMG
- en: action = policy(state)
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy](../Images/eab138ef553910f3be1dc2fc9a4c1d74.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Stochastic*: outputs **a probability distribution over actions.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Policy](../Images/651cfd4580146ecc7e4526d3c73cf0a3.png)'
  prefs: []
  type: TYPE_IMG
- en: policy(actions | state) = probability distribution over the set of actions given
    the current state
  prefs: []
  type: TYPE_NORMAL
- en: '![Policy Based](../Images/24f29ab03a77c489ff2e67423152c2f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Given an initial state, our stochastic policy will output probability distributions
    over the possible actions at that state.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pbm recap](../Images/ccacb85446b779ce48e2b9c14852ff2b.png) ![Pbm recap](../Images/b65b570d8d5bdf35333f56e519f8ebbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Value-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In value-based methods, instead of learning a policy function, we **learn a
    value function** that maps a state to the expected value **of being at that state.**
  prefs: []
  type: TYPE_NORMAL
- en: The value of a state is the **expected discounted return** the agent can get
    if it **starts in that state, and then acts according to our policy.**
  prefs: []
  type: TYPE_NORMAL
- en: “Act according to our policy” just means that our policy is **“going to the
    state with the highest value”.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Value based RL](../Images/9f6cecad619e80db36cfe941f6772544.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we see that our value function **defined values for each possible state.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Value based RL](../Images/a083a986fc5a289be300383a0bbc883f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thanks to our value function, at each step our policy will select the state
    with the biggest value defined by the value function: -7, then -6, then -5 (and
    so on) to attain the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to our value function, at each step our policy will select the state
    with the biggest value defined by the value function: -7, then -6, then -5 (and
    so on) to attain the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vbm recap](../Images/ea2aeb10b56230f8695a306775324a19.png) ![Vbm recap](../Images/645f2468d4ce43d60dde1ff7d573fe7c.png)'
  prefs: []
  type: TYPE_IMG
