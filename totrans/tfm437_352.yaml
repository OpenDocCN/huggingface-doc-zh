- en: LayoutLMV2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/layoutlmv2)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training
    for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by
    Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei
    Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves
    [LayoutLM](layoutlm) to obtain state-of-the-art results across several document
    image understanding benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'information extraction from scanned documents: the [FUNSD](https://guillaumejaume.github.io/FUNSD/)
    dataset (a collection of 199 annotated forms comprising more than 30,000 words),
    the [CORD](https://github.com/clovaai/cord) dataset (a collection of 800 receipts
    for training, 100 for validation and 100 for testing), the [SROIE](https://rrc.cvc.uab.es/?ch=13)
    dataset (a collection of 626 receipts for training and 347 receipts for testing)
    and the [Kleister-NDA](https://github.com/applicaai/kleister-nda) dataset (a collection
    of non-disclosure agreements from the EDGAR database, including 254 documents
    for training, 83 documents for validation, and 203 documents for testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)
    dataset (a collection of 400,000 images belonging to one of 16 classes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'document visual question answering: the [DocVQA](https://arxiv.org/abs/2007.00398)
    dataset (a collection of 50,000 questions defined on 12,000+ document images).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pre-training of text and layout has proved effective in a variety of visually-rich
    document understanding tasks due to its effective model architecture and the advantage
    of large-scale unlabeled scanned/digital-born documents. In this paper, we present
    LayoutLMv2 by pre-training text, layout and image in a multi-modal framework,
    where new model architectures and pre-training tasks are leveraged. Specifically,
    LayoutLMv2 not only uses the existing masked visual-language modeling task but
    also the new text-image alignment and text-image matching tasks in the pre-training
    stage, where cross-modality interaction is better learned. Meanwhile, it also
    integrates a spatial-aware self-attention mechanism into the Transformer architecture,
    so that the model can fully understand the relative positional relationship among
    different text blocks. Experiment results show that LayoutLMv2 outperforms strong
    baselines and achieves new state-of-the-art results on a wide variety of downstream
    visually-rich document understanding tasks, including FUNSD (0.7895 -> 0.8420),
    CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852),
    RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). The pre-trained LayoutLMv2
    model is publicly available at this https URL.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'LayoutLMv2 depends on `detectron2`, `torchvision` and `tesseract`. Run the
    following to install them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: (If you are developing for LayoutLMv2, note that passing the doctests also requires
    the installation of these packages.)
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates
    visual embeddings during pre-training (while LayoutLMv1 only adds visual embeddings
    during fine-tuning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv2 adds both a relative 1D attention bias as well as a spatial 2D attention
    bias to the attention scores in the self-attention layers. Details can be found
    on page 5 of the [paper](https://arxiv.org/abs/2012.14740).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demo notebooks on how to use the LayoutLMv2 model on RVL-CDIP, FUNSD, DocVQA,
    CORD can be found [here](https://github.com/NielsRogge/Transformers-Tutorials).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv2 uses Facebook AI‚Äôs [Detectron2](https://github.com/facebookresearch/detectron2/)
    package for its visual backbone. See [this link](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)
    for installation instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to `input_ids`, [forward()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model.forward)
    expects 2 additional inputs, namely `image` and `bbox`. The `image` input corresponds
    to the original document image in which the text tokens occur. The model expects
    each document image to be of size 224x224\. This means that if you have a batch
    of document images, `image` should be a tensor of shape (batch_size, 3, 224, 224).
    This can be either a `torch.Tensor` or a `Detectron2.structures.ImageList`. You
    don‚Äôt need to normalize the channels, as this is done by the model. Important
    to note is that the visual backbone expects BGR channels instead of RGB, as all
    models in Detectron2 are pre-trained using the BGR format. The `bbox` input are
    the bounding boxes (i.e. 2D-positions) of the input text tokens. This is identical
    to [LayoutLMModel](/docs/transformers/v4.37.2/en/model_doc/layoutlm#transformers.LayoutLMModel).
    These can be obtained using an external OCR engine such as Google‚Äôs [Tesseract](https://github.com/tesseract-ocr/tesseract)
    (there‚Äôs a [Python wrapper](https://pypi.org/project/pytesseract/) available).
    Each bounding box should be in (x0, y0, x1, y1) format, where (x0, y0) corresponds
    to the position of the upper left corner in the bounding box, and (x1, y1) represents
    the position of the lower right corner. Note that one first needs to normalize
    the bounding boxes to be on a 0-1000 scale. To normalize, you can use the following
    function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `width` and `height` correspond to the width and height of the original
    document in which the token occurs (before resizing the image). Those can be obtained
    using the Python Image Library (PIL) library for example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, this model includes a brand new [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    which can be used to directly prepare data for the model (including applying OCR
    under the hood). More information can be found in the ‚ÄúUsage‚Äù section below.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    will send the `image` input through its visual backbone to obtain a lower-resolution
    feature map, whose shape is equal to the `image_feature_pool_shape` attribute
    of [LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config).
    This feature map is then flattened to obtain a sequence of image tokens. As the
    size of the feature map is 7x7 by default, one obtains 49 image tokens. These
    are then concatenated with the text tokens, and send through the Transformer encoder.
    This means that the last hidden states of the model will have a length of 512
    + 49 = 561, if you pad the text tokens up to the max length. More generally, the
    last hidden states will have a shape of `seq_length` + `image_feature_pool_shape[0]`
    * `config.image_feature_pool_shape[1]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When calling [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained),
    a warning will be printed with a long list of parameter names that are not initialized.
    This is not a problem, as these parameters are batch normalization statistics,
    which are going to have values when fine-tuning on a custom dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to train the model in a distributed environment, make sure to call
    `synchronize_batch_norm` on the model in order to properly synchronize the batch
    normalization layers of the visual backbone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, there‚Äôs LayoutXLM, which is a multilingual version of LayoutLMv2\.
    More information can be found on [LayoutXLM‚Äôs documentation page](layoutxlm).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A list of official Hugging Face and community (indicated by üåé) resources to
    help you get started with LayoutLMv2\. If you‚Äôre interested in submitting a resource
    to be included here, please feel free to open a Pull Request and we‚Äôll review
    it! The resource should ideally demonstrate something new instead of duplicating
    an existing resource.
  prefs: []
  type: TYPE_NORMAL
- en: Text Classification
  prefs: []
  type: TYPE_NORMAL
- en: A notebook on how to [finetune LayoutLMv2 for text-classification on RVL-CDIP
    dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Text classification task guide](../tasks/sequence_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question Answering
  prefs: []
  type: TYPE_NORMAL
- en: A notebook on how to [finetune LayoutLMv2 for question-answering on DocVQA dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Question answering task guide](../tasks/question_answering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Document question answering task guide](../tasks/document_question_answering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token Classification
  prefs: []
  type: TYPE_NORMAL
- en: A notebook on how to [finetune LayoutLMv2 for token-classification on CORD dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A notebook on how to [finetune LayoutLMv2 for token-classification on FUNSD
    dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also: [Token classification task guide](../tasks/token_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage: LayoutLMv2Processor'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to prepare data for the model is to use [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor),
    which internally combines a image processor ([LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor))
    and a tokenizer ([LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast)).
    The image processor handles the image modality, while the tokenizer handles the
    text modality. A processor combines both, which is ideal for a multi-modal model
    like LayoutLMv2\. Note that you can still use both separately, if you only want
    to handle one modality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In short, one can provide a document image (and possibly additional data) to
    [LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor),
    and it will create the inputs expected by the model. Internally, the processor
    first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to apply OCR on the image to get a list of words and normalized bounding boxes,
    as well to resize the image to a given size in order to get the `image` input.
    The words and normalized bounding boxes are then provided to [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast),
    which converts them to token-level `input_ids`, `attention_mask`, `token_type_ids`,
    `bbox`. Optionally, one can provide word labels to the processor, which are turned
    into token-level `labels`.
  prefs: []
  type: TYPE_NORMAL
- en: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    uses [PyTesseract](https://pypi.org/project/pytesseract/), a Python wrapper around
    Google‚Äôs Tesseract OCR engine, under the hood. Note that you can still use your
    own OCR engine of choice, and provide the words and normalized boxes yourself.
    This requires initializing [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    with `apply_ocr` set to `False`.'
  prefs: []
  type: TYPE_NORMAL
- en: In total, there are 5 use cases that are supported by the processor. Below,
    we list them all. Note that each of these use cases work for both batched and
    non-batched inputs (we illustrate them for non-batched inputs).
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case 1: document image classification (training, inference) + token classification
    (inference), apply_ocr = True**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest case, in which the processor (actually the image processor)
    will perform OCR on the image to get the words and normalized bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Use case 2: document image classification (training, inference) + token classification
    (inference), apply_ocr=False**'
  prefs: []
  type: TYPE_NORMAL
- en: In case one wants to do OCR themselves, one can initialize the image processor
    with `apply_ocr` set to `False`. In that case, one should provide the words and
    corresponding (normalized) bounding boxes themselves to the processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Use case 3: token classification (training), apply_ocr=False**'
  prefs: []
  type: TYPE_NORMAL
- en: For token classification tasks (such as FUNSD, CORD, SROIE, Kleister-NDA), one
    can also provide the corresponding word labels in order to train a model. The
    processor will then convert these into token-level `labels`. By default, it will
    only label the first wordpiece of a word, and label the remaining wordpieces with
    -100, which is the `ignore_index` of PyTorch‚Äôs CrossEntropyLoss. In case you want
    all wordpieces of a word to be labeled, you can initialize the tokenizer with
    `only_label_first_subword` set to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Use case 4: visual question answering (inference), apply_ocr=True**'
  prefs: []
  type: TYPE_NORMAL
- en: For visual question answering tasks (such as DocVQA), you can provide a question
    to the processor. By default, the processor will apply OCR on the image, and create
    [CLS] question tokens [SEP] word tokens [SEP].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Use case 5: visual question answering (inference), apply_ocr=False**'
  prefs: []
  type: TYPE_NORMAL
- en: For visual question answering tasks (such as DocVQA), you can provide a question
    to the processor. If you want to perform OCR yourself, you can provide your own
    words and (normalized) bounding boxes to the processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2Config
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2Config`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/configuration_layoutlmv2.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) ‚Äî Vocabulary size of the
    LayoutLMv2 model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    or `TFLayoutLMv2Model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) ‚Äî Dimension of the encoder
    layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) ‚Äî Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) ‚Äî Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) ‚Äî Dimension of the
    ‚Äúintermediate‚Äù (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) ‚Äî The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) ‚Äî The dropout
    probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) ‚Äî The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) ‚Äî The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_vocab_size` (`int`, *optional*, defaults to 2) ‚Äî The vocabulary size
    of the `token_type_ids` passed when calling [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    or `TFLayoutLMv2Model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) ‚Äî The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) ‚Äî The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_2d_position_embeddings` (`int`, *optional*, defaults to 1024) ‚Äî The maximum
    value that the 2D position embedding might ever be used with. Typically set this
    to something large just in case (e.g., 1024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_rel_pos` (`int`, *optional*, defaults to 128) ‚Äî The maximum number of
    relative positions to be used in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rel_pos_bins` (`int`, *optional*, defaults to 32) ‚Äî The number of relative
    position bins to be used in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fast_qkv` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to use
    a single matrix for the queries, keys, values in the self-attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_rel_2d_pos` (`int`, *optional*, defaults to 256) ‚Äî The maximum number
    of relative 2D positions in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rel_2d_pos_bins` (`int`, *optional*, defaults to 64) ‚Äî The number of 2D relative
    position bins in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_feature_pool_shape` (`List[int]`, *optional*, defaults to [7, 7, 256])
    ‚Äî The shape of the average-pooled feature map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coordinate_size` (`int`, *optional*, defaults to 128) ‚Äî Dimension of the coordinate
    embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shape_size` (`int`, *optional*, defaults to 128) ‚Äî Dimension of the width
    and height embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_relative_attention_bias` (`bool`, *optional*, defaults to `True`) ‚Äî Whether
    or not to use a relative attention bias in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_spatial_attention_bias` (`bool`, *optional*, defaults to `True`) ‚Äî Whether
    or not to use a spatial attention bias in the self-attention mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`has_visual_segment_embedding` (`bool`, *optional*, defaults to `False`) ‚Äî
    Whether or not to add visual segment embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectron2_config_args` (`dict`, *optional*) ‚Äî Dictionary containing the configuration
    arguments of the Detectron2 visual backbone. Refer to [this file](https://github.com/microsoft/unilm/blob/master/layoutlmft/layoutlmft/models/layoutlmv2/detectron2_config.py)
    for details regarding default values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model).
    It is used to instantiate an LayoutLMv2 model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the LayoutLMv2 [microsoft/layoutlmv2-base-uncased](https://huggingface.co/microsoft/layoutlmv2-base-uncased)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2FeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2FeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_processing_utils.py#L550)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess an image or a batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv2ImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2ImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L93)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) ‚Äî Whether to resize the
    image‚Äôs (height, width) dimensions to `(size["height"], size["width"])`. Can be
    overridden by `do_resize` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]` *optional*, defaults to `{"height" -- 224, "width":
    224}`): Size of the image after resizing. Can be overridden by `size` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    ‚Äî Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply_ocr` (`bool`, *optional*, defaults to `True`) ‚Äî Whether to apply the
    Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden
    by `apply_ocr` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ocr_lang` (`str`, *optional*) ‚Äî The language, specified by its ISO code, to
    be used by the Tesseract OCR engine. By default, English is used. Can be overridden
    by `ocr_lang` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tesseract_config` (`str`, *optional*, defaults to `""`) ‚Äî Any additional custom
    configuration flags that are forwarded to the `config` parameter when calling
    Tesseract. For example: ‚Äò‚Äîpsm 6‚Äô. Can be overridden by `tesseract_config` in `preprocess`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a LayoutLMv2 image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py#L189)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) ‚Äî Image to preprocess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) ‚Äî Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) ‚Äî Desired size
    of the output image after resizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    ‚Äî Resampling filter to use if resizing the image. This can be one of the enum
    `PIL.Image` resampling filter. Only has an effect if `do_resize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apply_ocr` (`bool`, *optional*, defaults to `self.apply_ocr`) ‚Äî Whether to
    apply the Tesseract OCR engine to get words + normalized bounding boxes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ocr_lang` (`str`, *optional*, defaults to `self.ocr_lang`) ‚Äî The language,
    specified by its ISO code, to be used by the Tesseract OCR engine. By default,
    English is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tesseract_config` (`str`, *optional*, defaults to `self.tesseract_config`)
    ‚Äî Any additional custom configuration flags that are forwarded to the `config`
    parameter when calling Tesseract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) ‚Äî The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    ‚Äî The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.FIRST`: image in (num_channels, height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ChannelDimension.LAST`: image in (height, width, num_channels) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv2Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2Tokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L206)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Construct a LayoutLMv2 tokenizer. Based on WordPiece. [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    can be used to turn words, word-level bounding boxes and optional word labels
    to token-level `input_ids`, `attention_mask`, `token_type_ids`, `bbox`, and optional
    `labels` (for token classification).
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '[LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    runs end-to-end tokenization: punctuation splitting and wordpiece. It also turns
    the word-level bounding boxes into token-level bounding boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L430)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) ‚Äî The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`List[str]`, `List[List[str]]`) ‚Äî The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) ‚Äî Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) ‚Äî Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) ‚Äî Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) ‚Äî Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) ‚Äî Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) ‚Äî If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) ‚Äî If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) ‚Äî If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_token_type_ids` (`bool`, *optional*) ‚Äî Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer‚Äôs default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) ‚Äî Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer‚Äôs default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python‚Äôs tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_length` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to print
    more information and warnings. **kwargs ‚Äî passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` ‚Äî List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` ‚Äî List of bounding boxes to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` ‚Äî List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *‚Äútoken_type_ids‚Äù* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` ‚Äî List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *‚Äúattention_mask‚Äù* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` ‚Äî List of labels to be fed to a model. (when `word_labels` is specified).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overflowing_tokens` ‚Äî List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_truncated_tokens` ‚Äî Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens_mask` ‚Äî List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` ‚Äî The length of the inputs (when `return_length=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py#L410)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2TokenizerFast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2TokenizerFast`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L70)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) ‚Äî File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to
    lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) ‚Äî The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) ‚Äî The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) ‚Äî The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) ‚Äî The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) ‚Äî The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) ‚Äî The
    bounding box to use for the special [CLS] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep_token_box` (`List[int]`, *optional*, defaults to `[1000, 1000, 1000, 1000]`)
    ‚Äî The bounding box to use for the special [SEP] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_box` (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`) ‚Äî The
    bounding box to use for the special [PAD] token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_label` (`int`, *optional*, defaults to -100) ‚Äî The label to use
    for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch‚Äôs
    CrossEntropyLoss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`only_label_first_subword` (`bool`, *optional*, defaults to `True`) ‚Äî Whether
    or not to only label the first subword, in case word labels are provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) ‚Äî Whether
    or not to tokenize Chinese characters. This should likely be deactivated for Japanese
    (see [this issue](https://github.com/huggingface/transformers/issues/328)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strip_accents` (`bool`, *optional*) ‚Äî Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original LayoutLMv2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a ‚Äúfast‚Äù LayoutLMv2 tokenizer (backed by HuggingFace‚Äôs *tokenizers*
    library). Based on WordPiece.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py#L179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`text` (`str`, `List[str]`, `List[List[str]]`) ‚Äî The sequence or batch of sequences
    to be encoded. Each sequence can be a string, a list of strings (words of a single
    example or questions of a batch of examples) or a list of list of strings (batch
    of words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_pair` (`List[str]`, `List[List[str]]`) ‚Äî The sequence or batch of sequences
    to be encoded. Each sequence should be a list of strings (pretokenized string).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boxes` (`List[List[int]]`, `List[List[List[int]]]`) ‚Äî Word-level bounding
    boxes. Each bounding box should be normalized to be on a 0-1000 scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`word_labels` (`List[int]`, `List[List[int]]`, *optional*) ‚Äî Word-level integer
    labels (for token classification tasks such as FUNSD, CORD).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) ‚Äî Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) ‚Äî Activates and controls truncation. Accepts
    the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) ‚Äî Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stride` (`int`, *optional*, defaults to 0) ‚Äî If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) ‚Äî If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) ‚Äî If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_token_type_ids` (`bool`, *optional*) ‚Äî Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizer‚Äôs default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) ‚Äî Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizer‚Äôs default, defined by the `return_outputs` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return special tokens mask information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) ‚Äî Whether
    or not to return `(char_start, char_end)` for each token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Python‚Äôs tokenizer, this method will raise `NotImplementedError`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_length` (`bool`, *optional*, defaults to `False`) ‚Äî Whether or not
    to return the lengths of the encoded inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` (`bool`, *optional*, defaults to `True`) ‚Äî Whether or not to print
    more information and warnings. **kwargs ‚Äî passed to the `self.tokenize()` method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` ‚Äî List of token ids to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` ‚Äî List of bounding boxes to be fed to a model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_type_ids` ‚Äî List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *‚Äútoken_type_ids‚Äù* is in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` ‚Äî List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *‚Äúattention_mask‚Äù* is
    in `self.model_input_names`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`labels` ‚Äî List of labels to be fed to a model. (when `word_labels` is specified).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overflowing_tokens` ‚Äî List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_truncated_tokens` ‚Äî Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`special_tokens_mask` ‚Äî List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length` ‚Äî The length of the inputs (when `return_length=True`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences with word-level normalized bounding boxes
    and optional labels.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv2Processor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2Processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L27)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` (`LayoutLMv2ImageProcessor`, *optional*) ‚Äî An instance of
    [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor).
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`LayoutLMv2Tokenizer` or `LayoutLMv2TokenizerFast`, *optional*)
    ‚Äî An instance of [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a LayoutLMv2 processor which combines a LayoutLMv2 image processor
    and a LayoutLMv2 tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[LayoutLMv2Processor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor)
    offers all the functionalities you need to prepare data for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: It first uses [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    to resize document images to a fixed size, and optionally applies OCR to get words
    and normalized bounding boxes. These are then provided to [LayoutLMv2Tokenizer](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer)
    or [LayoutLMv2TokenizerFast](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast),
    which turns the words and bounding boxes into token-level `input_ids`, `attention_mask`,
    `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which
    are turned into token-level `labels` for token classification tasks (such as FUNSD,
    CORD).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/processing_layoutlmv2.py#L69)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This method first forwards the `images` argument to [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__).
    In case [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    was initialized with `apply_ocr` set to `True`, it passes the obtained words and
    bounding boxes along with the additional arguments to [**call**()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)
    and returns the output, together with resized `images`. In case [LayoutLMv2ImageProcessor](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor)
    was initialized with `apply_ocr` set to `False`, it passes the words (`text`/`text_pair`)
    and `boxes` specified by the user along with the additional arguments to [__call__()](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer.__call__)
    and returns the output, together with resized `images`.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the docstring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: LayoutLMv2Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2Model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L688)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare LayoutLMv2 Model transformer outputting raw hidden-states without any
    specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L802)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) ‚Äî
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*)
    ‚Äî Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) ‚Äî Batch of document images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) ‚Äî Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv2Model](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2ForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2ForSequenceClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L944)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv2 Model with a sequence classification head on top (a linear layer
    on top of the concatenation of the final hidden state of the [CLS] token, average-pooled
    initial visual embeddings and average-pooled final visual embeddings, e.g. for
    document image classification tasks such as the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/)
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L967)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) ‚Äî Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    ‚Äî Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) ‚Äî Batch of document images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) ‚Äî
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2ForTokenClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2ForTokenClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1126)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv2 Model with a token classification head on top (a linear layer on
    top of the text part of the hidden states) e.g. for sequence labeling (information
    extraction) tasks such as [FUNSD](https://guillaumejaume.github.io/FUNSD/), [SROIE](https://rrc.cvc.uab.es/?ch=13),
    [CORD](https://github.com/clovaai/cord) and [Kleister-NDA](https://github.com/applicaai/kleister-nda).
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1149)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) ‚Äî Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    ‚Äî Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) ‚Äî Batch of document images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    ‚Äî Labels for computing the token classification loss. Indices should be in `[0,
    ..., config.num_labels - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Classification loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`)
    ‚Äî Classification scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv2ForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: LayoutLMv2ForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.LayoutLMv2ForQuestionAnswering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1258)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    ‚Äî Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LayoutLMv2 Model with a span classification head on top for extractive question-answering
    tasks such as [DocVQA](https://rrc.cvc.uab.es/?ch=17) (a linear layer on top of
    the text part of the hidden-states output to compute `span start logits` and `span
    end logits`).
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py#L1280)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`) ‚Äî Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`bbox` (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*)
    ‚Äî Bounding boxes of each input sequence tokens. Selected in the range `[0, config.max_2d_position_embeddings-1]`.
    Each bounding box should be a normalized version in (x0, y0, x1, y1) format, where
    (x0, y0) corresponds to the position of the upper left corner in the bounding
    box, and (x1, y1) represents the position of the lower right corner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image` (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`
    or `detectron.structures.ImageList` whose `tensors` is of shape `(batch_size,
    num_channels, height, width)`) ‚Äî Batch of document images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `batch_size, sequence_length`,
    *optional*) ‚Äî Indices of positions of each input sequence tokens in the position
    embeddings. Selected in the range `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) ‚Äî Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) ‚Äî Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert *input_ids* indices into associated vectors than
    the model‚Äôs internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) ‚Äî Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) ‚Äî Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) ‚Äî Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*)
    ‚Äî Labels for position (index) of the start of the labelled span for computing
    the token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_positions` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) ‚Äî
    Labels for position (index) of the end of the labelled span for computing the
    token classification loss. Positions are clamped to the length of the sequence
    (`sequence_length`). Position outside of the sequence are not taken into account
    for computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.QuestionAnsweringModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([LayoutLMv2Config](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) ‚Äî Total span extraction loss is the sum of a Cross-Entropy for the
    start and end positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    ‚Äî Span-start scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`)
    ‚Äî Span-end scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) ‚Äî Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [LayoutLMv2ForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: In this example below, we give the LayoutLMv2 model an image (of texts) and
    ask it a question. It will give us a prediction of what it thinks the answer is
    (the span of the answer within the texts parsed from the image).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
