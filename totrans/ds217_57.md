# 批处理映射

> 原文链接：[https://huggingface.co/docs/datasets/about_map_batch](https://huggingface.co/docs/datasets/about_map_batch)

将[Dataset.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)的实用性与批处理模式结合起来非常强大。它允许您加快处理速度，并自由控制生成的数据集的大小。

## 需要速度

批处理映射的主要目标是加快处理速度。通常情况下，使用数据批次而不是单个示例更快。自然地，批处理映射适用于标记化。例如，🤗 [Tokenizers](https://huggingface.co/docs/tokenizers/python/latest/)库在批处理中的工作速度更快，因为它并行处理批次中所有示例的标记化。

## 输入大小 != 输出大小

控制生成数据集大小的能力可以用于许多有趣的用例。在[map](#map)部分中，有使用批处理映射的示例：

+   将长句子拆分为较短的块。

+   用额外的标记增强数据集。

了解这是如何工作的很有帮助，这样您就可以想出自己使用批处理映射的方法。此时，您可能想知道如何控制生成数据集的大小。答案是：**映射函数不必返回相同大小的输出批次**。

换句话说，您的映射函数输入可以是大小为`N`的批次，并返回大小为`M`的批次。输出`M`可以大于或小于`N`。这意味着您可以连接示例，将其分割，并甚至添加更多示例！

但是，请记住，输出字典中的所有值必须包含与输出字典中的其他字段**相同数量的元素**。否则，无法定义映射函数返回的输出中的示例数量。映射函数处理的连续批次之间的数量可能会有所不同。但是对于单个批次，输出字典的所有值应具有相同的长度（即元素数量）。

例如，从具有1列和3行的数据集中，如果您使用`map`返回具有两倍行数的新列，则会出现错误。在这种情况下，您将得到一列有3行，另一列有6行。如您所见，表将无效：

```py
>>> from datasets import Dataset
>>> dataset = Dataset.from_dict({"a": [0, 1, 2]})
>>> dataset.map(lambda batch: {"b": batch["a"] * 2}, batched=True)  # new column with 6 elements: [0, 1, 2, 0, 1, 2]
'ArrowInvalid: Column 1 named b expected length 3 but got length 6'
```

为使其有效，您必须删除其中一个列：

```py
>>> from datasets import Dataset
>>> dataset = Dataset.from_dict({"a": [0, 1, 2]})
>>> dataset_with_duplicates = dataset.map(lambda batch: {"b": batch["a"] * 2}, remove_columns=["a"], batched=True)
>>> len(dataset_with_duplicates)
6
```
