- en: Trajectory Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: This model is in maintenance mode only, so we won’t accept any new PRs changing
    its code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run into any issues running this model, please reinstall the last version
    that supported this model: v4.30.0. You can do so by running the following command:
    `pip install -U transformers==4.30.0`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Trajectory Transformer model was proposed in [Offline Reinforcement Learning
    as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael
    Janner, Qiyang Li, Sergey Levine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Reinforcement learning (RL) is typically concerned with estimating stationary
    policies or single-step models, leveraging the Markov property to factorize problems
    in time. However, we can also view RL as a generic sequence modeling problem,
    with the goal being to produce a sequence of actions that leads to a sequence
    of high rewards. Viewed in this way, it is tempting to consider whether high-capacity
    sequence prediction models that work well in other domains, such as natural-language
    processing, can also provide effective solutions to the RL problem. To this end,
    we explore how RL can be tackled with the tools of sequence modeling, using a
    Transformer architecture to model distributions over trajectories and repurposing
    beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies
    a range of design decisions, allowing us to dispense with many of the components
    common in offline RL algorithms. We demonstrate the flexibility of this approach
    across long-horizon dynamics prediction, imitation learning, goal-conditioned
    RL, and offline RL. Further, we show that this approach can be combined with existing
    model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon
    tasks.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [CarlCochet](https://huggingface.co/CarlCochet).
    The original code can be found [here](https://github.com/jannerm/trajectory-transformer).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This Transformer is used for deep reinforcement learning. To use it, you need
    to create sequences from actions, states and rewards from all previous timesteps.
    This model will treat all these elements together as one big sequence (a trajectory).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: TrajectoryTransformerConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TrajectoryTransformerConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/configuration_trajectory_transformer.py#L31)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 100) — Vocabulary size of the
    TrajectoryTransformer model. Defines the number of different tokens that can be
    represented by the `trajectories` passed when calling [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action_weight` (`int`, *optional*, defaults to 5) — Weight of the action in
    the loss function'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward_weight` (`int`, *optional*, defaults to 1) — Weight of the reward in
    the loss function'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_weight` (`int`, *optional*, defaults to 1) — Weight of the value in
    the loss function'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`block_size` (`int`, *optional*, defaults to 249) — Size of the blocks in the
    trajectory transformer.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action_dim` (`int`, *optional*, defaults to 6) — Dimension of the action space.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`observation_dim` (`int`, *optional*, defaults to 17) — Dimension of the observation
    space.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transition_dim` (`int`, *optional*, defaults to 25) — Dimension of the transition
    space.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layer` (`int`, *optional*, defaults to 4) — Number of hidden layers in the
    Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_head` (`int`, *optional*, defaults to 4) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_embd` (`int`, *optional*, defaults to 128) — Dimensionality of the embeddings
    and hidden states.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — The dropout ratio for the
    embeddings.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the attention.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"selu"` and `"gelu_new"` are supported.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 512) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kaiming_initializer_range` (`float, *optional*, defaults to 1) — A coefficient
    scaling the negative slope of the kaiming initializer rectifier for EinLinear
    layers.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models). Only relevant
    if `config.is_decoder=True`. Example —'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel).
    It is used to instantiate an TrajectoryTransformer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the TrajectoryTransformer
    [CarlCochet/trajectory-transformer-halfcheetah-medium-v2](https://huggingface.co/CarlCochet/trajectory-transformer-halfcheetah-medium-v2)
    architecture.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TrajectoryTransformerModel
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TrajectoryTransformerModel`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L399)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare TrajectoryTransformer Model transformer outputting raw hidden-states
    without any specific head on top. This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: the full GPT language model, with a context size of block_size
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py#L464)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '`trajectories` (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Batch of trajectories, where a trajectory is a sequence of states, actions and
    rewards.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`,
    *optional*) — Contains precomputed hidden-states (key and values in the attention
    blocks) as computed by the model (see `past_key_values` output below). Can be
    used to speed up sequential decoding. The `input_ids` which have their past given
    to this model should not be passed as `input_ids` as they have already been computed.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targets` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Desired targets used to compute the loss.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.deprecated.trajectory_transformer.modeling_trajectory_transformer.TrajectoryTransformerOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TrajectoryTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig))
    and inputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[torch.Tensor]]`, *optional*, returned when
    `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of length
    `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,
    sequence_length, embed_size_per_head)`). Contains pre-computed hidden-states (key
    and values in the attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    GPT2Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TrajectoryTransformerModel](/docs/transformers/v4.37.2/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此之后调用，因为前者负责运行前处理和后处理步骤，而后者则会默默地忽略它们。
- en: 'Examples:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
