# 🤗 Transformers 如何解决任务

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/tasks_explained](https://huggingface.co/docs/transformers/v4.37.2/en/tasks_explained)

在[🤗 Transformers 能做什么](task_summary)中，您了解了自然语言处理（NLP）、语音和音频、计算机视觉任务以及它们的一些重要应用。本页将仔细研究模型如何解决这些任务，并解释发生在幕后的情况。解决给定任务的方法有很多种，一些模型可能会实现特定的技术，甚至从新的角度来处理任务，但对于Transformer模型来说，总体思路是相同的。由于其灵活的架构，大多数模型都是编码器、解码器或编码器-解码器结构的变体。除了Transformer模型，我们的库还有几个卷积神经网络（CNNs），这些网络在今天仍然用于计算机视觉任务。我们还将解释现代CNN的工作原理。

为了解释任务是如何解决的，我们将详细介绍模型内部的运作方式，以输出有用的预测。

+   [Wav2Vec2](model_doc/wav2vec2) 用于音频分类和自动语音识别（ASR）

+   [Vision Transformer (ViT)](model_doc/vit) 和 [ConvNeXT](model_doc/convnext) 用于图像分类

+   [DETR](model_doc/detr) 用于目标检测

+   [Mask2Former](model_doc/mask2former) 用于图像分割

+   [GLPN](model_doc/glpn) 用于深度估计

+   [BERT](model_doc/bert) 用于像文本分类、标记分类和问答这样使用编码器的NLP任务

+   [GPT2](model_doc/gpt2) 用于像文本生成这样使用解码器的NLP任务

+   [BART](model_doc/bart) 用于像总结和翻译这样使用编码器-解码器的NLP任务

在继续之前，最好对原始Transformer架构有一些基本了解。了解编码器、解码器和注意力的工作原理将有助于您理解不同的Transformer模型是如何工作的。如果您刚开始或需要温习，请查看我们的[课程](https://huggingface.co/course/chapter1/4?fw=pt)获取更多信息！

## 语音和音频

[Wav2Vec2](model_doc/wav2vec2) 是一个自监督模型，它在未标记的语音数据上进行了预训练，并在标记数据上进行了微调，用于音频分类和自动语音识别。

![](../Images/e11e728e2c074a2fde6bbf1aa6c0959b.png)

这个模型有四个主要组件：

1.  一个*特征编码器*接收原始音频波形，将其归一化为零均值和单位方差，并将其转换为每个长度为20ms的特征向量序列。

1.  波形本质上是连续的，因此无法像文本序列那样分割成单独的单元。这就是为什么特征向量会传递给一个*量化模块*，该模块旨在学习离散的语音单元。语音单元是从一个称为*码书*的编码词集合中选择的（您可以将其视为词汇表）。从码书中选择最能代表连续音频输入的向量或语音单元，并将其传递到模型中。

1.  大约一半的特征向量是随机屏蔽的，并且被屏蔽的特征向量被馈送到一个*上下文网络*，这是一个Transformer编码器，还添加了相对位置嵌入。

1.  上下文网络的预训练目标是一个*对比任务*。模型必须从一组错误的预测中预测出被屏蔽的真实量化语音表示，从而鼓励模型找到最相似的上下文向量和量化语音单元（目标标签）。

现在wav2vec2已经预训练完成，您可以在您的数据上对其进行微调，用于音频分类或自动语音识别！

### 音频分类

要将预训练模型用于音频分类，在基础Wav2Vec2模型顶部添加一个序列分类头。分类头是一个线性层，接受编码器的隐藏状态。隐藏状态代表每个音频帧的学习特征，可以具有不同的长度。为了创建一个固定长度的向量，首先对隐藏状态进行汇总，然后将其转换为类标签上的logits。在logits和目标之间计算交叉熵损失，以找到最可能的类别。

准备好尝试音频分类了吗？查看我们完整的[音频分类指南](tasks/audio_classification)，了解如何微调Wav2Vec2并将其用于推断！

### 自动语音识别

要将预训练模型用于自动语音识别，在基础Wav2Vec2模型顶部添加一个语言建模头，用于[连接主义时间分类（CTC）](glossary#connectionist-temporal-classification-ctc)。语言建模头是一个线性层，接受编码器的隐藏状态并将其转换为logits。每个logit代表一个标记类别（标记数量来自任务词汇表）。在logits和目标之间计算CTC损失，以找到最可能的标记序列，然后将其解码为转录。

准备好尝试自动语音识别了吗？查看我们完整的[自动语音识别指南](tasks/asr)，了解如何微调Wav2Vec2并将其用于推断！

## 计算机视觉

有两种方法可以处理计算机视觉任务：

1.  将图像分割成一系列补丁，并使用Transformer并行处理它们。

1.  使用现代CNN，比如[ConvNeXT](model_doc/convnext)，它依赖卷积层但采用现代网络设计。

第三种方法将Transformer与卷积结合（例如[卷积视觉Transformer](model_doc/cvt)或[LeViT](model_doc/levit)）。我们不会讨论这些，因为它们只是将我们在这里研究的两种方法结合起来。

ViT和ConvNeXT通常用于图像分类，但对于其他视觉任务，如目标检测、分割和深度估计，我们将分别查看DETR、Mask2Former和GLPN；这些模型更适合这些任务。

### 图像分类

ViT和ConvNeXT都可以用于图像分类；主要区别在于ViT使用注意机制，而ConvNeXT使用卷积。

#### Transformer

[ViT](model_doc/vit)完全用纯Transformer架构替换了卷积。如果你熟悉原始Transformer，那么你已经基本理解了ViT。

![](../Images/603d60786d4fe02e975af3e3e9553816.png)

ViT引入的主要变化是如何将图像馈送到Transformer中：

1.  图像被分割成方形不重叠的补丁，每个补丁都被转换为一个向量或*补丁嵌入*。补丁嵌入是从一个卷积2D层生成的，该层创建了适当的输入维度（对于基础Transformer来说，每个补丁嵌入有768个值）。如果你有一个224x224像素的图像，你可以将其分割成196个16x16的图像补丁。就像文本被标记为单词一样，图像被“标记”为一系列补丁。

1.  一个*可学习的嵌入* - 一个特殊的`[CLS]`标记 - 被添加到补丁嵌入的开头，就像BERT一样。`[CLS]`标记的最终隐藏状态被用作附加分类头的输入；其他输出被忽略。这个标记帮助模型学习如何编码图像的表示。

1.  将补丁和可学习嵌入的最后一件事是添加*位置嵌入*，因为模型不知道图像补丁的顺序。位置嵌入也是可学习的，并且与补丁嵌入具有相同的大小。最后，所有嵌入都传递给Transformer编码器。

1.  输出，特别是只有带有`[CLS]`标记的输出，被传递到一个多层感知器头（MLP）。ViT的预训练目标只是分类。像其他分类头一样，MLP头将输出转换为类标签上的logits，并计算交叉熵损失以找到最可能的类别。

准备尝试图像分类？查看我们完整的[image classification guide](tasks/image_classification)来学习如何微调ViT并将其用于推断！

#### 卷积神经网络

本节简要解释了卷积，但了解它们如何改变图像的形状和大小将会很有帮助。如果您对卷积不熟悉，请查看fastai书中的[Convolution Neural Networks章节](https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb)！

[ConvNeXT](model_doc/convnext)是一种采用新的现代网络设计来提高性能的CNN架构。然而，卷积仍然是模型的核心。从高层次的角度来看，[卷积](glossary#convolution)是一种操作，其中一个较小的矩阵（*核*）与图像像素的一个小窗口相乘。它从中计算一些特征，比如特定的纹理或线条的曲率。然后它滑动到下一个像素窗口；卷积移动的距离被称为*步幅*。

![](../Images/03f73d57ab58f712f2d9ce87a23fe681.png)从[A guide to convolution arithmetic for deep learning.](https://arxiv.org/abs/1603.07285)中提取的不带填充或步幅的基本卷积。

您可以将此输出馈送到另一个卷积层，随着每个连续层，网络学习更复杂和抽象的事物，如热狗或火箭。在卷积层之间，通常会添加一个池化层来减少维度，并使模型更能够适应特征位置的变化。

![](../Images/5a951227302389e2bf2b426374fa7a82.png)

ConvNeXT以五种方式现代化CNN：

1.  改变每个阶段的块数，并使用更大的步幅和相应的核大小对图像进行“patchify”。不重叠的滑动窗口使得这种patchifying策略类似于ViT如何将图像分成补丁。

1.  一个*瓶颈*层会减少通道数量，然后通过进行1x1卷积来恢复，因为这样做更快，而且可以增加深度。一个反向瓶颈则相反，通过扩展通道数量然后再缩小，这样更节省内存。

1.  在瓶颈层中用*深度卷积*替换典型的3x3卷积层，深度卷积对每个输入通道分别应用卷积，然后在最后将它们堆叠在一起。这样可以扩大网络宽度，提高性能。

1.  ViT具有全局感受野，这意味着它可以一次看到更多图像，这要归功于其注意力机制。ConvNeXT尝试通过将核大小增加到7x7来复制这种效果。

1.  ConvNeXT还进行了几个层设计更改，模仿Transformer模型。激活和归一化层更少，激活函数从ReLU切换为GELU，并且使用LayerNorm代替BatchNorm。

来自卷积块的输出被传递到一个分类头，将输出转换为logits，并计算交叉熵损失以找到最可能的标签。

### 目标检测

[DETR](model_doc/detr), *DEtection TRansformer*，是一个将CNN与Transformer编码器-解码器结合起来的端到端目标检测模型。

![](../Images/7bcc42731f7b59a083ba5098cc335b73.png)

1.  一个预训练的CNN *骨干* 接收一幅图像，由其像素值表示，并创建其低分辨率特征图。对特征图应用1x1卷积以降低维度，并创建具有高级图像表示的新特征图。由于Transformer是一个顺序模型，特征图被展平成一系列特征向量，这些向量与位置嵌入相结合。

1.  特征向量传递给编码器，编码器使用其注意力层学习图像表示。接下来，编码器隐藏状态与解码器中的*对象查询*相结合。对象查询是学习的嵌入，专注于图像的不同区域，并在通过每个注意力层时更新。解码器隐藏状态传递给一个前馈网络，该网络预测每个对象查询的边界框坐标和类别标签，或者如果没有对象则为`无对象`。

    DETR并行解码每个对象查询以输出*N*个最终预测，其中*N*是查询的数量。与典型的自回归模型一次预测一个元素不同，目标检测是一个集合预测任务（`边界框`，`类别标签`），在一次传递中进行*N*次预测。

1.  DETR在训练过程中使用*二部匹配损失*来比较固定数量的预测和固定的一组真实标签。如果在*N*个标签集中有更少的真实标签，则它们将用`无对象`类进行填充。这个损失函数鼓励DETR找到预测和真实标签之间的一对一分配。如果边界框或类别标签不正确，则会产生损失。同样，如果DETR预测了一个不存在的对象，它将受到惩罚。这鼓励DETR在图像中找到其他对象，而不是专注于一个非常突出的对象。

在DETR之上添加了一个目标检测头，用于查找类别标签和边界框的坐标。目标检测头有两个组件：一个线性层将解码器隐藏状态转换为类别标签上的logits，以及一个MLP来预测边界框。

准备尝试目标检测？查看我们完整的[目标检测指南](tasks/object_detection)来学习如何微调DETR并将其用于推断！

### 图像分割

[Mask2Former](model_doc/mask2former)是一个通用架构，用于解决所有类型的图像分割任务。传统的分割模型通常针对图像分割的特定子任务进行定制，如实例、语义或全景分割。Mask2Former将这些任务中的每一个都视为一个*掩码分类*问题。掩码分类将像素分组成*N*个段，并为给定图像预测*N*个掩码及其相应的类别标签。我们将在本节中解释Mask2Former的工作原理，然后您可以在最后尝试微调SegFormer。

![](../Images/95e2688e3e32c64aa4b6487532aa9814.png)

Mask2Former有三个主要组件：

1.  一个[Swin](model_doc/swin)骨干接受一幅图像，并从3个连续的3x3卷积中创建一个低分辨率图像特征图。

1.  特征图传递给一个*像素解码器*，逐渐将低分辨率特征上采样为高分辨率的逐像素嵌入。像素解码器实际上生成多尺度特征（包含低分辨率和高分辨率特征），分辨率为原始图像的1/32、1/16和1/8。

1.  这些不同尺度的特征图依次被馈送到一个Transformer解码器层，以便从高分辨率特征中捕获小物体。Mask2Former的关键在于解码器中的*掩码注意力*机制。与可以关注整个图像的交叉注意力不同，掩码注意力只关注图像的某个区域。这样做更快，性能更好，因为图像的局部特征足以让模型学习。

1.  与[DETR](tasks_explained#object-detection)类似，Mask2Former还使用学习的对象查询，并将它们与像素解码器的图像特征组合以进行一组预测（`类标签`，`掩码预测`）。解码器隐藏状态被传递到线性层，并转换为类标签上的逻辑。计算逻辑和类标签之间的交叉熵损失以找到最可能的类标签。

    掩码预测是通过将像素嵌入与最终解码器隐藏状态相结合生成的。通过逻辑和地面真实掩码之间计算sigmoid交叉熵和dice损失以找到最可能的掩码。

准备尝试目标检测？查看我们完整的[图像分割指南](tasks/semantic_segmentation)来学习如何微调SegFormer并将其用于推断！

### 深度估计

[GLPN](model_doc/glpn)，*全局-局部路径网络*，是一个用于深度估计的Transformer，它将[SegFormer](model_doc/segformer)编码器与轻量级解码器结合起来。

![](../Images/ff344ea07d21e963e3c93f56069bfcb3.png)

1.  与ViT类似，图像被分割成一个序列的补丁，只是这些图像补丁更小。这对于密集预测任务如分割或深度估计更好。图像补丁被转换为补丁嵌入（有关如何创建补丁嵌入的更多详细信息，请参阅[图像分类](#image-classification)部分），然后馈送到编码器。

1.  编码器接受补丁嵌入，并通过多个编码器块传递它们。每个块由注意力和Mix-FFN层组成。后者的目的是提供位置信息。在每个编码器块的末尾是一个*补丁合并*层，用于创建分层表示。相邻补丁组的特征被串联起来，并且对串联特征应用线性层以减少补丁数量至1/4的分辨率。这成为下一个编码器块的输入，整个过程在那里重复，直到您获得分辨率为1/8、1/16和1/32的图像特征。

1.  轻量级解码器从编码器中获取最后的特征图（1/32比例）并将其上采样至1/16比例。然后，该特征被传递到*选择性特征融合（SFF）*模块中，该模块从注意力图中选择和组合每个特征的局部和全局特征，然后将其上采样至1/8。这个过程重复进行，直到解码特征与原始图像大小相同。输出通过两个卷积层，然后应用sigmoid激活以预测每个像素的深度。

## 自然语言处理

Transformer最初是为机器翻译而设计的，自那时以来，它实际上已经成为解决所有NLP任务的默认架构。一些任务适合Transformer的编码器结构，而其他任务更适合解码器。还有一些任务利用Transformer的编码器-解码器结构。

### 文本分类

[BERT](model_doc/bert)是一个仅编码器模型，是第一个有效实现深度双向性以通过同时关注单词两侧来学习文本更丰富表示的模型。

1.  BERT使用[WordPiece](tokenizer_summary#wordpiece)标记化来生成文本的标记嵌入。为了区分单个句子和一对句子之间的区别，添加了一个特殊的`[SEP]`标记来区分它们。在每个文本序列的开头添加了一个特殊的`[CLS]`标记。带有`[CLS]`标记的最终输出用作分类任务的分类头的输入。BERT还添加了一个段嵌入，用于表示一个标记属于一对句子中的第一句还是第二句。

1.  BERT使用两个目标进行预训练：掩码语言建模和下一句预测。在掩码语言建模中，输入标记的一定百分比被随机掩盖，模型需要预测这些标记。这解决了双向性的问题，其中模型可以作弊并看到所有单词并“预测”下一个单词。预测的掩码标记的最终隐藏状态传递给一个具有词汇表上的softmax的前馈网络，以预测掩码单词。

    第二个预训练目标是下一句预测。模型必须预测句子B是否跟在句子A后面。一半的时间句子B是下一个句子，另一半的时间，句子B是一个随机句子。预测，无论是下一个句子还是不是，都传递给一个具有两个类别（`IsNext`和`NotNext`）的softmax的前馈网络。

1.  输入嵌入通过多个编码器层传递以输出一些最终隐藏状态。

要使用预训练模型进行文本分类，需要在基本BERT模型的顶部添加一个序列分类头。序列分类头是一个线性层，接受最终的隐藏状态，并执行线性变换以将它们转换为对数。在对数和目标之间计算交叉熵损失，以找到最可能的标签。

准备好尝试文本分类了吗？查看我们完整的[文本分类指南](tasks/sequence_classification)来学习如何微调DistilBERT并将其用于推理！

### 标记分类

要将BERT用于像命名实体识别（NER）这样的标记分类任务，需要在基本BERT模型的顶部添加一个标记分类头。标记分类头是一个线性层，接受最终的隐藏状态，并执行线性变换以将它们转换为对数。在对数和每个标记之间计算交叉熵损失，以找到最可能的标签。

准备好尝试标记分类了吗？查看我们完整的[token分类指南](tasks/token_classification)来学习如何微调DistilBERT并将其用于推理！

### 问答

要将BERT用于问答，需要在基本BERT模型的顶部添加一个跨度分类头。这个线性层接受最终的隐藏状态，并执行线性变换以计算与答案对应的`span`起始和结束对数。在对数和标签位置之间计算交叉熵损失，以找到与答案对应的最可能跨度的文本。

准备好尝试问答了吗？查看我们完整的[问答指南](tasks/question_answering)来学习如何微调DistilBERT并将其用于推理！

💡注意一旦BERT被预训练后，使用它进行不同任务是多么容易。您只需要向预训练模型添加一个特定的头部，将隐藏状态操纵成您想要的输出！

### 文本生成

[GPT-2](model_doc/gpt2)是一个仅解码的模型，预训练了大量文本。它可以生成令人信服的（尽管不总是真实的！）文本，给定一个提示并完成其他NLP任务，如问答，尽管没有明确训练。

![](../Images/62b0162aa2f177113d6ce38517129f28.png)

1.  GPT-2使用[字节对编码（BPE）](tokenizer_summary#bytepair-encoding-bpe)对单词进行标记化并生成令牌嵌入。位置编码添加到令牌嵌入中，以指示序列中每个令牌的位置。输入嵌入通过多个解码器块传递以输出一些最终隐藏状态。在每个解码器块内，GPT-2使用*屏蔽自注意力*层，这意味着GPT-2不能关注未来的令牌。它只允许关注左侧的令牌。这与BERT的`mask`令牌不同，因为在屏蔽自注意力中，使用注意力掩码将未来的令牌得分设置为`0`。

1.  解码器的输出传递给语言建模头部，执行线性转换将隐藏状态转换为logits。标签是序列中的下一个令牌，通过将logits向右移动一个来创建。在移位logits和标签之间计算交叉熵损失，以输出下一个最可能的令牌。

GPT-2的预训练目标完全基于[因果语言建模](glossary#causal-language-modeling)，预测序列中的下一个单词。这使得GPT-2在涉及生成文本的任务中表现特别出色。

准备好尝试文本生成了吗？查看我们完整的[因果语言建模指南](tasks/language_modeling#causal-language-modeling)来学习如何微调DistilGPT-2并将其用于推理！

有关文本生成的更多信息，请查看[文本生成策略](generation_strategies)指南！

### 摘要

像[BART](model_doc/bart)和[T5](model_doc/t5)这样的编码器-解码器模型是为摘要任务的序列到序列模式而设计的。我们将在本节中解释BART的工作原理，然后您可以在最后尝试微调T5。

![](../Images/4b8bfc65e2d0ac5580bd76b9d3882f86.png)

1.  BART的编码器架构与BERT非常相似，接受文本的令牌和位置嵌入。BART通过破坏输入然后使用解码器重建来进行预训练。与具有特定破坏策略的其他编码器不同，BART可以应用任何类型的破坏。然而，*文本填充*破坏策略效果最好。在文本填充中，一些文本段被替换为一个**单个**`mask`令牌。这很重要，因为模型必须预测被屏蔽的令牌，并且它教会模型预测缺失令牌的数量。输入嵌入和屏蔽的段通过编码器传递以输出一些最终隐藏状态，但与BERT不同，BART不会在最后添加最终的前馈网络来预测一个单词。

1.  编码器的输出传递给解码器，解码器必须预测编码器输出中的屏蔽令牌和任何未损坏的令牌。这提供了额外的上下文来帮助解码器恢复原始文本。解码器的输出传递给语言建模头部，执行线性转换将隐藏状态转换为logits。在logits和标签之间计算交叉熵损失，标签只是向右移动的令牌。

准备好尝试摘要了吗？查看我们完整的[摘要指南](tasks/summarization)来学习如何微调T5并将其用于推理！

有关文本生成的更多信息，请查看[文本生成策略](generation_strategies)指南！

### 翻译

翻译是另一个序列到序列任务的例子，这意味着您可以使用像[BART](model_doc/bart)或[T5](model_doc/t5)这样的编码器-解码器模型来执行。我们将在本节中解释BART的工作原理，然后您可以在最后尝试微调T5。

BART通过添加一个单独的随机初始化编码器来适应翻译，将源语言映射到一个可以解码为目标语言的输入。这个新编码器的嵌入被传递给预训练编码器，而不是原始词嵌入。源编码器通过使用模型输出的交叉熵损失来更新源编码器、位置嵌入和输入嵌入进行训练。在这一步中，模型参数被冻结，所有模型参数在第二步中一起训练。

BART之后推出了多语言版本mBART，旨在用于翻译并在许多不同语言上进行预训练。

准备尝试翻译吗？查看我们的完整[翻译指南](tasks/summarization)来学习如何微调T5并将其用于推理！

要了解更多关于文本生成的信息，请查看[文本生成策略](generation_strategies)指南！
