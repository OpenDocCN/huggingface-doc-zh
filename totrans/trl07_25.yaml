- en: Using LLaMA models with TRL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LLaMAæ¨¡å‹ä¸TRL
- en: 'Original text: [https://huggingface.co/docs/trl/using_llama_models](https://huggingface.co/docs/trl/using_llama_models)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/trl/using_llama_models](https://huggingface.co/docs/trl/using_llama_models)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ve begun rolling out examples to use Metaâ€™s LLaMA models in `trl` (see [Metaâ€™s
    LLaMA release](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
    for the original LLaMA model).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å¼€å§‹æ¨å‡ºç¤ºä¾‹ï¼Œä»¥åœ¨`trl`ä¸­ä½¿ç”¨Metaçš„LLaMAæ¨¡å‹ï¼ˆè¯·å‚é˜…[Metaçš„LLaMAå‘å¸ƒ](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)ä»¥è·å–åŸå§‹LLaMAæ¨¡å‹ï¼‰ã€‚
- en: Efficient training strategies
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥
- en: 'Even training the smallest LLaMA model requires an enormous amount of memory.
    Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition
    to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer)
    in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB`
    just to fit in memory and would likely need more when you compute intermediate
    values such as attention scores. So you couldnâ€™t train the model even on a single
    80GB A100 like that. You can use some tricks, like more efficient optimizers of
    half-precision training, to squeeze a bit more into memory, but youâ€™ll run out
    sooner or later.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿è®­ç»ƒæœ€å°çš„LLaMAæ¨¡å‹ä¹Ÿéœ€è¦å¤§é‡å†…å­˜ã€‚ä¸€äº›å¿«é€Ÿçš„æ•°å­¦è®¡ç®—ï¼šåœ¨bf16ä¸­ï¼Œæ¯ä¸ªå‚æ•°ä½¿ç”¨2ä¸ªå­—èŠ‚ï¼ˆåœ¨fp32ä¸­ä¸º4ä¸ªå­—èŠ‚ï¼‰ï¼Œå¦å¤–è¿˜ä½¿ç”¨8ä¸ªå­—èŠ‚ï¼Œä¾‹å¦‚åœ¨Adamä¼˜åŒ–å™¨ä¸­ä½¿ç”¨çš„ï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…Transformersä¸­çš„[æ€§èƒ½æ–‡æ¡£](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer)ï¼‰ã€‚å› æ­¤ï¼Œä¸€ä¸ª7Bå‚æ•°æ¨¡å‹å°†ä½¿ç”¨`(2+8)*7B=70GB`æ‰èƒ½å®Œå…¨é€‚åº”å†…å­˜ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ç­‰ä¸­å¯èƒ½éœ€è¦æ›´å¤šã€‚å› æ­¤ï¼Œå³ä½¿åœ¨å•ä¸ª80GBçš„A100ä¸Šä¹Ÿæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§ï¼Œä¾‹å¦‚æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨æˆ–åŠç²¾åº¦è®­ç»ƒï¼Œå°†æ›´å¤šå†…å®¹å‹ç¼©åˆ°å†…å­˜ä¸­ï¼Œä½†è¿Ÿæ—©ä¼šç”¨å®Œã€‚
- en: Another option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques,
    such as the [`peft`](https://github.com/huggingface/peft) library, which can perform
    low-rank adaptation (LoRA) on a model loaded in 8-bit. For more on `peft` + `trl`,
    see the [docs](https://huggingface.co/docs/trl/sentiment_tuning_peft).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œä¾‹å¦‚[`peft`](https://github.com/huggingface/peft)åº“ï¼Œå®ƒå¯ä»¥åœ¨åŠ è½½ä¸º8ä½çš„æ¨¡å‹ä¸Šæ‰§è¡Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚æœ‰å…³`peft`
    + `trl`çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://huggingface.co/docs/trl/sentiment_tuning_peft)ã€‚
- en: Loading the model in 8bit reduces the memory footprint drastically since you
    only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory).
    Instead of training the original weights directly, LoRA adds small adapter layers
    on top of some specific layers (usually the attention layers); thus, the number
    of trainable parameters is drastically reduced.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åŠ è½½ä¸º8ä½ä¼šæ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼Œå› ä¸ºæƒé‡åªéœ€è¦æ¯ä¸ªå‚æ•°ä¸€ä¸ªå­—èŠ‚ï¼ˆä¾‹å¦‚ï¼Œ7B LlaMaåœ¨å†…å­˜ä¸­å ç”¨7GBï¼‰ã€‚LoRAä¸æ˜¯ç›´æ¥è®­ç»ƒåŸå§‹æƒé‡ï¼Œè€Œæ˜¯åœ¨æŸäº›ç‰¹å®šå±‚ï¼ˆé€šå¸¸æ˜¯æ³¨æ„åŠ›å±‚ï¼‰çš„é¡¶éƒ¨æ·»åŠ å°çš„é€‚é…å™¨å±‚ï¼Œä»è€Œå¤§å¤§å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚
- en: In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters
    (depending on the batch size and sequence length) to fit the entire fine-tuning
    setup. This enables fine-tuning larger models (up to 50-60B scale models on a
    NVIDIA A100 80GB) at low cost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯ä¸ºæ¯åäº¿å‚æ•°åˆ†é…çº¦1.2-1.4GBçš„å†…å­˜ï¼ˆå–å†³äºæ‰¹å¤„ç†å¤§å°å’Œåºåˆ—é•¿åº¦ï¼‰ï¼Œä»¥é€‚åº”æ•´ä¸ªå¾®è°ƒè®¾ç½®ã€‚è¿™å¯ä»¥ä»¥è¾ƒä½çš„æˆæœ¬å®ç°å¯¹æ›´å¤§æ¨¡å‹çš„å¾®è°ƒï¼ˆåœ¨NVIDIA
    A100 80GBä¸Šå¯è¾¾50-60Bè§„æ¨¡çš„æ¨¡å‹ï¼‰ã€‚
- en: 'Now we can fit very large models into a single GPU, but the training might
    still be very slow. The simplest strategy in this scenario is data parallelism:
    we replicate the same training setup into separate GPUs and pass different batches
    to each GPU. With this, you can parallelize the forward/backward passes of the
    model and scale with the number of GPUs.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†éå¸¸å¤§çš„æ¨¡å‹é€‚é…åˆ°å•ä¸ªGPUä¸­ï¼Œä½†è®­ç»ƒå¯èƒ½ä»ç„¶éå¸¸ç¼“æ…¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹æœ€ç®€å•çš„ç­–ç•¥æ˜¯æ•°æ®å¹¶è¡Œå¤„ç†ï¼šæˆ‘ä»¬å°†ç›¸åŒçš„è®­ç»ƒè®¾ç½®å¤åˆ¶åˆ°ä¸åŒçš„GPUä¸­ï¼Œå¹¶å°†ä¸åŒçš„æ‰¹æ¬¡ä¼ é€’ç»™æ¯ä¸ªGPUã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‚¨å¯ä»¥å¹¶è¡ŒåŒ–æ¨¡å‹çš„å‰å‘/åå‘ä¼ é€’ï¼Œå¹¶éšç€GPUæ•°é‡çš„å¢åŠ è€Œæ‰©å±•ã€‚
- en: '![chapter10_ddp.png](../Images/f281f3d2b69eb8ef7952621def3f8b06.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![chapter10_ddp.png](../Images/f281f3d2b69eb8ef7952621def3f8b06.png)'
- en: We use either the `transformers.Trainer` or `accelerate`, which both support
    data parallelism without any code changes, by simply passing arguments when calling
    the scripts with `torchrun` or `accelerate launch`. The following runs a training
    script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨`transformers.Trainer`æˆ–`accelerate`ï¼Œå®ƒä»¬éƒ½æ”¯æŒæ•°æ®å¹¶è¡Œå¤„ç†ï¼Œæ— éœ€ä»»ä½•ä»£ç æ›´æ”¹ï¼Œåªéœ€åœ¨è°ƒç”¨å¸¦æœ‰`torchrun`æˆ–`accelerate
    launch`çš„è„šæœ¬æ—¶ä¼ é€’å‚æ•°ã€‚ä»¥ä¸‹åˆ†åˆ«åœ¨å•å°æœºå™¨ä¸Šä½¿ç”¨8ä¸ªGPUè¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œä½¿ç”¨`accelerate`å’Œ`torchrun`ã€‚
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Supervised fine-tuning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒ
- en: Before we start training reward models and tuning our model with RL, it helps
    if the model is already good in the domain we are interested in. In our case,
    we want it to answer questions, while for other use cases, we might want it to
    follow instructions, in which case instruction tuning is a great idea. The easiest
    way to achieve this is by continuing to train the language model with the language
    modeling objective on texts from the domain or task. The [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)
    is enormous (over 10 million instructions), so we can easily train the language
    model on a subset of it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹å¹¶ä½¿ç”¨RLè°ƒæ•´æˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰ï¼Œå¦‚æœæ¨¡å‹åœ¨æˆ‘ä»¬æ„Ÿå…´è¶£çš„é¢†åŸŸå·²ç»å¾ˆå¥½ï¼Œå°†ä¼šæœ‰æ‰€å¸®åŠ©ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒèƒ½å›ç­”é—®é¢˜ï¼Œè€Œå¯¹äºå…¶ä»–ç”¨ä¾‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›å®ƒèƒ½éµå¾ªæŒ‡ä»¤ï¼Œè¿™ç§æƒ…å†µä¸‹æŒ‡ä»¤è°ƒæ•´æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸»æ„ã€‚å®ç°è¿™ä¸€ç‚¹çš„æœ€ç®€å•æ–¹æ³•æ˜¯ç»§ç»­ä½¿ç”¨æ¥è‡ªè¯¥é¢†åŸŸæˆ–ä»»åŠ¡çš„æ–‡æœ¬è¿›è¡Œè¯­è¨€æ¨¡å‹çš„è®­ç»ƒã€‚[StackExchangeæ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences)éå¸¸åºå¤§ï¼ˆè¶…è¿‡1000ä¸‡æ¡æŒ‡ä»¤ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åœ¨å…¶å­é›†ä¸Šè®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
- en: 'There is nothing special about fine-tuning the model before doing RLHF - itâ€™s
    just the causal language modeling objective from pretraining that we apply here.
    To use the data efficiently, we use a technique called packing: instead of having
    one text per sample in the batch and then padding to either the longest text or
    the maximal context of the model, we concatenate a lot of texts with a EOS token
    in between and cut chunks of the context size to fill the batch without any padding.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›è¡ŒRLHFä¹‹å‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„ - æˆ‘ä»¬åªæ˜¯åº”ç”¨äº†è¿™é‡Œçš„é¢„è®­ç»ƒå› æœè¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚ä¸ºäº†æœ‰æ•ˆä½¿ç”¨æ•°æ®ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§ç§°ä¸ºpackingçš„æŠ€æœ¯ï¼šä¸æ˜¯æ¯ä¸ªæ ·æœ¬åœ¨æ‰¹å¤„ç†ä¸­æœ‰ä¸€ä¸ªæ–‡æœ¬ï¼Œç„¶åå¡«å……åˆ°æœ€é•¿æ–‡æœ¬æˆ–æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡ï¼Œè€Œæ˜¯åœ¨è®¸å¤šæ–‡æœ¬ä¹‹é—´ä½¿ç”¨EOSæ ‡è®°è¿æ¥ï¼Œå¹¶åˆ‡å‰²ä¸Šä¸‹æ–‡å¤§å°çš„å—ä»¥å¡«å……æ‰¹å¤„ç†è€Œä¸è¿›è¡Œä»»ä½•å¡«å……ã€‚
- en: '![chapter10_preprocessing-clm.png](../Images/b6a16749788aead5596b057282494855.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![chapter10_preprocessing-clm.png](../Images/b6a16749788aead5596b057282494855.png)'
- en: With this approach the training is much more efficient as each token that is
    passed through the model is also trained in contrast to padding tokens which are
    usually masked from the loss. If you donâ€™t have much data and are more concerned
    about occasionally cutting off some tokens that are overflowing the context you
    can also use a classical data loader.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºé€šè¿‡æ¨¡å‹ä¼ é€’çš„æ¯ä¸ªæ ‡è®°ä¹Ÿä¼šå¾—åˆ°è®­ç»ƒï¼Œè€Œä¸åƒé€šå¸¸ä»æŸå¤±ä¸­å±è”½çš„å¡«å……æ ‡è®°ã€‚å¦‚æœæ‚¨æ²¡æœ‰å¤ªå¤šæ•°æ®ï¼Œå¹¶ä¸”æ›´æ‹…å¿ƒå¶å°”æˆªæ–­ä¸€äº›æº¢å‡ºä¸Šä¸‹æ–‡çš„æ ‡è®°ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ç»å…¸æ•°æ®åŠ è½½å™¨ã€‚
- en: The packing is handled by the `ConstantLengthDataset` and we can then use the
    `Trainer` after loading the model with `peft`. First, we load the model in int8,
    prepare it for training, and then add the LoRA adapters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConstantLengthDataset`å¤„ç†æ‰“åŒ…ï¼Œç„¶ååœ¨ä½¿ç”¨`peft`åŠ è½½æ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Trainer`ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»¥int8åŠ è½½æ¨¡å‹ï¼Œä¸ºè®­ç»ƒåšå‡†å¤‡ï¼Œç„¶åæ·»åŠ LoRAé€‚é…å™¨ã€‚'
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We train the model for a few thousand steps with the causal language modeling
    objective and save the model. Since we will tune the model again with different
    objectives, we merge the adapter weights with the original model weights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡ç›®æ ‡å¯¹æ¨¡å‹è¿›è¡Œå‡ åƒæ­¥çš„è®­ç»ƒï¼Œå¹¶ä¿å­˜æ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨ä¸åŒç›®æ ‡è°ƒæ•´æ¨¡å‹ï¼Œæˆ‘ä»¬å°†é€‚é…å™¨æƒé‡ä¸åŸå§‹æ¨¡å‹æƒé‡åˆå¹¶ã€‚
- en: '**Disclaimer:** due to LLaMAâ€™s license, we release only the adapter weights
    for this and the model checkpoints in the following sections. You can apply for
    access to the base modelâ€™s weights by filling out Meta AIâ€™s [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)
    and then converting them to the ğŸ¤— Transformers format by running this [script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py).
    Note that youâ€™ll also need to install ğŸ¤— Transformers from source until the `v4.28`
    is released.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å…è´£å£°æ˜ï¼š**ç”±äºLLaMAçš„è®¸å¯è¯ï¼Œæˆ‘ä»¬ä»…åœ¨ä»¥ä¸‹éƒ¨åˆ†å‘å¸ƒé€‚é…å™¨æƒé‡å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚æ‚¨å¯ä»¥é€šè¿‡å¡«å†™Meta AIçš„[è¡¨æ ¼](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)ç”³è¯·è®¿é—®åŸºç¡€æ¨¡å‹çš„æƒé‡ï¼Œç„¶åé€šè¿‡è¿è¡Œæ­¤[è„šæœ¬](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py)å°†å…¶è½¬æ¢ä¸ºğŸ¤—
    Transformersæ ¼å¼ã€‚è¯·æ³¨æ„ï¼Œæ‚¨è¿˜éœ€è¦å®‰è£…ğŸ¤— Transformersæºä»£ç ï¼Œç›´åˆ°`v4.28`å‘å¸ƒä¸ºæ­¢ã€‚'
- en: Now that we have fine-tuned the model for the task, we are ready to train a
    reward model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¸ºä»»åŠ¡å¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œæˆ‘ä»¬å‡†å¤‡è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ã€‚
- en: Reward modeling and human preferences
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥–åŠ±å»ºæ¨¡å’Œäººç±»åå¥½
- en: In principle, we could fine-tune the model using RLHF directly with the human
    annotations. However, this would require us to send some samples to humans for
    rating after each optimization iteration. This is expensive and slow due to the
    number of training samples needed for convergence and the inherent latency of
    human reading and annotator speed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨äººç±»æ³¨é‡Šå¯¹æ¨¡å‹è¿›è¡ŒRLHFå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™å°†è¦æ±‚æˆ‘ä»¬åœ¨æ¯æ¬¡ä¼˜åŒ–è¿­ä»£åå‘äººç±»å‘é€ä¸€äº›æ ·æœ¬è¿›è¡Œè¯„åˆ†ã€‚ç”±äºéœ€è¦æ”¶æ•›çš„è®­ç»ƒæ ·æœ¬æ•°é‡ä»¥åŠäººç±»é˜…è¯»å’Œæ³¨é‡Šè€…é€Ÿåº¦çš„å›ºæœ‰å»¶è¿Ÿï¼Œè¿™æ˜¯æ˜‚è´µä¸”ç¼“æ…¢çš„ã€‚
- en: 'A trick that works well instead of direct feedback is training a reward model
    on human annotations collected before the RL loop. The goal of the reward model
    is to imitate how a human would rate a text. There are several possible strategies
    to build a reward model: the most straightforward way would be to predict the
    annotation (e.g. a rating score or a binary value for â€œgoodâ€/â€badâ€). In practice,
    what works better is to predict the ranking of two examples, where the reward
    model is presented with two candidates `(y_k, y_j)` for a given prompt `x` and
    has to predict which one would be rated higher by a human annotator.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¾ˆå¥½çš„æŠ€å·§æ˜¯åœ¨RLå¾ªç¯ä¹‹å‰æ”¶é›†äººç±»æ³¨é‡Šå¹¶è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œè€Œä¸æ˜¯ç›´æ¥åé¦ˆã€‚å¥–åŠ±æ¨¡å‹çš„ç›®æ ‡æ˜¯æ¨¡ä»¿äººç±»å¦‚ä½•è¯„ä»·æ–‡æœ¬ã€‚æ„å»ºå¥–åŠ±æ¨¡å‹æœ‰å‡ ç§å¯èƒ½çš„ç­–ç•¥ï¼šæœ€ç›´æ¥çš„æ–¹å¼æ˜¯é¢„æµ‹æ³¨é‡Šï¼ˆä¾‹å¦‚è¯„åˆ†æˆ–â€œå¥½â€/â€œåâ€çš„äºŒè¿›åˆ¶å€¼ï¼‰ã€‚å®é™…ä¸Šï¼Œæ›´å¥½çš„åšæ³•æ˜¯é¢„æµ‹ä¸¤ä¸ªç¤ºä¾‹çš„æ’åï¼Œå…¶ä¸­å¥–åŠ±æ¨¡å‹å‘ˆç°ç»™å®šæç¤º`x`çš„ä¸¤ä¸ªå€™é€‰äºº`(y_k,
    y_j)`ï¼Œå¹¶ä¸”å¿…é¡»é¢„æµ‹å“ªä¸€ä¸ªä¼šè¢«äººç±»æ³¨é‡Šè€…è¯„ä¸ºæ›´é«˜ã€‚
- en: With the StackExchange dataset, we can infer which of the two answers was preferred
    by the users based on the score. With that information and the loss defined above,
    we can then modify the `transformers.Trainer` by adding a custom loss function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡StackExchangeæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ†æ•°æ¨æ–­ç”¨æˆ·æ›´å–œæ¬¢ä¸¤ä¸ªç­”æ¡ˆä¸­çš„å“ªä¸€ä¸ªã€‚æœ‰äº†è¿™äº›ä¿¡æ¯å’Œä¸Šé¢å®šä¹‰çš„æŸå¤±ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ è‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¿®æ”¹`transformers.Trainer`ã€‚
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We utilize a subset of a 100,000 pair of candidates and evaluate on a held-out
    set of 50,000\. With a modest training batch size of 4, we train the Llama model
    using the LoRA `peft` adapter for a single epoch using the Adam optimizer with
    BF16 precision. Our LoRA configuration is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ©ç”¨äº†100,000å¯¹å€™é€‰äººçš„å­é›†ï¼Œå¹¶åœ¨ä¸€ä¸ªåŒ…å«50,000ä¸ªæ ·æœ¬çš„ä¿ç•™é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä½¿ç”¨è¾ƒå°çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ä¸º4ï¼Œæˆ‘ä»¬ä½¿ç”¨LoRA `peft`é€‚é…å™¨åœ¨Adamä¼˜åŒ–å™¨å’ŒBF16ç²¾åº¦ä¸‹å¯¹Llamaæ¨¡å‹è¿›è¡Œå•ä¸ªæ—¶ä»£çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„LoRAé…ç½®æ˜¯ï¼š
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As detailed in the next section, the resulting adapter can be merged into the
    frozen model and saved for further downstream use.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹ä¸€èŠ‚è¯¦ç»†ä»‹ç»çš„é‚£æ ·ï¼Œç”Ÿæˆçš„é€‚é…å™¨å¯ä»¥åˆå¹¶åˆ°å†»ç»“æ¨¡å‹ä¸­å¹¶ä¿å­˜ä»¥ä¾›è¿›ä¸€æ­¥ä¸‹æ¸¸ä½¿ç”¨ã€‚
- en: Reinforcement Learning from Human Feedback
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ 
- en: 'With the fine-tuned language model and the reward model at hand, we are now
    ready to run the RL loop. It follows roughly three steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†ç»è¿‡å¾®è°ƒçš„è¯­è¨€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿è¡ŒRLå¾ªç¯ã€‚å¤§è‡´åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Generate responses from prompts,
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æç¤ºç”Ÿæˆå“åº”ï¼Œ
- en: Rate the responses with the reward model,
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹å“åº”è¿›è¡Œè¯„åˆ†ï¼Œ
- en: Run a reinforcement learning policy-optimization step with the ratings.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯„çº§è¿è¡Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–æ­¥éª¤ã€‚
- en: 'The Query and Response prompts are templated as follows before being tokenized
    and passed to the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å°†Queryå’ŒResponseæç¤ºæ ‡è®°åŒ–å¹¶ä¼ é€’ç»™æ¨¡å‹ä¹‹å‰ï¼Œè¿™äº›æç¤ºçš„æ¨¡æ¿å¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The same template was used for SFT, RM and RLHF stages. Once more, we utilize
    `peft` for memory-efficient training, which offers an extra advantage in the RLHF
    context. Here, the reference model and policy share the same base, the SFT model,
    which we load in 8-bit and freeze during training. We exclusively optimize the
    policyâ€™s LoRA weights using PPO while sharing the base modelâ€™s weights.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SFTã€RMå’ŒRLHFé˜¶æ®µéƒ½ä½¿ç”¨äº†ç›¸åŒçš„æ¨¡æ¿ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨`peft`è¿›è¡Œå†…å­˜é«˜æ•ˆè®­ç»ƒï¼Œåœ¨RLHFç¯å¢ƒä¸­æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ã€‚åœ¨è¿™é‡Œï¼Œå‚è€ƒæ¨¡å‹å’Œç­–ç•¥å…±äº«ç›¸åŒçš„åŸºç¡€ï¼Œå³SFTæ¨¡å‹ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»¥8ä½åŠ è½½å¹¶å†»ç»“ã€‚æˆ‘ä»¬ä¸“é—¨ä½¿ç”¨PPOä¼˜åŒ–ç­–ç•¥çš„LoRAæƒé‡ï¼ŒåŒæ—¶å…±äº«åŸºç¡€æ¨¡å‹çš„æƒé‡ã€‚
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the rest of the details and evaluation, please refer to our [blog post on
    StackLLaMA](https://huggingface.co/blog/stackllama).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³å…¶ä½™ç»†èŠ‚å’Œè¯„ä¼°ï¼Œè¯·å‚è€ƒæˆ‘ä»¬åœ¨StackLLaMAä¸Šçš„åšå®¢å¸–å­ã€‚
