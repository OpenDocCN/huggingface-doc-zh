- en: Launching your 🤗 Accelerate scripts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动您的🤗 Accelerate脚本
- en: 'Original text: [https://huggingface.co/docs/accelerate/basic_tutorials/launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/basic_tutorials/launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous tutorial, you were introduced to how to modify your current
    training script to use 🤗 Accelerate. The final version of that code is shown below:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个教程中，您已经了解了如何修改当前的训练脚本以使用🤗 Accelerate。该代码的最终版本如下所示：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But how do you run this code and have it utilize the special hardware available
    to it?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如何运行此代码并使其利用可用的特殊硬件呢？
- en: 'First, you should rewrite the above code into a function, and make it callable
    as a script. For example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您应该将上述代码重写为一个函数，并使其可调用为脚本。例如：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, you need to launch it with `accelerate launch`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要使用`accelerate launch`启动它。
- en: It’s recommended you run `accelerate config` before using `accelerate launch`
    to configure your environment to your liking. Otherwise 🤗 Accelerate will use
    very basic defaults depending on your system setup.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在使用`accelerate launch`之前运行`accelerate config`以根据您的喜好配置您的环境。否则，🤗 Accelerate将根据您的系统设置使用非常基本的默认值。
- en: Using accelerate launch
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用accelerate launch
- en: 🤗 Accelerate has a special CLI command to help you launch your code in your
    system through `accelerate launch`. This command wraps around all of the different
    commands needed to launch your script on various platforms, without you having
    to remember what each of them is.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 🤗 Accelerate具有一个特殊的CLI命令，可通过`accelerate launch`在您的系统中启动您的代码。此命令包装了在各种平台上启动脚本所需的所有不同命令，而无需记住每个命令是什么。
- en: If you are familiar with launching scripts in PyTorch yourself such as with
    `torchrun`, you can still do this. It is not required to use `accelerate launch`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉自己在PyTorch中启动脚本的方法，例如使用`torchrun`，您仍然可以这样做。不需要使用`accelerate launch`。
- en: 'You can launch your script quickly by using:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式快速启动您的脚本：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Just put `accelerate launch` at the start of your command, and pass in additional
    arguments and parameters to your script afterward like normal!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在命令的开头放置`accelerate launch`，然后像平常一样传递其他参数和参数给您的脚本！
- en: 'Since this runs the various torch spawn methods, all of the expected environment
    variables can be modified here as well. For example, here is how to use `accelerate
    launch` with a single GPU:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这会运行各种torch spawn方法，因此所有预期的环境变量也可以在此处进行修改。例如，以下是如何使用单个GPU运行`accelerate launch`：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also use `accelerate launch` without performing `accelerate config`
    first, but you may need to manually pass in the right configuration parameters.
    In this case, 🤗 Accelerate will make some hyperparameter decisions for you, e.g.,
    if GPUs are available, it will use all of them by default without the mixed precision.
    Here is how you would use all GPUs and train with mixed precision disabled:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在执行`accelerate config`之前使用`accelerate launch`，但可能需要手动传递正确的配置参数。在这种情况下，🤗
    Accelerate将为您做出一些超参数决策，例如，如果有GPU可用，默认情况下将使用所有GPU而不使用混合精度。以下是如何使用所有GPU并禁用混合精度进行训练：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or by specifying a number of GPUs to use:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过指定要使用的GPU数量：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get more specific you should pass in the needed parameters yourself. For
    instance, here is how you would also launch that same script on two GPUs using
    mixed precision while avoiding all of the warnings:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要更具体，您应该自己传递所需的参数。例如，以下是如何在两个GPU上使用混合精度启动相同脚本并避免所有警告：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For a complete list of parameters you can pass in, run:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看可以传递的参数的完整列表，请运行：
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Even if you are not using 🤗 Accelerate in your code, you can still use the launcher
    for starting your scripts!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您的代码中没有使用🤗 Accelerate，您仍然可以使用启动器启动您的脚本！
- en: 'For a visualization of this difference, that earlier `accelerate launch` on
    multi-gpu would look something like so with `torchrun`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这种差异，之前在多GPU上运行的`accelerate launch`将如下所示与`torchrun`一起：
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also launch your script utilizing the launch CLI as a python module
    itself, enabling the ability to pass in other python-specific launching behaviors.
    To do so, use `accelerate.commands.launch` instead of `accelerate launch`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以启动您的脚本，利用启动CLI作为Python模块本身，从而使能够传递其他特定于Python的启动行为。为此，请使用`accelerate.commands.launch`而不是`accelerate
    launch`：
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you want to execute the script with any other python flags, you can pass
    them in as well similar to `-m`, such as the below example enabling unbuffered
    stdout and stderr:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用任何其他Python标志执行脚本，也可以将它们传递进来，类似于`-m`，例如下面的示例启用无缓冲的标准输出和标准错误：
- en: '[PRE10]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can run your code on CPU as well! This is helpful for debugging and testing
    purposes on toy models and datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在CPU上运行您的代码！这对于在玩具模型和数据集上进行调试和测试非常有帮助。
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Why you should always use accelerate config
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么您应该始终使用accelerate config
- en: Why is it useful to the point you should **always** run `accelerate config`?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它如此有用，以至于您应该**始终**运行`accelerate config`？
- en: 'Remember that earlier call to `accelerate launch` as well as `torchrun`? Post
    configuration, to run that script with the needed parts you just need to use `accelerate
    launch` outright, without passing anything else in:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 记得之前调用`accelerate launch`和`torchrun`吗？配置完成后，要运行该脚本并使用所需部分，只需直接使用`accelerate
    launch`，无需传递其他内容：
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Custom Configurations
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义配置
- en: 'As briefly mentioned earlier, `accelerate launch` should be mostly used through
    combining set configurations made with the `accelerate config` command. These
    configs are saved to a `default_config.yaml` file in your cache folder for 🤗 Accelerate.
    This cache folder is located at (with decreasing order of priority):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前简要提到的，`accelerate launch`应主要通过结合使用`accelerate config`命令进行的设置配置。这些配置保存在您的缓存文件夹中的`default_config.yaml`文件中，用于🤗
    Accelerate。此缓存文件夹位于以下位置（按优先级递减）：
- en: The content of your environment variable `HF_HOME` suffixed with `accelerate`.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用您的环境变量`HF_HOME`的内容后缀为`accelerate`。
- en: If it does not exist, the content of your environment variable `XDG_CACHE_HOME`
    suffixed with `huggingface/accelerate`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不存在，则使用您的环境变量`XDG_CACHE_HOME`的内容后缀为`huggingface/accelerate`。
- en: If this does not exist either, the folder `~/.cache/huggingface/accelerate`.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这也不存在，则文件夹`~/.cache/huggingface/accelerate`。
- en: To have multiple configurations, the flag `--config_file` can be passed to the
    `accelerate launch` command paired with the location of the custom yaml.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要有多个配置，可以将标志`--config_file`传递给`accelerate launch`命令，并与自定义yaml的位置配对。
- en: 'An example yaml may look something like the following for two GPUs on a single
    machine using `fp16` for mixed precision:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例yaml可能如下所示，用于在单台机器上使用两个GPU，并使用`fp16`进行混合精度：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Launching a script from the location of that custom yaml file looks like the
    following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从自定义yaml文件的位置启动脚本如下所示：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multi-node training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多节点训练
- en: 'Multi-node training with 🤗Accelerate is similar to [multi-node training with
    torchrun](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html).
    The simplest way to launch a multi-node training run is to do the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用🤗Accelerate进行多节点训练类似于[使用torchrun进行多节点训练](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html)。启动多节点训练运行的最简单方法是执行以下操作：
- en: Copy your codebase and data to all nodes. (or place them on a shared filesystem)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的代码库和数据复制到所有节点。（或将它们放在共享文件系统上）
- en: Setup your python packages on all nodes.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有节点上设置您的Python软件包。
- en: Run `accelerate config` on the main single node first. After specifying the
    number of nodes, you will be asked to specify the rank of each node (this will
    be 0 for the main/master node), along with the IP address and port for the main
    process. This is required for the worker nodes to communicate with the main process.
    Afterwards, you can copy or send this config file across all of your nodes, changing
    the `machine_rank` to 1, 2,3, etc. to avoid having to run the command (or just
    follow their directions directly for launching with `torchrun` as well)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先在主单节点上运行`accelerate config`。在指定节点数后，您将被要求指定每个节点的等级（主/主节点的等级将为0），以及主进程的IP地址和端口。这是为了让工作节点与主进程进行通信。之后，您可以复制或发送此配置文件到所有节点，将`machine_rank`更改为1、2、3等，以避免运行该命令（或直接按照它们的指示使用`torchrun`启动）。
- en: Once you have done this, you can start your multi-node training run by running
    `accelerate launch` (or `torchrun`) on all nodes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，您可以通过在所有节点上运行`accelerate launch`（或`torchrun`）来启动多节点训练运行。
- en: It is required that the command be ran on all nodes for everything to start,
    not just running it from the main node. You can use something like SLURM or a
    different process executor to wrap around this requirement and call everything
    from a single command.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在所有节点上运行该命令才能启动所有内容，而不仅仅是从主节点运行它。您可以使用类似SLURM或不同的进程执行器来包装这个要求，并从单个命令中调用所有内容。
- en: It is recommended to use the intranet IP of your main node over the public IP
    for better latency. This is the `192.168.x.x` or the `172.x.x.x` address you see
    when you run `hostname -I` on the main node.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用主节点的内部IP而不是公共IP以获得更好的延迟。这是当您在主节点上运行`hostname -I`时看到的`192.168.x.x`或`172.x.x.x`地址。
- en: To get a better idea about multi-node training, check out our example for [multi-node
    training with FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地了解多节点训练，请查看我们关于[multi-node training with FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp)的示例。
