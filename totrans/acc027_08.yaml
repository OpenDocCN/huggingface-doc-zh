- en: Launching your ğŸ¤— Accelerate scripts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯åŠ¨æ‚¨çš„ğŸ¤— Accelerateè„šæœ¬
- en: 'Original text: [https://huggingface.co/docs/accelerate/basic_tutorials/launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/basic_tutorials/launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous tutorial, you were introduced to how to modify your current
    training script to use ğŸ¤— Accelerate. The final version of that code is shown below:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ä¸ªæ•™ç¨‹ä¸­ï¼Œæ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä¿®æ”¹å½“å‰çš„è®­ç»ƒè„šæœ¬ä»¥ä½¿ç”¨ğŸ¤— Accelerateã€‚è¯¥ä»£ç çš„æœ€ç»ˆç‰ˆæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But how do you run this code and have it utilize the special hardware available
    to it?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚ä½•è¿è¡Œæ­¤ä»£ç å¹¶ä½¿å…¶åˆ©ç”¨å¯ç”¨çš„ç‰¹æ®Šç¡¬ä»¶å‘¢ï¼Ÿ
- en: 'First, you should rewrite the above code into a function, and make it callable
    as a script. For example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ‚¨åº”è¯¥å°†ä¸Šè¿°ä»£ç é‡å†™ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œå¹¶ä½¿å…¶å¯è°ƒç”¨ä¸ºè„šæœ¬ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, you need to launch it with `accelerate launch`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä½¿ç”¨`accelerate launch`å¯åŠ¨å®ƒã€‚
- en: Itâ€™s recommended you run `accelerate config` before using `accelerate launch`
    to configure your environment to your liking. Otherwise ğŸ¤— Accelerate will use
    very basic defaults depending on your system setup.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºè®®åœ¨ä½¿ç”¨`accelerate launch`ä¹‹å‰è¿è¡Œ`accelerate config`ä»¥æ ¹æ®æ‚¨çš„å–œå¥½é…ç½®æ‚¨çš„ç¯å¢ƒã€‚å¦åˆ™ï¼ŒğŸ¤— Accelerateå°†æ ¹æ®æ‚¨çš„ç³»ç»Ÿè®¾ç½®ä½¿ç”¨éå¸¸åŸºæœ¬çš„é»˜è®¤å€¼ã€‚
- en: Using accelerate launch
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨accelerate launch
- en: ğŸ¤— Accelerate has a special CLI command to help you launch your code in your
    system through `accelerate launch`. This command wraps around all of the different
    commands needed to launch your script on various platforms, without you having
    to remember what each of them is.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Accelerateå…·æœ‰ä¸€ä¸ªç‰¹æ®Šçš„CLIå‘½ä»¤ï¼Œå¯é€šè¿‡`accelerate launch`åœ¨æ‚¨çš„ç³»ç»Ÿä¸­å¯åŠ¨æ‚¨çš„ä»£ç ã€‚æ­¤å‘½ä»¤åŒ…è£…äº†åœ¨å„ç§å¹³å°ä¸Šå¯åŠ¨è„šæœ¬æ‰€éœ€çš„æ‰€æœ‰ä¸åŒå‘½ä»¤ï¼Œè€Œæ— éœ€è®°ä½æ¯ä¸ªå‘½ä»¤æ˜¯ä»€ä¹ˆã€‚
- en: If you are familiar with launching scripts in PyTorch yourself such as with
    `torchrun`, you can still do this. It is not required to use `accelerate launch`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ç†Ÿæ‚‰è‡ªå·±åœ¨PyTorchä¸­å¯åŠ¨è„šæœ¬çš„æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨`torchrun`ï¼Œæ‚¨ä»ç„¶å¯ä»¥è¿™æ ·åšã€‚ä¸éœ€è¦ä½¿ç”¨`accelerate launch`ã€‚
- en: 'You can launch your script quickly by using:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¿«é€Ÿå¯åŠ¨æ‚¨çš„è„šæœ¬ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Just put `accelerate launch` at the start of your command, and pass in additional
    arguments and parameters to your script afterward like normal!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€åœ¨å‘½ä»¤çš„å¼€å¤´æ”¾ç½®`accelerate launch`ï¼Œç„¶ååƒå¹³å¸¸ä¸€æ ·ä¼ é€’å…¶ä»–å‚æ•°å’Œå‚æ•°ç»™æ‚¨çš„è„šæœ¬ï¼
- en: 'Since this runs the various torch spawn methods, all of the expected environment
    variables can be modified here as well. For example, here is how to use `accelerate
    launch` with a single GPU:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™ä¼šè¿è¡Œå„ç§torch spawnæ–¹æ³•ï¼Œå› æ­¤æ‰€æœ‰é¢„æœŸçš„ç¯å¢ƒå˜é‡ä¹Ÿå¯ä»¥åœ¨æ­¤å¤„è¿›è¡Œä¿®æ”¹ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨å•ä¸ªGPUè¿è¡Œ`accelerate launch`ï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also use `accelerate launch` without performing `accelerate config`
    first, but you may need to manually pass in the right configuration parameters.
    In this case, ğŸ¤— Accelerate will make some hyperparameter decisions for you, e.g.,
    if GPUs are available, it will use all of them by default without the mixed precision.
    Here is how you would use all GPUs and train with mixed precision disabled:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥åœ¨æ‰§è¡Œ`accelerate config`ä¹‹å‰ä½¿ç”¨`accelerate launch`ï¼Œä½†å¯èƒ½éœ€è¦æ‰‹åŠ¨ä¼ é€’æ­£ç¡®çš„é…ç½®å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒğŸ¤—
    Accelerateå°†ä¸ºæ‚¨åšå‡ºä¸€äº›è¶…å‚æ•°å†³ç­–ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæœ‰GPUå¯ç”¨ï¼Œé»˜è®¤æƒ…å†µä¸‹å°†ä½¿ç”¨æ‰€æœ‰GPUè€Œä¸ä½¿ç”¨æ··åˆç²¾åº¦ã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨æ‰€æœ‰GPUå¹¶ç¦ç”¨æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒï¼š
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or by specifying a number of GPUs to use:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…é€šè¿‡æŒ‡å®šè¦ä½¿ç”¨çš„GPUæ•°é‡ï¼š
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get more specific you should pass in the needed parameters yourself. For
    instance, here is how you would also launch that same script on two GPUs using
    mixed precision while avoiding all of the warnings:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ›´å…·ä½“ï¼Œæ‚¨åº”è¯¥è‡ªå·±ä¼ é€’æ‰€éœ€çš„å‚æ•°ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨ä¸¤ä¸ªGPUä¸Šä½¿ç”¨æ··åˆç²¾åº¦å¯åŠ¨ç›¸åŒè„šæœ¬å¹¶é¿å…æ‰€æœ‰è­¦å‘Šï¼š
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For a complete list of parameters you can pass in, run:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹å¯ä»¥ä¼ é€’çš„å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·è¿è¡Œï¼š
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Even if you are not using ğŸ¤— Accelerate in your code, you can still use the launcher
    for starting your scripts!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ‚¨çš„ä»£ç ä¸­æ²¡æœ‰ä½¿ç”¨ğŸ¤— Accelerateï¼Œæ‚¨ä»ç„¶å¯ä»¥ä½¿ç”¨å¯åŠ¨å™¨å¯åŠ¨æ‚¨çš„è„šæœ¬ï¼
- en: 'For a visualization of this difference, that earlier `accelerate launch` on
    multi-gpu would look something like so with `torchrun`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯è§†åŒ–è¿™ç§å·®å¼‚ï¼Œä¹‹å‰åœ¨å¤šGPUä¸Šè¿è¡Œçš„`accelerate launch`å°†å¦‚ä¸‹æ‰€ç¤ºä¸`torchrun`ä¸€èµ·ï¼š
- en: '[PRE8]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also launch your script utilizing the launch CLI as a python module
    itself, enabling the ability to pass in other python-specific launching behaviors.
    To do so, use `accelerate.commands.launch` instead of `accelerate launch`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å¯åŠ¨æ‚¨çš„è„šæœ¬ï¼Œåˆ©ç”¨å¯åŠ¨CLIä½œä¸ºPythonæ¨¡å—æœ¬èº«ï¼Œä»è€Œä½¿èƒ½å¤Ÿä¼ é€’å…¶ä»–ç‰¹å®šäºPythonçš„å¯åŠ¨è¡Œä¸ºã€‚ä¸ºæ­¤ï¼Œè¯·ä½¿ç”¨`accelerate.commands.launch`è€Œä¸æ˜¯`accelerate
    launch`ï¼š
- en: '[PRE9]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you want to execute the script with any other python flags, you can pass
    them in as well similar to `-m`, such as the below example enabling unbuffered
    stdout and stderr:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦ä½¿ç”¨ä»»ä½•å…¶ä»–Pythonæ ‡å¿—æ‰§è¡Œè„šæœ¬ï¼Œä¹Ÿå¯ä»¥å°†å®ƒä»¬ä¼ é€’è¿›æ¥ï¼Œç±»ä¼¼äº`-m`ï¼Œä¾‹å¦‚ä¸‹é¢çš„ç¤ºä¾‹å¯ç”¨æ— ç¼“å†²çš„æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯ï¼š
- en: '[PRE10]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can run your code on CPU as well! This is helpful for debugging and testing
    purposes on toy models and datasets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥åœ¨CPUä¸Šè¿è¡Œæ‚¨çš„ä»£ç ï¼è¿™å¯¹äºåœ¨ç©å…·æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œè°ƒè¯•å’Œæµ‹è¯•éå¸¸æœ‰å¸®åŠ©ã€‚
- en: '[PRE11]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Why you should always use accelerate config
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ‚¨åº”è¯¥å§‹ç»ˆä½¿ç”¨accelerate config
- en: Why is it useful to the point you should **always** run `accelerate config`?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤æœ‰ç”¨ï¼Œä»¥è‡³äºæ‚¨åº”è¯¥**å§‹ç»ˆ**è¿è¡Œ`accelerate config`ï¼Ÿ
- en: 'Remember that earlier call to `accelerate launch` as well as `torchrun`? Post
    configuration, to run that script with the needed parts you just need to use `accelerate
    launch` outright, without passing anything else in:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—ä¹‹å‰è°ƒç”¨`accelerate launch`å’Œ`torchrun`å—ï¼Ÿé…ç½®å®Œæˆåï¼Œè¦è¿è¡Œè¯¥è„šæœ¬å¹¶ä½¿ç”¨æ‰€éœ€éƒ¨åˆ†ï¼Œåªéœ€ç›´æ¥ä½¿ç”¨`accelerate
    launch`ï¼Œæ— éœ€ä¼ é€’å…¶ä»–å†…å®¹ï¼š
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Custom Configurations
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰é…ç½®
- en: 'As briefly mentioned earlier, `accelerate launch` should be mostly used through
    combining set configurations made with the `accelerate config` command. These
    configs are saved to a `default_config.yaml` file in your cache folder for ğŸ¤— Accelerate.
    This cache folder is located at (with decreasing order of priority):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä¹‹å‰ç®€è¦æåˆ°çš„ï¼Œ`accelerate launch`åº”ä¸»è¦é€šè¿‡ç»“åˆä½¿ç”¨`accelerate config`å‘½ä»¤è¿›è¡Œçš„è®¾ç½®é…ç½®ã€‚è¿™äº›é…ç½®ä¿å­˜åœ¨æ‚¨çš„ç¼“å­˜æ–‡ä»¶å¤¹ä¸­çš„`default_config.yaml`æ–‡ä»¶ä¸­ï¼Œç”¨äºğŸ¤—
    Accelerateã€‚æ­¤ç¼“å­˜æ–‡ä»¶å¤¹ä½äºä»¥ä¸‹ä½ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§é€’å‡ï¼‰ï¼š
- en: The content of your environment variable `HF_HOME` suffixed with `accelerate`.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨çš„ç¯å¢ƒå˜é‡`HF_HOME`çš„å†…å®¹åç¼€ä¸º`accelerate`ã€‚
- en: If it does not exist, the content of your environment variable `XDG_CACHE_HOME`
    suffixed with `huggingface/accelerate`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨æ‚¨çš„ç¯å¢ƒå˜é‡`XDG_CACHE_HOME`çš„å†…å®¹åç¼€ä¸º`huggingface/accelerate`ã€‚
- en: If this does not exist either, the folder `~/.cache/huggingface/accelerate`.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™ä¹Ÿä¸å­˜åœ¨ï¼Œåˆ™æ–‡ä»¶å¤¹`~/.cache/huggingface/accelerate`ã€‚
- en: To have multiple configurations, the flag `--config_file` can be passed to the
    `accelerate launch` command paired with the location of the custom yaml.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æœ‰å¤šä¸ªé…ç½®ï¼Œå¯ä»¥å°†æ ‡å¿—`--config_file`ä¼ é€’ç»™`accelerate launch`å‘½ä»¤ï¼Œå¹¶ä¸è‡ªå®šä¹‰yamlçš„ä½ç½®é…å¯¹ã€‚
- en: 'An example yaml may look something like the following for two GPUs on a single
    machine using `fp16` for mixed precision:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç¤ºä¾‹yamlå¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼Œç”¨äºåœ¨å•å°æœºå™¨ä¸Šä½¿ç”¨ä¸¤ä¸ªGPUï¼Œå¹¶ä½¿ç”¨`fp16`è¿›è¡Œæ··åˆç²¾åº¦ï¼š
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Launching a script from the location of that custom yaml file looks like the
    following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è‡ªå®šä¹‰yamlæ–‡ä»¶çš„ä½ç½®å¯åŠ¨è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multi-node training
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šèŠ‚ç‚¹è®­ç»ƒ
- en: 'Multi-node training with ğŸ¤—Accelerate is similar to [multi-node training with
    torchrun](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html).
    The simplest way to launch a multi-node training run is to do the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤—Accelerateè¿›è¡Œå¤šèŠ‚ç‚¹è®­ç»ƒç±»ä¼¼äº[ä½¿ç”¨torchrunè¿›è¡Œå¤šèŠ‚ç‚¹è®­ç»ƒ](https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html)ã€‚å¯åŠ¨å¤šèŠ‚ç‚¹è®­ç»ƒè¿è¡Œçš„æœ€ç®€å•æ–¹æ³•æ˜¯æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Copy your codebase and data to all nodes. (or place them on a shared filesystem)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„ä»£ç åº“å’Œæ•°æ®å¤åˆ¶åˆ°æ‰€æœ‰èŠ‚ç‚¹ã€‚ï¼ˆæˆ–å°†å®ƒä»¬æ”¾åœ¨å…±äº«æ–‡ä»¶ç³»ç»Ÿä¸Šï¼‰
- en: Setup your python packages on all nodes.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šè®¾ç½®æ‚¨çš„Pythonè½¯ä»¶åŒ…ã€‚
- en: Run `accelerate config` on the main single node first. After specifying the
    number of nodes, you will be asked to specify the rank of each node (this will
    be 0 for the main/master node), along with the IP address and port for the main
    process. This is required for the worker nodes to communicate with the main process.
    Afterwards, you can copy or send this config file across all of your nodes, changing
    the `machine_rank` to 1, 2,3, etc. to avoid having to run the command (or just
    follow their directions directly for launching with `torchrun` as well)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆåœ¨ä¸»å•èŠ‚ç‚¹ä¸Šè¿è¡Œ`accelerate config`ã€‚åœ¨æŒ‡å®šèŠ‚ç‚¹æ•°åï¼Œæ‚¨å°†è¢«è¦æ±‚æŒ‡å®šæ¯ä¸ªèŠ‚ç‚¹çš„ç­‰çº§ï¼ˆä¸»/ä¸»èŠ‚ç‚¹çš„ç­‰çº§å°†ä¸º0ï¼‰ï¼Œä»¥åŠä¸»è¿›ç¨‹çš„IPåœ°å€å’Œç«¯å£ã€‚è¿™æ˜¯ä¸ºäº†è®©å·¥ä½œèŠ‚ç‚¹ä¸ä¸»è¿›ç¨‹è¿›è¡Œé€šä¿¡ã€‚ä¹‹åï¼Œæ‚¨å¯ä»¥å¤åˆ¶æˆ–å‘é€æ­¤é…ç½®æ–‡ä»¶åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼Œå°†`machine_rank`æ›´æ”¹ä¸º1ã€2ã€3ç­‰ï¼Œä»¥é¿å…è¿è¡Œè¯¥å‘½ä»¤ï¼ˆæˆ–ç›´æ¥æŒ‰ç…§å®ƒä»¬çš„æŒ‡ç¤ºä½¿ç”¨`torchrun`å¯åŠ¨ï¼‰ã€‚
- en: Once you have done this, you can start your multi-node training run by running
    `accelerate launch` (or `torchrun`) on all nodes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆæ­¤æ“ä½œåï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šè¿è¡Œ`accelerate launch`ï¼ˆæˆ–`torchrun`ï¼‰æ¥å¯åŠ¨å¤šèŠ‚ç‚¹è®­ç»ƒè¿è¡Œã€‚
- en: It is required that the command be ran on all nodes for everything to start,
    not just running it from the main node. You can use something like SLURM or a
    different process executor to wrap around this requirement and call everything
    from a single command.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šè¿è¡Œè¯¥å‘½ä»¤æ‰èƒ½å¯åŠ¨æ‰€æœ‰å†…å®¹ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»ä¸»èŠ‚ç‚¹è¿è¡Œå®ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨ç±»ä¼¼SLURMæˆ–ä¸åŒçš„è¿›ç¨‹æ‰§è¡Œå™¨æ¥åŒ…è£…è¿™ä¸ªè¦æ±‚ï¼Œå¹¶ä»å•ä¸ªå‘½ä»¤ä¸­è°ƒç”¨æ‰€æœ‰å†…å®¹ã€‚
- en: It is recommended to use the intranet IP of your main node over the public IP
    for better latency. This is the `192.168.x.x` or the `172.x.x.x` address you see
    when you run `hostname -I` on the main node.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å»ºè®®ä½¿ç”¨ä¸»èŠ‚ç‚¹çš„å†…éƒ¨IPè€Œä¸æ˜¯å…¬å…±IPä»¥è·å¾—æ›´å¥½çš„å»¶è¿Ÿã€‚è¿™æ˜¯å½“æ‚¨åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿è¡Œ`hostname -I`æ—¶çœ‹åˆ°çš„`192.168.x.x`æˆ–`172.x.x.x`åœ°å€ã€‚
- en: To get a better idea about multi-node training, check out our example for [multi-node
    training with FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ›´å¥½åœ°äº†è§£å¤šèŠ‚ç‚¹è®­ç»ƒï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬å…³äº[multi-node training with FSDP](https://huggingface.co/blog/ram-efficient-pytorch-fsdp)çš„ç¤ºä¾‹ã€‚
