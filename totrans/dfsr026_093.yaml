- en: Controlled generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation](https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/161.a99d8f42.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: Controlling outputs generated by diffusion models has been long pursued by the
    community and is now an active research topic. In many popular diffusion models,
    subtle changes in inputs, both images and text prompts, can drastically change
    outputs. In an ideal world we want to be able to control how semantics are preserved
    and changed.
  prefs: []
  type: TYPE_NORMAL
- en: Most examples of preserving semantics reduce to being able to accurately map
    a change in input to a change in output. I.e. adding an adjective to a subject
    in a prompt preserves the entire image, only modifying the changed subject. Or,
    image variation of a particular subject preserves the subject’s pose.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are qualities of generated images that we would like to
    influence beyond semantic preservation. I.e. in general, we would like our outputs
    to be of good quality, adhere to a particular style, or be realistic.
  prefs: []
  type: TYPE_NORMAL
- en: We will document some of the techniques `diffusers` supports to control generation
    of diffusion models. Much is cutting edge research and can be quite nuanced. If
    something needs clarifying or you have a suggestion, don’t hesitate to open a
    discussion on the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)
    or a [GitHub issue](https://github.com/huggingface/diffusers/issues).
  prefs: []
  type: TYPE_NORMAL
- en: We provide a high level explanation of how the generation can be controlled
    as well as a snippet of the technicals. For more in depth explanations on the
    technicals, the original papers which are linked from the pipelines are always
    the best resources.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the use case, one should choose a technique accordingly. In many
    cases, these techniques can be combined. For example, one can combine Textual
    Inversion with SEGA to provide more semantic guidance to the outputs generated
    using Textual Inversion.
  prefs: []
  type: TYPE_NORMAL
- en: Unless otherwise mentioned, these are techniques that work with existing models
    and don’t require their own weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructPix2Pix](#instruct-pix2pix)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Pix2Pix Zero](#pix2pix-zero)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Attend and Excite](#attend-and-excite)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Semantic Guidance](#semantic-guidance-sega)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Self-attention Guidance](#self-attention-guidance-sag)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Depth2Image](#depth2image)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MultiDiffusion Panorama](#multidiffusion-panorama)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DreamBooth](#dreambooth)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Textual Inversion](#textual-inversion)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[ControlNet](#controlnet)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Prompt Weighting](#prompt-weighting)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Custom Diffusion](#custom-diffusion)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Model Editing](#model-editing)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DiffEdit](#diffedit)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[T2I-Adapter](#t2i-adapter)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[FABRIC](#fabric)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For convenience, we provide a table to denote which methods are inference-only
    and which require fine-tuning/training.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Inference only** | **Requires training / fine-tuning** | **Comments**
    |'
  prefs: []
  type: TYPE_TB
- en: '| :-: | :-: | :-: | :-: |'
  prefs: []
  type: TYPE_TB
- en: '| [InstructPix2Pix](#instruct-pix2pix) | ✅ | ❌ | Can additionally be fine-tuned
    for better'
  prefs: []
  type: TYPE_NORMAL
- en: performance on specific
  prefs: []
  type: TYPE_NORMAL
- en: edit instructions. |
  prefs: []
  type: TYPE_NORMAL
- en: '| [Pix2Pix Zero](#pix2pix-zero) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Attend and Excite](#attend-and-excite) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Semantic Guidance](#semantic-guidance-sega) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Self-attention Guidance](#self-attention-guidance-sag) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Depth2Image](#depth2image) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [MultiDiffusion Panorama](#multidiffusion-panorama) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [DreamBooth](#dreambooth) | ❌ | ✅ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Textual Inversion](#textual-inversion) | ❌ | ✅ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [ControlNet](#controlnet) | ✅ | ❌ | A ControlNet can be trained/fine-tuned
    on'
  prefs: []
  type: TYPE_NORMAL
- en: a custom conditioning. |
  prefs: []
  type: TYPE_NORMAL
- en: '| [Prompt Weighting](#prompt-weighting) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Custom Diffusion](#custom-diffusion) | ❌ | ✅ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Model Editing](#model-editing) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [DiffEdit](#diffedit) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [T2I-Adapter](#t2i-adapter) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [Fabric](#fabric) | ✅ | ❌ |  |'
  prefs: []
  type: TYPE_TB
- en: InstructPix2Pix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2211.09800)'
  prefs: []
  type: TYPE_NORMAL
- en: '[InstructPix2Pix](../api/pipelines/pix2pix) is fine-tuned from Stable Diffusion
    to support editing input images. It takes as inputs an image and a prompt describing
    an edit, and it outputs the edited image. InstructPix2Pix has been explicitly
    trained to work well with [InstructGPT](https://openai.com/blog/instruction-following/)-like
    prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: Pix2Pix Zero
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2302.03027)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Pix2Pix Zero](../api/pipelines/pix2pix_zero) allows modifying an image so
    that one concept or subject is translated to another one while preserving general
    image semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: The denoising process is guided from one conceptual embedding towards another
    conceptual embedding. The intermediate latents are optimized during the denoising
    process to push the attention maps towards reference attention maps. The reference
    attention maps are from the denoising process of the input image and are used
    to encourage semantic preservation.
  prefs: []
  type: TYPE_NORMAL
- en: Pix2Pix Zero can be used both to edit synthetic images as well as real images.
  prefs: []
  type: TYPE_NORMAL
- en: To edit synthetic images, one first generates an image given a caption. Next,
    we generate image captions for the concept that shall be edited and for the new
    target concept. We can use a model like [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)
    for this purpose. Then, “mean” prompt embeddings for both the source and target
    concepts are created via the text encoder. Finally, the pix2pix-zero algorithm
    is used to edit the synthetic image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To edit a real image, one first generates an image caption using a model like
    [BLIP](https://huggingface.co/docs/transformers/model_doc/blip). Then one applies
    DDIM inversion on the prompt and image to generate “inverse” latents. Similar
    to before, “mean” prompt embeddings for both source and target concepts are created
    and finally the pix2pix-zero algorithm in combination with the “inverse” latents
    is used to edit the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pix2Pix Zero is the first model that allows “zero-shot” image editing. This
    means that the model can edit an image in less than a minute on a consumer GPU
    as shown [here](../api/pipelines/pix2pix_zero#usage-example).
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned above, Pix2Pix Zero includes optimizing the latents (and not any
    of the UNet, VAE, or the text encoder) to steer the generation toward a specific
    concept. This means that the overall pipeline might require more memory than a
    standard [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img).
  prefs: []
  type: TYPE_NORMAL
- en: An important distinction between methods like InstructPix2Pix and Pix2Pix Zero
    is that the former involves fine-tuning the pre-trained weights while the latter
    does not. This means that you can apply Pix2Pix Zero to any of the available Stable
    Diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: Attend and Excite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2301.13826)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Attend and Excite](../api/pipelines/attend_and_excite) allows subjects in
    the prompt to be faithfully represented in the final image.'
  prefs: []
  type: TYPE_NORMAL
- en: A set of token indices are given as input, corresponding to the subjects in
    the prompt that need to be present in the image. During denoising, each token
    index is guaranteed to have a minimum attention threshold for at least one patch
    of the image. The intermediate latents are iteratively optimized during the denoising
    process to strengthen the attention of the most neglected subject token until
    the attention threshold is passed for all subject tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Like Pix2Pix Zero, Attend and Excite also involves a mini optimization loop
    (leaving the pre-trained weights untouched) in its pipeline and can require more
    memory than the usual [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img).
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Guidance (SEGA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2301.12247)'
  prefs: []
  type: TYPE_NORMAL
- en: '[SEGA](../api/pipelines/semantic_stable_diffusion) allows applying or removing
    one or more concepts from an image. The strength of the concept can also be controlled.
    I.e. the smile concept can be used to incrementally increase or decrease the smile
    of a portrait.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to how classifier free guidance provides guidance via empty prompt inputs,
    SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts
    can be applied simultaneously. Each conceptual prompt can either add or remove
    their concept depending on if the guidance is applied positively or negatively.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion
    process instead of performing any explicit gradient-based optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention Guidance (SAG)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2210.00939)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-attention Guidance](../api/pipelines/self_attention_guidance) improves
    the general quality of images.'
  prefs: []
  type: TYPE_NORMAL
- en: SAG provides guidance from predictions not conditioned on high-frequency details
    to fully conditioned images. The high frequency details are extracted out of the
    UNet self-attention maps.
  prefs: []
  type: TYPE_NORMAL
- en: Depth2Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Project](https://huggingface.co/stabilityai/stable-diffusion-2-depth)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Depth2Image](../api/pipelines/stable_diffusion/depth2img) is fine-tuned from
    Stable Diffusion to better preserve semantics for text guided image variation.'
  prefs: []
  type: TYPE_NORMAL
- en: It conditions on a monocular depth estimate of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: MultiDiffusion Panorama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2302.08113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[MultiDiffusion Panorama](../api/pipelines/panorama) defines a new generation
    process over a pre-trained diffusion model. This process binds together multiple
    diffusion generation methods that can be readily applied to generate high quality
    and diverse images. Results adhere to user-provided controls, such as desired
    aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight
    segmentation masks to bounding boxes. MultiDiffusion Panorama allows to generate
    high-quality images at arbitrary aspect ratios (e.g., panoramas).'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning your own models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to pre-trained models, Diffusers has training scripts for fine-tuning
    models on user-provided data.
  prefs: []
  type: TYPE_NORMAL
- en: DreamBooth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Project](https://dreambooth.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DreamBooth](../training/dreambooth) fine-tunes a model to teach it about a
    new subject. I.e. a few pictures of a person can be used to generate images of
    that person in different styles.'
  prefs: []
  type: TYPE_NORMAL
- en: Textual Inversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2208.01618)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Textual Inversion](../training/text_inversion) fine-tunes a model to teach
    it about a new concept. I.e. a few pictures of a style of artwork can be used
    to generate images in that style.'
  prefs: []
  type: TYPE_NORMAL
- en: ControlNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2302.05543)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ControlNet](../api/pipelines/controlnet) is an auxiliary network which adds
    an extra condition. There are 8 canonical pre-trained ControlNets trained on different
    conditionings such as edge detection, scribbles, depth maps, and semantic segmentations.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Weighting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Prompt weighting](../using-diffusers/weighted_prompts) is a simple technique
    that puts more attention weight on certain parts of the text input.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2212.04488)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Custom Diffusion](../training/custom_diffusion) only fine-tunes the cross-attention
    maps of a pre-trained text-to-image diffusion model. It also allows for additionally
    performing Textual Inversion. It supports multi-concept training by design. Like
    DreamBooth and Textual Inversion, Custom Diffusion is also used to teach a pre-trained
    text-to-image diffusion model about new concepts to generate outputs involving
    the concept(s) of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Editing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2303.08084)'
  prefs: []
  type: TYPE_NORMAL
- en: The [text-to-image model editing pipeline](../api/pipelines/model_editing) helps
    you mitigate some of the incorrect implicit assumptions a pre-trained text-to-image
    diffusion model might make about the subjects present in the input prompt. For
    example, if you prompt Stable Diffusion to generate images for “A pack of roses”,
    the roses in the generated images are more likely to be red. This pipeline helps
    you change that assumption.
  prefs: []
  type: TYPE_NORMAL
- en: DiffEdit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2210.11427)'
  prefs: []
  type: TYPE_NORMAL
- en: '[DiffEdit](../api/pipelines/diffedit) allows for semantic editing of input
    images along with input prompts while preserving the original input images as
    much as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: T2I-Adapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2302.08453)'
  prefs: []
  type: TYPE_NORMAL
- en: '[T2I-Adapter](../api/pipelines/stable_diffusion/adapter) is an auxiliary network
    which adds an extra condition. There are 8 canonical pre-trained adapters trained
    on different conditionings such as edge detection, sketch, depth maps, and semantic
    segmentations.'
  prefs: []
  type: TYPE_NORMAL
- en: Fabric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Paper](https://arxiv.org/abs/2307.10159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fabric](https://github.com/huggingface/diffusers/tree/442017ccc877279bcf24fbe92f92d3d0def191b6/examples/community#stable-diffusion-fabric-pipeline)
    is a training-free approach applicable to a wide range of popular diffusion models,
    which exploits the self-attention layer present in the most widely used architectures
    to condition the diffusion process on a set of feedback images.'
  prefs: []
  type: TYPE_NORMAL
