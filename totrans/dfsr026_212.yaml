- en: Attention Processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/3.17cd7ac9.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Docstring.93f6f462.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: An attention processor is a class for applying different types of attention
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: AttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.AttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Default processor for performing attention-related computations.
  prefs: []
  type: TYPE_NORMAL
- en: AttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.AttnProcessor2_0'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0).
  prefs: []
  type: TYPE_NORMAL
- en: FusedAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.FusedAttnProcessor2_0'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0). It uses fused projection layers. For self-attention
    modules, all projection matrices (i.e., query, key, value) are fused. For cross-attention
    modules, key and value projection matrices are fused.
  prefs: []
  type: TYPE_NORMAL
- en: This API is currently ðŸ§ª experimental in nature and can change in future.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.LoRAAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha:
    Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*) â€” The number of channels in the
    `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**network_alpha** (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.LoRAAttnProcessor2_0'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha:
    Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*) â€” The number of channels in the
    `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**network_alpha** (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism using PyTorch 2.0â€™s
    memory-efficient scaled dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.CustomDiffusionAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
  prefs: []
  type: TYPE_NORMAL
- en: '( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None
    cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**train_kv** (`bool`, defaults to `True`) â€” Whether to newly train the key
    and value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**train_q_out** (`bool`, defaults to `True`) â€” Whether to newly train query
    matrices corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_bias** (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing attention for the Custom Diffusion method.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
  prefs: []
  type: TYPE_NORMAL
- en: '( train_kv: bool = True train_q_out: bool = True hidden_size: Optional = None
    cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**train_kv** (`bool`, defaults to `True`) â€” Whether to newly train the key
    and value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**train_q_out** (`bool`, defaults to `True`) â€” Whether to newly train query
    matrices corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_bias** (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing attention for the Custom Diffusion method using PyTorch
    2.0â€™s memory-efficient scaled dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: AttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.AttnAddedKVProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Processor for performing attention-related computations with extra learnable
    key and value matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: AttnAddedKVProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.AttnAddedKVProcessor2_0'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Processor for performing scaled dot-product attention (enabled by default if
    youâ€™re using PyTorch 2.0), with extra learnable key and value matrices for the
    text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_size: int cross_attention_dim: Optional = None rank: int = 4 network_alpha:
    Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**network_alpha** (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism with extra learnable
    key and value matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: XFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.XFormersAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
  prefs: []
  type: TYPE_NORMAL
- en: '( attention_op: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**attention_op** (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing memory efficient attention using xFormers.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAXFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.LoRAXFormersAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
  prefs: []
  type: TYPE_NORMAL
- en: '( hidden_size: int cross_attention_dim: int rank: int = 4 attention_op: Optional
    = None network_alpha: Optional = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*) â€” The number of channels in the
    `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank** (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_op** (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**network_alpha** (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism with memory efficient
    attention using xFormers.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionXFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
  prefs: []
  type: TYPE_NORMAL
- en: '( train_kv: bool = True train_q_out: bool = False hidden_size: Optional = None
    cross_attention_dim: Optional = None out_bias: bool = True dropout: float = 0.0
    attention_op: Optional = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**train_kv** (`bool`, defaults to `True`) â€” Whether to newly train the key
    and value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**train_q_out** (`bool`, defaults to `True`) â€” Whether to newly train query
    matrices corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_attention_dim** (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**out_bias** (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**dropout** (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_op** (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing memory efficient attention using xFormers for the
    Custom Diffusion method.
  prefs: []
  type: TYPE_NORMAL
- en: SlicedAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.SlicedAttnProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
  prefs: []
  type: TYPE_NORMAL
- en: '( slice_size: int )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**slice_size** (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing sliced attention.
  prefs: []
  type: TYPE_NORMAL
- en: SlicedAttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
  prefs: []
  type: TYPE_NORMAL
- en: ( slice_size )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**slice_size** (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing sliced attention with extra learnable key and value
    matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
