- en: Attention Processor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/attnprocessor](https://huggingface.co/docs/diffusers/api/attnprocessor)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: An attention processor is a class for applying different types of attention
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: AttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.AttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L710)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Default processor for performing attention-related computations.
  prefs: []
  type: TYPE_NORMAL
- en: AttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.AttnProcessor2_0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1182)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0).
  prefs: []
  type: TYPE_NORMAL
- en: FusedAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.FusedAttnProcessor2_0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1267)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Processor for implementing scaled dot-product attention (enabled by default
    if youâ€™re using PyTorch 2.0). It uses fused projection layers. For self-attention
    modules, all projection matrices (i.e., query, key, value) are fused. For cross-attention
    modules, key and value projection matrices are fused.
  prefs: []
  type: TYPE_NORMAL
- en: This API is currently ðŸ§ª experimental in nature and can change in future.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1803)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.LoRAAttnProcessor2_0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1875)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism using PyTorch 2.0â€™s
    memory-efficient scaled dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L779)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing attention for the Custom Diffusion method.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionAttnProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.CustomDiffusionAttnProcessor2_0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1480)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing attention for the Custom Diffusion method using PyTorch
    2.0â€™s memory-efficient scaled dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: AttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L883)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Processor for performing attention-related computations with extra learnable
    key and value matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: AttnAddedKVProcessor2_0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.AttnAddedKVProcessor2_0`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L947)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Processor for performing scaled dot-product attention (enabled by default if
    youâ€™re using PyTorch 2.0), with extra learnable key and value matrices for the
    text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAAttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.LoRAAttnAddedKVProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L2029)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism with extra learnable
    key and value matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: XFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.XFormersAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1091)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing memory efficient attention using xFormers.
  prefs: []
  type: TYPE_NORMAL
- en: LoRAXFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.LoRAXFormersAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1950)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*) â€” The hidden size of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*) â€” The number of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rank` (`int`, defaults to 4) â€” The dimension of the LoRA update matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`network_alpha` (`int`, *optional*) â€” Equivalent to `alpha` but itâ€™s usage
    is specific to Kohya (A1111) style LoRAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`dict`) â€” Additional keyword arguments to pass to the `LoRALinearLayer`
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing the LoRA attention mechanism with memory efficient
    attention using xFormers.
  prefs: []
  type: TYPE_NORMAL
- en: CustomDiffusionXFormersAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.CustomDiffusionXFormersAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1364)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`train_kv` (`bool`, defaults to `True`) â€” Whether to newly train the key and
    value matrices corresponding to the text features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_q_out` (`bool`, defaults to `True`) â€” Whether to newly train query matrices
    corresponding to the latent image features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to `None`) â€” The hidden size of
    the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attention_dim` (`int`, *optional*, defaults to `None`) â€” The number
    of channels in the `encoder_hidden_states`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_bias` (`bool`, defaults to `True`) â€” Whether to include the bias parameter
    in `train_q_out`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) â€” The dropout probability
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_op` (`Callable`, *optional*, defaults to `None`) â€” The base [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase)
    to use as the attention operator. It is recommended to set to `None`, and allow
    xFormers to choose the best operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing memory efficient attention using xFormers for the
    Custom Diffusion method.
  prefs: []
  type: TYPE_NORMAL
- en: SlicedAttnProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.SlicedAttnProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1594)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing sliced attention.
  prefs: []
  type: TYPE_NORMAL
- en: SlicedAttnAddedKVProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.attention_processor.SlicedAttnAddedKVProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/attention_processor.py#L1681)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`slice_size` (`int`, *optional*) â€” The number of steps to compute attention.
    Uses as many slices as `attention_head_dim // slice_size`, and `attention_head_dim`
    must be a multiple of the `slice_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processor for implementing sliced attention with extra learnable key and value
    matrices for the text encoder.
  prefs: []
  type: TYPE_NORMAL
