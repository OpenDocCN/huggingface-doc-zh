- en: Tokenizer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/tokenizer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'A tokenizer is in charge of preparing the inputs for a model. The library contains
    tokenizers for all the models. Most of the tokenizers are available in two flavors:
    a full python implementation and a â€œFastâ€ implementation based on the Rust library
    [ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers). The â€œFastâ€ implementations
    allows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨è´Ÿè´£ä¸ºæ¨¡å‹å‡†å¤‡è¾“å…¥ã€‚è¯¥åº“åŒ…å«æ‰€æœ‰æ¨¡å‹çš„åˆ†è¯å™¨ã€‚å¤§å¤šæ•°åˆ†è¯å™¨æœ‰ä¸¤ç§ç‰ˆæœ¬ï¼šå®Œæ•´çš„Pythonå®ç°å’ŒåŸºäºRuståº“çš„â€œå¿«é€Ÿâ€å®ç°[ğŸ¤— Tokenizers](https://github.com/huggingface/tokenizers)ã€‚
    â€œå¿«é€Ÿâ€å®ç°å…è®¸ï¼š
- en: a significant speed-up in particular when doing batched tokenization and
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯åœ¨è¿›è¡Œæ‰¹é‡åˆ†è¯æ—¶ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«é€Ÿåº¦ã€‚
- en: additional methods to map between the original string (character and words)
    and the token space (e.g. getting the index of the token comprising a given character
    or the span of characters corresponding to a given token).
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢å¤–çš„æ–¹æ³•ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œæ ‡è®°ç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„æ ‡è®°çš„ç´¢å¼•æˆ–ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚
- en: The base classes [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    and [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    implement the common methods for encoding string inputs in model inputs (see below)
    and instantiating/saving python and â€œFastâ€ tokenizers either from a local file
    or directory or from a pretrained tokenizer provided by the library (downloaded
    from HuggingFaceâ€™s AWS S3 repository). They both rely on [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)
    that contains the common methods, and [SpecialTokensMixin](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºç±»[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)å’Œ[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)å®ç°äº†å¯¹æ¨¡å‹è¾“å…¥ä¸­çš„å­—ç¬¦ä¸²è¾“å…¥è¿›è¡Œç¼–ç çš„å¸¸ç”¨æ–¹æ³•ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥ä»æœ¬åœ°æ–‡ä»¶æˆ–ç›®å½•æˆ–ä»åº“æä¾›çš„é¢„è®­ç»ƒåˆ†è¯å™¨ï¼ˆä»HuggingFaceçš„AWS
    S3å­˜å‚¨åº“ä¸‹è½½ï¼‰å®ä¾‹åŒ–/ä¿å­˜Pythonå’Œâ€œå¿«é€Ÿâ€åˆ†è¯å™¨ã€‚å®ƒä»¬éƒ½ä¾èµ–äºåŒ…å«å¸¸ç”¨æ–¹æ³•çš„[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)ï¼Œä»¥åŠ[SpecialTokensMixin](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.SpecialTokensMixin)ã€‚
- en: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    and [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    thus implement the main methods for using all the tokenizers:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)å’Œ[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)å› æ­¤å®ç°äº†ä½¿ç”¨æ‰€æœ‰åˆ†è¯å™¨çš„ä¸»è¦æ–¹æ³•ï¼š'
- en: Tokenizing (splitting strings in sub-word token strings), converting tokens
    strings to ids and back, and encoding/decoding (i.e., tokenizing and converting
    to integers).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†è¯ï¼ˆå°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå­è¯æ ‡è®°å­—ç¬¦ä¸²ï¼‰ï¼Œå°†æ ‡è®°å­—ç¬¦ä¸²è½¬æ¢ä¸ºIDå¹¶è¿”å›ï¼Œä»¥åŠç¼–ç /è§£ç ï¼ˆå³ï¼Œåˆ†è¯å’Œè½¬æ¢ä¸ºæ•´æ•°ï¼‰ã€‚
- en: Adding new tokens to the vocabulary in a way that is independent of the underlying
    structure (BPE, SentencePieceâ€¦).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»¥ç‹¬ç«‹äºåº•å±‚ç»“æ„ï¼ˆBPEï¼ŒSentencePieceç­‰ï¼‰çš„æ–¹å¼å‘è¯æ±‡è¡¨ä¸­æ·»åŠ æ–°æ ‡è®°ã€‚
- en: 'Managing special tokens (like mask, beginning-of-sentence, etc.): adding them,
    assigning them to attributes in the tokenizer for easy access and making sure
    they are not split during tokenization.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®¡ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚æ©ç ï¼Œå¥å­å¼€å¤´ç­‰ï¼‰ï¼šæ·»åŠ å®ƒä»¬ï¼Œå°†å®ƒä»¬åˆ†é…ç»™åˆ†è¯å™¨ä¸­çš„å±æ€§ä»¥ä¾¿è½»æ¾è®¿é—®ï¼Œå¹¶ç¡®ä¿å®ƒä»¬åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ä¸è¢«æ‹†åˆ†ã€‚
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    holds the output of the [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)â€™s
    encoding methods (`__call__`, `encode_plus` and `batch_encode_plus`) and is derived
    from a Python dictionary. When the tokenizer is a pure python tokenizer, this
    class behaves just like a standard python dictionary and holds the various model
    inputs computed by these methods (`input_ids`, `attention_mask`â€¦). When the tokenizer
    is a â€œFastâ€ tokenizer (i.e., backed by HuggingFace [tokenizers library](https://github.com/huggingface/tokenizers)),
    this class provides in addition several advanced alignment methods which can be
    used to map between the original string (character and words) and the token space
    (e.g., getting the index of the token comprising a given character or the span
    of characters corresponding to a given token).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ä¿å­˜äº†[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)çš„ç¼–ç æ–¹æ³•ï¼ˆ`__call__`ï¼Œ`encode_plus`å’Œ`batch_encode_plus`ï¼‰çš„è¾“å‡ºï¼Œå¹¶ä»Pythonå­—å…¸æ´¾ç”Ÿã€‚å½“åˆ†è¯å™¨æ˜¯çº¯Pythonåˆ†è¯å™¨æ—¶ï¼Œæ­¤ç±»çš„è¡Œä¸ºå°±åƒæ ‡å‡†Pythonå­—å…¸ä¸€æ ·ï¼Œå¹¶ä¿å­˜è¿™äº›æ–¹æ³•è®¡ç®—çš„å„ç§æ¨¡å‹è¾“å…¥ï¼ˆ`input_ids`ï¼Œ`attention_mask`ç­‰ï¼‰ã€‚å½“åˆ†è¯å™¨æ˜¯â€œå¿«é€Ÿâ€åˆ†è¯å™¨ï¼ˆå³ç”±HuggingFaceçš„[tokenizersåº“](https://github.com/huggingface/tokenizers)æ”¯æŒï¼‰æ—¶ï¼Œæ­¤ç±»è¿˜æä¾›äº†å‡ ç§é«˜çº§å¯¹é½æ–¹æ³•ï¼Œå¯ç”¨äºåœ¨åŸå§‹å­—ç¬¦ä¸²ï¼ˆå­—ç¬¦å’Œå•è¯ï¼‰å’Œæ ‡è®°ç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ï¼ˆä¾‹å¦‚ï¼Œè·å–åŒ…å«ç»™å®šå­—ç¬¦çš„æ ‡è®°çš„ç´¢å¼•æˆ–ä¸ç»™å®šæ ‡è®°å¯¹åº”çš„å­—ç¬¦èŒƒå›´ï¼‰ã€‚'
- en: PreTrainedTokenizer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedTokenizer
- en: '### `class transformers.PreTrainedTokenizer`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PreTrainedTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L335)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model_max_length` (`int`, *optional*) â€” The maximum length (in number of tokens)
    for the inputs to the transformer model. When the tokenizer is loaded with [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    this will be set to the value stored for the associated model in `max_model_input_sizes`
    (see above). If no value is provided, will default to VERY_LARGE_INTEGER (`int(1e30)`).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°æ•°è®¡ï¼‰ã€‚å½“ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)åŠ è½½åˆ†è¯å™¨æ—¶ï¼Œè¿™å°†è®¾ç½®ä¸ºå­˜å‚¨åœ¨`max_model_input_sizes`ä¸­çš„ç›¸å…³æ¨¡å‹çš„å€¼ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚å¦‚æœæœªæä¾›å€¼ï¼Œå°†é»˜è®¤ä¸ºVERY_LARGE_INTEGERï¼ˆ`int(1e30)`ï¼‰ã€‚'
- en: '`padding_side` (`str`, *optional*) â€” The side on which the model should have
    padding applied. Should be selected between [â€˜rightâ€™, â€˜leftâ€™]. Default value is
    picked from the class attribute of the same name.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹åº”è¯¥åº”ç”¨å¡«å……çš„ä¸€ä¾§ã€‚åº”è¯¥åœ¨[â€˜rightâ€™ï¼Œâ€˜leftâ€™]ä¹‹é—´é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`truncation_side` (`str`, *optional*) â€” The side on which the model should
    have truncation applied. Should be selected between [â€˜rightâ€™, â€˜leftâ€™]. Default
    value is picked from the class attribute of the same name.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹åº”è¯¥åº”ç”¨æˆªæ–­çš„ä¸€ä¾§ã€‚åº”è¯¥åœ¨[â€˜rightâ€™ï¼Œâ€˜leftâ€™]ä¹‹é—´é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`chat_template` (`str`, *optional*) â€” A Jinja template string that will be
    used to format lists of chat messages. See [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)
    for a full description.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨çš„Jinjaæ¨¡æ¿å­—ç¬¦ä¸²ã€‚æŸ¥çœ‹[https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)è·å–å®Œæ•´æè¿°ã€‚'
- en: '`model_input_names` (`List[string]`, *optional*) â€” The list of inputs accepted
    by the forward pass of the model (like `"token_type_ids"` or `"attention_mask"`).
    Default value is picked from the class attribute of the same name.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names`ï¼ˆ`List[string]`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨¡å‹å‰å‘ä¼ é€’æ¥å—çš„è¾“å…¥åˆ—è¡¨ï¼ˆå¦‚`"token_type_ids"`æˆ–`"attention_mask"`ï¼‰ã€‚é»˜è®¤å€¼ä»åŒåçš„ç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the beginning of a sentence. Will be associated to `self.bos_token`
    and `self.bos_token_id`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºå¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.bos_token`å’Œ`self.bos_token_id`ç›¸å…³è”ã€‚'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the end of a sentence. Will be associated to `self.eos_token` and
    `self.eos_token_id`.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºå¥å­ç»“æŸçš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.eos_token`å’Œ`self.eos_token_id`ç›¸å…³è”ã€‚'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing an out-of-vocabulary token. Will be associated to `self.unk_token`
    and `self.unk_token_id`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºè¯æ±‡å¤–æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸`self.unk_token`å’Œ`self.unk_token_id`ç›¸å…³è”ã€‚'
- en: '`sep_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    separating two different sentences in the same input (used by BERT for instance).
    Will be associated to `self.sep_token` and `self.sep_token_id`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” åœ¨åŒä¸€è¾“å…¥ä¸­åˆ†éš”ä¸¤ä¸ªä¸åŒå¥å­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚BERTä½¿ç”¨ï¼‰ã€‚å°†ä¸`self.sep_token`å’Œ`self.sep_token_id`ç›¸å…³è”ã€‚'
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation. Will be associated to `self.pad_token`
    and `self.pad_token_id`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿æ ‡è®°æ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šæ ‡è®°ã€‚ç„¶åå°†è¢«æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚å°†ä¸`self.pad_token`å’Œ`self.pad_token_id`ç›¸å…³è”ã€‚'
- en: '`cls_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the class of the input (used by BERT for instance). Will be associated
    to `self.cls_token` and `self.cls_token_id`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºè¾“å…¥ç±»åˆ«çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚BERTä½¿ç”¨ï¼‰ã€‚å°†ä¸`self.cls_token`å’Œ`self.cls_token_id`ç›¸å…³è”ã€‚'
- en: '`mask_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing a masked token (used by masked-language modeling pretraining objectives,
    like BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`ï¼Œ*å¯é€‰*ï¼‰â€” è¡¨ç¤ºæ©ç æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç”¨äºæ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ï¼Œå¦‚BERTï¼‰ã€‚å°†ä¸`self.mask_token`å’Œ`self.mask_token_id`ç›¸å…³è”ã€‚'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) â€” A tuple or a list of additional special tokens. Add them here to
    ensure they are skipped when decoding with `skip_special_tokens` is set to True.
    If they are not part of the vocabulary, they will be added at the end of the vocabulary.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens`ï¼ˆ`str`æˆ–`tokenizers.AddedToken`çš„å…ƒç»„æˆ–åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰â€” é™„åŠ ç‰¹æ®Šæ ‡è®°çš„å…ƒç»„æˆ–åˆ—è¡¨ã€‚åœ¨æ­¤æ·»åŠ å®ƒä»¬ä»¥ç¡®ä¿åœ¨è®¾ç½®`skip_special_tokens`ä¸ºTrueæ—¶è§£ç æ—¶è·³è¿‡å®ƒä»¬ã€‚å¦‚æœå®ƒä»¬ä¸æ˜¯è¯æ±‡çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡çš„æœ«å°¾ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not the model should cleanup the spaces that were added when splitting the
    input text during the tokenization process.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ¨¡å‹æ˜¯å¦åº”æ¸…é™¤åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†è¾“å…¥æ–‡æœ¬æ—¶æ·»åŠ çš„ç©ºæ ¼ã€‚'
- en: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the special tokens should be split during the tokenization process. The
    default behavior is to not split special tokens. This means that if `<s>` is the
    `bos_token`, then `tokenizer.tokenize("<s>") = [''<s>`]. Otherwise, if `split_special_tokens=True`,
    then `tokenizer.tokenize("<s>")` will be give `[''<'', ''s'', ''>'']`. This argument
    is only supported for `slow` tokenizers for the moment.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚é»˜è®¤è¡Œä¸ºæ˜¯ä¸æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ„å‘³ç€å¦‚æœ
    `<s>` æ˜¯ `bos_token`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>") = [''<s>`]`ã€‚å¦åˆ™ï¼Œå¦‚æœ `split_special_tokens=True`ï¼Œé‚£ä¹ˆ
    `tokenizer.tokenize("<s>")` å°†ä¼šç»™å‡º `[''<'', ''s'', ''>'']`ã€‚æ­¤å‚æ•°ç›®å‰ä»…æ”¯æŒ`slow`ç±»å‹çš„åˆ†è¯å™¨ã€‚'
- en: Base class for all slow tokenizers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ…¢åˆ†è¯å™¨çš„åŸºç±»ã€‚
- en: Inherits from [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§æ‰¿è‡ª[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)ã€‚
- en: Handle all the shared methods for tokenization and special tokens as well as
    methods downloading/caching/loading pretrained tokenizers as well as adding tokens
    to the vocabulary.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ‰€æœ‰ç”¨äºæ ‡è®°åŒ–å’Œç‰¹æ®Šæ ‡è®°çš„å…±äº«æ–¹æ³•ï¼Œä»¥åŠç”¨äºä¸‹è½½/ç¼“å­˜/åŠ è½½é¢„è®­ç»ƒtokenizerä»¥åŠå‘è¯æ±‡è¡¨æ·»åŠ æ ‡è®°çš„æ–¹æ³•ã€‚
- en: This class also contain the added tokens in a unified way on top of all tokenizers
    so we donâ€™t have to handle the specific vocabulary augmentation methods of the
    various underlying dictionary structures (BPE, sentencepieceâ€¦).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»è¿˜ä»¥ç»Ÿä¸€çš„æ–¹å¼åŒ…å«äº†æ‰€æœ‰tokenizerçš„æ·»åŠ æ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…å¤„ç†å„ç§åº•å±‚å­—å…¸ç»“æ„ï¼ˆBPEã€sentencepieceç­‰ï¼‰çš„ç‰¹å®šè¯æ±‡å¢å¼ºæ–¹æ³•ã€‚
- en: Class attributes (overridden by derived classes)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å±æ€§ï¼ˆç”±æ´¾ç”Ÿç±»è¦†ç›–ï¼‰
- en: '`vocab_files_names` (`Dict[str, str]`) â€” A dictionary with, as keys, the `__init__`
    keyword name of each vocabulary file required by the model, and as associated
    values, the filename for saving the associated file (string).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_files_names` (`Dict[str, str]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œå€¼æ˜¯ä¿å­˜ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” A dictionary of
    dictionaries, with the high-level keys being the `__init__` keyword name of each
    vocabulary file required by the model, the low-level being the `short-cut-names`
    of the pretrained models with, as associated values, the `url` to the associated
    pretrained vocabulary file.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” ä¸€ä¸ªå­—å…¸çš„å­—å…¸ï¼Œé«˜çº§é”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œä½çº§é”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯ç›¸å…³é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶çš„`url`ã€‚'
- en: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” A dictionary with, as
    keys, the `short-cut-names` of the pretrained models, and as associated values,
    the maximum length of the sequence inputs of this model, or `None` if the model
    has no maximum input size.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯è¯¥æ¨¡å‹çš„åºåˆ—è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæ¨¡å‹æ²¡æœ‰æœ€å¤§è¾“å…¥å¤§å°ï¼Œåˆ™ä¸º`None`ã€‚'
- en: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” A dictionary
    with, as keys, the `short-cut-names` of the pretrained models, and as associated
    values, a dictionary of specific arguments to pass to the `__init__` method of
    the tokenizer class for this pretrained model when loading the tokenizer with
    the [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    method.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œå€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶ä¼ é€’ç»™tokenizerç±»çš„`__init__`æ–¹æ³•çš„ç‰¹å®šå‚æ•°ã€‚'
- en: '`model_input_names` (`List[str]`) â€” A list of inputs expected in the forward
    pass of the model.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[str]`) â€” æ¨¡å‹å‰å‘ä¼ é€’ä¸­é¢„æœŸçš„è¾“å…¥åˆ—è¡¨ã€‚'
- en: '`padding_side` (`str`) â€” The default value for the side on which the model
    should have padding applied. Should be `''right''` or `''left''`.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`) â€” æ¨¡å‹åº”ç”¨å¡«å……çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`''right''`æˆ–`''left''`ã€‚'
- en: '`truncation_side` (`str`) â€” The default value for the side on which the model
    should have truncation applied. Should be `''right''` or `''left''`.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`) â€” æ¨¡å‹åº”ç”¨æˆªæ–­çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`''right''`æˆ–`''left''`ã€‚'
- en: '#### `__call__`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰ â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„ `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ  `bos` æˆ– `eos` æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œ`str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰ â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™å°†é€ä¸ªæ ‡è®°æˆªæ–­ï¼Œä»åºåˆ—å¯¹ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥é€šè¿‡å‚æ•° `max_length` æŒ‡å®šï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹å¯æ¥å—çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”±æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º 0ï¼‰ â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True`
    æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `False`ï¼‰ â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆåˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²ç»åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º
    `True`ï¼Œåˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ç©ºæ ¼åˆ†å‰²ï¼‰ï¼Œç„¶åè¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰ â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´» `padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›
    `>= 7.5`ï¼ˆVoltaï¼‰çš„ NVIDIA ç¡¬ä»¶ä¸Šå¯ç”¨ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰
    â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`return_token_type_ids` (`bool`, *optional*) â€” Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›tokenç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›tokenç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯tokenç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„æ ‡è®°åºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy
    = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„æ ‡è®°ã€‚'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return special tokens mask information.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç‰¹æ®Šæ ‡è®°æ©ç ä¿¡æ¯ã€‚'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›æ¯ä¸ªæ ‡è®°çš„`(char_start, char_end)`ã€‚'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Pythonâ€™s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚
- en: '`return_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) â€” Whether or not to print
    more information and warnings. **kwargs â€” passed to the `self.tokenize()` method'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•'
- en: Returns
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š
- en: '`input_ids` â€” List of token ids to be fed to a model.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„token idåˆ—è¡¨ã€‚'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`token_type_ids` â€” List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *â€œtoken_type_idsâ€* is in `self.model_input_names`).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„tokenç±»å‹idåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*â€œtoken_type_idsâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯tokenç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`attention_mask` â€” List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *â€œattention_maskâ€* is
    in `self.model_input_names`).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` â€” æŒ‡å®šå“ªäº›æ ‡è®°åº”ç”±æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*â€œattention_maskâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`overflowing_tokens` â€” List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` â€” æº¢å‡ºæ ‡è®°åºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`num_truncated_tokens` â€” Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` â€” æˆªæ–­çš„æ ‡è®°æ•°ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`special_tokens_mask` â€” List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Šæ ‡è®°ï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—æ ‡è®°ï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚'
- en: '`length` â€” The length of the inputs (when `return_length=True`)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹è¿›è¡Œåˆ†è¯å’Œå‡†å¤‡æ¨¡å‹çš„ä¸»è¦æ–¹æ³•ã€‚
- en: '#### `add_tokens`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`new_tokens` (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`)
    â€” Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken`
    wraps a string token to let you personalize its behavior: whether this token should
    only match against a single word, whether this token should strip all potential
    whitespaces on the left side, whether this token should strip all potential whitespaces
    on the right side, etc.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_tokens`ï¼ˆ`str`ï¼Œ`tokenizers.AddedToken`æˆ–*str*åˆ—è¡¨æˆ–`tokenizers.AddedToken`ï¼‰â€”
    ä»…å½“å®ƒä»¬å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰æ·»åŠ æ ‡è®°ã€‚`tokenizers.AddedToken`åŒ…è£…ä¸€ä¸ªå­—ç¬¦ä¸²æ ‡è®°ï¼Œè®©æ‚¨ä¸ªæ€§åŒ–å…¶è¡Œä¸ºï¼šæ­¤æ ‡è®°æ˜¯å¦ä»…åŒ¹é…å•ä¸ªå•è¯ï¼Œæ­¤æ ‡è®°æ˜¯å¦åº”å‰¥ç¦»å·¦ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ï¼Œæ­¤æ ‡è®°æ˜¯å¦åº”å‰¥ç¦»å³ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ç­‰ã€‚'
- en: '`special_tokens` (`bool`, *optional*, defaults to `False`) â€” Can be used to
    specify if the token is a special token. This mostly change the normalization
    behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰- å¯ç”¨äºæŒ‡å®šæ ‡è®°æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ã€‚è¿™ä¸»è¦ä¼šæ”¹å˜æ ‡å‡†åŒ–è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œç‰¹æ®Šæ ‡è®°å¦‚CLSæˆ–[MASK]é€šå¸¸ä¸ä¼šè¢«å°å†™ï¼‰ã€‚'
- en: See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨HuggingFaceåˆ†è¯å™¨åº“ä¸­æŸ¥çœ‹`tokenizers.AddedToken`çš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: Returns
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°é‡ã€‚
- en: Add a list of new tokens to the tokenizer class. If the new tokens are not in
    the vocabulary, they are added to it with indices starting from length of the
    current vocabulary and and will be isolated before the tokenization algorithm
    is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm
    are therefore not treated in the same way.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‘åˆ†è¯å™¨ç±»æ·»åŠ ä¸€ç»„æ–°æ ‡è®°ã€‚å¦‚æœæ–°æ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„é•¿åº¦å¼€å§‹ï¼Œå¹¶ä¸”åœ¨åº”ç”¨åˆ†è¯ç®—æ³•ä¹‹å‰å°†è¢«éš”ç¦»ã€‚å› æ­¤ï¼Œæ·»åŠ çš„æ ‡è®°å’Œåˆ†è¯ç®—æ³•çš„è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ä¸ä¼šä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ã€‚
- en: Note, when adding new tokens to the vocabulary, you should make sure to also
    resize the token embedding matrix of the model so that its embedding matrix matches
    the tokenizer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µï¼Œä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)æ–¹æ³•ã€‚
- en: 'Examples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#### `add_special_tokens`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`special_tokens_dict` (dictionary *str* to *str* or `tokenizers.AddedToken`)
    â€” Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`,
    `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_dict`ï¼ˆå­—å…¸*str*åˆ°*str*æˆ–`tokenizers.AddedToken`ï¼‰- é”®åº”è¯¥åœ¨é¢„å®šä¹‰ç‰¹æ®Šå±æ€§åˆ—è¡¨ä¸­ï¼š[`bos_token`ã€`eos_token`ã€`unk_token`ã€`sep_token`ã€`pad_token`ã€`cls_token`ã€`mask_token`ã€`additional_special_tokens`]ã€‚'
- en: Tokens are only added if they are not already in the vocabulary (tested by checking
    if the tokenizer assign the index of the `unk_token` to them).
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åªæœ‰å½“æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ æ ‡è®°ï¼ˆé€šè¿‡æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦å°†`unk_token`çš„ç´¢å¼•åˆ†é…ç»™å®ƒä»¬è¿›è¡Œæµ‹è¯•ï¼‰ã€‚
- en: '`replace_additional_special_tokens` (`bool`, *optional*,, defaults to `True`)
    â€” If `True`, the existing list of additional special tokens will be replaced by
    the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens`
    is just extended. In the former case, the tokens will NOT be removed from the
    tokenizerâ€™s full vocabulary - they are only being flagged as non-special tokens.
    Remember, this only affects which tokens are skipped during decoding, not the
    `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous
    `additional_special_tokens` are still added tokens, and will not be split by the
    model.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_additional_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰- å¦‚æœä¸º`True`ï¼Œåˆ™ç°æœ‰çš„é¢å¤–ç‰¹æ®Šæ ‡è®°åˆ—è¡¨å°†è¢«æ›¿æ¢ä¸º`special_tokens_dict`ä¸­æä¾›çš„åˆ—è¡¨ã€‚å¦åˆ™ï¼Œ`self._additional_special_tokens`å°†åªæ˜¯æ‰©å±•ã€‚åœ¨å‰ä¸€ç§æƒ…å†µä¸‹ï¼Œè¿™äº›æ ‡è®°ä¸ä¼šä»åˆ†è¯å™¨çš„å®Œæ•´è¯æ±‡è¡¨ä¸­åˆ é™¤-å®ƒä»¬åªè¢«æ ‡è®°ä¸ºéç‰¹æ®Šæ ‡è®°ã€‚è¯·è®°ä½ï¼Œè¿™åªå½±å“è§£ç æ—¶è·³è¿‡å“ªäº›æ ‡è®°ï¼Œè€Œä¸æ˜¯`added_tokens_encoder`å’Œ`added_tokens_decoder`ã€‚è¿™æ„å‘³ç€ä»¥å‰çš„`additional_special_tokens`ä»ç„¶æ˜¯æ·»åŠ çš„æ ‡è®°ï¼Œå¹¶ä¸”ä¸ä¼šè¢«æ¨¡å‹æ‹†åˆ†ã€‚'
- en: Returns
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°é‡ã€‚
- en: Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and
    link them to class attributes. If special tokens are NOT in the vocabulary, they
    are added to it (indexed starting from the last index of the current vocabulary).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ç¼–ç å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°å­—å…¸ï¼ˆeosã€padã€clsç­‰ï¼‰å¹¶å°†å®ƒä»¬é“¾æ¥åˆ°ç±»å±æ€§ã€‚å¦‚æœç‰¹æ®Šæ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼ˆç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„æœ€åä¸€ä¸ªç´¢å¼•å¼€å§‹ï¼‰ã€‚
- en: When adding new tokens to the vocabulary, you should make sure to also resize
    the token embedding matrix of the model so that its embedding matrix matches the
    tokenizer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”è¯¥ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µï¼Œä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)æ–¹æ³•ã€‚
- en: 'Using `add_special_tokens` will ensure your special tokens can be used in several
    ways:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`add_special_tokens`å°†ç¡®ä¿æ‚¨çš„ç‰¹æ®Šæ ‡è®°å¯ä»¥ä»¥å¤šç§æ–¹å¼ä½¿ç”¨ï¼š
- en: Special tokens can be skipped when decoding using `skip_special_tokens = True`.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è§£ç æ—¶å¯ä»¥é€šè¿‡`skip_special_tokens = True`è·³è¿‡ç‰¹æ®Šæ ‡è®°ã€‚
- en: Special tokens are carefully handled by the tokenizer (they are never split),
    similar to `AddedTokens`.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨ä¼šä»”ç»†å¤„ç†ç‰¹æ®Šæ ‡è®°ï¼ˆå®ƒä»¬æ°¸è¿œä¸ä¼šè¢«æ‹†åˆ†ï¼‰ï¼Œç±»ä¼¼äº`AddedTokens`ã€‚
- en: You can easily refer to special tokens using tokenizer class attributes like
    `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and
    fine-tuning scripts.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨åˆ†è¯å™¨ç±»å±æ€§å¦‚`tokenizer.cls_token`è½»æ¾å¼•ç”¨ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ ·å¯ä»¥è½»æ¾å¼€å‘ä¸æ¨¡å‹æ— å…³çš„è®­ç»ƒå’Œå¾®è°ƒè„šæœ¬ã€‚
- en: When possible, special tokens are already registered for provided pretrained
    models (for instance [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token` is already registered to be :obj*â€™[CLS]â€™* and XLMâ€™s one is also registered
    to be `'</s>'`).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œç‰¹æ®Šæ ‡è®°å·²ç»ä¸ºæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹æ³¨å†Œï¼ˆä¾‹å¦‚[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token`å·²ç»æ³¨å†Œä¸º:obj*â€™[CLS]â€™*ï¼ŒXLMçš„ä¸€ä¸ªä¹Ÿå·²ç»æ³¨å†Œä¸º`'</s>'`ï¼‰ã€‚
- en: 'Examples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#### `apply_chat_template`'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `apply_chat_template`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`conversation` (Union[List[Dict[str, str]], â€œConversationâ€]) â€” A Conversation
    object or list of dicts with â€œroleâ€ and â€œcontentâ€ keys, representing the chat
    history so far.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversation`ï¼ˆUnion[List[Dict[str, str]]ï¼Œâ€œConversationâ€ï¼‰â€” ä¸€ä¸ªConversationå¯¹è±¡æˆ–å…·æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨ï¼Œè¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„èŠå¤©å†å²ã€‚'
- en: '`chat_template` (str, *optional*) â€” A Jinja template to use for this conversion.
    If this is not passed, the modelâ€™s default chat template will be used instead.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`ï¼ˆstrï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ­¤è½¬æ¢çš„Jinjaæ¨¡æ¿ã€‚å¦‚æœæœªä¼ é€’æ­¤å‚æ•°ï¼Œåˆ™å°†ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤èŠå¤©æ¨¡æ¿ã€‚'
- en: '`add_generation_prompt` (bool, *optional*) â€” Whether to end the prompt with
    the token(s) that indicate the start of an assistant message. This is useful when
    you want to generate a response from the model. Note that this argument will be
    passed to the chat template, and so it must be supported in the template for this
    argument to have any effect.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_generation_prompt`ï¼ˆboolï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä»¥æŒ‡ç¤ºåŠ©æ‰‹æ¶ˆæ¯å¼€å§‹çš„æ ‡è®°ç»“æŸæç¤ºã€‚å½“æ‚¨æƒ³ä»æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚è¯·æ³¨æ„ï¼Œæ­¤å‚æ•°å°†ä¼ é€’ç»™èŠå¤©æ¨¡æ¿ï¼Œå› æ­¤æ¨¡æ¿å¿…é¡»æ”¯æŒæ­¤å‚æ•°æ‰èƒ½äº§ç”Ÿä»»ä½•æ•ˆæœã€‚'
- en: '`tokenize` (`bool`, defaults to `True`) â€” Whether to tokenize the output. If
    `False`, the output will be a string.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å‡ºè¿›è¡Œæ ‡è®°åŒ–ã€‚å¦‚æœä¸º`False`ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚'
- en: '`padding` (`bool`, defaults to `False`) â€” Whether to pad sequences to the maximum
    length. Has no effect if tokenize is `False`.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†åºåˆ—å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚'
- en: '`truncation` (`bool`, defaults to `False`) â€” Whether to truncate sequences
    at the maximum length. Has no effect if tokenize is `False`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨æœ€å¤§é•¿åº¦å¤„æˆªæ–­åºåˆ—ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Maximum length (in tokens) to use for padding
    or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizerâ€™s
    `max_length` attribute will be used as a default.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¡«å……æˆ–æˆªæ–­çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨åˆ†è¯å™¨çš„`max_length`å±æ€§ä½œä¸ºé»˜è®¤å€¼ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors of a particular framework. Has no effect
    if tokenize is `False`. Acceptable values are:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™ä¸èµ·ä½œç”¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.Tensor` objects.'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.Tensor`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›NumPy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`''jax''`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs â€” Additional
    kwargs to pass to the tokenizer.'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''jax''`ï¼šè¿”å›JAX `jnp.ndarray`å¯¹è±¡ã€‚**tokenizer_kwargs â€” è¦ä¼ é€’ç»™åˆ†è¯å™¨çš„å…¶ä»–kwargsã€‚'
- en: Returns
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: A list of token ids representing the tokenized chat so far, including control
    tokens. This output is ready to pass to the model, either directly or via methods
    like `generate()`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„æ ‡è®°åŒ–èŠå¤©çš„æ ‡è®°idåˆ—è¡¨ï¼ŒåŒ…æ‹¬æ§åˆ¶æ ‡è®°ã€‚æ­¤è¾“å‡ºå·²å‡†å¤‡å¥½ä¼ é€’ç»™æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’æˆ–é€šè¿‡`generate()`ç­‰æ–¹æ³•ä¼ é€’ã€‚
- en: Converts a Conversation object or a list of dictionaries with `"role"` and `"content"`
    keys to a list of token ids. This method is intended for use with chat models,
    and will read the tokenizerâ€™s chat_template attribute to determine the format
    and control tokens to use when converting. When chat_template is None, it will
    fall back to the default_chat_template specified at the class level.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å°†Conversationå¯¹è±¡æˆ–å¸¦æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºæ ‡è®°idåˆ—è¡¨ã€‚æ­¤æ–¹æ³•æ—¨åœ¨ä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶å°†è¯»å–åˆ†è¯å™¨çš„chat_templateå±æ€§ä»¥ç¡®å®šåœ¨è½¬æ¢æ—¶è¦ä½¿ç”¨çš„æ ¼å¼å’Œæ§åˆ¶æ ‡è®°ã€‚å½“chat_templateä¸ºNoneæ—¶ï¼Œå°†é€€å›åˆ°ç±»çº§åˆ«æŒ‡å®šçš„default_chat_templateã€‚
- en: '#### `batch_decode`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`ï¼ˆ`Union[List[int]ï¼ŒList[List[int]]ï¼Œnp.ndarrayï¼Œtorch.Tensorï¼Œtf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥idçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­åˆ—è¡¨ã€‚
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨è§£ç å°†æ ‡è®°idçš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '#### `decode`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`ï¼ˆ`Union[intï¼ŒList[int]ï¼Œnp.ndarrayï¼Œtorch.Tensorï¼Œtf.Tensor]`ï¼‰â€” æ ‡è®°åŒ–è¾“å…¥idçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str`'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The decoded sentence.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­ã€‚
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ ‡è®°å™¨å’Œè¯æ±‡è¡¨å°† id åºåˆ—è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå…·æœ‰åˆ é™¤ç‰¹æ®Šæ ‡è®°å’Œæ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼çš„é€‰é¡¹ã€‚
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºæ‰§è¡Œ `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚
- en: '#### `encode`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]` or `List[int]`) â€” The first sequence to be encoded.
    This can be a string, a list of strings (tokenized string using the `tokenize`
    method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`
    method).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`ï¼ˆ`str`ï¼Œ`List[str]` æˆ– `List[int]`ï¼‰â€” è¦ç¼–ç çš„ç¬¬ä¸€ä¸ªåºåˆ—ã€‚å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨ `tokenize`
    æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–æ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨ `convert_tokens_to_ids` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚'
- en: '`text_pair` (`str`, `List[str]` or `List[int]`, *optional*) â€” Optional second
    sequence to be encoded. This can be a string, a list of strings (tokenized string
    using the `tokenize` method) or a list of integers (tokenized string ids using
    the `convert_tokens_to_ids` method).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`ï¼ˆ`str`ï¼Œ`List[str]` æˆ– `List[int]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦ç¼–ç çš„å¯é€‰ç¬¬äºŒä¸ªåºåˆ—ã€‚å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨
    `tokenize` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–æ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨ `convert_tokens_to_ids` æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸² idï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `True`ï¼‰â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„ `PretrainedTokenizerBase.build_inputs_with_special_tokens`
    å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥ id çš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ  `bos` æˆ– `eos` æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œå¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œ`str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º
    `False`ï¼‰â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™ä¼šé€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­ç§»é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨å‚æ•° `max_length`ï¼Œæˆ–è€…ä½¿ç”¨æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸è¿›è¡Œæˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚ XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å¦‚æœè®¾ç½®ä¸ºæ•°å­—ï¼Œå¹¶ä¸”`max_length`ä¸€èµ·è®¾ç½®ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„åˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²ç»åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºNERæˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºå¯ç”¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>=
    7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šçš„Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '**kwargs â€” Passed along to the `.tokenize()` method.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs â€” ä¼ é€’ç»™`.tokenize()`æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`ï¼Œ`torch.Tensor`ï¼Œ`tf.Tensor`æˆ–`np.ndarray`'
- en: The tokenized ids of the text.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬çš„æ ‡è®°åŒ–idã€‚
- en: Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºidï¼ˆæ•´æ•°ï¼‰åºåˆ—ã€‚
- en: Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ‰§è¡Œ`self.convert_tokens_to_ids(self.tokenize(text))`ç›¸åŒã€‚
- en: '#### `push_to_hub`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your tokenizer
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id`ï¼ˆ`str`ï¼‰â€” æ‚¨è¦å°†åˆ†è¯å™¨æ¨é€åˆ°çš„å­˜å‚¨åº“åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ°Hubä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º`repo_id`çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º`True`ï¼Œå¦åˆ™ä¸º`False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload tokenizer"`.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º`"Upload tokenizer"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” åˆ›å»ºçš„å­˜å‚¨åº“æ˜¯å¦åº”ä¸ºç§æœ‰ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token`ï¼ˆ`bool`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTPä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ`huggingface-cli login`æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ä¸­ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š`repo_url`ï¼Œåˆ™é»˜è®¤ä¸º`True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size`ï¼ˆ`int`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"5GB"`ï¼‰â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹åˆ†ç‰‡å°†æ¯ä¸ªå¤§å°ä½äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚`"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤è®¾ç½®ä¸º`"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„Google
    Colabå®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½•CPU OOMé—®é¢˜ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ›å»ºå…·æœ‰ä¸Šä¼ æ–‡ä»¶çš„PRæˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸ºsafetensorsæ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚'
- en: '`revision` (`str`, *optional*) â€” Branch to push the uploaded files to.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚'
- en: '`commit_description` (`str`, *optional*) â€” The description of the commit that
    will be created'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼‰â€” å°†è¦åˆ›å»ºçš„æäº¤çš„æè¿°'
- en: '`tags` (`List[str]`, *optional*) â€” List of tags to push on the Hub.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`ï¼ˆ`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€” è¦æ¨é€åˆ°Hubä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚'
- en: Upload the tokenizer files to the ğŸ¤— Model Hub.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åˆ†è¯å™¨æ–‡ä»¶ä¸Šä¼ åˆ°ğŸ¤—æ¨¡å‹Hubã€‚
- en: 'Examples:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#### `convert_ids_to_tokens`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_ids_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L953)'
- en: '[PRE12]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`ids` (`int` or `List[int]`) â€” The token id (or token ids) to convert to tokens.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ids`ï¼ˆ`int`æˆ–`List[int]`ï¼‰â€” è¦è½¬æ¢ä¸ºæ ‡è®°çš„æ ‡è®°idï¼ˆæˆ–æ ‡è®°idï¼‰ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: Returns
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str` or `List[str]`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`str` æˆ– `List[str]`'
- en: The decoded token(s).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„æ ‡è®°ã€‚
- en: Converts a single index or a sequence of indices in a token or a sequence of
    tokens, using the vocabulary and added tokens.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯æ±‡è¡¨å’Œæ·»åŠ çš„æ ‡è®°å°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—ã€‚
- en: '#### `convert_tokens_to_ids`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L630)'
- en: '[PRE13]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokens` (`str` or `List[str]`) â€” One or several token(s) to convert to token
    id(s).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokens` (`str` æˆ– `List[str]`) â€” è¦è½¬æ¢ä¸ºæ ‡è®° ID çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡è®°ã€‚'
- en: Returns
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int` or `List[int]`'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`int` æˆ– `List[int]`'
- en: The token id or list of token ids.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®° ID æˆ–æ ‡è®° ID åˆ—è¡¨ã€‚
- en: Converts a token string (or a sequence of tokens) in a single integer id (or
    a sequence of ids), using the vocabulary.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ ‡è®°å­—ç¬¦ä¸²ï¼ˆæˆ–æ ‡è®°åºåˆ—ï¼‰è½¬æ¢ä¸ºå•ä¸ªæ•´æ•° IDï¼ˆæˆ– ID åºåˆ—ï¼‰ï¼Œä½¿ç”¨è¯æ±‡è¡¨ã€‚
- en: '#### `get_added_vocab`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_added_vocab`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L415)'
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Returns
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Dict[str, int]`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, int]`'
- en: The added tokens.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ çš„æ ‡è®°ã€‚
- en: Returns the added tokens in the vocabulary as a dictionary of token to index.
    Results might be different from the fast call because for now we always add the
    tokens even if they are already in the vocabulary. This is something we should
    change.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡è¡¨ä¸­çš„æ·»åŠ æ ‡è®°ä½œä¸ºæ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸è¿”å›ã€‚ç»“æœå¯èƒ½ä¸å¿«é€Ÿè°ƒç”¨ä¸åŒï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬æ€»æ˜¯æ·»åŠ æ ‡è®°ï¼Œå³ä½¿å®ƒä»¬å·²ç»åœ¨è¯æ±‡è¡¨ä¸­ã€‚è¿™æ˜¯æˆ‘ä»¬åº”è¯¥æ›´æ”¹çš„äº‹æƒ…ã€‚
- en: '#### `num_special_tokens_to_add`'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_special_tokens_to_add`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L518)'
- en: '[PRE15]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pair` (`bool`, *optional*, defaults to `False`) â€” Whether the number of added
    tokens should be computed in the case of a sequence pair or a single sequence.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pair` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨åºåˆ—å¯¹æˆ–å•ä¸ªåºåˆ—çš„æƒ…å†µä¸‹æ˜¯å¦åº”è®¡ç®—æ·»åŠ çš„æ ‡è®°æ•°ã€‚'
- en: Returns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of special tokens added to sequences.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°åºåˆ—ä¸­çš„ç‰¹æ®Šæ ‡è®°æ•°ã€‚
- en: Returns the number of added tokens when encoding a sequence with special tokens.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä½¿ç”¨ç‰¹æ®Šæ ‡è®°ç¼–ç åºåˆ—æ—¶æ·»åŠ çš„æ ‡è®°æ•°ã€‚
- en: This encodes a dummy input and checks the number of added tokens, and is therefore
    not efficient. Do not put this inside your training loop.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šå¯¹ä¸€ä¸ªè™šæ‹Ÿè¾“å…¥è¿›è¡Œç¼–ç å¹¶æ£€æŸ¥æ·»åŠ çš„æ ‡è®°æ•°é‡ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸è¦å°†æ­¤æ”¾åœ¨è®­ç»ƒå¾ªç¯å†…ã€‚
- en: '#### `prepare_for_tokenization`'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `prepare_for_tokenization`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L891)'
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`) â€” The text to prepare.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`) â€” è¦å‡†å¤‡çš„æ–‡æœ¬ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆæ ‡è®°åŒ–ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º
    `True`ï¼Œåˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚è¿™å¯¹äº NER æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Keyword arguments to use for the
    tokenization.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ç”¨äºæ ‡è®°åŒ–çš„å…³é”®å­—å‚æ•°ã€‚'
- en: Returns
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Tuple[str, Dict[str, Any]]`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tuple[str, Dict[str, Any]]`'
- en: The prepared text and the unused kwargs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡å¥½çš„æ–‡æœ¬å’Œæœªä½¿ç”¨çš„ kwargsã€‚
- en: Performs any necessary transformations before tokenization.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ ‡è®°åŒ–ä¹‹å‰æ‰§è¡Œä»»ä½•å¿…è¦çš„è½¬æ¢ã€‚
- en: This method should pop the arguments from kwargs and return the remaining `kwargs`
    as well. We test the `kwargs` at the end of the encoding process to be sure all
    the arguments have been used.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ–¹æ³•åº”è¯¥ä» kwargs ä¸­å¼¹å‡ºå‚æ•°å¹¶è¿”å›å‰©ä½™çš„ `kwargs`ã€‚æˆ‘ä»¬åœ¨ç¼–ç è¿‡ç¨‹ç»“æŸæ—¶æµ‹è¯• `kwargs`ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½å·²ä½¿ç”¨ã€‚
- en: '#### `tokenize`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokenize`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils.py#L541)'
- en: '[PRE17]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`) â€” The sequence to be encoded.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`) â€” è¦ç¼–ç çš„åºåˆ—ã€‚'
- en: '*`*kwargs` (additional keyword arguments) â€” Passed along to the model-specific
    `prepare_for_tokenization` preprocessing method.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*`*kwargs`ï¼ˆé¢å¤–çš„å…³é”®å­—å‚æ•°ï¼‰ â€” ä¼ é€’ç»™ç‰¹å®šäºæ¨¡å‹çš„ `prepare_for_tokenization` é¢„å¤„ç†æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of tokens.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ—è¡¨ã€‚
- en: Converts a string into a sequence of tokens, using the tokenizer.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†è¯å™¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡è®°åºåˆ—ã€‚
- en: Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies
    (BPE/SentencePieces/WordPieces). Takes care of added tokens.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‰è¯æ±‡è¡¨ä¸­çš„å•è¯æˆ–å­è¯ï¼ˆBPE/SentencePieces/WordPiecesï¼‰æ‹†åˆ†ã€‚å¤„ç†æ·»åŠ çš„æ ‡è®°ã€‚
- en: PreTrainedTokenizerFast
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PreTrainedTokenizerFast
- en: The [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    depend on the [tokenizers](https://huggingface.co/docs/tokenizers) library. The
    tokenizers obtained from the ğŸ¤— tokenizers library can be loaded very simply into
    ğŸ¤— transformers. Take a look at the [Using tokenizers from ğŸ¤— tokenizers](../fast_tokenizers)
    page to understand how this is done.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)
    ä¾èµ–äº [tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚ä» ğŸ¤— tokenizers åº“è·å–çš„
    tokenizers å¯ä»¥éå¸¸ç®€å•åœ°åŠ è½½åˆ° ğŸ¤— transformers ä¸­ã€‚æŸ¥çœ‹ [Using tokenizers from ğŸ¤— tokenizers](../fast_tokenizers)
    é¡µé¢ä»¥äº†è§£å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œã€‚'
- en: '### `class transformers.PreTrainedTokenizerFast`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PreTrainedTokenizerFast`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L77)'
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`model_max_length` (`int`, *optional*) â€” The maximum length (in number of tokens)
    for the inputs to the transformer model. When the tokenizer is loaded with [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained),
    this will be set to the value stored for the associated model in `max_model_input_sizes`
    (see above). If no value is provided, will default to VERY_LARGE_INTEGER (`int(1e30)`).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_max_length` (`int`, *optional*) â€” è¾“å…¥åˆ°å˜æ¢å™¨æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°æ•°è®¡ï¼‰ã€‚å½“ä½¿ç”¨ [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    åŠ è½½åˆ†è¯å™¨æ—¶ï¼Œè¿™å°†è®¾ç½®ä¸ºå­˜å‚¨åœ¨ `max_model_input_sizes` ä¸­å…³è”æ¨¡å‹çš„å€¼ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚å¦‚æœæœªæä¾›å€¼ï¼Œå°†é»˜è®¤ä¸º VERY_LARGE_INTEGER
    (`int(1e30)`ï¼‰ã€‚'
- en: '`padding_side` (`str`, *optional*) â€” The side on which the model should have
    padding applied. Should be selected between [â€˜rightâ€™, â€˜leftâ€™]. Default value is
    picked from the class attribute of the same name.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`, *optional*) â€” æ¨¡å‹åº”è¯¥åœ¨å“ªä¸€ä¾§åº”ç”¨å¡«å……ã€‚åº”è¯¥åœ¨ [''right'', ''left'']
    ä¸­é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`truncation_side` (`str`, *optional*) â€” The side on which the model should
    have truncation applied. Should be selected between [â€˜rightâ€™, â€˜leftâ€™]. Default
    value is picked from the class attribute of the same name.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`, *optional*) â€” æ¨¡å‹åº”è¯¥åœ¨å“ªä¸€ä¾§åº”ç”¨æˆªæ–­ã€‚åº”è¯¥åœ¨ [''right'', ''left'']
    ä¸­é€‰æ‹©ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`chat_template` (`str`, *optional*) â€” A Jinja template string that will be
    used to format lists of chat messages. See [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)
    for a full description.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template` (`str`, *optional*) â€” ä¸€ä¸ª Jinja æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œç”¨äºæ ¼å¼åŒ–èŠå¤©æ¶ˆæ¯åˆ—è¡¨ã€‚è¯¦ç»†æè¿°è¯·å‚é˜… [https://huggingface.co/docs/transformers/chat_templating](https://huggingface.co/docs/transformers/chat_templating)ã€‚'
- en: '`model_input_names` (`List[string]`, *optional*) â€” The list of inputs accepted
    by the forward pass of the model (like `"token_type_ids"` or `"attention_mask"`).
    Default value is picked from the class attribute of the same name.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[string]`, *optional*) â€” æ¨¡å‹å‰å‘ä¼ é€’æ¥å—çš„è¾“å…¥åˆ—è¡¨ï¼ˆå¦‚ `"token_type_ids"`
    æˆ– `"attention_mask"`ï¼‰ã€‚é»˜è®¤å€¼ä»åŒåç±»å±æ€§ä¸­é€‰æ‹©ã€‚'
- en: '`bos_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the beginning of a sentence. Will be associated to `self.bos_token`
    and `self.bos_token_id`.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bos_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºå¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸
    `self.bos_token` å’Œ `self.bos_token_id` å…³è”ã€‚'
- en: '`eos_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the end of a sentence. Will be associated to `self.eos_token` and
    `self.eos_token_id`.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºå¥å­ç»“å°¾çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸
    `self.eos_token` å’Œ `self.eos_token_id` å…³è”ã€‚'
- en: '`unk_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing an out-of-vocabulary token. Will be associated to `self.unk_token`
    and `self.unk_token_id`.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºè¯æ±‡å¤–æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ã€‚å°†ä¸
    `self.unk_token` å’Œ `self.unk_token_id` å…³è”ã€‚'
- en: '`sep_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    separating two different sentences in the same input (used by BERT for instance).
    Will be associated to `self.sep_token` and `self.sep_token_id`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” ç”¨äºåœ¨åŒä¸€è¾“å…¥ä¸­åˆ†éš”ä¸¤ä¸ªä¸åŒå¥å­çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚
    BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸ `self.sep_token` å’Œ `self.sep_token_id` å…³è”ã€‚'
- en: '`pad_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    used to make arrays of tokens the same size for batching purpose. Will then be
    ignored by attention mechanisms or loss computation. Will be associated to `self.pad_token`
    and `self.pad_token_id`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” ç”¨äºä½¿æ ‡è®°æ•°ç»„å¤§å°ç›¸åŒä»¥è¿›è¡Œæ‰¹å¤„ç†çš„ç‰¹æ®Šæ ‡è®°ã€‚ç„¶åå°†è¢«æ³¨æ„æœºåˆ¶æˆ–æŸå¤±è®¡ç®—å¿½ç•¥ã€‚å°†ä¸
    `self.pad_token` å’Œ `self.pad_token_id` å…³è”ã€‚'
- en: '`cls_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing the class of the input (used by BERT for instance). Will be associated
    to `self.cls_token` and `self.cls_token_id`.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºè¾“å…¥ç±»åˆ«çš„ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚
    BERT ä½¿ç”¨ï¼‰ã€‚å°†ä¸ `self.cls_token` å’Œ `self.cls_token_id` å…³è”ã€‚'
- en: '`mask_token` (`str` or `tokenizers.AddedToken`, *optional*) â€” A special token
    representing a masked token (used by masked-language modeling pretraining objectives,
    like BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str` æˆ– `tokenizers.AddedToken`, *optional*) â€” è¡¨ç¤ºæ©ç æ ‡è®°çš„ç‰¹æ®Šæ ‡è®°ï¼ˆç”¨äºæ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ï¼Œå¦‚
    BERTï¼‰ã€‚å°†ä¸ `self.mask_token` å’Œ `self.mask_token_id` å…³è”ã€‚'
- en: '`additional_special_tokens` (tuple or list of `str` or `tokenizers.AddedToken`,
    *optional*) â€” A tuple or a list of additional special tokens. Add them here to
    ensure they are skipped when decoding with `skip_special_tokens` is set to True.
    If they are not part of the vocabulary, they will be added at the end of the vocabulary.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (å…ƒç»„æˆ–åˆ—è¡¨ï¼ŒåŒ…å« `str` æˆ– `tokenizers.AddedToken`, *optional*)
    â€” é™„åŠ ç‰¹æ®Šæ ‡è®°çš„å…ƒç»„æˆ–åˆ—è¡¨ã€‚åœ¨è¿™é‡Œæ·»åŠ å®ƒä»¬ä»¥ç¡®ä¿åœ¨ `skip_special_tokens` è®¾ç½®ä¸º True æ—¶è§£ç æ—¶è·³è¿‡å®ƒä»¬ã€‚å¦‚æœå®ƒä»¬ä¸æ˜¯è¯æ±‡çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬å°†è¢«æ·»åŠ åˆ°è¯æ±‡çš„æœ«å°¾ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*, defaults to `True`) â€” Whether
    or not the model should cleanup the spaces that were added when splitting the
    input text during the tokenization process.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¯¥æ¸…é™¤åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ‹†åˆ†è¾“å…¥æ–‡æœ¬æ—¶æ·»åŠ çš„ç©ºæ ¼ã€‚'
- en: '`split_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not the special tokens should be split during the tokenization process. The
    default behavior is to not split special tokens. This means that if `<s>` is the
    `bos_token`, then `tokenizer.tokenize("<s>") = [''<s>`]. Otherwise, if `split_special_tokens=True`,
    then `tokenizer.tokenize("<s>")` will be give `[''<'', ''s'', ''>'']`. This argument
    is only supported for `slow` tokenizers for the moment.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” åœ¨æ ‡è®°åŒ–è¿‡ç¨‹ä¸­æ˜¯å¦åº”è¯¥æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚é»˜è®¤è¡Œä¸ºæ˜¯ä¸æ‹†åˆ†ç‰¹æ®Šæ ‡è®°ã€‚è¿™æ„å‘³ç€å¦‚æœ
    `<s>` æ˜¯ `bos_token`ï¼Œé‚£ä¹ˆ `tokenizer.tokenize("<s>") = [''<s>`]ã€‚å¦åˆ™ï¼Œå¦‚æœ `split_special_tokens=True`ï¼Œé‚£ä¹ˆ
    `tokenizer.tokenize("<s>")` å°†ä¼šç»™å‡º `[''<'', ''s'', ''>'']`ã€‚æ­¤å‚æ•°ç›®å‰ä»…æ”¯æŒ `slow` tokenizersã€‚'
- en: '`tokenizer_object` (`tokenizers.Tokenizer`) â€” A `tokenizers.Tokenizer` object
    from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤— tokenizers](../fast_tokenizers)
    for more information.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_object` (`tokenizers.Tokenizer`) â€” ä¸€ä¸ªæ¥è‡ªğŸ¤— tokenizersçš„`tokenizers.Tokenizer`å¯¹è±¡ï¼Œç”¨äºå®ä¾‹åŒ–ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…[ä½¿ç”¨ğŸ¤—
    tokenizers](../fast_tokenizers)ã€‚'
- en: '`tokenizer_file` (`str`) â€” A path to a local JSON file representing a previously
    serialized `tokenizers.Tokenizer` object from ğŸ¤— tokenizers.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizer_file` (`str`) â€” ä¸€ä¸ªæŒ‡å‘æœ¬åœ°JSONæ–‡ä»¶çš„è·¯å¾„ï¼Œè¡¨ç¤ºä»¥å‰åºåˆ—åŒ–çš„`tokenizers.Tokenizer`å¯¹è±¡ã€‚'
- en: Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰å¿«é€Ÿåˆ†è¯å™¨çš„åŸºç±»ï¼ˆåŒ…è£…HuggingFaceåˆ†è¯å™¨åº“ï¼‰ã€‚
- en: Inherits from [PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§æ‰¿è‡ª[PreTrainedTokenizerBase](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase)ã€‚
- en: Handles all the shared methods for tokenization and special tokens, as well
    as methods for downloading/caching/loading pretrained tokenizers, as well as adding
    tokens to the vocabulary.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ‰€æœ‰åˆ†è¯å’Œç‰¹æ®Šæ ‡è®°çš„å…±äº«æ–¹æ³•ï¼Œä»¥åŠç”¨äºä¸‹è½½/ç¼“å­˜/åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨çš„æ–¹æ³•ï¼Œä»¥åŠå‘è¯æ±‡è¡¨æ·»åŠ æ ‡è®°ã€‚
- en: This class also contains the added tokens in a unified way on top of all tokenizers
    so we donâ€™t have to handle the specific vocabulary augmentation methods of the
    various underlying dictionary structures (BPE, sentencepieceâ€¦).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç±»è¿˜ä»¥ç»Ÿä¸€çš„æ–¹å¼åŒ…å«äº†æ‰€æœ‰åˆ†è¯å™¨çš„æ·»åŠ æ ‡è®°ï¼Œå› æ­¤æˆ‘ä»¬ä¸å¿…å¤„ç†å„ç§åº•å±‚å­—å…¸ç»“æ„ï¼ˆBPEã€sentencepieceç­‰ï¼‰çš„ç‰¹å®šè¯æ±‡å¢å¼ºæ–¹æ³•ã€‚
- en: Class attributes (overridden by derived classes)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å±æ€§ï¼ˆæ´¾ç”Ÿç±»è¦†ç›–ï¼‰
- en: '`vocab_files_names` (`Dict[str, str]`) â€” A dictionary with, as keys, the `__init__`
    keyword name of each vocabulary file required by the model, and as associated
    values, the filename for saving the associated file (string).'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_files_names` (`Dict[str, str]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºæ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œç›¸å…³å€¼ä¸ºä¿å­˜å…³è”æ–‡ä»¶çš„æ–‡ä»¶åï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚'
- en: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” A dictionary of
    dictionaries, with the high-level keys being the `__init__` keyword name of each
    vocabulary file required by the model, the low-level being the `short-cut-names`
    of the pretrained models with, as associated values, the `url` to the associated
    pretrained vocabulary file.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_vocab_files_map` (`Dict[str, Dict[str, str]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé«˜çº§é”®æ˜¯æ¨¡å‹æ‰€éœ€çš„æ¯ä¸ªè¯æ±‡æ–‡ä»¶çš„`__init__`å…³é”®å­—åç§°ï¼Œä½çº§é”®æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼æ˜¯å…³è”çš„é¢„è®­ç»ƒè¯æ±‡æ–‡ä»¶çš„`url`ã€‚'
- en: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” A dictionary with, as
    keys, the `short-cut-names` of the pretrained models, and as associated values,
    the maximum length of the sequence inputs of this model, or `None` if the model
    has no maximum input size.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_model_input_sizes` (`Dict[str, Optional[int]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼ä¸ºè¯¥æ¨¡å‹çš„åºåˆ—è¾“å…¥çš„æœ€å¤§é•¿åº¦ï¼Œå¦‚æœæ¨¡å‹æ²¡æœ‰æœ€å¤§è¾“å…¥å¤§å°ï¼Œåˆ™ä¸º`None`ã€‚'
- en: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” A dictionary
    with, as keys, the `short-cut-names` of the pretrained models, and as associated
    values, a dictionary of specific arguments to pass to the `__init__` method of
    the tokenizer class for this pretrained model when loading the tokenizer with
    the [from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)
    method.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pretrained_init_configuration` (`Dict[str, Dict[str, Any]]`) â€” ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºé¢„è®­ç»ƒæ¨¡å‹çš„`short-cut-names`ï¼Œç›¸å…³å€¼ä¸ºä¼ é€’ç»™åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ—¶tokenizerç±»çš„`__init__`æ–¹æ³•çš„ç‰¹å®šå‚æ•°å­—å…¸ï¼Œä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)æ–¹æ³•ã€‚'
- en: '`model_input_names` (`List[str]`) â€” A list of inputs expected in the forward
    pass of the model.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_input_names` (`List[str]`) â€” æ¨¡å‹å‰å‘ä¼ é€’ä¸­æœŸæœ›çš„è¾“å…¥åˆ—è¡¨ã€‚'
- en: '`padding_side` (`str`) â€” The default value for the side on which the model
    should have padding applied. Should be `''right''` or `''left''`.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_side` (`str`) â€” æ¨¡å‹åº”ç”¨å¡«å……çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`''right''`æˆ–`''left''`ã€‚'
- en: '`truncation_side` (`str`) â€” The default value for the side on which the model
    should have truncation applied. Should be `''right''` or `''left''`.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_side` (`str`) â€” æ¨¡å‹åº”ç”¨æˆªæ–­çš„é»˜è®¤æ–¹å‘ã€‚åº”ä¸º`''right''`æˆ–`''left''`ã€‚'
- en: '#### `__call__`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)'
- en: '[PRE19]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence or
    batch of sequences to be encoded. Each sequence can be a string or a list of strings
    (pretokenized string). If the sequences are provided as list of strings (pretokenized),
    you must set `is_split_into_words=True` (to lift the ambiguity with a batch of
    sequences).'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded. Each sequence can be a string or a list of
    strings (pretokenized string). If the sequences are provided as list of strings
    (pretokenized), you must set `is_split_into_words=True` (to lift the ambiguity
    with a batch of sequences).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The sequence
    or batch of sequences to be encoded as target texts. Each sequence can be a string
    or a list of strings (pretokenized string). If the sequences are provided as list
    of strings (pretokenized), you must set `is_split_into_words=True` (to lift the
    ambiguity with a batch of sequences).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯çš„ï¼‰ï¼Œå¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹æ¬¡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” The
    sequence or batch of sequences to be encoded as target texts. Each sequence can
    be a string or a list of strings (pretokenized string). If the sequences are provided
    as list of strings (pretokenized), you must set `is_split_into_words=True` (to
    lift the ambiguity with a batch of sequences).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰çš„å½¢å¼æä¾›ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸åºåˆ—æ‰¹æ¬¡çš„æ­§ä¹‰ï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥idçš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest''`: å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆæˆ–å¦‚æœåªæä¾›äº†å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œå¡«å……ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` æˆ– `''longest_first''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸­åˆ é™¤æœ€é•¿åºåˆ—ä¸­çš„ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`: ä½¿ç”¨å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` æˆ– `''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *å¯é€‰*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º0) â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œå½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«è¢«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„åˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>=
    7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`return_token_type_ids` (`bool`, *optional*) â€” Whether to return token type
    IDs. If left to the default, will return the token type IDs according to the specific
    tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_token_type_ids` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›tokenç±»å‹IDã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›tokenç±»å‹IDï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯tokenç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`return_attention_mask` (`bool`, *optional*) â€” Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific tokenizerâ€™s default, defined by the `return_outputs` attribute.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_attention_mask` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šåˆ†è¯å™¨çš„é»˜è®¤è®¾ç½®è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`return_overflowing_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return overflowing token sequences. If a pair of sequences of input
    ids (or a batch of pairs) is provided with `truncation_strategy = longest_first`
    or `True`, an error is raised instead of returning overflowing tokens.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_overflowing_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„tokenåºåˆ—ã€‚å¦‚æœæä¾›ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy
    = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„tokenã€‚'
- en: '`return_special_tokens_mask` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return special tokens mask information.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_special_tokens_mask` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç‰¹æ®Štokenæ©ç ä¿¡æ¯ã€‚'
- en: '`return_offsets_mapping` (`bool`, *optional*, defaults to `False`) â€” Whether
    or not to return `(char_start, char_end)` for each token.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_offsets_mapping` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›æ¯ä¸ªtokençš„`(char_start,
    char_end)`ã€‚'
- en: This is only available on fast tokenizers inheriting from [PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast),
    if using Pythonâ€™s tokenizer, this method will raise `NotImplementedError`.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿåˆ†è¯å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„åˆ†è¯å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚
- en: '`return_length` (`bool`, *optional*, defaults to `False`) â€” Whether or not
    to return the lengths of the encoded inputs.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_length` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚'
- en: '`verbose` (`bool`, *optional*, defaults to `True`) â€” Whether or not to print
    more information and warnings. **kwargs â€” passed to the `self.tokenize()` method'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•'
- en: Returns
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: 'A [BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)
    with the following fields:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š
- en: '`input_ids` â€” List of token ids to be fed to a model.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„token idåˆ—è¡¨ã€‚'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)'
- en: '`token_type_ids` â€” List of token type ids to be fed to a model (when `return_token_type_ids=True`
    or if *â€œtoken_type_idsâ€* is in `self.model_input_names`).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` â€” è¦é¦ˆé€ç»™æ¨¡å‹çš„tokenç±»å‹idåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*â€œtoken_type_idsâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯tokenç±»å‹IDï¼Ÿ](../glossary#token-type-ids)'
- en: '`attention_mask` â€” List of indices specifying which tokens should be attended
    to by the model (when `return_attention_mask=True` or if *â€œattention_maskâ€* is
    in `self.model_input_names`).'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` â€” æŒ‡å®šå“ªäº›tokenåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*â€œattention_maskâ€*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚'
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)'
- en: '`overflowing_tokens` â€” List of overflowing tokens sequences (when a `max_length`
    is specified and `return_overflowing_tokens=True`).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`overflowing_tokens` â€” æº¢å‡ºtokenåºåˆ—çš„åˆ—è¡¨ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`num_truncated_tokens` â€” Number of tokens truncated (when a `max_length` is
    specified and `return_overflowing_tokens=True`).'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_truncated_tokens` â€” æˆªæ–­çš„tokenæ•°é‡ï¼ˆå½“æŒ‡å®š`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚'
- en: '`special_tokens_mask` â€” List of 0s and 1s, with 1 specifying added special
    tokens and 0 specifying regular sequence tokens (when `add_special_tokens=True`
    and `return_special_tokens_mask=True`).'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Štokenï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—tokenï¼ˆå½“`add_special_tokens=True`å’Œ`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚'
- en: '`length` â€” The length of the inputs (when `return_length=True`)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    or one or several pair(s) of sequences.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸»è¦æ–¹æ³•æ ‡è®°åŒ–å¹¶ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹ã€‚
- en: '#### `add_tokens`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L975)'
- en: '[PRE20]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`new_tokens` (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`)
    â€” Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken`
    wraps a string token to let you personalize its behavior: whether this token should
    only match against a single word, whether this token should strip all potential
    whitespaces on the left side, whether this token should strip all potential whitespaces
    on the right side, etc.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_tokens` (`str`, `tokenizers.AddedToken`æˆ–*str*åˆ—è¡¨æˆ–`tokenizers.AddedToken`)
    â€” ä»…å½“è¿™äº›æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ è¿™äº›æ ‡è®°ã€‚`tokenizers.AddedToken`å°†å­—ç¬¦ä¸²æ ‡è®°åŒ…è£…èµ·æ¥ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä¸ªæ€§åŒ–å…¶è¡Œä¸ºï¼šè¿™ä¸ªæ ‡è®°æ˜¯å¦åªåŒ¹é…å•ä¸ªå•è¯ï¼Œè¿™ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥å»é™¤å·¦ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ï¼Œè¿™ä¸ªæ ‡è®°æ˜¯å¦åº”è¯¥å»é™¤å³ä¾§çš„æ‰€æœ‰æ½œåœ¨ç©ºæ ¼ç­‰ã€‚'
- en: '`special_tokens` (`bool`, *optional*, defaults to `False`) â€” Can be used to
    specify if the token is a special token. This mostly change the normalization
    behavior (special tokens like CLS or [MASK] are usually not lower-cased for instance).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” å¯ç”¨äºæŒ‡å®šè¯¥æ ‡è®°æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°ã€‚è¿™ä¸»è¦ä¼šæ”¹å˜æ ‡å‡†åŒ–è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œç‰¹æ®Šæ ‡è®°å¦‚CLSæˆ–[MASK]é€šå¸¸ä¸ä¼šè¢«è½¬æ¢ä¸ºå°å†™ï¼‰ã€‚'
- en: See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨HuggingFaceåˆ†è¯å™¨åº“ä¸­æŸ¥çœ‹`tokenizers.AddedToken`çš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: Returns
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°ã€‚
- en: Add a list of new tokens to the tokenizer class. If the new tokens are not in
    the vocabulary, they are added to it with indices starting from length of the
    current vocabulary and and will be isolated before the tokenization algorithm
    is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm
    are therefore not treated in the same way.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: å‘åˆ†è¯å™¨ç±»æ·»åŠ æ–°æ ‡è®°åˆ—è¡¨ã€‚å¦‚æœæ–°æ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°å…¶ä¸­ï¼Œç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„é•¿åº¦å¼€å§‹ï¼Œå¹¶ä¸”åœ¨åº”ç”¨åˆ†è¯ç®—æ³•ä¹‹å‰å°†è¢«éš”ç¦»ã€‚å› æ­¤ï¼Œæ·»åŠ çš„æ ‡è®°å’Œåˆ†è¯ç®—æ³•çš„è¯æ±‡è¡¨ä¸­çš„æ ‡è®°ä¸ä¼šä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ã€‚
- en: Note, when adding new tokens to the vocabulary, you should make sure to also
    resize the token embedding matrix of the model so that its embedding matrix matches
    the tokenizer.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µå¤§å°ï¼Œä»¥ä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)æ–¹æ³•ã€‚
- en: 'Examples:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE21]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#### `add_special_tokens`'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `add_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L873)'
- en: '[PRE22]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`special_tokens_dict` (dictionary *str* to *str* or `tokenizers.AddedToken`)
    â€” Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`,
    `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_dict`ï¼ˆå­—å…¸*str*åˆ°*str*æˆ–`tokenizers.AddedToken`ï¼‰ â€” é”®åº”åœ¨é¢„å®šä¹‰ç‰¹æ®Šå±æ€§åˆ—è¡¨ä¸­ï¼š[`bos_token`,
    `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,
    `additional_special_tokens`]ã€‚'
- en: Tokens are only added if they are not already in the vocabulary (tested by checking
    if the tokenizer assign the index of the `unk_token` to them).
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä»…å½“è¿™äº›æ ‡è®°å°šæœªåœ¨è¯æ±‡è¡¨ä¸­æ—¶æ‰ä¼šæ·»åŠ è¿™äº›æ ‡è®°ï¼ˆé€šè¿‡æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦å°†`unk_token`çš„ç´¢å¼•åˆ†é…ç»™å®ƒä»¬è¿›è¡Œæµ‹è¯•ï¼‰ã€‚
- en: '`replace_additional_special_tokens` (`bool`, *optional*,, defaults to `True`)
    â€” If `True`, the existing list of additional special tokens will be replaced by
    the list provided in `special_tokens_dict`. Otherwise, `self._additional_special_tokens`
    is just extended. In the former case, the tokens will NOT be removed from the
    tokenizerâ€™s full vocabulary - they are only being flagged as non-special tokens.
    Remember, this only affects which tokens are skipped during decoding, not the
    `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous
    `additional_special_tokens` are still added tokens, and will not be split by the
    model.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_additional_special_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” å¦‚æœä¸º`True`ï¼Œåˆ™ç°æœ‰çš„é¢å¤–ç‰¹æ®Šæ ‡è®°åˆ—è¡¨å°†è¢«`special_tokens_dict`ä¸­æä¾›çš„åˆ—è¡¨æ›¿æ¢ã€‚å¦åˆ™ï¼Œ`self._additional_special_tokens`å°†ä»…è¢«æ‰©å±•ã€‚åœ¨å‰ä¸€ç§æƒ…å†µä¸‹ï¼Œè¿™äº›æ ‡è®°å°†ä¸ä¼šä»åˆ†è¯å™¨çš„å®Œæ•´è¯æ±‡è¡¨ä¸­åˆ é™¤
    - å®ƒä»¬åªè¢«æ ‡è®°ä¸ºéç‰¹æ®Šæ ‡è®°ã€‚è¯·è®°ä½ï¼Œè¿™ä»…å½±å“è§£ç è¿‡ç¨‹ä¸­è·³è¿‡å“ªäº›æ ‡è®°ï¼Œè€Œä¸å½±å“`added_tokens_encoder`å’Œ`added_tokens_decoder`ã€‚è¿™æ„å‘³ç€ä»¥å‰çš„`additional_special_tokens`ä»ç„¶æ˜¯æ·»åŠ çš„æ ‡è®°ï¼Œå¹¶ä¸”ä¸ä¼šè¢«æ¨¡å‹æ‹†åˆ†ã€‚'
- en: Returns
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of tokens added to the vocabulary.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„æ ‡è®°æ•°ã€‚
- en: Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and
    link them to class attributes. If special tokens are NOT in the vocabulary, they
    are added to it (indexed starting from the last index of the current vocabulary).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ç¼–ç å™¨æ·»åŠ ç‰¹æ®Šæ ‡è®°å­—å…¸ï¼ˆeosï¼Œpadï¼Œclsç­‰ï¼‰å¹¶å°†å®ƒä»¬é“¾æ¥åˆ°ç±»å±æ€§ã€‚å¦‚æœç‰¹æ®Šæ ‡è®°ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œåˆ™å®ƒä»¬å°†è¢«æ·»åŠ åˆ°å…¶ä¸­ï¼ˆç´¢å¼•ä»å½“å‰è¯æ±‡è¡¨çš„æœ€åä¸€ä¸ªç´¢å¼•å¼€å§‹ï¼‰ã€‚
- en: When adding new tokens to the vocabulary, you should make sure to also resize
    the token embedding matrix of the model so that its embedding matrix matches the
    tokenizer.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‘è¯æ±‡è¡¨æ·»åŠ æ–°æ ‡è®°æ—¶ï¼Œæ‚¨åº”ç¡®ä¿è¿˜è°ƒæ•´æ¨¡å‹çš„æ ‡è®°åµŒå…¥çŸ©é˜µå¤§å°ï¼Œä»¥ä½¿å…¶åµŒå…¥çŸ©é˜µä¸åˆ†è¯å™¨åŒ¹é…ã€‚
- en: In order to do that, please use the [resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)
    method.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œè¯·ä½¿ç”¨[resize_token_embeddings()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings)æ–¹æ³•ã€‚
- en: 'Using `add_special_tokens` will ensure your special tokens can be used in several
    ways:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`add_special_tokens`å°†ç¡®ä¿æ‚¨çš„ç‰¹æ®Šæ ‡è®°å¯ä»¥ä»¥å¤šç§æ–¹å¼ä½¿ç”¨ï¼š
- en: Special tokens can be skipped when decoding using `skip_special_tokens = True`.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£ç æ—¶å¯ä»¥é€šè¿‡`skip_special_tokens = True`è·³è¿‡ç‰¹æ®Šæ ‡è®°ã€‚
- en: Special tokens are carefully handled by the tokenizer (they are never split),
    similar to `AddedTokens`.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šæ ‡è®°ç”±åˆ†è¯å™¨ä»”ç»†å¤„ç†ï¼ˆå®ƒä»¬æ°¸è¿œä¸ä¼šè¢«æ‹†åˆ†ï¼‰ï¼Œç±»ä¼¼äº`AddedTokens`ã€‚
- en: You can easily refer to special tokens using tokenizer class attributes like
    `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and
    fine-tuning scripts.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡åˆ†è¯å™¨ç±»å±æ€§ï¼ˆå¦‚`tokenizer.cls_token`ï¼‰è½»æ¾å¼•ç”¨ç‰¹æ®Šæ ‡è®°ã€‚è¿™ä½¿å¾—å¼€å‘ä¸æ¨¡å‹æ— å…³çš„è®­ç»ƒå’Œå¾®è°ƒè„šæœ¬å˜å¾—å®¹æ˜“ã€‚
- en: When possible, special tokens are already registered for provided pretrained
    models (for instance [BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token` is already registered to be :obj*â€™[CLS]â€™* and XLMâ€™s one is also registered
    to be `'</s>'`).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå·²ç»ä¸ºæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹æ³¨å†Œäº†ç‰¹æ®Šæ ‡è®°ï¼ˆä¾‹å¦‚[BertTokenizer](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizer)
    `cls_token`å·²ç»æ³¨å†Œä¸ºï¼šobj*â€™[CLS]â€™*ï¼ŒXLMçš„ä¸€ä¸ªä¹Ÿå·²ç»æ³¨å†Œä¸º`'</s>'`ï¼‰ã€‚
- en: 'Examples:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE23]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#### `apply_chat_template`'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `apply_chat_template`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L1678)'
- en: '[PRE24]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Parameters
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`conversation` (Union[List[Dict[str, str]], â€œConversationâ€]) â€” A Conversation
    object or list of dicts with â€œroleâ€ and â€œcontentâ€ keys, representing the chat
    history so far.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conversation`ï¼ˆUnion[List[Dict[str, str]], â€œConversationâ€ï¼‰â€” ä¸€ä¸ªConversationå¯¹è±¡æˆ–å¸¦æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨ï¼Œè¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢çš„èŠå¤©å†å²ã€‚'
- en: '`chat_template` (str, *optional*) â€” A Jinja template to use for this conversion.
    If this is not passed, the modelâ€™s default chat template will be used instead.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chat_template`ï¼ˆstrï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºæ­¤è½¬æ¢çš„Jinjaæ¨¡æ¿ã€‚å¦‚æœæœªä¼ é€’æ­¤å‚æ•°ï¼Œåˆ™å°†ä½¿ç”¨æ¨¡å‹çš„é»˜è®¤èŠå¤©æ¨¡æ¿ã€‚'
- en: '`add_generation_prompt` (bool, *optional*) â€” Whether to end the prompt with
    the token(s) that indicate the start of an assistant message. This is useful when
    you want to generate a response from the model. Note that this argument will be
    passed to the chat template, and so it must be supported in the template for this
    argument to have any effect.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_generation_prompt`ï¼ˆboolï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦ä»¥æŒ‡ç¤ºåŠ©æ‰‹æ¶ˆæ¯å¼€å§‹çš„æ ‡è®°ç»“æŸæç¤ºã€‚å½“æ‚¨æƒ³è¦ä»æ¨¡å‹ç”Ÿæˆå“åº”æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚è¯·æ³¨æ„ï¼Œæ­¤å‚æ•°å°†ä¼ é€’ç»™èŠå¤©æ¨¡æ¿ï¼Œå› æ­¤æ¨¡æ¿å¿…é¡»æ”¯æŒæ­¤å‚æ•°æ‰èƒ½äº§ç”Ÿä»»ä½•æ•ˆæœã€‚'
- en: '`tokenize` (`bool`, defaults to `True`) â€” Whether to tokenize the output. If
    `False`, the output will be a string.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å‡ºè¿›è¡Œåˆ†è¯ã€‚å¦‚æœä¸º`False`ï¼Œè¾“å‡ºå°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚'
- en: '`padding` (`bool`, defaults to `False`) â€” Whether to pad sequences to the maximum
    length. Has no effect if tokenize is `False`.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦å°†åºåˆ—å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚'
- en: '`truncation` (`bool`, defaults to `False`) â€” Whether to truncate sequences
    at the maximum length. Has no effect if tokenize is `False`.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨æœ€å¤§é•¿åº¦å¤„æˆªæ–­åºåˆ—ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚'
- en: '`max_length` (`int`, *optional*) â€” Maximum length (in tokens) to use for padding
    or truncation. Has no effect if tokenize is `False`. If not specified, the tokenizerâ€™s
    `max_length` attribute will be used as a default.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¡«å……æˆ–æˆªæ–­çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨åˆ†è¯å™¨çš„`max_length`å±æ€§ä½œä¸ºé»˜è®¤å€¼ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors of a particular framework. Has no effect
    if tokenize is `False`. Acceptable values are:'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›ç‰¹å®šæ¡†æ¶çš„å¼ é‡ã€‚å¦‚æœtokenizeä¸º`False`ï¼Œåˆ™æ— æ•ˆã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.Tensor` objects.'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`ï¼šè¿”å›TensorFlow `tf.Tensor`å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`ï¼šè¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚'
- en: '`''np''`: Return NumPy `np.ndarray` objects.'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`ï¼šè¿”å›NumPy `np.ndarray`å¯¹è±¡ã€‚'
- en: '`''jax''`: Return JAX `jnp.ndarray` objects. **tokenizer_kwargs â€” Additional
    kwargs to pass to the tokenizer.'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''jax''`ï¼šè¿”å›JAX `jnp.ndarray`å¯¹è±¡ã€‚**tokenizer_kwargs â€” ä¼ é€’ç»™åˆ†è¯å™¨çš„å…¶ä»–kwargsã€‚'
- en: Returns
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[int]`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: A list of token ids representing the tokenized chat so far, including control
    tokens. This output is ready to pass to the model, either directly or via methods
    like `generate()`.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºåˆ°ç›®å‰ä¸ºæ­¢æ ‡è®°åŒ–èŠå¤©çš„æ ‡è®°idåˆ—è¡¨ï¼ŒåŒ…æ‹¬æ§åˆ¶æ ‡è®°ã€‚æ­¤è¾“å‡ºå·²å‡†å¤‡å¥½ä¼ é€’ç»™æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’ï¼Œä¹Ÿå¯ä»¥é€šè¿‡`generate()`ç­‰æ–¹æ³•ä¼ é€’ã€‚
- en: Converts a Conversation object or a list of dictionaries with `"role"` and `"content"`
    keys to a list of token ids. This method is intended for use with chat models,
    and will read the tokenizerâ€™s chat_template attribute to determine the format
    and control tokens to use when converting. When chat_template is None, it will
    fall back to the default_chat_template specified at the class level.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: å°†Conversationå¯¹è±¡æˆ–å¸¦æœ‰â€œroleâ€å’Œâ€œcontentâ€é”®çš„å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºæ ‡è®°idåˆ—è¡¨ã€‚æ­¤æ–¹æ³•æ—¨åœ¨ä¸èŠå¤©æ¨¡å‹ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶å°†è¯»å–åˆ†è¯å™¨çš„chat_templateå±æ€§ä»¥ç¡®å®šåœ¨è½¬æ¢æ—¶è¦ä½¿ç”¨çš„æ ¼å¼å’Œæ§åˆ¶æ ‡è®°ã€‚å½“chat_templateä¸ºNoneæ—¶ï¼Œå°†é€€å›åˆ°ç±»çº§åˆ«æŒ‡å®šçš„default_chat_templateã€‚
- en: '#### `batch_decode`'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `batch_decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3692)'
- en: '[PRE25]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`ï¼ˆ`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥idçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åˆ é™¤è§£ç ä¸­çš„ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦æ¸…é™¤åˆ†è¯ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of decoded sentences.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç çš„å¥å­åˆ—è¡¨ã€‚
- en: Convert a list of lists of token ids into a list of strings by calling decode.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨decodeå°†æ ‡è®°idçš„åˆ—è¡¨åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '#### `decode`'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `decode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3726)'
- en: '[PRE26]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`)
    â€” List of tokenized input ids. Can be obtained using the `__call__` method.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids`ï¼ˆ`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`ï¼‰â€”
    æ ‡è®°åŒ–è¾“å…¥idçš„åˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”åœ¨è§£ç æ—¶æ˜¯å¦åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: '`clean_up_tokenization_spaces` (`bool`, *optional*) â€” Whether or not to clean
    up the tokenization spaces. If `None`, will default to `self.clean_up_tokenization_spaces`.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clean_up_tokenization_spaces`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€”æ˜¯å¦æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚å¦‚æœä¸º`None`ï¼Œå°†é»˜è®¤ä¸º`self.clean_up_tokenization_spaces`ã€‚'
- en: '`kwargs` (additional keyword arguments, *optional*) â€” Will be passed to the
    underlying model specific decode method.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs`ï¼ˆé™„åŠ å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰â€”å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str`'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`'
- en: The decoded sentence.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„å¥å­ã€‚
- en: Converts a sequence of ids in a string, using the tokenizer and vocabulary with
    options to remove special tokens and clean up tokenization spaces.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ä¸€ç³»åˆ—idè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨æ ‡è®°å™¨å’Œè¯æ±‡è¡¨ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†æ ‡è®°åŒ–ç©ºæ ¼ã€‚
- en: Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚
- en: '#### `encode`'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `encode`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2537)'
- en: '[PRE27]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Parameters
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text` (`str`, `List[str]` or `List[int]`) â€” The first sequence to be encoded.
    This can be a string, a list of strings (tokenized string using the `tokenize`
    method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`
    method).'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`ï¼ˆ`str`ï¼Œ`List[str]`æˆ–`List[int]`ï¼‰â€”è¦ç¼–ç çš„ç¬¬ä¸€ä¸ªåºåˆ—ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨`tokenize`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨`convert_tokens_to_ids`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²idï¼‰ã€‚'
- en: '`text_pair` (`str`, `List[str]` or `List[int]`, *optional*) â€” Optional second
    sequence to be encoded. This can be a string, a list of strings (tokenized string
    using the `tokenize` method) or a list of integers (tokenized string ids using
    the `convert_tokens_to_ids` method).'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_pair`ï¼ˆ`str`ï¼Œ`List[str]`æˆ–`List[int]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ç¼–ç çš„å¯é€‰ç¬¬äºŒä¸ªåºåˆ—ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆä½¿ç”¨`tokenize`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰æˆ–ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼ˆä½¿ç”¨`convert_tokens_to_ids`æ–¹æ³•è¿›è¡Œæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²idï¼‰ã€‚'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to add special tokens when encoding the sequences. This will use the underlying
    `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines
    which tokens are automatically added to the input ids. This is usefull if you
    want to add `bos` or `eos` tokens automatically.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥idçš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) â€” Activates and controls padding. Accepts the
    following values:'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼ˆ`bool`ï¼Œ`str`æˆ–[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`æˆ–`''longest''`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`ï¼šå¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`æˆ–`''do_not_pad''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`truncation` (`bool`, `str` or [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy),
    *optional*, defaults to `False`) â€” Activates and controls truncation. Accepts
    the following values:'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation`ï¼ˆ`bool`ï¼Œ`str`æˆ–[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€”æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š'
- en: '`True` or `''longest_first''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate token by token, removing
    a token from the longest sequence in the pair if a pair of sequences (or a batch
    of pairs) is provided.'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True`æˆ–`''longest_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚'
- en: '`''only_first''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the first sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_first''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚'
- en: '`''only_second''`: Truncate to a maximum length specified with the argument
    `max_length` or to the maximum acceptable input length for the model if that argument
    is not provided. This will only truncate the second sequence of a pair if a pair
    of sequences (or a batch of pairs) is provided.'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''only_second''`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹åºåˆ—ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False`æˆ–`''do_not_truncate''`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚'
- en: '`max_length` (`int`, *optional*) â€” Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”ç”±æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€æ§åˆ¶è¦ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º `None`ï¼Œåˆ™å¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¸­éœ€è¦æœ€å¤§é•¿åº¦ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚
- en: '`stride` (`int`, *optional*, defaults to 0) â€” If set to a number along with
    `max_length`, the overflowing tokens returned when `return_overflowing_tokens=True`
    will contain some tokens from the end of the truncated sequence returned to provide
    some overlap between truncated and overflowing sequences. The value of this argument
    defines the number of overlapping tokens.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœä¸ `max_length` ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“ `return_overflowing_tokens=True`
    æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«æˆªæ–­åºåˆ—æœ«å°¾çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚æ­¤å‚æ•°çš„å€¼å®šä¹‰é‡å æ ‡è®°çš„æ•°é‡ã€‚'
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„å…ˆåˆ†è¯ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†æˆå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º
    `True`ï¼Œåˆ†è¯å™¨å°†å‡å®šè¾“å…¥å·²ç»åˆ†æˆå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œåˆ†è¯ã€‚è¿™å¯¹äºNERæˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. Requires `padding` to be activated. This is
    especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute
    capability `>= 7.5` (Volta).'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´» `padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›
    `>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚'
- en: '**kwargs â€” Passed along to the `.tokenize()` method.'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**kwargs â€” ä¼ é€’ç»™ `.tokenize()` æ–¹æ³•ã€‚'
- en: Returns
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›å€¼
- en: '`List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`, `torch.Tensor`, `tf.Tensor` æˆ– `np.ndarray`'
- en: The tokenized ids of the text.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æœ¬çš„æ ‡è®°åŒ–idã€‚
- en: Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºidï¼ˆæ•´æ•°ï¼‰åºåˆ—ã€‚
- en: Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å½“äºæ‰§è¡Œ `self.convert_tokens_to_ids(self.tokenize(text))`ã€‚
- en: '#### `push_to_hub`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `push_to_hub`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/utils/hub.py#L755)'
- en: '[PRE28]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`repo_id` (`str`) â€” The name of the repository you want to push your tokenizer
    to. It should contain your organization name when pushing to a given organization.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`repo_id` (`str`) â€” è¦å°†åˆ†è¯å™¨æ¨é€åˆ°çš„å­˜å‚¨åº“åç§°ã€‚åœ¨æ¨é€åˆ°ç»™å®šç»„ç»‡æ—¶ï¼Œåº”åŒ…å«æ‚¨çš„ç»„ç»‡åç§°ã€‚'
- en: '`use_temp_dir` (`bool`, *optional*) â€” Whether or not to use a temporary directory
    to store the files saved before they are pushed to the Hub. Will default to `True`
    if there is no directory named like `repo_id`, `False` otherwise.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_temp_dir` (`bool`, *å¯é€‰*) â€” æ˜¯å¦ä½¿ç”¨ä¸´æ—¶ç›®å½•å­˜å‚¨åœ¨æ¨é€åˆ°Hubä¹‹å‰ä¿å­˜çš„æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰åä¸º `repo_id` çš„ç›®å½•ï¼Œåˆ™é»˜è®¤ä¸º
    `True`ï¼Œå¦åˆ™ä¸º `False`ã€‚'
- en: '`commit_message` (`str`, *optional*) â€” Message to commit while pushing. Will
    default to `"Upload tokenizer"`.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_message` (`str`, *å¯é€‰*) â€” æ¨é€æ—¶è¦æäº¤çš„æ¶ˆæ¯ã€‚é»˜è®¤ä¸º `"Upload tokenizer"`ã€‚'
- en: '`private` (`bool`, *optional*) â€” Whether or not the repository created should
    be private.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`private` (`bool`, *å¯é€‰*) â€” æ˜¯å¦åˆ›å»ºçš„å­˜å‚¨åº“åº”ä¸ºç§æœ‰ã€‚'
- en: '`token` (`bool` or `str`, *optional*) â€” The token to use as HTTP bearer authorization
    for remote files. If `True`, will use the token generated when running `huggingface-cli
    login` (stored in `~/.huggingface`). Will default to `True` if `repo_url` is not
    specified.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token` (`bool` æˆ– `str`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œå°†ä½¿ç”¨è¿è¡Œ `huggingface-cli
    login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface`ï¼‰ã€‚å¦‚æœæœªæŒ‡å®š `repo_url`ï¼Œåˆ™é»˜è®¤ä¸º `True`ã€‚'
- en: '`max_shard_size` (`int` or `str`, *optional*, defaults to `"5GB"`) â€” Only applicable
    for models. The maximum size for a checkpoint before being sharded. Checkpoints
    shard will then be each of size lower than this size. If expressed as a string,
    needs to be digits followed by a unit (like `"5MB"`). We default it to `"5GB"`
    so that users can easily load models on free-tier Google Colab instances without
    any CPU OOM issues.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_shard_size` (`int` æˆ– `str`, *å¯é€‰*, é»˜è®¤ä¸º `"5GB"`) â€” ä»…é€‚ç”¨äºæ¨¡å‹ã€‚åœ¨åˆ†ç‰‡ä¹‹å‰çš„æ£€æŸ¥ç‚¹çš„æœ€å¤§å¤§å°ã€‚ç„¶åï¼Œæ£€æŸ¥ç‚¹å°†åˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡çš„å¤§å°éƒ½å°äºæ­¤å¤§å°ã€‚å¦‚æœè¡¨ç¤ºä¸ºå­—ç¬¦ä¸²ï¼Œéœ€è¦æ˜¯æ•°å­—åè·Ÿä¸€ä¸ªå•ä½ï¼ˆå¦‚
    `"5MB"`ï¼‰ã€‚æˆ‘ä»¬å°†å…¶é»˜è®¤ä¸º `"5GB"`ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥åœ¨å…è´¹çš„Google Colabå®ä¾‹ä¸Šè½»æ¾åŠ è½½æ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°CPU OOMé—®é¢˜ã€‚'
- en: '`create_pr` (`bool`, *optional*, defaults to `False`) â€” Whether or not to create
    a PR with the uploaded files or directly commit.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_pr` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ä¸Šä¼ æ–‡ä»¶çš„PRæˆ–ç›´æ¥æäº¤ã€‚'
- en: '`safe_serialization` (`bool`, *optional*, defaults to `True`) â€” Whether or
    not to convert the model weights in safetensors format for safer serialization.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å°†æ¨¡å‹æƒé‡è½¬æ¢ä¸ºsafetensorsæ ¼å¼ä»¥è¿›è¡Œæ›´å®‰å…¨çš„åºåˆ—åŒ–ã€‚'
- en: '`revision` (`str`, *optional*) â€” Branch to push the uploaded files to.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`revision` (`str`, *å¯é€‰*) â€” è¦å°†ä¸Šä¼ çš„æ–‡ä»¶æ¨é€åˆ°çš„åˆ†æ”¯ã€‚'
- en: '`commit_description` (`str`, *optional*) â€” The description of the commit that
    will be created'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commit_description` (`str`, *å¯é€‰*) â€” å°†åˆ›å»ºçš„æäº¤çš„æè¿°'
- en: '`tags` (`List[str]`, *optional*) â€” List of tags to push on the Hub.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags` (`List[str]`, *å¯é€‰*) â€” è¦æ¨é€åˆ°Hubä¸Šçš„æ ‡ç­¾åˆ—è¡¨ã€‚'
- en: Upload the tokenizer files to the ğŸ¤— Model Hub.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åˆ†è¯å™¨æ–‡ä»¶ä¸Šä¼ åˆ° ğŸ¤— Model Hubã€‚
- en: 'Examples:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[PRE29]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#### `convert_ids_to_tokens`'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_ids_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L369)'
- en: '[PRE30]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Parameters
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`ids` (`int` or `List[int]`) â€” The token id (or token ids) to convert to tokens.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ids`ï¼ˆ`int`æˆ–`List[int]`ï¼‰-è¦è½¬æ¢ä¸ºæ ‡è®°çš„æ ‡è®°idï¼ˆæˆ–æ ‡è®°idï¼‰ã€‚'
- en: '`skip_special_tokens` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to remove special tokens in the decoding.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skip_special_tokens`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰-æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚'
- en: Returns
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`str` or `List[str]`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '`str`æˆ–`List[str]`'
- en: The decoded token(s).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç åçš„æ ‡è®°ã€‚
- en: Converts a single index or a sequence of indices in a token or a sequence of
    tokens, using the vocabulary and added tokens.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å•ä¸ªç´¢å¼•æˆ–ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ ‡è®°æˆ–æ ‡è®°åºåˆ—ï¼Œä½¿ç”¨è¯æ±‡è¡¨å’Œæ·»åŠ çš„æ ‡è®°ã€‚
- en: '#### `convert_tokens_to_ids`'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L314)'
- en: '[PRE31]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tokens` (`str` or `List[str]`) â€” One or several token(s) to convert to token
    id(s).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokens`ï¼ˆ`str`æˆ–`List[str]`ï¼‰-è¦è½¬æ¢ä¸ºæ ‡è®°idçš„ä¸€ä¸ªæˆ–å¤šä¸ªæ ‡è®°ã€‚'
- en: Returns
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int` or `List[int]`'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`æˆ–`List[int]`'
- en: The token id or list of token ids.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°idæˆ–æ ‡è®°idåˆ—è¡¨ã€‚
- en: Converts a token string (or a sequence of tokens) in a single integer id (or
    a sequence of ids), using the vocabulary.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ ‡è®°å­—ç¬¦ä¸²ï¼ˆæˆ–æ ‡è®°åºåˆ—ï¼‰è½¬æ¢ä¸ºå•ä¸ªæ•´æ•°idï¼ˆæˆ–idåºåˆ—ï¼‰ï¼Œä½¿ç”¨è¯æ±‡è¡¨ã€‚
- en: '#### `get_added_vocab`'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_added_vocab`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L238)'
- en: '[PRE32]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Returns
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`Dict[str, int]`'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dict[str, int]`'
- en: The added tokens.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ çš„æ ‡è®°ã€‚
- en: Returns the added tokens in the vocabulary as a dictionary of token to index.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯æ±‡è¡¨ä¸­æ·»åŠ çš„æ ‡è®°ä½œä¸ºæ ‡è®°åˆ°ç´¢å¼•çš„å­—å…¸è¿”å›ã€‚
- en: '#### `num_special_tokens_to_add`'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `num_special_tokens_to_add`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L348)'
- en: '[PRE33]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Parameters
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`pair` (`bool`, *optional*, defaults to `False`) â€” Whether the number of added
    tokens should be computed in the case of a sequence pair or a single sequence.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pair`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰-åœ¨åºåˆ—å¯¹æˆ–å•ä¸ªåºåˆ—çš„æƒ…å†µä¸‹æ˜¯å¦åº”è®¡ç®—æ·»åŠ çš„æ ‡è®°æ•°ã€‚'
- en: Returns
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Number of special tokens added to sequences.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ åˆ°åºåˆ—çš„ç‰¹æ®Šæ ‡è®°æ•°ã€‚
- en: Returns the number of added tokens when encoding a sequence with special tokens.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ç‰¹æ®Šæ ‡è®°å¯¹åºåˆ—è¿›è¡Œç¼–ç æ—¶è¿”å›æ·»åŠ çš„æ ‡è®°æ•°ã€‚
- en: This encodes a dummy input and checks the number of added tokens, and is therefore
    not efficient. Do not put this inside your training loop.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šå¯¹è™šæ‹Ÿè¾“å…¥è¿›è¡Œç¼–ç å¹¶æ£€æŸ¥æ·»åŠ çš„æ ‡è®°æ•°ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸è¦å°†å…¶æ”¾åœ¨è®­ç»ƒå¾ªç¯å†…ã€‚
- en: '#### `set_truncation_and_padding`'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_truncation_and_padding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L398)'
- en: '[PRE34]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`padding_strategy` ([PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy))
    â€” The kind of padding that will be applied to the input'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding_strategy`ï¼ˆ[PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼‰-å°†åº”ç”¨äºè¾“å…¥çš„å¡«å……ç±»å‹'
- en: '`truncation_strategy` ([TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy))
    â€” The kind of truncation that will be applied to the input'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation_strategy`ï¼ˆ[TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy)ï¼‰-å°†åº”ç”¨äºè¾“å…¥çš„æˆªæ–­ç±»å‹'
- en: '`max_length` (`int`) â€” The maximum size of a sequence.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`ï¼ˆ`int`ï¼‰-åºåˆ—çš„æœ€å¤§å¤§å°ã€‚'
- en: '`stride` (`int`) â€” The stride to use when handling overflow.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride`ï¼ˆ`int`ï¼‰-å¤„ç†æº¢å‡ºæ—¶è¦ä½¿ç”¨çš„æ­¥å¹…ã€‚'
- en: '`pad_to_multiple_of` (`int`, *optional*) â€” If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰-å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚è¿™å¯¹äºå¯ç”¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šçš„å¼ é‡æ ¸å¿ƒç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: Define the truncation and the padding strategies for fast tokenizers (provided
    by HuggingFace tokenizers library) and restore the tokenizer settings afterwards.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰å¿«é€Ÿæ ‡è®°å™¨çš„æˆªæ–­å’Œå¡«å……ç­–ç•¥ï¼ˆç”±HuggingFaceæ ‡è®°å™¨åº“æä¾›ï¼‰ï¼Œå¹¶åœ¨æ¢å¤æ ‡è®°å™¨è®¾ç½®åæ¢å¤æ ‡è®°å™¨è®¾ç½®ã€‚
- en: The provided tokenizer has no padding / truncation strategy before the managed
    section. If your tokenizer set a padding / truncation strategy before, then it
    will be reset to no padding / truncation when exiting the managed section.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: æä¾›çš„æ ‡è®°å™¨åœ¨å—ç®¡ç†éƒ¨åˆ†ä¹‹å‰æ²¡æœ‰å¡«å……/æˆªæ–­ç­–ç•¥ã€‚å¦‚æœæ‚¨çš„æ ‡è®°å™¨åœ¨ä¹‹å‰è®¾ç½®äº†å¡«å……/æˆªæ–­ç­–ç•¥ï¼Œåˆ™åœ¨é€€å‡ºå—ç®¡ç†éƒ¨åˆ†æ—¶å°†é‡ç½®ä¸ºæ— å¡«å……/æˆªæ–­ã€‚
- en: '#### `train_new_from_iterator`'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `train_new_from_iterator`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_fast.py#L687)'
- en: '[PRE35]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`text_iterator` (generator of `List[str]`) â€” The training corpus. Should be
    a generator of batches of texts, for instance a list of lists of texts if you
    have everything in memory.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text_iterator`ï¼ˆ`List[str]`çš„ç”Ÿæˆå™¨ï¼‰-è®­ç»ƒè¯­æ–™åº“ã€‚åº”è¯¥æ˜¯æ–‡æœ¬æ‰¹æ¬¡çš„ç”Ÿæˆå™¨ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæ‚¨å°†æ‰€æœ‰å†…å®¹å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œåˆ™åº”è¯¥æ˜¯æ–‡æœ¬åˆ—è¡¨çš„åˆ—è¡¨ã€‚'
- en: '`vocab_size` (`int`) â€” The size of the vocabulary you want for your tokenizer.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`ï¼ˆ`int`ï¼‰-æ‚¨è¦ä¸ºæ ‡è®°å™¨è®¾ç½®çš„è¯æ±‡è¡¨å¤§å°ã€‚'
- en: '`length` (`int`, *optional*) â€” The total number of sequences in the iterator.
    This is used to provide meaningful progress tracking'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰-è¿­ä»£å™¨ä¸­åºåˆ—çš„æ€»æ•°ã€‚è¿™ç”¨äºæä¾›æœ‰æ„ä¹‰çš„è¿›åº¦è·Ÿè¸ª'
- en: '`new_special_tokens` (list of `str` or `AddedToken`, *optional*) â€” A list of
    new special tokens to add to the tokenizer you are training.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_special_tokens`ï¼ˆ`str`æˆ–`AddedToken`çš„åˆ—è¡¨ï¼Œ*å¯é€‰*ï¼‰-è¦æ·»åŠ åˆ°æ­£åœ¨è®­ç»ƒçš„æ ‡è®°å™¨çš„æ–°ç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚'
- en: '`special_tokens_map` (`Dict[str, str]`, *optional*) â€” If you want to rename
    some of the special tokens this tokenizer uses, pass along a mapping old special
    token name to new special token name in this argument.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens_map` (`Dict[str, str]`, *å¯é€‰*) â€” å¦‚æœæ‚¨æƒ³è¦é‡å‘½åæ­¤åˆ†è¯å™¨ä½¿ç”¨çš„ä¸€äº›ç‰¹æ®Šæ ‡è®°ï¼Œè¯·åœ¨æ­¤å‚æ•°ä¸­ä¼ é€’ä¸€ä¸ªæ—§ç‰¹æ®Šæ ‡è®°åç§°åˆ°æ–°ç‰¹æ®Šæ ‡è®°åç§°çš„æ˜ å°„ã€‚'
- en: '`kwargs` (`Dict[str, Any]`, *optional*) â€” Additional keyword arguments passed
    along to the trainer from the ğŸ¤— Tokenizers library.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (`Dict[str, Any]`, *å¯é€‰*) â€” ä» ğŸ¤— Tokenizers åº“ä¼ é€’ç»™è®­ç»ƒå™¨çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚'
- en: Returns
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)'
- en: A new tokenizer of the same type as the original one, trained on `text_iterator`.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªä¸åŸå§‹åˆ†è¯å™¨ç›¸åŒç±»å‹çš„æ–°åˆ†è¯å™¨ï¼Œè®­ç»ƒäº `text_iterator`ã€‚
- en: Trains a tokenizer on a new corpus with the same defaults (in terms of special
    tokens or tokenization pipeline) as the current one.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸å½“å‰ç›¸åŒçš„é»˜è®¤å€¼ï¼ˆç‰¹æ®Šæ ‡è®°æˆ–æ ‡è®°åŒ–æµæ°´çº¿æ–¹é¢ï¼‰åœ¨æ–°è¯­æ–™åº“ä¸Šè®­ç»ƒä¸€ä¸ªåˆ†è¯å™¨ã€‚
- en: BatchEncoding
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BatchEncoding
- en: '### `class transformers.BatchEncoding`'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.BatchEncoding`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L176)'
- en: '[PRE36]'
  id: totrans-562
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Parameters
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`data` (`dict`, *optional*) â€” Dictionary of lists/arrays/tensors returned by
    the `__call__`/`encode_plus`/`batch_encode_plus` methods (â€˜input_idsâ€™, â€˜attention_maskâ€™,
    etc.).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` (`dict`, *å¯é€‰*) â€” ç”± `__call__`/`encode_plus`/`batch_encode_plus` æ–¹æ³•è¿”å›çš„åˆ—è¡¨/æ•°ç»„/å¼ é‡çš„å­—å…¸ï¼ˆ''input_ids''ï¼Œ''attention_mask''ç­‰ï¼‰ã€‚'
- en: '`encoding` (`tokenizers.Encoding` or `Sequence[tokenizers.Encoding]`, *optional*)
    â€” If the tokenizer is a fast tokenizer which outputs additional information like
    mapping from word/character space to token space the `tokenizers.Encoding` instance
    or list of instance (for batches) hold this information.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoding` (`tokenizers.Encoding` æˆ– `Sequence[tokenizers.Encoding]`, *å¯é€‰*)
    â€” å¦‚æœåˆ†è¯å™¨æ˜¯ä¸€ä¸ªå¿«é€Ÿåˆ†è¯å™¨ï¼Œè¾“å‡ºé¢å¤–ä¿¡æ¯å¦‚ä»å•è¯/å­—ç¬¦ç©ºé—´åˆ°æ ‡è®°ç©ºé—´çš„æ˜ å°„ï¼Œåˆ™ `tokenizers.Encoding` å®ä¾‹æˆ–å®ä¾‹åˆ—è¡¨ï¼ˆç”¨äºæ‰¹æ¬¡ï¼‰ä¿å­˜æ­¤ä¿¡æ¯ã€‚'
- en: '`tensor_type` (`Union[None, str, TensorType]`, *optional*) â€” You can give a
    tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy
    Tensors at initialization.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_type` (`Union[None, str, TensorType]`, *å¯é€‰*) â€” æ‚¨å¯ä»¥åœ¨æ­¤å¤„æä¾›ä¸€ä¸ª tensor_typeï¼Œä»¥åœ¨åˆå§‹åŒ–æ—¶å°†æ•´æ•°åˆ—è¡¨è½¬æ¢ä¸º
    PyTorch/TensorFlow/Numpy å¼ é‡ã€‚'
- en: '`prepend_batch_axis` (`bool`, *optional*, defaults to `False`) â€” Whether or
    not to add a batch axis when converting to tensors (see `tensor_type` above).'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepend_batch_axis` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” åœ¨è½¬æ¢ä¸ºå¼ é‡æ—¶æ˜¯å¦æ·»åŠ æ‰¹æ¬¡è½´ï¼ˆå‚è§ä¸Šé¢çš„ `tensor_type`ï¼‰ã€‚'
- en: '`n_sequences` (`Optional[int]`, *optional*) â€” You can give a tensor_type here
    to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at initialization.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_sequences` (`Optional[int]`, *å¯é€‰*) â€” æ‚¨å¯ä»¥åœ¨æ­¤å¤„æä¾›ä¸€ä¸ª tensor_typeï¼Œä»¥åœ¨åˆå§‹åŒ–æ—¶å°†æ•´æ•°åˆ—è¡¨è½¬æ¢ä¸º
    PyTorch/TensorFlow/Numpy å¼ é‡ã€‚'
- en: Holds the output of the [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__),
    [encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus)
    and [batch_encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)
    methods (tokens, attention_masks, etc).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿å­˜äº† [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__),
    [encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode_plus)
    å’Œ [batch_encode_plus()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_encode_plus)
    æ–¹æ³•çš„è¾“å‡ºï¼ˆtokens, attention_masksç­‰ï¼‰ã€‚
- en: This class is derived from a python dictionary and can be used as a dictionary.
    In addition, this class exposes utility methods to map from word/character space
    to token space.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ç±»æ´¾ç”Ÿè‡ª Python å­—å…¸ï¼Œå¯ç”¨ä½œå­—å…¸ã€‚æ­¤å¤–ï¼Œæ­¤ç±»å…¬å¼€äº†å®ç”¨æ–¹æ³•ï¼Œç”¨äºå°†å•è¯/å­—ç¬¦ç©ºé—´æ˜ å°„åˆ°æ ‡è®°ç©ºé—´ã€‚
- en: '#### `char_to_token`'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `char_to_token`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L555)'
- en: '[PRE37]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_char_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the word in the sequence'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_char_index` (`int`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡ä»…åŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•'
- en: '`char_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`char_index` (`int`, *å¯é€‰*) â€” å¦‚æœåœ¨ *batch_or_token_index* ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) â€” If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided character index belongs to.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0) â€” å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å­—ç¬¦ç´¢å¼•å±äºä¸€å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0æˆ–1ï¼‰ã€‚'
- en: Returns
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the token.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°çš„ç´¢å¼•ã€‚
- en: Get the index of the token in the encoded output comprising a character in the
    original string for a sequence of the batch.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ç¼–ç è¾“å‡ºä¸­åŒ…å«åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„åºåˆ—çš„æ ‡è®°ç´¢å¼•ã€‚
- en: 'Can be called as:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.char_to_token(char_index)` if batch size is 1'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.char_to_token(char_index)` å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º 1'
- en: '`self.char_to_token(batch_index, char_index)` if batch size is greater or equal
    to 1'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.char_to_token(batch_index, char_index)` å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæˆ–ç­‰äº 1'
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç çš„æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚
- en: '#### `char_to_word`'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `char_to_word`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L641)'
- en: '[PRE38]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_char_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the character in the
    original string.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_char_index` (`int`) â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡ä»…åŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„ç´¢å¼•ã€‚'
- en: '`char_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the character in the original string.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`char_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„ç´¢å¼•ã€‚'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) â€” If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided character index belongs to.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨æ¥æŒ‡å®šæä¾›çš„å­—ç¬¦ç´¢å¼•å±äºè¯¥å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0æˆ–1ï¼‰ã€‚'
- en: Returns
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int` or `List[int]`'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`æˆ–`List[int]`'
- en: Index or indices of the associated encoded token(s).
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: å…³è”ç¼–ç æ ‡è®°çš„ç´¢å¼•æˆ–ç´¢å¼•ã€‚
- en: Get the word in the original string corresponding to a character in the original
    string of a sequence of the batch.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ‰¹æ¬¡ä¸­åºåˆ—çš„åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°çš„å­—ç¬¦å¯¹åº”çš„å•è¯ã€‚
- en: 'Can be called as:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.char_to_word(char_index)` if batch size is 1'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.char_to_word(char_index)`
- en: '`self.char_to_word(batch_index, char_index)` if batch size is greater than
    1'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº1ï¼Œåˆ™ä¸º`self.char_to_word(batch_index, char_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç çš„æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚
- en: '#### `convert_to_tensors`'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_to_tensors`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L680)'
- en: '[PRE39]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Parameters
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`tensor_type` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) â€” The type of tensors to use. If `str`, should be one of the values
    of the enum [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType).
    If `None`, no modification is done.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensor_type`ï¼ˆ`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)ï¼Œ*å¯é€‰*ï¼‰â€”
    è¦ä½¿ç”¨çš„å¼ é‡ç±»å‹ã€‚å¦‚æœæ˜¯`str`ï¼Œåº”è¯¥æ˜¯æšä¸¾[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType)å€¼ä¹‹ä¸€ã€‚å¦‚æœä¸º`None`ï¼Œåˆ™ä¸è¿›è¡Œä¿®æ”¹ã€‚'
- en: '`prepend_batch_axis` (`int`, *optional*, defaults to `False`) â€” Whether or
    not to add the batch dimension during the conversion.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prepend_batch_axis`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” åœ¨è½¬æ¢è¿‡ç¨‹ä¸­æ˜¯å¦æ·»åŠ æ‰¹æ¬¡ç»´åº¦ã€‚'
- en: Convert the inner content to tensors.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å†…éƒ¨å†…å®¹è½¬æ¢ä¸ºå¼ é‡ã€‚
- en: '#### `sequence_ids`'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `sequence_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L319)'
- en: '[PRE40]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_index` (`int`, *optional*, defaults to 0) â€” The index to access in the
    batch.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” è¦è®¿é—®çš„æ‰¹æ¬¡ä¸­çš„ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Optional[int]]`'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the sequence id corresponding to each token. Special tokens
    added by the tokenizer are mapped to `None` and other tokens are mapped to the
    index of their corresponding sequence.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„åºåˆ—idçš„åˆ—è¡¨ã€‚ç”±åˆ†è¯å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”åºåˆ—çš„ç´¢å¼•ã€‚
- en: 'Return a list mapping the tokens to the id of their original sentences:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›å°†æ ‡è®°æ˜ å°„åˆ°å…¶åŸå§‹å¥å­çš„idçš„åˆ—è¡¨ï¼š
- en: '`None` for special tokens added around or between sequences,'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ·»åŠ åœ¨åºåˆ—å‘¨å›´æˆ–ä¹‹é—´çš„ç‰¹æ®Šæ ‡è®°ï¼Œä¸º`None`ï¼Œ
- en: '`0` for tokens corresponding to words in the first sequence,'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`è¡¨ç¤ºå¯¹åº”äºç¬¬ä¸€ä¸ªåºåˆ—ä¸­çš„å•è¯çš„æ ‡è®°ï¼Œ'
- en: '`1` for tokens corresponding to words in the second sequence when a pair of
    sequences was jointly encoded.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“ä¸€å¯¹åºåˆ—è¢«è”åˆç¼–ç æ—¶ï¼Œå¯¹äºç¬¬äºŒä¸ªåºåˆ—ä¸­çš„å•è¯å¯¹åº”çš„æ ‡è®°ï¼Œä¸º`1`ã€‚
- en: '#### `to`'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `to`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L773)'
- en: '[PRE41]'
  id: totrans-622
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`device` (`str` or `torch.device`) â€” The device to put the tensors on.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`ï¼ˆ`str`æˆ–`torch.device`ï¼‰â€” è¦æ”¾ç½®å¼ é‡çš„è®¾å¤‡ã€‚'
- en: Returns
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)'
- en: The same instance after modification.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿®æ”¹åçš„ç›¸åŒå®ä¾‹ã€‚
- en: Send all values to device by calling `v.to(device)` (PyTorch only).
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒç”¨`v.to(device)`å°†æ‰€æœ‰å€¼å‘é€åˆ°è®¾å¤‡ï¼ˆä»…é€‚ç”¨äºPyTorchï¼‰ã€‚
- en: '#### `token_to_chars`'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_chars`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L516)'
- en: '[PRE42]'
  id: totrans-631
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Parameters
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_token_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the token in the sequence.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`ï¼ˆ`int`ï¼‰â€” æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: '`token_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token or tokens in the sequence.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°æˆ–æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)'
- en: Span of characters in the original string, or None, if the token (e.g. ~~,~~
    ) doesnâ€™t correspond to any chars in the origin string.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„èŒƒå›´ï¼Œå¦‚æœæ ‡è®°ï¼ˆä¾‹å¦‚~~,~~ï¼‰ä¸å¯¹åº”äºåŸå§‹å­—ç¬¦ä¸²ä¸­çš„ä»»ä½•å­—ç¬¦ï¼Œåˆ™ä¸ºNoneã€‚
- en: Get the character span corresponding to an encoded token in a sequence of the
    batch.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ‰¹æ¬¡ä¸­åºåˆ—ä¸­ç¼–ç æ ‡è®°å¯¹åº”çš„å­—ç¬¦è·¨åº¦ã€‚
- en: 'Character spans are returned as a [CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)
    with:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: å­—ç¬¦è·¨åº¦ä»¥[CharSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.CharSpan)å½¢å¼è¿”å›ï¼Œå…·æœ‰ï¼š
- en: '`start` â€” Index of the first character in the original string associated to
    the token.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`â€” ä¸æ ‡è®°å…³è”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•ã€‚'
- en: '`end` â€” Index of the character following the last character in the original
    string associated to the token.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end`â€” è·Ÿéšä¸æ ‡è®°å…³è”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­æœ€åä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•ã€‚'
- en: 'Can be called as:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.token_to_chars(token_index)` if batch size is 1'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.token_to_chars(token_index)`
- en: '`self.token_to_chars(batch_index, token_index)` if batch size is greater or
    equal to 1'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äºæˆ–ç­‰äº1ï¼Œåˆ™ä¸º`self.token_to_chars(batch_index, token_index)`
- en: '#### `token_to_sequence`'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_sequence`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L386)'
- en: '[PRE43]'
  id: totrans-647
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Parameters
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_token_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprises one sequence, this can be the index of the token in the sequence.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: '`token_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token in the sequence.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the word in the input sequence.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥åºåˆ—ä¸­çš„å•è¯ç´¢å¼•ã€‚
- en: Get the index of the sequence represented by the given token. In the general
    use case, this method returns `0` for a single sequence or the first sequence
    of a pair, and `1` for the second sequence of a pair
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ç»™å®šæ ‡è®°è¡¨ç¤ºçš„åºåˆ—çš„ç´¢å¼•ã€‚åœ¨ä¸€èˆ¬ç”¨ä¾‹ä¸­ï¼Œæ­¤æ–¹æ³•å¯¹äºå•ä¸ªåºåˆ—æˆ–ä¸€å¯¹åºåˆ—çš„ç¬¬ä¸€ä¸ªåºåˆ—è¿”å›`0`ï¼Œå¯¹äºä¸€å¯¹åºåˆ—çš„ç¬¬äºŒä¸ªåºåˆ—è¿”å›`1`
- en: 'Can be called as:'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.token_to_sequence(token_index)` if batch size is 1'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.token_to_sequence(token_index)`
- en: '`self.token_to_sequence(batch_index, token_index)` if batch size is greater
    than 1'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº1ï¼Œåˆ™ä¸º`self.token_to_sequence(batch_index, token_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e., words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥åºåˆ—ä»¥é¢„æ ‡è®°åºåˆ—ï¼ˆå³ï¼Œå•è¯ç”±ç”¨æˆ·å®šä¹‰ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„æ ‡è®°åŒ–å•è¯å…³è”èµ·æ¥ã€‚
- en: '#### `token_to_word`'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `token_to_word`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L425)'
- en: '[PRE44]'
  id: totrans-661
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Parameters
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_token_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the token in the sequence.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_token_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: '`token_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the token in the sequence.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`int`'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '`int`'
- en: Index of the word in the input sequence.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥åºåˆ—ä¸­çš„å•è¯ç´¢å¼•ã€‚
- en: Get the index of the word corresponding (i.e. comprising) to an encoded token
    in a sequence of the batch.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ä¸æ‰¹æ¬¡åºåˆ—ä¸­ç¼–ç æ ‡è®°å¯¹åº”çš„å•è¯çš„ç´¢å¼•ã€‚
- en: 'Can be called as:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.token_to_word(token_index)` if batch size is 1'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.token_to_word(token_index)`
- en: '`self.token_to_word(batch_index, token_index)` if batch size is greater than
    1'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹æ¬¡å¤§å°å¤§äº1ï¼Œåˆ™ä¸º`self.token_to_word(batch_index, token_index)`
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e., words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥åºåˆ—ä»¥é¢„æ ‡è®°åºåˆ—ï¼ˆå³ï¼Œå•è¯ç”±ç”¨æˆ·å®šä¹‰ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„æ ‡è®°åŒ–å•è¯å…³è”èµ·æ¥ã€‚
- en: '#### `tokens`'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L301)'
- en: '[PRE45]'
  id: totrans-675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Parameters
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_index` (`int`, *optional*, defaults to 0) â€” The index to access in the
    batch.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€”è¦è®¿é—®çš„æ‰¹æ¬¡ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[str]`'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[str]`'
- en: The list of tokens at that index.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç´¢å¼•å¤„çš„æ ‡è®°åˆ—è¡¨ã€‚
- en: Return the list of tokens (sub-parts of the input strings after word/subword
    splitting and before conversion to integer indices) at a given batch index (only
    works for the output of a fast tokenizer).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ç»™å®šæ‰¹æ¬¡ç´¢å¼•å¤„çš„æ ‡è®°åˆ—è¡¨ï¼ˆåœ¨å•è¯/å­è¯æ‹†åˆ†åå’Œè½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ä¹‹å‰çš„è¾“å…¥å­—ç¬¦ä¸²çš„å­éƒ¨åˆ†ï¼‰ï¼ˆä»…é€‚ç”¨äºå¿«é€Ÿæ ‡è®°å™¨çš„è¾“å‡ºï¼‰ã€‚
- en: '#### `word_ids`'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_ids`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L367)'
- en: '[PRE46]'
  id: totrans-684
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Parameters
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_index` (`int`, *optional*, defaults to 0) â€” The index to access in the
    batch.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€”è¦è®¿é—®çš„æ‰¹æ¬¡ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Optional[int]]`'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the word corresponding to each token. Special tokens added
    by the tokenizer are mapped to `None` and other tokens are mapped to the index
    of their corresponding word (several tokens will be mapped to the same word index
    if they are parts of that word).
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåˆ—è¡¨ï¼ŒæŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„å•è¯ã€‚æ ‡è®°å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”å•è¯çš„ç´¢å¼•ï¼ˆå¦‚æœå®ƒä»¬æ˜¯è¯¥å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™å‡ ä¸ªæ ‡è®°å°†æ˜ å°„åˆ°ç›¸åŒçš„å•è¯ç´¢å¼•ï¼‰ã€‚
- en: Return a list mapping the tokens to their actual word in the initial sentence
    for a fast tokenizer.
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä¸€ä¸ªå°†æ ‡è®°æ˜ å°„åˆ°åˆå§‹å¥å­ä¸­å®é™…å•è¯çš„åˆ—è¡¨ï¼Œç”¨äºå¿«é€Ÿæ ‡è®°å™¨ã€‚
- en: '#### `word_to_chars`'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_to_chars`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L596)'
- en: '[PRE47]'
  id: totrans-693
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Parameters
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_word_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprise one sequence, this can be the index of the word in the sequence'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_word_index`ï¼ˆ`int`ï¼‰â€”æ‰¹æ¬¡ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹æ¬¡åªåŒ…å«ä¸€ä¸ªåºåˆ—ï¼Œè¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•'
- en: '`word_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹æ¬¡ç´¢å¼•ï¼Œåˆ™è¿™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) â€” If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided word index belongs to.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€”å¦‚æœæ‰¹æ¬¡ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å•è¯ç´¢å¼•å±äºè¯¥å¯¹ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0æˆ–1ï¼‰ã€‚'
- en: Returns
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`CharSpan` or `List[CharSpan]`'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '`CharSpan`æˆ–`List[CharSpan]`'
- en: 'Span(s) of the associated character or characters in the string. CharSpan are
    NamedTuple with:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å­—ç¬¦ä¸²ä¸­ç›¸å…³å­—ç¬¦æˆ–å­—ç¬¦çš„èŒƒå›´ã€‚CharSpanæ˜¯NamedTupleï¼Œå…·æœ‰ï¼š
- en: 'start: index of the first character associated to the token in the original
    string'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'start: åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°å…³è”çš„ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•'
- en: 'end: index of the character following the last character associated to the
    token in the original string'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'end: åŸå§‹å­—ç¬¦ä¸²ä¸­ä¸æ ‡è®°å…³è”çš„æœ€åä¸€ä¸ªå­—ç¬¦åé¢çš„å­—ç¬¦çš„ç´¢å¼•'
- en: Get the character span in the original string corresponding to given word in
    a sequence of the batch.
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–æ‰¹å¤„ç†åºåˆ—ä¸­ç»™å®šå•è¯å¯¹åº”çš„åŸå§‹å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦èŒƒå›´ã€‚
- en: 'Character spans are returned as a CharSpan NamedTuple with:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: å­—ç¬¦èŒƒå›´ä»¥CharSpan NamedTupleå½¢å¼è¿”å›ï¼š
- en: 'start: index of the first character in the original string'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'start: åŸå§‹å­—ç¬¦ä¸²ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦çš„ç´¢å¼•'
- en: 'end: index of the character following the last character in the original string'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'end: åŸå§‹å­—ç¬¦ä¸²ä¸­æœ€åä¸€ä¸ªå­—ç¬¦åé¢çš„å­—ç¬¦çš„ç´¢å¼•'
- en: 'Can be called as:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.word_to_chars(word_index)` if batch size is 1'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹å¤„ç†å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.word_to_chars(word_index)`
- en: '`self.word_to_chars(batch_index, word_index)` if batch size is greater or equal
    to 1'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœæ‰¹å¤„ç†å¤§å°å¤§äºæˆ–ç­‰äº1ï¼Œåˆ™ä¸º`self.word_to_chars(batch_index, word_index)`
- en: '#### `word_to_tokens`'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `word_to_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L463)'
- en: '[PRE48]'
  id: totrans-712
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Parameters
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_or_word_index` (`int`) â€” Index of the sequence in the batch. If the
    batch only comprises one sequence, this can be the index of the word in the sequence.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_or_word_index`ï¼ˆ`int`ï¼‰â€” æ‰¹å¤„ç†ä¸­åºåˆ—çš„ç´¢å¼•ã€‚å¦‚æœæ‰¹å¤„ç†ä»…åŒ…æ‹¬ä¸€ä¸ªåºåˆ—ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚'
- en: '`word_index` (`int`, *optional*) â€” If a batch index is provided in *batch_or_token_index*,
    this can be the index of the word in the sequence.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`word_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœåœ¨*batch_or_token_index*ä¸­æä¾›äº†æ‰¹å¤„ç†ç´¢å¼•ï¼Œåˆ™å¯ä»¥æ˜¯åºåˆ—ä¸­å•è¯çš„ç´¢å¼•ã€‚'
- en: '`sequence_index` (`int`, *optional*, defaults to 0) â€” If pair of sequences
    are encoded in the batch this can be used to specify which sequence in the pair
    (0 or 1) the provided word index belongs to.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” å¦‚æœæ‰¹å¤„ç†ä¸­ç¼–ç äº†ä¸€å¯¹åºåˆ—ï¼Œåˆ™å¯ä»¥ç”¨äºæŒ‡å®šæä¾›çš„å•è¯ç´¢å¼•å±äºä¸€å¯¹åºåˆ—ä¸­çš„å“ªä¸ªåºåˆ—ï¼ˆ0æˆ–1ï¼‰ã€‚'
- en: Returns
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: ([TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan),
    *optional*)
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: ([TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)ï¼Œ*å¯é€‰*)
- en: Span of tokens in the encoded sequence. Returns `None` if no tokens correspond
    to the word. This can happen especially when the token is a special token that
    has been used to format the tokenization. For example when we add a class token
    at the very beginning of the tokenization.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç åºåˆ—ä¸­çš„æ ‡è®°èŒƒå›´ã€‚å¦‚æœæ²¡æœ‰æ ‡è®°ä¸è¯¥å•è¯å¯¹åº”ï¼Œåˆ™è¿”å›`None`ã€‚è¿™å¯èƒ½ä¼šå‘ç”Ÿï¼Œç‰¹åˆ«æ˜¯å½“æ ‡è®°æ˜¯ç”¨äºæ ¼å¼åŒ–æ ‡è®°åŒ–çš„ç‰¹æ®Šæ ‡è®°æ—¶ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨æ ‡è®°åŒ–çš„å¼€å¤´æ·»åŠ ä¸€ä¸ªç±»æ ‡è®°æ—¶ã€‚
- en: Get the encoded token span corresponding to a word in a sequence of the batch.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ä¸æ‰¹å¤„ç†åºåˆ—ä¸­çš„å•è¯å¯¹åº”çš„ç¼–ç æ ‡è®°èŒƒå›´ã€‚
- en: 'Token spans are returned as a [TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)
    with:'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°èŒƒå›´ä»¥[TokenSpan](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.TokenSpan)å½¢å¼è¿”å›ï¼š
- en: '`start` â€” Index of the first token.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start` â€” ç¬¬ä¸€ä¸ªæ ‡è®°çš„ç´¢å¼•ã€‚'
- en: '`end` â€” Index of the token following the last token.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end` â€” æœ€åä¸€ä¸ªæ ‡è®°åé¢çš„æ ‡è®°çš„ç´¢å¼•ã€‚'
- en: 'Can be called as:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒç”¨ä¸ºï¼š
- en: '`self.word_to_tokens(word_index, sequence_index: int = 0)` if batch size is
    1'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¦‚æœæ‰¹å¤„ç†å¤§å°ä¸º1ï¼Œåˆ™ä¸º`self.word_to_tokens(word_index, sequence_index: int = 0)`'
- en: '`self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)` if
    batch size is greater or equal to 1'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¦‚æœæ‰¹å¤„ç†å¤§å°å¤§äºæˆ–ç­‰äº1ï¼Œåˆ™ä¸º`self.word_to_tokens(batch_index, word_index, sequence_index:
    int = 0)`'
- en: This method is particularly suited when the input sequences are provided as
    pre-tokenized sequences (i.e. words are defined by the user). In this case it
    allows to easily associate encoded tokens with provided tokenized words.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¾“å…¥åºåˆ—ä»¥é¢„åˆ†è¯åºåˆ—ï¼ˆå³ç”¨æˆ·å®šä¹‰çš„å•è¯ï¼‰æä¾›æ—¶ï¼Œæ­¤æ–¹æ³•ç‰¹åˆ«é€‚ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå…è®¸è½»æ¾å°†ç¼–ç æ ‡è®°ä¸æä¾›çš„åˆ†è¯å•è¯å…³è”èµ·æ¥ã€‚
- en: '#### `words`'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `words`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L343)'
- en: '[PRE49]'
  id: totrans-730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Parameters
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`batch_index` (`int`, *optional*, defaults to 0) â€” The index to access in the
    batch.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_index`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0ï¼‰â€” è¦è®¿é—®çš„æ‰¹å¤„ç†ä¸­çš„ç´¢å¼•ã€‚'
- en: Returns
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`List[Optional[int]]`'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[Optional[int]]`'
- en: A list indicating the word corresponding to each token. Special tokens added
    by the tokenizer are mapped to `None` and other tokens are mapped to the index
    of their corresponding word (several tokens will be mapped to the same word index
    if they are parts of that word).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: æŒ‡ç¤ºæ¯ä¸ªæ ‡è®°å¯¹åº”çš„å•è¯çš„åˆ—è¡¨ã€‚æ ‡è®°å™¨æ·»åŠ çš„ç‰¹æ®Šæ ‡è®°æ˜ å°„åˆ°`None`ï¼Œå…¶ä»–æ ‡è®°æ˜ å°„åˆ°å…¶å¯¹åº”å•è¯çš„ç´¢å¼•ï¼ˆå¦‚æœå®ƒä»¬æ˜¯è¯¥å•è¯çš„ä¸€éƒ¨åˆ†ï¼Œåˆ™å‡ ä¸ªæ ‡è®°å°†æ˜ å°„åˆ°ç›¸åŒçš„å•è¯ç´¢å¼•ï¼‰ã€‚
- en: Return a list mapping the tokens to their actual word in the initial sentence
    for a fast tokenizer.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå°†æ ‡è®°æ˜ å°„åˆ°åˆå§‹å¥å­ä¸­çš„å®é™…å•è¯ï¼Œä»¥ä¾¿å¿«é€Ÿæ ‡è®°åŒ–å™¨ä½¿ç”¨ã€‚
