# GPTSAN-japanese

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese)

## æ¦‚è¿°

GPTSAN-japanese æ¨¡å‹ç”±å‚æœ¬ä¿Šä¹‹ï¼ˆtanreinamaï¼‰åœ¨ä»“åº“ä¸­å‘å¸ƒã€‚

GPTSAN æ˜¯ä¸€ä¸ªä½¿ç”¨ Switch Transformer çš„æ—¥è¯­è¯­è¨€æ¨¡å‹ã€‚å®ƒå…·æœ‰ä¸ T5 è®ºæ–‡ä¸­ä»‹ç»çš„ Prefix LM æ¨¡å‹ç›¸åŒçš„ç»“æ„ï¼Œå¹¶æ”¯æŒæ–‡æœ¬ç”Ÿæˆå’Œæ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚è¿™äº›åŸºæœ¬ä»»åŠ¡åŒæ ·å¯ä»¥ç”¨äºç¿»è¯‘æˆ–æ‘˜è¦çš„å¾®è°ƒã€‚

### ç”¨æ³•ç¤ºä¾‹

`generate()` æ–¹æ³•å¯ç”¨äºä½¿ç”¨ GPTSAN-Japanese æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚

```py
>>> from transformers import AutoModel, AutoTokenizer
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").cuda()
>>> x_tok = tokenizer("ã¯ã€", prefix_text="ç¹”ç”°ä¿¡é•·", return_tensors="pt")
>>> torch.manual_seed(0)
>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)
>>> tokenizer.decode(gen_tok[0])
'ç¹”ç”°ä¿¡é•·ã¯ã€2004 å¹´ã«ã€æˆ¦å›½ BASARAã€ã®ãŸã‚ã«ã€è±Šè‡£ç§€å‰'
```

## GPTSAN ç‰¹ç‚¹

GPTSAN å…·æœ‰ä¸€äº›ç‹¬ç‰¹çš„ç‰¹ç‚¹ã€‚å®ƒå…·æœ‰ Prefix-LM çš„æ¨¡å‹ç»“æ„ã€‚å®ƒä½œä¸ºå‰ç¼€è¾“å…¥ token çš„ç§»ä½æ©ç è¯­è¨€æ¨¡å‹ã€‚æœªåŠ å‰ç¼€çš„è¾“å…¥è¡Œä¸ºç±»ä¼¼äºæ­£å¸¸çš„ç”Ÿæˆæ¨¡å‹ã€‚Spout å‘é‡æ˜¯ GPTSAN ç‰¹å®šçš„è¾“å…¥ã€‚Spout åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨éšæœºè¾“å…¥ï¼Œä½†åœ¨å¾®è°ƒæœŸé—´å¯ä»¥æŒ‡å®šæ–‡æœ¬ç±»åˆ«æˆ–ä»»æ„å‘é‡ã€‚è¿™å…è®¸æ‚¨æŒ‡ç¤ºç”Ÿæˆæ–‡æœ¬çš„å€¾å‘ã€‚GPTSAN å…·æœ‰åŸºäº Switch-Transformer çš„ç¨€ç–å‰é¦ˆã€‚æ‚¨è¿˜å¯ä»¥æ·»åŠ å…¶ä»–å±‚å¹¶éƒ¨åˆ†è®­ç»ƒå®ƒä»¬ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…åŸå§‹ GPTSAN ä»“åº“ã€‚

### Prefix-LM æ¨¡å‹

GPTSAN å…·æœ‰ T5 è®ºæ–‡ä¸­ç§°ä¸º Prefix-LM çš„æ¨¡å‹ç»“æ„ã€‚ï¼ˆåŸå§‹ GPTSAN ä»“åº“å°†å…¶ç§°ä¸º `hybrid`ï¼‰åœ¨ GPTSAN ä¸­ï¼ŒPrefix-LM çš„ `Prefix` éƒ¨åˆ†ï¼Œå³å¯ä»¥ç”±ä¸¤ä¸ª token å¼•ç”¨çš„è¾“å…¥ä½ç½®ï¼Œå¯ä»¥æŒ‡å®šä¸ºä»»æ„é•¿åº¦ã€‚å¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼Œä¹Ÿå¯ä»¥ä¸ºä¸åŒçš„é•¿åº¦æŒ‡å®šä¸åŒçš„é•¿åº¦ã€‚è¿™ä¸ªé•¿åº¦é€‚ç”¨äº tokenizer ä¸­è¾“å…¥çš„ `prefix_text` æ–‡æœ¬ã€‚tokenizer è¿”å› Prefix-LM çš„ `Prefix` éƒ¨åˆ†çš„æ©ç ä½œä¸º `token_type_ids`ã€‚æ¨¡å‹å°†å…¶ä¸­ `token_type_ids` ä¸º 1 çš„éƒ¨åˆ†è§†ä¸º `Prefix` éƒ¨åˆ†ï¼Œå³è¾“å…¥å¯ä»¥å¼•ç”¨å‰åä¸¤ä¸ª tokenã€‚

## ä½¿ç”¨æç¤º

é€šè¿‡ä¼ é€’ç»™è‡ªæ³¨æ„åŠ›çš„æ©ç æ¥æŒ‡å®šå‰ç¼€éƒ¨åˆ†ã€‚å½“ token_type_ids=None æˆ–å…¨éƒ¨ä¸ºé›¶æ—¶ï¼Œç­‰åŒäºå¸¸è§„å› æœæ©ç ã€‚

ä¾‹å¦‚ï¼š

> > > x_token = tokenizer(â€œï½±ï½²ï½³ï½´â€) input_ids: | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ | token_type_ids: | 1 | 0 | 0 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 0 0 0 0 0 | SEG | 1 1 0 0 0 0 | ï½± | 1 1 1 0 0 0 | ï½² | 1 1 1 1 0 0 | ï½³ | 1 1 1 1 1 0 | ï½´ | 1 1 1 1 1 1 |
> > > 
> > > x_token = tokenizer("", prefix_text=â€œï½±ï½²ï½³ï½´â€) input_ids: | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG | token_type_ids: | 1 | 1 | 1 | 1 | 1 | 0 | prefix_lm_mask: SOT | 1 1 1 1 1 0 | ï½± | 1 1 1 1 1 0 | ï½² | 1 1 1 1 1 0 | ï½³ | 1 1 1 1 1 0 | ï½´ | 1 1 1 1 1 0 | SEG | 1 1 1 1 1 1 |
> > > 
> > > x_token = tokenizer(â€œï½³ï½´â€, prefix_text=â€œï½±ï½²â€) input_ids: | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ | token_type_ids: | 1 | 1 | 1 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 1 1 0 0 0 | ï½± | 1 1 1 0 0 0 | ï½² | 1 1 1 0 0 0 | SEG | 1 1 1 1 0 0 | ï½³ | 1 1 1 1 1 0 | ï½´ | 1 1 1 1 1 1 |

### Spout Vector

Spout Vector æ˜¯ç”¨äºæ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„ç‰¹æ®Šå‘é‡ã€‚è¿™ä¸ªå‘é‡è¢«è§†ä¸ºè‡ªæ³¨æ„åŠ›ä¸­çš„ç¬¬ä¸€ä¸ªåµŒå…¥ï¼Œä»¥å°†é¢å¤–çš„æ³¨æ„åŠ›å¼•å…¥ç”Ÿæˆçš„ tokenã€‚åœ¨ä» `Tanrei/GPTSAN-japanese` å‘å¸ƒçš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼ŒSpout Vector æ˜¯ä¸€ä¸ªé€šè¿‡æ¨¡å‹ä¸­çš„ 8 ä¸ªå…¨è¿æ¥å±‚ä¼ é€’çš„ 128 ç»´å‘é‡ï¼Œå¹¶æŠ•å½±åˆ°å……å½“å¤–éƒ¨æ³¨æ„åŠ›çš„ç©ºé—´ã€‚ç”±å…¨è¿æ¥å±‚æŠ•å½±çš„ Spout Vector è¢«åˆ†å‰²ä»¥ä¼ é€’åˆ°æ‰€æœ‰è‡ªæ³¨æ„åŠ›ã€‚

## GPTSanJapaneseConfig

### `class transformers.GPTSanJapaneseConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/configuration_gptsan_japanese.py#L29)

```py
( vocab_size = 36000 max_position_embeddings = 1280 d_model = 1024 d_ff = 8192 d_ext = 4096 d_spout = 128 num_switch_layers = 10 num_ext_layers = 0 num_heads = 16 num_experts = 16 expert_capacity = 128 dropout_rate = 0.0 layer_norm_epsilon = 1e-05 router_bias = False router_jitter_noise = 0.0 router_dtype = 'float32' router_ignore_padding_tokens = False output_hidden_states = False output_attentions = False initializer_factor = 0.002 output_router_logits = False use_cache = True separator_token_id = 35998 pad_token_id = 35995 eos_token_id = 35999 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 36000) â€” GPTSANJapanese æ¨¡å‹çš„è¯æ±‡é‡ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨ GPTSanJapaneseModel æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒ token æ•°é‡ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 1280) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚é»˜è®¤è®¾ç½®ä¸º 1280ã€‚

+   `d_model` (`int`, *optional*, defaults to 1024) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å±‚çš„å¤§å°ã€‚

+   `d_ff` (`int`, *optional*, defaults to 8192) â€” æ¯ä¸ª`SwitchTransformersBlock`ä¸­é—´çº§å‰é¦ˆå±‚çš„å¤§å°ã€‚

+   `d_ext` (`int`, *optional*, defaults to 4096) â€” é¢å¤–å±‚ä¸­é—´å‰é¦ˆå±‚çš„å¤§å°ã€‚

+   `d_spout` (`int`, *optional*, defaults to 128) â€” `spout`å‘é‡çš„å¤§å°ã€‚

+   `num_switch_layers` (`int`, *optional*, defaults to 10) â€” Switch Transformer å±‚ä¸­çš„å±‚æ•°ã€‚

+   `num_ext_layers` (`int`, *optional*, defaults to 0) â€” é¢å¤–å±‚ä¸­çš„å±‚æ•°ã€‚

+   `num_heads` (`int`, *optional*, defaults to 16) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `num_experts` (`int`, *optional*, defaults to 16) â€” æ¯ä¸ª SwitchTransformer å±‚çš„ä¸“å®¶æ•°é‡ã€‚

+   `expert_capacity` (`int`, *optional*, defaults to 128) â€” æ¯ä¸ªä¸“å®¶å¯ä»¥å­˜å‚¨çš„ä»¤ç‰Œæ•°é‡ã€‚å¦‚æœè®¾ç½®ä¸º 1ï¼Œåˆ™æ¨¡å‹å°†è¡¨ç°å¾—åƒä¸€ä¸ªå¸¸è§„ Transformerã€‚

+   `dropout_rate` (`float`, *optional*, defaults to 0.0) â€” æ‰€æœ‰ dropout å±‚çš„æ¯”ç‡ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `router_bias` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦å‘è·¯ç”±å™¨æ·»åŠ åç½®ã€‚

+   `router_jitter_noise` (`float`, *optional*, defaults to 0.0) â€” æ·»åŠ åˆ°è·¯ç”±å™¨çš„å™ªå£°é‡ã€‚åœ¨é¢„æµ‹æœŸé—´å°†å…¶è®¾ç½®ä¸º 0.0ï¼Œæˆ–è€…åœ¨è®­ç»ƒæœŸé—´è®¾ç½®ä¸€ä¸ªå°å€¼ï¼ˆé€šå¸¸ä¸º 1e-2ï¼‰ã€‚

+   `router_dtype` (`str`, *optional*, default to `"float32"`) â€” ç”¨äºè·¯ç”±å™¨çš„`dtype`ã€‚æœ€å¥½å°†`dtype`ä¿æŒä¸ºåœ¨[è®ºæ–‡](https://arxiv.org/abs/2101.03961)ä¸­æŒ‡å®šçš„`"float32"`ç±»å‹ã€‚

+   `router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) â€” åœ¨è·¯ç”±æ—¶æ˜¯å¦å¿½ç•¥å¡«å……æ ‡è®°ã€‚

+   `output_hidden_states` (`bool`, *optional*, default to `False`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `output_attentions` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚

+   `initializer_factor` (`float`, *optional*, defaults to 0.002) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„å› å­ã€‚

+   `output_router_logits` (`bool`, *optional*, default to `False`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰ä¸“å®¶çš„è·¯ç”±å™¨ logitsã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚

è¿™æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œç”¨äºå­˜å‚¨ GPTSanJapaneseModel çš„é…ç½®ã€‚æ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª GPTSANJapanese æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº GPTSANJapanese[Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯» PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

## GPTSanJapaneseTokenizer

### `class transformers.GPTSanJapaneseTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74)

```py
( vocab_file emoji_file unk_token = '<|nottoken|>' pad_token = '<|separator|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' sep_token = '<|segmenter|>' do_clean_text = False **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚

+   `emoji_file` (`str`) â€” åŒ…å«è¡¨æƒ…ç¬¦å·çš„æ–‡ä»¶ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|nottoken|>"``ï¼‰--ç”¨äºæœªçŸ¥å­—ç¬¦çš„ä»¤ç‰Œ

+   `pad_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`â€œ<[UNK]åˆ†éš”ç¬¦[UNK]>â€`ï¼‰--ç”¨äºå¡«å……çš„ä»¤ç‰Œ

+   `bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|startoftext|>"``ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `sep_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|segmenter|>"``ï¼‰--ä¸€ä¸ªç‰¹æ®Šçš„ä»¤ç‰Œï¼Œç”¨äºåˆ†éš”å‰ç¼€éƒ¨åˆ†å’Œä¸€èˆ¬è¾“å…¥éƒ¨åˆ†çš„ä»¤ç‰Œã€‚

+   `do_clean_text`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ - æ˜¯å¦æ¸…ç† URLã€EMAILã€TELã€æ—¥è¯­æ—¥æœŸå’Œæ—¥è¯­ä»·æ ¼çš„æ–‡æœ¬ã€‚

æ­¤æ ‡è®°å™¨åŸºäº GPTNeoXJapaneseTokenizerï¼Œå¹¶å…·æœ‰ä»¥ä¸‹ä¿®æ”¹

+   æ­£ç¡®è§£ç å­—èŠ‚ 0~å­—èŠ‚ 255 çš„æ ‡è®°

+   æ·»åŠ äº† bagofword æ ‡è®°å¤„ç†

+   ä¸º Prefix-LM æ¨¡å‹è¿”å› token_type_idsã€‚ bagofword æ ‡è®°è¡¨ç¤ºå‰ä¸€ä¸ªæ ‡è®°çš„é‡å¤ï¼Œå¹¶åœ¨è§£ç æ—¶è½¬æ¢ä¸º 3 ä¸ªè¿ç»­çš„æ ‡è®°ã€‚æ­¤å¤–ï¼ŒåŸå§‹çš„æ—¥è¯­ç‰¹æ®Š Sub-Word-Encoding å·²åœ¨æ­¤å­˜å‚¨åº“ä¸­å‘å¸ƒï¼ˆ[`github.com/tanreinama/Japanese-BPEEncoder_V2`](https://github.com/tanreinama/Japanese-BPEEncoder_V2)ï¼‰ã€‚ token_type_ids æ˜¯ä¸€ä¸ªæ©ç ï¼ŒæŒ‡ç¤º Prefix-LM æ¨¡å‹çš„å‰ç¼€è¾“å…¥ä½ç½®ã€‚è¦æŒ‡å®šå‰ç¼€ä½ç½®ï¼Œè¯·ä¸º prefix_text æŒ‡å®šå‰ç¼€è¾“å…¥ï¼Œæˆ–å°†å‰ç¼€éƒ¨åˆ†å’Œå…¶åéƒ¨åˆ†ä½œä¸ºæ‰¹é‡è¾“å…¥çš„æ–‡æœ¬å¯¹æŒ‡å®šä¸ºå‰ç¼€éƒ¨åˆ†ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> # You can confirm both æ…¶å¿œ and æ…¶æ‡‰ are encoded to 17750
>>> tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"]
[35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

>>> # Both æ…¶å¿œ and æ…¶æ‡‰ are decoded to æ…¶å¿œ
>>> tokenizer.decode(tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"])
'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶å¿œ)å¤§å­¦å‡ºèº«'
```

å‰ç¼€-LM ç¤ºä¾‹ï¼š

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["input_ids"]
[35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

>>> # Mask for Prefix-LM inputs
>>> tokenizer("å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«", prefix_text="å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚")["token_type_ids"]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

æ‰¹é‡ç¼–ç ç¤ºä¾‹ï¼š

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["input_ids"]
[[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]

>>> # Mask for Prefix-LM inputs
>>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["token_type_ids"]
[[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]

>>> # Mask for padding
>>> tokenizer([["æ­¦ç”°ä¿¡ç„", "ã¯ã€"], ["ç¹”ç”°ä¿¡é•·", "ã®é…ä¸‹ã®ã€"]], padding=True)["attention_mask"]
[[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
```

#### `convert_tokens_to_string`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219)

```py
( tokens )
```

å°†ä¸€ç³»åˆ—æ ‡è®°ï¼ˆå­—ç¬¦ä¸²ï¼‰è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²ã€‚

#### `create_token_type_ids_from_sequences`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308)

```py
( token_ids_0: List token_ids_1: Optional = None )
```

æ ‡è®°å™¨è¿”å› token_type_ids ä½œä¸ºå‰ç¼€éƒ¨åˆ†å’Œå…¶ä½™éƒ¨åˆ†ä¹‹é—´çš„åˆ†éš”ç¬¦ã€‚ token_type_ids å¯¹äºå‰ç¼€éƒ¨åˆ†ä¸º 1ï¼Œå¯¹äºå…¶ä½™æ ‡è®°ä¸º 0ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("ï½±ï½²ï½³ï½´")
>>> # input_ids:      | SOT | SEG | ï½± | ï½² | ï½³ | ï½´ |
>>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |

>>> x_token = tokenizer("", prefix_text="ï½±ï½²ï½³ï½´")
>>> # input_ids:      | SOT | ï½± | ï½² | ï½³ | ï½´ | SEG |
>>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |

>>> x_token = tokenizer("ï½³ï½´", prefix_text="ï½±ï½²")
>>> # input_ids:      | SOT | ï½± | ï½² | SEG | ï½³ | ï½´ |
>>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
```

## GPTSanJapaneseModel

### `class transformers.GPTSanJapaneseModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855)

```py
( config: GPTSanJapaneseConfig )
```

å‚æ•°

+   `config`ï¼ˆGPTSanJapaneseConfigï¼‰ - å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸ GPTSAN-japanese æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚

[GPTSAN-japanese](https://github.com/tanreinama/GPTSAN)æ¨¡å‹æ˜¯åŸºäºé€šç”¨ Swich å˜å‹å™¨çš„æ—¥è¯­è¯­è¨€æ¨¡å‹

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None num_precontext: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰ - è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚GPTSAN-japanese æ˜¯ä¸€ä¸ªç”Ÿæˆå¥å­å»¶ç»­æˆ–é¢„æµ‹æ©ç ä½ç½®æ ‡è®°çš„æ¨¡å‹ã€‚ç”¨äºæ¨¡å‹è¾“å…¥çš„ç‰¹æ®Šæ ‡è®°ä¼šè‡ªåŠ¨é™„åŠ ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ - é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   å¯¹äº`æœªæ©ç `çš„æ ‡è®°ä¸º 1ï¼Œ

    +   å¯¹äº`masked`çš„æ ‡è®°ä¸º 0ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰ - ç”¨äºæ©ç›– Prefix-LM è¾“å…¥ä¸­çš„å‰ç¼€éƒ¨åˆ†çš„è¾“å…¥ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   å¯¹äº`prefix`è¾“å…¥çš„æ ‡è®°ï¼Œ

    +   å¯¹äº`not-prefix`è¾“å…¥çš„æ ‡è®°ä¸º 0ã€‚

+   `spout`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.d_spout)`çš„`torch.Tensor`ï¼‰ - é€šè¿‡ 8 å±‚ FFN è½¬æ¢çš„å‘é‡ï¼Œå¯ä»¥ç”¨äºæ›¿ä»£`past_key_values`ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º`config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`decoder_input_ids`ã€‚

+   `head_mask` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`ï¼Œ*optional*) â€” ç”¨äºå°†è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨ç½®é›¶çš„æ©ç ã€‚æ©ç å€¼é€‰å®šåœ¨`[0, 1]`ä¹‹é—´ï¼š

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ä»¥ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `decoder_inputs_embeds` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`ï¼Œ*optional*) â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œåˆ™å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*optional*) â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `router_logits` (`tuple(torch.FloatTensor)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_router_logits=True`æˆ–`config.add_router_probs=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, num_experts)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨æ¨¡å‹çš„è·¯ç”±å™¨å¯¹æ•°ï¼Œæœ‰åŠ©äºè®¡ç®—æ··åˆä¸“å®¶æ¨¡å‹çš„è¾…åŠ©æŸå¤±ã€‚

+   `num_precontext` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,1)`) â€” è¾“å…¥ä¸­`hybrid`æ ‡è®°çš„é•¿åº¦ã€‚ç›´åˆ°æ­¤é•¿åº¦çš„æ ‡è®°åŒæ—¶æŒ‡å‘å‰åï¼Œç±»ä¼¼äº BERTï¼Œä¹‹åçš„æ ‡è®°åªæŒ‡å‘å‰ï¼Œç±»ä¼¼äº GPTã€‚å¦è¯·å‚é˜…ï¼š[`github.com/tanreinama/GPTSAN/blob/main/report/model.md`](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)

GPTSanJapaneseModel çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

## GPTSanJapaneseForConditionalGeneration

### `class transformers.GPTSanJapaneseForConditionalGeneration`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100)

```py
( config: GPTSanJapaneseConfig )
```

å‚æ•°

+   `config`ï¼ˆGPTSanJapaneseConfigï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å…·æœ‰è¯­è¨€å»ºæ¨¡å¤´çš„è£¸ GPTSAN-japanese æ¨¡å‹ã€‚

[GPTSAN-japanese](https://github.com/tanreinama/GPTSAN)æ¨¡å‹æ˜¯åŸºäºé€šç”¨å¼€å…³å˜å‹å™¨çš„æ—¥è¯­è¯­è¨€æ¨¡å‹

æ­¤æ¨¡å‹è¿˜æ˜¯ä¸€ä¸ª PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1115)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None labels: Optional = None )
```

å‚æ•°

+   `input_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼‰- è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚GPTSAN-japanese æ˜¯ä¸€ä¸ªç”Ÿæˆå¥å­å»¶ç»­æˆ–é¢„æµ‹æ©ç ä½ç½®çš„æ ‡è®°çš„æ¨¡å‹ã€‚è¾“å…¥æ¨¡å‹æ‰€éœ€çš„ç‰¹æ®Šæ ‡è®°ä¼šè‡ªåŠ¨é™„åŠ ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`æœªè¢«æ©ç `çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`è¢«æ©ç `çš„æ ‡è®°ã€‚

    ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºæ©ç›–å‰ç¼€-LM è¾“å…¥ä¸­çš„å‰ç¼€éƒ¨åˆ†çš„è¾“å…¥ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

    +   1 è¡¨ç¤º`å‰ç¼€`è¾“å…¥çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤º`éå‰ç¼€`è¾“å…¥çš„æ ‡è®°ã€‚

+   `spout`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.d_spout)`çš„`torch.Tensor`ï¼‰- é€šè¿‡ 8 å±‚ FFN è½¬æ¢çš„å‘é‡ï¼Œå¯ä»¥ç”¨æ¥æ›¿ä»£`past_key_values`ã€‚

+   `past_key_values`ï¼ˆé•¿åº¦ä¸º`config.n_layers`çš„`tuple(tuple(torch.FloatTensor))`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`çš„å¼ é‡ï¼‰- åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚

    å¦‚æœä½¿ç”¨`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_size, 1)`çš„`decoder_input_ids`ï¼ˆé‚£äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰è€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨`[0, 1]`ä¹‹é—´ï¼š

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- å¦‚æœè®¾ç½®ä¸º`True`ï¼Œå°†è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `decoder_inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, target_sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰- å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`decoder_input_ids`ã€‚å¦‚æœä½¿ç”¨`past_key_values`ï¼Œå¯ä»¥é€‰æ‹©åªè¾“å…¥æœ€åä¸€ä¸ª`decoder_inputs_embeds`ï¼ˆå‚è§`past_key_values`ï¼‰ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`decoder_input_ids`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `router_logits` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_router_logits=True` æˆ– `config.add_router_probs=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, num_experts)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚è§£ç å™¨æ¨¡å‹çš„è·¯ç”±å™¨å¯¹æ•°ï¼Œç”¨äºè®¡ç®—æ··åˆä¸“å®¶æ¨¡å‹çš„è¾…åŠ©æŸå¤±ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[-100, 0, ..., config.vocab_size - 1]` ä¸­ã€‚æ‰€æœ‰è®¾ç½®ä¸º `-100` çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®— `[0, ..., config.vocab_size]` ä¸­çš„æ ‡ç­¾

GPTSanJapaneseForConditionalGeneration çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

å¸¦æœ‰å¸¸è§„ LM æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("ç¹”ç”°ä¿¡é•·ã¯ã€", return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> gen_token = model.generate(input_ids, max_new_tokens=50)
>>> tokenizer.decode(gen_token[0])
"ç¹”ç”°ä¿¡é•·ã¯ã€æ”¿æ²»ãƒ»è»äº‹ã®ä¸­æ¢ã¾ã§æŒæ¡ã—ãŸæ”¿æ²»å®¶ã§ã‚ã‚Šã€æ—¥æœ¬å²ä¸Šé¡ã‚’è¦‹ãªã„é©šç•°çš„ãªè»äº‹ä¾µæ”»ã‚’ç¶šã‘..."
```

å¸¦æœ‰å‰ç¼€-LM æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("", prefix_text="ç¹”ç”°ä¿¡é•·ã¯ã€", return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> token_type_ids = x_token.token_type_ids.to(device)
>>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)
>>> tokenizer.decode(gen_token[0])
"ç¹”ç”°ä¿¡é•·ã¯ã€æ”¿æ²»ãƒ»å¤–äº¤ã§æ•°ã€…ã®æˆ¦æœã‚’ä¸Šã’ã‚‹ãŒã€1568 å¹´ã‹ã‚‰ã¯ã€ã„ã‚ã‚†ã‚‹æœ¬èƒ½å¯ºã®å¤‰ã§ç´°å·æ™´å…ƒã«æš—æ®ºã•ã‚Œã‚‹..."
```

åŒæ—¶è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œæ©ç è¯­è¨€æ¨¡å‹

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> masked_sentence = "æ­¦ç”°ä¿¡ç„ã¯ã€<|inputmask|>æ™‚ä»£ãƒ•ã‚¡ãƒ³ãªã‚‰ãœã²æŠ¼ã•ãˆ<|inputmask|>ããŸã„åå°†ã®ä¸€äººã€‚"
>>> x_token = tokenizer("", prefix_text=masked_sentence, return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> token_type_ids = x_token.token_type_ids.to(device)
>>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)
>>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)
>>> tokenizer.decode(out_mlm_token[0])
"æ­¦ç”°ä¿¡ç„ã¯ã€æˆ¦å›½æ™‚ä»£ãƒ•ã‚¡ãƒ³ãªã‚‰ãœã²æŠ¼ã•ãˆã¦ãŠããŸã„åå°†ã®ä¸€äººã€‚"

>>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])
"æ­¦ç”°æ°ã®ä¸‰ä»£ã«æ¸¡ã£ãŸæ­¦ç”°å®¶ã®ã²ã¨ã‚Š\n ç”²æ–å¸‚ã«ä½ã‚€ã€æ—¥æœ¬å²ä¸Šæœ€å¤§ã®æˆ¦å›½å¤§åã€‚..."
```
