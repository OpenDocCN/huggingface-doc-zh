# GPTSAN-japanese

> 原始文本：[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gptsan-japanese)

## 概述

GPTSAN-japanese 模型由坂本俊之（tanreinama）在仓库中发布。

GPTSAN 是一个使用 Switch Transformer 的日语语言模型。它具有与 T5 论文中介绍的 Prefix LM 模型相同的结构，并支持文本生成和掩码语言建模任务。这些基本任务同样可以用于翻译或摘要的微调。

### 用法示例

`generate()` 方法可用于使用 GPTSAN-Japanese 模型生成文本。

```py
>>> from transformers import AutoModel, AutoTokenizer
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").cuda()
>>> x_tok = tokenizer("は、", prefix_text="織田信長", return_tensors="pt")
>>> torch.manual_seed(0)
>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)
>>> tokenizer.decode(gen_tok[0])
'織田信長は、2004 年に『戦国 BASARA』のために、豊臣秀吉'
```

## GPTSAN 特点

GPTSAN 具有一些独特的特点。它具有 Prefix-LM 的模型结构。它作为前缀输入 token 的移位掩码语言模型。未加前缀的输入行为类似于正常的生成模型。Spout 向量是 GPTSAN 特定的输入。Spout 在预训练时使用随机输入，但在微调期间可以指定文本类别或任意向量。这允许您指示生成文本的倾向。GPTSAN 具有基于 Switch-Transformer 的稀疏前馈。您还可以添加其他层并部分训练它们。有关详细信息，请参阅原始 GPTSAN 仓库。

### Prefix-LM 模型

GPTSAN 具有 T5 论文中称为 Prefix-LM 的模型结构。（原始 GPTSAN 仓库将其称为 `hybrid`）在 GPTSAN 中，Prefix-LM 的 `Prefix` 部分，即可以由两个 token 引用的输入位置，可以指定为任意长度。对于每个批次，也可以为不同的长度指定不同的长度。这个长度适用于 tokenizer 中输入的 `prefix_text` 文本。tokenizer 返回 Prefix-LM 的 `Prefix` 部分的掩码作为 `token_type_ids`。模型将其中 `token_type_ids` 为 1 的部分视为 `Prefix` 部分，即输入可以引用前后两个 token。

## 使用提示

通过传递给自注意力的掩码来指定前缀部分。当 token_type_ids=None 或全部为零时，等同于常规因果掩码。

例如：

> > > x_token = tokenizer(“ｱｲｳｴ”) input_ids: | SOT | SEG | ｱ | ｲ | ｳ | ｴ | token_type_ids: | 1 | 0 | 0 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 0 0 0 0 0 | SEG | 1 1 0 0 0 0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 1 0 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1 1 |
> > > 
> > > x_token = tokenizer("", prefix_text=“ｱｲｳｴ”) input_ids: | SOT | ｱ | ｲ | ｳ | ｴ | SEG | token_type_ids: | 1 | 1 | 1 | 1 | 1 | 0 | prefix_lm_mask: SOT | 1 1 1 1 1 0 | ｱ | 1 1 1 1 1 0 | ｲ | 1 1 1 1 1 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1 0 | SEG | 1 1 1 1 1 1 |
> > > 
> > > x_token = tokenizer(“ｳｴ”, prefix_text=“ｱｲ”) input_ids: | SOT | ｱ | ｲ | SEG | ｳ | ｴ | token_type_ids: | 1 | 1 | 1 | 0 | 0 | 0 | prefix_lm_mask: SOT | 1 1 1 0 0 0 | ｱ | 1 1 1 0 0 0 | ｲ | 1 1 1 0 0 0 | SEG | 1 1 1 1 0 0 | ｳ | 1 1 1 1 1 0 | ｴ | 1 1 1 1 1 1 |

### Spout Vector

Spout Vector 是用于控制文本生成的特殊向量。这个向量被视为自注意力中的第一个嵌入，以将额外的注意力引入生成的 token。在从 `Tanrei/GPTSAN-japanese` 发布的预训练模型中，Spout Vector 是一个通过模型中的 8 个全连接层传递的 128 维向量，并投影到充当外部注意力的空间。由全连接层投影的 Spout Vector 被分割以传递到所有自注意力。

## GPTSanJapaneseConfig

### `class transformers.GPTSanJapaneseConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/configuration_gptsan_japanese.py#L29)

```py
( vocab_size = 36000 max_position_embeddings = 1280 d_model = 1024 d_ff = 8192 d_ext = 4096 d_spout = 128 num_switch_layers = 10 num_ext_layers = 0 num_heads = 16 num_experts = 16 expert_capacity = 128 dropout_rate = 0.0 layer_norm_epsilon = 1e-05 router_bias = False router_jitter_noise = 0.0 router_dtype = 'float32' router_ignore_padding_tokens = False output_hidden_states = False output_attentions = False initializer_factor = 0.002 output_router_logits = False use_cache = True separator_token_id = 35998 pad_token_id = 35995 eos_token_id = 35999 **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 36000) — GPTSANJapanese 模型的词汇量。定义了在调用 GPTSanJapaneseModel 时可以表示的不同 token 数量。

+   `max_position_embeddings` (`int`, *optional*, defaults to 1280) — 该模型可能使用的最大序列长度。默认设置为 1280。

+   `d_model` (`int`, *optional*, defaults to 1024) — 编码器层和池化层的大小。

+   `d_ff` (`int`, *optional*, defaults to 8192) — 每个`SwitchTransformersBlock`中间级前馈层的大小。

+   `d_ext` (`int`, *optional*, defaults to 4096) — 额外层中间前馈层的大小。

+   `d_spout` (`int`, *optional*, defaults to 128) — `spout`向量的大小。

+   `num_switch_layers` (`int`, *optional*, defaults to 10) — Switch Transformer 层中的层数。

+   `num_ext_layers` (`int`, *optional*, defaults to 0) — 额外层中的层数。

+   `num_heads` (`int`, *optional*, defaults to 16) — Transformer 编码器中每个注意力层的注意力头数。

+   `num_experts` (`int`, *optional*, defaults to 16) — 每个 SwitchTransformer 层的专家数量。

+   `expert_capacity` (`int`, *optional*, defaults to 128) — 每个专家可以存储的令牌数量。如果设置为 1，则模型将表现得像一个常规 Transformer。

+   `dropout_rate` (`float`, *optional*, defaults to 0.0) — 所有 dropout 层的比率。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) — 层归一化层使用的 epsilon。

+   `router_bias` (`bool`, *optional*, defaults to `False`) — 是否向路由器添加偏置。

+   `router_jitter_noise` (`float`, *optional*, defaults to 0.0) — 添加到路由器的噪声量。在预测期间将其设置为 0.0，或者在训练期间设置一个小值（通常为 1e-2）。

+   `router_dtype` (`str`, *optional*, default to `"float32"`) — 用于路由器的`dtype`。最好将`dtype`保持为在[论文](https://arxiv.org/abs/2101.03961)中指定的`"float32"`类型。

+   `router_ignore_padding_tokens` (`bool`, *optional*, defaults to `False`) — 在路由时是否忽略填充标记。

+   `output_hidden_states` (`bool`, *optional*, default to `False`) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `output_attentions` (`bool`, *optional*, defaults to `False`) — 是否返回所有注意力层的注意力张量。

+   `initializer_factor` (`float`, *optional*, defaults to 0.002) — 用于初始化所有权重矩阵的因子。

+   `output_router_logits` (`bool`, *optional*, default to `False`) — 是否返回所有专家的路由器 logits。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。

这是一个配置类，用于存储 GPTSanJapaneseModel 的配置。根据指定的参数实例化一个 GPTSANJapanese 模型，定义模型架构。使用默认值实例化配置将产生类似于 GPTSANJapanese[Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese)架构的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

## GPTSanJapaneseTokenizer

### `class transformers.GPTSanJapaneseTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74)

```py
( vocab_file emoji_file unk_token = '<|nottoken|>' pad_token = '<|separator|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' sep_token = '<|segmenter|>' do_clean_text = False **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `emoji_file` (`str`) — 包含表情符号的文件。

+   `unk_token`（`str`，*optional*，默认为``"<|nottoken|>"``）--用于未知字符的令牌

+   `pad_token`（`str`，*optional*，默认为`“<[UNK]分隔符[UNK]>”`）--用于填充的令牌

+   `bos_token`（`str`，*optional*，默认为``"<|startoftext|>"``）--序列标记的开头。

+   `eos_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--序列结束标记。

+   `sep_token`（`str`，*optional*，默认为``"<|segmenter|>"``）--一个特殊的令牌，用于分隔前缀部分和一般输入部分的令牌。

+   `do_clean_text`（`bool`，*可选*，默认为`False`） - 是否清理 URL、EMAIL、TEL、日语日期和日语价格的文本。

此标记器基于 GPTNeoXJapaneseTokenizer，并具有以下修改

+   正确解码字节 0~字节 255 的标记

+   添加了 bagofword 标记处理

+   为 Prefix-LM 模型返回 token_type_ids。 bagofword 标记表示前一个标记的重复，并在解码时转换为 3 个连续的标记。此外，原始的日语特殊 Sub-Word-Encoding 已在此存储库中发布（[`github.com/tanreinama/Japanese-BPEEncoder_V2`](https://github.com/tanreinama/Japanese-BPEEncoder_V2)）。 token_type_ids 是一个掩码，指示 Prefix-LM 模型的前缀输入位置。要指定前缀位置，请为 prefix_text 指定前缀输入，或将前缀部分和其后部分作为批量输入的文本对指定为前缀部分。

示例：

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> # You can confirm both 慶応 and 慶應 are encoded to 17750
>>> tokenizer("吾輩は猫である🐯。実は慶応(慶應)大学出身")["input_ids"]
[35993, 35998, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

>>> # Both 慶応 and 慶應 are decoded to 慶応
>>> tokenizer.decode(tokenizer("吾輩は猫である🐯。実は慶応(慶應)大学出身")["input_ids"])
'吾輩は猫である🐯。実は慶応(慶応)大学出身'
```

前缀-LM 示例：

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> tokenizer("実は慶応(慶應)大学出身", prefix_text="吾輩は猫である🐯。")["input_ids"]
[35993, 34347, 31459, 30647, 31448, 25, 30659, 35729, 35676, 35998, 32417, 30647, 17750, 35589, 17750, 35590, 321, 1281]

>>> # Mask for Prefix-LM inputs
>>> tokenizer("実は慶応(慶應)大学出身", prefix_text="吾輩は猫である🐯。")["token_type_ids"]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

批量编码示例：

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> tokenizer([["武田信玄", "は、"], ["織田信長", "の配下の、"]], padding=True)["input_ids"]
[[35993, 8640, 25948, 35998, 30647, 35675, 35999, 35999], [35993, 10382, 9868, 35998, 30646, 9459, 30646, 35675]]

>>> # Mask for Prefix-LM inputs
>>> tokenizer([["武田信玄", "は、"], ["織田信長", "の配下の、"]], padding=True)["token_type_ids"]
[[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0]]

>>> # Mask for padding
>>> tokenizer([["武田信玄", "は、"], ["織田信長", "の配下の、"]], padding=True)["attention_mask"]
[[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]
```

#### `convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308)

```py
( token_ids_0: List token_ids_1: Optional = None )
```

标记器返回 token_type_ids 作为前缀部分和其余部分之间的分隔符。 token_type_ids 对于前缀部分为 1，对于其余标记为 0。

示例：

```py
>>> from transformers import GPTSanJapaneseTokenizer

>>> tokenizer = GPTSanJapaneseTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("ｱｲｳｴ")
>>> # input_ids:      | SOT | SEG | ｱ | ｲ | ｳ | ｴ |
>>> # token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |

>>> x_token = tokenizer("", prefix_text="ｱｲｳｴ")
>>> # input_ids:      | SOT | ｱ | ｲ | ｳ | ｴ | SEG |
>>> # token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |

>>> x_token = tokenizer("ｳｴ", prefix_text="ｱｲ")
>>> # input_ids:      | SOT | ｱ | ｲ | SEG | ｳ | ｴ |
>>> # token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
```

## GPTSanJapaneseModel

### `class transformers.GPTSanJapaneseModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855)

```py
( config: GPTSanJapaneseConfig )
```

参数

+   `config`（GPTSanJapaneseConfig） - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

裸 GPTSAN-japanese 模型变压器输出原始隐藏状态，没有特定的顶部头。

[GPTSAN-japanese](https://github.com/tanreinama/GPTSAN)模型是基于通用 Swich 变压器的日语语言模型

此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None num_precontext: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`） - 词汇表中输入序列标记的索引。GPTSAN-japanese 是一个生成句子延续或预测掩码位置标记的模型。用于模型输入的特殊标记会自动附加。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） - 避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   对于`未掩码`的标记为 1，

    +   对于`masked`的标记为 0。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） - 用于掩盖 Prefix-LM 输入中的前缀部分的输入。选择的掩码值为`[0, 1]`：

    +   对于`prefix`输入的标记，

    +   对于`not-prefix`输入的标记为 0。

+   `spout`（形状为`(batch_size, config.d_spout)`的`torch.Tensor`） - 通过 8 层 FFN 转换的向量，可以用于替代`past_key_values`。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为`config.n_layers`，每个元组有 4 个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量） — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用了`past_key_values`，用户可以选择仅输入最后的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）的形状为`(batch_size, 1)`的张量，而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于将自注意力模块的选定头部置零的掩码。掩码值选定在`[0, 1]`之间：

+   `use_cache` (`bool`, *optional*) — 如果设置为`True`，则返回`past_key_values`键值状态，可以用于加速解码（参见`past_key_values`）。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权，以便将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, target_sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用了`past_key_values`，则可以选择仅输入最后的`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制权，以便将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`，*optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回 ModelOutput 而不是普通元组。

+   `router_logits` (`tuple(torch.FloatTensor)`，*optional*，当传递`output_router_logits=True`或`config.add_router_probs=True`时返回） — 形状为`(batch_size, sequence_length, num_experts)`的`torch.FloatTensor`元组（每层一个）。解码器模型的路由器对数，有助于计算混合专家模型的辅助损失。

+   `num_precontext` (`torch.LongTensor`，形状为`(batch_size,1)`) — 输入中`hybrid`标记的长度。直到此长度的标记同时指向前后，类似于 BERT，之后的标记只指向前，类似于 GPT。另请参阅：[`github.com/tanreinama/GPTSAN/blob/main/report/model.md`](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md)

GPTSanJapaneseModel 的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行前处理和后处理步骤，而后者则默默地忽略它们。

## GPTSanJapaneseForConditionalGeneration

### `class transformers.GPTSanJapaneseForConditionalGeneration`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100)

```py
( config: GPTSanJapaneseConfig )
```

参数

+   `config`（GPTSanJapaneseConfig）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

具有语言建模头的裸 GPTSAN-japanese 模型。

[GPTSAN-japanese](https://github.com/tanreinama/GPTSAN)模型是基于通用开关变压器的日语语言模型

此模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1115)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None spout: Optional = None past_key_values: Optional = None head_mask: Optional = None use_cache: Optional = False inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None output_router_logits: Optional = None labels: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。GPTSAN-japanese 是一个生成句子延续或预测掩码位置的标记的模型。输入模型所需的特殊标记会自动附加。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1 表示`未被掩码`的标记，

    +   0 表示`被掩码`的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）- 用于掩盖前缀-LM 输入中的前缀部分的输入。掩码值选在`[0, 1]`之间：

    +   1 表示`前缀`输入的标记，

    +   0 表示`非前缀`输入的标记。

+   `spout`（形状为`(batch_size, config.d_spout)`的`torch.Tensor`）- 通过 8 层 FFN 转换的向量，可以用来替代`past_key_values`。

+   `past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有 4 个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量）- 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择只输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将它们的过去键值状态提供给此模型的）而不是形状为`(batch_size, sequence_length)`的所有`decoder_input_ids`。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值选在`[0, 1]`之间：

+   `use_cache`（`bool`，*可选*）- 如果设置为`True`，将返回`past_key_values`键值状态，并可用于加速解码（参见`past_key_values`）。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `decoder_inputs_embeds`（形状为`(batch_size, target_sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）- 可选地，您可以选择直接传递嵌入表示，而不是传递`decoder_input_ids`。如果使用`past_key_values`，可以选择只输入最后一个`decoder_inputs_embeds`（参见`past_key_values`）。如果您想要更多控制权来将`decoder_input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 ModelOutput 而不是一个普通元组。

+   `router_logits` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_router_logits=True` 或 `config.add_router_probs=True` 时返回) — 形状为 `(batch_size, sequence_length, num_experts)` 的 `torch.FloatTensor` 元组（每层一个）。解码器模型的路由器对数，用于计算混合专家模型的辅助损失。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算序列分类损失的标签。索引应在 `[-100, 0, ..., config.vocab_size - 1]` 中。所有设置为 `-100` 的标签都被忽略（掩码），损失仅计算 `[0, ..., config.vocab_size]` 中的标签

GPTSanJapaneseForConditionalGeneration 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

带有常规 LM 模型的文本生成

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("織田信長は、", return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> gen_token = model.generate(input_ids, max_new_tokens=50)
>>> tokenizer.decode(gen_token[0])
"織田信長は、政治・軍事の中枢まで掌握した政治家であり、日本史上類を見ない驚異的な軍事侵攻を続け..."
```

带有前缀-LM 模型的文本生成

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> x_token = tokenizer("", prefix_text="織田信長は、", return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> token_type_ids = x_token.token_type_ids.to(device)
>>> gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)
>>> tokenizer.decode(gen_token[0])
"織田信長は、政治・外交で数々の戦果を上げるが、1568 年からは、いわゆる本能寺の変で細川晴元に暗殺される..."
```

同时进行文本生成和掩码语言模型

```py
>>> from transformers import AutoModel, AutoTokenizer, trainer_utils

>>> device = "cuda"
>>> model = AutoModel.from_pretrained("Tanrei/GPTSAN-japanese").to(device)
>>> tokenizer = AutoTokenizer.from_pretrained("Tanrei/GPTSAN-japanese")
>>> masked_sentence = "武田信玄は、<|inputmask|>時代ファンならぜひ押さえ<|inputmask|>きたい名将の一人。"
>>> x_token = tokenizer("", prefix_text=masked_sentence, return_tensors="pt")
>>> trainer_utils.set_seed(30)
>>> input_ids = x_token.input_ids.to(device)
>>> token_type_ids = x_token.token_type_ids.to(device)
>>> out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=50)
>>> out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-1)
>>> tokenizer.decode(out_mlm_token[0])
"武田信玄は、戦国時代ファンならぜひ押さえておきたい名将の一人。"

>>> tokenizer.decode(out_lm_token[0][input_ids.shape[1] :])
"武田氏の三代に渡った武田家のひとり\n 甲斐市に住む、日本史上最大の戦国大名。..."
```
