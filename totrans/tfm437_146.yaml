- en: ByT5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ByT5
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained
    byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya
    Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin
    Raffel.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'ByT5模型由Linting Xue、Aditya Barua、Noah Constant、Rami Al-Rfou、Sharan Narang、Mihir
    Kale、Adam Roberts、Colin Raffel在[ByT5: Towards a token-free future with pre-trained
    byte-to-byte models](https://arxiv.org/abs/2105.13626)中提出。'
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*Most widely-used pre-trained language models operate on sequences of tokens
    corresponding to word or subword units. Encoding text as a sequence of tokens
    requires a tokenizer, which is typically created as an independent artifact from
    the model. Token-free models that instead operate directly on raw text (bytes
    or characters) have many benefits: they can process text in any language out of
    the box, they are more robust to noise, and they minimize technical debt by removing
    complex and error-prone text preprocessing pipelines. Since byte or character
    sequences are longer than token sequences, past work on token-free models has
    often introduced new model architectures designed to amortize the cost of operating
    directly on raw text. In this paper, we show that a standard Transformer architecture
    can be used with minimal modifications to process byte sequences. We carefully
    characterize the trade-offs in terms of parameter count, training FLOPs, and inference
    speed, and show that byte-level models are competitive with their token-level
    counterparts. We also demonstrate that byte-level models are significantly more
    robust to noise and perform better on tasks that are sensitive to spelling and
    pronunciation. As part of our contribution, we release a new set of pre-trained
    byte-level Transformer models based on the T5 architecture, as well as all code
    and data used in our experiments.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*大多数广泛使用的预训练语言模型操作的是与单词或子词单元对应的标记序列。将文本编码为标记序列需要一个分词器，通常作为模型的独立工件创建。直接在原始文本（字节或字符）上运行的无标记模型具有许多优点：它们可以直接处理任何语言的文本，对噪声更加稳健，并通过消除复杂和易出错的文本预处理流程来最小化技术债务。由于字节或字符序列比标记序列更长，过去关于无标记模型的工作通常引入了新的模型架构，旨在分摊直接在原始文本上运行的成本。在本文中，我们展示了标准Transformer架构可以在最小修改的情况下用于处理字节序列。我们仔细研究了参数数量、训练FLOPs和推理速度方面的权衡，并表明字节级模型与其标记级对应物具有竞争力。我们还证明了字节级模型对噪声更加稳健，并在对拼写和发音敏感的任务上表现更好。作为我们的贡献的一部分，我们发布了一组基于T5架构的新的预训练字节级Transformer模型，以及我们实验中使用的所有代码和数据。*'
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The original code can be found [here](https://github.com/google-research/byt5).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献的。原始代码可以在[这里](https://github.com/google-research/byt5)找到。
- en: ByT5’s architecture is based on the T5v1.1 model, refer to [T5v1.1’s documentation
    page](t5v1.1) for the API reference. They only differ in how inputs should be
    prepared for the model, see the code examples below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ByT5的架构基于T5v1.1模型，请参考[T5v1.1的文档页面](t5v1.1)获取API参考。它们只在输入如何为模型准备方面有所不同，请参见下面的代码示例。
- en: Since ByT5 was pre-trained unsupervisedly, there’s no real advantage to using
    a task prefix during single-task fine-tuning. If you are doing multi-task fine-tuning,
    you should use a prefix.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ByT5是无监督预训练的，单任务微调时使用任务前缀并没有真正的优势。如果进行多任务微调，则应使用前缀。
- en: Usage example
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法示例
- en: 'ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ByT5使用原始的UTF-8字节，因此可以在没有分词器的情况下使用：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For batched inference and training it is however recommended to make use of
    the tokenizer:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于批量推理和训练，建议使用分词器：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,
    since the model works directly on characters, the pretraining task is a bit different.
    Let’s corrupt some characters of the input sentence `"The dog chases a ball in
    the park."` and ask ByT5 to predict them for us.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与[T5](t5)类似，ByT5是在span-mask去噪任务上进行训练的。然而，由于该模型直接在字符上工作，预训练任务有些不同。让我们破坏输入句子`"The
    dog chases a ball in the park."`的一些字符，并要求ByT5为我们预测它们。
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ByT5Tokenizer
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ByT5Tokenizer
- en: '### `class transformers.ByT5Tokenizer`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.ByT5Tokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L28)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L28)'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 序列结束标记。'
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当构建使用特殊标记的序列时，这不是用于序列结束的标记。使用的标记是`sep_token`。
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`extra_ids` (`int`, *optional*, defaults to 125) — Add a number of extra ids
    added to the end of the vocabulary for use as sentinels. These tokens are accessible
    as “<extra>id{%d}>” where ”{%d}” is a number between 0 and extra_ids-1\. Extra
    tokens are indexed from the end of the vocabulary up to beginning (“<extra_id_0>”
    is the last token in the vocabulary like in ByT5 preprocessing see [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).</extra_id_0></extra>'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extra_ids` (`int`, *可选*, 默认为 125) — 添加一定数量的额外 ID，添加到词汇表末尾以用作哨兵。这些 token 可以作为“<extra>id{%d}>”访问，其中”{%d}”是
    0 到 extra_ids-1 之间的数字。额外 token 从词汇表末尾向开始索引（“<extra_id_0>”是词汇表中的最后一个 token，就像 ByT5
    预处理中看到的那样，请参见[此处](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)）。</extra_id_0></extra>'
- en: '`additional_special_tokens` (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`additional_special_tokens` (`List[str]`, *可选*) — 分词器使用的额外特殊 token。'
- en: Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 ByT5 分词器。ByT5 简单地使用原始字节 utf-8 编码。
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应参考此超类以获取有关这些方法的更多信息。
- en: '#### `build_inputs_with_special_tokens`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `build_inputs_with_special_tokens`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L172)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L172)'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — 将添加特殊 token 的 ID 列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。'
- en: Returns
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 带有适当特殊 token 的 [输入 ID](../glossary#input-ids) 列表。
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A sequence has the following
    format:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接和添加特殊 token，从序列或序列对构建用于序列分类任务的模型输入。序列的格式如下：
- en: 'single sequence: `X </s>`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个序列：`X </s>`
- en: 'pair of sequences: `A </s> B </s>`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列对：`A </s> B </s>`
- en: '#### `convert_tokens_to_string`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_tokens_to_string`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L218)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L218)'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Converts a sequence of tokens (string) in a single string.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将 token 序列（字符串）转换为单个字符串。
- en: '#### `create_token_type_ids_from_sequences`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `create_token_type_ids_from_sequences`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L150)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L150)'
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID 列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。'
- en: Returns
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: List of zeros.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 零的列表。
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. ByT5 does not make use of token type ids, therefore a list of zeros is returned.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从传递的两个序列创建一个用于序列对分类任务的掩码。ByT5 不使用 token 类型 id，因此返回一个零的列表。
- en: '#### `get_special_tokens_mask`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `get_special_tokens_mask`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L111)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L111)'
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_0` (`List[int]`) — ID 列表。'
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_ids_1` (`List[int]`, *可选*) — 序列对的可选第二个 ID 列表。'
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经为模型格式化了特殊
    token。'
- en: Returns
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`List[int]`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`List[int]`'
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个整数列表，范围为 [0, 1]：特殊 token 为 1，序列 token 为 0。
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从没有添加特殊 token 的 token 列表中检索序列 id。在使用分词器的 `prepare_for_model` 方法添加特殊 token 时调用此方法。
- en: See [ByT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/byt5#transformers.ByT5Tokenizer)
    for all details.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关所有详细信息，请参阅 [ByT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/byt5#transformers.ByT5Tokenizer)。
