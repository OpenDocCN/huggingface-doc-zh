- en: ByT5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/byt5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/83.b77bc5d4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained
    byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya
    Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin
    Raffel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Most widely-used pre-trained language models operate on sequences of tokens
    corresponding to word or subword units. Encoding text as a sequence of tokens
    requires a tokenizer, which is typically created as an independent artifact from
    the model. Token-free models that instead operate directly on raw text (bytes
    or characters) have many benefits: they can process text in any language out of
    the box, they are more robust to noise, and they minimize technical debt by removing
    complex and error-prone text preprocessing pipelines. Since byte or character
    sequences are longer than token sequences, past work on token-free models has
    often introduced new model architectures designed to amortize the cost of operating
    directly on raw text. In this paper, we show that a standard Transformer architecture
    can be used with minimal modifications to process byte sequences. We carefully
    characterize the trade-offs in terms of parameter count, training FLOPs, and inference
    speed, and show that byte-level models are competitive with their token-level
    counterparts. We also demonstrate that byte-level models are significantly more
    robust to noise and perform better on tasks that are sensitive to spelling and
    pronunciation. As part of our contribution, we release a new set of pre-trained
    byte-level Transformer models based on the T5 architecture, as well as all code
    and data used in our experiments.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).
    The original code can be found [here](https://github.com/google-research/byt5).
  prefs: []
  type: TYPE_NORMAL
- en: ByT5’s architecture is based on the T5v1.1 model, refer to [T5v1.1’s documentation
    page](t5v1.1) for the API reference. They only differ in how inputs should be
    prepared for the model, see the code examples below.
  prefs: []
  type: TYPE_NORMAL
- en: Since ByT5 was pre-trained unsupervisedly, there’s no real advantage to using
    a task prefix during single-task fine-tuning. If you are doing multi-task fine-tuning,
    you should use a prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Usage example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For batched inference and training it is however recommended to make use of
    the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,
    since the model works directly on characters, the pretraining task is a bit different.
    Let’s corrupt some characters of the input sentence `"The dog chases a ball in
    the park."` and ask ByT5 to predict them for us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ByT5Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ByT5Tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L28)'
  prefs: []
  type: TYPE_NORMAL
- en: ( eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' extra_ids = 125
    additional_special_tokens = None **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**eos_token** (`str`, *optional*, defaults to `"</s>"`) — The end of sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building a sequence using special tokens, this is not the token that is
    used for the end of sequence. The token used is the `sep_token`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**unk_token** (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_token** (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**extra_ids** (`int`, *optional*, defaults to 125) — Add a number of extra
    ids added to the end of the vocabulary for use as sentinels. These tokens are
    accessible as “<extra>id{%d}>” where ”{%d}” is a number between 0 and extra_ids-1\.
    Extra tokens are indexed from the end of the vocabulary up to beginning (“<extra_id_0>”
    is the last token in the vocabulary like in ByT5 preprocessing see [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).</extra_id_0></extra>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**additional_special_tokens** (`List[str]`, *optional*) — Additional special
    tokens used by the tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a ByT5 tokenizer. ByT5 simply uses raw bytes utf-8 encoding.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### build_inputs_with_special_tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L172)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs to which the special tokens will
    be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build model inputs from a sequence or a pair of sequence for sequence classification
    tasks by concatenating and adding special tokens. A sequence has the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'single sequence: `X </s>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pair of sequences: `A </s> B </s>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### convert_tokens_to_string'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L218)'
  prefs: []
  type: TYPE_NORMAL
- en: ( tokens )
  prefs: []
  type: TYPE_NORMAL
- en: Converts a sequence of tokens (string) in a single string.
  prefs: []
  type: TYPE_NORMAL
- en: '#### create_token_type_ids_from_sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L150)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: List of zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Create a mask from the two sequences passed to be used in a sequence-pair classification
    task. ByT5 does not make use of token type ids, therefore a list of zeros is returned.
  prefs: []
  type: TYPE_NORMAL
- en: '#### get_special_tokens_mask'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/byt5/tokenization_byt5.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens:
    bool = False ) → `List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**token_ids_0** (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_ids_1** (`List[int]`, *optional*) — Optional second list of IDs for
    sequence pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**already_has_special_tokens** (`bool`, *optional*, defaults to `False`) —
    Whether or not the token list is already formatted with special tokens for the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: See [ByT5Tokenizer](/docs/transformers/v4.37.2/en/model_doc/byt5#transformers.ByT5Tokenizer)
    for all details.
  prefs: []
  type: TYPE_NORMAL
