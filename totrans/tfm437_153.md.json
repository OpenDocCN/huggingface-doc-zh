["```py\n( vocab_size: int = 30720 hidden_size: int = 4096 num_attention_heads: int = 32 dim_head: int = 128 dim_ff: int = 10240 num_hidden_layers: int = 48 dropout_p: int = 0.0 position_bias_num_buckets: int = 512 position_bias_max_distance: int = 2048 eps: int = 1e-06 init_std: float = 1.0 prompt_types: int = 32 prompt_length: int = 32 segment_types: int = 32 use_cache: bool = True **kwargs )\n```", "```py\n>>> from transformers import CpmAntModel, CpmAntConfig\n\n>>> # Initializing a CPMAnt cpm-ant-10b style configuration\n>>> configuration = CpmAntConfig()\n\n>>> # Initializing a model from the cpm-ant-10b style configuration\n>>> model = CpmAntModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bod_token = '<d>' eod_token = '</d>' bos_token = '<s>' eos_token = '</s>' pad_token = '<pad>' unk_token = '<unk>' line_token = '</n>' space_token = '</_>' padding_side = 'left' **kwargs )\n```", "```py\n( token_ids_0: List token_ids_1: List = None ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) \u2192 export const metadata = 'undefined';List[int]\n```", "```py\n( config: CpmAntConfig )\n```", "```py\n( input_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None past_key_values: Optional = None use_cache: Optional = None return_dict: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, CpmAntModel\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openbmb/cpm-ant-10b\")\n>>> model = CpmAntModel.from_pretrained(\"openbmb/cpm-ant-10b\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: CpmAntConfig )\n```", "```py\n( input_ids: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None attention_mask: Optional = None **kwargs ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoTokenizer, CpmAntForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"openbmb/cpm-ant-10b\")\n>>> model = CpmAntForCausalLM.from_pretrained(\"openbmb/cpm-ant-10b\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs, labels=inputs[\"input_ids\"])\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n```"]