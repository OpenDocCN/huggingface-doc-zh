- en: Performing gradient accumulation with ðŸ¤— Accelerate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation is a technique where you can train on bigger batch sizes
    than your machine would normally be able to fit into memory. This is done by accumulating
    gradients over several batches, and only stepping the optimizer after a certain
    number of batches have been performed.
  prefs: []
  type: TYPE_NORMAL
- en: While technically standard gradient accumulation code would work fine in a distributed
    setup, it is not the most efficient method for doing so and you may experience
    considerable slowdowns!
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will see how to quickly setup gradient accumulation and
    perform it with the utilities provided in ðŸ¤— Accelerate, which can total to adding
    just one new line of code!
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will use a very simplistic PyTorch training loop that performs
    gradient accumulation every two batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Converting it to ðŸ¤— Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First the code shown earlier will be converted to utilize ðŸ¤— Accelerate without
    the special gradient accumulation helper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In its current state, this code is not going to perform gradient accumulation
    efficiently due to a process called gradient synchronization. Read more about
    that in the [Concepts tutorial](../concept_guides/gradient_synchronization)!
  prefs: []
  type: TYPE_NORMAL
- en: Letting ðŸ¤— Accelerate handle gradient accumulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All that is left now is to let ðŸ¤— Accelerate handle the gradient accumulation
    for us. To do so you should pass in a `gradient_accumulation_steps` parameter
    to [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator),
    dictating the number of steps to perform before each call to `step()` and how
    to automatically adjust the loss during the call to [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can pass in a `gradient_accumulation_plugin` parameter to
    the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    objectâ€™s `__init__`, which will allow you to further customize the gradient accumulation
    behavior. Read more about that in the [GradientAccumulationPlugin](../package_reference/accelerator#accelerate.utils.GradientAccumulationPlugin)
    docs.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here you can use the [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    context manager from inside your training loop to automatically perform the gradient
    accumulation for you! You just wrap it around the entire training part of our
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can remove all the special checks for the step number and the loss adjustment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is able to keep track of the batch number you are on and it will automatically
    know whether to step through the prepared optimizer and how to adjust the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Typically with gradient accumulation, you would need to adjust the number of
    steps to reflect the change in total batches you are training on. ðŸ¤— Accelerate
    automagically does this for you by default. Behind the scenes we instantiate a
    `GradientAccumulationPlugin` configured to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [state.GradientState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.GradientState)
    is syncâ€™d with the active dataloader being iterated upon. As such it assumes naively
    that when we have reached the end of the dataloader everything will sync and a
    step will be performed. To disable this, set `sync_with_dataloader` to be `False`
    in the `GradientAccumulationPlugin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The finished code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below is the finished implementation for performing gradient accumulation with
    ðŸ¤— Accelerate
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Itâ€™s important that **only one forward/backward** should be done inside the
    context manager `with accelerator.accumulate(model)`.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about what magic this wraps around, read the [Gradient Synchronization
    concept guide](../concept_guides/gradient_synchronization)
  prefs: []
  type: TYPE_NORMAL
- en: Self-contained example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a self-contained example that you can run to see gradient accumulation
    in action with ðŸ¤— Accelerate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
