- en: Performing gradient accumulation with 🤗 Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation is a technique where you can train on bigger batch sizes
    than your machine would normally be able to fit into memory. This is done by accumulating
    gradients over several batches, and only stepping the optimizer after a certain
    number of batches have been performed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: While technically standard gradient accumulation code would work fine in a distributed
    setup, it is not the most efficient method for doing so and you may experience
    considerable slowdowns!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial you will see how to quickly setup gradient accumulation and
    perform it with the utilities provided in 🤗 Accelerate, which can total to adding
    just one new line of code!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will use a very simplistic PyTorch training loop that performs
    gradient accumulation every two batches:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Converting it to 🤗 Accelerate
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First the code shown earlier will be converted to utilize 🤗 Accelerate without
    the special gradient accumulation helper:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In its current state, this code is not going to perform gradient accumulation
    efficiently due to a process called gradient synchronization. Read more about
    that in the [Concepts tutorial](../concept_guides/gradient_synchronization)!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Letting 🤗 Accelerate handle gradient accumulation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All that is left now is to let 🤗 Accelerate handle the gradient accumulation
    for us. To do so you should pass in a `gradient_accumulation_steps` parameter
    to [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator),
    dictating the number of steps to perform before each call to `step()` and how
    to automatically adjust the loss during the call to [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Alternatively, you can pass in a `gradient_accumulation_plugin` parameter to
    the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    object’s `__init__`, which will allow you to further customize the gradient accumulation
    behavior. Read more about that in the [GradientAccumulationPlugin](../package_reference/accelerator#accelerate.utils.GradientAccumulationPlugin)
    docs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'From here you can use the [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    context manager from inside your training loop to automatically perform the gradient
    accumulation for you! You just wrap it around the entire training part of our
    code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can remove all the special checks for the step number and the loss adjustment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is able to keep track of the batch number you are on and it will automatically
    know whether to step through the prepared optimizer and how to adjust the loss.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Typically with gradient accumulation, you would need to adjust the number of
    steps to reflect the change in total batches you are training on. 🤗 Accelerate
    automagically does this for you by default. Behind the scenes we instantiate a
    `GradientAccumulationPlugin` configured to do this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The [state.GradientState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.GradientState)
    is sync’d with the active dataloader being iterated upon. As such it assumes naively
    that when we have reached the end of the dataloader everything will sync and a
    step will be performed. To disable this, set `sync_with_dataloader` to be `False`
    in the `GradientAccumulationPlugin`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The finished code
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below is the finished implementation for performing gradient accumulation with
    🤗 Accelerate
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It’s important that **only one forward/backward** should be done inside the
    context manager `with accelerator.accumulate(model)`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about what magic this wraps around, read the [Gradient Synchronization
    concept guide](../concept_guides/gradient_synchronization)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这个包装的魔力是什么，请阅读[梯度同步概念指南](../concept_guides/gradient_synchronization)。
- en: Self-contained example
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自包含示例
- en: 'Here is a self-contained example that you can run to see gradient accumulation
    in action with 🤗 Accelerate:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个自包含示例，您可以运行它来查看🤗 Accelerate中的梯度累积。
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
