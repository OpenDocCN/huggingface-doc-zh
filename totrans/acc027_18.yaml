- en: Performing gradient accumulation with ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ğŸ¤— Accelerateæ‰§è¡Œæ¢¯åº¦ç´¯ç§¯
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Gradient accumulation is a technique where you can train on bigger batch sizes
    than your machine would normally be able to fit into memory. This is done by accumulating
    gradients over several batches, and only stepping the optimizer after a certain
    number of batches have been performed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ç´¯ç§¯æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥åœ¨æ¯”æ‚¨çš„æœºå™¨é€šå¸¸èƒ½å¤Ÿå®¹çº³çš„æ›´å¤§æ‰¹æ¬¡å¤§å°ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™æ˜¯é€šè¿‡åœ¨å‡ ä¸ªæ‰¹æ¬¡ä¸Šç´¯ç§¯æ¢¯åº¦ï¼Œå¹¶ä¸”åªæœ‰åœ¨æ‰§è¡Œäº†ä¸€å®šæ•°é‡çš„æ‰¹æ¬¡ä¹‹åæ‰ä¼šè°ƒæ•´ä¼˜åŒ–å™¨ã€‚
- en: While technically standard gradient accumulation code would work fine in a distributed
    setup, it is not the most efficient method for doing so and you may experience
    considerable slowdowns!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åœ¨æŠ€æœ¯ä¸Šï¼Œæ ‡å‡†çš„æ¢¯åº¦ç´¯ç§¯ä»£ç åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­å¯ä»¥æ­£å¸¸å·¥ä½œï¼Œä½†è¿™å¹¶ä¸æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ç›¸å½“å¤§çš„å‡é€Ÿï¼
- en: In this tutorial you will see how to quickly setup gradient accumulation and
    perform it with the utilities provided in ğŸ¤— Accelerate, which can total to adding
    just one new line of code!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å°†çœ‹åˆ°å¦‚ä½•å¿«é€Ÿè®¾ç½®æ¢¯åº¦ç´¯ç§¯å¹¶ä½¿ç”¨ğŸ¤— Accelerateæä¾›çš„å®ç”¨ç¨‹åºæ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ï¼Œè¿™å¯èƒ½åªéœ€è¦æ·»åŠ ä¸€è¡Œæ–°ä»£ç ï¼
- en: 'This example will use a very simplistic PyTorch training loop that performs
    gradient accumulation every two batches:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç¤ºä¾‹å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸ç®€å•çš„PyTorchè®­ç»ƒå¾ªç¯ï¼Œæ¯ä¸¤ä¸ªæ‰¹æ¬¡æ‰§è¡Œä¸€æ¬¡æ¢¯åº¦ç´¯ç§¯ï¼š
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Converting it to ğŸ¤— Accelerate
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†å…¶è½¬æ¢ä¸ºğŸ¤— Accelerate
- en: 'First the code shown earlier will be converted to utilize ğŸ¤— Accelerate without
    the special gradient accumulation helper:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå…ˆå‰æ˜¾ç¤ºçš„ä»£ç å°†è¢«è½¬æ¢ä¸ºåˆ©ç”¨ğŸ¤— Accelerateè€Œä¸ä½¿ç”¨ç‰¹æ®Šçš„æ¢¯åº¦ç´¯ç§¯åŠ©æ‰‹ï¼š
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In its current state, this code is not going to perform gradient accumulation
    efficiently due to a process called gradient synchronization. Read more about
    that in the [Concepts tutorial](../concept_guides/gradient_synchronization)!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰çŠ¶æ€ä¸‹ï¼Œç”±äºæ¢¯åº¦åŒæ­¥çš„è¿‡ç¨‹ï¼Œè¿™æ®µä»£ç ä¸ä¼šæœ‰æ•ˆåœ°æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ã€‚åœ¨[Concepts tutorial](../concept_guides/gradient_synchronization)ä¸­äº†è§£æ›´å¤šä¿¡æ¯ï¼
- en: Letting ğŸ¤— Accelerate handle gradient accumulation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©ğŸ¤— Accelerateå¤„ç†æ¢¯åº¦ç´¯ç§¯
- en: 'All that is left now is to let ğŸ¤— Accelerate handle the gradient accumulation
    for us. To do so you should pass in a `gradient_accumulation_steps` parameter
    to [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator),
    dictating the number of steps to perform before each call to `step()` and how
    to automatically adjust the loss during the call to [backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å‰©ä¸‹çš„å°±æ˜¯è®©ğŸ¤— Accelerateä¸ºæˆ‘ä»¬å¤„ç†æ¢¯åº¦ç´¯ç§¯ã€‚ä¸ºæ­¤ï¼Œæ‚¨åº”è¯¥åœ¨[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)ä¸­ä¼ å…¥ä¸€ä¸ª`gradient_accumulation_steps`å‚æ•°ï¼ŒæŒ‡å®šåœ¨æ¯æ¬¡è°ƒç”¨`step()`ä¹‹å‰æ‰§è¡Œçš„æ­¥æ•°ä»¥åŠå¦‚ä½•åœ¨è°ƒç”¨[backward()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.backward)æœŸé—´è‡ªåŠ¨è°ƒæ•´æŸå¤±ï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Alternatively, you can pass in a `gradient_accumulation_plugin` parameter to
    the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    objectâ€™s `__init__`, which will allow you to further customize the gradient accumulation
    behavior. Read more about that in the [GradientAccumulationPlugin](../package_reference/accelerator#accelerate.utils.GradientAccumulationPlugin)
    docs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ‚¨å¯ä»¥åœ¨[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)å¯¹è±¡çš„`__init__`ä¸­ä¼ å…¥ä¸€ä¸ª`gradient_accumulation_plugin`å‚æ•°ï¼Œè¿™å°†å…è®¸æ‚¨è¿›ä¸€æ­¥è‡ªå®šä¹‰æ¢¯åº¦ç´¯ç§¯è¡Œä¸ºã€‚åœ¨[GradientAccumulationPlugin](../package_reference/accelerator#accelerate.utils.GradientAccumulationPlugin)æ–‡æ¡£ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚
- en: 'From here you can use the [accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)
    context manager from inside your training loop to automatically perform the gradient
    accumulation for you! You just wrap it around the entire training part of our
    code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œæ‚¨å¯ä»¥åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨[accumulate()](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator.accumulate)ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨ä¸ºæ‚¨æ‰§è¡Œæ¢¯åº¦ç´¯ç§¯ï¼æ‚¨åªéœ€å°†å…¶åŒ…è£…åœ¨æˆ‘ä»¬ä»£ç çš„æ•´ä¸ªè®­ç»ƒéƒ¨åˆ†å‘¨å›´ï¼š
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can remove all the special checks for the step number and the loss adjustment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åˆ é™¤æ‰€æœ‰å…³äºæ­¥æ•°å’ŒæŸå¤±è°ƒæ•´çš„ç‰¹æ®Šæ£€æŸ¥ï¼š
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see the [Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)
    is able to keep track of the batch number you are on and it will automatically
    know whether to step through the prepared optimizer and how to adjust the loss.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œ[Accelerator](/docs/accelerate/v0.27.2/en/package_reference/accelerator#accelerate.Accelerator)èƒ½å¤Ÿè·Ÿè¸ªæ‚¨æ‰€åœ¨çš„æ‰¹æ¬¡å·ï¼Œå¹¶ä¸”å®ƒå°†è‡ªåŠ¨çŸ¥é“æ˜¯å¦è¦é€šè¿‡å‡†å¤‡å¥½çš„ä¼˜åŒ–å™¨è¿›è¡Œæ­¥è¿›ä»¥åŠå¦‚ä½•è°ƒæ•´æŸå¤±ã€‚
- en: Typically with gradient accumulation, you would need to adjust the number of
    steps to reflect the change in total batches you are training on. ğŸ¤— Accelerate
    automagically does this for you by default. Behind the scenes we instantiate a
    `GradientAccumulationPlugin` configured to do this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œæ‚¨éœ€è¦è°ƒæ•´æ­¥æ•°ä»¥åæ˜ æ‚¨æ­£åœ¨è®­ç»ƒçš„æ€»æ‰¹æ¬¡çš„å˜åŒ–ã€‚ğŸ¤— Accelerateé»˜è®¤ä¼šè‡ªåŠ¨ä¸ºæ‚¨æ‰§è¡Œæ­¤æ“ä½œã€‚åœ¨å¹•åï¼Œæˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸€ä¸ªé…ç½®ä¸ºæ‰§è¡Œæ­¤æ“ä½œçš„`GradientAccumulationPlugin`ã€‚
- en: 'The [state.GradientState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.GradientState)
    is syncâ€™d with the active dataloader being iterated upon. As such it assumes naively
    that when we have reached the end of the dataloader everything will sync and a
    step will be performed. To disable this, set `sync_with_dataloader` to be `False`
    in the `GradientAccumulationPlugin`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[state.GradientState](/docs/accelerate/v0.27.2/en/package_reference/state#accelerate.state.GradientState)ä¸æ­£åœ¨è¿­ä»£çš„æ´»åŠ¨æ•°æ®åŠ è½½å™¨è¿›è¡ŒåŒæ­¥ã€‚å› æ­¤ï¼Œå®ƒå¤©çœŸåœ°å‡è®¾å½“æˆ‘ä»¬åˆ°è¾¾æ•°æ®åŠ è½½å™¨çš„æœ«å°¾æ—¶ï¼Œä¸€åˆ‡éƒ½ä¼šåŒæ­¥å¹¶æ‰§è¡Œä¸€æ­¥ã€‚è¦ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨`GradientAccumulationPlugin`ä¸­å°†`sync_with_dataloader`è®¾ç½®ä¸º`False`ï¼š'
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The finished code
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®Œæˆçš„ä»£ç 
- en: Below is the finished implementation for performing gradient accumulation with
    ğŸ¤— Accelerate
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä½¿ç”¨ğŸ¤— Accelerateæ‰§è¡Œæ¢¯åº¦ç´¯ç§¯çš„å®Œæˆå®ç°
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Itâ€™s important that **only one forward/backward** should be done inside the
    context manager `with accelerator.accumulate(model)`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨åŠ é€Ÿå™¨.accumulate(model)çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­åªåº”è¯¥æ‰§è¡Œä¸€æ¬¡å‰å‘/åå‘æ“ä½œ**ã€‚'
- en: To learn more about what magic this wraps around, read the [Gradient Synchronization
    concept guide](../concept_guides/gradient_synchronization)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£è¿™ä¸ªåŒ…è£…çš„é­”åŠ›æ˜¯ä»€ä¹ˆï¼Œè¯·é˜…è¯»[æ¢¯åº¦åŒæ­¥æ¦‚å¿µæŒ‡å—](../concept_guides/gradient_synchronization)ã€‚
- en: Self-contained example
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªåŒ…å«ç¤ºä¾‹
- en: 'Here is a self-contained example that you can run to see gradient accumulation
    in action with ğŸ¤— Accelerate:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªè‡ªåŒ…å«ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œå®ƒæ¥æŸ¥çœ‹ğŸ¤— Accelerateä¸­çš„æ¢¯åº¦ç´¯ç§¯ã€‚
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
