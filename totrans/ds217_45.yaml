- en: Load tabular data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ è½½è¡¨æ ¼æ•°æ®
- en: 'Original text: [https://huggingface.co/docs/datasets/tabular_load](https://huggingface.co/docs/datasets/tabular_load)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/tabular_load](https://huggingface.co/docs/datasets/tabular_load)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'A tabular dataset is a generic dataset used to describe any data stored in
    rows and columns, where the rows represent an example and the columns represent
    a feature (can be continuous or categorical). These datasets are commonly stored
    in CSV files, Pandas DataFrames, and in database tables. This guide will show
    you how to load and create a tabular dataset from:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼æ•°æ®é›†æ˜¯ç”¨äºæè¿°ä»¥è¡Œå’Œåˆ—å­˜å‚¨çš„ä»»ä½•æ•°æ®çš„é€šç”¨æ•°æ®é›†ï¼Œå…¶ä¸­è¡Œä»£è¡¨ä¸€ä¸ªç¤ºä¾‹ï¼Œåˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼ˆå¯ä»¥æ˜¯è¿ç»­çš„æˆ–åˆ†ç±»çš„ï¼‰ã€‚è¿™äº›æ•°æ®é›†é€šå¸¸å­˜å‚¨åœ¨CSVæ–‡ä»¶ã€Pandasæ•°æ®æ¡†å’Œæ•°æ®åº“è¡¨ä¸­ã€‚æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»ä»¥ä¸‹å†…å®¹åŠ è½½å’Œåˆ›å»ºè¡¨æ ¼æ•°æ®é›†ï¼š
- en: CSV files
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSVæ–‡ä»¶
- en: Pandas DataFrames
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandasæ•°æ®æ¡†
- en: Databases
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®åº“
- en: CSV files
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSVæ–‡ä»¶
- en: 'ğŸ¤— Datasets can read CSV files by specifying the generic `csv` dataset builder
    name in the [load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)
    method. To load more than one CSV file, pass them as a list to the `data_files`
    parameter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†å¯ä»¥é€šè¿‡åœ¨[load_dataset()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_dataset)æ–¹æ³•ä¸­æŒ‡å®šé€šç”¨çš„`csv`æ•°æ®é›†æ„å»ºå™¨åç§°æ¥è¯»å–CSVæ–‡ä»¶ã€‚è¦åŠ è½½å¤šä¸ªCSVæ–‡ä»¶ï¼Œå°†å®ƒä»¬ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™`data_files`å‚æ•°ï¼š
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also map specific CSV files to the train and test splits:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥å°†ç‰¹å®šçš„CSVæ–‡ä»¶æ˜ å°„åˆ°è®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ï¼š
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To load remote CSV files, pass the URLs instead:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŠ è½½è¿œç¨‹CSVæ–‡ä»¶ï¼Œè¯·ä¼ é€’URLsï¼š
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To load zipped CSV files:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŠ è½½å‹ç¼©çš„CSVæ–‡ä»¶ï¼š
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Pandas DataFrames
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandasæ•°æ®æ¡†
- en: 'ğŸ¤— Datasets also supports loading datasets from [Pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
    with the [from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas)
    method:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤—æ•°æ®é›†è¿˜æ”¯æŒä½¿ç”¨[from_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_pandas)æ–¹æ³•ä»[Pandasæ•°æ®æ¡†](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)åŠ è½½æ•°æ®é›†ï¼š
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use the `splits` parameter to specify the name of the dataset split:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`splits`å‚æ•°æŒ‡å®šæ•°æ®é›†æ‹†åˆ†çš„åç§°ï¼š
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If the dataset doesnâ€™t look as expected, you should explicitly [specify your
    dataset features](loading#specify-features). A [pandas.Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)
    may not always carry enough information for Arrow to automatically infer a data
    type. For example, if a DataFrame is of length `0` or if the Series only contains
    `None/NaN` objects, the type is set to `null`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ•°æ®é›†çœ‹èµ·æ¥ä¸å¦‚é¢„æœŸï¼Œæ‚¨åº”è¯¥æ˜ç¡®[æŒ‡å®šæ•°æ®é›†ç‰¹å¾](loading#specify-features)ã€‚[pandas.Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html)å¯èƒ½å¹¶ä¸æ€»æ˜¯æºå¸¦è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä¾¿Arrowè‡ªåŠ¨æ¨æ–­æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœDataFrameçš„é•¿åº¦ä¸º`0`ï¼Œæˆ–è€…SeriesåªåŒ…å«`None/NaN`å¯¹è±¡ï¼Œç±»å‹å°†è¢«è®¾ç½®ä¸º`null`ã€‚
- en: Databases
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®åº“
- en: Datasets stored in databases are typically accessed with SQL queries. With ğŸ¤—
    Datasets, you can connect to a database, query for the data you need, and create
    a dataset out of it. Then you can use all the processing features of ğŸ¤— Datasets
    to prepare your dataset for training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸é€šè¿‡SQLæŸ¥è¯¢è®¿é—®å­˜å‚¨åœ¨æ•°æ®åº“ä¸­çš„æ•°æ®é›†ã€‚ä½¿ç”¨ğŸ¤—æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥è¿æ¥åˆ°æ•°æ®åº“ï¼ŒæŸ¥è¯¢æ‰€éœ€çš„æ•°æ®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ğŸ¤—æ•°æ®é›†çš„æ‰€æœ‰å¤„ç†åŠŸèƒ½æ¥å‡†å¤‡æ‚¨çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚
- en: SQLite
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SQLite
- en: SQLite is a small, lightweight database that is fast and easy to set up. You
    can use an existing database if youâ€™d like, or follow along and start from scratch.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SQLiteæ˜¯ä¸€ä¸ªå°å‹ã€è½»é‡çº§çš„æ•°æ®åº“ï¼Œå¿«é€Ÿä¸”æ˜“äºè®¾ç½®ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ç°æœ‰çš„æ•°æ®åº“ï¼Œæˆ–è€…è·Ÿç€å¼€å§‹ä»å¤´å¼€å§‹ã€‚
- en: 'Start by creating a quick SQLite database with this [Covid-19 data](https://github.com/nytimes/covid-19-data/blob/master/us-states.csv)
    from the New York Times:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œé€šè¿‡è¿™ä¸ª[çº½çº¦æ—¶æŠ¥çš„Covid-19æ•°æ®](https://github.com/nytimes/covid-19-data/blob/master/us-states.csv)åˆ›å»ºä¸€ä¸ªå¿«é€Ÿçš„SQLiteæ•°æ®åº“ï¼š
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This creates a `states` table in the `us_covid_data.db` database which you can
    now load into a dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨`us_covid_data.db`æ•°æ®åº“ä¸­åˆ›å»ºä¸€ä¸ª`states`è¡¨ï¼Œæ‚¨ç°åœ¨å¯ä»¥å°†å…¶åŠ è½½åˆ°æ•°æ®é›†ä¸­ã€‚
- en: To connect to the database, youâ€™ll need the [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    that identifies your database. Connecting to a database with a URI caches the
    returned dataset. The URI string differs for each database dialect, so be sure
    to check the [Database URLs](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)
    for whichever database youâ€™re using.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿æ¥åˆ°æ•°æ®åº“ï¼Œæ‚¨éœ€è¦æ ‡è¯†æ•°æ®åº“çš„[URIå­—ç¬¦ä¸²](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)ã€‚ä½¿ç”¨URIå­—ç¬¦ä¸²è¿æ¥åˆ°æ•°æ®åº“ä¼šç¼“å­˜è¿”å›çš„æ•°æ®é›†ã€‚æ¯ä¸ªæ•°æ®åº“æ–¹è¨€çš„URIå­—ç¬¦ä¸²éƒ½ä¸åŒï¼Œå› æ­¤è¯·ç¡®ä¿æŸ¥çœ‹æ‚¨ä½¿ç”¨çš„æ•°æ®åº“çš„[æ•°æ®åº“URL](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls)ã€‚
- en: 'For SQLite, it is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºSQLiteï¼Œæ˜¯è¿™æ ·çš„ï¼š
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the table by passing the table name and URI to [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†è¡¨åå’ŒURIä¼ é€’ç»™[from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)æ¥åŠ è½½è¡¨æ ¼ï¼š
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then you can use all of ğŸ¤— Datasets process features like [filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)
    for example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‰€æœ‰ğŸ¤—æ•°æ®é›†å¤„ç†åŠŸèƒ½ï¼Œä¾‹å¦‚[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ï¼š
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can also load a dataset from a SQL query instead of an entire table, which
    is useful for querying and joining multiple tables.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä»SQLæŸ¥è¯¢è€Œä¸æ˜¯æ•´ä¸ªè¡¨åŠ è½½æ•°æ®é›†ï¼Œè¿™å¯¹äºæŸ¥è¯¢å’Œè¿æ¥å¤šä¸ªè¡¨éå¸¸æœ‰ç”¨ã€‚
- en: 'Load the dataset by passing your query and URI to [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æŸ¥è¯¢å’ŒURIä¼ é€’ç»™[from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)æ¥åŠ è½½æ•°æ®é›†ï¼š
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then you can use all of ğŸ¤— Datasets process features like [filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)
    for example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‰€æœ‰ğŸ¤—æ•°æ®é›†å¤„ç†åŠŸèƒ½ï¼Œä¾‹å¦‚[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ï¼š
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: PostgreSQL
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: You can also connect and load a dataset from a PostgreSQL database, however
    we wonâ€™t directly demonstrate how in the documentation because the example is
    only meant to be run in a notebook. Instead, take a look at how to install and
    setup a PostgreSQL server in this [notebook](https://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/sql_with_huggingface_datasets.ipynb#scrollTo=d83yGQMPHGFi)!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨ä¹Ÿå¯ä»¥è¿æ¥å¹¶ä»PostgreSQLæ•°æ®åº“åŠ è½½æ•°æ®é›†ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸ä¼šåœ¨æ–‡æ¡£ä¸­ç›´æ¥æ¼”ç¤ºå¦‚ä½•æ“ä½œï¼Œå› ä¸ºç¤ºä¾‹åªèƒ½åœ¨ç¬”è®°æœ¬ä¸­è¿è¡Œã€‚ç›¸åï¼Œè¯·æŸ¥çœ‹å¦‚ä½•åœ¨è¿™ä¸ª[notebook](https://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/sql_with_huggingface_datasets.ipynb#scrollTo=d83yGQMPHGFi)ä¸­å®‰è£…å’Œè®¾ç½®PostgreSQLæœåŠ¡å™¨ï¼
- en: After youâ€™ve setup your PostgreSQL database, you can use the [from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)
    method to load a dataset from a table or query.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¾ç½®å¥½PostgreSQLæ•°æ®åº“ä¹‹åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[from_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.from_sql)æ–¹æ³•ä»è¡¨æ ¼æˆ–æŸ¥è¯¢ä¸­åŠ è½½æ•°æ®é›†ã€‚
