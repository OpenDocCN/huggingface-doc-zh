# 使用LCM-LoRA进行推理

> 原始文本：[https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora](https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora)

潜在一致性模型（LCM）使得在通常2-4步内生成高质量图像成为可能，从而可以在几乎实时的设置中使用扩散模型。

从[官方网站](https://latent-consistency-models.github.io/)：

> LCMs可以从任何预训练的稳定扩散（SD）中提炼出来，仅需4,000个训练步骤（~32 A100 GPU小时）即可生成高质量的768 x 768分辨率图像，仅需2~4步甚至一步，显著加速文本到图像的生成。我们使用LCM来提炼Dreamshaper-V7版本的SD，仅需4,000个训练迭代。

有关LCMs的更多技术概述，请参阅[论文](https://huggingface.co/papers/2310.04378)。

然而，每个模型需要单独进行潜在一致性提炼。LCM-LoRA的核心思想是仅训练几个适配器层，适配器在这种情况下是LoRA。这样，我们就不必训练完整的模型，并保持可管理的可训练参数数量。然后，生成的LoRAs可以应用于任何经过微调的模型版本，而无需单独提炼它们。此外，LoRAs可以应用于图像到图像、ControlNet/T2I-Adapter、修补、AnimateDiff等。LCM-LoRA还可以与其他LoRAs结合，以在很少的步骤（4-8）中生成样式化图像。

LCM-LoRAs可用于[stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)、[stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)和[SSD-1B](https://huggingface.co/segmind/SSD-1B)模型。所有检查点都可以在此[集合](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6)中找到。

有关LCM-LoRA的更多详细信息，请参阅[技术报告](https://huggingface.co/papers/2311.05556)。

本指南展示了如何使用LCM-LoRAs进行推理

+   文本到图像

+   图像到图像

+   与样式LoRAs结合

+   ControlNet/T2I-Adapter

+   修补

+   AnimateDiff

在阅读本指南之前，我们将看一下使用LCM-LoRAs进行推理的一般工作流程。LCM-LoRAs类似于其他稳定扩散LoRAs，因此它们可以与支持LoRAs的任何[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)一起使用。

+   加载特定任务的流水线和模型。

+   将调度程序设置为[LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler)。

+   加载模型的LCM-LoRA权重。

+   将`guidance_scale`减小到`[1.0, 2.0]`之间，并将`num_inference_steps`设置在[4, 8]之间。

+   使用常规参数对流水线进行推理。

让我们看看如何在不同任务中使用LCM-LoRAs进行推理。

首先，请确保您已安装[peft](https://github.com/huggingface/peft)，以获得更好的LoRA支持。

```py
pip install -U peft
```

## 文本到图像

您将使用[StableDiffusionXLPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline)与调度程序：[LCMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lcm#diffusers.LCMScheduler)，然后加载LCM-LoRA。与LCM-LoRA和调度程序一起，该流水线实现了快速推理工作流，克服了扩散模型缓慢的迭代性质。

```py
import torch
from diffusers import DiffusionPipeline, LCMScheduler

pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    variant="fp16",
    torch_dtype=torch.float16
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")

prompt = "Self-portrait oil painting, a beautiful cyborg with golden hair, 8k"

generator = torch.manual_seed(42)
image = pipe(
    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0
).images[0]
```

![](../Images/2ca06b97873717e791e8cb391d65c97e.png)

请注意，我们只使用4个生成步骤，这比标准SDXL通常使用的步骤要少得多。

您可能已经注意到我们设置了`guidance_scale=1.0`，这会禁用无分类器引导。这是因为LCM-LoRA是在有引导的情况下训练的，所以在这种情况下批量大小不必加倍。这会导致更快的推理时间，但负面提示对去噪过程没有任何影响。

您也可以在LCM-LoRA中使用引导，但由于训练模型的性质对`guidance_scale`值非常敏感，高值可能会导致生成图像中的伪影。在我们的实验中，我们发现最佳值在[1.0, 2.0]范围内。

### 使用经过微调的模型进行推理

如上所述，LCM-LoRA可以应用于任何经过微调的模型版本，而无需单独提取它们。让我们看看如何使用经过微调的模型进行推理。在这个例子中，我们将使用[animagine-xl](https://huggingface.co/Linaqruf/animagine-xl)模型，这是用于生成动漫的SDXL模型的经过微调版本。

```py
from diffusers import DiffusionPipeline, LCMScheduler

pipe = DiffusionPipeline.from_pretrained(
    "Linaqruf/animagine-xl",
    variant="fp16",
    torch_dtype=torch.float16
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")

prompt = "face focus, cute, masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, night, turtleneck"

generator = torch.manual_seed(0)
image = pipe(
    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0
).images[0]
```

![](../Images/7ca0f83858b96602e6d6532ddecb9a22.png)

## 图像到图像

LCM-LoRA也可以应用于图像到图像的任务。让我们看看如何使用LCM进行图像到图像的生成。在这个例子中，我们将使用[dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7)模型和LCM-LoRA进行`stable-diffusion-v1-5`。

```py
import torch
from diffusers import AutoPipelineForImage2Image, LCMScheduler
from diffusers.utils import make_image_grid, load_image

pipe = AutoPipelineForImage2Image.from_pretrained(
    "Lykon/dreamshaper-7",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)
prompt = "Astronauts in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
generator = torch.manual_seed(0)
image = pipe(
    prompt,
    image=init_image,
    num_inference_steps=4,
    guidance_scale=1,
    strength=0.6,
    generator=generator
).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

![](../Images/57491291a2825c214ac6c8fda2b34cfe.png)

根据您的提示和提供的图像，您可以获得不同的结果。为了获得最佳结果，我们建议尝试不同的值来选择`num_inference_steps`、`strength`和`guidance_scale`参数中的最佳值。

## 与风格化LoRA结合

LCM-LoRA可以与其他LoRA结合，以在很少的步骤（4-8）中生成风格化图像。在下面的例子中，我们将使用LCM-LoRA与[papercut LoRA](TheLastBen/Papercut_SDXL)。要了解如何组合LoRA，请参考[此指南](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters)。

```py
import torch
from diffusers import DiffusionPipeline, LCMScheduler

pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    variant="fp16",
    torch_dtype=torch.float16
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LoRAs
pipe.load_lora_weights("latent-consistency/lcm-lora-sdxl", adapter_name="lcm")
pipe.load_lora_weights("TheLastBen/Papercut_SDXL", weight_name="papercut.safetensors", adapter_name="papercut")

# Combine LoRAs
pipe.set_adapters(["lcm", "papercut"], adapter_weights=[1.0, 0.8])

prompt = "papercut, a cute fox"
generator = torch.manual_seed(0)
image = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]
image
```

![](../Images/13ecbe2ca5401bbe4f7c504d04cc6b95.png)

## ControlNet/T2I-Adapter

让我们看看如何使用ControlNet/T2I-Adapter和LCM-LoRA进行推理。

### ControlNet

对于这个例子，我们将使用SD-v1-5模型和带有canny ControlNet的SD-v1-5的LCM-LoRA。

```py
import torch
import cv2
import numpy as np
from PIL import Image

from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler
from diffusers.utils import load_image

image = load_image(
    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
).resize((512, 512))

image = np.array(image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    controlnet=controlnet,
    torch_dtype=torch.float16,
    safety_checker=None,
    variant="fp16"
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")

generator = torch.manual_seed(0)
image = pipe(
    "the mona lisa",
    image=canny_image,
    num_inference_steps=4,
    guidance_scale=1.5,
    controlnet_conditioning_scale=0.8,
    cross_attention_kwargs={"scale": 1},
    generator=generator,
).images[0]
make_image_grid([canny_image, image], rows=1, cols=2)
```

![](../Images/e92fb093ce9ab261a6081a45cf6ba97c.png)

在这个例子中的推理参数可能不适用于所有例子，所以我们建议您尝试不同的值来选择`num_inference_steps`、`guidance_scale`、`controlnet_conditioning_scale`和`cross_attention_kwargs`参数中的最佳值。

### T2I-Adapter

这个例子展示了如何使用LCM-LoRA与[Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0)和SDXL。

```py
import torch
import cv2
import numpy as np
from PIL import Image

from diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler
from diffusers.utils import load_image, make_image_grid

# Prepare image
# Detect the canny map in low resolution to avoid high-frequency details
image = load_image(
    "https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg"
).resize((384, 384))

image = np.array(image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image).resize((1024, 1024))

# load adapter
adapter = T2IAdapter.from_pretrained("TencentARC/t2i-adapter-canny-sdxl-1.0", torch_dtype=torch.float16, varient="fp16").to("cuda")

pipe = StableDiffusionXLAdapterPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0", 
    adapter=adapter,
    torch_dtype=torch.float16,
    variant="fp16", 
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")

prompt = "Mystical fairy in real, magic, 4k picture, high quality"
negative_prompt = "extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured"

generator = torch.manual_seed(0)
image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=canny_image,
    num_inference_steps=4,
    guidance_scale=1.5, 
    adapter_conditioning_scale=0.8, 
    adapter_conditioning_factor=1,
    generator=generator,
).images[0]
make_image_grid([canny_image, image], rows=1, cols=2)
```

![](../Images/2ee6c4de3add72aae6b96e5673406b21.png)

## 修补

LCM-LoRA也可以用于修补。

```py
import torch
from diffusers import AutoPipelineForInpainting, LCMScheduler
from diffusers.utils import load_image, make_image_grid

pipe = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

# generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
generator = torch.manual_seed(0)
image = pipe(
    prompt=prompt,
    image=init_image,
    mask_image=mask_image,
    generator=generator,
    num_inference_steps=4,
    guidance_scale=4, 
).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```

![](../Images/b5493832202c5c129c84462826847ba1.png)

## AnimateDiff

`AnimateDiff`允许您使用稳定扩散模型对图像进行动画处理。为了获得良好的结果，我们需要生成多个帧（16-24），使用标准SD模型可能会非常慢。LCM-LoRA可以显著加快这个过程，因为每个帧只需要进行4-8步。让我们看看如何使用LCM-LoRA和AnimateDiff进行动画处理。

```py
import torch
from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler
from diffusers.utils import export_to_gif

adapter = MotionAdapter.from_pretrained("diffusers/animatediff-motion-adapter-v1-5")
pipe = AnimateDiffPipeline.from_pretrained(
    "frankjoshua/toonyou_beta6",
    motion_adapter=adapter,
).to("cuda")

# set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5", adapter_name="lcm")
pipe.load_lora_weights("guoyww/animatediff-motion-lora-zoom-in", weight_name="diffusion_pytorch_model.safetensors", adapter_name="motion-lora")

pipe.set_adapters(["lcm", "motion-lora"], adapter_weights=[0.55, 1.2])

prompt = "best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress"
generator = torch.manual_seed(0)
frames = pipe(
    prompt=prompt,
    num_inference_steps=5,
    guidance_scale=1.25,
    cross_attention_kwargs={"scale": 1},
    num_frames=24,
    generator=generator
).frames[0]
export_to_gif(frames, "animation.gif")
```

![](../Images/43e783334497532daab457d32f83c51a.png)
