- en: Comparing performance between different device setups
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较不同设备设置之间的性能
- en: 'Original text: [https://huggingface.co/docs/accelerate/concept_guides/performance](https://huggingface.co/docs/accelerate/concept_guides/performance)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/accelerate/concept_guides/performance](https://huggingface.co/docs/accelerate/concept_guides/performance)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and comparing the performance from different setups can be quite
    tricky if you don’t know what to look for. For example, you cannot run the same
    script with the same batch size across TPU, multi-GPU, and single-GPU with Accelerate
    and expect your results to line up.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 评估和比较来自不同设置的性能可能会非常棘手，如果您不知道要查找什么。例如，您不能在TPU、多GPU和单GPU上使用相同的脚本和相同的批量大小运行Accelerate，并期望您的结果能够对齐。
- en: But why?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么？
- en: 'There are three reasons for this that this tutorial will cover:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程将涵盖三个原因：
- en: '**Setting the right seeds**'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置正确的种子**'
- en: '**Observed Batch Sizes**'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**观察到的批量大小**'
- en: '**Learning Rates**'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学习率**'
- en: Setting the Seed
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置种子
- en: 'While this issue has not come up as much, make sure to use [utils.set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)
    to fully set the seed in all distributed cases so training will be reproducible:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个问题并没有出现得很多，但请确保在所有分布式情况下使用[utils.set_seed()](/docs/accelerate/v0.27.2/en/package_reference/utilities#accelerate.utils.set_seed)来完全设置种子，以便训练是可重现的：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Why is this important? Under the hood this will set **5** different seed settings:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？在幕后，这将设置**5**个不同的种子设置：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The random state, numpy’s state, torch, torch’s cuda state, and if TPUs are
    available torch_xla’s cuda state.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随机状态、numpy的状态、torch、torch的cuda状态，以及如果TPUs可用，torch_xla的cuda状态。
- en: Observed Batch Sizes
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察到的批量大小
- en: When training with Accelerate, the batch size passed to the dataloader is the
    **batch size per GPU**. What this entails is a batch size of 64 on two GPUs is
    truly a batch size of 128\. As a result, when testing on a single GPU this needs
    to be accounted for, as well as similarly for TPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Accelerate进行训练时，传递给数据加载器的批量大小是**每个GPU的批量大小**。这意味着在两个GPU上的批量大小为64实际上是128。因此，在单个GPU上进行测试时，需要考虑到这一点，以及类似地对于TPUs。
- en: 'The below table can be used as a quick reference to try out different batch
    sizes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下表可用作快速参考，尝试不同的批量大小：
- en: In this example, there are two GPUs for “Multi-GPU” and a TPU pod with 8 workers
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，有两个GPU用于“多GPU”，以及一个具有8个工作节点的TPU pod
- en: '| Single GPU Batch Size | Multi-GPU Equivalent Batch Size | TPU Equivalent
    Batch Size |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 单个GPU批量大小 | 多GPU等效批量大小 | TPU等效批量大小 |'
- en: '| --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 256 | 128 | 32 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 128 | 32 |'
- en: '| 128 | 64 | 16 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 64 | 16 |'
- en: '| 64 | 32 | 8 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 32 | 8 |'
- en: '| 32 | 16 | 4 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 16 | 4 |'
- en: Learning Rates
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: 'As noted in multiple sources[[1](https://aws.amazon.com/blogs/machine-learning/scalable-multi-node-deep-learning-training-using-gpus-in-the-aws-cloud/)][[2](https://docs.nvidia.com/clara/clara-train-sdk/pt/model.html#classification-models-multi-gpu-training)],
    the learning rate should be scaled *linearly* based on the number of devices present.
    The below snippet shows doing so with Accelerate:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如多个来源所指出[[1](https://aws.amazon.com/blogs/machine-learning/scalable-multi-node-deep-learning-training-using-gpus-in-the-aws-cloud/)][[2](https://docs.nvidia.com/clara/clara-train-sdk/pt/model.html#classification-models-multi-gpu-training)]，学习率应该根据设备数量*线性*缩放。下面的代码片段展示了如何在Accelerate中实现这一点：
- en: Since users can have their own learning rate schedulers defined, we leave this
    up to the user to decide if they wish to scale their learning rate or not.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于用户可以定义自己的学习率调度程序，我们将此留给用户决定是否希望调整他们的学习率。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You will also find that `accelerate` will step the learning rate based on the
    number of processes being trained on. This is because of the observed batch size
    noted earlier. So in the case of 2 GPUs, the learning rate will be stepped twice
    as often as a single GPU to account for the batch size being twice as large (if
    no changes to the batch size on the single GPU instance are made).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会发现`accelerate`将根据正在训练的进程数量调整学习率。这是因为早期观察到的批量大小。因此，在使用2个GPU的情况下，学习率将比单个GPU频繁调整两倍，以考虑批量大小增加两倍（如果不对单个GPU实例的批量大小进行更改）。
- en: Gradient Accumulation and Mixed Precision
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度累积和混合精度
- en: When using gradient accumulation and mixed precision, due to how gradient averaging
    works (accumulation) and the precision loss (mixed precision), some degradation
    in performance is expected. This will be explicitly seen when comparing the batch-wise
    loss between different compute setups. However, the overall loss, metric, and
    general performance at the end of training should be *roughly* the same.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用梯度累积和混合精度时，由于梯度平均（累积）和精度损失（混合精度）的工作方式，预计会出现一定程度的性能下降。这将在比较不同计算设置之间的逐批损失时明显看到。然而，在训练结束时，总体损失、指标和性能应该是*大致*相同的。
