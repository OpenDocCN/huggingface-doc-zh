# PixArt-α

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/pixart](https://huggingface.co/docs/diffusers/api/pipelines/pixart)

![](../Images/5a666bb8097e14b9e3eb092b69530169.png)

[PixArt-α：用于逼真文本到图像合成的扩散Transformer快速训练](https://huggingface.co/papers/2310.00426) 作者是Junsong Chen、Jincheng Yu、Chongjian Ge、Lewei Yao、Enze Xie、Yue Wu、Zhongdao Wang、James Kwok、Ping Luo、Huchuan Lu和Zhenguo Li。

论文摘要如下：

*最先进的文本到图像（T2I）模型需要巨大的训练成本（例如，数百万GPU小时），严重阻碍了AIGC社区的基本创新，同时增加了CO2排放。本文介绍了PIXART-α，一种基于Transformer的T2I扩散模型，其图像生成质量与最先进的图像生成器（例如Imagen、SDXL，甚至Midjourney）竞争力相当，达到接近商业应用标准。此外，它支持高分辨率图像合成，最高可达1024像素分辨率，训练成本低，如图1和2所示。为了实现这一目标，提出了三个核心设计：（1）训练策略分解：我们设计了三个不同的训练步骤，分别优化像素依赖性、文本-图像对齐和图像美学质量；（2）高效的T2I Transformer：我们将交叉注意力模块整合到扩散Transformer（DiT）中，注入文本条件并简化计算密集型的类条件分支；（3）高信息量数据：我们强调文本-图像对中概念密度的重要性，并利用大型视觉语言模型自动标记密集的伪标题，以帮助文本-图像对齐学习。因此，PIXART-α的训练速度明显超过现有的大规模T2I模型，例如，PIXART-α仅占稳定扩散v1.5的训练时间的10.8%（675 vs. 6,250 A100 GPU天），节省近30万美元（26,000 vs. 320,000美元），减少90%的CO2排放。此外，与更大的SOTA模型RAPHAEL相比，我们的训练成本仅为1%。大量实验证明PIXART-α在图像质量、艺术性和语义控制方面表现出色。我们希望PIXART-α能为AIGC社区和初创公司提供新的见解，加速他们从零开始构建自己的高质量低成本生成模型。*

您可以在[PixArt-alpha/PixArt-alpha](https://github.com/PixArt-alpha/PixArt-alpha)找到原始代码库，以及在[PixArt-alpha](https://huggingface.co/PixArt-alpha)找到所有可用的检查点。

关于这个管道的一些注意事项：

+   它使用Transformer骨干（而不是UNet）进行去噪。因此，它与[DiT](./dit)具有相似的架构。

+   它是使用从T5计算得出的文本条件进行训练的。这一方面使得该管道更擅长遵循具有复杂细节的文本提示。

+   它擅长以不同的宽高比生成高分辨率图像。为了获得最佳结果，作者建议一些大小范围，可以在[这里](https://github.com/PixArt-alpha/PixArt-alpha/blob/08fbbd281ec96866109bdd2cdb75f2f58fb17610/diffusion/data/datasets/utils.py)找到。

+   它与最先进的文本到图像生成系统（截至目前）如Stable Diffusion XL、Imagen和DALL-E 2的质量相媲美，同时比它们更高效。

确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。

## 在8GB GPU VRAM以下进行推理

通过在8GB GPU VRAM下加载8位精度的文本编码器来运行[PixArtAlphaPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pixart#diffusers.PixArtAlphaPipeline)。让我们通过一个完整的示例来了解一下。

首先，安装[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)库：

```py
pip install -U bitsandbytes
```

然后以8位加载文本编码器：

```py
from transformers import T5EncoderModel
from diffusers import PixArtAlphaPipeline
import torch

text_encoder = T5EncoderModel.from_pretrained(
    "PixArt-alpha/PixArt-XL-2-1024-MS",
    subfolder="text_encoder",
    load_in_8bit=True,
    device_map="auto",

)
pipe = PixArtAlphaPipeline.from_pretrained(
    "PixArt-alpha/PixArt-XL-2-1024-MS",
    text_encoder=text_encoder,
    transformer=None,
    device_map="auto"
)
```

现在，使用`pipe`来编码一个提示：

```py
with torch.no_grad():
    prompt = "cute cat"
    prompt_embeds, prompt_attention_mask, negative_embeds, negative_prompt_attention_mask = pipe.encode_prompt(prompt)
```

自从文本嵌入已经计算出来后，从内存中删除`text_encoder`和`pipe`，并释放一些GPU VRAM：

```py
import gc 

def flush():
    gc.collect()
    torch.cuda.empty_cache()

del text_encoder
del pipe
flush()
```

然后使用提示嵌入计算潜变量作为输入：

```py
pipe = PixArtAlphaPipeline.from_pretrained(
    "PixArt-alpha/PixArt-XL-2-1024-MS",
    text_encoder=None,
    torch_dtype=torch.float16,
).to("cuda")

latents = pipe(
    negative_prompt=None, 
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    prompt_attention_mask=prompt_attention_mask,
    negative_prompt_attention_mask=negative_prompt_attention_mask,
    num_images_per_prompt=1,
    output_type="latent",
).images

del pipe.transformer
flush()
```

请注意，在初始化`pipe`时，您将`text_encoder`设置为`None`，以便不加载它。

一旦计算出潜变量，将其传递给VAE以解码为真实图像：

```py
with torch.no_grad():
    image = pipe.vae.decode(latents / pipe.vae.config.scaling_factor, return_dict=False)[0]
image = pipe.image_processor.postprocess(image, output_type="pil")[0]
image.save("cat.png")
```

通过删除未使用的组件并刷新GPU VRAM，您应该能够在不到8GB的GPU VRAM下运行[PixArtAlphaPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/pixart#diffusers.PixArtAlphaPipeline)。

![](../Images/f9bdd252908e7f92b9ab48ae4b8febe6.png)

如果您想要报告内存使用情况，请运行此[脚本](https://gist.github.com/sayakpaul/3ae0f847001d342af27018a96f467e4e)。

在8位计算的文本嵌入可能会影响生成图像的质量，因为由于减少的精度导致的表示空间中的信息丢失。建议比较有和没有8位的输出。

在加载`text_encoder`时，将`load_in_8bit`设置为`True`。您还可以指定`load_in_4bit`，以进一步减少内存需求至7GB以下。

## PixArtAlphaPipeline

### `class diffusers.PixArtAlphaPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L182)

```py
( tokenizer: T5Tokenizer text_encoder: T5EncoderModel vae: AutoencoderKL transformer: Transformer2DModel scheduler: DPMSolverMultistepScheduler )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）— 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `text_encoder`（`T5EncoderModel`）— 冻结的文本编码器。PixArt-Alpha使用[T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel)，具体来说是[t5-v1_1-xxl](https://huggingface.co/PixArt-alpha/PixArt-alpha/tree/main/t5-v1_1-xxl)变体。

+   `tokenizer`（`T5Tokenizer`）— 类[T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer)的分词器。

+   `transformer`（[Transformer2DModel](/docs/diffusers/v0.26.3/en/api/models/transformer2d#diffusers.Transformer2DModel））— 一个文本条件的`Transformer2DModel`，用于去噪编码图像潜变量。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)）— 一个调度器，用于与`transformer`结合使用以去噪编码的图像潜变量。

用于使用PixArt-Alpha进行文本到图像生成的管道。

这个模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有管道实现的通用方法（例如下载或保存，运行在特定设备上等）。

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L666)

```py
( prompt: Union = None negative_prompt: str = '' num_inference_steps: int = 20 timesteps: List = None guidance_scale: float = 4.5 num_images_per_prompt: Optional = 1 height: Optional = None width: Optional = None eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None prompt_attention_mask: Optional = None negative_prompt_embeds: Optional = None negative_prompt_attention_mask: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 clean_caption: bool = True use_resolution_binning: bool = True **kwargs ) → export const metadata = 'undefined';ImagePipelineOutput or tuple
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 用于指导图像生成的提示。如果未定义，则必须传递`prompt_embeds`。

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用于指导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。当不使用指导时（即，如果`guidance_scale`小于`1`，则忽略）。

+   `num_inference_steps`（`int`，*可选*，默认为100）— 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `timesteps`（`List[int]`，*可选*）— 用于去噪过程的自定义时间步。如果未定义，则使用等间距的`num_inference_steps`时间步。必须按降序排列。

+   `guidance_scale` (`float`, *可选*, 默认为4.5) — 在[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导尺度。`guidance_scale`定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程2的`w`。通过设置`guidance_scale > 1`启用指导尺度。更高的指导尺度鼓励生成与文本`prompt`密切相关的图像，通常以降低图像质量为代价。

+   `num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。

+   `height` (`int`, *可选*, 默认为self.unet.config.sample_size) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为self.unet.config.sample_size) — 生成图像的像素宽度。

+   `eta` (`float`, *可选*, 默认为0.0) — 对应于DDIM论文中的参数eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于[schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，对其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)列表，使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机`generator`进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `prompt_attention_mask` (`torch.FloatTensor`, *可选*) — 针对文本嵌入的预生成注意力掩码。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。对于PixArt-Alpha，此负面提示应为""。如果未提供，将从`negative_prompt`输入参数生成负面提示嵌入。

+   `negative_prompt_attention_mask` (`torch.FloatTensor`, *可选*) — 针对负面文本嵌入的预生成注意力掩码。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。可选择[PIL](https://pillow.readthedocs.io/en/stable/)：`PIL.Image.Image` 或 `np.array`。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回`~pipelines.stable_diffusion.IFPipelineOutput`而不是普通元组。

+   `callback` (`Callable`, *可选*) — 在推断过程中每`callback_steps`步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为1) — `callback`函数将被调用的频率。如果未指定，将在每一步调用回调。

+   `clean_caption` (`bool`, *可选*, 默认为`True`) — 在创建嵌入之前是否清理标题。需要安装`beautifulsoup4`和`ftfy`。如果未安装依赖项，将从原始提示创建嵌入。

+   `use_resolution_binning` (`bool` 默认为`True`) — 如果设置为`True`，则首先将请求的高度和宽度映射到最接近的分辨率，使用`ASPECT_RATIO_1024_BIN`。在生成的潜变量解码为图像后，它们将被调整回请求的分辨率。适用于生成非正方形图像。

返回

[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput) 或 `tuple`

如果`return_dict`为`True`，则返回[ImagePipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_unclip#diffusers.ImagePipelineOutput)，否则返回一个`tuple`，其中第一个元素是包含生成图像的列表

调用管道生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import PixArtAlphaPipeline

>>> # You can replace the checkpoint id with "PixArt-alpha/PixArt-XL-2-512x512" too.
>>> pipe = PixArtAlphaPipeline.from_pretrained("PixArt-alpha/PixArt-XL-2-1024-MS", torch_dtype=torch.float16)
>>> # Enable memory optimizations.
>>> pipe.enable_model_cpu_offload()

>>> prompt = "A small cactus with a happy face in the Sahara desert."
>>> image = pipe(prompt).images[0]
```

#### `classify_height_width_bin`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L634)

```py
( height: int width: int ratios: dict )
```

返回分箱的高度和宽度。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py#L251)

```py
( prompt: Union do_classifier_free_guidance: bool = True negative_prompt: str = '' num_images_per_prompt: int = 1 device: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None prompt_attention_mask: Optional = None negative_prompt_attention_mask: Optional = None clean_caption: bool = False **kwargs )
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 要编码的提示

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 不用来指导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。在不使用指导时被忽略（即如果`guidance_scale`小于`1`，则被忽略）。对于PixArt-Alpha，应该是""。

+   `do_classifier_free_guidance` (`bool`, *optional*, 默认为`True`) — 是否使用分类器自由指导。

+   `num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量 device — (`torch.device`, *optional*): 放置生成的嵌入的torch设备

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，则将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负面文本嵌入。对于PixArt-Alpha，应该是""字符串的嵌入。

+   `clean_caption`（bool，默认为`False`）— 如果为`True`，函数将在编码之前对提供的标题进行预处理和清理。

将提示编码为文本编码器隐藏状态。
