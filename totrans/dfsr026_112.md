# UNet2DModel

> 原文链接: [https://huggingface.co/docs/diffusers/api/models/unet2d](https://huggingface.co/docs/diffusers/api/models/unet2d)

[UNet](https://huggingface.co/papers/1505.04597) 模型最初由Ronneberger等人提出，用于生物医学图像分割，但它也常用于🤗 Diffusers，因为它输出与输入相同大小的图像。它是扩散系统中最重要的组件之一，因为它促进了实际的扩散过程。在🤗 Diffusers中有几个UNet模型的变体，取决于其维度数量以及是否是条件模型。这是一个2D UNet模型。

论文摘要如下:

*有很大的共识，成功训练深度网络需要成千上万个注释的训练样本。在本文中，我们提出了一种依赖于强大数据增强的网络和训练策略，以更有效地利用可用的注释样本。该架构由一个收缩路径和一个对称扩展路径组成，用于捕获上下文和实现精确的定位。我们展示了这样一个网络可以从很少的图像中进行端到端的训练，并且在ISBI挑战中（用于电子显微镜堆中神经元结构的分割）上胜过先前最佳方法（滑动窗口卷积网络）。使用相同的网络在透射光学显微镜图像（相差和DIC）上训练，我们在这些类别上以很大的优势赢得了2015年的ISBI细胞跟踪挑战。此外，该网络速度很快。对512x512图像的分割在最新的GPU上不到一秒。完整的实现（基于Caffe）和训练的网络可在[http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)上获得。*

## UNet2DModel

### `class diffusers.UNet2DModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d.py#L40)

```py
( sample_size: Union = None in_channels: int = 3 out_channels: int = 3 center_input_sample: bool = False time_embedding_type: str = 'positional' freq_shift: int = 0 flip_sin_to_cos: bool = True down_block_types: Tuple = ('DownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D') up_block_types: Tuple = ('AttnUpBlock2D', 'AttnUpBlock2D', 'AttnUpBlock2D', 'UpBlock2D') block_out_channels: Tuple = (224, 448, 672, 896) layers_per_block: int = 2 mid_block_scale_factor: float = 1 downsample_padding: int = 1 downsample_type: str = 'conv' upsample_type: str = 'conv' dropout: float = 0.0 act_fn: str = 'silu' attention_head_dim: Optional = 8 norm_num_groups: int = 32 attn_norm_num_groups: Optional = None norm_eps: float = 1e-05 resnet_time_scale_shift: str = 'default' add_attention: bool = True class_embed_type: Optional = None num_class_embeds: Optional = None num_train_timesteps: Optional = None )
```

参数

+   `sample_size` (`int` 或 `Tuple[int, int]`, *可选*, 默认为`None`) — 输入/输出样本的高度和宽度。维度必须是`2 ** (len(block_out_channels) - 1)`的倍数。

+   `in_channels` (`int`, *可选*, 默认为3) — 输入样本中的通道数。

+   `out_channels` (`int`, *可选*, 默认为3) — 输出中的通道数。

+   `center_input_sample` (`bool`, *可选*, 默认为`False`) — 是否将输入样本居中。

+   `time_embedding_type` (`str`, *可选*, 默认为`"positional"`) — 要使用的时间嵌入类型。

+   `freq_shift` (`int`, *可选*, 默认为0) — 傅立叶时间嵌入的频移。

+   `flip_sin_to_cos` (`bool`, *可选*, 默认为`True`) — 是否将正弦翻转为余弦以用于傅立叶时间嵌入。

+   `down_block_types` (`Tuple[str]`, *可选*, 默认为`("DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D")`) — 下采样块类型的元组。

+   `mid_block_type` (`str`, *可选*, 默认为`"UNetMidBlock2D"`) — UNet中间块的类型，可以是`UNetMidBlock2D`或`UnCLIPUNetMidBlock2D`。

+   `up_block_types` (`Tuple[str]`, *可选*, 默认为`("AttnUpBlock2D", "AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D")`) — 上采样块类型的元组。

+   `block_out_channels` (`Tuple[int]`, *可选*, 默认为`(224, 448, 672, 896)`) — 块输出通道的元组。

+   `layers_per_block` (`int`, *可选*, 默认为`2`) — 每个块的层数。

+   `mid_block_scale_factor` (`float`, *可选*, 默认为`1`) — 中间块的比例因子。

+   `downsample_padding` (`int`, *可选*, 默认为`1`) — 下采样卷积的填充。

+   `downsample_type` (`str`, *可选*, 默认为`conv`) — 下采样层的下采样类型。选择`conv`或“resnet”之间。

+   `upsample_type` (`str`, *可选*, 默认为`conv`) — 上采样层的上采样类型。选择`conv`或“resnet”之间。

+   `dropout` (`float`, *optional*, defaults to 0.0) — 要使用的 dropout 概率。

+   `act_fn` (`str`, *optional*, defaults to `"silu"`) — 要使用的激活函数。

+   `attention_head_dim` (`int`, *optional*, defaults to `8`) — 注意力头维度。

+   `norm_num_groups` (`int`, *optional*, defaults to `32`) — 规范化的组数。

+   `attn_norm_num_groups` (`int`, *optional*, defaults to `None`) — 如果设置为整数，则在中间块的 `Attention` 层中创建一个组规范化层，具有给定的组数。如果保持为 `None`，则仅当 `resnet_time_scale_shift` 设置为 `default` 时才会创建组规范化层，如果创建，将具有 `norm_num_groups` 组。

+   `norm_eps` (`float`, *optional*, defaults to `1e-5`) — 规范化的 epsilon。

+   `resnet_time_scale_shift` (`str`, *optional*, defaults to `"default"`) — ResNet 块的时间尺度偏移配置（参见 `ResnetBlock2D`）。可选择 `default` 或 `scale_shift`。

+   `class_embed_type` (`str`, *optional*, defaults to `None`) — 要使用的类别嵌入类型，最终将与时间嵌入相加。可选择 `None`、`"timestep"` 或 `"identity"`。

+   `num_class_embeds` (`int`, *optional*, defaults to `None`) — 当执行类别条件化时，可学习嵌入矩阵的输入维度，将被投影到 `time_embed_dim`，并且 `class_embed_type` 等于 `None`。

一个接受嘈杂样本和时间步长并返回一个形状为样本的 UNet 模型。

此模型继承自 [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。查看超类文档以了解为所有模型实现的通用方法（如下载或保存）。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d.py#L243)

```py
( sample: FloatTensor timestep: Union class_labels: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';~models.unet_2d.UNet2DOutput or tuple
```

参数

+   `sample` (`torch.FloatTensor`） — 具有以下形状的嘈杂输入张量 `(batch, channel, height, width)`。

+   `timestep` (`torch.FloatTensor` 或 `float` 或 `int`) — 用于去噪输入的时间步数。

+   `class_labels` (`torch.FloatTensor`, *optional*, defaults to `None`) — 用于条件化的可选类别标签。它们的嵌入将与时间步嵌入相加。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回一个 `~models.unet_2d.UNet2DOutput` 而不是一个普通的 tuple。

返回

`~models.unet_2d.UNet2DOutput` 或 `tuple`

如果 `return_dict` 为 True，则返回一个 `~models.unet_2d.UNet2DOutput`，否则返回一个 `tuple`，其中第一个元素是样本张量。

[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel) 的前向方法。

## UNet2DOutput

### `class diffusers.models.unets.unet_2d.UNet2DOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_2d.py#L27)

```py
( sample: FloatTensor )
```

参数

+   `sample` (`torch.FloatTensor`，形状为 `(batch_size, num_channels, height, width)`） — 模型最后一层的隐藏状态输出。

[UNet2DModel](/docs/diffusers/v0.26.3/en/api/models/unet2d#diffusers.UNet2DModel) 的输出。
