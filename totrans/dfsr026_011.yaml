- en: Load LoRAs for inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/156.decf7e89.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Tip.230e2334.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/DocNotebookDropdown.5fa27ace.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: There are many adapters (with LoRAs being the most common type) trained in different
    styles to achieve different effects. You can even combine multiple adapters to
    create new and unique images. With the ü§ó [PEFT](https://huggingface.co/docs/peft/index)
    integration in ü§ó Diffusers, it is really easy to load and manage adapters for
    inference. In this guide, you‚Äôll learn how to use different adapters with [Stable
    Diffusion XL (SDXL)](../api/pipelines/stable_diffusion/stable_diffusion_xl) for
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this guide, you‚Äôll use LoRA as the main adapter technique, so we‚Äôll
    use the terms LoRA and adapter interchangeably. You should have some familiarity
    with LoRA, and if you don‚Äôt, we welcome you to check out the [LoRA guide](https://huggingface.co/docs/peft/conceptual_guides/lora).
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs first install all the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let‚Äôs load a pipeline with a SDXL checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, load a LoRA checkpoint with the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: With the ü§ó PEFT integration, you can assign a specific `adapter_name` to the
    checkpoint, which let‚Äôs you easily switch between different LoRA checkpoints.
    Let‚Äôs call this adapter `"toy"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And then perform inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![toy-face](../Images/5113690135aa0bd78ddd1d3e7c53479d.png)'
  prefs: []
  type: TYPE_IMG
- en: With the `adapter_name` parameter, it is really easy to use another adapter
    for inference! Load the [nerijs/pixel-art-xl](https://huggingface.co/nerijs/pixel-art-xl)
    adapter that has been fine-tuned to generate pixel art images, and let‚Äôs call
    it `"pixel"`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline automatically sets the first loaded adapter (`"toy"`) as the active
    adapter. But you can activate the `"pixel"` adapter with the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs now generate an image with the second adapter and check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![pixel-art](../Images/d01ebc0347358d4583b5ade26b841a6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Combine multiple adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also perform multi-adapter inference where you combine different adapter
    checkpoints for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate two LoRA checkpoints and specify the weight for how the checkpoints
    should be combined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have set these two adapters, let‚Äôs generate an image from the combined
    adapters!
  prefs: []
  type: TYPE_NORMAL
- en: LoRA checkpoints in the diffusion community are almost always obtained with
    [DreamBooth](https://huggingface.co/docs/diffusers/main/en/training/dreambooth).
    DreamBooth training often relies on ‚Äútrigger‚Äù words in the input text prompts
    in order for the generation results to look as expected. When you combine multiple
    LoRA checkpoints, it‚Äôs important to ensure the trigger words for the corresponding
    LoRA checkpoints are present in the input text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: The trigger words for [CiroN2022/toy-face](https://hf.co/CiroN2022/toy-face)
    and [nerijs/pixel-art-xl](https://hf.co/nerijs/pixel-art-xl) are found in their
    repositories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![toy-face-pixel-art](../Images/99de7c335e766f24d86d686716cfbe1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Impressive! As you can see, the model was able to generate an image that mixes
    the characteristics of both adapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go back to using only one adapter, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate the `"toy"` adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![toy-face-again](../Images/c176e32c69a0df1e6ed4b22f5a94350b.png)'
  prefs: []
  type: TYPE_IMG
- en: If you want to switch to only the base model, disable all LoRAs with the [disable_lora()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![no-lora](../Images/13732666737056b846ce11880fe30289.png)'
  prefs: []
  type: TYPE_IMG
- en: Monitoring active adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have attached multiple adapters in this tutorial, and if you‚Äôre feeling
    a bit lost on what adapters have been attached to the pipeline‚Äôs components, you
    can easily check the list of active adapters using the [get_active_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_active_adapters)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also get the active adapters of each pipeline component with [get_list_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_list_adapters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Fusing adapters into the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use PEFT to easily fuse/unfuse multiple adapters directly into the model
    weights (both UNet and text encoder) using the [fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)
    method, which can lead to a speed-up in inference and lower VRAM usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also fuse some adapters using `adapter_names` for faster generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Saving a pipeline after fusing the adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To properly save a pipeline after it‚Äôs been loaded with the adapters, it should
    be serialized like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
