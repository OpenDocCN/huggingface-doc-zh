- en: Load LoRAs for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are many adapters (with LoRAs being the most common type) trained in different
    styles to achieve different effects. You can even combine multiple adapters to
    create new and unique images. With the 🤗 [PEFT](https://huggingface.co/docs/peft/index)
    integration in 🤗 Diffusers, it is really easy to load and manage adapters for
    inference. In this guide, you’ll learn how to use different adapters with [Stable
    Diffusion XL (SDXL)](../api/pipelines/stable_diffusion/stable_diffusion_xl) for
    inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this guide, you’ll use LoRA as the main adapter technique, so we’ll
    use the terms LoRA and adapter interchangeably. You should have some familiarity
    with LoRA, and if you don’t, we welcome you to check out the [LoRA guide](https://huggingface.co/docs/peft/conceptual_guides/lora).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first install all the required libraries.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s load a pipeline with a SDXL checkpoint:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, load a LoRA checkpoint with the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: With the 🤗 PEFT integration, you can assign a specific `adapter_name` to the
    checkpoint, which let’s you easily switch between different LoRA checkpoints.
    Let’s call this adapter `"toy"`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then perform inference:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![toy-face](../Images/5113690135aa0bd78ddd1d3e7c53479d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: With the `adapter_name` parameter, it is really easy to use another adapter
    for inference! Load the [nerijs/pixel-art-xl](https://huggingface.co/nerijs/pixel-art-xl)
    adapter that has been fine-tuned to generate pixel art images, and let’s call
    it `"pixel"`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline automatically sets the first loaded adapter (`"toy"`) as the active
    adapter. But you can activate the `"pixel"` adapter with the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method as shown below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s now generate an image with the second adapter and check the result:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![pixel-art](../Images/d01ebc0347358d4583b5ade26b841a6f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Combine multiple adapters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also perform multi-adapter inference where you combine different adapter
    checkpoints for inference.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Once again, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate two LoRA checkpoints and specify the weight for how the checkpoints
    should be combined.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have set these two adapters, let’s generate an image from the combined
    adapters!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: LoRA checkpoints in the diffusion community are almost always obtained with
    [DreamBooth](https://huggingface.co/docs/diffusers/main/en/training/dreambooth).
    DreamBooth training often relies on “trigger” words in the input text prompts
    in order for the generation results to look as expected. When you combine multiple
    LoRA checkpoints, it’s important to ensure the trigger words for the corresponding
    LoRA checkpoints are present in the input text prompts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The trigger words for [CiroN2022/toy-face](https://hf.co/CiroN2022/toy-face)
    and [nerijs/pixel-art-xl](https://hf.co/nerijs/pixel-art-xl) are found in their
    repositories.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![toy-face-pixel-art](../Images/99de7c335e766f24d86d686716cfbe1c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Impressive! As you can see, the model was able to generate an image that mixes
    the characteristics of both adapters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go back to using only one adapter, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate the `"toy"` adapter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![toy-face-again](../Images/c176e32c69a0df1e6ed4b22f5a94350b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: If you want to switch to only the base model, disable all LoRAs with the [disable_lora()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)
    method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![no-lora](../Images/13732666737056b846ce11880fe30289.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![no-lora](../Images/13732666737056b846ce11880fe30289.png)'
- en: Monitoring active adapters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监视活动适配器
- en: 'You have attached multiple adapters in this tutorial, and if you’re feeling
    a bit lost on what adapters have been attached to the pipeline’s components, you
    can easily check the list of active adapters using the [get_active_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_active_adapters)
    method:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，您已经连接了多个适配器，如果您对已连接到管道组件的适配器感到有些迷茫，您可以使用[get_active_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_active_adapters)方法轻松检查活动适配器的列表：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also get the active adapters of each pipeline component with [get_list_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_list_adapters):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用[get_list_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_list_adapters)获取每个管道组件的活动适配器：
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Fusing adapters into the model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将适配器融合到模型中
- en: You can use PEFT to easily fuse/unfuse multiple adapters directly into the model
    weights (both UNet and text encoder) using the [fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)
    method, which can lead to a speed-up in inference and lower VRAM usage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用PEFT轻松地将多个适配器直接融合/解除融合到模型权重中（UNet和文本编码器均可），使用[fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)方法，这可以加快推理速度并降低VRAM使用率。
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can also fuse some adapters using `adapter_names` for faster generation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`adapter_names`来融合一些适配器，以加快生成速度：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Saving a pipeline after fusing the adapters
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在融合适配器后保存管道
- en: 'To properly save a pipeline after it’s been loaded with the adapters, it should
    be serialized like so:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载适配器后正确保存管道之前，应该像这样序列化：
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
