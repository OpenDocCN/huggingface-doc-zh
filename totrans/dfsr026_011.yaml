- en: Load LoRAs for inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: There are many adapters (with LoRAs being the most common type) trained in different
    styles to achieve different effects. You can even combine multiple adapters to
    create new and unique images. With the ğŸ¤— [PEFT](https://huggingface.co/docs/peft/index)
    integration in ğŸ¤— Diffusers, it is really easy to load and manage adapters for
    inference. In this guide, youâ€™ll learn how to use different adapters with [Stable
    Diffusion XL (SDXL)](../api/pipelines/stable_diffusion/stable_diffusion_xl) for
    inference.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this guide, youâ€™ll use LoRA as the main adapter technique, so weâ€™ll
    use the terms LoRA and adapter interchangeably. You should have some familiarity
    with LoRA, and if you donâ€™t, we welcome you to check out the [LoRA guide](https://huggingface.co/docs/peft/conceptual_guides/lora).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s first install all the required libraries.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, letâ€™s load a pipeline with a SDXL checkpoint:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, load a LoRA checkpoint with the [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights)
    method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: With the ğŸ¤— PEFT integration, you can assign a specific `adapter_name` to the
    checkpoint, which letâ€™s you easily switch between different LoRA checkpoints.
    Letâ€™s call this adapter `"toy"`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then perform inference:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![toy-face](../Images/5113690135aa0bd78ddd1d3e7c53479d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: With the `adapter_name` parameter, it is really easy to use another adapter
    for inference! Load the [nerijs/pixel-art-xl](https://huggingface.co/nerijs/pixel-art-xl)
    adapter that has been fine-tuned to generate pixel art images, and letâ€™s call
    it `"pixel"`.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline automatically sets the first loaded adapter (`"toy"`) as the active
    adapter. But you can activate the `"pixel"` adapter with the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method as shown below:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Letâ€™s now generate an image with the second adapter and check the result:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![pixel-art](../Images/d01ebc0347358d4583b5ade26b841a6f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Combine multiple adapters
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can also perform multi-adapter inference where you combine different adapter
    checkpoints for inference.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Once again, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate two LoRA checkpoints and specify the weight for how the checkpoints
    should be combined.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have set these two adapters, letâ€™s generate an image from the combined
    adapters!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: LoRA checkpoints in the diffusion community are almost always obtained with
    [DreamBooth](https://huggingface.co/docs/diffusers/main/en/training/dreambooth).
    DreamBooth training often relies on â€œtriggerâ€ words in the input text prompts
    in order for the generation results to look as expected. When you combine multiple
    LoRA checkpoints, itâ€™s important to ensure the trigger words for the corresponding
    LoRA checkpoints are present in the input text prompts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The trigger words for [CiroN2022/toy-face](https://hf.co/CiroN2022/toy-face)
    and [nerijs/pixel-art-xl](https://hf.co/nerijs/pixel-art-xl) are found in their
    repositories.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![toy-face-pixel-art](../Images/99de7c335e766f24d86d686716cfbe1c.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Impressive! As you can see, the model was able to generate an image that mixes
    the characteristics of both adapters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go back to using only one adapter, use the [set_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.set_adapters)
    method to activate the `"toy"` adapter:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![toy-face-again](../Images/c176e32c69a0df1e6ed4b22f5a94350b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: If you want to switch to only the base model, disable all LoRAs with the [disable_lora()](/docs/diffusers/v0.26.3/en/api/loaders/unet#diffusers.loaders.UNet2DConditionLoadersMixin.disable_lora)
    method.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![no-lora](../Images/13732666737056b846ce11880fe30289.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![no-lora](../Images/13732666737056b846ce11880fe30289.png)'
- en: Monitoring active adapters
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›‘è§†æ´»åŠ¨é€‚é…å™¨
- en: 'You have attached multiple adapters in this tutorial, and if youâ€™re feeling
    a bit lost on what adapters have been attached to the pipelineâ€™s components, you
    can easily check the list of active adapters using the [get_active_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_active_adapters)
    method:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæ‚¨å·²ç»è¿æ¥äº†å¤šä¸ªé€‚é…å™¨ï¼Œå¦‚æœæ‚¨å¯¹å·²è¿æ¥åˆ°ç®¡é“ç»„ä»¶çš„é€‚é…å™¨æ„Ÿåˆ°æœ‰äº›è¿·èŒ«ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[get_active_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_active_adapters)æ–¹æ³•è½»æ¾æ£€æŸ¥æ´»åŠ¨é€‚é…å™¨çš„åˆ—è¡¨ï¼š
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also get the active adapters of each pipeline component with [get_list_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_list_adapters):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[get_list_adapters()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.get_list_adapters)è·å–æ¯ä¸ªç®¡é“ç»„ä»¶çš„æ´»åŠ¨é€‚é…å™¨ï¼š
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Fusing adapters into the model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†é€‚é…å™¨èåˆåˆ°æ¨¡å‹ä¸­
- en: You can use PEFT to easily fuse/unfuse multiple adapters directly into the model
    weights (both UNet and text encoder) using the [fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)
    method, which can lead to a speed-up in inference and lower VRAM usage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨PEFTè½»æ¾åœ°å°†å¤šä¸ªé€‚é…å™¨ç›´æ¥èåˆ/è§£é™¤èåˆåˆ°æ¨¡å‹æƒé‡ä¸­ï¼ˆUNetå’Œæ–‡æœ¬ç¼–ç å™¨å‡å¯ï¼‰ï¼Œä½¿ç”¨[fuse_lora()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.fuse_lora)æ–¹æ³•ï¼Œè¿™å¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦å¹¶é™ä½VRAMä½¿ç”¨ç‡ã€‚
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can also fuse some adapters using `adapter_names` for faster generation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨`adapter_names`æ¥èåˆä¸€äº›é€‚é…å™¨ï¼Œä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦ï¼š
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Saving a pipeline after fusing the adapters
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨èåˆé€‚é…å™¨åä¿å­˜ç®¡é“
- en: 'To properly save a pipeline after itâ€™s been loaded with the adapters, it should
    be serialized like so:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ è½½é€‚é…å™¨åæ­£ç¡®ä¿å­˜ç®¡é“ä¹‹å‰ï¼Œåº”è¯¥åƒè¿™æ ·åºåˆ—åŒ–ï¼š
- en: '[PRE14]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
