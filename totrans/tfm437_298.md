# ViTMatte

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitmatte](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitmatte)

## æ¦‚è¿°

ViTMatteæ¨¡å‹æ˜¯ç”±å§šæ™¯å³°ã€ç‹å…´åˆšã€æ¨æ ‘ç”Ÿã€ç‹å®å…ƒåœ¨[ä½¿ç”¨é¢„è®­ç»ƒçš„æ™®é€šè§†è§‰Transformerå¢å¼ºå›¾åƒæŠ å›¾](https://arxiv.org/abs/2305.15272)ä¸­æå‡ºçš„ã€‚ViTMatteåˆ©ç”¨æ™®é€šçš„[è§†è§‰Transformer](vit)æ¥è¿›è¡Œå›¾åƒæŠ å›¾ä»»åŠ¡ï¼Œå³å‡†ç¡®ä¼°è®¡å›¾åƒå’Œè§†é¢‘ä¸­çš„å‰æ™¯å¯¹è±¡çš„è¿‡ç¨‹ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æœ€è¿‘ï¼Œæ™®é€šè§†è§‰Transformerï¼ˆViTsï¼‰åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿™è¦å½’åŠŸäºå®ƒä»¬å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å°šæœªå¾æœå›¾åƒæŠ å›¾é—®é¢˜ã€‚æˆ‘ä»¬å‡è®¾å›¾åƒæŠ å›¾ä¹Ÿå¯ä»¥é€šè¿‡ViTså¾—åˆ°æå‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆä¸”ç¨³å¥çš„åŸºäºViTçš„æŠ å›¾ç³»ç»Ÿï¼Œåä¸ºViTMatteã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ï¼ˆiï¼‰æ··åˆæ³¨æ„åŠ›æœºåˆ¶ç»“åˆå·ç§¯é¢ˆéƒ¨ï¼Œå¸®åŠ©ViTsåœ¨æŠ å›¾ä»»åŠ¡ä¸­å®ç°å‡ºè‰²çš„æ€§èƒ½-è®¡ç®—æŠ˜è¡·ã€‚ ï¼ˆiiï¼‰æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†èŠ‚æ•è·æ¨¡å—ï¼Œå®ƒåªç”±ç®€å•è½»é‡çº§å·ç§¯ç»„æˆï¼Œä»¥è¡¥å……æŠ å›¾æ‰€éœ€çš„è¯¦ç»†ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒViTMatteæ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡ç®€æ´çš„é€‚åº”æ€§é‡Šæ”¾ViTåœ¨å›¾åƒæŠ å›¾ä¸­æ½œåŠ›çš„å·¥ä½œã€‚å®ƒä»ViTåˆ°æŠ å›¾ç»§æ‰¿äº†è®¸å¤šä¼˜è¶Šçš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬å„ç§é¢„è®­ç»ƒç­–ç•¥ã€ç®€æ´çš„æ¶æ„è®¾è®¡å’Œçµæ´»çš„æ¨æ–­ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨Composition-1kå’ŒDistinctions-646ä¸Šè¯„ä¼°äº†ViTMatteï¼Œè¿™æ˜¯å›¾åƒæŠ å›¾æœ€å¸¸ç”¨çš„åŸºå‡†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¤§å¹…è¶…è¶Šäº†å…ˆå‰çš„æŠ å›¾ä½œå“ã€‚*

æ­¤æ¨¡å‹ç”±[nielsr](https://huggingface.co/nielsr)è´¡çŒ®ã€‚åŸå§‹ä»£ç å¯åœ¨[æ­¤å¤„](https://github.com/hustvl/ViTMatte)æ‰¾åˆ°ã€‚

![drawing](../Images/7e84c30063be5fee2342c39797706d98.png) ViTMatteé«˜å±‚æ¦‚è¿°ã€‚æ‘˜è‡ª[åŸå§‹è®ºæ–‡ã€‚](https://arxiv.org/abs/2305.15272)

## èµ„æº

æä¾›ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨ViTMatteã€‚

+   å…³äºä½¿ç”¨[VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting)è¿›è¡Œæ¨æ–­çš„æ¼”ç¤ºç¬”è®°æœ¬ï¼ŒåŒ…æ‹¬èƒŒæ™¯æ›¿æ¢ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViTMatte)æ‰¾åˆ°ã€‚

æ¨¡å‹æœŸæœ›å›¾åƒå’Œtrimapï¼ˆè¿æ¥åœ¨ä¸€èµ·ï¼‰ä½œä¸ºè¾“å…¥ã€‚ä¸ºæ­¤ç›®çš„ä½¿ç”¨`ViTMatteImageProcessor`ã€‚

## VitMatteConfig

### `class transformers.VitMatteConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/configuration_vitmatte.py#L32)

```py
( backbone_config: PretrainedConfig = None hidden_size: int = 384 batch_norm_eps: float = 1e-05 initializer_range: float = 0.02 convstream_hidden_sizes: List = [48, 96, 192] fusion_hidden_sizes: List = [256, 128, 64, 32] **kwargs )
```

å‚æ•°

+   `backbone_config` (`PretrainedConfig` or `dict`, *optional*, defaults to `VitDetConfig()`) â€” ä¸»å¹²æ¨¡å‹çš„é…ç½®ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 384) â€” è§£ç å™¨çš„è¾“å…¥é€šé“æ•°ã€‚

+   `batch_norm_eps` (`float`, *optional*, defaults to 1e-05) â€” æ‰¹é‡å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `convstream_hidden_sizes` (`List[int]`, *optional*, defaults to `[48, 96, 192]`) â€” ConvStreamæ¨¡å—çš„è¾“å‡ºé€šé“æ•°ã€‚

+   `fusion_hidden_sizes` (`List[int]`, *optional*, defaults to `[256, 128, 64, 32]`) â€” Fusionå—çš„è¾“å‡ºé€šé“æ•°ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting) é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª ViTMatte æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº ViTMatte [hustvl/vitmatte-small-composition-1k](https://huggingface.co/hustvl/vitmatte-small-composition-1k) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import VitMatteConfig, VitMatteForImageMatting

>>> # Initializing a ViTMatte hustvl/vitmatte-small-composition-1k style configuration
>>> configuration = VitMatteConfig()

>>> # Initializing a model (with random weights) from the hustvl/vitmatte-small-composition-1k style configuration
>>> model = VitMatteForImageMatting(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `to_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/configuration_vitmatte.py#L100)

```py
( )
```

å°†æ­¤å®ä¾‹åºåˆ—åŒ–ä¸º Python å­—å…¸ã€‚è¦†ç›–é»˜è®¤çš„ [to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict)ã€‚è¿”å›ï¼š`Dict[str, any]`ï¼šæ„æˆæ­¤é…ç½®å®ä¾‹çš„æ‰€æœ‰å±æ€§çš„å­—å…¸ï¼Œ

## VitMatteImageProcessor

### `class transformers.VitMatteImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/image_processing_vitmatte.py#L41)

```py
( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True size_divisibility: int = 32 **kwargs )
```

å‚æ•°

+   `do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æŒ‰æŒ‡å®šæ¯”ä¾‹ `rescale_factor` é‡æ–°ç¼©æ”¾å›¾åƒã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_rescale` å‚æ•°è¦†ç›–ã€‚

+   `rescale_factor` (`int` æˆ– `float`, *optional*, é»˜è®¤ä¸º `1/255`) â€” å¦‚æœé‡æ–°ç¼©æ”¾å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„ç¼©æ”¾å› å­ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `rescale_factor` å‚æ•°è¦†ç›–ã€‚

+   `do_normalize` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œè§„èŒƒåŒ–ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_normalize` å‚æ•°è¦†ç›–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º `IMAGENET_STANDARD_MEAN`) â€” å¦‚æœè§„èŒƒåŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„å‡å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒä¸­é€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_mean` å‚æ•°è¦†ç›–ã€‚

+   `image_std` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º `IMAGENET_STANDARD_STD`) â€” å¦‚æœè§„èŒƒåŒ–å›¾åƒï¼Œåˆ™ä½¿ç”¨çš„æ ‡å‡†å·®ã€‚è¿™æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æˆ–ä¸å›¾åƒä¸­é€šé“æ•°ç›¸åŒé•¿åº¦çš„æµ®ç‚¹æ•°åˆ—è¡¨ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `image_std` å‚æ•°è¦†ç›–ã€‚

+   `do_pad` (`bool`, *optional*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦å¡«å……å›¾åƒä»¥ä½¿å®½åº¦å’Œé«˜åº¦å¯è¢« `size_divisibility` æ•´é™¤ã€‚å¯ä»¥è¢« `preprocess` æ–¹æ³•ä¸­çš„ `do_pad` å‚æ•°è¦†ç›–ã€‚

+   `size_divisibility` (`int`, *optional*, é»˜è®¤ä¸º 32) â€” å›¾åƒçš„å®½åº¦å’Œé«˜åº¦å°†è¢«å¡«å……ä¸ºå¯è¢«æ­¤æ•°å­—æ•´é™¤ã€‚

æ„å»ºä¸€ä¸ª ViTMatte å›¾åƒå¤„ç†å™¨ã€‚

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/image_processing_vitmatte.py#L131)

```py
( images: Union trimaps: Union do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None size_divisibility: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

å‚æ•°

+   `images` (`ImageInput`) â€” é¢„å¤„ç†çš„å›¾åƒã€‚æœŸæœ›å•ä¸ªå›¾åƒæˆ–æ‰¹é‡å›¾åƒï¼Œåƒç´ å€¼èŒƒå›´ä¸º 0 åˆ° 255ã€‚å¦‚æœä¼ å…¥åƒç´ å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å›¾åƒï¼Œè¯·è®¾ç½® `do_rescale=False`ã€‚

+   `trimaps` (`ImageInput`) â€” é¢„å¤„ç†çš„ Trimapã€‚

+   `do_rescale` (`bool`, *optional*, é»˜è®¤ä¸º `self.do_rescale`) â€” æ˜¯å¦å°†å›¾åƒå€¼é‡æ–°ç¼©æ”¾åˆ° [0 - 1]ã€‚

+   `rescale_factor` (`float`, *optional*, é»˜è®¤ä¸º `self.rescale_factor`) â€” å¦‚æœ `do_rescale` è®¾ç½®ä¸º `True`ï¼Œåˆ™ç”¨äºé‡æ–°ç¼©æ”¾å›¾åƒçš„é‡æ–°ç¼©æ”¾å› å­ã€‚

+   `do_normalize` (`bool`, *optional*, é»˜è®¤ä¸º `self.do_normalize`) â€” æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œè§„èŒƒåŒ–ã€‚

+   `image_mean` (`float` æˆ– `List[float]`, *optional*, é»˜è®¤ä¸º `self.image_mean`) â€” å¦‚æœè®¾ç½® `do_normalize` ä¸º `True`ï¼Œåˆ™ä½¿ç”¨çš„å›¾åƒå‡å€¼ã€‚

+   `image_std`ï¼ˆ`float`æˆ–`List[float]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.image_std`ï¼‰-å¦‚æœ`do_normalize`è®¾ç½®ä¸º`True`ï¼Œè¦ä½¿ç”¨çš„å›¾åƒæ ‡å‡†å·®ã€‚

+   `do_pad`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.do_pad`ï¼‰-æ˜¯å¦å¡«å……å›¾åƒã€‚

+   `size_divisibility`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.size_divisibility`ï¼‰-å¦‚æœ`do_pad`è®¾ç½®ä¸º`True`ï¼Œåˆ™å¡«å……å›¾åƒçš„å¤§å°å¯è¢«æ•´é™¤ã€‚

+   `return_tensors`ï¼ˆ`str`æˆ–`TensorType`ï¼Œ*å¯é€‰*ï¼‰-è¦è¿”å›çš„å¼ é‡ç±»å‹ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   æœªè®¾ç½®ï¼šè¿”å›ä¸€ä¸ª`np.ndarray`åˆ—è¡¨ã€‚

    +   `TensorType.TENSORFLOW`æˆ–`'tf'`ï¼šè¿”å›ç±»å‹ä¸º`tf.Tensor`çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.PYTORCH`æˆ–`'pt'`ï¼šè¿”å›ç±»å‹ä¸º`torch.Tensor`çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.NUMPY`æˆ–`'np'`ï¼šè¿”å›ç±»å‹ä¸º`np.ndarray`çš„æ‰¹æ¬¡ã€‚

    +   `TensorType.JAX`æˆ–`'jax'`ï¼šè¿”å›ç±»å‹ä¸º`jax.numpy.ndarray`çš„æ‰¹æ¬¡ã€‚

+   `data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`ChannelDimension.FIRST`ï¼‰-è¾“å‡ºå›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   æœªè®¾ç½®ï¼šä½¿ç”¨è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚

+   `input_data_format`ï¼ˆ`ChannelDimension`æˆ–`str`ï¼Œ*å¯é€‰*ï¼‰-è¾“å…¥å›¾åƒçš„é€šé“ç»´åº¦æ ¼å¼ã€‚å¦‚æœæœªè®¾ç½®ï¼Œåˆ™ä»è¾“å…¥å›¾åƒä¸­æ¨æ–­é€šé“ç»´åº¦æ ¼å¼ã€‚å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š

    +   `"channels_first"`æˆ–`ChannelDimension.FIRST`ï¼šå›¾åƒä»¥ï¼ˆé€šé“æ•°ï¼Œé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

    +   `"channels_last"`æˆ–`ChannelDimension.LAST`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé€šé“æ•°ï¼‰æ ¼å¼ã€‚

    +   `"none"`æˆ–`ChannelDimension.NONE`ï¼šå›¾åƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰æ ¼å¼ã€‚

é¢„å¤„ç†ä¸€å¼ å›¾åƒæˆ–ä¸€æ‰¹å›¾åƒã€‚

## VitMatteForImageMatting

`transformers.VitMatteForImageMatting`ç±»

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/modeling_vitmatte.py#L253)

```py
( config )
```

å‚æ•°

+   æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚ä½¿ç”¨

+   ä½œä¸ºå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚-é…ç½®ï¼ˆ[UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfigï¼‰ï¼šå…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åˆ©ç”¨ä»»ä½•è§†è§‰éª¨å¹²çš„ViTMatteæ¡†æ¶ï¼Œä¾‹å¦‚ADE20kï¼ŒCityScapesã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/modeling_vitmatte.py#L268)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `pixel_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-åƒç´ å€¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæä¾›ï¼Œå°†å¿½ç•¥å¡«å……ã€‚å¯ä»¥ä½¿ç”¨[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)è·å–åƒç´ å€¼ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[VitMatteImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰-æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ï¼Œå¦‚æœéª¨å¹²ç½‘ç»œå…·æœ‰è¿™äº›å±‚ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰-æ˜¯å¦è¿”å›éª¨å¹²ç½‘ç»œæ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰-æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, height, width)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰-ç”¨äºè®¡ç®—æŸå¤±çš„åœ°é¢å®å†µå›¾åƒæŠ å›¾ã€‚

è¿”å›

`transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput`æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª`transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput`æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[VitMatteConfig](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteConfig)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” æŸå¤±ã€‚

+   `alphas`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰ â€” ä¼°è®¡çš„alphaå€¼ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ã€‚æ¨¡å‹åœ¨æ¯ä¸ªé˜¶æ®µè¾“å‡ºçš„éšè—çŠ¶æ€ï¼ˆä¹Ÿç§°ä¸ºç‰¹å¾å›¾ï¼‰ã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, patch_size, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import VitMatteImageProcessor, VitMatteForImageMatting
>>> import torch
>>> from PIL import Image
>>> from huggingface_hub import hf_hub_download

>>> processor = VitMatteImageProcessor.from_pretrained("hustvl/vitmatte-small-composition-1k")
>>> model = VitMatteForImageMatting.from_pretrained("hustvl/vitmatte-small-composition-1k")

>>> filepath = hf_hub_download(
...     repo_id="hf-internal-testing/image-matting-fixtures", filename="image.png", repo_type="dataset"
... )
>>> image = Image.open(filepath).convert("RGB")
>>> filepath = hf_hub_download(
...     repo_id="hf-internal-testing/image-matting-fixtures", filename="trimap.png", repo_type="dataset"
... )
>>> trimap = Image.open(filepath).convert("L")

>>> # prepare image + trimap for the model
>>> inputs = processor(images=image, trimaps=trimap, return_tensors="pt")

>>> with torch.no_grad():
...     alphas = model(**inputs).alphas
>>> print(alphas.shape)
torch.Size([1, 1, 640, 960])
```
