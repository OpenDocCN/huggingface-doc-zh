# ViTMatte

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitmatte](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vitmatte)

## 概述

ViTMatte模型是由姚景峰、王兴刚、杨树生、王宝元在[使用预训练的普通视觉Transformer增强图像抠图](https://arxiv.org/abs/2305.15272)中提出的。ViTMatte利用普通的[视觉Transformer](vit)来进行图像抠图任务，即准确估计图像和视频中的前景对象的过程。

论文摘要如下：

*最近，普通视觉Transformer（ViTs）在各种计算机视觉任务中表现出色，这要归功于它们强大的建模能力和大规模预训练。然而，它们尚未征服图像抠图问题。我们假设图像抠图也可以通过ViTs得到提升，并提出了一种新的高效且稳健的基于ViT的抠图系统，名为ViTMatte。我们的方法利用了（i）混合注意力机制结合卷积颈部，帮助ViTs在抠图任务中实现出色的性能-计算折衷。 （ii）此外，我们引入了细节捕获模块，它只由简单轻量级卷积组成，以补充抠图所需的详细信息。据我们所知，ViTMatte是第一个通过简洁的适应性释放ViT在图像抠图中潜力的工作。它从ViT到抠图继承了许多优越的特性，包括各种预训练策略、简洁的架构设计和灵活的推断策略。我们在Composition-1k和Distinctions-646上评估了ViTMatte，这是图像抠图最常用的基准，我们的方法取得了最先进的性能，并大幅超越了先前的抠图作品。*

此模型由[nielsr](https://huggingface.co/nielsr)贡献。原始代码可在[此处](https://github.com/hustvl/ViTMatte)找到。

![drawing](../Images/7e84c30063be5fee2342c39797706d98.png) ViTMatte高层概述。摘自[原始论文。](https://arxiv.org/abs/2305.15272)

## 资源

提供一份官方Hugging Face和社区（由🌎表示）资源列表，以帮助您开始使用ViTMatte。

+   关于使用[VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting)进行推断的演示笔记本，包括背景替换，可以在[这里](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViTMatte)找到。

模型期望图像和trimap（连接在一起）作为输入。为此目的使用`ViTMatteImageProcessor`。

## VitMatteConfig

### `class transformers.VitMatteConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/configuration_vitmatte.py#L32)

```py
( backbone_config: PretrainedConfig = None hidden_size: int = 384 batch_norm_eps: float = 1e-05 initializer_range: float = 0.02 convstream_hidden_sizes: List = [48, 96, 192] fusion_hidden_sizes: List = [256, 128, 64, 32] **kwargs )
```

参数

+   `backbone_config` (`PretrainedConfig` or `dict`, *optional*, defaults to `VitDetConfig()`) — 主干模型的配置。

+   `hidden_size` (`int`, *optional*, defaults to 384) — 解码器的输入通道数。

+   `batch_norm_eps` (`float`, *optional*, defaults to 1e-05) — 批量归一化层使用的epsilon。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `convstream_hidden_sizes` (`List[int]`, *optional*, defaults to `[48, 96, 192]`) — ConvStream模块的输出通道数。

+   `fusion_hidden_sizes` (`List[int]`, *optional*, defaults to `[256, 128, 64, 32]`) — Fusion块的输出通道数。

这是用于存储 [VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting) 配置的配置类。它用于根据指定的参数实例化一个 ViTMatte 模型，定义模型架构。使用默认值实例化配置将产生类似于 ViTMatte [hustvl/vitmatte-small-composition-1k](https://huggingface.co/hustvl/vitmatte-small-composition-1k) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import VitMatteConfig, VitMatteForImageMatting

>>> # Initializing a ViTMatte hustvl/vitmatte-small-composition-1k style configuration
>>> configuration = VitMatteConfig()

>>> # Initializing a model (with random weights) from the hustvl/vitmatte-small-composition-1k style configuration
>>> model = VitMatteForImageMatting(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `to_dict`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/configuration_vitmatte.py#L100)

```py
( )
```

将此实例序列化为 Python 字典。覆盖默认的 [to_dict()](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig.to_dict)。返回：`Dict[str, any]`：构成此配置实例的所有属性的字典，

## VitMatteImageProcessor

### `class transformers.VitMatteImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/image_processing_vitmatte.py#L41)

```py
( do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_pad: bool = True size_divisibility: int = 32 **kwargs )
```

参数

+   `do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按指定比例 `rescale_factor` 重新缩放图像。可以被 `preprocess` 方法中的 `do_rescale` 参数覆盖。

+   `rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新缩放图像，则使用的缩放因子。可以被 `preprocess` 方法中的 `rescale_factor` 参数覆盖。

+   `do_normalize` (`bool`, *optional*, 默认为 `True`) — 是否对图像进行规范化。可以被 `preprocess` 方法中的 `do_normalize` 参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_MEAN`) — 如果规范化图像，则使用的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_STD`) — 如果规范化图像，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

+   `do_pad` (`bool`, *optional*, 默认为 `True`) — 是否填充图像以使宽度和高度可被 `size_divisibility` 整除。可以被 `preprocess` 方法中的 `do_pad` 参数覆盖。

+   `size_divisibility` (`int`, *optional*, 默认为 32) — 图像的宽度和高度将被填充为可被此数字整除。

构建一个 ViTMatte 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/image_processing_vitmatte.py#L131)

```py
( images: Union trimaps: Union do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None size_divisibility: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 预处理的图像。期望单个图像或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `trimaps` (`ImageInput`) — 预处理的 Trimap。

+   `do_rescale` (`bool`, *optional*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放到 [0 - 1]。

+   `rescale_factor` (`float`, *optional*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则用于重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`, *optional*, 默认为 `self.do_normalize`) — 是否对图像进行规范化。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_mean`) — 如果设置 `do_normalize` 为 `True`，则使用的图像均值。

+   `image_std`（`float`或`List[float]`，*可选*，默认为`self.image_std`）-如果`do_normalize`设置为`True`，要使用的图像标准差。

+   `do_pad`（`bool`，*可选*，默认为`self.do_pad`）-是否填充图像。

+   `size_divisibility`（`int`，*可选*，默认为`self.size_divisibility`）-如果`do_pad`设置为`True`，则填充图像的大小可被整除。

+   `return_tensors`（`str`或`TensorType`，*可选*）-要返回的张量类型。可以是以下之一：

    +   未设置：返回一个`np.ndarray`列表。

    +   `TensorType.TENSORFLOW`或`'tf'`：返回类型为`tf.Tensor`的批次。

    +   `TensorType.PYTORCH`或`'pt'`：返回类型为`torch.Tensor`的批次。

    +   `TensorType.NUMPY`或`'np'`：返回类型为`np.ndarray`的批次。

    +   `TensorType.JAX`或`'jax'`：返回类型为`jax.numpy.ndarray`的批次。

+   `data_format`（`ChannelDimension`或`str`，*可选*，默认为`ChannelDimension.FIRST`）-输出图像的通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   未设置：使用输入图像的通道维度格式。

+   `input_data_format`（`ChannelDimension`或`str`，*可选*）-输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"`或`ChannelDimension.FIRST`：图像以（通道数，高度，宽度）格式。

    +   `"channels_last"`或`ChannelDimension.LAST`：图像以（高度，宽度，通道数）格式。

    +   `"none"`或`ChannelDimension.NONE`：图像以（高度，宽度）格式。

预处理一张图像或一批图像。

## VitMatteForImageMatting

`transformers.VitMatteForImageMatting`类

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/modeling_vitmatte.py#L253)

```py
( config )
```

参数

+   此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。使用

+   作为常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。-配置（[UperNetConfig](/docs/transformers/v4.37.2/en/model_doc/upernet#transformers.UperNetConfig）：具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

利用任何视觉骨干的ViTMatte框架，例如ADE20k，CityScapes。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vitmatte/modeling_vitmatte.py#L268)

```py
( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）-像素值。默认情况下，如果提供，将忽略填充。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[VitMatteImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `output_attentions`（`bool`，*可选*）-是否返回所有注意力层的注意力张量，如果骨干网络具有这些层。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）-是否返回骨干网络所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）-是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, height, width)`的`torch.LongTensor`，*可选*）-用于计算损失的地面实况图像抠图。

返回

`transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.vitmatte.modeling_vitmatte.ImageMattingOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置（[VitMatteConfig](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteConfig)）和输入而异的各种元素。

+   `loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`时返回） — 损失。

+   `alphas`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`） — 估计的alpha值。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[VitMatteForImageMatting](/docs/transformers/v4.37.2/en/model_doc/vitmatte#transformers.VitMatteForImageMatting)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import VitMatteImageProcessor, VitMatteForImageMatting
>>> import torch
>>> from PIL import Image
>>> from huggingface_hub import hf_hub_download

>>> processor = VitMatteImageProcessor.from_pretrained("hustvl/vitmatte-small-composition-1k")
>>> model = VitMatteForImageMatting.from_pretrained("hustvl/vitmatte-small-composition-1k")

>>> filepath = hf_hub_download(
...     repo_id="hf-internal-testing/image-matting-fixtures", filename="image.png", repo_type="dataset"
... )
>>> image = Image.open(filepath).convert("RGB")
>>> filepath = hf_hub_download(
...     repo_id="hf-internal-testing/image-matting-fixtures", filename="trimap.png", repo_type="dataset"
... )
>>> trimap = Image.open(filepath).convert("L")

>>> # prepare image + trimap for the model
>>> inputs = processor(images=image, trimaps=trimap, return_tensors="pt")

>>> with torch.no_grad():
...     alphas = model(**inputs).alphas
>>> print(alphas.shape)
torch.Size([1, 1, 640, 960])
```
