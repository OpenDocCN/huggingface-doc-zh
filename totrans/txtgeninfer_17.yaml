- en: Tensor Parallelism
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量并行
- en: 'Original text: [https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism](https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '原文链接: [https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism](https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Tensor parallelism is a technique used to fit a large model in multiple GPUs.
    For example, when multiplying the input tensors with the first weight tensor,
    the matrix multiplication is equivalent to splitting the weight tensor column-wise,
    multiplying each column with the input separately, and then concatenating the
    separate outputs. These outputs are then transferred from the GPUs and concatenated
    together to get the final result, like below 👇
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行是一种技术，用于在多个GPU中适配大型模型。例如，当将输入张量与第一个权重张量相乘时，矩阵乘法等同于按列拆分权重张量，分别将每列与输入相乘，然后将单独的输出连接起来。这些输出然后从GPU传输并连接在一起以获得最终结果，如下所示👇
- en: '![Image courtesy of Anton Lozkhov](../Images/2434150b9bb14baed7418fdaf716c588.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片由Anton Lozkhov提供](../Images/2434150b9bb14baed7418fdaf716c588.png)'
- en: Tensor Parallelism only works for [models officially supported](../supported_models),
    it will not work when falling back to `transformers`. You can get more information
    about unsupported models [here](../basic_tutorials/non_core_models).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行仅适用于[官方支持的模型](../supported_models)，当回退到`transformers`时将无法使用。您可以在[这里](../basic_tutorials/non_core_models)获取有关不受支持模型的更多信息。
- en: You can learn a lot more details about tensor-parallelism from [the `transformers`
    docs](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[transformers文档](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism)中了解更多有关张量并行的详细信息。
