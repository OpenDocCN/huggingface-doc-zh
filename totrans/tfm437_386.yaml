- en: PatchTST
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term
    Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie,
    Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level the model vectorizes time series into patches of a given size
    and encodes the resulting sequence of vectors via a Transformer that then outputs
    the prediction length forecast via an appropriate head. The model is illustrated
    in the following figure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![model](../Images/be8bf2205fa880351692bb155b259f27.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*We propose an efficient design of Transformer-based models for multivariate
    time series forecasting and self-supervised representation learning. It is based
    on two key components: (i) segmentation of time series into subseries-level patches
    which are served as input tokens to Transformer; (ii) channel-independence where
    each channel contains a single univariate time series that shares the same embedding
    and Transformer weights across all the series. Patching design naturally has three-fold
    benefit: local semantic information is retained in the embedding; computation
    and memory usage of the attention maps are quadratically reduced given the same
    look-back window; and the model can attend longer history. Our channel-independent
    patch time series Transformer (PatchTST) can improve the long-term forecasting
    accuracy significantly when compared with that of SOTA Transformer-based models.
    We also apply our model to self-supervised pre-training tasks and attain excellent
    fine-tuning performance, which outperforms supervised training on large datasets.
    Transferring of masked pre-trained representation on one dataset to others also
    produces SOTA forecasting accuracy.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong),
    [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12),
    [wmgifford](https://huggingface.co/wmgifford), and [kashif](https://huggingface.co/kashif).
    The original code can be found [here](https://github.com/yuqinie98/PatchTST).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model can also be used for time series classification and time series regression.
    See the respective [PatchTSTForClassification](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForClassification)
    and [PatchTSTForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForRegression)
    classes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: PatchTSTConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/configuration_patchtst.py#L31)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '`num_input_channels` (`int`, *optional*, defaults to 1) — The size of the target
    variable which by default is 1 for univariate targets. Would be > 1 in case of
    multivariate targets.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_length` (`int`, *optional*, defaults to 32) — The context length of
    the input sequence.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distribution_output` (`str`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model when loss is “nll”. Could be either “student_t”,
    “normal” or “negative_binomial”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss` (`str`, *optional*, defaults to `"mse"`) — The loss function for the
    model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (“nll”) and for point estimates it is the mean
    squared error “mse”.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_length` (`int`, *optional*, defaults to 1) — Define the patch length
    of the patchification process.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_stride` (`int`, *optional*, defaults to 1) — Define the stride of the
    patchification process.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 3) — Number of hidden layers.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_model` (`int`, *optional*, defaults to 128) — Dimensionality of the transformer
    layers.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share_embedding` (`bool`, *optional*, defaults to `True`) — Sharing the input
    embedding across all channels.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channel_attention` (`bool`, *optional*, defaults to `False`) — Activate channel
    attention block in the Transformer to allow channels to attend each other.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffn_dim` (`int`, *optional*, defaults to 512) — Dimension of the “intermediate”
    (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_type` (`str` , *optional*, defaults to `"batchnorm"`) — Normalization
    at each Transformer layer. Can be `"batchnorm"` or `"layernorm"`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_eps` (`float`, *optional*, defaults to 1e-05) — A value added to the
    denominator for numerical stability of normalization.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the Transformer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positional_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    in the positional embedding layer.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path_dropout` (`float`, *optional*, defaults to 0.0) — The dropout path in
    the residual block.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ff_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    used between the two layers of the feed-forward networks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, *optional*, defaults to `True`) — Whether to add bias in the
    feed-forward networks.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) — The non-linear
    activation function (string) in the Transformer.`"gelu"` and `"relu"` are supported.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre_norm` (`bool`, *optional*, defaults to `True`) — Normalization is applied
    before self-attention if pre_norm is set to `True`. Otherwise, normalization is
    applied after residual block.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positional_encoding_type` (`str`, *optional*, defaults to `"sincos"`) — Positional
    encodings. Options `"random"` and `"sincos"` are supported.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cls_token` (`bool`, *optional*, defaults to `False`) — Whether cls token
    is used.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share_projection` (`bool`, *optional*, defaults to `True`) — Sharing the projection
    layer across different channels in the forecast head.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaling` (`Union`, *optional*, defaults to `"std"`) — Whether to scale the
    input targets via “mean” scaler, “std” scaler or no scaler if `None`. If `True`,
    the scaler is set to “mean”.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_mask_input` (`bool`, *optional*) — Apply masking during the pretraining.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_type` (`str`, *optional*, defaults to `"random"`) — Masking type. Only
    `"random"` and `"forecast"` are currently supported.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_mask_ratio` (`float`, *optional*, defaults to 0.5) — Masking ratio
    applied to mask the input data during random pretraining.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_forecast_mask_patches` (`int` or `list`, *optional*, defaults to `[2]`)
    — Number of patches to be masked at the end of each batch sample. If it is an
    integer, all the samples in the batch will have the same number of masked patches.
    If it is a list, samples in the batch will be randomly masked by numbers defined
    in the list. This argument is only used for forecast pretraining.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channel_consistent_masking` (`bool`, *optional*, defaults to `False`) — If
    channel consistent masking is True, all the channels will have the same masking
    pattern.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unmasked_channel_indices` (`list`, *optional*) — Indices of channels that
    are not masked during pretraining. Values in the list are number between 1 and
    `num_input_channels`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_value` (`int`, *optional*, defaults to 0) — Values in the masked patches
    will be filled by `mask_value`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooling_type` (`str`, *optional*, defaults to `"mean"`) — Pooling of the embedding.
    `"mean"`, `"max"` and `None` are supported.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for head.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction_length` (`int`, *optional*, defaults to 24) — The prediction horizon
    that the model will output.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_targets` (`int`, *optional*, defaults to 1) — Number of targets for regression
    and classification tasks. For classification, it is the number of classes.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_range` (`list`, *optional*) — Output range for regression task. The
    range of output values can be set to enforce the model to produce values within
    a range.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples is generated in parallel for probabilistic prediction.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [PatchTSTModel](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTModel).
    It is used to instantiate an PatchTST model according to the specified arguments,
    defining the model architecture. [ibm/patchtst](https://huggingface.co/ibm/patchtst)
    architecture.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PatchTSTModel
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTModel`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1147)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare PatchTST Model outputting raw hidden-states without any specific head.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1170)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_values` (`torch.BoolTensor` of shape `(batch_size, prediction_length,
    num_input_channels)`, *optional*) — Future target values associated with the `past_values`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PatchTSTForPrediction
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForPrediction`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1638)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for prediction model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1672)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_values` (`torch.Tensor` of shape `(bs, forecast_len, num_input_channels)`,
    *optional*) — Future target values associated with the `past_values`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: PatchTSTForClassification
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForClassification`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1443)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for classification model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1462)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_values` (`torch.Tensor`, *optional*) — Labels associates with the `past_values`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: PatchTSTForPretraining
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForPretraining`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1291)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for pretrain model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1306)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: PatchTSTForRegression
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForRegression`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1884)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for regression model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1915)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — 输入模型的序列'
- en: '`target_values` (`torch.Tensor` of shape `(bs, num_input_channels)`) — Target
    values associates with the `past_values`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_values` (`torch.Tensor` of shape `(bs, num_input_channels)`) — 与`past_values`相关联的目标值'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示观察到的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示缺失的值（即被零替换的NaN）。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有层的输出注意力'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个`ModelOutput`而不是一个普通的元组。'
- en: 'Examples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
