- en: PatchTST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term
    Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie,
    Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level the model vectorizes time series into patches of a given size
    and encodes the resulting sequence of vectors via a Transformer that then outputs
    the prediction length forecast via an appropriate head. The model is illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![model](../Images/be8bf2205fa880351692bb155b259f27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We propose an efficient design of Transformer-based models for multivariate
    time series forecasting and self-supervised representation learning. It is based
    on two key components: (i) segmentation of time series into subseries-level patches
    which are served as input tokens to Transformer; (ii) channel-independence where
    each channel contains a single univariate time series that shares the same embedding
    and Transformer weights across all the series. Patching design naturally has three-fold
    benefit: local semantic information is retained in the embedding; computation
    and memory usage of the attention maps are quadratically reduced given the same
    look-back window; and the model can attend longer history. Our channel-independent
    patch time series Transformer (PatchTST) can improve the long-term forecasting
    accuracy significantly when compared with that of SOTA Transformer-based models.
    We also apply our model to self-supervised pre-training tasks and attain excellent
    fine-tuning performance, which outperforms supervised training on large datasets.
    Transferring of masked pre-trained representation on one dataset to others also
    produces SOTA forecasting accuracy.*'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong),
    [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12),
    [wmgifford](https://huggingface.co/wmgifford), and [kashif](https://huggingface.co/kashif).
    The original code can be found [here](https://github.com/yuqinie98/PatchTST).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model can also be used for time series classification and time series regression.
    See the respective [PatchTSTForClassification](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForClassification)
    and [PatchTSTForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForRegression)
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: PatchTSTConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/configuration_patchtst.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_input_channels` (`int`, *optional*, defaults to 1) — The size of the target
    variable which by default is 1 for univariate targets. Would be > 1 in case of
    multivariate targets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_length` (`int`, *optional*, defaults to 32) — The context length of
    the input sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distribution_output` (`str`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model when loss is “nll”. Could be either “student_t”,
    “normal” or “negative_binomial”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss` (`str`, *optional*, defaults to `"mse"`) — The loss function for the
    model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (“nll”) and for point estimates it is the mean
    squared error “mse”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_length` (`int`, *optional*, defaults to 1) — Define the patch length
    of the patchification process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_stride` (`int`, *optional*, defaults to 1) — Define the stride of the
    patchification process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 3) — Number of hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_model` (`int`, *optional*, defaults to 128) — Dimensionality of the transformer
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share_embedding` (`bool`, *optional*, defaults to `True`) — Sharing the input
    embedding across all channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channel_attention` (`bool`, *optional*, defaults to `False`) — Activate channel
    attention block in the Transformer to allow channels to attend each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ffn_dim` (`int`, *optional*, defaults to 512) — Dimension of the “intermediate”
    (often named feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_type` (`str` , *optional*, defaults to `"batchnorm"`) — Normalization
    at each Transformer layer. Can be `"batchnorm"` or `"layernorm"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_eps` (`float`, *optional*, defaults to 1e-05) — A value added to the
    denominator for numerical stability of normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positional_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    in the positional embedding layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`path_dropout` (`float`, *optional*, defaults to 0.0) — The dropout path in
    the residual block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ff_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    used between the two layers of the feed-forward networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias` (`bool`, *optional*, defaults to `True`) — Whether to add bias in the
    feed-forward networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) — The non-linear
    activation function (string) in the Transformer.`"gelu"` and `"relu"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pre_norm` (`bool`, *optional*, defaults to `True`) — Normalization is applied
    before self-attention if pre_norm is set to `True`. Otherwise, normalization is
    applied after residual block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positional_encoding_type` (`str`, *optional*, defaults to `"sincos"`) — Positional
    encodings. Options `"random"` and `"sincos"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cls_token` (`bool`, *optional*, defaults to `False`) — Whether cls token
    is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`share_projection` (`bool`, *optional*, defaults to `True`) — Sharing the projection
    layer across different channels in the forecast head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaling` (`Union`, *optional*, defaults to `"std"`) — Whether to scale the
    input targets via “mean” scaler, “std” scaler or no scaler if `None`. If `True`,
    the scaler is set to “mean”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_mask_input` (`bool`, *optional*) — Apply masking during the pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_type` (`str`, *optional*, defaults to `"random"`) — Masking type. Only
    `"random"` and `"forecast"` are currently supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_mask_ratio` (`float`, *optional*, defaults to 0.5) — Masking ratio
    applied to mask the input data during random pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_forecast_mask_patches` (`int` or `list`, *optional*, defaults to `[2]`)
    — Number of patches to be masked at the end of each batch sample. If it is an
    integer, all the samples in the batch will have the same number of masked patches.
    If it is a list, samples in the batch will be randomly masked by numbers defined
    in the list. This argument is only used for forecast pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channel_consistent_masking` (`bool`, *optional*, defaults to `False`) — If
    channel consistent masking is True, all the channels will have the same masking
    pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unmasked_channel_indices` (`list`, *optional*) — Indices of channels that
    are not masked during pretraining. Values in the list are number between 1 and
    `num_input_channels`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_value` (`int`, *optional*, defaults to 0) — Values in the masked patches
    will be filled by `mask_value`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooling_type` (`str`, *optional*, defaults to `"mean"`) — Pooling of the embedding.
    `"mean"`, `"max"` and `None` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction_length` (`int`, *optional*, defaults to 24) — The prediction horizon
    that the model will output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_targets` (`int`, *optional*, defaults to 1) — Number of targets for regression
    and classification tasks. For classification, it is the number of classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_range` (`list`, *optional*) — Output range for regression task. The
    range of output values can be set to enforce the model to produce values within
    a range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples is generated in parallel for probabilistic prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of an [PatchTSTModel](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTModel).
    It is used to instantiate an PatchTST model according to the specified arguments,
    defining the model architecture. [ibm/patchtst](https://huggingface.co/ibm/patchtst)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: PatchTSTModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1147)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare PatchTST Model outputting raw hidden-states without any specific head.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_values` (`torch.BoolTensor` of shape `(batch_size, prediction_length,
    num_input_channels)`, *optional*) — Future target values associated with the `past_values`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: PatchTSTForPrediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForPrediction`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1638)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for prediction model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1672)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`future_values` (`torch.Tensor` of shape `(bs, forecast_len, num_input_channels)`,
    *optional*) — Future target values associated with the `past_values`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: PatchTSTForClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1443)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for classification model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1462)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_values` (`torch.Tensor`, *optional*) — Labels associates with the `past_values`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: PatchTSTForPretraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForPretraining`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1291)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for pretrain model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1306)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: PatchTSTForRegression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.PatchTSTForRegression`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1884)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PatchTST for regression model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1915)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_values` (`torch.Tensor` of shape `(bs, num_input_channels)`) — Target
    values associates with the `past_values`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for values that are `observed`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
