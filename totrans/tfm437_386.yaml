- en: PatchTST
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PatchTST
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/patchtst)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term
    Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie,
    Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'PatchTST模型由Yuqi Nie、Nam H. Nguyen、Phanwadee Sinthong和Jayant Kalagnanam在[A Time
    Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730)中提出。'
- en: 'At a high level the model vectorizes time series into patches of a given size
    and encodes the resulting sequence of vectors via a Transformer that then outputs
    the prediction length forecast via an appropriate head. The model is illustrated
    in the following figure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，该模型将时间序列向量化为给定大小的补丁，并通过一个Transformer对生成的向量序列进行编码，然后通过适当的头部输出预测长度的预测。该模型如下图所示：
- en: '![model](../Images/be8bf2205fa880351692bb155b259f27.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![model](../Images/be8bf2205fa880351692bb155b259f27.png)'
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*We propose an efficient design of Transformer-based models for multivariate
    time series forecasting and self-supervised representation learning. It is based
    on two key components: (i) segmentation of time series into subseries-level patches
    which are served as input tokens to Transformer; (ii) channel-independence where
    each channel contains a single univariate time series that shares the same embedding
    and Transformer weights across all the series. Patching design naturally has three-fold
    benefit: local semantic information is retained in the embedding; computation
    and memory usage of the attention maps are quadratically reduced given the same
    look-back window; and the model can attend longer history. Our channel-independent
    patch time series Transformer (PatchTST) can improve the long-term forecasting
    accuracy significantly when compared with that of SOTA Transformer-based models.
    We also apply our model to self-supervised pre-training tasks and attain excellent
    fine-tuning performance, which outperforms supervised training on large datasets.
    Transferring of masked pre-trained representation on one dataset to others also
    produces SOTA forecasting accuracy.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们提出了一种用于多变量时间序列预测和自监督表示学习的基于Transformer的模型的高效设计。它基于两个关键组件：(i) 将时间序列分割为子系列级别的补丁，这些补丁作为输入标记提供给Transformer；(ii)
    通道独立性，其中每个通道包含一个单变量时间序列，共享相同的嵌入和Transformer权重。补丁设计自然具有三重好处：在嵌入中保留局部语义信息；在给定相同回顾窗口的情况下，注意力图的计算和内存使用量呈二次减少；模型可以关注更长的历史。我们的通道独立补丁时间序列Transformer（PatchTST）可以显著提高长期预测的准确性，与SOTA基于Transformer的模型相比。我们还将我们的模型应用于自监督预训练任务，并获得出色的微调性能，优于大型数据集上的监督训练。将一个数据集上的掩码预训练表示转移到其他数据集也会产生SOTA的预测准确性。*'
- en: This model was contributed by [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong),
    [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12),
    [wmgifford](https://huggingface.co/wmgifford), and [kashif](https://huggingface.co/kashif).
    The original code can be found [here](https://github.com/yuqinie98/PatchTST).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[namctin](https://huggingface.co/namctin)、[gsinthong](https://huggingface.co/gsinthong)、[diepi](https://huggingface.co/diepi)、[vijaye12](https://huggingface.co/vijaye12)、[wmgifford](https://huggingface.co/wmgifford)和[kashif](https://huggingface.co/kashif)贡献。原始代码可在[此处](https://github.com/yuqinie98/PatchTST)找到。
- en: Usage tips
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: The model can also be used for time series classification and time series regression.
    See the respective [PatchTSTForClassification](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForClassification)
    and [PatchTSTForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForRegression)
    classes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型还可用于时间序列分类和时间序列回归。请参阅相应的[PatchTSTForClassification](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForClassification)和[PatchTSTForRegression](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTForRegression)类。
- en: PatchTSTConfig
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTConfig
- en: '### `class transformers.PatchTSTConfig`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/configuration_patchtst.py#L31)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/configuration_patchtst.py#L31)'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`num_input_channels` (`int`, *optional*, defaults to 1) — The size of the target
    variable which by default is 1 for univariate targets. Would be > 1 in case of
    multivariate targets.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_input_channels` (`int`, *optional*, 默认为1) — 目标变量的大小，默认情况下为单变量目标的1。在多变量目标的情况下会大于1。'
- en: '`context_length` (`int`, *optional*, defaults to 32) — The context length of
    the input sequence.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`context_length` (`int`, *optional*, 默认为32) — 输入序列的上下文长度。'
- en: '`distribution_output` (`str`, *optional*, defaults to `"student_t"`) — The
    distribution emission head for the model when loss is “nll”. Could be either “student_t”,
    “normal” or “negative_binomial”.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distribution_output` (`str`, *optional*, 默认为`"student_t"`) — 当损失为“nll”时，模型的分布发射头。可以是“student_t”、“normal”或“negative_binomial”之一。'
- en: '`loss` (`str`, *optional*, defaults to `"mse"`) — The loss function for the
    model corresponding to the `distribution_output` head. For parametric distributions
    it is the negative log likelihood (“nll”) and for point estimates it is the mean
    squared error “mse”.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss` (`str`, *optional*, 默认为`"mse"`) — 与`distribution_output`头部对应的模型损失函数。对于参数分布，为负对数似然（“nll”），对于点估计，为均方误差“mse”。'
- en: '`patch_length` (`int`, *optional*, defaults to 1) — Define the patch length
    of the patchification process.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_length` (`int`, *optional*, 默认为1) — 定义补丁化过程的补丁长度。'
- en: '`patch_stride` (`int`, *optional*, defaults to 1) — Define the stride of the
    patchification process.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_stride` (`int`, *optional*, 默认为1) — 定义补丁化过程的步幅。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 3) — Number of hidden layers.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为3) — 隐藏层的数量。'
- en: '`d_model` (`int`, *optional*, defaults to 128) — Dimensionality of the transformer
    layers.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d_model` (`int`, *optional*, 默认为128) — Transformer层的维度。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为4) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`share_embedding` (`bool`, *optional*, defaults to `True`) — Sharing the input
    embedding across all channels.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`share_embedding` (`bool`, *optional*, 默认为`True`) — 在所有通道之间共享输入嵌入。'
- en: '`channel_attention` (`bool`, *optional*, defaults to `False`) — Activate channel
    attention block in the Transformer to allow channels to attend each other.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channel_attention` (`bool`, *optional*, 默认为`False`) — 激活Transformer中的通道注意力块，允许通道相互关注。'
- en: '`ffn_dim` (`int`, *optional*, defaults to 512) — Dimension of the “intermediate”
    (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ffn_dim` (`int`, *optional*, 默认为512) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`norm_type` (`str` , *optional*, defaults to `"batchnorm"`) — Normalization
    at each Transformer layer. Can be `"batchnorm"` or `"layernorm"`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_type` (`str` , *optional*, 默认为`"batchnorm"`) — 每个Transformer层的归一化。可以是`"batchnorm"`或`"layernorm"`。'
- en: '`norm_eps` (`float`, *optional*, defaults to 1e-05) — A value added to the
    denominator for numerical stability of normalization.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_eps` (`float`, *optional*, 默认为1e-05) — 添加到归一化分母以提高数值稳定性的值。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for the attention probabilities.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, 默认为0.0) — 注意力概率的dropout概率。'
- en: '`dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for all fully connected layers in the Transformer.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dropout` (`float`, *optional*, 默认为0.0) — Transformer中所有全连接层的dropout概率。'
- en: '`positional_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    in the positional embedding layer.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`positional_dropout` (`float`, *optional*, 默认为0.0) — 位置嵌入层中的dropout概率。'
- en: '`path_dropout` (`float`, *optional*, defaults to 0.0) — The dropout path in
    the residual block.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path_dropout` (`float`, *optional*, 默认为0.0) — 残差块中的路径dropout。'
- en: '`ff_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    used between the two layers of the feed-forward networks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ff_dropout` (`float`, *optional*, 默认为0.0) — 在前馈网络的两层之间使用的dropout概率。'
- en: '`bias` (`bool`, *optional*, defaults to `True`) — Whether to add bias in the
    feed-forward networks.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` (`bool`, *optional*, 默认为`True`) — 是否在前馈网络中添加偏置。'
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) — The non-linear
    activation function (string) in the Transformer.`"gelu"` and `"relu"` are supported.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`, *optional*, 默认为`"gelu"`) — Transformer中的非线性激活函数（字符串）。支持`"gelu"`和`"relu"`。'
- en: '`pre_norm` (`bool`, *optional*, defaults to `True`) — Normalization is applied
    before self-attention if pre_norm is set to `True`. Otherwise, normalization is
    applied after residual block.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_norm` (`bool`, *optional*, 默认为`True`) — 如果`pre_norm`设置为`True`，则在自注意力之前应用归一化。否则，在残差块之后应用归一化。'
- en: '`positional_encoding_type` (`str`, *optional*, defaults to `"sincos"`) — Positional
    encodings. Options `"random"` and `"sincos"` are supported.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`positional_encoding_type` (`str`, *optional*, 默认为`"sincos"`) — 位置编码。支持选项`"random"`和`"sincos"`。'
- en: '`use_cls_token` (`bool`, *optional*, defaults to `False`) — Whether cls token
    is used.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cls_token` (`bool`, *optional*, 默认为`False`) — 是否使用cls标记。'
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated normal weight initialization distribution.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_std` (`float`, *optional*, 默认为0.02) — 截断正态权重初始化分布的标准差。'
- en: '`share_projection` (`bool`, *optional*, defaults to `True`) — Sharing the projection
    layer across different channels in the forecast head.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`share_projection` (`bool`, *optional*, 默认为`True`) — 在预测头部中跨不同通道共享投影层。'
- en: '`scaling` (`Union`, *optional*, defaults to `"std"`) — Whether to scale the
    input targets via “mean” scaler, “std” scaler or no scaler if `None`. If `True`,
    the scaler is set to “mean”.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling` (`Union`, *optional*, 默认为`"std"`) — 是否通过“mean”缩放器、“std”缩放器或如果为`None`则不缩放来缩放输入目标。如果为`True`，则缩放器设置为“mean”。'
- en: '`do_mask_input` (`bool`, *optional*) — Apply masking during the pretraining.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_mask_input` (`bool`, *optional*) — 在预训练期间应用屏蔽。'
- en: '`mask_type` (`str`, *optional*, defaults to `"random"`) — Masking type. Only
    `"random"` and `"forecast"` are currently supported.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_type` (`str`, *optional*, 默认为`"random"`) — 屏蔽类型。目前仅支持`"random"`和`"forecast"`。'
- en: '`random_mask_ratio` (`float`, *optional*, defaults to 0.5) — Masking ratio
    applied to mask the input data during random pretraining.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_mask_ratio` (`float`, *optional*, 默认为0.5) — 用于在随机预训练期间屏蔽输入数据的屏蔽比例。'
- en: '`num_forecast_mask_patches` (`int` or `list`, *optional*, defaults to `[2]`)
    — Number of patches to be masked at the end of each batch sample. If it is an
    integer, all the samples in the batch will have the same number of masked patches.
    If it is a list, samples in the batch will be randomly masked by numbers defined
    in the list. This argument is only used for forecast pretraining.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_forecast_mask_patches` (`int`或`list`, *optional*, 默认为`[2]`) — 每个批次样本末尾要屏蔽的补丁数量。如果是整数，则批次中的所有样本将具有相同数量的屏蔽补丁。如果是列表，则批次中的样本将被随机屏蔽，屏蔽数量由列表中定义。此参数仅用于预测预训练。'
- en: '`channel_consistent_masking` (`bool`, *optional*, defaults to `False`) — If
    channel consistent masking is True, all the channels will have the same masking
    pattern.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`channel_consistent_masking` (`bool`, *optional*, 默认为`False`) — 如果通道一致屏蔽为True，则所有通道将具有相同的屏蔽模式。'
- en: '`unmasked_channel_indices` (`list`, *optional*) — Indices of channels that
    are not masked during pretraining. Values in the list are number between 1 and
    `num_input_channels`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unmasked_channel_indices` (`list`, *optional*) — 未在预训练期间屏蔽的通道索引。列表中的值为1到`num_input_channels`之间的数字。'
- en: '`mask_value` (`int`, *optional*, defaults to 0) — Values in the masked patches
    will be filled by `mask_value`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_value` (`int`, *optional*, 默认为0) — 屏蔽补丁中的值将被`mask_value`填充。'
- en: '`pooling_type` (`str`, *optional*, defaults to `"mean"`) — Pooling of the embedding.
    `"mean"`, `"max"` and `None` are supported.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooling_type` (`str`, *optional*, 默认为`"mean"`) — 嵌入的池化。支持`"mean"`、`"max"`和`None`。'
- en: '`head_dropout` (`float`, *optional*, defaults to 0.0) — The dropout probability
    for head.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_dropout` (`float`, *optional*, 默认为0.0) — 头部的dropout概率。'
- en: '`prediction_length` (`int`, *optional*, defaults to 24) — The prediction horizon
    that the model will output.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prediction_length`（`int`，*可选*，默认为 24）— 模型将输出的预测时间范围。'
- en: '`num_targets` (`int`, *optional*, defaults to 1) — Number of targets for regression
    and classification tasks. For classification, it is the number of classes.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_targets`（`int`，*可选*，默认为 1）— 回归和分类任务的目标数量。对于分类，它是类的数量。'
- en: '`output_range` (`list`, *optional*) — Output range for regression task. The
    range of output values can be set to enforce the model to produce values within
    a range.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_range`（`list`，*可选*）— 回归任务的输出范围。可以设置输出值的范围以强制模型生成在范围内的值。'
- en: '`num_parallel_samples` (`int`, *optional*, defaults to 100) — The number of
    samples is generated in parallel for probabilistic prediction.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_parallel_samples`（`int`，*可选*，默认为 100）— 并行生成的样本数，用于概率预测。'
- en: This is the configuration class to store the configuration of an [PatchTSTModel](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTModel).
    It is used to instantiate an PatchTST model according to the specified arguments,
    defining the model architecture. [ibm/patchtst](https://huggingface.co/ibm/patchtst)
    architecture.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置类，用于存储 [PatchTSTModel](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTModel)
    的配置。它用于根据指定的参数实例化 PatchTST 模型，定义模型架构。[ibm/patchtst](https://huggingface.co/ibm/patchtst)
    架构。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    可用于控制模型输出。阅读 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PatchTSTModel
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTModel
- en: '### `class transformers.PatchTSTModel`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1147)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1147)'
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig)）—
    具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare PatchTST Model outputting raw hidden-states without any specific head.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 裸 PatchTST 模型输出原始隐藏状态，没有特定的头。该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1170)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1170)'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为 `(bs, sequence_length, num_input_channels)` 的 `torch.Tensor`，*必需*）—
    输入序列到模型'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（形状为 `(batch_size, sequence_length, num_input_channels)`
    的 `torch.BoolTensor`，*可选*）— 布尔掩码，指示哪些 `past_values` 被观察到，哪些是缺失的。掩码值选在 `[0, 1]`：'
- en: 1 for values that are `observed`,
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示 `观察到` 的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示 `缺失` 的值（即被零替换的 NaN）。
- en: '`future_values` (`torch.BoolTensor` of shape `(batch_size, prediction_length,
    num_input_channels)`, *optional*) — Future target values associated with the `past_values`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values`（形状为 `(batch_size, prediction_length, num_input_channels)` 的
    `torch.BoolTensor`，*可选*）— 与 `past_values` 相关的未来目标值'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有层的输出注意力'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回 `ModelOutput` 而不是普通元组。'
- en: 'Examples:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: PatchTSTForPrediction
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTForPrediction
- en: '### `class transformers.PatchTSTForPrediction`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTForPrediction`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1638)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1638)'
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The PatchTST for prediction model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测的PatchTST模型。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1672)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1672)'
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.Tensor`，形状为`(bs, sequence_length, num_input_channels)`，*required*)
    — 输入序列到模型'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor`，形状为`(batch_size, sequence_length,
    num_input_channels)`，*optional*) — 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`范围内：'
- en: 1 for values that are `observed`,
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`观察到`的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`缺失`的值（即用零替换的NaN）。
- en: '`future_values` (`torch.Tensor` of shape `(bs, forecast_len, num_input_channels)`,
    *optional*) — Future target values associated with the `past_values`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`future_values` (`torch.Tensor`，形状为`(bs, forecast_len, num_input_channels)`，*optional*)
    — 与`past_values`相关联的未来目标值'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有层的注意力输出'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回`ModelOutput`而不是普通元组。'
- en: 'Examples:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: PatchTSTForClassification
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTForClassification
- en: '### `class transformers.PatchTSTForClassification`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTForClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1443)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1443)'
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The PatchTST for classification model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类的PatchTST模型。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库实现的所有模型的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1462)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1462)'
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.Tensor`，形状为`(bs, sequence_length, num_input_channels)`，*required*)
    — 输入序列到模型'
- en: '`target_values` (`torch.Tensor`, *optional*) — Labels associates with the `past_values`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_values` (`torch.Tensor`, *optional*) — 与`past_values`相关联的标签'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor`，形状为`(batch_size, sequence_length,
    num_input_channels)`，*optional*) — 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`范围内：'
- en: 1 for values that are `observed`,
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示值是`observed`，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`missing`的值（即被零替换的NaN）为0。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）-是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）-是否返回所有层的输出注意力'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）-是否返回`ModelOutput`而不是普通元组。'
- en: 'Examples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: PatchTSTForPretraining
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTForPretraining
- en: '### `class transformers.PatchTSTForPretraining`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTForPretraining`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1291)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1291)'
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig））-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The PatchTST for pretrain model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预训练模型的PatchTST。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以了解所有与一般使用和行为相关的事项。
- en: '#### `forward`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1306)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1306)'
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values`（形状为`(bs, sequence_length, num_input_channels)`的`torch.Tensor`，*必需*）-输入序列到模型'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask`（形状为`(batch_size, sequence_length, num_input_channels)`的`torch.BoolTensor`，*可选*）-布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`中：'
- en: 1 for values that are `observed`,
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示值是`observed`，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`missing`的值（即被零替换的NaN）为0。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）-是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）-是否返回所有层的输出注意力'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）-是否返回`ModelOutput`而不是普通元组。'
- en: 'Examples:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: PatchTSTForRegression
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PatchTSTForRegression
- en: '### `class transformers.PatchTSTForRegression`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.PatchTSTForRegression`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1884)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1884)'
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`（[PatchTSTConfig](/docs/transformers/v4.37.2/en/model_doc/patchtst#transformers.PatchTSTConfig））-模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The PatchTST for regression model. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 用于回归模型的PatchTST。此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存，调整输入嵌入，修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以了解所有与一般使用和行为相关的事项。
- en: '#### `forward`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1915)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/patchtst/modeling_patchtst.py#L1915)'
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — Input sequence to the model'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_values` (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`,
    *required*) — 输入模型的序列'
- en: '`target_values` (`torch.Tensor` of shape `(bs, num_input_channels)`) — Target
    values associates with the `past_values`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_values` (`torch.Tensor` of shape `(bs, num_input_channels)`) — 与`past_values`相关联的目标值'
- en: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — Boolean mask to indicate which `past_values`
    were observed and which were missing. Mask values selected in `[0, 1]`:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`past_observed_mask` (`torch.BoolTensor` of shape `(batch_size, sequence_length,
    num_input_channels)`, *optional*) — 布尔掩码，指示哪些`past_values`是观察到的，哪些是缺失的。掩码值选在`[0,
    1]`之间：'
- en: 1 for values that are `observed`,
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示观察到的值，
- en: 0 for values that are `missing` (i.e. NaNs that were replaced by zeros).
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示缺失的值（即被零替换的NaN）。
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the output
    attention of all layers'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有层的输出注意力'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a `ModelOutput`
    instead of a plain tuple.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个`ModelOutput`而不是一个普通的元组。'
- en: 'Examples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
