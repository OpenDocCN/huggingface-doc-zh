# 低精度训练方法

> 原始文本：[`huggingface.co/docs/accelerate/concept_guides/low_precision_training`](https://huggingface.co/docs/accelerate/concept_guides/low_precision_training)

新型硬件的发布导致了新的训练范式的出现，更好地利用它们。目前，这以使用诸如 [TransformersEngine](https://github.com/NVIDIA/TransformerEngine)（TE）或 [MS-AMP](https://github.com/Azure/MS-AMP/tree/main) 等软件包进行 8 位精度训练的形式存在。

对于今天讨论的主题的介绍，我们建议查看 低精度使用指南，因为本文档将经常引用它。

## 快速图表

以下是 MS-AMP 文档中显示训练过程中每个解决方案的不同位精度的快速图表：

| 优化级别 | 计算（GEMM） | 通信 | 权重 | 主权重 | 权重梯度 | 优化器状态 |
| --- | --- | --- | --- | --- | --- | --- |
| FP16 AMP | FP16 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |
| Nvidia TE | FP8 | FP32 | FP32 | N/A | FP32 | FP32+FP32 |
| MS-AMP O1 | FP8 | FP8 | FP16 | N/A | FP8 | FP32+FP32 |
| MS-AMP O2 | FP8 | FP8 | FP16 | N/A | FP8 | FP8+FP16 |
| MS-AMP O3 | FP8 | FP8 | FP8 | FP16 | FP8 | FP8+FP16 |

## TransformersEngine

`TransformersEngine` 是尝试在 8 位浮点数中进行训练的第一个解决方案。它通过使用模型中某些层的替换层来工作，这些替换层利用它们的 FP8 引擎来减少位数（例如从 32 到 8），而不会降低模型的最终准确性。

具体来说，🤗 Accelerate 将使用 `TransformersEngine` 版本找到并替换以下层：

+   `nn.LayerNorm` 替换为 `te.LayerNorm`

+   `nn.Linear` 替换为 `te.Linear`

因此，我们最终得到一个模型，其中大部分层都是 BF16，而一些层是 FP8，从而减少了一些内存。

据我们观察，当模型中的大多数层由这两个层组成以进行替换时，使用 `TransformerEngine` 才会开始显示性能提升。因此，只有较大的模型在参数数量约为几十亿及以上时才显示出性能改进。

`TransformerEngine` 可以接收许多不同的参数，定制它如何执行 FP8 计算以及它们的功能。下面是参数的完整列表：

+   `margin`：用于梯度缩放的边距。

+   `interval`：用于重新计算缩放因子的间隔。

+   `fp8_format`：用于 FP8 配方的格式。必须是`E4M3`或`HYBRID`之一。

+   `amax_history_len`：用于缩放因子计算的历史长度

+   `amax_compute_algo`：用于缩放因子计算的算法。必须是`max`或`most_recent`之一。

+   `override_linear_precision`：是否在更高精度中执行 `fprop`、`dgrad` 和 `wgrad` GEMMS。

您可以将这些参数作为 utils.FP8RecipeKwargs 的一部分进行定制，以帮助优化模型的性能。

如果我们注意到前面提到的图表，TE 简单地将计算层转换为 FP8，而其他所有内容都是 FP32。因此，这最终利用了大部分内存，但在训练过程中保证了最少的最终准确性损失。

## MS-AMP

MS-AMP 通过提供三种不同的优化级别来采用不同的方法来使用 `TransformersEngine`，以将更多操作转换为 FP8 或 FP16。

+   基本优化级别（`O1`）将权重的通信（如在 DDP 中）传递为 FP8，在 FP16 中存储模型的权重，并将优化器状态保留在 FP32 中。这种优化级别的主要好处是我们可以将通信带宽减少一半。此外，由于一半的内容都是 FP8，权重被转换为 FP16，因此可以节省更多的 GPU 内存。值得注意的是，优化器状态仍保留在 FP32 中。

+   第二个优化级别（`O2`）通过降低优化器状态的精度来改进。一个是 FP8，另一个是 FP16。通常已经表明，这只会提供一个净增益，不会降低最终的准确性，提高训练速度，并减少内存，因为现在每个状态都是 FP16 或 FP8。

+   最后，MS-AMP 有第三个优化级别（`O3`），在 DDP 场景（如 DeepSpeed）中有所帮助。模型的权重在内存中完全转换为 FP8，主权重现在存储在 FP16 中。这样可以通过最高因子完全减少内存，因为现在几乎所有内容都是 FP8，只有两个状态留在 FP16 中。目前，只支持 DeepSpeed 版本 0.9.2 及以下，因此这种能力不包括在🤗 Accelerate 集成中。

## 结合两者

需要进行更多实验，但已经注意到，结合 MS-AMP 和 TransformersEngine 可以依靠 NVIDIA 优化的 FP8 运算符以及利用 MS-AMP 减少内存开销来实现最高吞吐量。
