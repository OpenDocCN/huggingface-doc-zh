["```py\napt install swig cmake\n```", "```py\npip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt\n```", "```py\nsudo apt-get update\napt install python-opengl\napt install ffmpeg\napt install xvfb\npip3 install pyvirtualdisplay\n```", "```py\nimport os\n\nos.kill(os.getpid(), 9)\n```", "```py\n# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()\n```", "```py\nimport gymnasium\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\nfrom huggingface_hub import (\n    notebook_login,\n)  # To log to our Hugging Face account to be able to upload models to the Hub.\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.monitor import Monitor\n```", "```py\nimport gymnasium as gym\n\n# First, we create our environment called LunarLander-v2\nenv = gym.make(\"LunarLander-v2\")\n\n# Then we reset this environment\nobservation, info = env.reset()\n\nfor _ in range(20):\n    # Take a random action\n    action = env.action_space.sample()\n    print(\"Action taken:\", action)\n\n    # Do this action in the environment and get\n    # next_state, reward, terminated, truncated and info\n    observation, reward, terminated, truncated, info = env.step(action)\n\n    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n    if terminated or truncated:\n        # Reset the environment\n        print(\"Environment is reset\")\n        observation, info = env.reset()\n\nenv.close()\n```", "```py\n# We create our environment with gym.make(\"<name_of_the_environment>\")\nenv = gym.make(\"LunarLander-v2\")\nenv.reset()\nprint(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"Observation Space Shape\", env.observation_space.shape)\nprint(\"Sample observation\", env.observation_space.sample())  # Get a random observation\n```", "```py\nprint(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"Action Space Shape\", env.action_space.n)\nprint(\"Action Space Sample\", env.action_space.sample())  # Take a random action\n```", "```py\n# Create the environment\nenv = make_vec_env(\"LunarLander-v2\", n_envs=16)\n```", "```py\n# Create environment\nenv = gym.make('LunarLander-v2')\n\n# Instantiate the agent\nmodel = PPO('MlpPolicy', env, verbose=1)\n# Train the agent\nmodel.learn(total_timesteps=int(2e5))\n```", "```py\n# TODO: Define a PPO MlpPolicy architecture\n# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n# if we had frames as input we would use CnnPolicy\nmodel =\n```", "```py\n# SOLUTION\n# We added some parameters to accelerate the training\nmodel = PPO(\n    policy=\"MlpPolicy\",\n    env=env,\n    n_steps=1024,\n    batch_size=64,\n    n_epochs=4,\n    gamma=0.999,\n    gae_lambda=0.98,\n    ent_coef=0.01,\n    verbose=1,\n)\n```", "```py\n# TODO: Train it for 1,000,000 timesteps\n\n# TODO: Specify file name for model and save the model to file\nmodel_name = \"ppo-LunarLander-v2\"\n```", "```py\n# SOLUTION\n# Train it for 1,000,000 timesteps\nmodel.learn(total_timesteps=1000000)\n# Save the model\nmodel_name = \"ppo-LunarLander-v2\"\nmodel.save(model_name)\n```", "```py\n# TODO: Evaluate the agent\n# Create a new environment for evaluation\neval_env =\n\n# Evaluate the model with 10 evaluation episodes and deterministic=True\nmean_reward, std_reward = \n\n# Print the results\n```", "```py\n# @title\neval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```", "```py\nnotebook_login()\n!git config --global credential.helper store\n```", "```py\nimport gymnasium as gym\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n## TODO: Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\nrepo_id = \n\n# TODO: Define the name of the environment\nenv_id = \n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"\"\n\n## TODO: Define the commit message\ncommit_message = \"\"\n\n# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\npackage_to_hub(model=model, # Our trained model\n               model_name=model_name, # The name of our trained model \n               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n               env_id=env_id, # Name of the environment\n               eval_env=eval_env, # Evaluation Environment\n               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n               commit_message=commit_message)\n```", "```py\nimport gymnasium as gym\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_sb3 import package_to_hub\n\n# PLACE the variables you've just defined two cells above\n# Define the name of the environment\nenv_id = \"LunarLander-v2\"\n\n# TODO: Define the model architecture we used\nmodel_architecture = \"PPO\"\n\n## Define a repo_id\n## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n## CHANGE WITH YOUR REPO ID\nrepo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine \ud83d\ude04\n\n## Define the commit message\ncommit_message = \"Upload PPO LunarLander-v2 trained agent\"\n\n# Create the evaluation env and set the render_mode=\"rgb_array\"\neval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n\n# PLACE the package_to_hub function you've just filled here\npackage_to_hub(\n    model=model,  # Our trained model\n    model_name=model_name,  # The name of our trained model\n    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n    env_id=env_id,  # Name of the environment\n    eval_env=eval_env,  # Evaluation Environment\n    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n    commit_message=commit_message,\n)\n```", "```py\n!pip install shimmy\n```", "```py\nfrom huggingface_sb3 import load_from_hub\n\nrepo_id = \"Classroom-workshop/assignment2-omar\"  # The repo_id\nfilename = \"ppo-LunarLander-v2.zip\"  # The model filename.zip\n\n# When the model was trained on Python 3.8 the pickle protocol is 5\n# But Python 3.6, 3.7 use protocol 4\n# In order to get compatibility we need to:\n# 1\\. Install pickle5 (we done it at the beginning of the colab)\n# 2\\. Create a custom empty object we pass as parameter to PPO.load()\ncustom_objects = {\n    \"learning_rate\": 0.0,\n    \"lr_schedule\": lambda _: 0.0,\n    \"clip_range\": lambda _: 0.0,\n}\n\ncheckpoint = load_from_hub(repo_id, filename)\nmodel = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\n```", "```py\n# @title\neval_env = Monitor(gym.make(\"LunarLander-v2\"))\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\nprint(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n```"]