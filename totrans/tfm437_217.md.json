["```py\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\ntar -xvf 8b_base_model_release.tar\npython src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```", "```py\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```", "```py\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\n\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\n```", "```py\n( vocab_size = 262144 hidden_size = 4096 intermediate_size = 16384 num_hidden_layers = 36 num_attention_heads = 64 hidden_act = 'relu2' max_position_embeddings = 16384 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True tie_word_embeddings = False rope_theta = 25000.0 rope_scaling = None qk_layernorm = True hidden_dropout = 0.0 attention_dropout = 0.0 partial_rotary_factor = 0.5 pad_token_id = None bos_token_id = 1 eos_token_id = 2 **kwargs )\n```", "```py\n>>> from transformers import PersimmonModel, PersimmonConfig\n\n>>> # Initializing a Persimmon persimmon-7b style configuration\n>>> configuration = PersimmonConfig()\n```", "```py\n( config: PersimmonConfig )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, PersimmonForCausalLM\n\n>>> model = PersimmonForCausalLM.from_pretrained(\"adept/persimmon-8b-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"adept/persimmon-8b-base\")\n\n>>> prompt = \"human: Hey, what should I eat for dinner?\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'human: Hey, what should I eat for dinner?\\n\\ncat: \ud83d\udc31\\n\\nhuman: \ud83d\ude10\\n\\n'\n```", "```py\n( config )\n```", "```py\n( input_ids: LongTensor = None attention_mask: Optional = None position_ids: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )\n```"]