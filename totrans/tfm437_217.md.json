["```py\ngit clone https://github.com/persimmon-ai-labs/adept-inference\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\ntar -xvf 8b_base_model_release.tar\npython src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\n    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt\n    --ada_lib_path /path/to/adept-inference\n```", "```py\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\ntar -xvf 8b_base_model_release.tar\n```", "```py\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\n\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\n```", "```py\n>>> from transformers import PersimmonModel, PersimmonConfig\n\n>>> # Initializing a Persimmon persimmon-7b style configuration\n>>> configuration = PersimmonConfig()\n```", "```py\n>>> from transformers import AutoTokenizer, PersimmonForCausalLM\n\n>>> model = PersimmonForCausalLM.from_pretrained(\"adept/persimmon-8b-base\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"adept/persimmon-8b-base\")\n\n>>> prompt = \"human: Hey, what should I eat for dinner?\"\n>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n>>> # Generate\n>>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n>>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n'human: Hey, what should I eat for dinner?\\n\\ncat: \ud83d\udc31\\n\\nhuman: \ud83d\ude10\\n\\n'\n```"]