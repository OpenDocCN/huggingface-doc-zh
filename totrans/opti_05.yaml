- en: ðŸ¤— Optimum notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/optimum/notebooks](https://huggingface.co/docs/optimum/notebooks)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: You can find here a list of the notebooks associated with each accelerator in
    ðŸ¤— Optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Optimum Habana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Notebook | Description | Colab | Studio Lab |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| [How to use DeepSpeed to train models with billions of parameters on Habana
    Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL
    for causal language modeling on Habana Gaudi. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: Optimum Intel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenVINO
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Notebook | Description | Colab | Studio Lab |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | Explains how to export your model to OpenVINO and run inference with OpenVINO
    Runtime on various tasks | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | Show how to apply post-training quantization on a question answering model using
    [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with
    OpenVINO | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [Compare outputs of a quantized Stable Diffusion model with its full-precision
    counterpart](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | Show how to load and compare outputs from two Stable Diffusion models with different
    precision | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_quantization.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: Neural Compressor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Notebook | Description | Colab | Studio Lab |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | Show how to apply quantization while training your model using Intel [Neural
    Compressor](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open
    in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: Optimum ONNX Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Notebook | Description | Colab | Studio Lab |'
  prefs: []
  type: TYPE_TB
- en: '| :-- | :-- | :-- | --: |'
  prefs: []
  type: TYPE_TB
- en: '| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime)
    for any GLUE task. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [How to fine-tune a model for text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | Show how to DistilBERT model on GLUE tasks using [ONNX Runtime](https://github.com/microsoft/onnxruntime).
    | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [How to fine-tune a model for summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | Show how to fine-tune a T5 model on the BBC news corpus. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | Show how to fine-tune a DeBERTa model on the squad. | [![Open in Colab](../Images/7e2db436150c38a00650f96925aa5581.png)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    | [![Open in AWS Studio](../Images/b853c984b1efccec36ff5b904fac75b9.png)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering_ort.ipynb)
    |'
  prefs: []
  type: TYPE_TB
