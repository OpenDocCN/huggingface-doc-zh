# 迭代训练器

> 原始文本：[https://huggingface.co/docs/trl/iterative_sft_trainer](https://huggingface.co/docs/trl/iterative_sft_trainer)

迭代微调是一种训练方法，可以在优化步骤之间执行自定义操作（例如生成和过滤）。在 TRL 中，我们提供了一个易于使用的 API，可以仅用几行代码迭代微调您的模型。

## 用法

要快速开始，请实例化一个模型和一个分词器。

```py

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

trainer = IterativeSFTTrainer(
    model,
    tokenizer
)

```

您可以选择向 step 函数提供字符串列表或张量列表。

#### 使用张量列表作为输入:

```py

inputs = {
    "input_ids": input_ids,
    "attention_mask": attention_mask
}

trainer.step(**inputs)

```

#### 使用字符串列表作为输入：

```py

inputs = {
    "texts": texts
}

trainer.step(**inputs)

```

对于因果语言模型，标签将自动从 input_ids 或文本创建。当使用序列到序列模型时，您将需要提供自己的标签或文本标签。

## IterativeTrainer

### `class trl.IterativeSFTTrainer`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L39)

```py
( model: PreTrainedModel = None args: TrainingArguments = None tokenizer: PreTrainedTokenizerBase = None optimizers: Tuple = (None, None) data_collator: Optional = None eval_dataset: Union = None max_length: Optional = None truncation_mode: Optional = 'keep_end' preprocess_logits_for_metrics: Optional = None compute_metrics: Optional = None optimize_device_cache: Optional = False )
```

参数

+   *`*model**` (`PreTrainedModel`) — 要优化的模型，可以是 ‘AutoModelForCausalLM’ 或 ‘AutoModelForSeq2SeqLM’。 — 查看 `PreTrainedModel` 的文档以获取更多详细信息。

+   *`*args**` (`transformers.TrainingArguments`) — 用于训练的参数。

+   *`*tokenizer**` (`PreTrainedTokenizerBase`) — 用于编码数据的分词器。查看 `transformers.PreTrainedTokenizer` 和 `transformers.PreTrainedTokenizerFast` 的文档以获取更多详细信息。

+   *`*optimizers**` (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`) — — 用于训练的优化器和调度器。

+   *`*data_collator**` (Union[DataCollatorForLanguageModeling, DataCollatorForSeq2Seq], *optional*) — 用于训练和传递给数据加载器的数据整合器。

+   *`*eval_dataset**` (`datasets.Dataset`) — 用于评估的数据集。

+   *`*max_length**` (`int`, 默认为 `None`) — 输入的最大长度。

+   *`*truncation_mode**` (`str`, 默认为 `keep_end`) — 使用的截断模式，可以是 `keep_end` 或 `keep_start`。

+   *`*preprocess_logits_for_metrics**` (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`) — 用于在计算指标之前预处理对数的函数。

+   *`*compute_metrics**` (`Callable[[EvalPrediction], Dict]`, *optional*) — 用于计算指标的函数。必须接受 `EvalPrediction` 并返回一个字符串到指标值的字典。

+   *`*optimize_device_cache` * *(`bool`,* optional*, 默认为 `False`) — 优化 CUDA 缓存以实现略微更高效的内存训练。 —

IterativeSFTTrainer 可用于微调需要在优化之间执行一些步骤的模型。

#### `step`

[< source >](https://github.com/huggingface/trl/blob/v0.7.10/trl/trainer/iterative_sft_trainer.py#L229)

```py
( input_ids: Optional = None attention_mask: Optional = None labels: Optional = None texts: Optional = None texts_labels: Optional = None ) → export const metadata = 'undefined';dict[str, Any]
```

参数

+   `input_ids` (List`torch.LongTensor`) — 包含 input_ids 的张量列表（如果未提供，将使用文本）

+   `attention_mask` (List`torch.LongTensor`, , *optional*) — 包含 attention_mask 的张量列表

+   `labels` (List`torch.FloatTensor`, *optional*) — 包含标签的张量列表（如果设置为 None，将默认为 input_ids）

+   `texts` (List`str`, *optional*) — 包含文本输入的字符串列表（如果未提供，则将直接使用input_ids）

+   `texts_labels` (List`str`, *optional*) — 包含文本标签的字符串列表（如果设置为 None，将默认为文本）

返回

`dict[str, Any]`

训练统计摘要

给定一组 input_ids、attention_mask 和 labels 或一组文本和 text_labels，运行一个优化步骤。
