- en: Utilities for Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils](https://huggingface.co/docs/transformers/v4.37.2/en/internal/generation_utils)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/27.7d3059af.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: This page lists all the utility functions used by [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    [greedy_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search),
    [contrastive_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search),
    [sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.sample),
    [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search),
    [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample),
    [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search),
    and [constrained_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search).
  prefs: []
  type: TYPE_NORMAL
- en: Most of those are only useful if you are studying the code of the generate methods
    in the library.
  prefs: []
  type: TYPE_NORMAL
- en: Generate Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output of [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)
    is an instance of a subclass of [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput).
    This output is a data structure containing all the information returned by [generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate),
    but that can also be used as tuple or dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `generation_output` object is a [GenerateDecoderOnlyOutput](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput),
    as we can see in the documentation of that class below, it means it has the following
    attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences`: the generated sequences of tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (optional): the prediction scores of the language modelling head,
    for each generation step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (optional): the hidden states of the model, for each generation
    step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (optional): the attention weights of the model, for each generation
    step'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we have the `scores` since we passed along `output_scores=True`, but we
    don’t have `hidden_states` and `attentions` because we didn’t pass `output_hidden_states=True`
    or `output_attentions=True`.
  prefs: []
  type: TYPE_NORMAL
- en: You can access each attribute as you would usually do, and if that attribute
    has not been returned by the model, you will get `None`. Here for instance `generation_output.scores`
    are all the generated prediction scores of the language modeling head, and `generation_output.attentions`
    is `None`.
  prefs: []
  type: TYPE_NORMAL
- en: When using our `generation_output` object as a tuple, it only keeps the attributes
    that don’t have `None` values. Here, for instance, it has two elements, `loss`
    then `logits`, so
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: will return the tuple `(generation_output.sequences, generation_output.scores)`
    for instance.
  prefs: []
  type: TYPE_NORMAL
- en: When using our `generation_output` object as a dictionary, it only keeps the
    attributes that don’t have `None` values. Here, for instance, it has two keys
    that are `sequences` and `scores`.
  prefs: []
  type: TYPE_NORMAL
- en: We document here all output types.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.generation.GenerateDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L96)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs of decoder-only generation models, when using non-beam methods.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.GenerateEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs of encoder-decider generation models, when using non-beam methods.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.GenerateBeamDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L178)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam transition scores for each
    vocabulary token at each generation step. Beam transition scores consisting of
    log probabilities of tokens conditioned on log softmax of previously generated
    tokens in this beam. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`torch.LongTensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads,
    generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs of decoder-only generation models, when using beam methods.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.GenerateBeamEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`) — The generated sequences. The second dimension (sequence_length)
    is either equal to `max_length` or shorter if all batches finished early due to
    the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam transition scores for each
    vocabulary token at each generation step. Beam transition scores consisting of
    log probabilities of tokens conditioned on log softmax of previously generated
    tokens in this beam. Tuple of `torch.FloatTensor` with up to `max_new_tokens`
    elements (one element for each generated token), with each tensor of shape `(batch_size*num_beams,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`torch.LongTensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `torch.LongTensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size,
    num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for
    the output of each layer) of shape `(batch_size*num_beams*num_return_sequences,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_attentions=True` is passed or `config.output_attentions=True`) —
    Tuple (one element for each generated token) of tuples (one element for each layer
    of the decoder) of `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — NOTE: some
    models have a different `past_key_values` format, confirm with the model’s documentation.
    Usually a Tuple (one element for each layer of the decoder) of tuples (two elements,
    key tensor and value tensor). The first Tuple is of length `config.n_layers`,
    with each tuple having 2 tensors of shape `(batch_size, num_heads, sequence_length,
    embed_size_per_head)`) and optionally if `config.is_encoder_decoder=True` 2 additional
    tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outputs of encoder-decoder generation models, when using beam methods.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFGreedySearchEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L85)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of encoder-decoder generation models using greedy search.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFGreedySearchDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of decoder-only generation models using greedy search.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFSampleEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L155)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_return_sequences,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size*num_return_sequences, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_return_sequences, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_return_sequences, num_heads,
    generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_return_sequences,
    generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of encoder-decoder generation models using sampling.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFSampleDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L127)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_return_sequences,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(num_return_sequences*batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(num_return_sequences*batch_size, generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of decoder-only generation models using sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFBeamSearchEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L232)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. `Tuple of` tf.Tensor`with up to`max_new_tokens`elements (one
    element for each generated token), with each tensor of shape`(batch_size*num_beams,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences,
    num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences,
    generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of encoder-decoder generation models using beam search.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFBeamSearchDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L197)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size*num_return_sequences)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of decoder-only generation models using beam search.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFBeamSampleEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L317)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_beams, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size * num_return_sequence)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size*num_beams, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size*num_beams, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size*num_beams, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of encoder-decoder generation models using beam sampling.
    Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFBeamSampleDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L282)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)`)
    — The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequences_scores` (`tf.Tensor` of shape `(batch_size * num_return_sequence)`,
    *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`)
    — Final beam scores of the generated `sequences`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed beam scores for each
    vocabulary token at each generation step. Beam scores consisting of log softmax
    scores for each vocabulary token and sum of log softmax of previously generated
    tokens in this beam. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size*num_beams*num_return_sequences,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`tf.Tensor`, *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Beam indices of generated token
    id at each generation step. `tf.Tensor` of shape `(batch_size*num_return_sequences,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size*num_beams, generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of decoder-only generation models using beam sample.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFContrastiveSearchEncoderDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L393)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple of `tf.Tensor` (one for
    each layer of the decoder) of shape `(batch_size, num_heads, sequence_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_attentions=True` is passed or `config.output_attentions=True`) — Tuple
    (one element for each generated token) of tuples (one element for each layer of
    the decoder) of `tf.Tensor` of shape `(batch_size, num_heads, generated_length,
    sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple (one element for each generated token) of tuples (one element for each
    layer of the decoder) of `tf.Tensor` of shape `(batch_size, generated_length,
    hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of encoder-decoder generation models using contrastive
    search. Hidden states and attention weights of the decoder (respectively the encoder)
    can be accessed via the encoder_attentions and the encoder_hidden_states attributes
    (respectively the decoder_attentions and the decoder_hidden_states attributes)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.TFContrastiveSearchDecoderOnlyOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L366)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — The generated
    sequences. The second dimension (sequence_length) is either equal to `max_length`
    or shorter if all batches finished early due to the `eos_token_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`tuple(tf.Tensor)` *optional*, returned when `output_scores=True`
    is passed or when `config.output_scores=True`) — Processed prediction scores of
    the language modeling head (scores for each vocabulary token before SoftMax) at
    each generation step. Tuple of `tf.Tensor` with up to `max_new_tokens` elements
    (one element for each generated token), with each tensor of shape `(batch_size,
    config.vocab_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_attentions=True`
    is passed or `config.output_attentions=True`) — Tuple (one element for each generated
    token) of tuples (one element for each layer of the decoder) of `tf.Tensor` of
    shape `(batch_size, num_heads, generated_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tuple(tf.Tensor))`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple (one element for
    each generated token) of tuples (one element for each layer of the decoder) of
    `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Base class for outputs of decoder-only generation models using contrastive search.
  prefs: []
  type: TYPE_NORMAL
- en: FLAX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.generation.FlaxSampleOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L68)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax Base class for outputs of decoder-only generation models using sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: “Returns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.FlaxGreedySearchOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax Base class for outputs of decoder-only generation models using greedy search.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: “Returns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.generation.FlaxBeamSearchOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_utils.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` (`jnp.ndarray` of shape `(batch_size, max_length)`) — The generated
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`jnp.ndarray` of shape `(batch_size,)`) — The scores (log probabilities)
    of the generated sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax Base class for outputs of decoder-only generation models using greedy search.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: “Returns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
- en: LogitsProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    can be used to modify the prediction scores of a language model head for generation.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.AlternatingCodebooksLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2011)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_start_len` (`int`) — The length of the initial input sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semantic_vocab_size` (`int`) — Vocabulary size of the semantic part, i.e number
    of tokens associated to the semantic vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codebook_size` (`int`) — Number of tokens associated to the codebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing alternated generation between the two codebooks of Bark.'
  prefs: []
  type: TYPE_NORMAL
- en: This logits processor is exclusively compatible with [Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)’s
    fine submodel. See the model documentation for examples.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2040)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.ClassifierFreeGuidanceLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1947)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`guidance_scale` (float) — The guidance scale for classifier free guidance
    (CFG). CFG is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages
    the model to generate samples that are more closely linked to the input prompt,
    usually at the expense of poorer quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    for classifier free guidance (CFG). The scores are split over the batch dimension,
    where the first half correspond to the conditional logits (predicted from the
    input prompt) and the second half correspond to the unconditional logits (predicted
    from an empty or ‘null’ prompt). The processor computes a weighted average across
    the conditional and unconditional logits, parameterised by the `guidance_scale`.'
  prefs: []
  type: TYPE_NORMAL
- en: See [the paper](https://arxiv.org/abs/2306.05284) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: This logits processor is exclusively compatible with [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1995)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.EncoderNoRepeatNGramLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L874)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`encoder_ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur
    within the encoder input ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_input_ids` (`int`) — The encoder_input_ids that should not be repeated
    within the decoder ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that works similarly to [NoRepeatNGramLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.NoRepeatNGramLogitsProcessor),
    but applied exclusively to prevent the repetition of n-grams present in the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: It was designed to promote chattiness in a language model, by preventing the
    generation of n-grams present in previous conversation rounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L923)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.EncoderRepetitionPenaltyLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L342)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`penalty` (`float`) — The parameter for repetition penalty. 1.0 means no penalty.
    Above 1.0 rewards prompt tokens. Between 0.0 and 1.0 penalizes prompt tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_input_ids` (`torch.LongTensor`) — The encoder_input_ids that should
    be repeated within the decoder ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that works similarly to [RepetitionPenaltyLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.RepetitionPenaltyLogitsProcessor),
    but with an *inverse* penalty that is applied to the tokens present in the prompt.
    In other words, a penalty above 1.0 increases the odds of selecting tokens that
    were present in the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: It was designed to avoid hallucination in input-grounded tasks, like summarization.
    Although originally intended for encoder-decoder models, it can also be used with
    decoder-only models like LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L386)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.EpsilonLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L603)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon` (`float`) — If set to > 0, only the most tokens with probabilities
    `epsilon` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs epsilon-sampling, i.e. restricting to tokens with `prob >= epsilon`.
    Takes the largest min_tokens_to_keep tokens if no tokens satisfy this constraint.
    See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L656)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.EtaLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L670)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`epsilon` (`float`) — A float value in the range (0, 1). Hyperparameter used
    to calculate the dynamic cutoff value, `eta`. The suggested values from the paper
    ranges from 3e-4 to 4e-3 depending on the size of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All values that are
    found to be below the dynamic cutoff value, `eta`, are set to this float value.
    This parameter is useful when logits need to be modified for very low probability
    tokens that should be excluded from generation entirely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Specifies the minimum
    number of tokens that must be kept for generation, regardless of their probabilities.
    For example, if `min_tokens_to_keep` is set to 1, at least one token will always
    be kept for generation, even if all tokens have probabilities below the cutoff
    `eta`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs eta-sampling, a technique to filter out tokens with probabilities
    below a dynamic cutoff value, `eta`, which is calculated based on a combination
    of the hyperparameter `epsilon` and the entropy of the token probabilities, i.e.
    `eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`. Takes the largest
    min_tokens_to_keep tokens if no tokens satisfy this constraint. It addresses the
    issue of poor quality in long samples of text generated by neural language models
    leading to more coherent and fluent text. See [Truncation Sampling as Language
    Model Desmoothing](https://arxiv.org/abs/2210.15191) for more information. Note:
    `do_sample` must be set to `True` for this `LogitsWarper` to work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L733)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ExponentialDecayLengthPenalty`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1494)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`exponential_decay_length_penalty` (`tuple(int, float)`) — This tuple shall
    consist of: `(start_index, decay_factor)` where `start_index` indicates where
    penalty starts and `decay_factor` represents the factor of exponential decay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_ids_seq_length` (`int`) — The length of the input sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that exponentially increases the score of the `eos_token_id` after `start_index`
    has been reached. This allows generating shorter sequences without having a hard
    cutoff, allowing the `eos_token` to be predicted in a meaningful position.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1574)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ForcedBOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1378)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces the specified token as the first generated token. Used with encoder-decoder
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1413)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ForcedEOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1423)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the token to force as
    the last generated token when `max_length` is reached. Optionally, use a list
    to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1462)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ForceTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1710)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This processor takes a list of pairs of integers which indicates a mapping from
    generation indices to token indices that will be forced before generation. The
    processor will set their log probs to `inf` so that they are sampled at their
    corresponding index. Originally created for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1751)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.HammingDiversityLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1245)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`diversity_penalty` (`float`) — This value is subtracted from a beam’s score
    if it generates a token same as any beam from other group at a particular time.
    A higher `diversity_penalty` will enforce greater diversity among the beams. Adjusting
    this value can help strike a balance between diversity and natural likelihood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`) — Number of beams for beam search. 1 means no beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_groups` (`int`) — Number of groups to divide `num_beams` into in
    order to ensure diversity among different groups of beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces diverse beam search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this logits processor is only effective for [PreTrainedModel.group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search).
    See [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)
    for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional beam search often generates very similar sequences across different
    beams. `HammingDiversityLogitsProcessor` addresses this by penalizing beams that
    generate tokens already chosen by other beams in the same time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1332)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`current_tokens` (`torch.LongTensor` of shape `(batch_size)`) — Indices of
    input sequence tokens in the vocabulary, corresponding to the tokens selected
    by the other beam groups in the current generation step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_group_idx` (`int`) — The index of the beam group currently being processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.InfNanRemoveLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1473)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that removes all `nan` and `inf` values to avoid the generation method to fail.
    Note that using the logits processor should only be used if necessary since it
    can slow down the generation method.'
  prefs: []
  type: TYPE_NORMAL
- en: This logits processor has no `generate` example, as there shouldn’t be a correct
    combination of flags that warrants its use.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1482)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.LogitNormalization`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1585)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    and [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    for normalizing the scores using log-softmax. It’s important to normalize the
    scores during beam search, after applying the logits processors or warpers, since
    the search algorithm used in this library doesn’t do it (it only does it before,
    but they may need re-normalization) but it still supposes that the scores are
    normalized when comparing the hypotheses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1616)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.LogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit processors that can be applied during generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L47)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.LogitsProcessorList`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: This class can be used to create a list of [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    or [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    or [LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L71)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional kwargs that are specific
    to a logits processor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.LogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L54)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.MinLengthLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L102)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing a min-length by setting EOS probability to 0\. Note that, for decoder-only
    models like most LLMs, the length includes the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L151)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.MinNewTokensLengthLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L160)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt_length_to_skip` (`int`) — The input tokens length. Not a valid argument
    when used with `generate` as it will automatically assign the input length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_new_tokens` (`int`) — The minimum *new* tokens length below which the
    score of `eos_token_id` is set to `-float("Inf")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforcing a min-length of new tokens by setting EOS (End-Of-Sequence) token probability
    to 0. Contrarily to [MinLengthLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.MinLengthLogitsProcessor),
    this processor ignores the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L212)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.NoBadWordsLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1090)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bad_words_ids` (`List[List[int]]`) — List of list of token ids that are not
    allowed to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces that specified sequences will never be selected.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the token ids of the words that should not appear in the generated
    text, make sure to set `add_prefix_space=True` when initializing the tokenizer,
    and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The `add_prefix_space`
    argument is only supported for some slow tokenizers, as fast tokenizers’ prefixing
    behaviours come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.NoRepeatNGramLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L816)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'N-grams are groups of “n” consecutive words, characters, or tokens taken from
    a sequence of text. Given the sentence: “She runs fast”, the bi-grams (n=2) would
    be (“she”, “runs”) and (“runs”, “fast”). In text generation, avoiding repetitions
    of word sequences provides a more diverse output. This [LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    enforces no repetition of n-grams by setting the scores of banned tokens to negative
    infinity which eliminates those tokens from consideration when further processing
    the scores. Note that, for decoder-only models like most LLMs, the prompt is also
    considered to obtain the n-grams. [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).'
  prefs: []
  type: TYPE_NORMAL
- en: Use n-gram penalties with care. For instance, penalizing 2-grams (bigrams) in
    an article about the city of New York might lead to undesirable outcomes where
    the city’s name appears only once in the entire text. [Reference](https://huggingface.co/blog/how-to-generate)
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L863)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.PrefixConstrainedLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1177)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prefix_allowed_tokens_fn` (`Callable[[int, torch.Tensor], List[int]]`) — This
    function constraints the beam search to allowed tokens only at each step. This
    function takes 2 arguments `inputs_ids` and the batch ID `batch_id`. It has to
    return a list with the allowed tokens for the next generation step conditioned
    on the previously generated tokens `inputs_ids` and the batch ID `batch_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that enforces constrained generation and is useful for prefix-conditioned constrained
    generation. See [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1228)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.RepetitionPenaltyLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L288)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`penalty` (`float`) — The parameter for repetition penalty. 1.0 means no penalty.
    Above 1.0 penalizes previously generated tokens. Between 0.0 and 1.0 rewards previously
    generated tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that prevents the repetition of previous tokens through a penalty. This penalty
    is applied at most once per token. Note that, for decoder-only models like most
    LLMs, the considered tokens include the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: In the original [paper](https://arxiv.org/pdf/1909.05858.pdf), the authors suggest
    the use of a penalty of around 1.2 to achieve a good balance between truthful
    generation and lack of repetition. To penalize and reduce repetition, use `penalty`
    values above 1.0, where a higher value penalizes more strongly. To reward and
    encourage repetition, use `penalty` values between 0.0 and 1.0, where a lower
    value rewards more strongly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L331)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.SequenceBiasLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L942)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence_bias` (`Dict[Tuple[int], float]`) — Dictionary that maps a sequence
    of tokens to its bias term. Positive biases increase the odds of the sequence
    being selected, while negative biases do the opposite. If a sequence has a length
    of 1, its bias will always be applied. Otherwise, the bias will only be applied
    if the sequence in question is about to be completed (in the token selection step
    after this processor is applied).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that applies an additive bias on sequences. The bias is applied to the last token
    of a sequence when the next generated token can complete it. Consequently, to
    take the most of biasing sequences with more than one token, consider using beam
    methods (to gracefully work around partially completed sequences that have a negative
    bias) and applying the bias to their prefixes (to ensure the bias is applied earlier).'
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the token ids of the sequences that you want to bias, make sure
    to set `add_prefix_space=True` when initializing the tokenizer, and use `tokenizer(bad_words,
    add_special_tokens=False).input_ids`. The `add_prefix_space` argument is only
    supported for some slow tokenizers, as fast tokenizers’ prefixing behaviours come
    from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1013)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.SuppressTokensAtBeginLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1622)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[SuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.SuppressTokensAtBeginLogitsProcessor)
    supresses a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    are not generated at the begining. Originally created for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1664)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.SuppressTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1672)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: This processor can be used to suppress a list of tokens. The processor will
    set their log probs to `-inf` so that they are not generated. Originally created
    for [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1704)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TemperatureLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L222)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature` (`float`) — Strictly positive float value used to modulate the
    logits distribution. A value smaller than `1` decreases randomness (and vice versa),
    with `0` being equivalent to shifting all probability mass to the most likely
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    for temperature (exponential scaling output probability distribution), which effectively
    means that it can control the randomness of the predicted tokens. Often used together
    with [TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper)
    and [TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper).'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that `do_sample=True` is included in the `generate` arguments otherwise
    the temperature value won’t have any effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L282)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TopKLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L462)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements. Often
    used together with [TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)
    and [TopPLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopPLogitsWarper).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L506)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TopPLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L397)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=
    prob_cut_off. Often used together with [TemperatureLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TemperatureLogitsWarper)
    and [TopKLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TopKLogitsWarper).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L446)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TypicalLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L515)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`mass` (`float`, *optional*, defaults to 0.9) — Value of typical_p between
    0 and 1 inclusive, defaults to 0.9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsWarper)
    that performs typical decoding. Inspired on how humans use language, it prioritizes
    tokens whose log probability is close to the entropy of the token probability
    distribution. This means that the most likely tokens may be discarded in the process.'
  prefs: []
  type: TYPE_NORMAL
- en: See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L579)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.UnbatchedClassifierFreeGuidanceLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2055)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`guidance_scale` (`float`) — The guidance scale for classifier free guidance
    (CFG). CFG is enabled by setting `guidance_scale != 1`. Higher guidance scale
    encourages the model to generate samples that are more closely linked to the input
    prompt, usually at the expense of poorer quality. A value smaller than 1 has the
    opposite effect, while making the negative prompt provided with negative_prompt_ids
    (if any) act as a positive prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` (`PreTrainedModel`) — The model computing the unconditional scores.
    Supposedly the same as the one computing the conditional scores. Both models must
    use the same tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unconditional_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of input sequence tokens in the vocabulary for the unconditional
    branch. If unset, will default to the last token of the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unconditional_attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Attention mask for unconditional_ids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether to cache key/values
    during the negative prompt forward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logits processor for Classifier-Free Guidance (CFG). The processors computes
    a weighted average across scores from prompt conditional and prompt unconditional
    (or negative) logits, parameterized by the `guidance_scale`. The unconditional
    scores are computed internally by prompting `model` with the `unconditional_ids`
    branch.
  prefs: []
  type: TYPE_NORMAL
- en: See [the paper](https://arxiv.org/abs/2306.17806) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L2161)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.WhisperTimeStampLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1761)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`generate_config` (`GenerateConfig`) — The generate config used to generate
    the output. The following parameters are required: eos_token_id (`int`, *optional*,
    defaults to 50257): The id of the *end-of-sequence* token. no_timestamps_token_id
    (`int`, *optional*, defaults to 50363): The id of the `"<|notimestamps|>"` token.
    max_initial_timestamp_index (`int`, *optional*, defaults to 1): Used to set the
    maximum value of the initial timestamp. This is used to prevent the model from
    predicting timestamps that are too far in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`begin_index` (`Optional`, *optional*) — Token index of the first token that
    is generated by the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_detect_timestamp_from_logprob` (`bool`, *optional*) — Whether timestamps
    can be predicted from logprobs over all timestamps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.LogitsProcessor)
    that modifies the logits for the generation of timestamps in the transcription.
    When the input tokens are at a specific threshold, the processor sets the scores
    to negative infinity. The processor makes sure that timestamp tokens appear in
    pairs, by masking out the logits that would break this pairing pattern. This is
    done to maintain the consistency and structure of generated timestamps. It also
    ensures that when the predicted probability of sampling any of the timestamp token
    is greater than any individual non-timestamp token, those non-timestamp logits
    are set to negative infinity. This is done to ensure the generation of timestamps
    over other potential tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: See [the paper](https://arxiv.org/abs/2212.04356) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/logits_process.py#L1843)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be logits for each vocabulary
    when not using beam search or log softmax for each vocabulary token when using
    beam search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.TFForcedBOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L448)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces the specified token as the first generated token.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L462)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFForcedEOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L478)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`) — The id of the token to force as the last generated
    token when `max_length` is reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L495)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFForceTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L551)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: This processor takes a list of pairs of integers which indicates a mapping from
    generation indices to token indices that will be forced before sampling. The processor
    will set their log probs to `0` and all other tokens to `-inf` so that they are
    sampled at their corresponding index.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L567)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit processors that can be applied during generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L56)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: TF method for processing logits.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TFLogitsProcessorList`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L75)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: This class can be used to create a list of [TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TFLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L67)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`tf.Tensor` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cur_len` (`int`) — The current length of valid input sequence tokens. In the
    TF implementation, the input_ids’ sequence length is the maximum length generate
    can produce, and we need to know which of its tokens are valid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.Tensor` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: TF method for warping logits.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TFMinLengthLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L202)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    enforcing a min-length by setting EOS probability to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L228)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFNoBadWordsLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L288)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bad_words_ids` (`List[List[int]]`) — List of list of token ids that are not
    allowed to be generated. In order to get the tokens of the words that should not
    appear in the generated text, make sure to set `add_prefix_space=True` when initializing
    the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`.
    The `add_prefix_space` argument is only supported for some slow tokenizers, as
    fast tokenizers’ prefixing behaviours come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces that specified sequences will never be sampled.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L367)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFNoRepeatNGramLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L388)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`ngram_size` (`int`) — All ngrams of size `ngram_size` can only occur once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    that enforces no repetition of n-grams. See [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L427)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFRepetitionPenaltyLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L238)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`repetition_penalty` (`float`) — The parameter for repetition penalty. 1.0
    means no penalty. See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsProcessor)
    enforcing an exponential penalty on repeated sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L280)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFSuppressTokensAtBeginLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L511)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[TFSuppressTokensAtBeginLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFSuppressTokensAtBeginLogitsProcessor)
    suppresses a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    at not sampled at the begining of the generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L522)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFSuppressTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L535)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: This processor can be used to suppress a list of tokens. The processor will
    set their log probs to `-inf` so that they are not sampled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L542)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFTemperatureLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L98)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature` (`float`) — The value used to module the logits distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    for temperature (exponential scaling output probability distribution).'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFTopKLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.TFTopPLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L146)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TFLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.TFLogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to <= prob_cut_off.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_logits_process.py#L170)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: FLAX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxForcedBOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L194)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`) — The id of the token to force as the first generated
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that enforces the specified token as the first generated token.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L206)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxForcedEOSTokenLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L216)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`) — The maximum length of the sequence to be generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`) — The id of the token to force as the last generated
    token when `max_length` is reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that enforces the specified token as the last generated token when `max_length`
    is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L231)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxForceTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L315)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`force_token_map` (`list`) — Map giving token ids and indices where they will
    be forced to be sampled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    that takes a list of pairs of integers which indicates a mapping from generation
    indices to token indices that will be forced before sampling. The processor will
    set their log probs to 0 and all other tokens to `-inf` so that they are sampled
    at their corresponding index.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L337)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L50)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit processors that can be applied during generation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L53)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: Flax method for processing logits.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxLogitsProcessorList`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: This class can be used to create a list of [FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    or [FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    to subsequently process a `scores` input tensor. This class inherits from list
    and adds a specific ***call*** method to apply each [FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    or [FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    to the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L79)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L61)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all logit warpers that can be applied during generation
    with multinomial sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — Indices
    of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`) — Prediction
    scores of a language modeling head. These can be logits for each vocabulary when
    not using beam search or log softmax for each vocabulary token when using beam
    search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional logits processor specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`jnp.ndarray` of shape `(batch_size, config.vocab_size)`'
  prefs: []
  type: TYPE_NORMAL
- en: The processed prediction scores.
  prefs: []
  type: TYPE_NORMAL
- en: Flax method for warping logits.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.FlaxMinLengthLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L241)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`min_length` (`int`) — The minimum length below which the score of `eos_token_id`
    is set to `-float("Inf")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`) — The id of the *end-of-sequence* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    enforcing a min-length by setting EOS probability to 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L262)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxSuppressTokensAtBeginLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L271)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`begin_suppress_tokens` (`List[int]`) — Tokens to not sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`begin_index` (`int`) — Index where the tokens are suppressed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    supressing a list of tokens as soon as the `generate` function starts generating
    using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens`
    are not sampled at the begining of the generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L288)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxSuppressTokensLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L296)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`suppress_tokens` (`list`) — Tokens to not sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsProcessor](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsProcessor)
    suppressing a list of tokens at each decoding step. The processor will set their
    log probs to be `-inf` so they are not sampled.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L309)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxTemperatureLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L95)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`temperature` (`float`) — The value used to module the logits distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    for temperature (exponential scaling output probability distribution).'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L110)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxTopKLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_k` (`int`) — The number of highest probability vocabulary tokens to keep
    for top-k-filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    that performs top-k, i.e. restricting to the k highest probability elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L179)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxTopPLogitsWarper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L115)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p` (`float`) — If set to < 1, only the smallest set of most probable tokens
    with probabilities that add up to `top_p` or higher are kept for generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_value` (`float`, *optional*, defaults to -inf) — All filtered values
    will be set to this float value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimum number of
    tokens that cannot be filtered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FlaxLogitsWarper](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.FlaxLogitsWarper)
    that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <=
    prob_cut_off.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L139)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.FlaxWhisperTimeStampLogitsProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L363)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`generate_config` (`GenerateConfig`) — The generate config used to generate
    the output. The following parameters are required: eos_token_id (`int`, *optional*,
    defaults to 50257): The id of the *end-of-sequence* token. no_timestamps_token_id
    (`int`, *optional*, defaults to 50363): The id of the `"<|notimestamps|>"` token.
    max_initial_timestamp_index (`int`, *optional*, defaults to 1): Used to set the
    maximum value of the initial timestamp. This is used to prevent the model from
    predicting timestamps that are too far in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whisper specific Processor. This processor can be used to force a list of tokens.
    The processor will set their log probs to `inf` so that they are sampled at their
    corresponding index.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/flax_logits_process.py#L397)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: StoppingCriteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [StoppingCriteria](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.StoppingCriteria)
    can be used to change when to stop generation (other than EOS token). Please note
    that this is exclusively available to our PyTorch implementations.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.StoppingCriteria`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L37)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all stopping criteria that can be applied during generation.
  prefs: []
  type: TYPE_NORMAL
- en: If your stopping criteria depends on the `scores` input, make sure you pass
    `return_dict_in_generate=True, output_scores=True` to `generate`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L44)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.StoppingCriteriaList`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.MaxLengthCriteria`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_length` (`int`) — The maximum length that the output sequence can have
    in number of tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*) — The maximum model length, as
    defined by the model’s `config.max_position_embeddings` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This class can be used to stop generation whenever the full generated number
    of tokens exceeds `max_length`. Keep in mind for decoder-only type of transformers,
    this will include the initial prompted tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L65)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '### `class transformers.MaxTimeCriteria`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L107)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`max_time` (`float`) — The maximum allowed time in seconds for the generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initial_time` (`float`, *optional*, defaults to `time.time()`) — The start
    of the generation allowed time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This class can be used to stop generation whenever the full generation exceeds
    some amount of time. By default, the time will start being counted when you initialize
    this function. You can override this by passing an `initial_time`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/stopping_criteria.py#L124)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`scores` (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`) —
    Prediction scores of a language modeling head. These can be scores for each vocabulary
    token before SoftMax or scores for each vocabulary token after SoftMax. If this
    stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,
    output_scores=True` to `generate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional stopping criteria specific
    kwargs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    can be used to force the generation to include specific tokens or sequences in
    the output. Please note that this is exclusively available to our PyTorch implementations.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.Constraint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all constraints that can be applied during generation.
    It must define how the constraint can be satisfied.
  prefs: []
  type: TYPE_NORMAL
- en: All classes that inherit Constraint must follow the requirement that
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: will always terminate (halt).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `advance`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: token_ids(`torch.tensor`)
  prefs: []
  type: TYPE_NORMAL
- en: Must be a tensor of a list of indexable tokens, not some integer.
  prefs: []
  type: TYPE_NORMAL
- en: When called, returns the token that would take this constraint one step closer
    to being fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `copy`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L113)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: constraint(`Constraint`)
  prefs: []
  type: TYPE_NORMAL
- en: The same constraint as the one being called from.
  prefs: []
  type: TYPE_NORMAL
- en: Creates a new instance of this constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `does_advance`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L60)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: Reads in a token and returns whether it creates progress.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `remaining`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L104)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: Returns the number of remaining steps of `advance()` in order to complete this
    constraint.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L94)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: Resets the state of this constraint to its initialization. We would call this
    in cases where the fulfillment of a constraint is abrupted by an unwanted token.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `test`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: Tests whether this constraint has been properly defined.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `update`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L69)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: stepped(`bool`)
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether this constraint has become one step closer to being fulfuilled. completed(`bool`):
    Whether this constraint has been completely fulfilled by this token being generated.
    reset (`bool`): Whether this constraint has reset its progress by this token being
    generated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reads in a token and returns booleans that indicate the progress made by it.
    This function will update the state of this object unlikes `does_advance(self,
    token_id: int)`.'
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t to test whether a certain token will advance the progress; it’s to
    update its state as if it has been generated. This becomes important if token_id
    != desired token (refer to else statement in PhrasalConstraint)
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.PhrasalConstraint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L129)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids` (`List[int]`) — The id of the token that must be generated by the
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    enforcing that an ordered sequence of tokens is included in the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.DisjunctiveConstraint`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L261)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`nested_token_ids` (`List[List[int]]`) — A list of words, where each word is
    a list of ids. This constraint is fulfilled by generating just one from the list
    of words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A special [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    that is fulfilled by fulfilling just one of several constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.ConstraintListState`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L351)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`constraints` (`List[Constraint]`) — A list of [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    objects that must be fulfilled by the beam scorer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A class for beam scorers to track its progress through a list of constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `advance`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L383)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: The list of tokens to generate such that we can make progress. By “list” we
    don’t mean the list of token that will fully fulfill a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given constraints `c_i = {t_ij | j == # of tokens}`, If we’re not in the middle
    of progressing through a specific constraint `c_i`, we return:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[t_k1 for k in indices of unfulfilled constraints]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are in the middle of a constraint, then we return: `[t_ij]`, where `i`
    is the index of the inprogress constraint, `j` is the next step for the constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: Though we don’t care which constraint is fulfilled first, if we are in the progress
    of fulfilling a constraint, that’s the only one we’ll return.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reset`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_constraints.py#L418)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: 'token_ids: the tokens generated thus far to reset the state of the progress
    through constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: BeamSearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.BeamScorer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L91)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: Abstract base class for all beam scorers that are used for [beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_search)
    and [beam_sample()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L97)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`next_scores` (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`)
    — Current scores of the top `2 * num_beams` non-finished beam hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_tokens` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`) —
    `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished
    beam hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_indices` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`)
    — Beam indices indicating to which beam hypothesis the `next_tokens` correspond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices indicating to
    which beam hypothesis each token correspond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`group_index` (`int`, *optional*) — The index of the group of beams. Used with
    [group_beam_search()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`UserDict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary composed of the fields as defined above:'
  prefs: []
  type: TYPE_NORMAL
- en: '`next_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Updated scores of all non-finished beams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Next tokens to be added to the non-finished beam_hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Beam indices indicating to which beam the next tokens shall be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `finalize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L109)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`final_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The final scores of all non-finished beams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`final_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The last tokens to be added to the non-finished beam_hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`final_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — The beam indices indicating to which beam the `final_beam_tokens` shall be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`'
  prefs: []
  type: TYPE_NORMAL
- en: The generated sequences. The second dimension (sequence_length) is either equal
    to `max_length` or shorter if all batches finished early due to the `eos_token_id`.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.BeamSearchScorer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L123)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — Batch Size of `input_ids` for which standard beam search
    decoding is run in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`) — Number of beams for beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`torch.device`) — Defines the device type (*e.g.*, `"cpu"` or `"cuda"`)
    on which this instance of `BeamSearchScorer` will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_hyps_to_keep` (`int`, *optional*, defaults to 1) — The number of
    beam hypotheses that shall be returned upon calling [finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — The maximum length of the sequence to be
    generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    implementing standard beam search decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Adapted in part from [Facebook’s XLM beam search code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).
  prefs: []
  type: TYPE_NORMAL
- en: Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan’s
    DBS implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)
  prefs: []
  type: TYPE_NORMAL
- en: '#### `process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L215)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '#### `finalize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L318)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '### `class transformers.ConstrainedBeamSearchScorer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L415)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` (`int`) — Batch Size of `input_ids` for which standard beam search
    decoding is run in parallel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beams` (`int`) — Number of beams for beam search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`constraints` (`List[Constraint]`) — A list of positive constraints represented
    as `Constraint` objects that must be fulfilled in the generation output. For more
    information, the documentation of [Constraint](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.Constraint)
    should be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`torch.device`) — Defines the device type (*e.g.*, `"cpu"` or `"cuda"`)
    on which this instance of `BeamSearchScorer` will be allocated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`length_penalty` (`float`, *optional*, defaults to 1.0) — Exponential penalty
    to the length that is used with beam-based generation. It is applied as an exponent
    to the sequence length, which in turn is used to divide the score of the sequence.
    Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty`
    > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter
    sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_early_stopping` (`bool` or `str`, *optional*, defaults to `False`) — Controls
    the stopping condition for beam-based methods, like beam-search. It accepts the
    following values: `True`, where the generation stops as soon as there are `num_beams`
    complete candidates; `False`, where an heuristic is applied and the generation
    stops when is it very unlikely to find better candidates; `"never"`, where the
    beam search procedure only stops when there cannot be better candidates (canonical
    beam search algorithm).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_hyps_to_keep` (`int`, *optional*, defaults to 1) — The number of
    beam hypotheses that shall be returned upon calling [finalize()](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamSearchScorer.finalize).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_beam_groups` (`int`, *optional*, defaults to 1) — Number of groups to
    divide `num_beams` into in order to ensure diversity among different groups of
    beams. See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — The maximum length of the sequence to be
    generated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[BeamScorer](/docs/transformers/v4.37.2/en/internal/generation_utils#transformers.BeamScorer)
    implementing constrained beam search decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L509)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using any class inheriting from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`next_scores` (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`)
    — Current scores of the top `2 * num_beams` non-finished beam hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_tokens` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`) —
    `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished
    beam hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_indices` (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`)
    — Beam indices indicating to which beam hypothesis the `next_tokens` correspond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores_for_all_vocab` (`torch.FloatTensor` of shape `(batch_size * num_beams,
    sequence_length)`) — The scores of all tokens in the vocabulary for each of the
    beam hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*) — The id of the *padding* token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`Union[int, List[int]]`, *optional*) — The id of the *end-of-sequence*
    token. Optionally, use a list to set multiple *end-of-sequence* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beam_indices` (`torch.LongTensor`, *optional*) — Beam indices indicating to
    which beam hypothesis each token correspond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_prompt_len` (`int`, *optional*) — The length of prompt that is included
    in the input to decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`UserDict`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A dictionary composed of the fields as defined above:'
  prefs: []
  type: TYPE_NORMAL
- en: '`next_beam_scores` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Updated scores of all non-finished beams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_beam_tokens` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Next tokens to be added to the non-finished beam_hypotheses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_beam_indices` (`torch.FloatTensor` of shape `(batch_size * num_beams)`)
    — Beam indices indicating to which beam the next tokens shall be added.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `finalize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/beam_search.py#L807)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: Utilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '#### `transformers.top_k_top_p_filtering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/utils.py#L4610)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 0) — If > 0, only keep the top k tokens
    with highest probability (top-k filtering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If < 1.0, only keep the top
    tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering
    is described in Holtzman et al. ([http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimumber of tokens
    we keep per batch example in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
  prefs: []
  type: TYPE_NORMAL
- en: 'From: [https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `transformers.tf_top_k_top_p_filtering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/tf_utils.py#L3092)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`top_k` (`int`, *optional*, defaults to 0) — If > 0, only keep the top k tokens
    with highest probability (top-k filtering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p` (`float`, *optional*, defaults to 1.0) — If < 1.0, only keep the top
    tokens with cumulative probability >= top_p (nucleus filtering). Nucleus filtering
    is described in Holtzman et al. ([http://arxiv.org/abs/1904.09751](http://arxiv.org/abs/1904.09751))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_tokens_to_keep` (`int`, *optional*, defaults to 1) — Minimumber of tokens
    we keep per batch example in the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
  prefs: []
  type: TYPE_NORMAL
- en: 'From: [https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317](https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317)'
  prefs: []
  type: TYPE_NORMAL
- en: Streamers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TextStreamer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L38)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` (`AutoTokenizer`) — The tokenized used to decode the tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_prompt` (`bool`, *optional*, defaults to `False`) — Whether to skip the
    prompt to `.generate()` or not. Useful e.g. for chatbots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode_kwargs` (`dict`, *optional*) — Additional keyword arguments to pass
    to the tokenizer’s `decode` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple text streamer that prints the token(s) to stdout as soon as entire words
    are formed.
  prefs: []
  type: TYPE_NORMAL
- en: The API for the streamer classes is still under development and may change in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '#### `end`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L116)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: Flushes any remaining cache and prints a newline to stdout.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `on_finalized_text`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: Prints the new text to stdout. If the stream is ending, also prints a newline.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `put`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: Receives tokens, decodes them, and prints them to stdout as soon as they form
    entire words.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.TextIteratorStreamer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L159)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`tokenizer` (`AutoTokenizer`) — The tokenized used to decode the tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skip_prompt` (`bool`, *optional*, defaults to `False`) — Whether to skip the
    prompt to `.generate()` or not. Useful e.g. for chatbots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`float`, *optional*) — The timeout for the text queue. If `None`,
    the queue will block indefinitely. Useful to handle exceptions in `.generate()`,
    when it is called in a separate thread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decode_kwargs` (`dict`, *optional*) — Additional keyword arguments to pass
    to the tokenizer’s `decode` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streamer that stores print-ready text in a queue, to be used by a downstream
    application as an iterator. This is useful for applications that benefit from
    acessing the generated text in a non-blocking way (e.g. in an interactive Gradio
    demo).
  prefs: []
  type: TYPE_NORMAL
- en: The API for the streamer classes is still under development and may change in
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '#### `on_finalized_text`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/generation/streamers.py#L213)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: Put the new text in the queue. If the stream is ending, also put a stop signal
    in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Caches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Cache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: Base, abstract class for all caches. The actual data structure is specific to
    each subclass.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `update`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L11)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. These are specific to each subclass and allow new types of cache
    to be created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.DynamicCache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L57)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: A cache that grows dynamically as more tokens are generated. This is the default
    for generative models.
  prefs: []
  type: TYPE_NORMAL
- en: It stores the Key and Value states as a list of tensors, one for each layer.
    The expected shape for each tensor is `[batch_size, num_heads, seq_len, head_dim]`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `update`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L95)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. No additional arguments are used in `DynamicCache`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_seq_length`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L132)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: Returns the sequence length of the cached states. A layer index can be optionally
    passed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reorder_cache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L142)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: Reorders the cache for beam search, given the selected beam indices.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_legacy_cache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L150)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: Converts the `DynamicCache` instance into the its equivalent in the legacy cache
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_legacy_cache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L157)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: Converts a cache in the legacy cache format into an equivalent `DynamicCache`.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class transformers.SinkCache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L168)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`window_length` (`int`) — The length of the context window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_sink_tokens` (`int`) — The number of sink tokens. See the original paper
    for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453).
    It allows the model to generate beyond the length of its context window, without
    losing fluency in the conversation. As it discards past tokens, the model will
    lose the ability to generate tokens that depend on the context that was discarded.
  prefs: []
  type: TYPE_NORMAL
- en: It stores the Key and Value states as a list of tensors, one for each layer.
    The expected shape for each tensor is `[batch_size, num_heads, seq_len, head_dim]`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `update`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`key_states` (`torch.Tensor`) — The new key states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`value_states` (`torch.Tensor`) — The new value states to cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_idx` (`int`) — The index of the layer to cache the states for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_kwargs` (`Dict[str, Any]`, `optional`) — Additional arguments for the
    cache subclass. The following arguments can be used in `SinkCache`: `sin`, `cos`
    and `partial_rotation_size`. These arguments are used with models using RoPE,
    to recompute the rotation as the tokens are shifted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the cache with the new `key_states` and `value_states` for the layer
    `layer_idx`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_seq_length`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L226)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: Returns the sequence length of the cached states. A layer index can be optionally
    passed.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `reorder_cache`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/cache_utils.py#L316)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: Reorders the cache for beam search, given the selected beam indices.
  prefs: []
  type: TYPE_NORMAL
