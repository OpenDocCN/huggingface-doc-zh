# X-MOD

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xmod](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/xmod)

## 概述

X-MOD模型是由Jonas Pfeiffer、Naman Goyal、Xi Lin、Xian Li、James Cross、Sebastian Riedel和Mikel Artetxe在[Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255)中提出的。X-MOD扩展了多语言掩码语言模型，如[XLM-R](xlm-roberta)，在预训练期间包含特定于语言的模块化组件（*语言适配器*）。在微调中，每个Transformer层中的语言适配器被冻结。

论文摘要如下：

*多语言预训练模型因多语言性的诅咒而备受困扰，导致随着覆盖更多语言，每种语言的性能下降。我们通过引入语言特定模块来解决这个问题，这使我们能够增加模型的总容量，同时保持每种语言的可训练参数总数恒定。与以往学习语言特定组件的后续工作相比，我们从一开始就对我们的跨语言模块（X-MOD）模型的模块进行预训练。我们在自然语言推理、命名实体识别和问答方面的实验表明，我们的方法不仅减轻了语言之间的负面干扰，还实现了积极的迁移，从而提高了单语和跨语言性能。此外，我们的方法使得可以在后续添加语言而不会出现性能下降，不再将模型的使用限制在预训练语言集合中。*

这个模型是由[jvamvas](https://huggingface.co/jvamvas)贡献的。原始代码可以在[这里](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/fairseq/models/xmod)找到，原始文档可以在[这里](https://github.com/facebookresearch/fairseq/tree/58cc6cca18f15e6d56e3f60c959fe4f878960a60/examples/xmod)找到。

## 使用提示

提示：

+   X-MOD类似于[XLM-R](xlm-roberta)，但不同之处在于需要指定输入语言，以便激活正确的语言适配器。

+   主要模型 - 基础和大型 - 具有81种语言的适配器。

## 适配器使用

### 输入语言

有两种指定输入语言的方法：

1.  在使用模型之前设置默认语言：

```py
from transformers import XmodModel

model = XmodModel.from_pretrained("facebook/xmod-base")
model.set_default_language("en_XX")
```

1.  通过为每个样本显式传递语言适配器的索引：

```py
import torch

input_ids = torch.tensor(
    [
        [0, 581, 10269, 83, 99942, 136, 60742, 23, 70, 80583, 18276, 2],
        [0, 1310, 49083, 443, 269, 71, 5486, 165, 60429, 660, 23, 2],
    ]
)
lang_ids = torch.LongTensor(
    [
        0,  # en_XX
        8,  # de_DE
    ]
)
output = model(input_ids, lang_ids=lang_ids)
```

### 微调

论文建议在微调期间冻结嵌入层和语言适配器。提供了一个这样做的方法：

```py
model.freeze_embeddings_and_language_adapters()
# Fine-tune the model ...
```

### 跨语言转移

在微调后，可以通过激活目标语言的语言适配器来测试零样本跨语言转移：

```py
model.set_default_language("de_DE")
# Evaluate the model on German examples ...
```

## 资源

+   [文本分类任务指南](../tasks/sequence_classification)

+   [标记分类任务指南](../tasks/token_classification)

+   [问答任务指南](../tasks/question_answering)

+   [因果语言建模任务指南](../tasks/language_modeling)

+   [掩码语言建模任务指南](../tasks/masked_language_modeling)

+   [多项选择任务指南](../tasks/multiple_choice)

## XmodConfig

### `class transformers.XmodConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/configuration_xmod.py#L40)

```py
( vocab_size = 30522 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True classifier_dropout = None pre_norm = False adapter_reduction_factor = 2 adapter_layer_norm = False adapter_reuse_layer_norm = True ln_before_adapter = True languages = ('en_XX',) default_language = None **kwargs )
```

参数

+   `vocab_size` (`int`, *可选*, 默认为30522) — X-MOD模型的词汇量。定义了在调用[XmodModel](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodModel)时可以表示的不同标记的数量。

+   `hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`、`"relu"`、`"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢失比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 此模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024或2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 2) — 在调用 [XmodModel](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodModel) 时传递的 `token_type_ids` 的词汇量大小。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层规范化层使用的 epsilon。

+   `position_embedding_type` (`str`, *optional*, defaults to `"absolute"`) — 位置嵌入的类型。选择 `"absolute"`、`"relative_key"`、`"relative_key_query"` 中的一个。对于位置嵌入，请使用 `"absolute"`。有关 `"relative_key"` 的更多信息，请参考 [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关 `"relative_key_query"` 的更多信息，请参考 [Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658) 中的 *Method 4*。

+   `is_decoder` (`bool`, *optional*, defaults to `False`) — 模型是否用作解码器。如果为 `False`，则模型用作编码器。

+   `use_cache` (`bool`, *optional*, defaults to `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。仅在 `config.is_decoder=True` 时相关。

+   `classifier_dropout` (`float`, *optional*) — 分类头的丢失比率。

+   `pre_norm` (`bool`, *optional*, defaults to `False`) — 是否在每个块之前应用层规范化。

+   `adapter_reduction_factor` (`int` or `float`, *optional*, defaults to 2) — 适配器的维度相对于 `hidden_size` 减少的因子。

+   `adapter_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否在适配器模块之前应用新的层规范化（所有适配器共享）。

+   `adapter_reuse_layer_norm` (`bool`, *optional*, defaults to `True`) — 是否重用第二层规范化并在适配器模块之前应用它。

+   `ln_before_adapter` (`bool`, *optional*, defaults to `True`) — 是否在适配器模块周围的残差连接之前应用层规范化。

+   `languages` (`Iterable[str]`, *optional*, defaults to `["en_XX"]`) — 适配器模块应初始化的语言代码的可迭代对象。

+   `default_language` (`str`, *optional*) — 默认语言的语言代码。如果未明确传递语言代码给前向方法，则假定输入是这种语言。

这是用于存储[XmodModel](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodModel)配置的配置类。根据指定的参数实例化X-MOD模型，定义模型架构。使用默认值实例化配置将产生类似于[facebook/xmod-base](https://huggingface.co/facebook/xmod-base)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import XmodConfig, XmodModel

>>> # Initializing an X-MOD facebook/xmod-base style configuration
>>> configuration = XmodConfig()

>>> # Initializing a model (with random weights) from the facebook/xmod-base style configuration
>>> model = XmodModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## XmodModel

### `class transformers.XmodModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L776)

```py
( config add_pooling_layer = True )
```

参数

+   `config`（[XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)）— 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸X-MOD模型变压器输出原始隐藏状态，没有特定的头部。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型还是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

该模型既可以作为编码器（仅具有自注意力），也可以作为解码器，此时在自注意力层之间添加了一层交叉注意力，遵循Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin在*Attention is all you need*中描述的架构。

为了作为解码器运行，模型需要使用`is_decoder`参数初始化，设置为`True`。要在Seq2Seq模型中使用，模型需要使用`is_decoder`参数和`add_cross_attention`设置为`True`进行初始化；然后期望在前向传递中输入`encoder_hidden_states`。

.. _*注意力就是你所需要的*：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L826)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `lang_ids`（`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*）— 每个样本应激活的语言适配器的索引。默认值为对应于`self.config.default_language`的索引。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]` 之间：

    +   1 表示未被掩码的标记，

    +   0 表示被掩码的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选在 `[0, 1]` 之间：

    +   0 对应 *句子A* 标记，

    +   1 对应 *句子B* 标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记的位置嵌入的索引。选在范围 `[0, config.max_position_embeddings - 1]` 内。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的特定头部失效的掩码。掩码值选在 `[0, 1]` 之间：

    +   1 表示头部未被掩码，

    +   0 表示头部被掩码。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量中的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量中的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `encoder_hidden_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用。掩码值选在 `[0, 1]` 之间：

    +   1 表示未被掩码的标记，

    +   0 表示被掩码的标记。

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`，长度为 `config.n_layers`，每个元组包含 4 个张量 —

+   `of` 形状为 `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` 的张量 — 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用 `past_key_values`，用户可以选择仅输入最后的 `decoder_input_ids`（没有将其过去的键值状态提供给此模型的那些）的形状为 `(batch_size, 1)` 的张量，而不是形状为 `(batch_size, sequence_length)` 的所有 `decoder_input_ids`。

+   `use_cache` (`bool`, *optional*) — 如果设置为 `True`，则返回 `past_key_values` 键值状态，可用于加速解码（参见 `past_key_values`）。

[XmodModel](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodModel) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

## XmodForCausalLM

### `class transformers.XmodForCausalLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L968)

```py
( config )
```

参数

+   `config` ([XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

X-MOD模型，在顶部带有`语言建模`头部，用于CLM微调。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头部等）。

该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L996)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None past_key_values: Tuple = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `lang_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个样本应激活的语言适配器的索引。默认值为对应于`self.config.default_language`的索引。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示标记未被`masked`，

    +   0表示被`masked`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*的标记，

    +   1对应于*句子B*的标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） - 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*） - 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `encoder_hidden_states`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*） - 编码器最后一层的隐藏状态序列。如果模型配置为解码器，则在交叉注意力中使用。

+   `encoder_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*） - 用于避免在编码器输入的填充标记索引上执行注意力的掩码。如果模型配置为解码器，则在交叉注意力中使用此掩码。掩码值选在`[0, 1]`之间：

    +   1表示`未被掩码`的标记，

    +   0表示`被掩码`的标记。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*） - 用于计算从左到右的语言建模损失（下一个单词预测）的标签。索引应在`[-100, 0, ..., config.vocab_size]`中（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `past_key_values`（长度为`config.n_layers`的`tuple(tuple(torch.FloatTensor))`，每个元组有4个形状为`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`的张量） - 包含注意力块的预计算键和值隐藏状态。可用于加速解码。

    如果使用`past_key_values`，用户可以选择仅输入最后一个形状为`(batch_size, 1)`的`decoder_input_ids`（那些没有将其过去的键值状态提供给此模型的）而不是所有形状为`(batch_size, sequence_length)`的`decoder_input_ids`。

+   `use_cache`（`bool`，*可选*） - 如果设置为`True`，则返回`past_key_values`键值状态，可用于加速解码（参见`past_key_values`）。

    返回 - `transformers.modeling_outputs.CausalLMOutputWithCrossAttentions`或`tuple(torch.FloatTensor)`

    示例 -

[XmodForCausalLM](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForCausalLM)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## XmodForMaskedLM

### `class transformers.XmodForMaskedLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1136)

```py
( config )
```

参数

+   `config`（[XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)） - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有顶部`语言建模`头的X-MOD模型。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1167)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None encoder_hidden_states: Optional = None encoder_attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `lang_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 分别为每个样本应激活的语言适配器的索引。默认值：对应于`self.config.default_language`的索引。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`中：

    +   对于未被`masked`的标记为1，

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*的标记，

    +   1对应于*句子B*的标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 输入序列标记的每个位置的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。选择的掩码值在`[0, 1]`中：

    +   1表示头部是`not masked`，

    +   0表示头部是`masked`。

+   `inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制权来将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`中（请参阅`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`中的标记。

+   `kwargs`（`Dict[str, any]`，可选，默认为*{}*）— 用于隐藏已弃用的旧参数。

[XmodForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## XmodForSequenceClassification

### `class transformers.XmodForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1260)

```py
( config )
```

参数

+   `config`（[XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

X-MOD 模型变压器，顶部带有序列分类/回归头（池化输出的线性层），例如 GLUE 任务。

此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `前向`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1280)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`）— 词汇表中输入序列标记的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    什么是输入 ID？

+   `lang_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）— 应为每个样本激活的语言适配器的索引。默认值：与 `self.config.default_language` 对应的索引。

+   `attention_mask`（形状为 `(batch_size, sequence_length)` 的 `torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。选择的掩码值在 `[0, 1]` 中：

    +   1 用于 `未被掩码` 的标记，

    +   0 用于被 `掩码` 的标记。

    什么是注意力掩码？

+   `token_type_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 的标记，

    +   1 对应于 *句子 B* 的标记。

    什么是标记类型 ID？

+   `position_ids`（形状为 `(batch_size, sequence_length)` 的 `torch.LongTensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围 `[0, config.max_position_embeddings - 1]` 中选择。

    什么是位置 ID？

+   `head_mask`（形状为 `(num_heads,)` 或 `(num_layers, num_heads)` 的 `torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部无效的掩码。选择的掩码值在 `[0, 1]` 中：

    +   1 表示头部 `未被掩码`，

    +   0 表示头部被 `掩码`。

+   `inputs_embeds`（形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（`torch.LongTensor`，形状为`(batch_size,)`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

[XmodForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForSequenceClassification)的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## XmodForMultipleChoice

### `class transformers.XmodForMultipleChoice`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1353)

```py
( config )
```

参数

+   `config`（[XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

X-MOD模型，顶部带有多选分类头部（顶部的线性层和softmax），例如用于RocStories/SWAG任务。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1372)

```py
( input_ids: Optional = None lang_ids: Optional = None token_type_ids: Optional = None attention_mask: Optional = None labels: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（`torch.LongTensor`，形状为`(batch_size, num_choices, sequence_length)`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `lang_ids`（`torch.LongTensor`，形状为`(batch_size, num_choices, sequence_length)`，*可选*）— 应为每个样本激活的语言适配器的索引。默认值：对应于`self.config.default_language`的索引。

+   `attention_mask`（`torch.FloatTensor`，形状为`(batch_size, num_choices, sequence_length)`，*可选*）— 避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   对于`not masked`的标记为1，

    +   对于`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`之间：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*) — 每个输入序列标记在位置嵌入中的位置索引。选择范围为`[0, config.max_position_embeddings - 1]`。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示头部未被屏蔽，

    +   0表示头部被屏蔽。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — 用于计算多项选择分类损失的标签。索引应在`[0, ..., num_choices-1]`范围内，其中`num_choices`是输入张量第二维的大小。（参见上面的`input_ids`）

[XmodForMultipleChoice](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForMultipleChoice)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## XmodForTokenClassification

### `class transformers.XmodForTokenClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1442)

```py
( config )
```

参数

+   `config` ([XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

X-MOD模型在顶部具有一个标记分类头部（隐藏状态输出顶部的线性层），例如用于命名实体识别（NER）任务。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1465)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `lang_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个样本应激活的语言适配器的索引。默认值：对应于`self.config.default_language`的索引。

+   `attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于避免在填充标记索引上执行注意力的蒙版。蒙版值选在`[0, 1]`范围内：

    +   1表示标记未被`masked`，

    +   对于被`masked`的标记，索引为0。

    [什么是注意力蒙版？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 指示输入的第一部分和第二部分的段标记索引。索引选在`[0, 1]`范围内：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 每个输入序列标记在位置嵌入中的位置索引。选在范围`[0, config.max_position_embeddings - 1]`内。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*) — 用于使自注意力模块的选定头部失效的蒙版。蒙版值选在`[0, 1]`范围内：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*) — 可选地，可以直接传递嵌入表示，而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*) — 用于计算标记分类损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。

[XmodForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForTokenClassification)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## XmodForQuestionAnswering

### `class transformers.XmodForQuestionAnswering`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1544)

```py
( config )
```

参数

+   `config` ([XmodConfig](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

X-MOD 模型在顶部具有一个跨度分类头，用于提取式问答任务，如 SQuAD（在隐藏状态输出的顶部有线性层，用于计算 `span start logits` 和 `span end logits`）。

此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档，了解库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头部等）。

此模型还是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档，了解所有与一般用法和行为相关的事项。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/xmod/modeling_xmod.py#L1563)

```py
( input_ids: Optional = None lang_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None inputs_embeds: Optional = None start_positions: Optional = None end_positions: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 词汇表中输入序列令牌的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `lang_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 应为每个样本激活的语言适配器的索引。默认值：对应于 `self.config.default_language` 的索引。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充令牌索引上执行注意力的掩码。掩码值选择在 `[0, 1]` 之间：

    +   1 表示未被 `masked` 的令牌，

    +   0 表示被 `masked` 的令牌。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段标记索引，指示输入的第一部分和第二部分。索引选择在 `[0, 1]` 之间：

    +   0 对应于 *句子 A* 令牌，

    +   1 对应于 *句子 B* 令牌。

    [什么是令牌类型 ID？](../glossary#token-type-ids)

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 每个输入序列令牌在位置嵌入中的位置索引。选择范围为 `[0, config.max_position_embeddings - 1]`。

    [什么是位置 ID？](../glossary#position-ids)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选择在 `[0, 1]` 之间：

    +   1 表示未被 `masked` 的头部，

    +   0 表示头部被 `masked`。

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制权，以便将 `input_ids` 索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。更多细节请参阅返回张量中的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通元组。

+   `start_positions` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算标记跨度起始位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度 (`sequence_length`)。序列之外的位置不会被考虑在内，用于计算损失。

+   `end_positions` (`torch.LongTensor`，形状为 `(batch_size,)`，*可选*) — 用于计算标记跨度结束位置（索引）的标签，以计算标记分类损失。位置被夹紧到序列的长度 (`sequence_length`)。序列之外的位置不会被考虑在内，用于计算损失。

[XmodForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/xmod#transformers.XmodForQuestionAnswering) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
