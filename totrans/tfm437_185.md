# Jukebox

> [`huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/jukebox)

## 概述

Jukebox 模型在 [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf) 中由 Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever 提出。它引入了一个生成音乐模型，可以生成可以根据艺术家、流派和歌词进行条件化的一分钟长样本。

论文摘要如下：

*我们推出了 Jukebox，这是一个在原始音频领域生成带有歌唱的音乐的模型。我们使用多尺度 VQ-VAE 来压缩原始音频的长上下文为离散代码，并使用自回归 Transformers 对其进行建模。我们展示了规模上的组合模型可以生成高保真度和多样化的歌曲，连贯性可达多分钟。我们可以根据艺术家和流派来引导音乐和声音风格，并根据不对齐的歌词来使歌唱更可控。我们发布了数千个非精选样本，以及模型权重和代码。*

如下图所示，Jukebox 由 3 个仅解码器模型的 `priors` 组成。它们遵循 [使用稀疏 Transformers 生成长序列](https://arxiv.org/abs/1904.10509) 中描述的架构，经过修改以支持更长的上下文长度。首先，使用自编码器对文本歌词进行编码。接下来，第一个（也称为 `top_prior`）prior 关注从歌词编码器提取的最后隐藏状态。先前的 priors 通过 `AudioConditionner` 模块分别连接到前一个 priors。`AudioConditioner` 将先前 prior 的输出上采样到特定音频帧每秒的原始标记。元数据，如 *艺术家、流派和时间*，以起始标记和时间数据的位置嵌入的形式传递给每个 prior。隐藏状态被映射到 VQVAE 中最接近的码书向量，以将它们转换为原始音频。

![JukeboxModel](img/5772cc27b249201f12c4d49bc48dfd5e.png)

该模型由 [Arthur Zucker](https://huggingface.co/ArthurZ) 贡献。原始代码可以在 [这里](https://github.com/openai/jukebox) 找到。

## 使用提示

+   该模型仅支持推理。这主要是因为训练需要大量内存。欢迎提交 PR 并添加缺失的内容，以实现与 Hugging Face Trainer 的完全集成！

+   该模型非常慢，使用 V100 GPU 上的 5b 顶部 prior 生成一分钟长的音频需要 8 小时。为了自动处理模型应在其上执行的设备，请使用 `accelerate`。

+   与论文相反，prior 的顺序从 `0` 到 `1`，因为这样更直观：我们从 `0` 开始采样。

+   基于预先采样（在原始音频上进行采样）需要比祖先采样更多的内存，并且应该将 `fp16` 设置为 `True`。

该模型由 [Arthur Zucker](https://huggingface.co/ArthurZ) 贡献。原始代码可以在 [这里](https://github.com/openai/jukebox) 找到。

## JukeboxConfig

`transformers.JukeboxConfig` 类

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L495)

```py
( vqvae_config = None prior_config_list = None nb_priors = 3 sampling_rate = 44100 timing_dims = 64 min_duration = 0 max_duration = 600.0 max_nb_genres = 5 metadata_conditioning = True **kwargs )
```

参数

+   `vqvae_config` (`JukeboxVQVAEConfig`, *可选*) — `JukeboxVQVAE` 模型的配置。

+   `prior_config_list` (`List[JukeboxPriorConfig]`, *可选*) — 模型中每个 `JukeboxPrior` 的配置列表。原始架构使用了 3 个 priors。

+   `nb_priors` (`int`, *可选*, 默认为 3) — 将依次采样标记的先前模型数量。每个 prior 都是条件自回归（解码器）模型，除了顶部 prior 可以包括歌词编码器。可用的模型是使用顶部 prior 和 2 个上采样 prior 进行训练的。

+   `sampling_rate` (`int`, *optional*, defaults to 44100) — 原始音频的采样率。

+   `timing_dims` (`int`, *optional*, defaults to 64) — JukeboxRangeEmbedding 层的维度，相当于传统的位置嵌入层。定时嵌入层将当前采样音频中的绝对位置和相对位置转换为长度为`timing_dims`的张量，该张量将添加到音乐标记中。

+   `min_duration` (`int`, *optional*, defaults to 0) — 生成音频的最小持续时间

+   `max_duration` (`float`, *optional*, defaults to 600.0) — 生成音频的最大持续时间

+   `max_nb_genres` (`int`, *optional*, defaults to 5) — 可用于调节单个样本的最大流派数量。

+   `metadata_conditioning` (`bool`, *optional*, defaults to `True`) — 是否使用元数据调节，对应于艺术家、流派和最小/最大持续时间。

这是一个配置类，用于存储 JukeboxModel 的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读来自 PretrainedConfig 的文档以获取更多信息。使用默认值实例化配置将产生类似于[openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)架构的配置。

下采样和步幅用于确定输入序列的下采样。例如，下采样 = (5,3)，步幅 = (2, 2) 将使音频下采样为 2⁵ = 32，以获得第一级代码，以及 2**8 = 256，以获得第二级代码。这在训练顶层先验和上采样器时通常是正确的。

示例：

```py
>>> from transformers import JukeboxModel, JukeboxConfig

>>> # Initializing a Jukebox configuration
>>> configuration = JukeboxConfig()

>>> # Initializing a model from the configuration
>>> model = JukeboxModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

#### `from_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L598)

```py
( prior_configs: List vqvae_config: JukeboxVQVAEConfig **kwargs ) → export const metadata = 'undefined';JukeboxConfig
```

返回

JukeboxConfig

配置对象的一个实例

从剪辑文本模型配置和剪辑视觉模型配置实例化一个 JukeboxConfig（或派生类）的实例。

## JukeboxPriorConfig

### `class transformers.JukeboxPriorConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L143)

```py
( act_fn = 'quick_gelu' level = 0 alignment_head = 2 alignment_layer = 68 attention_multiplier = 0.25 attention_pattern = 'enc_dec_with_lyrics' attn_dropout = 0 attn_res_scale = False blocks = 64 conv_res_scale = None num_layers = 72 emb_dropout = 0 encoder_config = None encoder_loss_fraction = 0.4 hidden_size = 2048 init_scale = 0.2 is_encoder_decoder = True lyric_vocab_size = 80 mask = False max_duration = 600 max_nb_genres = 1 merged_decoder = True metadata_conditioning = True metadata_dims = [604, 7898] min_duration = 0 mlp_multiplier = 1.0 music_vocab_size = 2048 n_ctx = 6144 n_heads = 2 nb_relevant_lyric_tokens = 384 res_conv_depth = 3 res_conv_width = 128 res_convolution_multiplier = 1 res_dilation_cycle = None res_dilation_growth_rate = 1 res_downs_t = [3, 2, 2] res_strides_t = [2, 2, 2] resid_dropout = 0 sampling_rate = 44100 spread = None timing_dims = 64 zero_out = False **kwargs )
```

参数

+   `act_fn` (`str`, *optional*, defaults to `"quick_gelu"`) — 激活函数。

+   `alignment_head` (`int`, *optional*, defaults to 2) — 负责歌词和音乐之间对齐的头部。仅用于计算歌词到音频的对齐

+   `alignment_layer` (`int`, *optional*, defaults to 68) — 负责歌词和音乐之间对齐的层的索引。仅用于计算歌词到音频的对齐

+   `attention_multiplier` (`float`, *optional*, defaults to 0.25) — 用于定义注意力层隐藏维度的乘数系数。0.25 表示将使用模型宽度的 0.25。

+   `attention_pattern` (`str`, *optional*, defaults to `"enc_dec_with_lyrics"`) — 解码器使用的注意力模式

+   `attn_dropout` (`int`, *optional*, defaults to 0) — 解码器中注意力层后的丢弃概率。

+   `attn_res_scale` (`bool`, *optional*, defaults to `False`) — 是否在注意力调节器块中缩放残差。

+   `blocks` (`int`, *optional*, defaults to 64) — `block_attn` 中使用的块数。长度为 seq_len 的序列在`JukeboxAttention`层中被分解为`[blocks, seq_len // blocks]`。

+   `conv_res_scale` (`int`, *optional*) — 是否要在条件块中缩放残差。由于顶层先验没有条件块，因此默认值为`None`，不应修改。

+   `num_layers` (`int`, *optional*, 默认为 72) — 变压器架构的层数。

+   `emb_dropout` (`int`, *optional*, 默认为 0) — 歌词解码器中使用的嵌入丢失。

+   `encoder_config` (`JukeboxPriorConfig`, *optional*) — 对歌词先验进行建模的编码器配置。

+   `encoder_loss_fraction` (`float`, *optional*, 默认为 0.4) — 用于歌词编码器损失前面的乘法因子。

+   `hidden_size` (`int`, *optional*, 默认为 2048) — 注意力层的隐藏维度。

+   `init_scale` (`float`, *optional*, 默认为 0.2) — 先验模块的初始化比例。

+   `is_encoder_decoder` (`bool`, *optional*, 默认为`True`) — 先验是否为编码器-解码器模型。如果不是，并且`nb_relevant_lyric_tokens`大于 0，则应为歌词编码指定`encoder`参数。

+   `mask` (`bool`, *optional*, 默认为`False`) — 是否要屏蔽注意力中的先前位置。

+   `max_duration` (`int`, *optional*, 默认为 600) — 生成歌曲的最大支持持续时间（以秒为单位）。

+   `max_nb_genres` (`int`, *optional*, 默认为 1) — 可用于条件模型的最大流派数量。

+   `merged_decoder` (`bool`, *optional*, 默认为`True`) — 解码器和编码器输入是否合并。这用于分离的编码器-解码器架构

+   `metadata_conditioning` (`bool`, *optional*, 默认为`True`) — 是否要在艺术家和流派元数据上进行条件。

+   `metadata_dims` (`List[int]`, *optional*, 默认为`[604, 7898]`) — 用于训练先验模型的嵌入层的流派数量和艺术家数量。

+   `min_duration` (`int`, *optional*, 默认为 0) — 模型训练的生成音频的最小持续时间。

+   `mlp_multiplier` (`float`, *optional*, 默认为 1.0) — 用于定义 MLP 层隐藏维度的乘数系数。0.25 表示将使用模型宽度的 0.25。

+   `music_vocab_size` (`int`, *optional*, 默认为 2048) — 不同音乐标记的数量。应与`JukeboxVQVAEConfig.nb_discrete_codes`类似。

+   `n_ctx` (`int`, *optional*, 默认为 6144) — 每个先验的上下文标记数量。上下文标记是在生成音乐标记时所关注的音乐标记。

+   `n_heads` (`int`, *optional*, 默认为 2) — 注意力头的数量。

+   `nb_relevant_lyric_tokens` (`int`, *optional*, 默认为 384) — 在采样长度为`n_ctx`的单个窗口时使用的歌词标记数量

+   `res_conv_depth` (`int`, *optional*, 默认为 3) — 用于在`JukeboxMusicTokenConditioner`中上采样先前采样音频的`JukeboxDecoderConvBock`的深度。

+   `res_conv_width` (`int`, *optional*, 默认为 128) — 用于在`JukeboxMusicTokenConditioner`中上采样先前采样音频的`JukeboxDecoderConvBock`的宽度。

+   `res_convolution_multiplier` (`int`, *optional*, 默认为 1) — 用于缩放`JukeboxResConv1DBlock`的`hidden_dim`的乘数。

+   `res_dilation_cycle` (`int`, *optional*) — 用于定义`JukeboxMusicTokenConditioner`的扩张周期。通常类似于 VQVAE 相应级别中使用的周期。第一个先验不使用它，因为它不是基于上一级标记的。

+   `res_dilation_growth_rate` (`int`, *optional*, 默认为 1) — 用于`JukeboxMusicTokenConditioner`的每个卷积块之间的扩张增长率

+   `res_downs_t` (`List[int]`, *optional*, 默认为`[3, 2, 2]`) — 音频调节网络中使用的下采样率

+   `res_strides_t` (`List[int]`, *optional*, 默认为`[2, 2, 2]`) — 音频调节网络中使用的步幅

+   `resid_dropout` (`int`, *optional*, defaults to 0) — 注意力模式中使用的残差丢失。

+   `sampling_rate` (`int`, *optional*, defaults to 44100) — 用于训练的采样率。

+   `spread` (`int`, *optional*) — `summary_spread_attention`模式中使用的扩展

+   `timing_dims` (`int`, *optional*, defaults to 64) — 时间嵌入的维度。

+   `zero_out` (`bool`, *optional*, defaults to `False`) — 初始化时是否将卷积权重归零。

这是配置类，用于存储 JukeboxPrior 的配置。根据指定的参数实例化`JukeboxPrior`，定义模型架构。使用默认值实例化配置将产生类似于 openai/jukebox-1b-lyrics -1b-lyrics)架构顶级先验的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

## JukeboxVQVAEConfig

### `class transformers.JukeboxVQVAEConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/configuration_jukebox.py#L372)

```py
( act_fn = 'relu' nb_discrete_codes = 2048 commit = 0.02 conv_input_shape = 1 conv_res_scale = False embed_dim = 64 hop_fraction = [0.125, 0.5, 0.5] levels = 3 lmu = 0.99 multipliers = [2, 1, 1] res_conv_depth = 4 res_conv_width = 32 res_convolution_multiplier = 1 res_dilation_cycle = None res_dilation_growth_rate = 3 res_downs_t = [3, 2, 2] res_strides_t = [2, 2, 2] sample_length = 1058304 init_scale = 0.2 zero_out = False **kwargs )
```

参数

+   `act_fn` (`str`, *optional*, defaults to `"relu"`) — 模型的激活函数。

+   `nb_discrete_codes` (`int`, *optional*, defaults to 2048) — VQVAE 的代码数量。

+   `commit` (`float`, *optional*, defaults to 0.02) — 提交损失乘数。

+   `conv_input_shape` (`int`, *optional*, defaults to 1) — 音频通道数。

+   `conv_res_scale` (`bool`, *optional*, defaults to `False`) — 是否对`JukeboxResConv1DBlock`的残差进行缩放。

+   `embed_dim` (`int`, *optional*, defaults to 64) — 码书向量的嵌入维度。

+   `hop_fraction` (`List[int]`, *optional*, defaults to `[0.125, 0.5, 0.5]`) — 在继续采样过程时使用的非交叉窗口的比例。

+   `levels` (`int`, *optional*, defaults to 3) — VQVAE 中使用的分层级别数。

+   `lmu` (`float`, *optional*, defaults to 0.99) — 用于码书更新的指数移动平均系数。有关详细信息，请参阅原始[VQVAE 论文](https://arxiv.org/pdf/1711.00937v2.pdf)的附录 A.1

+   `multipliers` (`List[int]`, *optional*, defaults to `[2, 1, 1]`) — 每个级别使用的深度和宽度乘数。用于`res_conv_width`和`res_conv_depth`

+   `res_conv_depth` (`int`, *optional*, defaults to 4) — 编码器和解码器块的深度。如果没有使用`multipliers`，则每个级别的深度相同。

+   `res_conv_width` (`int`, *optional*, defaults to 32) — 编码器和解码器块的宽度。如果没有使用`multipliers`，则每个级别的宽度相同。

+   `res_convolution_multiplier` (`int`, *optional*, defaults to 1) — `JukeboxResConv1DBlock`中使用的隐藏维度的缩放因子。

+   `res_dilation_cycle` (`int`, *optional*) — `JukeboxResnet`中使用的膨胀周期值。如果使用 int，每个新的 Conv1 块的深度将减少`res_dilation_cycle`的幂。

+   `res_dilation_growth_rate` (`int`, *optional*, defaults to 3) — VQVAE 中使用的 Resnet 膨胀增长率（膨胀增长率 ** 深度）

+   `res_downs_t` (`List[int]`, *optional*, defaults to `[3, 2, 2]`) — 分层 VQ-VAE 每个级别的下采样率。

+   `res_strides_t` (`List[int]`, *optional*, defaults to `[2, 2, 2]`) — 分层 VQ-VAE 每个级别使用的步幅。

+   `sample_length` (`int`, *optional*, defaults to 1058304) — 提供 VQVAE 的最大输入形状。用于计算每个级别的输入形状。

+   `init_scale` (`float`, *optional*, defaults to 0.2) — 初始化比例。

+   `zero_out` (`bool`, *optional*, defaults to `False`) — 是否在初始化时将卷积权重归零。

这是用于存储 JukeboxVQVAE 配置的配置类。它用于根据指定的参数实例化`JukeboxVQVAE`，定义模型架构。使用默认值实例化配置将产生与[openai/jukebox-1b-lyrics](https://huggingface.co/openai/jukebox-1b-lyrics)架构的 VQVAE 类似的配置。

配置对象继承自 PretrainedConfig，可用于控制模型输出。阅读 PretrainedConfig 的文档以获取更多信息。

## JukeboxTokenizer

### `class transformers.JukeboxTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L59)

```py
( artists_file genres_file lyrics_file version = ['v3', 'v2', 'v2'] max_n_lyric_tokens = 512 n_genres = 5 unk_token = '<|endoftext|>' **kwargs )
```

参数

+   `artists_file` (`str`) — 包含艺术家和 id 之间映射的词汇文件的路径。默认文件支持“v2”和“v3”。

+   `genres_file` (`str`) — 包含流派和 id 之间映射的词汇文件的路径。

+   `lyrics_file` (`str`) — 包含歌词分词所需字符的词汇文件的路径。

+   `version` (`List[str]`, `optional`, default to `["v3", "v2", "v2"]`) — 分词器版本的列表。`5b-lyrics`的顶级先验模型是使用`v3`而不是`v2`进行训练的。

+   `n_genres` (`int`, `optional`, defaults to 1) — 用于作曲的最大流派数量。

+   `max_n_lyric_tokens` (`int`, `optional`, defaults to 512) — 保留的最大歌词标记数量。

+   `unk_token`（`str`，*optional*，默认为``"<|endoftext|>"``）--未知令牌。词汇表中没有的令牌无法转换为 ID，而是设置为该令牌。

构建一个 Jukebox 分词器。Jukebox 可以根据 3 种不同的输入进行条件化：

+   艺术家，唯一的 id 与提供的字典中的每位艺术家相关联。

+   流派，唯一的 id 与提供的字典中的每个流派相关联。

+   歌词，基于字符的分词。必须使用词汇表中包含的字符列表进行初始化。

这个分词器不需要训练。它应该能够处理不同数量的输入：因为模型的条件化可以在三个不同的查询上进行。如果未提供 None，则将使用默认值。

取决于模型应该被条件化的流派数量（`n_genres`）。

```py
>>> from transformers import JukeboxTokenizer

>>> tokenizer = JukeboxTokenizer.from_pretrained("openai/jukebox-1b-lyrics")
>>> tokenizer("Alan Jackson", "Country Rock", "old town road")["input_ids"]
[tensor([[   0,    0,    0, 6785,  546,   41,   38,   30,   76,   46,   41,   49,
           40,   76,   44,   41,   27,   30]]), tensor([[  0,   0,   0, 145,   0]]), tensor([[  0,   0,   0, 145,   0]])]
```

您可以通过在实例化此分词器时或在对某些文本调用它时传递`add_prefix_space=True`来避免这种行为，但由于模型不是以这种方式进行预训练的，因此可能会导致性能下降。

如果未提供任何内容，则流派和艺术家将随机选择或设置为 None

这个分词器继承自 PreTrainedTokenizer，其中包含大多数主要方法。用户应该参考：这个超类以获取有关这些方法的更多信息。

但是代码不允许这样做，只支持从各种流派进行作曲。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/tokenization_jukebox.py#L372)

```py
( save_directory: str filename_prefix: Optional = None )
```

参数

+   `save_directory` (`str`) — 保存的目录路径。如果不存在，将创建该目录。

+   `filename_prefix` (`Optional[str]`, *optional*) — 添加到分词器保存的文件名称前缀。

将分词器的词汇字典保存到提供的保存目录中。

## JukeboxModel

### `class transformers.JukeboxModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2291)

```py
( config )
```

参数

+   `config`（`JukeboxConfig`）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 from_pretrained()方法以加载模型权重。

用于音乐生成的基本 JUKEBOX 模型。支持 4 种采样技术：`primed_sample`、`upsample`、`continue_sample`和`ancestral_sample`。它没有`forward`方法，因为训练不是端到端的。如果要微调模型，建议使用`JukeboxPrior`类并分别训练每个先验。

此模型继承自 PreTrainedModel。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取与一般用法和行为相关的所有内容。

#### `ancestral_sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2573)

```py
( labels n_samples = 1 **sampling_kwargs )
```

参数

+   `labels`（`List[torch.LongTensor]`）- 长度为`n_sample`的列表，形状为`(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)`，元数据，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。

+   `n_samples`（`int`，*可选*，默认为 1）- 要并行生成的样本数量。

根据提供的`labels`生成音乐标记。将从所需的先验级别开始，并自动上采样序列。如果要创建音频，应调用`model.decode(tokens)`，这将使用 VQ-VAE 解码器将音乐标记转换为原始音频。

示例：

```py
>>> from transformers import AutoTokenizer, JukeboxModel, set_seed

>>> model = JukeboxModel.from_pretrained("openai/jukebox-1b-lyrics", min_duration=0).eval()
>>> tokenizer = AutoTokenizer.from_pretrained("openai/jukebox-1b-lyrics")

>>> lyrics = "Hey, are you awake? Can you talk to me?"
>>> artist = "Zac Brown Band"
>>> genre = "Country"
>>> metas = tokenizer(artist=artist, genres=genre, lyrics=lyrics)
>>> set_seed(0)
>>> music_tokens = model.ancestral_sample(metas.input_ids, sample_length=400)

>>> with torch.no_grad():
...     model.decode(music_tokens)[:, :10].squeeze(-1)
tensor([[-0.0219, -0.0679, -0.1050, -0.1203, -0.1271, -0.0936, -0.0396, -0.0405,
    -0.0818, -0.0697]])
```

#### `primed_sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2649)

```py
( raw_audio labels **sampling_kwargs )
```

参数

+   `raw_audio`（长度为`n_samples`的`List[torch.Tensor]`）- 用作每个将生成的样本的条件信息的原始音频列表。

+   `labels`（长度为`n_sample`的`List[torch.LongTensor]`，形状为`(self.levels, self.config.max_nb_genre + lyric_sequence_length)`）- 元数据列表，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。

+   `sampling_kwargs`（`Dict[Any]`）- `_sample`函数使用的各种额外采样参数。可以在`_sample`函数文档中看到参数的详细列表。

根据提供的`raw_audio`生成原始音频，该音频将用作每个生成级别的条件。音频被编码为音乐标记，使用 VQ-VAE 的 3 个级别。这些标记被用作每个级别的条件，这意味着不需要祖先抽样。

#### `continue_sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2619)

```py
( music_tokens labels **sampling_kwargs )
```

参数

+   `music_tokens`（长度为`self.levels`的`List[torch.LongTensor]`）- 一系列音乐标记，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于特定级别的生成。

+   `labels`（长度为`n_sample`的`List[torch.LongTensor]`，形状为`(self.levels, self.config.max_nb_genre + lyric_sequence_length)`）- 元数据列表，例如`artist_id`、`genre_id`和用于条件生成的完整歌词标记列表。

+   `sampling_kwargs` (`Dict[Any]`) — 由`_sample`函数使用的各种额外采样参数。参数的详细列表可以在`_sample`函数文档中看到。

生成先前生成标记的延续。

#### `upsample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2634)

```py
( music_tokens labels **sampling_kwargs )
```

参数

+   `music_tokens` (`List[torch.LongTensor]`，长度为`self.levels`) — 一系列音乐标记，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于某个级别的生成。

+   `labels` (`List[torch.LongTensor]`，长度为`n_sample`，形状为`(self.levels, self.config.max_nb_genre + lyric_sequence_length)` — 包括元数据，如`artist_id`、`genre_id`以及用于条件生成的完整歌词标记列表。

+   `sampling_kwargs` (`Dict[Any]`) — 由`_sample`函数使用的各种额外采样参数。参数的详细列表可以在`_sample`函数文档中看到。

使用级别`level`的先验对音乐标记序列进行上采样。

#### `_sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2434)

```py
( music_tokens labels sample_levels metas = None chunk_size = 32 sampling_temperature = 0.98 lower_batch_size = 16 max_batch_size = 16 sample_length_in_seconds = 24 compute_alignments = False sample_tokens = None offset = 0 save_results = True sample_length = None )
```

参数

+   `music_tokens` (`List[torch.LongTensor]`) — 音乐标记序列，长度为`self.levels`，将用作继续采样过程的上下文。应该有`self.levels`个张量，每个对应于某个级别的生成。

+   `labels` (`List[torch.LongTensor]`) — 长度为`n_sample`的列表，形状为`(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)`，包括`artist_id`、`genre_id`等元数据以及用于条件生成的完整歌词标记列表。

+   `sample_levels` (`List[int]`) — 要进行采样的期望级别列表。级别等同于先验列表中的先验索引

+   `metas` (`List[Any]`, *可选*) — 用于生成`labels`的元数据

+   `chunk_size` (`int`, *可选*, 默认为 32) — 音频块的大小，用于以块的形式填充内存，以防止 OOM 错误。更大的块意味着更快的内存填充但更多的消耗。

+   `sampling_temperature` (`float`, *可选*, 默认为 0.98) — 用于调整采样随机性的温度。

+   `lower_batch_size` (`int`, *可选*, 默认为 16) — 低级别先验的最大批处理大小

+   `max_batch_size` (`int`, *可选*, 默认为 16) — 顶层先验的最大批处理大小

+   `sample_length_in_seconds` (`int`, *可选*, 默认为 24) — 生成的长度（秒数）

+   `compute_alignments` (`bool`, *可选*, 默认为`False`) — 是否计算歌词和音频之间的对齐，使用顶级先验

+   `sample_tokens` (`int`, *可选*) — 每个级别应采样的精确标记数。这对于运行虚拟实验非常有用

+   `offset` (`int`, *可选*, 默认为 0) — 用作条件的音频偏移，对应于音乐中的起始样本。如果偏移大于 0，则歌词将被移位以考虑这一点

+   `save_results` (`bool`, *可选*, 默认为`True`) — 是否保存中间结果。如果为`True`，将生成一个以开始时间命名的文件夹。

+   `sample_length` (`int`, *可选*) — 生成的长度（样本数）。

用于生成音乐标记的核心采样函数。在每一步保存生成的原始音频，遍历提供的级别列表。

返回：torch.Tensor

示例：

```py
>>> from transformers import AutoTokenizer, JukeboxModel, set_seed
>>> import torch

>>> metas = dict(artist="Zac Brown Band", genres="Country", lyrics="I met a traveller from an antique land")
>>> tokenizer = AutoTokenizer.from_pretrained("openai/jukebox-1b-lyrics")
>>> model = JukeboxModel.from_pretrained("openai/jukebox-1b-lyrics", min_duration=0).eval()

>>> labels = tokenizer(**metas)["input_ids"]
>>> set_seed(0)
>>> zs = [torch.zeros(1, 0, dtype=torch.long) for _ in range(3)]
>>> zs = model._sample(zs, labels, [0], sample_length=40 * model.priors[0].raw_to_tokens, save_results=False)
>>> zs[0]
tensor([[1853, 1369, 1150, 1869, 1379, 1789,  519,  710, 1306, 1100, 1229,  519,
      353, 1306, 1379, 1053,  519,  653, 1631, 1467, 1229, 1229,   10, 1647,
     1254, 1229, 1306, 1528, 1789,  216, 1631, 1434,  653,  475, 1150, 1528,
     1804,  541, 1804, 1434]])
```

## JukeboxPrior

### `class transformers.JukeboxPrior`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L1769)

```py
( config: JukeboxPriorConfig level = None nb_priors = 3 vqvae_encoder = None vqvae_decoder = None )
```

参数

+   `config`（`JukeboxPriorConfig`）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

+   `level`（`int`，*可选*）— 先验的当前级别。应在范围`[0,nb_priors]`内。

+   `nb_priors`（`int`，*可选*，默认为 3）— 先验总数。

+   `vqvae_encoder`（`Callable`，*可选*）— VQVAE 编码器在模型前向传递中使用的编码方法。传递函数而不是 vqvae 模块以避免获取参数。

+   `vqvae_decoder`（`Callable`，*可选*）— 在模型的前向传递中使用的 VQVAE 解码器的解码方法。传递函数而不是 vqvae 模块以避免获取参数。

JukeboxPrior 类是各种条件和变压器的包装器。JukeboxPrior 可以被视为在音乐上训练的语言模型。它们对下一个`音乐标记`预测任务进行建模。如果定义了（歌词）`编码器`，它还对歌词上的`下一个字符`预测进行建模。可以基于定时、艺术家、流派、歌词和来自较低级先验的代码进行条件化。

#### `sample`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2059)

```py
( n_samples music_tokens = None music_tokens_conds = None metadata = None temp = 1.0 top_k = 0 top_p = 0.0 chunk_size = None sample_tokens = None )
```

参数

+   `n_samples`（`int`）— 要生成的样本数。

+   `music_tokens`（`List[torch.LongTensor]`，*可选*）— 当前级别上先前生成的标记。用作生成的上下文。

+   `music_tokens_conds`（`List[torch.FloatTensor]`，*可选*）— 由先前先验模型生成的上层音乐标记。如果生成不是基于上层标记的，则为`None`。

+   `metadata`（`List[torch.LongTensor]`，*可选*）— 包含包含艺术家、流派和歌词标记的元数据张量的列表。

+   `temp`（`float`，*可选*，默认为 1.0）— 采样温度。

+   `top_k`（`int`，*可选*，默认为 0）— 用于过滤的前 k 个概率。

+   `top_p`（`float`，*可选*，默认为 0.0）— 用于过滤的前 p 个概率。

+   `chunk_size`（`int`，*可选*）— 用于准备变压器缓存的块的大小。

+   `sample_tokens`（`int`，*可选*）— 要采样的标记数。

使用提供的条件和元数据对标记窗口进行祖先/主要采样。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L2227)

```py
( hidden_states: Tensor metadata: Optional decode: Optional = False get_preds: Optional = False )
```

参数

+   `hidden_states`（`torch.Tensor`）— 应为原始音频的隐藏状态

+   `metadata`（`List[torch.LongTensor]`，*可选*）— 包含歌词和元数据标记的元数据条件张量的列表。

+   `decode`（`bool`，*可选*，默认为`False`）— 是否解码编码为标记。

+   `get_preds`（`bool`，*可选*，默认为`False`）— 是否返回模型的实际预测。

使用`vqvae`编码器对隐藏状态进行编码，然后在`forward_tokens`函数中预测下一个标记。损失是`encoder`损失和`decoder`损失的总和。

## JukeboxVQVAE

### `class transformers.JukeboxVQVAE`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L595)

```py
( config: JukeboxVQVAEConfig )
```

参数

+   `config`（`JukeboxConfig`）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 from_pretrained()方法以加载模型权重。

Jukebox 中使用的分层 VQ-VAE 模型。该模型遵循[Will Williams, Sam Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty](https://arxiv.org/abs/2002.08111)的 Hierarchical VQVAE 论文。

该模型继承自 PreTrainedModel。查看超类文档以获取库为其所有模型实现的通用方法（例如下载或保存，调整输入嵌入，修剪头等）。

该模型还是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L739)

```py
( raw_audio: FloatTensor )
```

参数

+   `raw_audio` (`torch.FloatTensor`) — 音频输入，将被编码和解码。

VQ-VAE 的前向传递，将`raw_audio`编码为潜在状态，然后为每个级别解码。计算提交损失，确保编码器计算的嵌入接近码书向量。

示例：

```py
>>> from transformers import JukeboxVQVAE, set_seed
>>> import torch

>>> model = JukeboxVQVAE.from_pretrained("openai/jukebox-1b-lyrics").eval()
>>> set_seed(0)
>>> zs = [torch.randint(100, (4, 1))]
>>> model.decode(zs).shape
torch.Size([4, 8, 1])
```

#### `encode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L709)

```py
( input_audio start_level = 0 end_level = None bs_chunks = 1 )
```

参数

+   `input_audio`（`torch.Tensor`）— 将被编码为其离散表示的原始音频，使用码书。将为每个样本序列计算最接近码书的`code`。

+   `start_level`（`int`，*可选*，默认为 0）— 编码过程将开始的级别。默认为 0。

+   `end_level`（`int`，*可选*）— 编码过程将开始的级别。默认为 None。

+   `bs_chunks`（int，*可选*，默认为 1）— 要同时处理的原始音频块数。

将`input_audio`转换为由`music_tokens`组成的离散表示。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/jukebox/modeling_jukebox.py#L673)

```py
( music_tokens start_level = 0 end_level = None bs_chunks = 1 )
```

参数

+   `music_tokens`（`torch.LongTensor`）— 将通过使用码书解码为原始音频的音乐标记张量。每个音乐标记应该是码书中对应`code`向量的索引。

+   `start_level`（`int`，*可选*）— 解码过程将开始的级别。默认为 0。

+   `end_level`（`int`，*可选*）— 解码过程将开始的级别。默认为 None。

+   `bs_chunks`（int，*可选*）— 要同时处理的块数。

将输入的`music_tokens`转换为它们的`raw_audio`表示。
