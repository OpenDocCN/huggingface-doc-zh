# FLAVA

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flava](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/flava)

## 概述

FLAVA模型在[FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482)中由Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach和Douwe Kiela提出，并已被CVPR 2022接受。

该论文旨在创建一个统一的基础模型，可以跨越视觉、语言以及视觉-语言多模态任务。

论文摘要如下：

*最先进的视觉和视觉-语言模型依赖于大规模视觉-语言预训练，以在各种下游任务上获得良好的性能。通常，这些模型通常是跨模态（对比）或多模态（具有早期融合），但不是两者兼具；它们通常只针对特定的模态或任务。一个有前途的方向是使用一个单一的整体通用模型，作为“基础”，一次性针对所有模态 —— 一个真正的视觉和语言基础模型应该擅长视觉任务、语言任务以及视觉和语言任务。我们介绍FLAVA作为这样一个模型，并展示了在涵盖这些目标模态的35个任务范围内的出色性能。*

该模型由[aps](https://huggingface.co/aps)贡献。原始代码可在[此处](https://github.com/facebookresearch/multimodal/tree/main/examples/flava)找到。

## FlavaConfig

### `class transformers.FlavaConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L468)

```py
( image_config: Dict = None text_config: Dict = None multimodal_config: Dict = None image_codebook_config: Dict = None hidden_size: int = 768 layer_norm_eps: float = 1e-12 projection_dim: int = 768 init_codebook: bool = True logit_scale_init_value: float = 2.6592 initializer_range: float = 0.02 ce_ignore_index: int = -100 mim_weight: float = 1.0 mlm_weight: float = 1.0 global_contrastive_weight: float = 1.0 itm_weight: float = 1.0 mmm_image_weight: float = 1.0 mmm_text_weight: float = 1.0 global_backprop_contrastive: bool = True skip_unmasked_multimodal_encoder: bool = True return_loss: bool = True **kwargs )
```

参数

+   `text_config` (`dict`, *optional*) — 用于初始化[FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig)的配置选项字典。

+   `image_config` (`dict`, *optional*) — 用于初始化[FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig)的配置选项字典。

+   `multimodal_config` (`dict`, *optional*) — 用于初始化[FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig)的配置选项字典。

+   `hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化层的维度。

+   `layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon值。

+   `projection_dim` (`int`, *optional*, 默认为512) — 文本和图像投影层的维度。

+   `logit_scale_init_value` (`float`, *optional*, 默认为2.6592) — *logit_scale*参数的初始值。默认值与原始FLAVA/CLIP实现相同。

+   `initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `ce_ignore_index` (`int`, *optional*, 默认为-100) — 用于忽略的交叉熵索引。

+   `mim_weight` (`float`, *optional*, 默认为1.0) — 分配给MIM（Masked Image Modeling）单模态损失的权重。

+   `mlm_weight` (`float`, *optional*, 默认为1.0) — 分配给MLM（Masked Language Modeling）单模态损失的权重。

+   `global_contrastive_weight` (`float`, *optional*, 默认为1.0) — 分配给全局对比度交叉对齐损失的权重。

+   `itm_weight` (`float`, *optional*, 默认为1.0) — 分配给图像-文本匹配多模态损失的权重。

+   `mmm_image_weight` (`float`, *optional*, 默认为1.0) — 分配给MMM损失图像部分的权重。

+   `mmm_text_weight` (`float`, *optional*, 默认为1.0) — 分配给MMM损失文本部分的权重。

+   `global_backprop_contrastive` (`bool`, *optional*, 默认为`True`) — 是否在对比损失中通过所有工作器进行全局反向传播。

+   `skip_unmasked_multimodal_encoder`（`bool`，*可选*，默认为`True`）— 是否跳过运行未屏蔽的多模态编码器，其输出不被FLAVA损失使用。

+   `return_loss`（`bool`，*可选*，默认为`True`）— 是否返回损失或不返回

+   `kwargs`（*可选*）— 关键字参数的字典。

[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)是用于存储[FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)配置的配置类。根据指定的参数实例化FLAVA模型，定义文本模型、图像模型、图像码书和多模态模型配置。使用默认值实例化配置将产生类似于FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining

>>> # Initializing a FlavaConfig with style configuration
>>> configuration = FlavaConfig()

>>> # Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration
>>> model = FlavaModel(configuration)
>>> model_pre = FlavaForPreTraining(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
>>> configuration_pre = model_pre.config
```

#### `from_configs`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L742)

```py
( image_config: FlavaImageConfig text_config: FlavaTextConfig multimodal_config: FlavaMultimodalConfig image_codebook_config: FlavaImageCodebookConfig **kwargs ) → export const metadata = 'undefined';FlavaConfig
```

返回

[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)

配置对象的实例

从flava文本模型配置、flava图像模型配置、flava多模态模型和flava码书模型配置中实例化一个[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)（或派生类）。

## FlavaTextConfig

### `class transformers.FlavaTextConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L150)

```py
( vocab_size: int = 30522 type_vocab_size: int = 2 max_position_embeddings: int = 512 position_embedding_type: str = 'absolute' hidden_size: int = 768 num_hidden_layers: int = 12 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: str = 'gelu' hidden_dropout_prob: float = 0.0 attention_probs_dropout_prob: float = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 pad_token_id: int = 0 qkv_bias: bool = True **kwargs )
```

参数

+   `vocab_size`（`int`，*可选*，默认为30522）— BERT模型的词汇量。定义在调用[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)时可以表示的不同标记数量。

+   `type_vocab_size`（`int`，*可选*，默认为2）— 在调用[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)时传递的`token_type_ids`的词汇量大小。请注意，即使文本编码器允许`token_type_ids`的值为2，但对于仅文本的预训练和微调，类似于RoBERTa，只使用1。

+   `max_position_embeddings`（`int`，*可选*，默认为512）— 模型可能使用的最大序列长度。通常设置为较大的值以防万一（例如512、1024或2048）。对于VL，传递给模型的max_length为77。

+   `position_embedding_type`（`str`，*可选*，默认为`"absolute"`）— 位置嵌入的类型。选择`"absolute"`、`"relative_key"`、`"relative_key_query"`中的一个。对于位置嵌入，请使用`"absolute"`。有关`"relative_key"`的更多信息，请参考[Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155)。有关`"relative_key_query"`的更多信息，请参考[Improve Transformer Models with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658)中的*Method 4*。

+   `hidden_size`（`int`，*可选*，默认为768）— 编码器层和池化层的维度。

+   `num_hidden_layers`（`int`，*可选*，默认为12）— Transformer编码器中的隐藏层数。

+   `num_attention_heads`（`int`，*可选*，默认为12）— Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size`（`int`，*可选*，默认为3072）— Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。

这是用于存储[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)配置的配置类。根据指定的参数实例化FLAVA模型，定义模型架构。

使用默认值实例化配置将产生类似于FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import FlavaTextConfig, FlavaTextModel

>>> # Initializing a FlavaTextModel with  style configuration
>>> configuration = FlavaTextConfig()

>>> # Initializing a FlavaTextModel model (with random weights) from the style configuration
>>> model = FlavaTextModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FlavaImageConfig

### `class transformers.FlavaImageConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L31)

```py
( hidden_size: int = 768 num_hidden_layers: int = 12 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: int = 'gelu' hidden_dropout_prob: float = 0.0 attention_probs_dropout_prob: float = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 image_size: int = 224 patch_size: int = 16 num_channels: int = 3 qkv_bias: bool = True mask_token: bool = True vocab_size: int = 8192 **kwargs )
```

参数

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数量。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 16) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。

+   `mask_token` (`bool`, *optional*, defaults to `True`) — 是否使用掩码标记。用于FLAVA的MIM（Masked Image Modeling）损失。

+   `vocab_size` (`int`，*可选*，默认为8192) — 与[FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)一起用于FLAVA的MIM（Masked Image Modeling）损失的[FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook)的词汇表大小。

这是用于存储[FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)配置的配置类。根据指定的参数实例化FLAVA模型，定义模型架构。

使用默认值实例化配置将产生类似于FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import FlavaImageConfig, FlavaImageModel

>>> # Initializing a FlavaImageModel with  style configuration
>>> configuration = FlavaImageConfig()

>>> # Initializing a FlavaImageModel model (with random weights) from the style configuration
>>> model = FlavaImageModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FlavaMultimodalConfig

### `class transformers.FlavaMultimodalConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L280)

```py
( hidden_size: int = 768 num_hidden_layers: int = 6 num_attention_heads: int = 12 intermediate_size: int = 3072 hidden_act: int = 'gelu' hidden_dropout_prob: int = 0.0 attention_probs_dropout_prob: int = 0.0 initializer_range: float = 0.02 layer_norm_eps: float = 1e-12 qkv_bias: bool = True use_cls_token: bool = True **kwargs )
```

参数

+   `hidden_size` (`int`，*可选*，默认为768) — 编码器层和池化层的维度。

+   `num_hidden_layers` (`int`，*可选*，默认为6) — Transformer编码器中的隐藏层数。

+   `num_attention_heads` (`int`，*可选*，默认为12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`，*可选*，默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str`或`function`，*可选*，默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`，*可选*，默认为0.0) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。

+   `attention_probs_dropout_prob` (`float`，*可选*，默认为0.0) — 注意力概率的dropout比率。

+   `initializer_range` (`float`，*可选*，默认为0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`，*可选*，默认为1e-12) — 层归一化层使用的epsilon。

+   `qkv_bias` (`bool`，*可选*，默认为`True`) — 是否向查询、键和值添加偏置。

+   `use_cls_token` (`bool`，*可选*，默认为`True`) — 是否在多模态设置中使用额外的CLS标记。通常由FLAVA模型需要。

这是用于存储[FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel)配置的配置类。根据指定的参数实例化FLAVA模型，定义模型架构。

使用默认值实例化配置将产生类似于FLAVA [facebook/flava-full](https://huggingface.co/facebook/flava-full) 架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import FlavaMultimodalConfig, FlavaMultimodalModel

>>> # Initializing a FlavaMultimodalModel with  style configuration
>>> configuration = FlavaMultimodalConfig()

>>> # Initializing a FlavaMultimodalModel model (with random weights) from the style configuration
>>> model = FlavaMultimodalModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## FlavaImageCodebookConfig

### `class transformers.FlavaImageCodebookConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/configuration_flava.py#L383)

```py
( num_groups: int = 4 input_channels: int = 3 num_blocks_per_group: int = 2 hidden_size: int = 256 vocab_size: int = 8192 freeze: int = True initializer_range: float = 0.02 **kwargs )
```

## FlavaProcessor

### `class transformers.FlavaProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L28)

```py
( image_processor = None tokenizer = None **kwargs )
```

参数

+   `image_processor`（[FlavaImageProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageProcessor)，*可选*）— 图像处理器是必需的输入。

+   `tokenizer`（[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)，*可选*）— Tokenizer是必需的输入。

构造一个FLAVA处理器，将FLAVA图像处理器和FLAVA标记器包装成单个处理器。

[FlavaProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaProcessor)提供了[FlavaImageProcessor](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageProcessor)和[BertTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertTokenizerFast)的所有功能。有关更多信息，请参阅`__call__()`和[decode()](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaProcessor.decode)。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L131)

```py
( *args **kwargs )
```

这种方法将其所有参数转发给BertTokenizerFast的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参考此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/processing_flava.py#L138)

```py
( *args **kwargs )
```

这种方法将其所有参数转发给BertTokenizerFast的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参考此方法的文档字符串。

## FlavaFeatureExtractor

### `class transformers.FlavaFeatureExtractor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/feature_extraction_flava.py#L26)

```py
( *args **kwargs )
```

## FlavaImageProcessor

### `class transformers.FlavaImageProcessor`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/image_processing_flava.py#L135)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None return_image_mask: bool = False input_size_patches: int = 14 total_mask_patches: int = 75 mask_group_min_patches: int = 16 mask_group_max_patches: Optional = None mask_group_min_aspect_ratio: float = 0.3 mask_group_max_aspect_ratio: Optional = None return_codebook_pixels: bool = False codebook_do_resize: bool = True codebook_size: bool = None codebook_resample: int = <Resampling.LANCZOS: 1> codebook_do_center_crop: bool = True codebook_crop_size: int = None codebook_do_rescale: bool = True codebook_rescale_factor: Union = 0.00392156862745098 codebook_do_map_pixels: bool = True codebook_do_normalize: bool = True codebook_image_mean: Union = None codebook_image_std: Union = None **kwargs )
```

参数

+   `do_resize`（`bool`，*可选*，默认为`True`）— 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`中的`do_resize`参数覆盖。

+   `size`（`Dict[str, int]` *可选*，默认为`{"height" -- 224, "width": 224}`）：调整大小后的图像尺寸。可以被`preprocess`中的`size`参数覆盖。

+   `resample`（`PILImageResampling`，*可选*，默认为`PILImageResampling.BICUBIC`）— 如果调整图像大小，则要使用的重采样滤镜。可以被`preprocess`中的`resample`参数覆盖。

+   `do_center_crop`（`bool`，*可选*，默认为`True`）— 是否对图像进行中心裁剪。可以被`preprocess`中的`do_center_crop`参数覆盖。

+   `crop_size`（`Dict[str, int]` *可选*，默认为`{"height" -- 224, "width": 224}`）：中心裁剪后图像的大小`(crop_size["height"], crop_size["width"])`。可以被`preprocess`中的`crop_size`参数覆盖。

+   `do_rescale`（`bool`，*可选*，默认为`True`）— 是否按指定比例`rescale_factor`对图像进行重新缩放。可以被`preprocess`中的`do_rescale`参数覆盖。

+   `rescale_factor`（`int`或`float`，*可选*，默认为`1/255`）— 如果重新缩放图像，则要使用的比例因子。可以被`preprocess`中的`rescale_factor`参数覆盖。

+   `do_normalize`（`bool`，*可选*，默认为`True`）— 是否对图像进行归一化。可以被`preprocess`中的`do_normalize`参数覆盖。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_MEAN`) — 如果对图像进行标准化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_mean` 参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *optional*, 默认为 `IMAGENET_STANDARD_STD`) — 如果对图像进行标准化，则使用的标准差。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被 `preprocess` 方法中的 `image_std` 参数覆盖。

+   `return_image_mask` (`bool`, *optional*, 默认为 `False`) — 是否返回图像掩码。可以被 `preprocess` 中的 `return_image_mask` 参数覆盖。

+   `input_size_patches` (`int`, *optional*, 默认为 14) — 图像中高度和宽度方向的补丁数。14x14 = 总共 196 个补丁。可以被 `preprocess` 中的 `input_size_patches` 参数覆盖。

+   `total_mask_patches` (`int`, *optional*, 默认为 75) — 应该被遮蔽的总补丁数。可以被 `preprocess` 中的 `total_mask_patches` 参数覆盖。

+   `mask_group_min_patches` (`int`, *optional*, 默认为 16) — 应该被遮蔽的最小补丁数。可以被 `preprocess` 中的 `mask_group_min_patches` 参数覆盖。

+   `mask_group_max_patches` (`int`, *optional*) — 应该被遮蔽的最大补丁数。可以被 `preprocess` 中的 `mask_group_max_patches` 参数覆盖。

+   `mask_group_min_aspect_ratio` (`float`, *optional*, 默认为 0.3) — 掩码窗口的最小长宽比。可以被 `preprocess` 中的 `mask_group_min_aspect_ratio` 参数覆盖。

+   `mask_group_max_aspect_ratio` (`float`, *optional*) — 掩码窗口的最大长宽比。可以被 `preprocess` 中的 `mask_group_max_aspect_ratio` 参数覆盖。

+   `codebook_do_resize` (`bool`, *optional*, 默认为 `True`) — 是否将输入调整大小以适应码书。可以被 `preprocess` 中的 `codebook_size` 参数覆盖。

+   `codebook_size` (`Dict[str, int]`, *optional*, 默认为 `{"height" -- 224, "width": 224}`): 将输入调整大小以适应码书到指定大小。可以被 `preprocess` 中的 `codebook_size` 参数覆盖。

+   `codebook_resample` (`PILImageResampling`, *optional*, 默认为 `PILImageResampling.LANCZOS`) — 如果调整码书图像大小，则使用的重采样滤波器。可以被 `preprocess` 中的 `codebook_resample` 参数覆盖。

+   `codebook_do_center_crop` (`bool`, *optional*, 默认为 `True`) — 是否在码书输入中心裁剪输入。如果输入尺寸沿任何边缘小于 `codebook_crop_size`，则图像将填充为 0，然后进行中心裁剪。可以被 `preprocess` 中的 `codebook_do_center_crop` 参数覆盖。

+   `codebook_crop_size` (`Dict[str, int]`, *optional*, 默认为 `{"height" -- 224, "width": 224}`): 应用中心裁剪时，码书输入的期望输出大小。可以被 `preprocess` 中的 `codebook_crop_size` 参数覆盖。

+   `codebook_do_rescale` (`bool`, *optional*, 默认为 `True`) — 是否按照指定的比例因子 `codebook_rescale_factor` 重新缩放码书输入。可以被 `preprocess` 中的 `codebook_do_rescale` 参数覆盖。

+   `codebook_rescale_factor` (`int` 或 `float`, *optional*, 默认为 `1/255`) — 如果重新缩放码书图像，则定义要使用的比例因子。可以被 `preprocess` 中的 `codebook_rescale_factor` 参数覆盖。

+   `codebook_do_map_pixels` (`bool`, *optional*, 默认为 `True`) — 是否将码书输入的像素值映射到 (1 - 2e)x + e。可以被 `preprocess` 中的 `codebook_do_map_pixels` 参数覆盖。

+   `codebook_do_normalize` (`bool`, *optional*, 默认为 `True`) — 是否对用于 codebook 的输入进行规范化，使用 `codebook_image_mean` 和 `codebook_image_std`。可以被 `preprocess` 中的 `codebook_do_normalize` 参数覆盖。

+   `codebook_image_mean` (`Optional[Union[float, Iterable[float]]]`, *optional*, 默认为 `[0, 0, 0]`) — 每个通道的均值序列，在为 codebook 规范化图像时使用。可以被 `preprocess` 中的 `codebook_image_mean` 参数覆盖。

+   `codebook_image_std` (`Optional[Union[float, Iterable[float]]]`, *optional*, 默认为 `[0.5, 0.5, 0.5]`) — 每个通道的标准差序列，在为 codebook 规范化图像时使用。可以被 `preprocess` 中的 `codebook_image_std` 参数覆盖。

构建一个 Flava 图像处理器。

#### `preprocess`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/image_processing_flava.py#L447)

```py
( images: Union do_resize: Optional = None size: Dict = None resample: Resampling = None do_center_crop: Optional = None crop_size: Optional = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None return_image_mask: Optional = None input_size_patches: Optional = None total_mask_patches: Optional = None mask_group_min_patches: Optional = None mask_group_max_patches: Optional = None mask_group_min_aspect_ratio: Optional = None mask_group_max_aspect_ratio: Optional = None return_codebook_pixels: Optional = None codebook_do_resize: Optional = None codebook_size: Optional = None codebook_resample: Optional = None codebook_do_center_crop: Optional = None codebook_crop_size: Optional = None codebook_do_rescale: Optional = None codebook_rescale_factor: Optional = None codebook_do_map_pixels: Optional = None codebook_do_normalize: Optional = None codebook_image_mean: Optional = None codebook_image_std: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个图像或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *optional*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *optional*, 默认为 `self.size`) — 图像的大小。

+   `resample` (`int`, *optional*, 默认为 `self.resample`) — 如果调整图像大小，则使用的重采样滤波器。这可以是 `PILImageResampling` 枚举值之一，仅在 `do_resize` 设置为 `True` 时有效。

+   `do_center_crop` (`bool`, *optional*, 默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`, *optional*, 默认为 `self.crop_size`) — 中心裁剪的大小。仅在 `do_center_crop` 设置为 `True` 时有效。

+   `do_rescale` (`bool`, *optional*, 默认为 `self.do_rescale`) — 是否将图像值重新缩放在 [0 - 1] 之间。

+   `rescale_factor` (`float`, *optional*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则用于重新缩放图像的重新缩放因子。

+   `do_normalize` (`bool`, *optional*, 默认为 `self.do_normalize`) — 是否对图像进行规范化。

+   `image_mean` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_mean`) — 图像均值。

+   `image_std` (`float` 或 `List[float]`, *optional*, 默认为 `self.image_std`) — 图像标准差。

+   `return_image_mask` (`bool`, *optional*, 默认为 `self.return_image_mask`) — 是否返回图像掩码。

+   `input_size_patches` (`int`, *optional*, 默认为 `self.input_size_patches`) — 从图像中提取的补丁的大小。

+   `total_mask_patches` (`int`, *optional*, 默认为 `self.total_mask_patches`) — 从图像中提取的总补丁数。

+   `mask_group_min_patches` (`int`, *optional*, 默认为 `self.mask_group_min_patches`) — 从图像中提取的最小补丁数。

+   `mask_group_max_patches` (`int`, *optional*, 默认为 `self.mask_group_max_patches`) — 从图像中提取的补丁的最大数量。

+   `mask_group_min_aspect_ratio` (`float`, *optional*, 默认为 `self.mask_group_min_aspect_ratio`) — 从图像中提取的补丁的最小长宽比。

+   `mask_group_max_aspect_ratio` (`float`, *optional*, 默认为 `self.mask_group_max_aspect_ratio`) — 从图像中提取的补丁的最大长宽比。

+   `return_codebook_pixels` (`bool`, *optional*, 默认为 `self.return_codebook_pixels`) — 是否返回 codebook 像素。

+   `codebook_do_resize` (`bool`, *optional*, 默认为 `self.codebook_do_resize`) — 是否调整 codebook 像素的大小。

+   `codebook_size` (`Dict[str, int]`, *optional*, 默认为 `self.codebook_size`) — codebook 像素的大小。

+   `codebook_resample` (`int`, *可选*, 默认为 `self.codebook_resample`) — 如果调整码书像素大小，则要使用的重采样滤波器。这可以是枚举 `PILImageResampling` 中的一个。仅在 `codebook_do_resize` 设置为 `True` 时有效。

+   `codebook_do_center_crop` (`bool`, *可选*, 默认为 `self.codebook_do_center_crop`) — 是否对码书像素进行中心裁剪。

+   `codebook_crop_size` (`Dict[str, int]`, *可选*, 默认为 `self.codebook_crop_size`) — 码书像素中心裁剪的大小。仅在 `codebook_do_center_crop` 设置为 `True` 时有效。

+   `codebook_do_rescale` (`bool`, *可选*, 默认为 `self.codebook_do_rescale`) — 是否将码书像素值重新缩放到 [0 - 1] 之间。

+   `codebook_rescale_factor` (`float`, *可选*, 默认为 `self.codebook_rescale_factor`) — 如果 `codebook_do_rescale` 设置为 `True`，则用于重新缩放码书像素的重新缩放因子。

+   `codebook_do_map_pixels` (`bool`, *可选*, 默认为 `self.codebook_do_map_pixels`) — 是否映射码书像素值。

+   `codebook_do_normalize` (`bool`, *可选*, 默认为 `self.codebook_do_normalize`) — 是否对码书像素进行归一化。

+   `codebook_image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.codebook_image_mean`) — 如果 `codebook_do_normalize` 设置为 `True`，则用于归一化码书像素的码书像素均值。

+   `codebook_image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.codebook_image_std`) — 如果 `codebook_do_normalize` 设置为 `True`，则用于归一化码书像素的码书像素标准差。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置: 返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`: 返回类型为 `tf.Tensor` 的批次。

    +   `TensorType.PYTORCH` 或 `'pt'`: 返回类型为 `torch.Tensor` 的批次。

    +   `TensorType.NUMPY` 或 `'np'`: 返回类型为 `np.ndarray` 的批次。

    +   `TensorType.JAX` 或 `'jax'`: 返回类型为 `jax.numpy.ndarray` 的批次。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`: 图像以 (通道数、高度、宽度) 格式。

    +   `ChannelDimension.LAST`: 图像以 (高度、宽度、通道数) 格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数、高度、宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度、宽度、通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以 (高度、宽度) 格式。

预处理图像或图像批次。

## FlavaForPreTraining

### `class transformers.FlavaForPreTraining`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1714)

```py
( config: FlavaConfig image_codebook: Optional = None )
```

参数

+   `config` ([FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

+   `image_codebook` (`nn.Module`) — 如果传入，图像码书将设置为此值。否则，将使用配置中定义的 image_codebook_config 作为第一个参数进行初始化。

用于预训练的 FLAVA 模型，输出损失、嵌入、对数和变换器输出。

这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1764)

```py
( input_ids: Optional = None input_ids_masked: Optional = None pixel_values: Optional = None codebook_pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None bool_masked_pos: Optional = None position_ids: Optional = None image_attention_mask: Optional = None skip_unmasked_multimodal_encoder: bool = None mlm_labels: Optional = None mim_labels: Optional = None itm_labels: Optional = None output_attentions: Optional = None output_hidden_states: bool = True return_dict: Optional = None return_loss: Optional = None ) → export const metadata = 'undefined';transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids_masked`（形状为`(batch_size, text_seq_len)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。这些是原始任务的掩盖版本，用于MLM。索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)以及`DataCollatorForMaskedLanguageModeling`获取。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)

+   `input_ids`（形状为`(batch_size, text_seq_len)`的`torch.LongTensor`）— 词汇表中输入序列标记的索引。索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)

+   `token_type_ids`（形状为`(batch_size, text_seq_len)`的`torch.LongTensor`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `bool_masked_pos`（形状为`(batch_size, image_num_patches)`的`torch.BoolTensor`）— 布尔掩码位置。指示哪些补丁被掩盖（1），哪些没有（0）。

+   `interpolate_pos_encoding`（*布尔值*，*可选*）— 是否插值预训练位置编码。

+   `image_attention_mask`（形状为`(batch_size, image_num_patches)`的`torch.FloatTensor`，*可选*）— 用于避免特定于图像的填充令牌索引执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   1表示`未被掩盖`的标记，

    +   0表示被`掩盖`的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `skip_unmasked_multimodal_encoder`（*布尔值*，*可选*）— 跳过未掩盖输入的多模态编码器的任何计算。FLAVA预训练目前不需要未掩盖的多模态嵌入或输出。

+   `mlm_labels`（形状为`(batch_size, text_seq_len)`的`torch.LongTensor`，*可选*）— 用于计算从左到右的语言和多模态掩码建模损失（下一个词预测）的标签。索引应在`[-100, 0, ..., text_config.vocab_size - 1]`中（参见`input_ids`文档字符串）。索引设置为`-100`的标记将被忽略（掩盖），损失仅计算具有标签在`[0, ..., text_config.vocab_size - 1]`中的标记。

+   `mim_labels`（形状为`(batch_size, image_num_patches)`的`torch.LongTensor`，*可选*）- 用于计算图像和多模态掩码建模损失的标签。索引应在`[-100, 0, ..., image_config.vocab_size - 1]`中。将索引设置为`-100`的标记将被忽略（masked），仅对具有标签在`[0, ..., image_config.vocab_size - 1]`中的标记计算损失。如果未传递，则它们将使用分配给模型的图像码书自动生成。默认情况下，它使用[FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook)。请参阅[FlavaImageCodebook](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebook)以了解如何生成mim_labels。

+   `itm_labels`（形状为`(batch_size, 1)`的`torch.LongTensor`，*可选*）- 用于计算图像文本匹配损失的标签。0表示配对不匹配，1表示匹配。标签为0的配对也将被跳过用于计算MMM和全局对比损失。

+   `return_loss`（`bool`，*可选*，默认为None）- 是否返回计算的损失。

+   `attention_mask`（形状为`(batch_size, text_seq_len)`的`torch.FloatTensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`中：

    +   1表示未被`masked`的标记，

    +   0表示被`masked`的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部失效的掩码。掩码值选定在`[0, 1]`中：

    +   1表示头部未被`masked`，

    +   0表示头部是`masked`。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

    示例 -

返回

`transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput`或`tuple(torch.FloatTensor)`

一个`transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput`或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包括根据配置（`<class 'transformers.models.flava.configuration_flava.FlavaConfig'>`）和输入的不同元素。

+   `loss`（`torch.FloatTensor`，*可选*，当`return_loss`为True时返回）- 为此模型计算的总损失。

+   `loss_info`（`FlavaLosses`）- FLAVA预训练损失的详细信息。检查`FlavaLosses`类描述以获取关键信息。

+   `image_embeddings`（形状为`(batch_size, output_dim)`的`torch.FloatTensor`，*可选*，当存在`pixel_values`时返回）- 这些基本上是[FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)的汇总输出的图像嵌入。

+   `image_output`（`BaseModelOutputWithPooling`，*可选*，当存在`pixel_values`时返回）- [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)的输出。

+   `text_embeddings`（形状为`(batch_size, output_dim)`的`torch.FloatTensor`，*可选*，当存在`input_ids`时返回）- 这些基本上是[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的汇总输出的文本嵌入。

+   `text_output`（`BaseModelOutputWithPooling`，*可选*，当存在`input_ids`时返回）- [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的输出。

+   `multimodal_embeddings`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，当`input_ids`和`pixel_values`存在且`skip_unmasked_multimodal_encoder`为`None`或`False`时返回）- 这些多模态嵌入基本上是[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的汇总输出。

+   `multimodal_output`（`BaseModelOutputWithPooling`，当`input_ids`和`pixel_values`存在且`skip_unmasked_multimodal_encoder`为`None`或`False`时返回）- [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel)的输出。

+   `image_masked_embeddings`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，当`pixel_values`存在时返回）- 这些图像嵌入基本上是[FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)的汇总输出。使用`bool_masked_pos`来创建被屏蔽的图像。

+   `image_masked_output`（`BaseModelOutputWithPooling`，*可选*，当`pixel_values`存在时返回）- [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)的输出。使用`bool_masked_pos`来创建被屏蔽的图像。

+   `text_masked_embeddings`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，当`input_ids_masked`存在时返回）- 这些文本嵌入基本上是[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的汇总输出。

+   `text_masked_output`（`BaseModelOutputWithPooling`，*可选*，当`input_ids_masked`存在时返回）- [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的输出。

+   `multimodal_masked_embeddings`（`torch.FloatTensor`，形状为`(batch_size, output_dim)`，*可选*，当`input_ids`和`pixel_values`存在时返回）- 这些多模态嵌入基本上是[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的汇总输出。

+   `multimodal_masked_output`（`BaseModelOutputWithPooling`，当`input_ids_masked`和`pixel_values`存在时返回）- [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel)的输出。

+   `mim_logits`（`torch.FloatTensor`，形状为`(batch_size, num_image_patches, image_vocab_size)`或形状为`(total_masked_patches, image_vocab_size)`，*可选*，当`pixel_values`存在且`input_ids_masked`不存在时返回）- MIM单模态损失的logits。使用`book_masked_pos`来获取被屏蔽的补丁。当`bool_masked_pos`中有一些补丁被屏蔽时，返回扁平化的输出。

+   `mlm_logits`（`torch.FloatTensor`，形状为`(batch_size, text_seq_length, text_vocab_size)`或形状为`(total_masked_seq_length, text_vocab_size)`，*可选*，当`input_ids_masked`存在且`pixel_values`不存在时返回）- MLM单模态损失的logits。当`input_ids_masked`中有一些标记被屏蔽时，返回扁平化的输出。

+   `itm_logits`（`torch.FloatTensor`，形状为`(batch_size, 2)`，*可选*，当`input_ids_masked`和`pixel_values`存在时返回）- ITM损失的logits。请注意，ITM损失是在FLAVA中对被屏蔽的配对进行计算的。

+   `mmm_image_logits`（`torch.FloatTensor`，形状为`(batch_size, num_image_patches, image_vocab_size)`或形状为`(total_masked_patches, image_vocab_size)`，*可选*，当`pixel_values`和`input_ids_masked`存在时返回）- MMM图像多模态损失的logits。使用`book_masked_pos`来获取被屏蔽的补丁。当`bool_masked_pos`中有一些补丁被屏蔽时，返回扁平化的输出。

+   `mmm_text_logits`（形状为`(batch_size, text_seq_length, text_vocab_size)`或形状为`(total_masked_seq_length, text_vocab_size)`的`torch.FloatTensor`，*可选*，当`pixel_values`和`input_ids_masked`存在时返回）- 用于MMM文本多模态损失的logits。当`input_ids_masked`中有一些标记被屏蔽时，返回扁平化的输出。

+   `contrastive_logits_per_image`（形状为`(image_batch_size, text_batch_size)`的`torch.FloatTensor`）- `image_embeddings`和`text_embeddings`之间的缩放点积分数，但分别通过FLAVA的`image_projection`和`text_projection`层。这代表了图像文本相似性得分。这是在未屏蔽的图像和文本上计算的。

+   `contrastive_logits_per_text`（形状为`(text_batch_size, image_batch_size)`的`torch.FloatTensor`）- `text_embeddings`和`image_embeddings`之间的缩放点积分数，但分别通过FLAVA的`text_projection`和`image_projection`层。这是在未屏蔽的图像和文本上计算的。

[FlavaForPreTraining](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaForPreTraining)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

## FlavaModel

### `class transformers.FlavaModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1179)

```py
( config: FlavaConfig )
```

参数

+   `config`（[FlavaConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaConfig)）- 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的FLAVA模型变压器输出原始隐藏状态，没有特定的头部。这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1323)

```py
( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None token_type_ids: Optional = None bool_masked_pos: Optional = None position_ids: Optional = None image_attention_mask: Optional = None skip_multimodal_encoder: Optional = None output_attentions: Optional = None output_hidden_states: bool = True return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.flava.modeling_flava.FlavaModelOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `bool_masked_pos`（形状为`(batch_size, image_num_patches)`的`torch.BoolTensor`）- 布尔掩码位置。指示哪些补丁被屏蔽（1），哪些没有（0）。

+   `interpolate_pos_encoding`（`bool`，*可选*）- 是否插值预训练位置编码。

+   `input_ids`（形状为`(batch_size, image_num_patches + text_seq_len)`的`torch.LongTensor`）- 词汇表中输入序列标记的索引。索引可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)

+   `token_type_ids` (`torch.LongTensor`，形状为 `(batch_size, image_num_patches + text_seq_len)`，*可选*) — 段令牌索引，指示输入的第一部分和第二部分。 索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 令牌，

    +   1 对应于 *句子 B* 令牌。 [什么是令牌类型 ID？](../glossary#token-type-ids)

+   `attention_mask` (`torch.FloatTensor`，形状为 `(batch_size, image_num_patches + text_seq_len)`，*可选*) — 用于避免在填充令牌索引上执行注意力的掩码。 选择的掩码值为 `[0, 1]`：

    +   1 表示未被“掩盖”的令牌。

    +   0 表示被“掩盖”的令牌。 [什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask` (`torch.FloatTensor`，形状为 `(num_heads,)` 或 `(num_layers, num_heads)`，*可选*) — 用于使自注意力模块的选定头部无效的掩码。 选择的掩码值在 `[0, 1]` 中：

    +   1 表示头部未被“掩盖”，

    +   0 表示头部被“掩盖”。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `skip_multimodal_encoder` (*bool*，*可选*) — 跳过多模态编码的任何计算。 如果不打算使用多模态编码，则此选项很有用。

返回

`transformers.models.flava.modeling_flava.FlavaModelOutput` 或 `tuple(torch.FloatTensor)`

一个 `transformers.models.flava.modeling_flava.FlavaModelOutput` 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时），包括根据配置 (`<class 'transformers.models.flava.configuration_flava.FlavaConfig'>`) 和输入而异的各种元素。

+   `image_embeddings` (`torch.FloatTensor`，形状为 `(batch_size, output_dim)`，*可选*，当 `pixel_values` 存在时返回) — 基本上是 [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel) 的汇总输出的图像嵌入。

+   `image_output` (`BaseModelOutputWithPooling`，*可选*，当 `pixel_values` 存在时返回) — [FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel) 的输出。

+   `text_embeddings` (`torch.FloatTensor`，形状为 `(batch_size, output_dim)`，*可选*，当 `input_ids` 存在时返回) — 基本上是 [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel) 的汇总输出的文本嵌入。

+   `text_output` (`BaseModelOutputWithPooling`，*可选*，当 `input_ids` 存在时返回) — [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel) 的输出。

+   `multimodal_embeddings` (`torch.FloatTensor`，形状为 `(batch_size, output_dim)`，*可选*，当 `input_ids` 和 `pixel_values` 存在且 `skip_multimodal_encoder` 为 `None` 或 `False` 时返回) — 基本上是 [FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel) 的汇总输出的多模态嵌入。

+   `multimodal_output` (`BaseModelOutputWithPooling`，当 `input_ids` 和 `pixel_values` 存在且 `skip_multimodal_encoder` 为 `None` 或 `False` 时返回) — [FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel) 的输出。

[FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel) 的前向方法，覆盖了 `__call__` 特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from PIL import Image
>>> import requests
>>> from transformers import AutoProcessor, FlavaModel

>>> model = FlavaModel.from_pretrained("facebook/flava-full")
>>> processor = AutoProcessor.from_pretrained("facebook/flava-full")

>>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

>>> outputs = model(**inputs)

>>> image_embeddings = outputs.image_embeddings
>>> text_embeddings = outputs.text_embeddings
>>> multimodal_embeddings = outputs.multimodal_embeddings

>>> outputs.image_embeddings.shape
torch.Size([1, 197, 768])

>>> text_embeddings.shape
torch.Size([1, 7, 768])

>>> multimodal_embeddings.shape
torch.Size([1, 205, 768])
```

#### `get_text_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1229)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `input_ids`（形状为`(batch_size, text_seq_length)`的`torch.LongTensor`）— 输入序列标记在词汇表中的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。[什么是输入ID？](../glossary#input-ids)

+   `token_type_ids`（形状为`(batch_size, text_seq_length)`的`torch.LongTensor`，*可选*）— 段标记索引，指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)

+   `attention_mask`（形状为`(batch_size, text_seq_length)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被“masked”的标记，

    +   对于被`masked`掉的标记，值为0。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块的选定头部失效的掩码。掩码值选择在`[0, 1]`中：

    +   1表示头部未被“masked”,

    +   0表示头部被`masked`。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

[FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

#### `get_image_features`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1273)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None interpolate_pos_encoding: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None )
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `bool_masked_pos`（形状为`(batch_size, image_num_patches)`的`torch.BoolTensor`）— 布尔掩码位置。指示哪些补丁被掩盖（1），哪些没有（0）。

+   `interpolate_pos_encoding`（`bool`，*可选*）— 是否插值预训练位置编码。

+   `attention_mask`（形状为`(batch_size, image_num_patches)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选择在`[0, 1]`之间：

    +   1表示未被“masked”的标记，

    +   对于被`masked`掉的标记，值为0。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中选择的头部失效的掩码。掩码值选在`[0, 1]`之间：

    +   1表示头部`未被掩盖`,

    +   0表示头部`被掩盖`。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

[FlavaModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## FlavaImageCodebook

### `class transformers.FlavaImageCodebook`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1500)

```py
( config: FlavaImageCodebookConfig **kwargs: Any )
```

参数

+   `config` ([FlavaImageCodebookConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageCodebookConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

FLAVA的图像代码簿模型受到DALL-E原始编码器的启发。输出原始隐藏状态，可用于根据DALL-E的词汇为基于图像的MIM生成图像标记。用于为MIM生成图像的图像标记，请使用`get_codebook_indices`。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1590)

```py
( pixel_values: FloatTensor )
```

#### `get_codebook_indices`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1558)

```py
( pixel_values: Tensor )
```

#### `get_codebook_probs`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1586)

```py
( pixel_values: Tensor )
```

## FlavaTextModel

### `class transformers.FlavaTextModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L976)

```py
( config: FlavaTextConfig add_pooling_layer: bool = True )
```

参数

+   `config` ([FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的FLAVA文本模型变压器输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1011)

```py
( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None position_ids: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`) — Indices of input sequence tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer). See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) for details. [What are input IDs?](../glossary#input-ids)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, text_seq_length)`, *optional*) — 段标记索引，用于指示输入的第一部分和第二部分。索引选定在`[0, 1]`之间：

    +   0对应于*句子A*标记。

    +   1对应于*句子B*标记。[什么是标记类型ID？](../glossary#token-type-ids)

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, text_seq_length)`, *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示未被掩盖的标记，

    +   0表示被掩盖的标记。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) — 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`的元组（如果传递`return_dict=False`或当`config.return_dict=False`时），包括根据配置（[FlavaTextConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextConfig)）和输入的不同元素。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态的序列。

+   `pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) — Last layer hidden-state of the first token of the sequence (classification token) after further processing through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns the classification token after processing through a linear layer and a tanh activation function. The linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。

    模型每一层的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[FlavaTextModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaTextModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlavaTextModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/flava-full")
>>> model = FlavaTextModel.from_pretrained("facebook/flava-full")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## FlavaImageModel

### `class transformers.FlavaImageModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L877)

```py
( config: FlavaImageConfig add_pooling_layer: bool = True )
```

参数

+   `config`（[FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的FLAVA图像模型变换器输出没有特定头部的原始隐藏状态。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L914)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None interpolate_pos_encoding: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[FlavaImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `bool_masked_pos`（形状为`(batch_size, image_num_patches)`的`torch.BoolTensor`）— 布尔掩码位置。指示哪些补丁被掩盖（1），哪些没有（0）。

+   `interpolate_pos_encoding`（`bool`，*可选*）— 是否插值预训练位置编码。

+   `attention_mask`（`torch.FloatTensor`，形状为`(batch_size, image_num_patches)`，*可选*）— 用于避免在填充令牌索引上执行注意力的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*可选*）— 用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`之间：

    +   1表示头部未被`masked`。

    +   0表示头部被`masked`。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，具体取决于配置（[FlavaImageConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageConfig)）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）— 模型最后一层的隐藏状态的序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）— 经过辅助预训练任务的最后一层隐藏状态的序列第一个标记（分类标记）在通过用于辅助预训练任务的层进一步处理后的输出。例如，对于BERT系列模型，这将返回通过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出+每层的输出）。

    模型在每一层的输出处的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[FlavaImageModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaImageModel)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, FlavaImageModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("facebook/flava-full")
>>> model = FlavaImageModel.from_pretrained("facebook/flava-full")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## FlavaMultimodalModel

### `class transformers.FlavaMultimodalModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1081)

```py
( config: FlavaMultimodalConfig add_pooling_layer = True )
```

参数

+   `config`（[FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的FLAVA多模型变压器输出原始隐藏状态，没有特定的头部。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/flava/modeling_flava.py#L1113)

```py
( hidden_states: Tensor attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `hidden_states`（形状为`(batch_size, image_num_patches + text_seq_len, hidden_size)`的`torch.FloatTensor`）— 单模编码器的连接隐藏状态。

+   `attention_mask`（形状为`(batch_size, image_num_patches + text_seq_len)`的`torch.FloatTensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：

    +   1表示`未被掩码`的标记，

    +   对于被`masked`的标记为0。[什么是注意力掩码？](../glossary#attention-mask)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*） — 用于使自注意力模块中选择的头部失效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被`masked`，

    +   0表示头部被`masked`。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[FlavaMultimodalConfig](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalConfig)）和输入而异的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`） — 模型最后一层的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`） — 经过用于辅助预训练任务的层进一步处理后，序列第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入的输出和每层的输出各一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[FlavaMultimodalModel](/docs/transformers/v4.37.2/en/model_doc/flava#transformers.FlavaMultimodalModel) 的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, FlavaMultimodalModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("facebook/flava-full")
>>> model = FlavaMultimodalModel.from_pretrained("facebook/flava-full")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```
