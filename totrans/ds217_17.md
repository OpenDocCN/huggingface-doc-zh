# 处理

> 原始文本：[https://huggingface.co/docs/datasets/process](https://huggingface.co/docs/datasets/process)

🤗 Datasets提供了许多工具来修改数据集的结构和内容。这些工具对于整理数据集、创建额外列、在特征和格式之间转换等方面非常重要。

本指南将向您展示如何：

+   重新排序行并拆分数据集。

+   重命名和删除列，以及其他常见的列操作。

+   对数据集中的每个示例应用处理函数。

+   连接数据集。

+   应用自定义格式转换。

+   保存和导出处理过的数据集。

有关处理其他数据集模态的详细信息，请查看[处理音频数据集指南](./audio_process)、[处理图像数据集指南](./image_process)或[处理文本数据集指南](./nlp_process)。

本指南中的示例使用MRPC数据集，但请随意加载您选择的任何数据集并跟随操作！

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("glue", "mrpc", split="train")
```

本指南中的所有处理方法都会返回一个新的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象。修改不是就地进行的。请注意不要覆盖您之前的数据集！

## 排序、洗牌、选择、拆分和分片

有几个函数用于重新排列数据集的结构。这些函数对于仅选择您想要的行、创建训练和测试拆分以及将非常大的数据集分成较小的块非常有用。

### 排序

使用[sort()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.sort)根据其数值值对列值进行排序。提供的列必须是NumPy兼容的。

```py
>>> dataset["label"][:10]
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
>>> sorted_dataset = dataset.sort("label")
>>> sorted_dataset["label"][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> sorted_dataset["label"][-10:]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

在内部，这会创建一个根据列的值排序的索引列表。然后使用这个索引映射来访问底层Arrow表中的正确行。

### 洗牌

[shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)函数会随机重新排列列值。您可以在此函数中指定`generator`参数，以使用不同的`numpy.random.Generator`来更好地控制用于洗牌数据集的算法。

```py
>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)
>>> shuffled_dataset["label"][:10]
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]
```

洗牌会取得索引列表`[0:len(my_dataset)]`并对其进行洗牌以创建一个索引映射。然而，一旦您的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)有了一个索引映射，速度可能会变慢10倍。这是因为需要额外的步骤来使用索引映射获取要读取的行索引，最重要的是，您不再读取连续的数据块。为恢复速度，您需要再次使用[Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices)将整个数据集重写到磁盘上，以删除索引映射。或者，您可以切换到[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)并利用其快速的近似洗牌[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)：

```py
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)
>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### 选择和过滤

在数据集中有两种过滤行的选项：[select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)和[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)。

+   [select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)根据索引列表返回行：

```py
>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
>>> len(small_dataset)
6
```

+   [filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)返回符合指定条件的行：

```py
>>> start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
>>> len(start_with_ar)
6
>>> start_with_ar["sentence1"]
['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',
'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',
'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',
"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .",
'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'
]
```

[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)也可以通过设置`with_indices=True`来按索引进行过滤：

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> len(even_dataset)
1834
>>> len(dataset) / 2
1834.0
```

除非要保留的索引列表是连续的，否则这些方法在内部也会创建一个索引映射。

### 拆分

[train_test_split()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.train_test_split)函数在数据集没有训练和测试拆分时创建训练和测试拆分。这允许您调整每个拆分中的相对比例或绝对数量的样本。在下面的示例中，使用`test_size`参数创建一个原始数据集的10%的测试拆分：

```py
>>> dataset.train_test_split(test_size=0.1)
{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),
'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}
>>> 0.1 * len(dataset)
366.8
```

默认情况下拆分是随机的，但您可以设置`shuffle=False`来防止洗牌。

### 分片

🤗 数据集支持分片，将一个非常大的数据集分成预定义数量的块。在[shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard)中指定`num_shards`参数以确定要将数据集分割成的分片数。您还需要提供要返回的分片的`index`参数。

例如，[imdb](https://huggingface.co/datasets/imdb)数据集有25000个示例：

```py
>>> from datasets import load_dataset
>>> datasets = load_dataset("imdb", split="train")
>>> print(dataset)
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
```

将数据集分成四个块后，第一个块将只有6250个示例：

```py
>>> dataset.shard(num_shards=4, index=0)
Dataset({
    features: ['text', 'label'],
    num_rows: 6250
})
>>> print(25000/4)
6250.0
```

## 重命名、移除、转换和展平

以下函数允许您修改数据集的列。这些函数对重命名或移除列、将列更改为新的特征集以及展平嵌套列结构很有用。

### 重命名

当需要在数据集中重命名列时，请使用[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)。与原始列相关的特征实际上被移动到新列名下，而不仅仅是替换原始列。

使用原始列的名称和新列名称提供[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)：

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.rename_column("sentence1", "sentenceA")
>>> dataset = dataset.rename_column("sentence2", "sentenceB")
>>> dataset
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
```

### 移除

当需要移除一个或多个列时，提供要移除的列名给[remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)函数。通过提供列名列表来移除多个列：

```py
>>> dataset = dataset.remove_columns("label")
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.remove_columns(["sentence1", "sentence2"])
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

相反，[select_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select_columns)选择要保留的一个或多个列，并移除其余列。此函数接受一个或列名列表：

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns('idx')
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

### 转换

[cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast)函数转换一个或多个列的特征类型。此函数接受您的新[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)作为其参数。下面的示例演示了如何更改[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)和[Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)特征：

```py
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=["negative", "positive"])
>>> new_features["idx"] = Value("int64")
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

仅当原始特征类型和新特征类型兼容时，转换才起作用。例如，如果原始列仅包含1和0，则可以将具有特征类型`Value("int32")`的列转换为`Value("bool")`。

使用[cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)函数来更改单个列的特征类型。将列名和其新特征类型作为参数传递：

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

### 展平

有时，一列可以是几种类型的嵌套结构。看一下来自SQuAD数据集的下面的嵌套结构：

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("squad", split="train")
>>> dataset.features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
'context': Value(dtype='string', id=None),
'id': Value(dtype='string', id=None),
'question': Value(dtype='string', id=None),
'title': Value(dtype='string', id=None)}
```

`answers`字段包含两个子字段：`text`和`answer_start`。使用[flatten()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten)函数将子字段提取到它们自己的独立列中：

```py
>>> flat_dataset = dataset.flatten()
>>> flat_dataset
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
 num_rows: 87599
})
```

注意子字段现在是独立的列：`answers.text`和`answers.answer_start`。

## 映射

🤗 Datasets的一些更强大的应用来自于使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)函数。[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)的主要目的是加速处理函数。它允许您将处理函数应用于数据集中的每个示例，独立地或批处理。该函数甚至可以创建新的行和列。

在以下示例中，将数据集中每个`sentence1`值的前缀设置为'My sentence: '。

首先创建一个函数，将'My sentence: '添加到每个句子的开头。该函数需要接受并输出一个`dict`：

```py
>>> def add_prefix(example):
...     example["sentence1"] = 'My sentence: ' + example["sentence1"]
...     return example
```

现在使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)将`add_prefix`函数应用于整个数据集：

```py
>>> updated_dataset = small_dataset.map(add_prefix)
>>> updated_dataset["sentence1"][:5]
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
```

让我们看另一个例子，除了这次，您将使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)删除一个列。当您删除一列时，只有在提供示例给映射函数后才会删除。这使得映射函数可以在删除之前使用列的内容。

使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)中的`remove_columns`参数指定要移除的列：

```py
>>> updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])
>>> updated_dataset.column_names
['sentence2', 'label', 'idx', 'new_sentence']
```

🤗 Datasets还有一个[remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)函数，它更快，因为它不会复制剩余列的数据。

如果设置`with_indices=True`，还可以使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)与索引一起使用。下面的示例将索引添加到每个句子的开头：

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
>>> updated_dataset["sentence2"][:5]
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 "1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
 "2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
 '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
 '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
```

### 多进程

多进程通过在CPU上并行处理进程显著加快处理速度。在[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)中设置`num_proc`参数以设置要使用的进程数：

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, num_proc=4)
```

如果设置`with_rank=True`，[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)还可以与进程的等级一起使用。这类似于`with_indices`参数。如果已经存在`index`，则映射函数中的`with_rank`参数在`index`之后。

```py
>>> import torch
>>> from multiprocess import set_start_method
>>> from transformers import AutoTokenizer, AutoModelForCausalLM 
>>> from datasets import load_dataset
>>> 
>>> # Get an example dataset
>>> dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")
>>> 
>>> # Get an example model and its tokenizer 
>>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat").eval()
>>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat")
>>>
>>> def gpu_computation(batch, rank):
...     # Move the model on the right GPU if it's not there already
...     device = f"cuda:{(rank or 0) % torch.cuda.device_count()}"
...     model.to(device)
...     
...     # Your big GPU call goes here, for example:
...     chats = [[
...         {"role": "system", "content": "You are a helpful assistant."},
...         {"role": "user", "content": prompt}
...     ] for prompt in batch["prompt"]]
...     texts = [tokenizer.apply_chat_template(
...         chat,
...         tokenize=False,
...         add_generation_prompt=True
...     ) for chat in chats]
...     model_inputs = tokenizer(texts, padding=True, return_tensors="pt").to(device)
...     with torch.no_grad():
...         outputs = model.generate(**model_inputs, max_new_tokens=512)
...     batch["output"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)
...     return batch
>>>
>>> if __name__ == "__main__":
...     set_start_method("spawn")
...     updated_dataset = dataset.map(
...         gpu_computation,
...         batched=True,
...         batch_size=16,
...         with_rank=True,
...         num_proc=torch.cuda.device_count(),  # one process per GPU
...     )
```

等级的主要用例是在多个GPU上并行计算。这需要设置`multiprocess.set_start_method("spawn")`。如果不这样做，您将收到以下CUDA错误：

```py
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.
```

### 批处理

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)函数支持处理示例批次。通过设置`batched=True`来操作批次。默认批处理大小为1000，但您可以使用`batch_size`参数进行调整。批处理使得可以将长句子拆分为较短的块和数据增强等有趣的应用。

#### 拆分长示例

当示例太长时，您可能希望将其拆分为几个较小的块。首先创建一个函数：

1.  将`sentence1`字段拆分为50个字符的块。

1.  将所有块堆叠在一起以创建新数据集。

```py
>>> def chunk_examples(examples):
...     chunks = []
...     for sentence in examples["sentence1"]:
...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
...     return {"chunks": chunks}
```

使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)函数应用该函数：

```py
>>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
>>> chunked_dataset[:10]
{'chunks': ['Amrozi accused his brother , whom he called " the ',
            'witness " , of deliberately distorting his evidenc',
            'e .',
            "Yucaipa owned Dominick 's before selling the chain",
            ' to Safeway in 1998 for $ 2.5 billion .',
            'They had published an advertisement on the Interne',
            't on June 10 , offering the cargo for sale , he ad',
            'ded .',
            'Around 0335 GMT , Tab shares were up 19 cents , or',
            ' 4.4 % , at A $ 4.56 , having earlier set a record']}
```

请注意，现在句子被拆分为较短的块，并且数据集中有更多的行。

```py
>>> dataset
Dataset({
 features: ['sentence1', 'sentence2', 'label', 'idx'],
 num_rows: 3668
})
>>> chunked_dataset
Dataset(schema: {'chunks': 'string'}, num_rows: 10470)
```

#### 数据增强

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)函数也可以用于数据增强。下面的示例为句子中的一个掩码标记生成额外的单词。

在🤗 Transformers的[FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline)中加载并使用[RoBERTA](https://huggingface.co/roberta-base)模型：

```py
>>> from random import randint
>>> from transformers import pipeline

>>> fillmask = pipeline("fill-mask", model="roberta-base")
>>> mask_token = fillmask.tokenizer.mask_token
>>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)
```

创建一个函数来随机选择要在句子中屏蔽的单词。该函数还应返回原始句子以及RoBERTA生成的前两个替代词。

```py
>>> def augment_data(examples):
...     outputs = []
...     for sentence in examples["sentence1"]:
...         words = sentence.split(' ')
...         K = randint(1, len(words)-1)
...         masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
...         predictions = fillmask(masked_sentence)
...         augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
...         outputs += [sentence] + augmented_sequences
...
...     return {"data": outputs}
```

使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)在整个数据集上应用函数：

```py
>>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
>>> augmented_dataset[:9]["data"]
['Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately withholding his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately suppressing his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately destroying his evidence.',
 "Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
 'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',
 "Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.",
 'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'
]
```

对于每个原始句子，RoBERTA使用三个备选词对一个随机单词进行增强。原始单词`distorting`被`withholding`、`suppressing`和`destroying`补充。

### 处理多个拆分

许多数据集具有可以使用[DatasetDict.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.map)同时处理的拆分。例如，通过以下方式对训练集和测试集中的`sentence1`字段进行标记化：

```py
>>> from datasets import load_dataset

# load all the splits
>>> dataset = load_dataset('glue', 'mrpc')
>>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)
>>> encoded_dataset["train"][0]
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
'label': 1,
'idx': 0,
'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

### 分布式使用

在分布式设置中使用[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)时，还应使用[torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier)。这确保主进程执行映射，而其他进程加载结果，从而避免重复工作。

以下示例展示了如何使用`torch.distributed.barrier`来同步进程：

```py
>>> from datasets import Dataset
>>> import torch.distributed

>>> dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

>>> if training_args.local_rank > 0:
...     print("Waiting for main process to perform the mapping")
...     torch.distributed.barrier()

>>> dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

>>> if training_args.local_rank == 0:
...     print("Loading results from main process")
...     torch.distributed.barrier()
```

## 连接

如果不同数据集共享相同的列类型，则可以将它们连接在一起。使用[concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets)连接数据集：

```py
>>> from datasets import concatenate_datasets, load_dataset

>>> bookcorpus = load_dataset("bookcorpus", split="train")
>>> wiki = load_dataset("wikipedia", "20220301.en", split="train")
>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])  # only keep the 'text' column

>>> assert bookcorpus.features.type == wiki.features.type
>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

您还可以通过设置`axis=1`来水平连接两个数据集，只要这些数据集具有相同数量的行：

```py
>>> from datasets import Dataset
>>> bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
>>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```

### 交错

您还可以通过从每个数据集中交替获取示例来将多个数据集混合在一起，从而创建一个新数据集。这被称为*交错*，可以通过[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)函数实现。[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)和[concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets)都适用于常规[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)和[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)对象。请参考[Stream](./stream#interleave)指南，了解如何交错[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)对象的示例。

您可以为每个原始数据集定义采样概率，以指定如何交错这些数据集。在这种情况下，新数据集是通过从随机数据集逐个获取示例构建的，直到其中一个数据集耗尽样本。

```py
>>> seed = 42
>>> probabilities = [0.3, 0.5, 0.2]
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
>>> dataset["a"]
[10, 11, 20, 12, 0, 21, 13]
```

您还可以指定`stopping_strategy`。默认策略`first_exhausted`是一种子采样策略，即数据集构建在其中一个数据集耗尽样本时停止。您可以指定`stopping_strategy=all_exhausted`来执行一种过采样策略。在这种情况下，数据集构建将在每个数据集的每个样本至少添加一次后停止。实际上，这意味着如果一个数据集耗尽，它将返回到该数据集的开头，直到达到停止条件。请注意，如果未指定采样概率，则新数据集将具有`max_length_datasets*nb_dataset`个样本。

```py
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

## 格式

[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)函数可更改列的格式，使其与一些常见数据格式兼容。在`type`参数中指定您想要的输出以及要格式化的列。格式化是即时应用的。

例如，通过设置`type="torch"`创建PyTorch张量：

```py
>>> import torch
>>> dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

[with_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_format)函数还可以更改列的格式，但它返回一个新的[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)对象：

```py
>>> dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

🤗 数据集还支持其他常见数据格式，如NumPy、Pandas和JAX。查看[使用 TensorFlow 与数据集](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset)指南，了解如何高效创建 TensorFlow 数据集的更多细节。

如果您需要将数据集重置为其原始格式，请使用[reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)函数：

```py
>>> dataset.format
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
>>> dataset.reset_format()
>>> dataset.format
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### 格式转换

[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)函数在运行时应用自定义格式转换。此函数将替换任何先前指定的格式。例如，您可以使用此函数在运行时对标记进行标记化和填充。仅当访问示例时才应用标记化：

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> def encode(batch):
...     return tokenizer(batch["sentence1"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
>>> dataset.set_transform(encode)
>>> dataset.format
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

您还可以使用[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)函数解码不受[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)支持的格式。例如，[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)特征使用[`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/) - 一个快速简单的安装库 - 但它不支持较少常见的音频格式。在这里，您可以使用[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)在运行时应用自定义解码转换。您可以自由选择任何库来解码音频文件。

以下示例使用[`pydub`](http://pydub.com/)包打开一个`soundfile`不支持的音频格式：

```py
>>> import numpy as np
>>> from pydub import AudioSegment

>>> audio_dataset_amr = Dataset.from_dict({"audio": ["audio_samples/audio.amr"]})

>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):
...     def pydub_decode_file(audio_path):
...         sound = AudioSegment.from_file(audio_path)
...         if sound.frame_rate != sampling_rate:
...             sound = sound.set_frame_rate(sampling_rate)
...         channel_sounds = sound.split_to_mono()
...         samples = [s.get_array_of_samples() for s in channel_sounds]
...         fp_arr = np.array(samples).T.astype(np.float32)
...         fp_arr /= np.iinfo(samples[0].typecode).max
...         return fp_arr
...
...     batch["audio"] = [pydub_decode_file(audio_path) for audio_path in batch["audio"]]
...     return batch

>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)
```

## 保存

一旦您完成处理数据集，您可以使用[save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)保存并以后重复使用。

通过提供要保存数据集的目录路径来保存数据集：

```py
>>> encoded_dataset.save_to_disk("path/of/my/dataset/directory")
```

使用[load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk)函数重新加载数据集：

```py
>>> from datasets import load_from_disk
>>> reloaded_dataset = load_from_disk("path/of/my/dataset/directory")
```

想要将数据集保存到云存储提供商吗？阅读我们的[云存储](./filesystems)指南，了解如何将数据集保存到 AWS 或 Google Cloud Storage。

## 导出

🤗 数据集还支持导出，这样您就可以在其他应用程序中使用数据集。以下表格显示当前支持的文件格式，您可以导出到这些格式：

| 文件类型 | 导出方法 |
| --- | --- |
| CSV | [Dataset.to_csv()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_csv) |
| JSON | [Dataset.to_json()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_json) |
| Parquet | [Dataset.to_parquet()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_parquet) |
| SQL | [Dataset.to_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_sql) |
| 内存中的 Python 对象 | [Dataset.to_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_pandas) 或 [Dataset.to_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_dict) |

例如，像这样将数据集导出到 CSV 文件：

```py
>>> encoded_dataset.to_csv("path/of/my/dataset.csv")
```
