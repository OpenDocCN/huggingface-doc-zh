# å¤„ç†

> åŽŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/datasets/process](https://huggingface.co/docs/datasets/process)

ðŸ¤— Datasetsæä¾›äº†è®¸å¤šå·¥å…·æ¥ä¿®æ”¹æ•°æ®é›†çš„ç»“æž„å’Œå†…å®¹ã€‚è¿™äº›å·¥å…·å¯¹äºŽæ•´ç†æ•°æ®é›†ã€åˆ›å»ºé¢å¤–åˆ—ã€åœ¨ç‰¹å¾å’Œæ ¼å¼ä¹‹é—´è½¬æ¢ç­‰æ–¹é¢éžå¸¸é‡è¦ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   é‡æ–°æŽ’åºè¡Œå¹¶æ‹†åˆ†æ•°æ®é›†ã€‚

+   é‡å‘½åå’Œåˆ é™¤åˆ—ï¼Œä»¥åŠå…¶ä»–å¸¸è§çš„åˆ—æ“ä½œã€‚

+   å¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹åº”ç”¨å¤„ç†å‡½æ•°ã€‚

+   è¿žæŽ¥æ•°æ®é›†ã€‚

+   åº”ç”¨è‡ªå®šä¹‰æ ¼å¼è½¬æ¢ã€‚

+   ä¿å­˜å’Œå¯¼å‡ºå¤„ç†è¿‡çš„æ•°æ®é›†ã€‚

æœ‰å…³å¤„ç†å…¶ä»–æ•°æ®é›†æ¨¡æ€çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[å¤„ç†éŸ³é¢‘æ•°æ®é›†æŒ‡å—](./audio_process)ã€[å¤„ç†å›¾åƒæ•°æ®é›†æŒ‡å—](./image_process)æˆ–[å¤„ç†æ–‡æœ¬æ•°æ®é›†æŒ‡å—](./nlp_process)ã€‚

æœ¬æŒ‡å—ä¸­çš„ç¤ºä¾‹ä½¿ç”¨MRPCæ•°æ®é›†ï¼Œä½†è¯·éšæ„åŠ è½½æ‚¨é€‰æ‹©çš„ä»»ä½•æ•°æ®é›†å¹¶è·Ÿéšæ“ä½œï¼

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("glue", "mrpc", split="train")
```

æœ¬æŒ‡å—ä¸­çš„æ‰€æœ‰å¤„ç†æ–¹æ³•éƒ½ä¼šè¿”å›žä¸€ä¸ªæ–°çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ã€‚ä¿®æ”¹ä¸æ˜¯å°±åœ°è¿›è¡Œçš„ã€‚è¯·æ³¨æ„ä¸è¦è¦†ç›–æ‚¨ä¹‹å‰çš„æ•°æ®é›†ï¼

## æŽ’åºã€æ´—ç‰Œã€é€‰æ‹©ã€æ‹†åˆ†å’Œåˆ†ç‰‡

æœ‰å‡ ä¸ªå‡½æ•°ç”¨äºŽé‡æ–°æŽ’åˆ—æ•°æ®é›†çš„ç»“æž„ã€‚è¿™äº›å‡½æ•°å¯¹äºŽä»…é€‰æ‹©æ‚¨æƒ³è¦çš„è¡Œã€åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ä»¥åŠå°†éžå¸¸å¤§çš„æ•°æ®é›†åˆ†æˆè¾ƒå°çš„å—éžå¸¸æœ‰ç”¨ã€‚

### æŽ’åº

ä½¿ç”¨[sort()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.sort)æ ¹æ®å…¶æ•°å€¼å€¼å¯¹åˆ—å€¼è¿›è¡ŒæŽ’åºã€‚æä¾›çš„åˆ—å¿…é¡»æ˜¯NumPyå…¼å®¹çš„ã€‚

```py
>>> dataset["label"][:10]
[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]
>>> sorted_dataset = dataset.sort("label")
>>> sorted_dataset["label"][:10]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
>>> sorted_dataset["label"][-10:]
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

åœ¨å†…éƒ¨ï¼Œè¿™ä¼šåˆ›å»ºä¸€ä¸ªæ ¹æ®åˆ—çš„å€¼æŽ’åºçš„ç´¢å¼•åˆ—è¡¨ã€‚ç„¶åŽä½¿ç”¨è¿™ä¸ªç´¢å¼•æ˜ å°„æ¥è®¿é—®åº•å±‚Arrowè¡¨ä¸­çš„æ­£ç¡®è¡Œã€‚

### æ´—ç‰Œ

[shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shuffle)å‡½æ•°ä¼šéšæœºé‡æ–°æŽ’åˆ—åˆ—å€¼ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å‡½æ•°ä¸­æŒ‡å®š`generator`å‚æ•°ï¼Œä»¥ä½¿ç”¨ä¸åŒçš„`numpy.random.Generator`æ¥æ›´å¥½åœ°æŽ§åˆ¶ç”¨äºŽæ´—ç‰Œæ•°æ®é›†çš„ç®—æ³•ã€‚

```py
>>> shuffled_dataset = sorted_dataset.shuffle(seed=42)
>>> shuffled_dataset["label"][:10]
[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]
```

æ´—ç‰Œä¼šå–å¾—ç´¢å¼•åˆ—è¡¨`[0:len(my_dataset)]`å¹¶å¯¹å…¶è¿›è¡Œæ´—ç‰Œä»¥åˆ›å»ºä¸€ä¸ªç´¢å¼•æ˜ å°„ã€‚ç„¶è€Œï¼Œä¸€æ—¦æ‚¨çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)æœ‰äº†ä¸€ä¸ªç´¢å¼•æ˜ å°„ï¼Œé€Ÿåº¦å¯èƒ½ä¼šå˜æ…¢10å€ã€‚è¿™æ˜¯å› ä¸ºéœ€è¦é¢å¤–çš„æ­¥éª¤æ¥ä½¿ç”¨ç´¢å¼•æ˜ å°„èŽ·å–è¦è¯»å–çš„è¡Œç´¢å¼•ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œæ‚¨ä¸å†è¯»å–è¿žç»­çš„æ•°æ®å—ã€‚ä¸ºæ¢å¤é€Ÿåº¦ï¼Œæ‚¨éœ€è¦å†æ¬¡ä½¿ç”¨[Dataset.flatten_indices()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten_indices)å°†æ•´ä¸ªæ•°æ®é›†é‡å†™åˆ°ç£ç›˜ä¸Šï¼Œä»¥åˆ é™¤ç´¢å¼•æ˜ å°„ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åˆ‡æ¢åˆ°[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)å¹¶åˆ©ç”¨å…¶å¿«é€Ÿçš„è¿‘ä¼¼æ´—ç‰Œ[IterableDataset.shuffle()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)ï¼š

```py
>>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)
>>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)
```

### é€‰æ‹©å’Œè¿‡æ»¤

åœ¨æ•°æ®é›†ä¸­æœ‰ä¸¤ç§è¿‡æ»¤è¡Œçš„é€‰é¡¹ï¼š[select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)å’Œ[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ã€‚

+   [select()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select)æ ¹æ®ç´¢å¼•åˆ—è¡¨è¿”å›žè¡Œï¼š

```py
>>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
>>> len(small_dataset)
6
```

+   [filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)è¿”å›žç¬¦åˆæŒ‡å®šæ¡ä»¶çš„è¡Œï¼š

```py
>>> start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
>>> len(start_with_ar)
6
>>> start_with_ar["sentence1"]
['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',
'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',
'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',
"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .",
'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'
]
```

[filter()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.filter)ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®`with_indices=True`æ¥æŒ‰ç´¢å¼•è¿›è¡Œè¿‡æ»¤ï¼š

```py
>>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
>>> len(even_dataset)
1834
>>> len(dataset) / 2
1834.0
```

é™¤éžè¦ä¿ç•™çš„ç´¢å¼•åˆ—è¡¨æ˜¯è¿žç»­çš„ï¼Œå¦åˆ™è¿™äº›æ–¹æ³•åœ¨å†…éƒ¨ä¹Ÿä¼šåˆ›å»ºä¸€ä¸ªç´¢å¼•æ˜ å°„ã€‚

### æ‹†åˆ†

[train_test_split()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.train_test_split)å‡½æ•°åœ¨æ•°æ®é›†æ²¡æœ‰è®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†æ—¶åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ‹†åˆ†ã€‚è¿™å…è®¸æ‚¨è°ƒæ•´æ¯ä¸ªæ‹†åˆ†ä¸­çš„ç›¸å¯¹æ¯”ä¾‹æˆ–ç»å¯¹æ•°é‡çš„æ ·æœ¬ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨`test_size`å‚æ•°åˆ›å»ºä¸€ä¸ªåŽŸå§‹æ•°æ®é›†çš„10%çš„æµ‹è¯•æ‹†åˆ†ï¼š

```py
>>> dataset.train_test_split(test_size=0.1)
{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),
'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}
>>> 0.1 * len(dataset)
366.8
```

é»˜è®¤æƒ…å†µä¸‹æ‹†åˆ†æ˜¯éšæœºçš„ï¼Œä½†æ‚¨å¯ä»¥è®¾ç½®`shuffle=False`æ¥é˜²æ­¢æ´—ç‰Œã€‚

### åˆ†ç‰‡

ðŸ¤— æ•°æ®é›†æ”¯æŒåˆ†ç‰‡ï¼Œå°†ä¸€ä¸ªéžå¸¸å¤§çš„æ•°æ®é›†åˆ†æˆé¢„å®šä¹‰æ•°é‡çš„å—ã€‚åœ¨[shard()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.shard)ä¸­æŒ‡å®š`num_shards`å‚æ•°ä»¥ç¡®å®šè¦å°†æ•°æ®é›†åˆ†å‰²æˆçš„åˆ†ç‰‡æ•°ã€‚æ‚¨è¿˜éœ€è¦æä¾›è¦è¿”å›žçš„åˆ†ç‰‡çš„`index`å‚æ•°ã€‚

ä¾‹å¦‚ï¼Œ[imdb](https://huggingface.co/datasets/imdb)æ•°æ®é›†æœ‰25000ä¸ªç¤ºä¾‹ï¼š

```py
>>> from datasets import load_dataset
>>> datasets = load_dataset("imdb", split="train")
>>> print(dataset)
Dataset({
    features: ['text', 'label'],
    num_rows: 25000
})
```

å°†æ•°æ®é›†åˆ†æˆå››ä¸ªå—åŽï¼Œç¬¬ä¸€ä¸ªå—å°†åªæœ‰6250ä¸ªç¤ºä¾‹ï¼š

```py
>>> dataset.shard(num_shards=4, index=0)
Dataset({
    features: ['text', 'label'],
    num_rows: 6250
})
>>> print(25000/4)
6250.0
```

## é‡å‘½åã€ç§»é™¤ã€è½¬æ¢å’Œå±•å¹³

ä»¥ä¸‹å‡½æ•°å…è®¸æ‚¨ä¿®æ”¹æ•°æ®é›†çš„åˆ—ã€‚è¿™äº›å‡½æ•°å¯¹é‡å‘½åæˆ–ç§»é™¤åˆ—ã€å°†åˆ—æ›´æ”¹ä¸ºæ–°çš„ç‰¹å¾é›†ä»¥åŠå±•å¹³åµŒå¥—åˆ—ç»“æž„å¾ˆæœ‰ç”¨ã€‚

### é‡å‘½å

å½“éœ€è¦åœ¨æ•°æ®é›†ä¸­é‡å‘½ååˆ—æ—¶ï¼Œè¯·ä½¿ç”¨[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)ã€‚ä¸ŽåŽŸå§‹åˆ—ç›¸å…³çš„ç‰¹å¾å®žé™…ä¸Šè¢«ç§»åŠ¨åˆ°æ–°åˆ—åä¸‹ï¼Œè€Œä¸ä»…ä»…æ˜¯æ›¿æ¢åŽŸå§‹åˆ—ã€‚

ä½¿ç”¨åŽŸå§‹åˆ—çš„åç§°å’Œæ–°åˆ—åç§°æä¾›[rename_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.rename_column)ï¼š

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.rename_column("sentence1", "sentenceA")
>>> dataset = dataset.rename_column("sentence2", "sentenceB")
>>> dataset
Dataset({
    features: ['sentenceA', 'sentenceB', 'label', 'idx'],
    num_rows: 3668
})
```

### ç§»é™¤

å½“éœ€è¦ç§»é™¤ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—æ—¶ï¼Œæä¾›è¦ç§»é™¤çš„åˆ—åç»™[remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)å‡½æ•°ã€‚é€šè¿‡æä¾›åˆ—ååˆ—è¡¨æ¥ç§»é™¤å¤šä¸ªåˆ—ï¼š

```py
>>> dataset = dataset.remove_columns("label")
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.remove_columns(["sentence1", "sentence2"])
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

ç›¸åï¼Œ[select_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.select_columns)é€‰æ‹©è¦ä¿ç•™çš„ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—ï¼Œå¹¶ç§»é™¤å…¶ä½™åˆ—ã€‚æ­¤å‡½æ•°æŽ¥å—ä¸€ä¸ªæˆ–åˆ—ååˆ—è¡¨ï¼š

```py
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'label', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])
>>> dataset
Dataset({
    features: ['sentence1', 'sentence2', 'idx'],
    num_rows: 3668
})
>>> dataset = dataset.select_columns('idx')
>>> dataset
Dataset({
    features: ['idx'],
    num_rows: 3668
})
```

### è½¬æ¢

[cast()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast)å‡½æ•°è½¬æ¢ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—çš„ç‰¹å¾ç±»åž‹ã€‚æ­¤å‡½æ•°æŽ¥å—æ‚¨çš„æ–°[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)ä½œä¸ºå…¶å‚æ•°ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•æ›´æ”¹[ClassLabel](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.ClassLabel)å’Œ[Value](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Value)ç‰¹å¾ï¼š

```py
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
'idx': Value(dtype='int32', id=None)}

>>> from datasets import ClassLabel, Value
>>> new_features = dataset.features.copy()
>>> new_features["label"] = ClassLabel(names=["negative", "positive"])
>>> new_features["idx"] = Value("int64")
>>> dataset = dataset.cast(new_features)
>>> dataset.features
{'sentence1': Value(dtype='string', id=None),
'sentence2': Value(dtype='string', id=None),
'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),
'idx': Value(dtype='int64', id=None)}
```

ä»…å½“åŽŸå§‹ç‰¹å¾ç±»åž‹å’Œæ–°ç‰¹å¾ç±»åž‹å…¼å®¹æ—¶ï¼Œè½¬æ¢æ‰èµ·ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœåŽŸå§‹åˆ—ä»…åŒ…å«1å’Œ0ï¼Œåˆ™å¯ä»¥å°†å…·æœ‰ç‰¹å¾ç±»åž‹`Value("int32")`çš„åˆ—è½¬æ¢ä¸º`Value("bool")`ã€‚

ä½¿ç”¨[cast_column()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.cast_column)å‡½æ•°æ¥æ›´æ”¹å•ä¸ªåˆ—çš„ç‰¹å¾ç±»åž‹ã€‚å°†åˆ—åå’Œå…¶æ–°ç‰¹å¾ç±»åž‹ä½œä¸ºå‚æ•°ä¼ é€’ï¼š

```py
>>> dataset.features
{'audio': Audio(sampling_rate=44100, mono=True, id=None)}

>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
>>> dataset.features
{'audio': Audio(sampling_rate=16000, mono=True, id=None)}
```

### å±•å¹³

æœ‰æ—¶ï¼Œä¸€åˆ—å¯ä»¥æ˜¯å‡ ç§ç±»åž‹çš„åµŒå¥—ç»“æž„ã€‚çœ‹ä¸€ä¸‹æ¥è‡ªSQuADæ•°æ®é›†çš„ä¸‹é¢çš„åµŒå¥—ç»“æž„ï¼š

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset("squad", split="train")
>>> dataset.features
{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),
'context': Value(dtype='string', id=None),
'id': Value(dtype='string', id=None),
'question': Value(dtype='string', id=None),
'title': Value(dtype='string', id=None)}
```

`answers`å­—æ®µåŒ…å«ä¸¤ä¸ªå­å­—æ®µï¼š`text`å’Œ`answer_start`ã€‚ä½¿ç”¨[flatten()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.flatten)å‡½æ•°å°†å­å­—æ®µæå–åˆ°å®ƒä»¬è‡ªå·±çš„ç‹¬ç«‹åˆ—ä¸­ï¼š

```py
>>> flat_dataset = dataset.flatten()
>>> flat_dataset
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],
 num_rows: 87599
})
```

æ³¨æ„å­å­—æ®µçŽ°åœ¨æ˜¯ç‹¬ç«‹çš„åˆ—ï¼š`answers.text`å’Œ`answers.answer_start`ã€‚

## æ˜ å°„

ðŸ¤— Datasetsçš„ä¸€äº›æ›´å¼ºå¤§çš„åº”ç”¨æ¥è‡ªäºŽä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ã€‚[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)çš„ä¸»è¦ç›®çš„æ˜¯åŠ é€Ÿå¤„ç†å‡½æ•°ã€‚å®ƒå…è®¸æ‚¨å°†å¤„ç†å‡½æ•°åº”ç”¨äºŽæ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹ï¼Œç‹¬ç«‹åœ°æˆ–æ‰¹å¤„ç†ã€‚è¯¥å‡½æ•°ç”šè‡³å¯ä»¥åˆ›å»ºæ–°çš„è¡Œå’Œåˆ—ã€‚

åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œå°†æ•°æ®é›†ä¸­æ¯ä¸ª`sentence1`å€¼çš„å‰ç¼€è®¾ç½®ä¸º'My sentence: 'ã€‚

é¦–å…ˆåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†'My sentence: 'æ·»åŠ åˆ°æ¯ä¸ªå¥å­çš„å¼€å¤´ã€‚è¯¥å‡½æ•°éœ€è¦æŽ¥å—å¹¶è¾“å‡ºä¸€ä¸ª`dict`ï¼š

```py
>>> def add_prefix(example):
...     example["sentence1"] = 'My sentence: ' + example["sentence1"]
...     return example
```

çŽ°åœ¨ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å°†`add_prefix`å‡½æ•°åº”ç”¨äºŽæ•´ä¸ªæ•°æ®é›†ï¼š

```py
>>> updated_dataset = small_dataset.map(add_prefix)
>>> updated_dataset["sentence1"][:5]
['My sentence: Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',
'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',
]
```

è®©æˆ‘ä»¬çœ‹å¦ä¸€ä¸ªä¾‹å­ï¼Œé™¤äº†è¿™æ¬¡ï¼Œæ‚¨å°†ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)åˆ é™¤ä¸€ä¸ªåˆ—ã€‚å½“æ‚¨åˆ é™¤ä¸€åˆ—æ—¶ï¼Œåªæœ‰åœ¨æä¾›ç¤ºä¾‹ç»™æ˜ å°„å‡½æ•°åŽæ‰ä¼šåˆ é™¤ã€‚è¿™ä½¿å¾—æ˜ å°„å‡½æ•°å¯ä»¥åœ¨åˆ é™¤ä¹‹å‰ä½¿ç”¨åˆ—çš„å†…å®¹ã€‚

ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)ä¸­çš„`remove_columns`å‚æ•°æŒ‡å®šè¦ç§»é™¤çš„åˆ—ï¼š

```py
>>> updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])
>>> updated_dataset.column_names
['sentence2', 'label', 'idx', 'new_sentence']
```

ðŸ¤— Datasetsè¿˜æœ‰ä¸€ä¸ª[remove_columns()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.remove_columns)å‡½æ•°ï¼Œå®ƒæ›´å¿«ï¼Œå› ä¸ºå®ƒä¸ä¼šå¤åˆ¶å‰©ä½™åˆ—çš„æ•°æ®ã€‚

å¦‚æžœè®¾ç½®`with_indices=True`ï¼Œè¿˜å¯ä»¥ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)ä¸Žç´¢å¼•ä¸€èµ·ä½¿ç”¨ã€‚ä¸‹é¢çš„ç¤ºä¾‹å°†ç´¢å¼•æ·»åŠ åˆ°æ¯ä¸ªå¥å­çš„å¼€å¤´ï¼š

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
>>> updated_dataset["sentence2"][:5]
['0: Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
 "1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .",
 "2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .",
 '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',
 '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'
]
```

### å¤šè¿›ç¨‹

å¤šè¿›ç¨‹é€šè¿‡åœ¨CPUä¸Šå¹¶è¡Œå¤„ç†è¿›ç¨‹æ˜¾è‘—åŠ å¿«å¤„ç†é€Ÿåº¦ã€‚åœ¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)ä¸­è®¾ç½®`num_proc`å‚æ•°ä»¥è®¾ç½®è¦ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼š

```py
>>> updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, num_proc=4)
```

å¦‚æžœè®¾ç½®`with_rank=True`ï¼Œ[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)è¿˜å¯ä»¥ä¸Žè¿›ç¨‹çš„ç­‰çº§ä¸€èµ·ä½¿ç”¨ã€‚è¿™ç±»ä¼¼äºŽ`with_indices`å‚æ•°ã€‚å¦‚æžœå·²ç»å­˜åœ¨`index`ï¼Œåˆ™æ˜ å°„å‡½æ•°ä¸­çš„`with_rank`å‚æ•°åœ¨`index`ä¹‹åŽã€‚

```py
>>> import torch
>>> from multiprocess import set_start_method
>>> from transformers import AutoTokenizer, AutoModelForCausalLM 
>>> from datasets import load_dataset
>>> 
>>> # Get an example dataset
>>> dataset = load_dataset("fka/awesome-chatgpt-prompts", split="train")
>>> 
>>> # Get an example model and its tokenizer 
>>> model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen1.5-0.5B-Chat").eval()
>>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen1.5-0.5B-Chat")
>>>
>>> def gpu_computation(batch, rank):
...     # Move the model on the right GPU if it's not there already
...     device = f"cuda:{(rank or 0) % torch.cuda.device_count()}"
...     model.to(device)
...     
...     # Your big GPU call goes here, for example:
...     chats = [[
...         {"role": "system", "content": "You are a helpful assistant."},
...         {"role": "user", "content": prompt}
...     ] for prompt in batch["prompt"]]
...     texts = [tokenizer.apply_chat_template(
...         chat,
...         tokenize=False,
...         add_generation_prompt=True
...     ) for chat in chats]
...     model_inputs = tokenizer(texts, padding=True, return_tensors="pt").to(device)
...     with torch.no_grad():
...         outputs = model.generate(**model_inputs, max_new_tokens=512)
...     batch["output"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)
...     return batch
>>>
>>> if __name__ == "__main__":
...     set_start_method("spawn")
...     updated_dataset = dataset.map(
...         gpu_computation,
...         batched=True,
...         batch_size=16,
...         with_rank=True,
...         num_proc=torch.cuda.device_count(),  # one process per GPU
...     )
```

ç­‰çº§çš„ä¸»è¦ç”¨ä¾‹æ˜¯åœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œè®¡ç®—ã€‚è¿™éœ€è¦è®¾ç½®`multiprocess.set_start_method("spawn")`ã€‚å¦‚æžœä¸è¿™æ ·åšï¼Œæ‚¨å°†æ”¶åˆ°ä»¥ä¸‹CUDAé”™è¯¯ï¼š

```py
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.
```

### æ‰¹å¤„ç†

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°æ”¯æŒå¤„ç†ç¤ºä¾‹æ‰¹æ¬¡ã€‚é€šè¿‡è®¾ç½®`batched=True`æ¥æ“ä½œæ‰¹æ¬¡ã€‚é»˜è®¤æ‰¹å¤„ç†å¤§å°ä¸º1000ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨`batch_size`å‚æ•°è¿›è¡Œè°ƒæ•´ã€‚æ‰¹å¤„ç†ä½¿å¾—å¯ä»¥å°†é•¿å¥å­æ‹†åˆ†ä¸ºè¾ƒçŸ­çš„å—å’Œæ•°æ®å¢žå¼ºç­‰æœ‰è¶£çš„åº”ç”¨ã€‚

#### æ‹†åˆ†é•¿ç¤ºä¾‹

å½“ç¤ºä¾‹å¤ªé•¿æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›å°†å…¶æ‹†åˆ†ä¸ºå‡ ä¸ªè¾ƒå°çš„å—ã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼š

1.  å°†`sentence1`å­—æ®µæ‹†åˆ†ä¸º50ä¸ªå­—ç¬¦çš„å—ã€‚

1.  å°†æ‰€æœ‰å—å †å åœ¨ä¸€èµ·ä»¥åˆ›å»ºæ–°æ•°æ®é›†ã€‚

```py
>>> def chunk_examples(examples):
...     chunks = []
...     for sentence in examples["sentence1"]:
...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
...     return {"chunks": chunks}
```

ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°åº”ç”¨è¯¥å‡½æ•°ï¼š

```py
>>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
>>> chunked_dataset[:10]
{'chunks': ['Amrozi accused his brother , whom he called " the ',
            'witness " , of deliberately distorting his evidenc',
            'e .',
            "Yucaipa owned Dominick 's before selling the chain",
            ' to Safeway in 1998 for $ 2.5 billion .',
            'They had published an advertisement on the Interne',
            't on June 10 , offering the cargo for sale , he ad',
            'ded .',
            'Around 0335 GMT , Tab shares were up 19 cents , or',
            ' 4.4 % , at A $ 4.56 , having earlier set a record']}
```

è¯·æ³¨æ„ï¼ŒçŽ°åœ¨å¥å­è¢«æ‹†åˆ†ä¸ºè¾ƒçŸ­çš„å—ï¼Œå¹¶ä¸”æ•°æ®é›†ä¸­æœ‰æ›´å¤šçš„è¡Œã€‚

```py
>>> dataset
Dataset({
 features: ['sentence1', 'sentence2', 'label', 'idx'],
 num_rows: 3668
})
>>> chunked_dataset
Dataset(schema: {'chunks': 'string'}, num_rows: 10470)
```

#### æ•°æ®å¢žå¼º

[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)å‡½æ•°ä¹Ÿå¯ä»¥ç”¨äºŽæ•°æ®å¢žå¼ºã€‚ä¸‹é¢çš„ç¤ºä¾‹ä¸ºå¥å­ä¸­çš„ä¸€ä¸ªæŽ©ç æ ‡è®°ç”Ÿæˆé¢å¤–çš„å•è¯ã€‚

åœ¨ðŸ¤— Transformersçš„[FillMaskPipeline](https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline)ä¸­åŠ è½½å¹¶ä½¿ç”¨[RoBERTA](https://huggingface.co/roberta-base)æ¨¡åž‹ï¼š

```py
>>> from random import randint
>>> from transformers import pipeline

>>> fillmask = pipeline("fill-mask", model="roberta-base")
>>> mask_token = fillmask.tokenizer.mask_token
>>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)
```

åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥éšæœºé€‰æ‹©è¦åœ¨å¥å­ä¸­å±è”½çš„å•è¯ã€‚è¯¥å‡½æ•°è¿˜åº”è¿”å›žåŽŸå§‹å¥å­ä»¥åŠRoBERTAç”Ÿæˆçš„å‰ä¸¤ä¸ªæ›¿ä»£è¯ã€‚

```py
>>> def augment_data(examples):
...     outputs = []
...     for sentence in examples["sentence1"]:
...         words = sentence.split(' ')
...         K = randint(1, len(words)-1)
...         masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
...         predictions = fillmask(masked_sentence)
...         augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
...         outputs += [sentence] + augmented_sequences
...
...     return {"data": outputs}
```

ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨å‡½æ•°ï¼š

```py
>>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
>>> augmented_dataset[:9]["data"]
['Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately withholding his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately suppressing his evidence.',
 'Amrozi accused his brother, whom he called " the witness ", of deliberately destroying his evidence.',
 "Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .",
 'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',
 "Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.",
 'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'
]
```

å¯¹äºŽæ¯ä¸ªåŽŸå§‹å¥å­ï¼ŒRoBERTAä½¿ç”¨ä¸‰ä¸ªå¤‡é€‰è¯å¯¹ä¸€ä¸ªéšæœºå•è¯è¿›è¡Œå¢žå¼ºã€‚åŽŸå§‹å•è¯`distorting`è¢«`withholding`ã€`suppressing`å’Œ`destroying`è¡¥å……ã€‚

### å¤„ç†å¤šä¸ªæ‹†åˆ†

è®¸å¤šæ•°æ®é›†å…·æœ‰å¯ä»¥ä½¿ç”¨[DatasetDict.map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.DatasetDict.map)åŒæ—¶å¤„ç†çš„æ‹†åˆ†ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„`sentence1`å­—æ®µè¿›è¡Œæ ‡è®°åŒ–ï¼š

```py
>>> from datasets import load_dataset

# load all the splits
>>> dataset = load_dataset('glue', 'mrpc')
>>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)
>>> encoded_dataset["train"][0]
{'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .',
'label': 1,
'idx': 0,
'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

### åˆ†å¸ƒå¼ä½¿ç”¨

åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ä½¿ç”¨[map()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.map)æ—¶ï¼Œè¿˜åº”ä½¿ç”¨[torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier)ã€‚è¿™ç¡®ä¿ä¸»è¿›ç¨‹æ‰§è¡Œæ˜ å°„ï¼Œè€Œå…¶ä»–è¿›ç¨‹åŠ è½½ç»“æžœï¼Œä»Žè€Œé¿å…é‡å¤å·¥ä½œã€‚

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨`torch.distributed.barrier`æ¥åŒæ­¥è¿›ç¨‹ï¼š

```py
>>> from datasets import Dataset
>>> import torch.distributed

>>> dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

>>> if training_args.local_rank > 0:
...     print("Waiting for main process to perform the mapping")
...     torch.distributed.barrier()

>>> dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

>>> if training_args.local_rank == 0:
...     print("Loading results from main process")
...     torch.distributed.barrier()
```

## è¿žæŽ¥

å¦‚æžœä¸åŒæ•°æ®é›†å…±äº«ç›¸åŒçš„åˆ—ç±»åž‹ï¼Œåˆ™å¯ä»¥å°†å®ƒä»¬è¿žæŽ¥åœ¨ä¸€èµ·ã€‚ä½¿ç”¨[concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets)è¿žæŽ¥æ•°æ®é›†ï¼š

```py
>>> from datasets import concatenate_datasets, load_dataset

>>> bookcorpus = load_dataset("bookcorpus", split="train")
>>> wiki = load_dataset("wikipedia", "20220301.en", split="train")
>>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])  # only keep the 'text' column

>>> assert bookcorpus.features.type == wiki.features.type
>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])
```

æ‚¨è¿˜å¯ä»¥é€šè¿‡è®¾ç½®`axis=1`æ¥æ°´å¹³è¿žæŽ¥ä¸¤ä¸ªæ•°æ®é›†ï¼Œåªè¦è¿™äº›æ•°æ®é›†å…·æœ‰ç›¸åŒæ•°é‡çš„è¡Œï¼š

```py
>>> from datasets import Dataset
>>> bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
>>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)
```

### äº¤é”™

æ‚¨è¿˜å¯ä»¥é€šè¿‡ä»Žæ¯ä¸ªæ•°æ®é›†ä¸­äº¤æ›¿èŽ·å–ç¤ºä¾‹æ¥å°†å¤šä¸ªæ•°æ®é›†æ··åˆåœ¨ä¸€èµ·ï¼Œä»Žè€Œåˆ›å»ºä¸€ä¸ªæ–°æ•°æ®é›†ã€‚è¿™è¢«ç§°ä¸º*äº¤é”™*ï¼Œå¯ä»¥é€šè¿‡[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)å‡½æ•°å®žçŽ°ã€‚[interleave_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.interleave_datasets)å’Œ[concatenate_datasets()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.concatenate_datasets)éƒ½é€‚ç”¨äºŽå¸¸è§„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å’Œ[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)å¯¹è±¡ã€‚è¯·å‚è€ƒ[Stream](./stream#interleave)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•äº¤é”™[IterableDataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.IterableDataset)å¯¹è±¡çš„ç¤ºä¾‹ã€‚

æ‚¨å¯ä»¥ä¸ºæ¯ä¸ªåŽŸå§‹æ•°æ®é›†å®šä¹‰é‡‡æ ·æ¦‚çŽ‡ï¼Œä»¥æŒ‡å®šå¦‚ä½•äº¤é”™è¿™äº›æ•°æ®é›†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–°æ•°æ®é›†æ˜¯é€šè¿‡ä»Žéšæœºæ•°æ®é›†é€ä¸ªèŽ·å–ç¤ºä¾‹æž„å»ºçš„ï¼Œç›´åˆ°å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†è€—å°½æ ·æœ¬ã€‚

```py
>>> seed = 42
>>> probabilities = [0.3, 0.5, 0.2]
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
>>> dataset["a"]
[10, 11, 20, 12, 0, 21, 13]
```

æ‚¨è¿˜å¯ä»¥æŒ‡å®š`stopping_strategy`ã€‚é»˜è®¤ç­–ç•¥`first_exhausted`æ˜¯ä¸€ç§å­é‡‡æ ·ç­–ç•¥ï¼Œå³æ•°æ®é›†æž„å»ºåœ¨å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†è€—å°½æ ·æœ¬æ—¶åœæ­¢ã€‚æ‚¨å¯ä»¥æŒ‡å®š`stopping_strategy=all_exhausted`æ¥æ‰§è¡Œä¸€ç§è¿‡é‡‡æ ·ç­–ç•¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®é›†æž„å»ºå°†åœ¨æ¯ä¸ªæ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬è‡³å°‘æ·»åŠ ä¸€æ¬¡åŽåœæ­¢ã€‚å®žé™…ä¸Šï¼Œè¿™æ„å‘³ç€å¦‚æžœä¸€ä¸ªæ•°æ®é›†è€—å°½ï¼Œå®ƒå°†è¿”å›žåˆ°è¯¥æ•°æ®é›†çš„å¼€å¤´ï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ¡ä»¶ã€‚è¯·æ³¨æ„ï¼Œå¦‚æžœæœªæŒ‡å®šé‡‡æ ·æ¦‚çŽ‡ï¼Œåˆ™æ–°æ•°æ®é›†å°†å…·æœ‰`max_length_datasets*nb_dataset`ä¸ªæ ·æœ¬ã€‚

```py
>>> d1 = Dataset.from_dict({"a": [0, 1, 2]})
>>> d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
>>> d3 = Dataset.from_dict({"a": [20, 21, 22]})
>>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
>>> dataset["a"]
[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]
```

## æ ¼å¼

[set_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_format)å‡½æ•°å¯æ›´æ”¹åˆ—çš„æ ¼å¼ï¼Œä½¿å…¶ä¸Žä¸€äº›å¸¸è§æ•°æ®æ ¼å¼å…¼å®¹ã€‚åœ¨`type`å‚æ•°ä¸­æŒ‡å®šæ‚¨æƒ³è¦çš„è¾“å‡ºä»¥åŠè¦æ ¼å¼åŒ–çš„åˆ—ã€‚æ ¼å¼åŒ–æ˜¯å³æ—¶åº”ç”¨çš„ã€‚

ä¾‹å¦‚ï¼Œé€šè¿‡è®¾ç½®`type="torch"`åˆ›å»ºPyTorchå¼ é‡ï¼š

```py
>>> import torch
>>> dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

[with_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.with_format)å‡½æ•°è¿˜å¯ä»¥æ›´æ”¹åˆ—çš„æ ¼å¼ï¼Œä½†å®ƒè¿”å›žä¸€ä¸ªæ–°çš„[Dataset](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset)å¯¹è±¡ï¼š

```py
>>> dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
```

ðŸ¤— æ•°æ®é›†è¿˜æ”¯æŒå…¶ä»–å¸¸è§æ•°æ®æ ¼å¼ï¼Œå¦‚NumPyã€Pandaså’ŒJAXã€‚æŸ¥çœ‹[ä½¿ç”¨ TensorFlow ä¸Žæ•°æ®é›†](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•é«˜æ•ˆåˆ›å»º TensorFlow æ•°æ®é›†çš„æ›´å¤šç»†èŠ‚ã€‚

å¦‚æžœæ‚¨éœ€è¦å°†æ•°æ®é›†é‡ç½®ä¸ºå…¶åŽŸå§‹æ ¼å¼ï¼Œè¯·ä½¿ç”¨[reset_format()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.reset_format)å‡½æ•°ï¼š

```py
>>> dataset.format
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
>>> dataset.reset_format()
>>> dataset.format
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### æ ¼å¼è½¬æ¢

[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)å‡½æ•°åœ¨è¿è¡Œæ—¶åº”ç”¨è‡ªå®šä¹‰æ ¼å¼è½¬æ¢ã€‚æ­¤å‡½æ•°å°†æ›¿æ¢ä»»ä½•å…ˆå‰æŒ‡å®šçš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ­¤å‡½æ•°åœ¨è¿è¡Œæ—¶å¯¹æ ‡è®°è¿›è¡Œæ ‡è®°åŒ–å’Œå¡«å……ã€‚ä»…å½“è®¿é—®ç¤ºä¾‹æ—¶æ‰åº”ç”¨æ ‡è®°åŒ–ï¼š

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
>>> def encode(batch):
...     return tokenizer(batch["sentence1"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
>>> dataset.set_transform(encode)
>>> dataset.format
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)å‡½æ•°è§£ç ä¸å—[Features](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Features)æ”¯æŒçš„æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œ[Audio](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Audio)ç‰¹å¾ä½¿ç”¨[`soundfile`](https://python-soundfile.readthedocs.io/en/0.11.0/) - ä¸€ä¸ªå¿«é€Ÿç®€å•çš„å®‰è£…åº“ - ä½†å®ƒä¸æ”¯æŒè¾ƒå°‘å¸¸è§çš„éŸ³é¢‘æ ¼å¼ã€‚åœ¨è¿™é‡Œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[set_transform()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.set_transform)åœ¨è¿è¡Œæ—¶åº”ç”¨è‡ªå®šä¹‰è§£ç è½¬æ¢ã€‚æ‚¨å¯ä»¥è‡ªç”±é€‰æ‹©ä»»ä½•åº“æ¥è§£ç éŸ³é¢‘æ–‡ä»¶ã€‚

ä»¥ä¸‹ç¤ºä¾‹ä½¿ç”¨[`pydub`](http://pydub.com/)åŒ…æ‰“å¼€ä¸€ä¸ª`soundfile`ä¸æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼š

```py
>>> import numpy as np
>>> from pydub import AudioSegment

>>> audio_dataset_amr = Dataset.from_dict({"audio": ["audio_samples/audio.amr"]})

>>> def decode_audio_with_pydub(batch, sampling_rate=16_000):
...     def pydub_decode_file(audio_path):
...         sound = AudioSegment.from_file(audio_path)
...         if sound.frame_rate != sampling_rate:
...             sound = sound.set_frame_rate(sampling_rate)
...         channel_sounds = sound.split_to_mono()
...         samples = [s.get_array_of_samples() for s in channel_sounds]
...         fp_arr = np.array(samples).T.astype(np.float32)
...         fp_arr /= np.iinfo(samples[0].typecode).max
...         return fp_arr
...
...     batch["audio"] = [pydub_decode_file(audio_path) for audio_path in batch["audio"]]
...     return batch

>>> audio_dataset_amr.set_transform(decode_audio_with_pydub)
```

## ä¿å­˜

ä¸€æ—¦æ‚¨å®Œæˆå¤„ç†æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨[save_to_disk()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk)ä¿å­˜å¹¶ä»¥åŽé‡å¤ä½¿ç”¨ã€‚

é€šè¿‡æä¾›è¦ä¿å­˜æ•°æ®é›†çš„ç›®å½•è·¯å¾„æ¥ä¿å­˜æ•°æ®é›†ï¼š

```py
>>> encoded_dataset.save_to_disk("path/of/my/dataset/directory")
```

ä½¿ç”¨[load_from_disk()](/docs/datasets/v2.17.0/en/package_reference/loading_methods#datasets.load_from_disk)å‡½æ•°é‡æ–°åŠ è½½æ•°æ®é›†ï¼š

```py
>>> from datasets import load_from_disk
>>> reloaded_dataset = load_from_disk("path/of/my/dataset/directory")
```

æƒ³è¦å°†æ•°æ®é›†ä¿å­˜åˆ°äº‘å­˜å‚¨æä¾›å•†å—ï¼Ÿé˜…è¯»æˆ‘ä»¬çš„[äº‘å­˜å‚¨](./filesystems)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•å°†æ•°æ®é›†ä¿å­˜åˆ° AWS æˆ– Google Cloud Storageã€‚

## å¯¼å‡º

ðŸ¤— æ•°æ®é›†è¿˜æ”¯æŒå¯¼å‡ºï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥åœ¨å…¶ä»–åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ•°æ®é›†ã€‚ä»¥ä¸‹è¡¨æ ¼æ˜¾ç¤ºå½“å‰æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œæ‚¨å¯ä»¥å¯¼å‡ºåˆ°è¿™äº›æ ¼å¼ï¼š

| æ–‡ä»¶ç±»åž‹ | å¯¼å‡ºæ–¹æ³• |
| --- | --- |
| CSV | [Dataset.to_csv()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_csv) |
| JSON | [Dataset.to_json()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_json) |
| Parquet | [Dataset.to_parquet()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_parquet) |
| SQL | [Dataset.to_sql()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_sql) |
| å†…å­˜ä¸­çš„ Python å¯¹è±¡ | [Dataset.to_pandas()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_pandas) æˆ– [Dataset.to_dict()](/docs/datasets/v2.17.0/en/package_reference/main_classes#datasets.Dataset.to_dict) |

ä¾‹å¦‚ï¼Œåƒè¿™æ ·å°†æ•°æ®é›†å¯¼å‡ºåˆ° CSV æ–‡ä»¶ï¼š

```py
>>> encoded_dataset.to_csv("path/of/my/dataset.csv")
```
