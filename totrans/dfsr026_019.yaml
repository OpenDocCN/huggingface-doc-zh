- en: Load safetensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors](https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/diffusers/v0.26.3/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/start.99629b4a.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/scheduler.182ea377.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/singletons.fade7992.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.1f6d62f6.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/paths.108a236d.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/entry/app.2b3eaeb0.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/index.abf12888.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/0.3862a335.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/nodes/191.6b916b11.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/CodeBlock.57fe6e13.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/DocNotebookDropdown.5fa27ace.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/diffusers/v0.26.3/en/_app/immutable/chunks/Heading.16916d63.js">
  prefs: []
  type: TYPE_NORMAL
- en: '[safetensors](https://github.com/huggingface/safetensors) is a safe and fast
    file format for storing and loading tensors. Typically, PyTorch model weights
    are saved or *pickled* into a `.bin` file with Pythonâ€™s [`pickle`](https://docs.python.org/3/library/pickle.html)
    utility. However, `pickle` is not secure and pickled files may contain malicious
    code that can be executed. safetensors is a secure alternative to `pickle`, making
    it ideal for sharing model weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide will show you how you load `.safetensor` files, and how to convert
    Stable Diffusion model weights stored in other formats to `.safetensor`. Before
    you start, make sure you have safetensors installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you look at the [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main)
    repository, youâ€™ll see weights inside the `text_encoder`, `unet` and `vae` subfolders
    are stored in the `.safetensors` format. By default, ðŸ¤— Diffusers automatically
    loads these `.safetensors` files from their subfolders if theyâ€™re available in
    the model repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more explicit control, you can optionally set `use_safetensors=True` (if
    `safetensors` is not installed, youâ€™ll get an error message asking you to install
    it):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, model weights are not necessarily stored in separate subfolders like
    in the example above. Sometimes, all the weights are stored in a single `.safetensors`
    file. In this case, if the weights are Stable Diffusion weights, you can load
    the file directly with the [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file)
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Convert to safetensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all weights on the Hub are available in the `.safetensors` format, and you
    may encounter weights stored as `.bin`. In this case, use the [Convert Space](https://huggingface.co/spaces/diffusers/convert)
    to convert the weights to `.safetensors`. The Convert Space downloads the pickled
    weights, converts them, and opens a Pull Request to upload the newly converted
    `.safetensors` file on the Hub. This way, if there is any malicious code contained
    in the pickled files, theyâ€™re uploaded to the Hub - which has a [security scanner](https://huggingface.co/docs/hub/security-pickle#hubs-security-scanner)
    to detect unsafe files and suspicious pickle imports - instead of your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the model with the new `.safetensors` weights by specifying the
    reference to the Pull Request in the `revision` parameter (you can also test it
    in this [Check PR](https://huggingface.co/spaces/diffusers/check_pr) Space on
    the Hub), for example `refs/pr/22`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Why use safetensors?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several reasons for using safetensors:'
  prefs: []
  type: TYPE_NORMAL
- en: Safety is the number one reason for using safetensors. As open-source and model
    distribution grows, it is important to be able to trust the model weights you
    downloaded donâ€™t contain any malicious code. The current size of the header in
    safetensors prevents parsing extremely large JSON files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading speed between switching models is another reason to use safetensors,
    which performs zero-copy of the tensors. It is especially fast compared to `pickle`
    if youâ€™re loading the weights to CPU (the default case), and just as fast if not
    faster when directly loading the weights to GPU. Youâ€™ll only notice the performance
    difference if the model is already loaded, and not if youâ€™re downloading the weights
    or loading the model for the first time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The time it takes to load the entire pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'But the actual time it takes to load 500MB of the model weights is only:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Lazy loading is also supported in safetensors, which is useful in distributed
    settings to only load some of the tensors. This format allowed the [BLOOM](https://huggingface.co/bigscience/bloom)
    model to be loaded in 45 seconds on 8 GPUs instead of 10 minutes with regular
    PyTorch weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
