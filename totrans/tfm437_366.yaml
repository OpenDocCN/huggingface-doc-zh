- en: SAM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sam](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sam)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAM (Segment Anything Model) was proposed in [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf)
    by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
    Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar,
    Ross Girshick.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The model can be used to predict segmentation masks of any object of interest
    given an input image.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![example image](../Images/b675e4971b3688355f2116729e579b05.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce the Segment Anything (SA) project: a new task, model, and dataset
    for image segmentation. Using our efficient model in a data collection loop, we
    built the largest segmentation dataset to date (by far), with over 1 billion masks
    on 11M licensed and privacy respecting images. The model is designed and trained
    to be promptable, so it can transfer zero-shot to new image distributions and
    tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot
    performance is impressive — often competitive with or even superior to prior fully
    supervised results. We are releasing the Segment Anything Model (SAM) and corresponding
    dataset (SA-1B) of 1B masks and 11M images at [https://segment-anything.com](https://segment-anything.com)
    to foster research into foundation models for computer vision.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'Tips:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The model predicts binary masks that states the presence or not of the object
    of interest given an image.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model predicts much better results if input 2D points and/or input bounding
    boxes are provided
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can prompt multiple points for the same image, and predict a single mask.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the model is not supported yet
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the paper, textual input should be also supported. However, at
    this time of writing this seems to be not supported according to [the official
    repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and
    [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/facebookresearch/segment-anything).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example on how to run mask generation given an image and a 2D point:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also process your own masks alongside the input images in the processor
    to be passed to the model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Resources:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb)
    for using the model.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/automatic_mask_generation.ipynb)
    for using the automatic mask generation pipeline.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb)
    for inference with MedSAM, a fine-tuned version of SAM on the medical domain.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb)
    for fine-tuning the model on custom data.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SamConfig
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamConfig`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L237)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '`vision_config` (Union[`dict`, `SamVisionConfig`], *optional*) — Dictionary
    of configuration options used to initialize [SamVisionConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamVisionConfig).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_encoder_config` (Union[`dict`, `SamPromptEncoderConfig`], *optional*)
    — Dictionary of configuration options used to initialize [SamPromptEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamPromptEncoderConfig).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_decoder_config` (Union[`dict`, `SamMaskDecoderConfig`], *optional*) —
    Dictionary of configuration options used to initialize [SamMaskDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamMaskDecoderConfig).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_decoder_config` (Union[`dict`, `SamMaskDecoderConfig`], *optional*) —
    用于初始化 [SamMaskDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamMaskDecoderConfig)
    的配置选项字典。'
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kwargs` (*optional*) — 关键字参数的字典。'
- en: '[SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig)
    is the configuration class to store the configuration of a [SamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamModel).
    It is used to instantiate a SAM model according to the specified arguments, defining
    the vision model, prompt-encoder model and mask decoder configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the SAM-ViT-H [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig)
    是用于存储 [SamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamModel)
    配置的类。它用于根据指定的参数实例化 SAM 模型，定义视觉模型、提示编码器模型和掩码解码器配置。使用默认值实例化配置将产生类似于 SAM-ViT-H [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    架构的配置。'
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自
    [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    的文档以获取更多信息。
- en: 'Example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '示例:'
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: SamVisionConfig
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamVisionConfig
- en: '### `class transformers.SamVisionConfig`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamVisionConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L139)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L139)'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化层的维度。'
- en: '`output_channels` (`int`, *optional*, defaults to 256) — Dimensionality of
    the output channels in the Patch Encoder.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_channels` (`int`, *optional*, 默认为256) — Patch Encoder 中输出通道的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer 编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer 编码器中每个注意力层的注意力头数量。'
- en: '`num_channels` (`int`, *optional*, defaults to 3) — Number of channels in the
    input image.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_channels` (`int`, *optional*, 默认为3) — 输入图像中的通道数。'
- en: '`image_size` (`int`, *optional*, defaults to 1024) — Expected resolution. Target
    size of the resized input image.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *optional*, 默认为1024) — 期望的分辨率。调整大小的输入图像的目标尺寸。'
- en: '`patch_size` (`int`, *optional*, defaults to 16) — Size of the patches to be
    extracted from the input image.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *optional*, 默认为16) — 从输入图像中提取的补丁的大小。'
- en: '`hidden_act` (`str`, *optional*, defaults to `"gelu"`) — The non-linear activation
    function (function or string)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`, *optional*, 默认为`"gelu"`) — 非线性激活函数（函数或字符串）。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-06) — 层归一化层使用的 epsilon。'
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_dropout` (`float`, *optional*, 默认为0.0) — 注意力概率的丢弃比率。'
- en: '`initializer_range` (`float`, *optional*, defaults to 1e-10) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为1e-10) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to query, key, value projections.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`qkv_bias` (`bool`, *optional*, 默认为`True`) — 是否为查询、键、值投影添加偏置。'
- en: '`mlp_ratio` (`float`, *optional*, defaults to 4.0) — Ratio of mlp hidden dim
    to embedding dim.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_ratio` (`float`, *optional*, 默认为4.0) — mlp 隐藏维度与嵌入维度的比率。'
- en: '`use_abs_pos` (`bool`, *optional*, defaults to `True`) — Whether to use absolute
    position embedding.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_abs_pos` (`bool`, *optional*, 默认为`True`) — 是否使用绝对位置嵌入。'
- en: '`use_rel_pos` (`bool`, *optional*, defaults to `True`) — Whether to use relative
    position embedding.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_rel_pos` (`bool`, *optional*, 默认为`True`) — 是否使用相对位置嵌入。'
- en: '`window_size` (`int`, *optional*, defaults to 14) — Window size for relative
    position.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size` (`int`, *optional*, 默认为14) — 相对位置的窗口大小。'
- en: '`global_attn_indexes` (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`)
    — The indexes of the global attention layers.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_attn_indexes` (`List[int]`, *optional*, 默认为`[2, 5, 8, 11]`) — 全局注意力层的索引。'
- en: '`num_pos_feats` (`int`, *optional*, defaults to 128) — The dimensionality of
    the position embedding.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_pos_feats` (`int`, *optional*, 默认为128) — 位置嵌入的维度。'
- en: '`mlp_dim` (`int`, *optional*) — The dimensionality of the MLP layer in the
    Transformer encoder. If `None`, defaults to `mlp_ratio * hidden_size`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_dim` (`int`, *optional*) — Transformer 编码器中 MLP 层的维度。如果为 `None`，则默认为 `mlp_ratio
    * hidden_size`。'
- en: This is the configuration class to store the configuration of a `SamVisionModel`.
    It is used to instantiate a SAM vision encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration defaults will yield
    a similar configuration to that of the SAM ViT-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储 `SamVisionModel` 配置的类。它用于根据指定的参数实例化 SAM 视觉编码器，定义模型架构。使用默认值实例化配置将产生类似于
    SAM ViT-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: SamMaskDecoderConfig
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamMaskDecoderConfig
- en: '### `class transformers.SamMaskDecoderConfig`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamMaskDecoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L78)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L78)'
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    hidden states.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为256) — 隐藏状态的维度。'
- en: '`hidden_act` (`str`, *optional*, defaults to `"relu"`) — The non-linear activation
    function used inside the `SamMaskDecoder` module.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`, *可选*, 默认为`"relu"`) — 在`SamMaskDecoder`模块内部使用的非线性激活函数。'
- en: '`mlp_dim` (`int`, *optional*, defaults to 2048) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp_dim` (`int`, *可选*, 默认为2048) — Transformer编码器中“中间”（即前馈）层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 2) — Number of hidden layers
    in the Transformer encoder.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *可选*, 默认为2) — Transformer编码器中的隐藏层数。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *可选*, 默认为8) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`attention_downsample_rate` (`int`, *optional*, defaults to 2) — The downsampling
    rate of the attention layer.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_downsample_rate` (`int`, *可选*, 默认为2) — 注意力层的下采样率。'
- en: '`num_multimask_outputs` (`int`, *optional*, defaults to 3) — The number of
    outputs from the `SamMaskDecoder` module. In the Segment Anything paper, this
    is set to 3.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_multimask_outputs` (`int`, *可选*, 默认为3) — `SamMaskDecoder`模块的输出数量。在“Segment
    Anything”论文中，此值设置为3。'
- en: '`iou_head_depth` (`int`, *optional*, defaults to 3) — The number of layers
    in the IoU head module.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iou_head_depth` (`int`, *可选*, 默认为3) — IoU头模块中的层数。'
- en: '`iou_head_hidden_dim` (`int`, *optional*, defaults to 256) — The dimensionality
    of the hidden states in the IoU head module.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iou_head_hidden_dim` (`int`, *可选*, 默认为256) — IoU头模块中隐藏状态的维度。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *可选*, 默认为1e-06) — 层归一化层使用的epsilon。'
- en: This is the configuration class to store the configuration of a `SamMaskDecoder`.
    It is used to instantiate a SAM mask decoder to the specified arguments, defining
    the model architecture. Instantiating a configuration defaults will yield a similar
    configuration to that of the SAM-vit-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储`SamMaskDecoder`配置的配置类。它用于实例化一个SAM掩码解码器到指定的参数，定义模型架构。实例化配置默认将产生类似于SAM-vit-h
    [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: SamPromptEncoderConfig
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamPromptEncoderConfig
- en: '### `class transformers.SamPromptEncoderConfig`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamPromptEncoderConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L31)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L31)'
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    hidden states.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *可选*, 默认为256) — 隐藏状态的维度。'
- en: '`image_size` (`int`, *optional*, defaults to 1024) — The expected output resolution
    of the image.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_size` (`int`, *可选*, 默认为1024) — 图像的预期输出分辨率。'
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patch_size` (`int`, *可选*, 默认为16) — 每个补丁的大小（分辨率）。'
- en: '`mask_input_channels` (`int`, *optional*, defaults to 16) — The number of channels
    to be fed to the `MaskDecoder` module.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_input_channels` (`int`, *可选*, 默认为16) — 要馈送到`MaskDecoder`模块的通道数。'
- en: '`num_point_embeddings` (`int`, *optional*, defaults to 4) — The number of point
    embeddings to be used.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_point_embeddings` (`int`, *可选*, 默认为4) — 要使用的点嵌入数量。'
- en: '`hidden_act` (`str`, *optional*, defaults to `"gelu"`) — The non-linear activation
    function in the encoder and pooler.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数。'
- en: This is the configuration class to store the configuration of a `SamPromptEncoder`.
    The `SamPromptEncoder` module is used to encode the input 2D points and bounding
    boxes. Instantiating a configuration defaults will yield a similar configuration
    to that of the SAM-vit-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储`SamPromptEncoder`配置的配置类。`SamPromptEncoder`模块用于编码输入的2D点和边界框。实例化配置默认将产生类似于SAM-vit-h
    [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)架构的配置。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: SamProcessor
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamProcessor
- en: '### `class transformers.SamProcessor`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/processing_sam.py#L35)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/processing_sam.py#L35)'
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Parameters
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`image_processor` (`SamImageProcessor`) — An instance of [SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor).
    The image processor is a required input.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_processor` (`SamImageProcessor`) — [SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor)的一个实例。图像处理器是一个必需的输入。'
- en: Constructs a SAM processor which wraps a SAM image processor and an 2D points
    & Bounding boxes processor into a single processor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 构造一个SAM处理器，将SAM图像处理器和2D点和边界框处理器包装成一个单一处理器。
- en: '[SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor)
    offers all the functionalities of [SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for more information.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor)提供了[SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor)的所有功能。有关更多信息，请参阅[**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)的文档字符串。'
- en: SamImageProcessor
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamImageProcessor
- en: '### `class transformers.SamImageProcessor`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamImageProcessor`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L64)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L64)'
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *可选*, 默认为 `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以通过`preprocess`方法中的`do_resize`参数进行覆盖。'
- en: '`size` (`dict`, *optional*, defaults to `{"longest_edge" -- 1024}`): Size of
    the output image after resizing. Resizes the longest edge of the image to match
    `size["longest_edge"]` while maintaining the aspect ratio. Can be overridden by
    the `size` parameter in the `preprocess` method.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`dict`, *可选*, 默认为 `{"longest_edge" -- 1024}`): 调整大小后的输出图像大小。将图像的最长边调整为匹配`size["longest_edge"]`，同时保持纵横比。可以通过`preprocess`方法中的`size`参数进行覆盖。'
- en: '`mask_size` (`dict`, *optional*, defaults to `{"longest_edge" -- 256}`): Size
    of the output segmentation map after resizing. Resizes the longest edge of the
    image to match `size["longest_edge"]` while maintaining the aspect ratio. Can
    be overridden by the `mask_size` parameter in the `preprocess` method.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_size` (`dict`, *可选*, 默认为 `{"longest_edge" -- 256}`): 调整大小后的输出分割地图大小。将图像的最长边调整为匹配`size["longest_edge"]`，同时保持纵横比。可以通过`preprocess`方法中的`mask_size`参数进行覆盖。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *可选*, 默认为 `Resampling.BILINEAR`) — 如果调整图像大小，则使用的重采样滤波器。可以通过`preprocess`方法中的`resample`参数进行覆盖。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Wwhether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *可选*, 默认为 `True`) — 是否按指定比例`rescale_factor`重新缩放图像。可以通过`preprocess`方法中的`do_rescale`参数进行覆盖。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Only has an effect if `do_rescale` is set
    to `True`. Can be overridden by the `rescale_factor` parameter in the `preprocess`
    method.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale_factor` (`int` 或 `float`, *可选*, 默认为 `1/255`) — 如果重新缩放图像，则使用的比例因子。仅在`do_rescale`设置为`True`时有效。可以通过`preprocess`方法中的`rescale_factor`参数进行覆盖。'
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_normalize` (`bool`, *可选*, 默认为 `True`) — 是否对图像进行归一化。可以通过`preprocess`方法中的`do_normalize`参数进行覆盖。可以通过`preprocess`方法中的`do_normalize`参数进行覆盖。'
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_DEFAULT_MEAN`) —
    如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_mean`参数进行覆盖。可以通过`preprocess`方法中的`image_mean`参数进行覆盖。'
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_DEFAULT_STD`) — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过`preprocess`方法中的`image_std`参数进行覆盖。可以通过`preprocess`方法中的`image_std`参数进行覆盖。'
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to the specified `pad_size`. Can be overridden by the `do_pad` parameter in the
    `preprocess` method.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`dict`, *optional*, defaults to `{"height" -- 1024, "width": 1024}`):
    Size of the output image after padding. Can be overridden by the `pad_size` parameter
    in the `preprocess` method.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_pad_size` (`dict`, *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the output segmentation map after padding. Can be overridden by
    the `mask_pad_size` parameter in the `preprocess` method.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `True`) — Whether to convert
    the image to RGB.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SAM image processor.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '#### `filter_masks`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L805)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Parameters
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '`masks` (`Union[torch.Tensor, tf.Tensor]`) — Input masks.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iou_scores` (`Union[torch.Tensor, tf.Tensor]`) — List of IoU scores.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`original_size` (`Tuple[int,int]`) — Size of the orginal image.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cropped_box_image` (`np.array`) — The cropped image.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pred_iou_thresh` (`float`, *optional*, defaults to 0.88) — The threshold for
    the iou scores.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_thresh` (`float`, *optional*, defaults to 0.95) — The threshold
    for the stability score.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0) — The threshold for the
    predicted masks.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_offset` (`float`, *optional*, defaults to 1) — The offset
    for the stability score used in the `_compute_stability_score` method.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters the predicted masks by selecting only the ones that meets several criteria.
    The first criterion being that the iou scores needs to be greater than `pred_iou_thresh`.
    The second criterion is that the stability score needs to be greater than `stability_score_thresh`.
    The method also converts the predicted masks to bounding boxes and pad the predicted
    masks if necessary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate_crop_boxes`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L740)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.array`) — Input original image'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_size` (`int`) — Target size of the resized image'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_n_layers` (`int`, *optional*, defaults to 0) — If >0, mask prediction
    will be run again on crops of the image. Sets the number of layers to run, where
    each layer has 2**i_layer number of image crops.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_ratio` (`float`, *optional*, defaults to 512/1500) — Sets the degree
    to which crops overlap. In the first crop layer, crops will overlap by this fraction
    of the image length. Later layers with more crops scale down this overlap.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`points_per_crop` (`int`, *optional*, defaults to 32) — Number of points to
    sample from each crop.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_n_points_downscale_factor` (`List[int]`, *optional*, defaults to 1) —
    The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`torch.device`, *optional*, defaults to None) — Device to use for
    the computation. If None, cpu will be used.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) — The channel
    dimension format of the input image. If not provided, it will be inferred.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a list of crop boxes of different sizes. Each layer has (2**i)**2
    boxes for the ith layer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pad_image`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L163)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Parameters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.ndarray`) — Image to pad.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`) — Size of the output image after padding.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`str` or `ChannelDimension`, *optional*) — The data format of
    the image. Can be either “channels_first” or “channels_last”. If `None`, the `data_format`
    of the `image` will be used.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format` (`str` 或 `ChannelDimension`, *optional*) — 图像的数据格式。可以是 “channels_first”
    或 “channels_last”。如果为 `None`，将使用 `image` 的 `data_format`。'
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) — The channel
    dimension format of the input image. If not provided, it will be inferred.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_data_format` (`str` 或 `ChannelDimension`, *optional*) — 输入图像的通道维度格式。如果未提供，将被推断。'
- en: Pad an image to `(pad_size["height"], pad_size["width"])` with zeros to the
    right and bottom.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 用零填充图像至 `(pad_size["height"], pad_size["width"])`，填充到右侧和底部。
- en: '#### `post_process_for_mask_generation`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_for_mask_generation`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L717)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L717)'
- en: '[PRE12]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`all_masks` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all predicted
    segmentation masks'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_masks` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — 所有预测的分割掩码列表'
- en: '`all_scores` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all predicted
    iou scores'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_scores` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — 所有预测的iou分数列表'
- en: '`all_boxes` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all bounding
    boxes of the predicted masks'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_boxes` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — 所有预测掩码的边界框列表'
- en: '`crops_nms_thresh` (`float`) — Threshold for NMS (Non Maximum Suppression)
    algorithm.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`crops_nms_thresh` (`float`) — NMS（非最大抑制）算法的阈值。'
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`, *optional*, 默认为 `pt`) — 如果为 `pt`，返回 `torch.Tensor`。如果为
    `tf`，返回 `tf.Tensor`。'
- en: Post processes mask that are generated by calling the Non Maximum Suppression
    algorithm on the predicted masks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对通过调用预测掩码上的非最大抑制算法生成的掩码进行后处理。
- en: '#### `post_process_masks`'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `post_process_masks`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L573)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L573)'
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`masks` (`Union[List[torch.Tensor], List[np.ndarray], List[tf.Tensor]]`) —
    Batched masks from the mask_decoder in (batch_size, num_channels, height, width)
    format.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`masks` (`Union[List[torch.Tensor], List[np.ndarray], List[tf.Tensor]]`) —
    来自 mask_decoder 的批量掩码，格式为 (batch_size, num_channels, height, width)。'
- en: '`original_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`) —
    The original sizes of each image before it was resized to the model’s expected
    input shape, in (height, width) format.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`original_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`) —
    每个图像在调整大小为模型期望的输入形状之前的原始尺寸，格式为 (height, width)。'
- en: '`reshaped_input_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`)
    — The size of each image as it is fed to the model, in (height, width) format.
    Used to remove padding.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reshaped_input_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`)
    — 每个图像作为输入模型时的大小，格式为 (height, width)。用于去除填充。'
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.0) — The threshold to
    use for binarizing the masks.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_threshold` (`float`, *optional*, 默认为 0.0) — 用于对掩码进行二值化的阈值。'
- en: '`binarize` (`bool`, *optional*, defaults to `True`) — Whether to binarize the
    masks.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binarize` (`bool`, *optional*, 默认为 `True`) — 是否对掩码进行二值化。'
- en: '`pad_size` (`int`, *optional*, defaults to `self.pad_size`) — The target size
    the images were padded to before being passed to the model. If None, the target
    size is assumed to be the processor’s `pad_size`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_size` (`int`, *optional*, 默认为 `self.pad_size`) — 图像传递给模型之前填充到的目标大小。如果为
    None，则假定目标大小为处理器的 `pad_size`。'
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — If `"pt"`, return
    PyTorch tensors. If `"tf"`, return TensorFlow tensors.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str`, *optional*, 默认为 `"pt"`) — 如果为 `"pt"`，返回 PyTorch 张量。如果为
    `"tf"`，返回 TensorFlow 张量。'
- en: Returns
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: (`Union[torch.Tensor, tf.Tensor]`)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: (`Union[torch.Tensor, tf.Tensor]`)
- en: Batched masks in batch_size, num_channels, height, width) format, where (height,
    width) is given by original_size.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 批量掩码，格式为 (batch_size, num_channels, height, width)，其中 (height, width) 由 original_size
    给出。
- en: Remove padding and upscale masks to the original image size.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 去除填充并将掩码放大到原始图像大小。
- en: '#### `preprocess`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `preprocess`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L389)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L389)'
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parameters
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images` (`ImageInput`) — 要预处理的图像。期望单个图像或批量图像，像素值范围为 0 到 255。如果传入像素值在 0 到 1
    之间的图像，请设置 `do_rescale=False`。'
- en: '`segmentation_maps` (`ImageInput`, *optional*) — Segmentation map to preprocess.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`segmentation_maps` (`ImageInput`, *optional*) — 要预处理的分割地图。'
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_resize` (`bool`, *optional*, 默认为 `self.do_resize`) — 是否调整图像大小。'
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Controls the
    size of the image after `resize`. The longest edge of the image is resized to
    `size["longest_edge"]` whilst preserving the aspect ratio.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size` (`Dict[str, int]`, *optional*, 默认为 `self.size`) — 控制 `resize` 后图像的大小。图像的最长边被调整为
    `size["longest_edge"]`，同时保持纵横比。'
- en: '`mask_size` (`Dict[str, int]`, *optional*, defaults to `self.mask_size`) —
    Controls the size of the segmentation map after `resize`. The longest edge of
    the image is resized to `size["longest_edge"]` whilst preserving the aspect ratio.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_size` (`Dict[str, int]`, *optional*, 默认为 `self.mask_size`) — 控制 `resize`
    后分割地图的大小。图像的最长边被调整为 `size["longest_edge"]`，同时保持纵横比。'
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resample` (`PILImageResampling`, *optional*, 默认为 `self.resample`) — 调整图像大小时要使用的
    `PILImageResampling` 过滤器，例如 `PILImageResampling.BILINEAR`。'
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image pixel values by rescaling factor.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_rescale` (`bool`, *optional*, 默认为 `self.do_rescale`) — 是否通过缩放因子重新缩放图像像素值。'
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `self.rescale_factor`)
    — Rescale factor to apply to the image pixel values.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to normalize the image by if `do_normalize` is set to `True`.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to normalize the image by if `do_normalize` is set
    to `True`.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_pad` (`bool`, *optional*, defaults to `self.do_pad`) — Whether to pad the
    image.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`, *optional*, defaults to `self.pad_size`) — Controls
    the size of the padding applied to the image. The image is padded to `pad_size["height"]`
    and `pad_size["width"]` if `do_pad` is set to `True`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_pad_size` (`Dict[str, int]`, *optional*, defaults to `self.mask_pad_size`)
    — Controls the size of the padding applied to the segmentation map. The image
    is padded to `mask_pad_size["height"]` and `mask_pad_size["width"]` if `do_pad`
    is set to `True`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    Whether to convert the image to RGB.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the channel dimension format of the input image.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '#### `resize`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L211)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.ndarray`) — Image to resize.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`) — Dictionary in the format `{"longest_edge": int}`
    specifying the size of the output image. The longest edge of the image will be
    resized to the specified size, while the other edge will be resized to maintain
    the aspect ratio. resample — `PILImageResampling` filter to use when resizing
    the image e.g. `PILImageResampling.BILINEAR`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*) — The channel dimension
    format for the output image. If unset, the channel dimension format of the input
    image is used. Can be one of:'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '`np.ndarray`'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The resized image.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Resize an image to `(size["height"], size["width"])`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: SamModel
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SamModel
- en: '### `class transformers.SamModel`'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.SamModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1180)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1180)'
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Segment Anything Model (SAM) for generating segmentation masks, given an input
    image and optional 2D location and bounding boxes. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成分割掩模的Segment Anything Model (SAM)，给定输入图像和可选的2D位置和边界框。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1278)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1278)'
- en: '[PRE17]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor).
    See `SamProcessor.__call__()` for details.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pixel_values` (`torch.FloatTensor`，形状为`(batch_size, num_channels, height,
    width)`) — 像素值。像素值可以使用[SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor)获得。查看`SamProcessor.__call__()`以获取详细信息。'
- en: '`input_points` (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`)
    — Input 2D spatial points, this is used by the prompt encoder to encode the prompt.
    Generally yields to much better results. The points can be obtained by passing
    a list of list of list to the processor that will create corresponding `torch`
    tensors of dimension 4\. The first dimension is the image batch size, the second
    dimension is the point batch size (i.e. how many segmentation masks do we want
    the model to predict per input point), the third dimension is the number of points
    per segmentation mask (it is possible to pass multiple points for a single mask),
    and the last dimension is the x (vertical) and y (horizontal) coordinates of the
    point. If a different number of points is passed either for each image, or for
    each mask, the processor will create “PAD” points that will correspond to the
    (0, 0) coordinate, and the computation of the embedding will be skipped for these
    points using the labels.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_points` (`torch.FloatTensor`，形状为`(batch_size, num_points, 2)`) — 输入2D空间点，这由提示编码器用于编码提示。通常会产生更好的结果。点可以通过将列表的列表的列表传递给处理器来获得，处理器将创建相应的维度为4的`torch`张量。第一维是图像批处理大小，第二维是点批处理大小（即模型要预测每个输入点的分割掩模数量），第三维是每个分割掩模的点数（可以为单个掩模传递多个点），最后一维是点的x（垂直）和y（水平）坐标。如果为每个图像或每个掩模传递了不同数量的点，则处理器将创建“PAD”点，这些点将对应于（0,
    0）坐标，并且将跳过这些点的嵌入计算使用标签。'
- en: '`input_labels` (`torch.LongTensor` of shape `(batch_size, point_batch_size,
    num_points)`) — Input labels for the points, this is used by the prompt encoder
    to encode the prompt. According to the official implementation, there are 3 types
    of labels'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_labels` (`torch.LongTensor`，形状为`(batch_size, point_batch_size, num_points)`)
    — 点的输入标签，这由提示编码器用于编码提示。根据官方实现，有3种类型的标签'
- en: '`1`: the point is a point that contains the object of interest'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`: 该点是包含感兴趣对象的点'
- en: '`0`: the point is a point that does not contain the object of interest'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`: 该点是不包含感兴趣对象的点'
- en: '`-1`: the point corresponds to the background'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-1`: 该点对应于背景'
- en: 'We added the label:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们添加了标签：
- en: '`-10`: the point is a padding point, thus should be ignored by the prompt encoder'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-10`: 该点是填充点，因此应该被提示编码器忽略'
- en: The padding labels should be automatically done by the processor.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 填充标签应该由处理器自动完成。
- en: '`input_boxes` (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`) —
    Input boxes for the points, this is used by the prompt encoder to encode the prompt.
    Generally yields to much better generated masks. The boxes can be obtained by
    passing a list of list of list to the processor, that will generate a `torch`
    tensor, with each dimension corresponding respectively to the image batch size,
    the number of boxes per image and the coordinates of the top left and botton right
    point of the box. In the order (`x1`, `y1`, `x2`, `y2`):'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_boxes` (`torch.FloatTensor`，形状为`(batch_size, num_boxes, 4)`) — 用于点的输入框，这由提示编码器用于编码提示。通常会产生更好的生成掩模。框可以通过将列表的列表的列表传递给处理器来获得，处理器将生成一个`torch`张量，每个维度分别对应于图像批处理大小、每个图像的框数和框的左上角和右下角点的坐标。按顺序为(`x1`,
    `y1`, `x2`, `y2`)：'
- en: '`x1`: the x coordinate of the top left point of the input box'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x1`: 输入框左上角点的x坐标'
- en: '`y1`: the y coordinate of the top left point of the input box'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y1`: 输入框左上角点的y坐标'
- en: '`x2`: the x coordinate of the bottom right point of the input box'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y2`: the y coordinate of the bottom right point of the input box'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_masks` (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`)
    — SAM model also accepts segmentation masks as input. The mask will be embedded
    by the prompt encoder to generate a corresponding embedding, that will be fed
    later on to the mask decoder. These masks needs to be manually fed by the user,
    and they need to be of shape (`batch_size`, `image_size`, `image_size`).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_channels,
    window_size, window_size)`) — Image embeddings, this is used by the mask decder
    to generate masks and iou scores. For more memory efficient computation, users
    can first retrieve the image embeddings using the `get_image_embeddings` method,
    and then feed them to the `forward` method instead of feeding the `pixel_values`.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimask_output` (`bool`, *optional*) — In the original implementation and
    paper, the model always outputs 3 masks per image (or per point / per bounding
    box if relevant). However, it is possible to just output a single mask, that corresponds
    to the “best” mask, by specifying `multimask_output=False`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_similarity` (`torch.FloatTensor`, *optional*) — Attention similarity
    tensor, to be provided to the mask decoder for target-guided attention in case
    the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_embedding` (`torch.FloatTensor`, *optional*) — Embedding of the target
    concept, to be provided to the mask decoder for target-semantic prompting in case
    the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example —
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamModel)
    forward method, overrides the `__call__` special method.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: TFSamModel
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFSamModel`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_tf_sam.py#L1410)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment Anything Model (SAM) for generating segmentation masks, given an input
    image and optional 2D location and bounding boxes. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a TensorFlow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TensorFlow Model and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_tf_sam.py#L1505)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Pixel values can be obtained using [SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor).
    See `SamProcessor.__call__()` for details.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_points` (`tf.Tensor` of shape `(batch_size, num_points, 2)`) — Input
    2D spatial points, this is used by the prompt encoder to encode the prompt. Generally
    yields to much better results. The points can be obtained by passing a list of
    list of list to the processor that will create corresponding `tf` tensors of dimension
    4\. The first dimension is the image batch size, the second dimension is the point
    batch size (i.e. how many segmentation masks do we want the model to predict per
    input point), the third dimension is the number of points per segmentation mask
    (it is possible to pass multiple points for a single mask), and the last dimension
    is the x (vertical) and y (horizontal) coordinates of the point. If a different
    number of points is passed either for each image, or for each mask, the processor
    will create “PAD” points that will correspond to the (0, 0) coordinate, and the
    computation of the embedding will be skipped for these points using the labels.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_labels` (`tf.Tensor` of shape `(batch_size, point_batch_size, num_points)`)
    — Input labels for the points, this is used by the prompt encoder to encode the
    prompt. According to the official implementation, there are 3 types of labels'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: the point is a point that contains the object of interest'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0`: the point is a point that does not contain the object of interest'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-1`: the point corresponds to the background'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We added the label:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`-10`: the point is a padding point, thus should be ignored by the prompt encoder'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding labels should be automatically done by the processor.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_boxes` (`tf.Tensor` of shape `(batch_size, num_boxes, 4)`) — Input boxes
    for the points, this is used by the prompt encoder to encode the prompt. Generally
    yields to much better generated masks. The boxes can be obtained by passing a
    list of list of list to the processor, that will generate a `tf` tensor, with
    each dimension corresponding respectively to the image batch size, the number
    of boxes per image and the coordinates of the top left and botton right point
    of the box. In the order (`x1`, `y1`, `x2`, `y2`):'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x1`: the x coordinate of the top left point of the input box'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y1`: the y coordinate of the top left point of the input box'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x2`: the x coordinate of the bottom right point of the input box'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y2`: the y coordinate of the bottom right point of the input box'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_masks` (`tf.Tensor` of shape `(batch_size, image_size, image_size)`)
    — SAM model also accepts segmentation masks as input. The mask will be embedded
    by the prompt encoder to generate a corresponding embedding, that will be fed
    later on to the mask decoder. These masks needs to be manually fed by the user,
    and they need to be of shape (`batch_size`, `image_size`, `image_size`).'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`tf.Tensor` of shape `(batch_size, output_channels, window_size,
    window_size)`) — Image embeddings, this is used by the mask decder to generate
    masks and iou scores. For more memory efficient computation, users can first retrieve
    the image embeddings using the `get_image_embeddings` method, and then feed them
    to the `call` method instead of feeding the `pixel_values`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimask_output` (`bool`, *optional*) — In the original implementation and
    paper, the model always outputs 3 masks per image (or per point / per bounding
    box if relevant). However, it is possible to just output a single mask, that corresponds
    to the “best” mask, by specifying `multimask_output=False`.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFSamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.TFSamModel)
    forward method, overrides the `__call__` special method.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
