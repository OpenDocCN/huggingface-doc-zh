- en: SAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sam](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/sam)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SAM (Segment Anything Model) was proposed in [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf)
    by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
    Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar,
    Ross Girshick.
  prefs: []
  type: TYPE_NORMAL
- en: The model can be used to predict segmentation masks of any object of interest
    given an input image.
  prefs: []
  type: TYPE_NORMAL
- en: '![example image](../Images/b675e4971b3688355f2116729e579b05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce the Segment Anything (SA) project: a new task, model, and dataset
    for image segmentation. Using our efficient model in a data collection loop, we
    built the largest segmentation dataset to date (by far), with over 1 billion masks
    on 11M licensed and privacy respecting images. The model is designed and trained
    to be promptable, so it can transfer zero-shot to new image distributions and
    tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot
    performance is impressive — often competitive with or even superior to prior fully
    supervised results. We are releasing the Segment Anything Model (SAM) and corresponding
    dataset (SA-1B) of 1B masks and 11M images at [https://segment-anything.com](https://segment-anything.com)
    to foster research into foundation models for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: The model predicts binary masks that states the presence or not of the object
    of interest given an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model predicts much better results if input 2D points and/or input bounding
    boxes are provided
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can prompt multiple points for the same image, and predict a single mask.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning the model is not supported yet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the paper, textual input should be also supported. However, at
    this time of writing this seems to be not supported according to [the official
    repository](https://github.com/facebookresearch/segment-anything/issues/4#issuecomment-1497626844).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model was contributed by [ybelkada](https://huggingface.co/ybelkada) and
    [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/facebookresearch/segment-anything).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example on how to run mask generation given an image and a 2D point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can also process your own masks alongside the input images in the processor
    to be passed to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb)
    for using the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/huggingface/notebooks/blob/main/examples/automatic_mask_generation.ipynb)
    for using the automatic mask generation pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Run_inference_with_MedSAM_using_HuggingFace_Transformers.ipynb)
    for inference with MedSAM, a fine-tuned version of SAM on the medical domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb)
    for fine-tuning the model on custom data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SamConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vision_config` (Union[`dict`, `SamVisionConfig`], *optional*) — Dictionary
    of configuration options used to initialize [SamVisionConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamVisionConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_encoder_config` (Union[`dict`, `SamPromptEncoderConfig`], *optional*)
    — Dictionary of configuration options used to initialize [SamPromptEncoderConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamPromptEncoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_decoder_config` (Union[`dict`, `SamMaskDecoderConfig`], *optional*) —
    Dictionary of configuration options used to initialize [SamMaskDecoderConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamMaskDecoderConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (*optional*) — Dictionary of keyword arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig)
    is the configuration class to store the configuration of a [SamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamModel).
    It is used to instantiate a SAM model according to the specified arguments, defining
    the vision model, prompt-encoder model and mask decoder configs. Instantiating
    a configuration with the defaults will yield a similar configuration to that of
    the SAM-ViT-H [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: SamVisionConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamVisionConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L139)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_channels` (`int`, *optional*, defaults to 256) — Dimensionality of
    the output channels in the Patch Encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_channels` (`int`, *optional*, defaults to 3) — Number of channels in the
    input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 1024) — Expected resolution. Target
    size of the resized input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — Size of the patches to be
    extracted from the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str`, *optional*, defaults to `"gelu"`) — The non-linear activation
    function (function or string)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 1e-10) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`qkv_bias` (`bool`, *optional*, defaults to `True`) — Whether to add a bias
    to query, key, value projections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_ratio` (`float`, *optional*, defaults to 4.0) — Ratio of mlp hidden dim
    to embedding dim.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_abs_pos` (`bool`, *optional*, defaults to `True`) — Whether to use absolute
    position embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_rel_pos` (`bool`, *optional*, defaults to `True`) — Whether to use relative
    position embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` (`int`, *optional*, defaults to 14) — Window size for relative
    position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`global_attn_indexes` (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`)
    — The indexes of the global attention layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_pos_feats` (`int`, *optional*, defaults to 128) — The dimensionality of
    the position embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_dim` (`int`, *optional*) — The dimensionality of the MLP layer in the
    Transformer encoder. If `None`, defaults to `mlp_ratio * hidden_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a `SamVisionModel`.
    It is used to instantiate a SAM vision encoder according to the specified arguments,
    defining the model architecture. Instantiating a configuration defaults will yield
    a similar configuration to that of the SAM ViT-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: SamMaskDecoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamMaskDecoderConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L78)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str`, *optional*, defaults to `"relu"`) — The non-linear activation
    function used inside the `SamMaskDecoder` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp_dim` (`int`, *optional*, defaults to 2048) — Dimensionality of the “intermediate”
    (i.e., feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 2) — Number of hidden layers
    in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 8) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_downsample_rate` (`int`, *optional*, defaults to 2) — The downsampling
    rate of the attention layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_multimask_outputs` (`int`, *optional*, defaults to 3) — The number of
    outputs from the `SamMaskDecoder` module. In the Segment Anything paper, this
    is set to 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iou_head_depth` (`int`, *optional*, defaults to 3) — The number of layers
    in the IoU head module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iou_head_hidden_dim` (`int`, *optional*, defaults to 256) — The dimensionality
    of the hidden states in the IoU head module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-06) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a `SamMaskDecoder`.
    It is used to instantiate a SAM mask decoder to the specified arguments, defining
    the model architecture. Instantiating a configuration defaults will yield a similar
    configuration to that of the SAM-vit-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: SamPromptEncoderConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamPromptEncoderConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/configuration_sam.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 256) — Dimensionality of the
    hidden states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_size` (`int`, *optional*, defaults to 1024) — The expected output resolution
    of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patch_size` (`int`, *optional*, defaults to 16) — The size (resolution) of
    each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_input_channels` (`int`, *optional*, defaults to 16) — The number of channels
    to be fed to the `MaskDecoder` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_point_embeddings` (`int`, *optional*, defaults to 4) — The number of point
    embeddings to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str`, *optional*, defaults to `"gelu"`) — The non-linear activation
    function in the encoder and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a `SamPromptEncoder`.
    The `SamPromptEncoder` module is used to encode the input 2D points and bounding
    boxes. Instantiating a configuration defaults will yield a similar configuration
    to that of the SAM-vit-h [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: SamProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/processing_sam.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image_processor` (`SamImageProcessor`) — An instance of [SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor).
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SAM processor which wraps a SAM image processor and an 2D points
    & Bounding boxes processor into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor)
    offers all the functionalities of [SamImageProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamImageProcessor).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: SamImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamImageProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `True`) — Whether to resize the
    image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`dict`, *optional*, defaults to `{"longest_edge" -- 1024}`): Size of
    the output image after resizing. Resizes the longest edge of the image to match
    `size["longest_edge"]` while maintaining the aspect ratio. Can be overridden by
    the `size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_size` (`dict`, *optional*, defaults to `{"longest_edge" -- 256}`): Size
    of the output segmentation map after resizing. Resizes the longest edge of the
    image to match `size["longest_edge"]` while maintaining the aspect ratio. Can
    be overridden by the `mask_size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`)
    — Resampling filter to use if resizing the image. Can be overridden by the `resample`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `True`) — Wwhether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Only has an effect if `do_rescale` is set
    to `True`. Can be overridden by the `rescale_factor` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_pad` (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to the specified `pad_size`. Can be overridden by the `do_pad` parameter in the
    `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`dict`, *optional*, defaults to `{"height" -- 1024, "width": 1024}`):
    Size of the output image after padding. Can be overridden by the `pad_size` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_pad_size` (`dict`, *optional*, defaults to `{"height" -- 256, "width":
    256}`): Size of the output segmentation map after padding. Can be overridden by
    the `mask_pad_size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `True`) — Whether to convert
    the image to RGB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a SAM image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `filter_masks`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L805)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`masks` (`Union[torch.Tensor, tf.Tensor]`) — Input masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iou_scores` (`Union[torch.Tensor, tf.Tensor]`) — List of IoU scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`original_size` (`Tuple[int,int]`) — Size of the orginal image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cropped_box_image` (`np.array`) — The cropped image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pred_iou_thresh` (`float`, *optional*, defaults to 0.88) — The threshold for
    the iou scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_thresh` (`float`, *optional*, defaults to 0.95) — The threshold
    for the stability score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0) — The threshold for the
    predicted masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stability_score_offset` (`float`, *optional*, defaults to 1) — The offset
    for the stability score used in the `_compute_stability_score` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters the predicted masks by selecting only the ones that meets several criteria.
    The first criterion being that the iou scores needs to be greater than `pred_iou_thresh`.
    The second criterion is that the stability score needs to be greater than `stability_score_thresh`.
    The method also converts the predicted masks to bounding boxes and pad the predicted
    masks if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `generate_crop_boxes`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L740)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.array`) — Input original image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_size` (`int`) — Target size of the resized image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_n_layers` (`int`, *optional*, defaults to 0) — If >0, mask prediction
    will be run again on crops of the image. Sets the number of layers to run, where
    each layer has 2**i_layer number of image crops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`overlap_ratio` (`float`, *optional*, defaults to 512/1500) — Sets the degree
    to which crops overlap. In the first crop layer, crops will overlap by this fraction
    of the image length. Later layers with more crops scale down this overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`points_per_crop` (`int`, *optional*, defaults to 32) — Number of points to
    sample from each crop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crop_n_points_downscale_factor` (`List[int]`, *optional*, defaults to 1) —
    The number of points-per-side sampled in layer n is scaled down by crop_n_points_downscale_factor**n.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device` (`torch.device`, *optional*, defaults to None) — Device to use for
    the computation. If None, cpu will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) — The channel
    dimension format of the input image. If not provided, it will be inferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a list of crop boxes of different sizes. Each layer has (2**i)**2
    boxes for the ith layer.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `pad_image`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L163)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.ndarray`) — Image to pad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`) — Size of the output image after padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`str` or `ChannelDimension`, *optional*) — The data format of
    the image. Can be either “channels_first” or “channels_last”. If `None`, the `data_format`
    of the `image` will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`str` or `ChannelDimension`, *optional*) — The channel
    dimension format of the input image. If not provided, it will be inferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad an image to `(pad_size["height"], pad_size["width"])` with zeros to the
    right and bottom.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_for_mask_generation`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L717)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`all_masks` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all predicted
    segmentation masks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`all_scores` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all predicted
    iou scores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`all_boxes` (`Union[List[torch.Tensor], List[tf.Tensor]]`) — List of all bounding
    boxes of the predicted masks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`crops_nms_thresh` (`float`) — Threshold for NMS (Non Maximum Suppression)
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `pt`) — If `pt`, returns `torch.Tensor`.
    If `tf`, returns `tf.Tensor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post processes mask that are generated by calling the Non Maximum Suppression
    algorithm on the predicted masks.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `post_process_masks`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L573)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`masks` (`Union[List[torch.Tensor], List[np.ndarray], List[tf.Tensor]]`) —
    Batched masks from the mask_decoder in (batch_size, num_channels, height, width)
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`original_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`) —
    The original sizes of each image before it was resized to the model’s expected
    input shape, in (height, width) format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reshaped_input_sizes` (`Union[torch.Tensor, tf.Tensor, List[Tuple[int,int]]]`)
    — The size of each image as it is fed to the model, in (height, width) format.
    Used to remove padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_threshold` (`float`, *optional*, defaults to 0.0) — The threshold to
    use for binarizing the masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`binarize` (`bool`, *optional*, defaults to `True`) — Whether to binarize the
    masks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`int`, *optional*, defaults to `self.pad_size`) — The target size
    the images were padded to before being passed to the model. If None, the target
    size is assumed to be the processor’s `pad_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str`, *optional*, defaults to `"pt"`) — If `"pt"`, return
    PyTorch tensors. If `"tf"`, return TensorFlow tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: (`Union[torch.Tensor, tf.Tensor]`)
  prefs: []
  type: TYPE_NORMAL
- en: Batched masks in batch_size, num_channels, height, width) format, where (height,
    width) is given by original_size.
  prefs: []
  type: TYPE_NORMAL
- en: Remove padding and upscale masks to the original image size.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `preprocess`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L389)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`images` (`ImageInput`) — Image to preprocess. Expects a single or batch of
    images with pixel values ranging from 0 to 255\. If passing in images with pixel
    values between 0 and 1, set `do_rescale=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`segmentation_maps` (`ImageInput`, *optional*) — Segmentation map to preprocess.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_resize` (`bool`, *optional*, defaults to `self.do_resize`) — Whether to
    resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`, *optional*, defaults to `self.size`) — Controls the
    size of the image after `resize`. The longest edge of the image is resized to
    `size["longest_edge"]` whilst preserving the aspect ratio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_size` (`Dict[str, int]`, *optional*, defaults to `self.mask_size`) —
    Controls the size of the segmentation map after `resize`. The longest edge of
    the image is resized to `size["longest_edge"]` whilst preserving the aspect ratio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resample` (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_rescale` (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image pixel values by rescaling factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rescale_factor` (`int` or `float`, *optional*, defaults to `self.rescale_factor`)
    — Rescale factor to apply to the image pixel values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_normalize` (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_mean` (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to normalize the image by if `do_normalize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_std` (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to normalize the image by if `do_normalize` is set
    to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_pad` (`bool`, *optional*, defaults to `self.do_pad`) — Whether to pad the
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_size` (`Dict[str, int]`, *optional*, defaults to `self.pad_size`) — Controls
    the size of the padding applied to the image. The image is padded to `pad_size["height"]`
    and `pad_size["width"]` if `do_pad` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_pad_size` (`Dict[str, int]`, *optional*, defaults to `self.mask_pad_size`)
    — Controls the size of the padding applied to the segmentation map. The image
    is padded to `mask_pad_size["height"]` and `mask_pad_size["width"]` if `do_pad`
    is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_convert_rgb` (`bool`, *optional*, defaults to `self.do_convert_rgb`) —
    Whether to convert the image to RGB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the channel dimension format of the input image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `resize`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/image_processing_sam.py#L211)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`image` (`np.ndarray`) — Image to resize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size` (`Dict[str, int]`) — Dictionary in the format `{"longest_edge": int}`
    specifying the size of the output image. The longest edge of the image will be
    resized to the specified size, while the other edge will be resized to maintain
    the aspect ratio. resample — `PILImageResampling` filter to use when resizing
    the image e.g. `PILImageResampling.BILINEAR`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format` (`ChannelDimension` or `str`, *optional*) — The channel dimension
    format for the output image. If unset, the channel dimension format of the input
    image is used. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_data_format` (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`np.ndarray`'
  prefs: []
  type: TYPE_NORMAL
- en: The resized image.
  prefs: []
  type: TYPE_NORMAL
- en: Resize an image to `(size["height"], size["width"])`.
  prefs: []
  type: TYPE_NORMAL
- en: SamModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.SamModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1180)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment Anything Model (SAM) for generating segmentation masks, given an input
    image and optional 2D location and bounding boxes. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_sam.py#L1278)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) — Pixel values. Pixel values can be obtained using [SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor).
    See `SamProcessor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_points` (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`)
    — Input 2D spatial points, this is used by the prompt encoder to encode the prompt.
    Generally yields to much better results. The points can be obtained by passing
    a list of list of list to the processor that will create corresponding `torch`
    tensors of dimension 4\. The first dimension is the image batch size, the second
    dimension is the point batch size (i.e. how many segmentation masks do we want
    the model to predict per input point), the third dimension is the number of points
    per segmentation mask (it is possible to pass multiple points for a single mask),
    and the last dimension is the x (vertical) and y (horizontal) coordinates of the
    point. If a different number of points is passed either for each image, or for
    each mask, the processor will create “PAD” points that will correspond to the
    (0, 0) coordinate, and the computation of the embedding will be skipped for these
    points using the labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_labels` (`torch.LongTensor` of shape `(batch_size, point_batch_size,
    num_points)`) — Input labels for the points, this is used by the prompt encoder
    to encode the prompt. According to the official implementation, there are 3 types
    of labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: the point is a point that contains the object of interest'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0`: the point is a point that does not contain the object of interest'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-1`: the point corresponds to the background'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We added the label:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`-10`: the point is a padding point, thus should be ignored by the prompt encoder'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding labels should be automatically done by the processor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_boxes` (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`) —
    Input boxes for the points, this is used by the prompt encoder to encode the prompt.
    Generally yields to much better generated masks. The boxes can be obtained by
    passing a list of list of list to the processor, that will generate a `torch`
    tensor, with each dimension corresponding respectively to the image batch size,
    the number of boxes per image and the coordinates of the top left and botton right
    point of the box. In the order (`x1`, `y1`, `x2`, `y2`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x1`: the x coordinate of the top left point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y1`: the y coordinate of the top left point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x2`: the x coordinate of the bottom right point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y2`: the y coordinate of the bottom right point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_masks` (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`)
    — SAM model also accepts segmentation masks as input. The mask will be embedded
    by the prompt encoder to generate a corresponding embedding, that will be fed
    later on to the mask decoder. These masks needs to be manually fed by the user,
    and they need to be of shape (`batch_size`, `image_size`, `image_size`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`torch.FloatTensor` of shape `(batch_size, output_channels,
    window_size, window_size)`) — Image embeddings, this is used by the mask decder
    to generate masks and iou scores. For more memory efficient computation, users
    can first retrieve the image embeddings using the `get_image_embeddings` method,
    and then feed them to the `forward` method instead of feeding the `pixel_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimask_output` (`bool`, *optional*) — In the original implementation and
    paper, the model always outputs 3 masks per image (or per point / per bounding
    box if relevant). However, it is possible to just output a single mask, that corresponds
    to the “best” mask, by specifying `multimask_output=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_similarity` (`torch.FloatTensor`, *optional*) — Attention similarity
    tensor, to be provided to the mask decoder for target-guided attention in case
    the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_embedding` (`torch.FloatTensor`, *optional*) — Embedding of the target
    concept, to be provided to the mask decoder for target-semantic prompting in case
    the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example —
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [SamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: TFSamModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFSamModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_tf_sam.py#L1410)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([SamConfig](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment Anything Model (SAM) for generating segmentation masks, given an input
    image and optional 2D location and bounding boxes. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a TensorFlow [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TensorFlow Model and refer to the TensorFlow documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/sam/modeling_tf_sam.py#L1505)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pixel_values` (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`)
    — Pixel values. Pixel values can be obtained using [SamProcessor](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.SamProcessor).
    See `SamProcessor.__call__()` for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_points` (`tf.Tensor` of shape `(batch_size, num_points, 2)`) — Input
    2D spatial points, this is used by the prompt encoder to encode the prompt. Generally
    yields to much better results. The points can be obtained by passing a list of
    list of list to the processor that will create corresponding `tf` tensors of dimension
    4\. The first dimension is the image batch size, the second dimension is the point
    batch size (i.e. how many segmentation masks do we want the model to predict per
    input point), the third dimension is the number of points per segmentation mask
    (it is possible to pass multiple points for a single mask), and the last dimension
    is the x (vertical) and y (horizontal) coordinates of the point. If a different
    number of points is passed either for each image, or for each mask, the processor
    will create “PAD” points that will correspond to the (0, 0) coordinate, and the
    computation of the embedding will be skipped for these points using the labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_labels` (`tf.Tensor` of shape `(batch_size, point_batch_size, num_points)`)
    — Input labels for the points, this is used by the prompt encoder to encode the
    prompt. According to the official implementation, there are 3 types of labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1`: the point is a point that contains the object of interest'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0`: the point is a point that does not contain the object of interest'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-1`: the point corresponds to the background'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We added the label:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`-10`: the point is a padding point, thus should be ignored by the prompt encoder'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding labels should be automatically done by the processor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`input_boxes` (`tf.Tensor` of shape `(batch_size, num_boxes, 4)`) — Input boxes
    for the points, this is used by the prompt encoder to encode the prompt. Generally
    yields to much better generated masks. The boxes can be obtained by passing a
    list of list of list to the processor, that will generate a `tf` tensor, with
    each dimension corresponding respectively to the image batch size, the number
    of boxes per image and the coordinates of the top left and botton right point
    of the box. In the order (`x1`, `y1`, `x2`, `y2`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x1`: the x coordinate of the top left point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y1`: the y coordinate of the top left point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x2`: the x coordinate of the bottom right point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y2`: the y coordinate of the bottom right point of the input box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_masks` (`tf.Tensor` of shape `(batch_size, image_size, image_size)`)
    — SAM model also accepts segmentation masks as input. The mask will be embedded
    by the prompt encoder to generate a corresponding embedding, that will be fed
    later on to the mask decoder. These masks needs to be manually fed by the user,
    and they need to be of shape (`batch_size`, `image_size`, `image_size`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_embeddings` (`tf.Tensor` of shape `(batch_size, output_channels, window_size,
    window_size)`) — Image embeddings, this is used by the mask decder to generate
    masks and iou scores. For more memory efficient computation, users can first retrieve
    the image embeddings using the `get_image_embeddings` method, and then feed them
    to the `call` method instead of feeding the `pixel_values`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimask_output` (`bool`, *optional*) — In the original implementation and
    paper, the model always outputs 3 masks per image (or per point / per bounding
    box if relevant). However, it is possible to just output a single mask, that corresponds
    to the “best” mask, by specifying `multimask_output=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFSamModel](/docs/transformers/v4.37.2/en/model_doc/sam#transformers.TFSamModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
