- en: Zero-shot image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/zero_shot_image_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/zero_shot_image_classification)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot image classification is a task that involves classifying images into
    different categories using a model that was not explicitly trained on data containing
    labeled examples from those specific categories.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, image classification requires training a model on a specific
    set of labeled images, and this model learns to “map” certain image features to
    labels. When there’s a need to use such model for a classification task that introduces
    a new set of labels, fine-tuning is required to “recalibrate” the model.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, zero-shot or open vocabulary image classification models are typically
    multi-modal models that have been trained on a large dataset of images and associated
    descriptions. These models learn aligned vision-language representations that
    can be used for many downstream tasks including zero-shot image classification.
  prefs: []
  type: TYPE_NORMAL
- en: This is a more flexible approach to image classification that allows models
    to generalize to new and unseen categories without the need for additional training
    data and enables users to query images with free-form text descriptions of their
    target objects .
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide you’ll learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: create a zero-shot image classification pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: run zero-shot image classification inference by hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Zero-shot image classification pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest way to try out inference with a model supporting zero-shot image
    classification is to use the corresponding [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, choose an image you’d like to classify.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Photo of an owl](../Images/ceeab937a1338358216d31c99c500d27.png)'
  prefs: []
  type: TYPE_IMG
- en: Pass the image and the candidate object labels to the pipeline. Here we pass
    the image directly; other suitable options include a local path to an image or
    an image url. The candidate labels can be simple words like in this example, or
    more descriptive.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Zero-shot image classification by hand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you’ve seen how to use the zero-shot image classification pipeline,
    let’s take a look how you can run zero-shot image classification manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the model and associated processor from a [checkpoint on the
    Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads).
    Here we’ll use the same checkpoint as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a different image to switch things up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Photo of a car](../Images/32d5d4f7954f70dbb6de697af5252857.png)'
  prefs: []
  type: TYPE_IMG
- en: Use the processor to prepare the inputs for the model. The processor combines
    an image processor that prepares the image for the model by resizing and normalizing
    it, and a tokenizer that takes care of the text inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Pass the inputs through the model, and post-process the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
