- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/api-inference/quicktour](https://huggingface.co/docs/api-inference/quicktour)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/api-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/entry/start.e0f2a481.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/scheduler.b8dd6794.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/singletons.b4b7c713.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/index.4e4eb7ec.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/paths.c627adaa.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/entry/app.c06d8cf1.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/index.1dbdfbc6.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/nodes/0.d33efbda.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/nodes/6.396e9ae4.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/InferenceApi.c5fe9515.js">
    <link rel="modulepreload" href="/docs/api-inference/main/en/_app/immutable/chunks/Heading.e9c64afe.js">
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s have a quick look at the ðŸ¤— Hosted Inference API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Main features:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Leverage **150,000+ Transformer, Diffusers, or Timm models** (T5, Blenderbot,
    Bart, GPT-2, Pegasus...)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upload, manage and serve your **own models privately**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run Classification, NER, Conversational, Summarization, Translation, Question-Answering,
    Embeddings Extraction tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get up to **10x inference speedup** to reduce user latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerated inference for a number of supported models on CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run **large models** that are challenging to deploy in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale up to 1,000 requests per second with **automatic scaling** built-in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ship new NLP, CV, Audio, or RL features faster** as new models become available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build your business on a platform powered by the reference open source project
    in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get your API Token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Register](https://huggingface.co/join) or [Login](https://huggingface.co/login).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a User Access or API token [in your Hugging Face profile settings](https://huggingface.co/settings/tokens).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should see a token `hf_xxxxx` (old tokens are `api_XXXXXXXX` or `api_org_XXXXXXX`).
  prefs: []
  type: TYPE_NORMAL
- en: If you do not submit your API token when sending requests to the API, you will
    not be able to run inference on your private models.
  prefs: []
  type: TYPE_NORMAL
- en: Running Inference with API Requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to choose which model you are going to run. Go to the [Model
    Hub](https://huggingface.co/models) and select the model you want to use. If you
    are unsure where to start, make sure to check the [recommended models for each
    ML task](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters)
    available, or the [Tasks](https://huggingface.co/tasks) overview.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Letâ€™s use [gpt2](https://huggingface.co/gpt2) as an example. To run inference,
    simply use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: PythonJavaScriptcURL
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: API Options and Parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the task (aka pipeline) the model is configured for, the request
    will accept specific parameters. When sending requests to run any model, API options
    allow you to specify the caching and model loading behavior. All API options and
    parameters are detailed here [`detailed_parameters`](detailed_parameters).
  prefs: []
  type: TYPE_NORMAL
- en: Using CPU-Accelerated Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an API customer, your API token will automatically enable CPU-Accelerated
    inference on your requests if the model type is supported. For instance, if you
    compare gpt2 model inference through our API with CPU-Acceleration, compared to
    running inference on the model out of the box on a local setup, you should measure
    a **~10x speedup**. The specific performance boost depends on the model and input
    payload (and your local hardware).
  prefs: []
  type: TYPE_NORMAL
- en: To verify you are using the CPU-Accelerated version of a model you can check
    the x-compute-type header of your requests, which should be cpu+optimized. If
    you do not see it, it simply means not all optimizations are turned on. This can
    be for various factors; the model might have been added recently to transformers,
    or the model can be optimized in several different ways and the best one depends
    on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: If you contact us at [api-enterprise@huggingface.co](mailto:api-enterprise@huggingface.co),
    weâ€™ll be able to increase the inference speed for you, depending on your actual
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Model Loading and latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Hosted Inference API can serve predictions on-demand from over 100,000 models
    deployed on the Hugging Face Hub, dynamically loaded on shared infrastructure.
    If the requested model is not loaded in memory, the Hosted Inference API will
    start by loading the model into memory and returning a 503 response, before it
    can respond with the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: If your use case requires large volume or predictable latencies, you can use
    our paid solution [Inference Endpoints](https://huggingface.co/inference-endpoints)
    to easily deploy your models on dedicated, fully-managed infrastructure. With
    Inference Endpoints you can quickly create endpoints on the cloud, region, CPU
    or GPU compute instance of your choice.
  prefs: []
  type: TYPE_NORMAL
