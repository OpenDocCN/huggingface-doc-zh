["```py\npip install accelerate\n\n# upgrade\npip install accelerate --upgrade\n```", "```py\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"your-model\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n)\n```", "```py\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```", "```py\n# resume from latest checkpoint\ntrainer.train(resume_from_checkpoint=True)\n\n# resume from specific checkpoint saved in output directory\ntrainer.train(resume_from_checkpoint=\"your-model/checkpoint-1000\")\n```", "```py\nfrom torch import nn\nfrom transformers import Trainer\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss for 3 labels with different weights\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```", "```py\nfrom transformers import TrainerCallback\n\nclass EarlyStoppingCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step >= self.num_steps:\n            return {\"should_training_stop\": True}\n        else:\n            return {}\n```", "```py\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callback=[EarlyStoppingCallback()],\n)\n```", "```py\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\n\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\n\ntrainer = Trainer(...)\n```", "```py\nmy_app.py ... --log_level warning --log_level_replica error\n```", "```py\nfrom transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(..., neftune_noise_alpha=0.1)\ntrainer = Trainer(..., args=training_args)\n```", "```py\ncompute_environment: LOCAL_MACHINE                                                                                             \ndistributed_type: MULTI_GPU                                                                                                    \ndowncast_bf16: 'no'\ngpu_ids: all\nmachine_rank: 0 #change rank as per the node\nmain_process_ip: 192.168.20.1\nmain_process_port: 9898\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 2\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```", "```py\naccelerate launch \\\n    ./examples/pytorch/text-classification/run_glue.py \\\n    --model_name_or_path bert-base-cased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 16 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3 \\\n    --output_dir /tmp/$TASK_NAME/ \\\n    --overwrite_output_dir\n```", "```py\naccelerate launch --num_processes=2 \\\n    --use_fsdp \\\n    --mixed_precision=bf16 \\\n    --fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \\\n    --fsdp_transformer_layer_cls_to_wrap=\"BertLayer\" \\\n    --fsdp_sharding_strategy=1 \\\n    --fsdp_state_dict_type=FULL_STATE_DICT \\\n    ./examples/pytorch/text-classification/run_glue.py\n    --model_name_or_path bert-base-cased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --max_seq_length 128 \\\n    --per_device_train_batch_size 16 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3 \\\n    --output_dir /tmp/$TASK_NAME/ \\\n    --overwrite_output_dir\n```"]