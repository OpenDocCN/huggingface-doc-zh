# InstructPix2Pix

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/pix2pix](https://huggingface.co/docs/diffusers/api/pipelines/pix2pix)

[InstructPix2Pix：学习遵循图像编辑说明](https://huggingface.co/papers/2211.09800)由Tim Brooks、Aleksander Holynski和Alexei A. Efros撰写。

该论文的摘要为：

*我们提出了一种根据人类指令编辑图像的方法：给定一个输入图像和一条书面指令告诉模型该做什么，我们的模型遵循这些指令来编辑图像。为了解决这个问题的训练数据，我们结合了两个大型预训练模型的知识-语言模型（GPT-3）和文本到图像模型（稳定扩散）-生成了大量的图像编辑示例数据集。我们的条件扩散模型InstructPix2Pix是在我们生成的数据上训练的，并且在推理时推广到真实图像和用户编写的指令。由于它在前向传递中执行编辑，不需要每个示例的微调或反演，我们的模型可以在几秒钟内快速编辑图像。我们展示了对各种输入图像和书面指令的引人注目的编辑结果。*

您可以在[项目页面](https://www.timothybrooks.com/instruct-pix2pix)、[原始代码库](https://github.com/timothybrooks/instruct-pix2pix)和[演示](https://huggingface.co/spaces/timbrooks/instruct-pix2pix)中找到有关InstructPix2Pix的其他信息。

确保查看调度器指南以了解如何在调度器速度和质量之间进行权衡，并查看重用组件跨管道部分以了解如何有效地将相同组件加载到多个管道中。

## 稳定扩散指导Pix2Pix管道

### `class diffusers.StableDiffusionInstructPix2PixPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L75)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL））- 变分自动编码器（VAE）模型，用于对图像进行编码和解码到和从潜在表示中。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel））- 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer））- 用于对文本进行标记的`CLIPTokenizer`。

+   `unet`（[UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel））- 用于去噪编码图像潜在的`UNet2DConditionModel`。

+   `scheduler`（[SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin））- 与`unet`结合使用以去噪编码图像潜在的调度器。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `safety_checker`（`StableDiffusionSafetyChecker`）- 估计生成的图像是否可能被视为具有冒犯性或有害性的分类模块。请参考[模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5)以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor`（[CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor））- 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

根据文本指令进行像素级图像编辑的流水线（基于 Stable Diffusion）。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有流水线实现的通用方法（下载、保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载 LoRA 权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存 LoRA 权重

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载 IP 适配器

#### `__call__`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L159)

```py
( prompt: Union = None image: Union = None num_inference_steps: int = 100 guidance_scale: float = 7.5 image_guidance_scale: float = 1.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`torch.FloatTensor` `np.ndarray`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 表示要根据`prompt`重新绘制的图像批次的`Image`或张量。也可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。

+   `num_inference_steps` (`int`, *可选*, 默认为100) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。

+   `guidance_scale` (`float`, *可选*, 默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，以降低图像质量为代价。当`guidance_scale > 1`时启用引导比例。

+   `image_guidance_scale` (`float`, *可选*, 默认为1.5) — 将生成的图像推向初始`image`。设置`image_guidance_scale > 1`可以启用图像引导比例。更高的图像引导比例鼓励生成的图像与源`image`紧密相关，通常以降低图像质量为代价。此流水线需要至少`1`的值。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 指导在图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。当不使用引导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *可选*, 默认为1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中将被忽略。

+   `generator` (`torch.Generator`, *可选*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中抽样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，则通过使用提供的随机`generator`进行抽样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）— 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds`将从`negative_prompt`输入参数生成。ip_adapter_image —（`PipelineImageInput`，*可选*）：可选的图像输入以与IP适配器一起使用。

+   `output_type`（`str`，*可选*，默认为`"pil"`）— 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)而不是普通元组。

+   `callback_on_step_end`（`Callable`，*可选*）— 在推断期间每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包括由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs`（`List`，*可选*）— 用于`callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> import PIL
>>> import requests
>>> import torch
>>> from io import BytesIO

>>> from diffusers import StableDiffusionInstructPix2PixPipeline

>>> def download_image(url):
...     response = requests.get(url)
...     return PIL.Image.open(BytesIO(response.content)).convert("RGB")

>>> img_url = "https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png"

>>> image = download_image(img_url).resize((512, 512))

>>> pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
...     "timbrooks/instruct-pix2pix", torch_dtype=torch.float16
... )
>>> pipe = pipe.to("cuda")

>>> prompt = "make the mountains snowy"
>>> image = pipe(prompt=prompt, image=image).images[0]
```

#### `load_textual_inversion`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`或`List[str或os.PathLike]`或`Dict`或`List[Dict]`）— 可以是以下之一或它们的列表：

    +   一个字符串，预训练模型在Hub上托管的*模型ID*（例如`sd-concepts-library/low-poly-hd-logos-icons`）。

    +   包含文本反转权重的路径为*目录*（例如`./my_text_inversion_directory/）。

    +   包含文本反转权重的*文件*路径（例如`./my_text_inversions.pt`）。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token`（`str`或`List[str]`，*可选*）— 覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是列表，则`token`也必须是相同长度的列表。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)，*可选*）— 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用self.tokenizer。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，*可选*）— 用于标记文本的`CLIPTokenizer`。如果未指定，函数将使用self.tokenizer。

+   `weight_name`（`str`，*可选*）— 自定义权重文件的名称。应在以下情况使用：

    +   保存的文本反转文件以🤗 Diffusers格式保存，但是保存在特定权重名称下，例如`text_inv.bin`。

    +   保存的文本反转文件以Automatic1111格式保存。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 下载预训练模型配置时缓存的目录路径，如果不使用标准缓存。

+   `force_download` (`bool`, *optional*, defaults to `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, defaults to `False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，任何未完全下载的文件将被删除。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。代理将在每个请求上使用。

+   `local_files_only` (`bool`, *optional*, defaults to `False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，模型将不会从Hub下载。

+   `token` (`str` or *bool*, *optional*) — 用作远程文件HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *optional*, defaults to `"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`, *optional*, defaults to `""`) — 在Hub或本地较大模型存储库中模型文件的子文件夹位置。

+   `mirror` (`str`, *optional*) — 镜像源，用于解决在中国下载模型时的可访问性问题。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的文本编码器中（支持🤗 Diffusers和Automatic1111格式）。

示例：

要加载🤗 Diffusers格式中的文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载Automatic1111格式中的文本反转嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)），然后加载向量。

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `load_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str` or `os.PathLike` or `dict`) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `kwargs` (`dict`, *optional*) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `adapter_name` (`str`, *optional*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是要加载的适配器总数。

将在`self.unet`和`self.text_encoder`中加载指定的LoRA权重。

所有kwargs都将转发到`self.lora_state_dict`。

查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)以获取有关如何加载状态字典的更多详细信息。

查看[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)以获取有关如何将状态字典加载到`self.unet`的更多详细信息。

查看[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)以获取有关如何将状态字典加载到`self.text_encoder`的更多详细信息。

#### `save_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory` (`str` or `os.PathLike`) — 保存LoRA参数的目录。如果不存在，将被创建。

+   `unet_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）— 与`unet`对应的LoRA层的状态字典。

+   `text_encoder_lora_layers`（`Dict[str, torch.nn.Module]`或`Dict[str, torch.Tensor]`）— 与`text_encoder`对应的LoRA层的状态字典。必须显式传递文本编码器LoRA状态字典，因为它来自🤗 Transformers。

+   `is_main_process`（`bool`，*可选*，默认为`True`）— 调用此函数的进程是否为主进程。在分布式训练期间很有用，当您需要在所有进程上调用此函数时。在这种情况下，仅在主进程上设置`is_main_process=True`，以避免竞争条件。

+   `save_function`（`Callable`）— 用于保存状态字典的函数。在分布式训练期间，当您需要用另一种方法替换`torch.save`时很有用。可以通过环境变量`DIFFUSERS_SAVE_MODE`进行配置。

+   `safe_serialization`（`bool`，*可选*，默认为`True`）— 是否使用`safetensors`保存模型，还是使用传统的PyTorch方式与`pickle`。

保存与UNet和文本编码器对应的LoRA参数。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L832)

```py
( )
```

禁用了启用的FreeU机制。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py#L809)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1`（`float`）— 用于阻尼跳过特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `s2`（`float`）— 用于阻尼跳过特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `b1`（`float`）— 用于放大骨干特征贡献的阶段1的缩放因子。

+   `b2`（`float`）— 用于放大骨干特征贡献的阶段2的缩放因子。

启用了FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

## StableDiffusionXLInstructPix2PixPipeline

### `class diffusers.StableDiffusionXLInstructPix2PixPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L120)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )
```

参数

+   `vae`（[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)）— 变分自动编码器（VAE）模型，用于对图像进行编码和解码以及从潜在表示中。

+   `text_encoder`（`CLIPTextModel`）— 冻结的文本编码器。Stable Diffusion XL使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)的文本部分，具体是[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)变体。

+   `text_encoder_2`（`CLIPTextModelWithProjection`）— 第二个冻结的文本编码器。Stable Diffusion XL使用[CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection)的文本和池部分，具体是[laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)变体。

+   `tokenizer`（`CLIPTokenizer`）— 类[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)的分词器。

+   `tokenizer_2` (`CLIPTokenizer`) — 第二个类的Tokenizer [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer)。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪编码图像潜在特征的条件U-Net架构。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与`unet`结合使用的调度器，用于去噪编码图像潜在特征。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `requires_aesthetics_score` (`bool`, *可选*, 默认为`"False"`) — `unet`在推断期间是否需要通过审美评分条件。也请参阅`stabilityai/stable-diffusion-xl-refiner-1-0`的配置。

+   `force_zeros_for_empty_prompt` (`bool`, *可选*, 默认为`"True"`) — 是否强制将负提示嵌入始终设置为0。也请参阅`stabilityai/stable-diffusion-xl-base-1-0`的配置。

+   `add_watermarker` (`bool`, *可选*) — 是否使用[invisible_watermark库](https://github.com/ShieldMnt/invisible-watermark/)来给输出图像添加水印。如果未定义，且该软件包已安装，则默认为True，否则将不使用水印。

基于稳定扩散XL的像素级图像编辑流水线，通过以下文本指令。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以了解库为所有流水线实现的通用方法（如下载或保存、在特定设备上运行等）。

该流水线还继承了以下加载方法：

+   [load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion) 用于加载文本反演嵌入

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.StableDiffusionXLLoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   `save_lora_weights()` 用于保存LoRA权重

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L652)

```py
( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 100 denoising_end: Optional = None guidance_scale: float = 5.0 image_guidance_scale: float = 1.5 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None ) → export const metadata = 'undefined';~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 引导图像生成的提示或提示。如果未定义，则必须传递`prompt_embeds`。

+   `prompt_2` (`str` 或 `List[str]`, *可选*) — 发送到`tokenizer_2`和`text_encoder_2`的提示或提示。如果未定义，则在两个文本编码器中使用`prompt`。

+   `image` (`torch.FloatTensor` 或 `PIL.Image.Image` 或 `np.ndarray` 或 `List[torch.FloatTensor]` 或 `List[PIL.Image.Image]` 或 `List[np.ndarray]`) — 要使用流水线修改的图像。

+   `height` (`int`, *可选*, 默认为self.unet.config.sample_size * self.vae_scale_factor) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为self.unet.config.sample_size * self.vae_scale_factor) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推断速度。

+   `denoising_end` (`float`, *可选*) — 当指定时，确定在有意提前终止之前完成的总去噪过程的分数（介于0.0和1.0之间）。因此，返回的样本仍将保留由调度程序选择的离散时间步确定的大量噪声。当此管道形成“去噪器混合”多管道设置的一部分时，应理想地利用 `denoising_end` 参数，如[`Refining the Image Output`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)中所述。

+   `guidance_scale` (`float`, *可选*, 默认为 5.0) — 在[Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)中定义的指导比例。`guidance_scale` 定义为[Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf)中方程式2的 `w`。通过设置 `guidance_scale > 1` 来启用指导比例。更高的指导比例鼓励生成与文本 `prompt` 密切相关的图像，通常以降低图像质量为代价。

+   `image_guidance_scale` (`float`, *可选*, 默认为 1.5) — 图像指导比例用于将生成的图像推向初始图像 `image`。通过设置 `image_guidance_scale > 1` 来启用图像指导比例。更高的图像指导比例鼓励生成与源图像 `image` 密切相关的图像，通常以降低图像质量为代价。此管道需要至少 `1` 的值。

+   `negative_prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的负面提示。如果未定义，则必须传递 `negative_prompt_embeds`。如果不使用指导（即，如果 `guidance_scale` 小于 `1`，则忽略）。

+   `negative_prompt_2` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的负面提示，将发送到 `tokenizer_2` 和 `text_encoder_2`。如果未定义，则在两个文本编码器中都使用 `negative_prompt`。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于DDIM论文中的参数 eta (η)：[https://arxiv.org/abs/2010.02502](https://arxiv.org/abs/2010.02502)。仅适用于[schedulers.DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，对其他情况将被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 一个或多个[torch生成器](https://pytorch.org/docs/stable/generated/torch.Generator.html)，用于使生成过程确定性。

+   `latents` (`torch.FloatTensor`, *可选*) — 预生成的嘈杂潜变量，从高斯分布中采样，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成 `negative_prompt_embeds`。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，池化文本嵌入将从 `prompt` 输入参数生成。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *可选*) — 预生成的负面池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从 `negative_prompt` 输入参数生成池化的负面提示嵌入。

+   `output_type` (`str`, *可选*, 默认为 `"pil"`) — 生成图像的输出格式。选择在 [PIL](https://pillow.readthedocs.io/en/stable/) 中的 `PIL.Image.Image` 或 `np.array` 之间。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 `~pipelines.stable_diffusion.StableDiffusionXLPipelineOutput` 而不是一个普通的 tuple。

+   `callback` (`Callable`, *可选*) — 推断期间每 `callback_steps` 步调用的函数。该函数将使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — `callback` 函数将被调用的频率。如果未指定，将在每一步调用回调函数。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定，将传递给 `AttentionProcessor` 的 kwargs 字典，如在 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中的 `self.processor` 中定义。

+   `guidance_rescale` (`float`, *可选*, 默认为 0.0) — [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 提出的指导缩放因子。`guidance_scale` 在 [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf) 的方程式 16 中定义。指导缩放因子应在使用零终端 SNR 时修复过曝光问题。

+   `original_size` (`Tuple[int]`, *可选*, 默认为 (1024, 1024)) — 如果 `original_size` 与 `target_size` 不同，图像将呈现为缩小或放大。如果未指定，`original_size` 默认为 `(height, width)`。这是 SDXL 微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 第2.2节。

+   `crops_coords_top_left` (`Tuple[int]`, *可选*, 默认为 (0, 0)) — `crops_coords_top_left` 可用于生成一个看起来从位置 `crops_coords_top_left` 向下“裁剪”的图像。通常通过将 `crops_coords_top_left` 设置为 (0, 0) 来实现有利的、居中的图像。这是 SDXL 微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 第2.2节。

+   `target_size` (`Tuple[int]`, *可选*, 默认为 (1024, 1024)) — 对于大多数情况，`target_size` 应设置为生成图像的期望高度和宽度。如果未指定，将默认为 `(height, width)`。这是 SDXL 微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 第2.2节。

+   `aesthetic_score` (`float`, *可选*, 默认为 6.0) — 通过影响正文条件来模拟生成图像的美学评分。这是 SDXL 微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 第2.2节。

+   `negative_aesthetic_score` (`float`, *可选*, 默认为 2.5) — 这是 SDXL 微调节的一部分，详见 [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952) 第2.2节。可用于通过影响负面文本条件来模拟生成图像的美学评分。

返回值

`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput` 或 `tuple`

如果 `return_dict` 为 True，则返回 `~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`，否则返回一个 `tuple`。当返回一个 tuple 时，第一个元素是包含生成图像的列表。

调用管道进行生成时调用的函数。

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionXLInstructPix2PixPipeline
>>> from diffusers.utils import load_image

>>> resolution = 768
>>> image = load_image(
...     "https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png"
... ).resize((resolution, resolution))
>>> edit_instruction = "Turn sky into a cloudy one"

>>> pipe = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(
...     "diffusers/sdxl-instructpix2pix-768", torch_dtype=torch.float16
... ).to("cuda")

>>> edited_image = pipe(
...     prompt=edit_instruction,
...     image=image,
...     height=resolution,
...     width=resolution,
...     guidance_scale=3.0,
...     image_guidance_scale=1.5,
...     num_inference_steps=30,
... ).images[0]
>>> edited_image
```

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L648)

```py
( )
```

如果启用了FreeU机制，则将其禁用。

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L217)

```py
( )
```

禁用分片VAE解码。如果之前调用了`enable_vae_slicing`，则此方法将返回到一步计算解码。

#### `disable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L233)

```py
( )
```

禁用分片VAE解码。如果之前调用了`enable_vae_tiling`，则此方法将返回到一步计算解码。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L625)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1`（`float`）— 用于减弱跳过特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `s2`（`float`）— 用于减弱跳过特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效果”。

+   `b1`（`float`）— 用于放大骨干特征贡献的阶段1的缩放因子。

+   `b2`（`float`）— 用于放大骨干特征贡献的阶段2的缩放因子。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L208)

```py
( )
```

启用分片VAE解码。

当启用此选项时，VAE将分割输入张量以在多个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

#### `enable_vae_tiling`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L224)

```py
( )
```

启用分片VAE解码。

当启用此选项时，VAE将分割输入张量以在多个步骤中计算解码和编码。这对于节省大量内存并允许处理更大的图像非常有用。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_instruct_pix2pix.py#L240)

```py
( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）— 要编码的提示

+   `prompt_2`（`str`或`List[str]`，*可选*）— 要发送到`tokenizer_2`和`text_encoder_2`的提示。如果未定义，则在两个文本编码器中使用`prompt`。设备 — (`torch.device`): torch设备

+   `num_images_per_prompt`（`int`）— 每个提示应生成的图像数量

+   `do_classifier_free_guidance`（`bool`）— 是否使用分类器自由指导

+   `negative_prompt`（`str`或`List[str]`，*可选*）— 不用于指导图像生成的提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `negative_prompt_2` (`str` or `List[str]`, *optional*) — 用于不指导图像生成的提示或提示，将发送到`tokenizer_2`和`text_encoder_2`。如果未定义，则在两个文本编码器中使用`negative_prompt`。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `pooled_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成池化的文本嵌入。

+   `negative_pooled_prompt_embeds` (`torch.FloatTensor`, *optional`) — 预生成的负池化文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成池化的negative_prompt_embeds。

+   `lora_scale` (`float`, *optional*) — 如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的lora比例。

将提示编码为文本编码器隐藏状态。
