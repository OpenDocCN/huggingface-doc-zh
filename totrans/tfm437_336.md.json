["```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n>>> model = BridgeTowerForContrastiveLearning.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs.logits[0, 1].item()\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000360943.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> text = \"a <mask> looking out of the window\"\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForMaskedLM.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())\n\n>>> print(results)\n.a cat looking out of the window.\n```", "```py\n( share_cross_modal_transformer_layers = True hidden_act = 'gelu' hidden_size = 768 initializer_factor = 1 layer_norm_eps = 1e-05 share_link_tower_layers = False link_tower_type = 'add' num_attention_heads = 12 num_hidden_layers = 6 tie_word_embeddings = False init_layernorm_from_vision_encoder = False text_config = None vision_config = None **kwargs )\n```", "```py\n>>> from transformers import BridgeTowerModel, BridgeTowerConfig\n\n>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration\n>>> configuration = BridgeTowerConfig()\n\n>>> # Initializing a model from the BridgeTower/bridgetower-base style configuration\n>>> model = BridgeTowerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( text_config: BridgeTowerTextConfig vision_config: BridgeTowerVisionConfig **kwargs )\n```", "```py\n( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 initializer_factor = 1 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size = 1 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2 position_embedding_type = 'absolute' use_cache = True **kwargs )\n```", "```py\n>>> from transformers import BridgeTowerTextConfig\n\n>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the text model\n>>> configuration = BridgeTowerTextConfig()\n\n>>> # Accessing the configuration\n>>> configuration\n```", "```py\n( hidden_size = 768 num_hidden_layers = 12 num_channels = 3 patch_size = 16 image_size = 288 initializer_factor = 1 layer_norm_eps = 1e-05 stop_gradient = False share_layernorm = True remove_last_layer = False **kwargs )\n```", "```py\n>>> from transformers import BridgeTowerVisionConfig\n\n>>> # Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the vision model\n>>> configuration = BridgeTowerVisionConfig()\n\n>>> # Accessing the configuration\n>>> configuration\n```", "```py\n( do_resize: bool = True size: Dict = 288 size_divisor: int = 32 resample: Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_center_crop: bool = True do_pad: bool = True **kwargs )\n```", "```py\n( images: Union do_resize: Optional = None size: Optional = None size_divisor: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None do_pad: Optional = None do_center_crop: Optional = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image_processor tokenizer )\n```", "```py\n( images text: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of: Optional = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True return_tensors: Union = None **kwargs )\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None image_token_type_idx: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerModel\n>>> from PIL import Image\n>>> import requests\n\n>>> # prepare image and text\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> text = \"hello world\"\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base\")\n>>> model = BridgeTowerModel.from_pretrained(\"BridgeTower/bridgetower-base\")\n\n>>> inputs = processor(image, text, return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n>>> outputs.keys()\nodict_keys(['text_features', 'image_features', 'pooler_output'])\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = True return_dict: Optional = None return_loss: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForContrastiveLearning\n>>> import requests\n>>> from PIL import Image\n>>> import torch\n\n>>> image_urls = [\n...     \"https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg\",\n...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n... ]\n>>> texts = [\"two dogs in a car\", \"two cats sleeping on a couch\"]\n>>> images = [Image.open(requests.get(url, stream=True).raw) for url in image_urls]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n>>> model = BridgeTowerForContrastiveLearning.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n\n>>> inputs = processor(images, texts, padding=True, return_tensors=\"pt\")\n>>> loss = model(**inputs, return_loss=True).loss\n\n>>> inputs = processor(images, texts[::-1], padding=True, return_tensors=\"pt\")\n>>> loss_swapped = model(**inputs, return_loss=True).loss\n\n>>> print(\"Loss\", round(loss.item(), 4))\nLoss 0.0019\n\n>>> print(\"Loss with swapped images\", round(loss_swapped.item(), 4))\nLoss with swapped images 2.126\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MaskedLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForMaskedLM\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"http://images.cocodataset.org/val2017/000000360943.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n>>> text = \"a <mask> looking out of the window\"\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForMaskedLM.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # prepare inputs\n>>> encoding = processor(image, text, return_tensors=\"pt\")\n\n>>> # forward pass\n>>> outputs = model(**encoding)\n\n>>> results = processor.decode(outputs.logits.argmax(dim=-1).squeeze(0).tolist())\n\n>>> print(results)\n.a cat looking out of the window.\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None token_type_ids: Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\n>>> import requests\n>>> from PIL import Image\n\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n>>> texts = [\"An image of two cats chilling on a couch\", \"A football player scoring a goal\"]\n\n>>> processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n>>> model = BridgeTowerForImageAndTextRetrieval.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n\n>>> # forward pass\n>>> scores = dict()\n>>> for text in texts:\n...     # prepare inputs\n...     encoding = processor(image, text, return_tensors=\"pt\")\n...     outputs = model(**encoding)\n...     scores[text] = outputs.logits[0, 1].item()\n```"]