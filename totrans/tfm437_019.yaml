- en: Token classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/tasks/token_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/token_classification)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/tasks/token_classification](https://huggingface.co/docs/transformers/v4.37.2/en/tasks/token_classification)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube-nocookie.com/embed/wVHdVlPScxA](https://www.youtube-nocookie.com/embed/wVHdVlPScxA)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/wVHdVlPScxA](https://www.youtube-nocookie.com/embed/wVHdVlPScxA)'
- en: Token classification assigns a label to individual tokens in a sentence. One
    of the most common token classification tasks is Named Entity Recognition (NER).
    NER attempts to find a label for each entity in a sentence, such as a person,
    location, or organization.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡è®°åˆ†ç±»ä¸ºå¥å­ä¸­çš„æ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªæ ‡ç­¾ã€‚æœ€å¸¸è§çš„æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¹‹ä¸€æ˜¯å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚NERè¯•å›¾ä¸ºå¥å­ä¸­çš„æ¯ä¸ªå®ä½“æ‰¾åˆ°ä¸€ä¸ªæ ‡ç­¾ï¼Œæ¯”å¦‚äººåã€åœ°ç‚¹æˆ–ç»„ç»‡ã€‚
- en: 'This guide will show you how to:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š
- en: Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the
    [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨WNUT 17æ•°æ®é›†ä¸Šå¯¹[DistilBERT](https://huggingface.co/distilbert-base-uncased)è¿›è¡Œå¾®è°ƒï¼Œä»¥æ£€æµ‹æ–°å®ä½“ã€‚
- en: Use your finetuned model for inference.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚
- en: 'The task illustrated in this tutorial is supported by the following model architectures:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä¸­æ¼”ç¤ºçš„ä»»åŠ¡ç”±ä»¥ä¸‹æ¨¡å‹æ¶æ„æ”¯æŒï¼š
- en: '[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird),
    [BioGpt](../model_doc/biogpt), [BLOOM](../model_doc/bloom), [BROS](../model_doc/bros),
    [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert),
    [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2),
    [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie),
    [ErnieM](../model_doc/ernie_m), [ESM](../model_doc/esm), [Falcon](../model_doc/falcon),
    [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel),
    [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode),
    [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [I-BERT](../model_doc/ibert),
    [LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3),
    [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke),
    [MarkupLM](../model_doc/markuplm), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt),
    [MRA](../model_doc/mra), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer),
    [Phi](../model_doc/phi), [QDQBert](../model_doc/qdqbert), [RemBERT](../model_doc/rembert),
    [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm),
    [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert),
    [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl),
    [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[ALBERT](../model_doc/albert), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird),
    [BioGpt](../model_doc/biogpt), [BLOOM](../model_doc/bloom), [BROS](../model_doc/bros),
    [CamemBERT](../model_doc/camembert), [CANINE](../model_doc/canine), [ConvBERT](../model_doc/convbert),
    [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2),
    [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie),
    [ErnieM](../model_doc/ernie_m), [ESM](../model_doc/esm), [Falcon](../model_doc/falcon),
    [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel),
    [GPT-Sw3](../model_doc/gpt-sw3), [OpenAI GPT-2](../model_doc/gpt2), [GPTBigCode](../model_doc/gpt_bigcode),
    [GPT Neo](../model_doc/gpt_neo), [GPT NeoX](../model_doc/gpt_neox), [I-BERT](../model_doc/ibert),
    [LayoutLM](../model_doc/layoutlm), [LayoutLMv2](../model_doc/layoutlmv2), [LayoutLMv3](../model_doc/layoutlmv3),
    [LiLT](../model_doc/lilt), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke),
    [MarkupLM](../model_doc/markuplm), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert),
    [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MPT](../model_doc/mpt),
    [MRA](../model_doc/mra), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer),
    [Phi](../model_doc/phi), [QDQBert](../model_doc/qdqbert), [RemBERT](../model_doc/rembert),
    [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm),
    [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert),
    [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl),
    [XLNet](../model_doc/xlnet), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)'
- en: 'Before you begin, make sure you have all the necessary libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We encourage you to login to your Hugging Face account so you can upload and
    share your model with the community. When prompted, enter your token to login:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„Hugging Faceè´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å’Œåˆ†äº«æ‚¨çš„æ¨¡å‹ç»™ç¤¾åŒºã€‚åœ¨æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Load WNUT 17 dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ è½½WNUT 17æ•°æ®é›†
- en: 'Start by loading the WNUT 17 dataset from the ğŸ¤— Datasets library:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆä»ğŸ¤—æ•°æ®é›†åº“ä¸­åŠ è½½WNUT 17æ•°æ®é›†ï¼š
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then take a look at an example:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åçœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each number in `ner_tags` represents an entity. Convert the numbers to their
    label names to find out what the entities are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`ner_tags`ä¸­çš„æ¯ä¸ªæ•°å­—ä»£è¡¨ä¸€ä¸ªå®ä½“ã€‚å°†æ•°å­—è½¬æ¢ä¸ºå®ƒä»¬çš„æ ‡ç­¾åç§°ï¼Œä»¥æ‰¾å‡ºè¿™äº›å®ä½“æ˜¯ä»€ä¹ˆï¼š'
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The letter that prefixes each `ner_tag` indicates the token position of the
    entity:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ª`ner_tag`å‰ç¼€çš„å­—æ¯è¡¨ç¤ºå®ä½“çš„æ ‡è®°ä½ç½®ï¼š
- en: '`B-` indicates the beginning of an entity.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`B-`è¡¨ç¤ºä¸€ä¸ªå®ä½“çš„å¼€å§‹ã€‚'
- en: '`I-` indicates a token is contained inside the same entity (for example, the
    `State` token is a part of an entity like `Empire State Building`).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-`è¡¨ç¤ºä¸€ä¸ªå•è¯åŒ…å«åœ¨åŒä¸€ä¸ªå®ä½“ä¸­ï¼ˆä¾‹å¦‚ï¼Œ`State`å•è¯æ˜¯`Empire State Building`è¿™æ ·ä¸€ä¸ªå®ä½“çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚'
- en: '`0` indicates the token doesnâ€™t correspond to any entity.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`è¡¨ç¤ºè¯¥æ ‡è®°ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚'
- en: Preprocess
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„å¤„ç†
- en: '[https://www.youtube-nocookie.com/embed/iY2AZYdZAr0](https://www.youtube-nocookie.com/embed/iY2AZYdZAr0)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube-nocookie.com/embed/iY2AZYdZAr0](https://www.youtube-nocookie.com/embed/iY2AZYdZAr0)'
- en: 'The next step is to load a DistilBERT tokenizer to preprocess the `tokens`
    field:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ä¸€ä¸ªDistilBERTåˆ†è¯å™¨æ¥é¢„å¤„ç†`tokens`å­—æ®µï¼š
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you saw in the example `tokens` field above, it looks like the input has
    already been tokenized. But the input actually hasnâ€™t been tokenized yet and youâ€™ll
    need to set `is_split_into_words=True` to tokenize the words into subwords. For
    example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, this adds some special tokens `[CLS]` and `[SEP]` and the subword
    tokenization creates a mismatch between the input and labels. A single word corresponding
    to a single label may now be split into two subwords. Youâ€™ll need to realign the
    tokens and labels by:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Mapping all tokens to their corresponding word with the [`word_ids`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.word_ids)
    method.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]` so theyâ€™re
    ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)).
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only labeling the first token of a given word. Assign `-100` to other subtokens
    from the same word.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is how you can create a function to realign the tokens and labels, and
    truncate sequences to be no longer than DistilBERTâ€™s maximum input length:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets
    [map](https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.map)
    function. You can speed up the `map` function by setting `batched=True` to process
    multiple elements of the dataset at once:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now create a batch of examples using [DataCollatorWithPadding](/docs/transformers/v4.37.2/en/main_classes/data_collator#transformers.DataCollatorWithPadding).
    Itâ€™s more efficient to *dynamically pad* the sentences to the longest length in
    a batch during collation, instead of padding the whole dataset to the maximum
    length.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: TensorFlowHide TensorFlow content
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Evaluate
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Including a metric during training is often helpful for evaluating your modelâ€™s
    performance. You can quickly load a evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index)
    library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval)
    framework (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)
    to learn more about how to load and compute a metric). Seqeval actually produces
    several scores: precision, recall, F1, and accuracy.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Get the NER labels first, and then create a function that passes your true
    predictions and true labels to `compute` to calculate the scores:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Your `compute_metrics` function is ready to go now, and youâ€™ll return to it
    when you setup your training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Train
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you start training your model, create a map of the expected ids to their
    labels with `id2label` and `label2id`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: PytorchHide Pytorch content
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: If you arenâ€™t familiar with finetuning a model with the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer),
    take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Youâ€™re ready to start training your model now! Load DistilBERT with [AutoModelForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoModelForTokenClassification)
    along with the number of expected labels, and the label mappings:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At this point, only three steps remain:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Define your training hyperparameters in [TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments).
    The only required parameter is `output_dir` which specifies where to save your
    model. Youâ€™ll push this model to the Hub by setting `push_to_hub=True` (you need
    to be signed in to Hugging Face to upload your model). At the end of each epoch,
    the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    will evaluate the seqeval scores and save the training checkpoint.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the training arguments to [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    along with the model, dataset, tokenizer, data collator, and `compute_metrics`
    function.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ`compute_metrics`å‡½æ•°ã€‚
- en: Call [train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)
    to finetune your model.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒç”¨[train()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.train)æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once training is completed, share your model to the Hub with the [push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)
    method so everyone can use your model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åŸ¹è®­å®Œæˆåï¼Œä½¿ç”¨[push_to_hub()](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer.push_to_hub)æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹å…±äº«åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: TensorFlowHide TensorFlow content
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—TensorFlowå†…å®¹
- en: If you arenâ€™t familiar with finetuning a model with Keras, take a look at the
    basic tutorial [here](../training#train-a-tensorflow-model-with-keras)!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨Keraså¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹åŸºæœ¬æ•™ç¨‹[è¿™é‡Œ](../training#train-a-tensorflow-model-with-keras)ï¼
- en: 'To finetune a model in TensorFlow, start by setting up an optimizer function,
    learning rate schedule, and some training hyperparameters:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨TensorFlowä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·é¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è°ƒåº¦å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then you can load DistilBERT with [TFAutoModelForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForTokenClassification)
    along with the number of expected labels, and the label mappings:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ‚¨å¯ä»¥åŠ è½½DistilBERTä¸[TFAutoModelForTokenClassification](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.TFAutoModelForTokenClassification)ä»¥åŠé¢„æœŸæ ‡ç­¾çš„æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Convert your datasets to the `tf.data.Dataset` format with [prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[prepare_tf_dataset()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)å°†æ•°æ®é›†è½¬æ¢ä¸º`tf.data.Dataset`æ ¼å¼ï¼š
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method).
    Note that Transformers models all have a default task-relevant loss function,
    so you donâ€™t need to specify one unless you want to:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼ŒTransformersæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šï¼š
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The last two things to setup before you start training is to compute the seqeval
    scores from the predictions, and provide a way to push your model to the Hub.
    Both are done by using [Keras callbacks](../main_classes/keras_callbacks).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰è®¾ç½®çš„æœ€åä¸¤ä»¶äº‹æ˜¯ä»é¢„æµ‹ä¸­è®¡ç®—seqevalåˆ†æ•°ï¼Œå¹¶æä¾›ä¸€ç§å°†æ‚¨çš„æ¨¡å‹æ¨é€åˆ°Hubçš„æ–¹æ³•ã€‚è¿™ä¸¤è€…éƒ½å¯ä»¥ä½¿ç”¨[Keras callbacks](../main_classes/keras_callbacks)æ¥å®Œæˆã€‚
- en: 'Pass your `compute_metrics` function to [KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„`compute_metrics`å‡½æ•°ä¼ é€’ç»™[KerasMetricCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼š
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Specify where to push your model and tokenizer in the [PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[PushToHubCallback](/docs/transformers/v4.37.2/en/main_classes/keras_callbacks#transformers.PushToHubCallback)ä¸­æŒ‡å®šè¦æ¨é€æ¨¡å‹å’Œåˆ†è¯å™¨çš„ä½ç½®ï¼š
- en: '[PRE22]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then bundle your callbacks together:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†æ‚¨çš„å›è°ƒæ†ç»‘åœ¨ä¸€èµ·ï¼š
- en: '[PRE23]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, youâ€™re ready to start training your model! Call [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)
    with your training and validation datasets, the number of epochs, and your callbacks
    to finetune the model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ‚¨å·²ç»å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€æ—¶ä»£æ•°å’Œå›è°ƒè°ƒç”¨[`fit`](https://keras.io/api/models/model_training_apis/#fit-method)æ¥å¾®è°ƒæ¨¡å‹ï¼š
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Once training is completed, your model is automatically uploaded to the Hub
    so everyone can use it!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ°Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼
- en: For a more in-depth example of how to finetune a model for token classification,
    take a look at the corresponding [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)
    or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£å¦‚ä½•ä¸ºæ ‡è®°åˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„[PyTorchç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)æˆ–[TensorFlowç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ã€‚
- en: Inference
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Great, now that youâ€™ve finetuned a model, you can use it for inference!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼
- en: 'Grab some text youâ€™d like to run inference on:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–ä¸€äº›æ‚¨æƒ³è¦è¿è¡Œæ¨ç†çš„æ–‡æœ¬ï¼š
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The simplest way to try out your finetuned model for inference is to use it
    in a [pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline).
    Instantiate a `pipeline` for NER with your model, and pass your text to it:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)ä¸­ä½¿ç”¨å®ƒã€‚ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªNERçš„`pipeline`ï¼Œå¹¶å°†æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can also manually replicate the results of the `pipeline` if youâ€™d like:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶`pipeline`çš„ç»“æœï¼š
- en: PytorchHide Pytorch content
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—Pytorchå†…å®¹
- en: 'Tokenize the text and return PyTorch tensors:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›PyTorchå¼ é‡ï¼š
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Pass your inputs to the model and return the `logits`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Get the class with the highest probability, and use the modelâ€™s `id2label`
    mapping to convert it to a text label:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼š
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: TensorFlowHide TensorFlow content
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: éšè—TensorFlowå†…å®¹
- en: 'Tokenize the text and return TensorFlow tensors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–å¹¶è¿”å›TensorFlowå¼ é‡ï¼š
- en: '[PRE30]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Pass your inputs to the model and return the `logits`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å›`logits`ï¼š
- en: '[PRE31]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Get the class with the highest probability, and use the modelâ€™s `id2label`
    mapping to convert it to a text label:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„`id2label`æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾ï¼š
- en: '[PRE32]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
