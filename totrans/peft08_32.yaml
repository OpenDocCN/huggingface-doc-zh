- en: Configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/package_reference/config](https://huggingface.co/docs/peft/package_reference/config)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/19.3b00404d.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Docstring.270658d8.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
  prefs: []
  type: TYPE_NORMAL
- en: '`PeftConfigMixin` is the base configuration class for storing the adapter configuration
    of a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel),
    and [PromptLearningConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PromptLearningConfig)
    is the base configuration class for soft prompt methods (p-tuning, prefix tuning,
    and prompt tuning). These base classes contain methods for saving and loading
    model configurations from the Hub, specifying the PEFT method to use, type of
    task to perform, and model configurations like number of layers and number of
    attention heads.'
  prefs: []
  type: TYPE_NORMAL
- en: PeftConfigMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.config.PeftConfigMixin`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`peft_type` (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft
    method to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the base configuration class for PEFT adapter models. It contains all
    the methods that are common to all PEFT adapter models. This class inherits from
    [PushToHubMixin](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.utils.PushToHubMixin)
    which contains the methods to push your model to the Hub. The method `save_pretrained`
    will save the configuration of your adapter model in a directory. The method `from_pretrained`
    will load the configuration of your adapter model from a directory.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_json_file`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L153)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`path_json_file` (`str`) — The path to the json file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loads a configuration file from a json file.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_peft_type`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`kwargs` (configuration keyword arguments) — Keyword arguments passed along
    to the configuration initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method loads the configuration of your adapter model from a set of kwargs.
  prefs: []
  type: TYPE_NORMAL
- en: The appropriate configuration type is determined by the `peft_type` argument.
    If `peft_type` is not provided, the calling class type is instantiated.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L120)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str`) — The directory or the Hub repository
    id where the configuration is saved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Additional keyword arguments
    passed along to the child class initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method loads the configuration of your adapter model from a directory.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L49)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str`) — The directory where the configuration will be saved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (additional keyword arguments, *optional*) — Additional keyword arguments
    passed along to the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This method saves the configuration of your adapter model in a directory.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `to_dict`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Returns the configuration for your adapter model as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: PeftConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PeftConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L221)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`peft_type` (Union[`~peft.utils.config.PeftType`, `str`]) — The type of Peft
    method to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`task_type` (Union[`~peft.utils.config.TaskType`, `str`]) — The type of task
    to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inference_mode` (`bool`, defaults to `False`) — Whether to use the Peft model
    in inference mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the base configuration class to store the configuration of a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
  prefs: []
  type: TYPE_NORMAL
- en: PromptLearningConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class peft.PromptLearningConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/peft/blob/v0.8.2/src/peft/config.py#L241)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`num_virtual_tokens` (`int`) — The number of virtual tokens to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_dim` (`int`) — The hidden embedding dimension of the base transformer
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_transformer_submodules` (`int`) — The number of transformer submodules
    in the base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`) — The number of attention heads in the base transformer
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers` (`int`) — The number of layers in the base transformer model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the base configuration class to store the configuration of `PrefixTuning`,
    [PromptEncoder](/docs/peft/v0.8.2/en/package_reference/p_tuning#peft.PromptEncoder),
    or `PromptTuning`.
  prefs: []
  type: TYPE_NORMAL
