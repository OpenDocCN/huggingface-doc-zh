["```py\npip install -q datasets transformers evaluate\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n```", "```py\nsemantic_segmentation = pipeline(\"image-segmentation\", \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\nresults = semantic_segmentation(image)\nresults\n```", "```py\n[{'score': None,\n  'label': 'road',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'sidewalk',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'building',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'wall',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'pole',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'traffic sign',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'vegetation',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'terrain',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'sky',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': None,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```", "```py\nresults[-1][\"mask\"]\n```", "```py\ninstance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\nresults = instance_segmentation(Image.open(image))\nresults\n```", "```py\n[{'score': 0.999944,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999945,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999652,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.903529,\n  'label': 'person',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```", "```py\nresults[2][\"mask\"]\n```", "```py\npanoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\nresults = panoptic_segmentation(Image.open(image))\nresults\n```", "```py\n[{'score': 0.999981,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999958,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.99997,\n  'label': 'vegetation',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999575,\n  'label': 'pole',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999958,\n  'label': 'building',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999634,\n  'label': 'road',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.996092,\n  'label': 'sidewalk',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.999221,\n  'label': 'car',\n  'mask': <PIL.Image.Image image mode=L size=612x415>},\n {'score': 0.99987,\n  'label': 'sky',\n  'mask': <PIL.Image.Image image mode=L size=612x415>}]\n```", "```py\n>>> from datasets import load_dataset\n\n>>> ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n```", "```py\n>>> ds = ds.train_test_split(test_size=0.2)\n>>> train_ds = ds[\"train\"]\n>>> test_ds = ds[\"test\"]\n```", "```py\n>>> train_ds[0]\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,\n 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,\n 'scene_category': 368}\n```", "```py\n>>> import json\n>>> from huggingface_hub import cached_download, hf_hub_url\n\n>>> repo_id = \"huggingface/label-files\"\n>>> filename = \"ade20k-id2label.json\"\n>>> id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type=\"dataset\")), \"r\"))\n>>> id2label = {int(k): v for k, v in id2label.items()}\n>>> label2id = {v: k for k, v in id2label.items()}\n>>> num_labels = len(id2label)\n```", "```py\n    from datasets import Dataset, DatasetDict, Image\n\n    image_paths_train = [\"path/to/image_1.jpg/jpg\", \"path/to/image_2.jpg/jpg\", ..., \"path/to/image_n.jpg/jpg\"]\n    label_paths_train = [\"path/to/annotation_1.png\", \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n\n    image_paths_validation = [...]\n    label_paths_validation = [...]\n\n    def create_dataset(image_paths, label_paths):\n        dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n                                    \"label\": sorted(label_paths)})\n        dataset = dataset.cast_column(\"image\", Image())\n        dataset = dataset.cast_column(\"label\", Image())\n        return dataset\n\n    # step 1: create Dataset objects\n    train_dataset = create_dataset(image_paths_train, label_paths_train)\n    validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n\n    # step 2: create DatasetDict\n    dataset = DatasetDict({\n         \"train\": train_dataset,\n         \"validation\": validation_dataset,\n         }\n    )\n\n    # step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\n    dataset.push_to_hub(\"your-name/dataset-repo\")\n\n    # optionally, you can push to a private repo on the Hub\n    # dataset.push_to_hub(\"name of repo on the hub\", private=True)\n    ```", "```py\n    import json\n    # simple example\n    id2label = {0: 'cat', 1: 'dog'}\n    with open('id2label.json', 'w') as fp:\n    json.dump(id2label, fp)\n    ```", "```py\n>>> from transformers import AutoImageProcessor\n\n>>> checkpoint = \"nvidia/mit-b0\"\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)\n```", "```py\n>>> from torchvision.transforms import ColorJitter\n\n>>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n```", "```py\n>>> def train_transforms(example_batch):\n...     images = [jitter(x) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n\n>>> def val_transforms(example_batch):\n...     images = [x for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n```", "```py\n>>> train_ds.set_transform(train_transforms)\n>>> test_ds.set_transform(val_transforms)\n```", "```py\n>>> import tensorflow as tf\n\n>>> def aug_transforms(image):\n...     image = tf.keras.utils.img_to_array(image)\n...     image = tf.image.random_brightness(image, 0.25)\n...     image = tf.image.random_contrast(image, 0.5, 2.0)\n...     image = tf.image.random_saturation(image, 0.75, 1.25)\n...     image = tf.image.random_hue(image, 0.1)\n...     image = tf.transpose(image, (2, 0, 1))\n...     return image\n\n>>> def transforms(image):\n...     image = tf.keras.utils.img_to_array(image)\n...     image = tf.transpose(image, (2, 0, 1))\n...     return image\n```", "```py\n>>> def train_transforms(example_batch):\n...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n\n>>> def val_transforms(example_batch):\n...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n...     labels = [x for x in example_batch[\"annotation\"]]\n...     inputs = image_processor(images, labels)\n...     return inputs\n```", "```py\n>>> train_ds.set_transform(train_transforms)\n>>> test_ds.set_transform(val_transforms)\n```", "```py\n>>> import evaluate\n\n>>> metric = evaluate.load(\"mean_iou\")\n```", "```py\n>>> import numpy as np\n>>> import torch\n>>> from torch import nn\n\n>>> def compute_metrics(eval_pred):\n...     with torch.no_grad():\n...         logits, labels = eval_pred\n...         logits_tensor = torch.from_numpy(logits)\n...         logits_tensor = nn.functional.interpolate(\n...             logits_tensor,\n...             size=labels.shape[-2:],\n...             mode=\"bilinear\",\n...             align_corners=False,\n...         ).argmax(dim=1)\n\n...         pred_labels = logits_tensor.detach().cpu().numpy()\n...         metrics = metric.compute(\n...             predictions=pred_labels,\n...             references=labels,\n...             num_labels=num_labels,\n...             ignore_index=255,\n...             reduce_labels=False,\n...         )\n...         for key, value in metrics.items():\n...             if isinstance(value, np.ndarray):\n...                 metrics[key] = value.tolist()\n...         return metrics\n```", "```py\n>>> def compute_metrics(eval_pred):\n...     logits, labels = eval_pred\n...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n...     logits_resized = tf.image.resize(\n...         logits,\n...         size=tf.shape(labels)[1:],\n...         method=\"bilinear\",\n...     )\n\n...     pred_labels = tf.argmax(logits_resized, axis=-1)\n...     metrics = metric.compute(\n...         predictions=pred_labels,\n...         references=labels,\n...         num_labels=num_labels,\n...         ignore_index=-1,\n...         reduce_labels=image_processor.do_reduce_labels,\n...     )\n\n...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n\n...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n...     return {\"val_\" + k: v for k, v in metrics.items()}\n```", "```py\n>>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n\n>>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n```", "```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"segformer-b0-scene-parse-150\",\n...     learning_rate=6e-5,\n...     num_train_epochs=50,\n...     per_device_train_batch_size=2,\n...     per_device_eval_batch_size=2,\n...     save_total_limit=3,\n...     evaluation_strategy=\"steps\",\n...     save_strategy=\"steps\",\n...     save_steps=20,\n...     eval_steps=20,\n...     logging_steps=1,\n...     eval_accumulation_steps=5,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=train_ds,\n...     eval_dataset=test_ds,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> from transformers import create_optimizer\n\n>>> batch_size = 2\n>>> num_epochs = 50\n>>> num_train_steps = len(train_ds) * num_epochs\n>>> learning_rate = 6e-5\n>>> weight_decay_rate = 0.01\n\n>>> optimizer, lr_schedule = create_optimizer(\n...     init_lr=learning_rate,\n...     num_train_steps=num_train_steps,\n...     weight_decay_rate=weight_decay_rate,\n...     num_warmup_steps=0,\n... )\n```", "```py\n>>> from transformers import TFAutoModelForSemanticSegmentation\n\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n... )\n>>> model.compile(optimizer=optimizer)  # No loss argument!\n```", "```py\n>>> from transformers import DefaultDataCollator\n\n>>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n\n>>> tf_train_dataset = train_ds.to_tf_dataset(\n...     columns=[\"pixel_values\", \"label\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n\n>>> tf_eval_dataset = test_ds.to_tf_dataset(\n...     columns=[\"pixel_values\", \"label\"],\n...     shuffle=True,\n...     batch_size=batch_size,\n...     collate_fn=data_collator,\n... )\n```", "```py\n>>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n\n>>> metric_callback = KerasMetricCallback(\n...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n... )\n\n>>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n\n>>> callbacks = [metric_callback, push_to_hub_callback]\n```", "```py\n>>> model.fit(\n...     tf_train_dataset,\n...     validation_data=tf_eval_dataset,\n...     callbacks=callbacks,\n...     epochs=num_epochs,\n... )\n```", "```py\n>>> image = ds[0][\"image\"]\n>>> image\n```", "```py\n>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n>>> encoding = image_processor(image, return_tensors=\"pt\")\n>>> pixel_values = encoding.pixel_values.to(device)\n```", "```py\n>>> outputs = model(pixel_values=pixel_values)\n>>> logits = outputs.logits.cpu()\n```", "```py\n>>> upsampled_logits = nn.functional.interpolate(\n...     logits,\n...     size=image.size[::-1],\n...     mode=\"bilinear\",\n...     align_corners=False,\n... )\n\n>>> pred_seg = upsampled_logits.argmax(dim=1)[0]\n```", "```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n>>> inputs = image_processor(image, return_tensors=\"tf\")\n```", "```py\n>>> from transformers import TFAutoModelForSemanticSegmentation\n\n>>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n>>> logits = model(**inputs).logits\n```", "```py\n>>> logits = tf.transpose(logits, [0, 2, 3, 1])\n\n>>> upsampled_logits = tf.image.resize(\n...     logits,\n...     # We reverse the shape of `image` because `image.size` returns width and height.\n...     image.size[::-1],\n... )\n\n>>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n\n>>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n>>> palette = np.array(ade_palette())\n>>> for label, color in enumerate(palette):\n...     color_seg[pred_seg == label, :] = color\n>>> color_seg = color_seg[..., ::-1]  # convert to BGR\n\n>>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n>>> img = img.astype(np.uint8)\n\n>>> plt.figure(figsize=(15, 10))\n>>> plt.imshow(img)\n>>> plt.show()\n```"]