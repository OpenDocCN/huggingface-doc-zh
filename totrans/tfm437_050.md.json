["```py\n>>> from tokenizers import Tokenizer\n>>> from tokenizers.models import BPE\n>>> from tokenizers.trainers import BpeTrainer\n>>> from tokenizers.pre_tokenizers import Whitespace\n\n>>> tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n>>> trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n\n>>> tokenizer.pre_tokenizer = Whitespace()\n>>> files = [...]\n>>> tokenizer.train(files, trainer)\n```", "```py\n>>> from transformers import PreTrainedTokenizerFast\n\n>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n```", "```py\n>>> tokenizer.save(\"tokenizer.json\")\n```", "```py\n>>> from transformers import PreTrainedTokenizerFast\n\n>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n```"]