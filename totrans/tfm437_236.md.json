["```py\n>>> from transformers import AutoTokenizer, SwitchTransformersModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersModel.from_pretrained(\"google/switch-base-8\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for SwitchTransformersModel.\n>>> # This is not needed for torch's SwitchTransformersForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-8\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(input_ids)\n>>> # . To, let\u2019s say you have a dog. To summarize:\n>>> # Since the model has been trained on MLM, this will output gibberish\n```", "```py\n>>> from transformers import AutoTokenizer, SwitchTransformersEncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersEncoderModel.from_pretrained(\"google/switch-base-8\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```"]