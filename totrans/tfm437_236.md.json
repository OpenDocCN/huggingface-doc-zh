["```py\n( vocab_size = 32128 d_model = 768 d_kv = 64 d_ff = 2048 expert_capacity = 64 num_layers = 12 num_sparse_encoder_layers = 3 num_decoder_layers = 12 num_sparse_decoder_layers = 3 num_heads = 12 num_experts = 8 router_bias = False router_jitter_noise = 0.01 router_dtype = 'float32' router_ignore_padding_tokens = False relative_attention_num_buckets = 32 relative_attention_max_distance = 128 dropout_rate = 0.1 layer_norm_epsilon = 1e-06 router_z_loss_coef = 0.001 router_aux_loss_coef = 0.001 initializer_factor = 1.0 dense_act_fn = 'relu' is_encoder_decoder = True add_router_probs = False use_cache = True pad_token_id = 0 eos_token_id = 1 **kwargs )\n```", "```py\n( config: SwitchTransformersConfig )\n```", "```py\n( hidden_states: Tensor ) \u2192 export const metadata = 'undefined';router_probabilities (torch.Tensor)\n```", "```py\n( hidden_states: Tensor )\n```", "```py\n( config: SwitchTransformersConfig expert_class: Module = <class 'transformers.models.switch_transformers.modeling_switch_transformers.SwitchTransformersDenseActDense'> )\n```", "```py\n( hidden_states )\n```", "```py\n( config: SwitchTransformersConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, SwitchTransformersModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersModel.from_pretrained(\"google/switch-base-8\")\n\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n>>> # preprocess: Prepend decoder_input_ids with start token which is pad token for SwitchTransformersModel.\n>>> # This is not needed for torch's SwitchTransformersForConditionalGeneration as it does this internally using labels arg.\n>>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n>>> # forward pass\n>>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n( config: SwitchTransformersConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None decoder_input_ids: Optional = None decoder_attention_mask: Optional = None head_mask: Optional = None decoder_head_mask: Optional = None cross_attn_head_mask: Optional = None encoder_outputs: Optional = None past_key_values: Optional = None inputs_embeds: Optional = None decoder_inputs_embeds: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = True return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Seq2SeqMoEOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-8\")\n\n>>> # training\n>>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n>>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n>>> outputs = model(input_ids=input_ids, labels=labels)\n>>> loss = outputs.loss\n>>> logits = outputs.logits\n\n>>> # inference\n>>> input_ids = tokenizer(\n...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model.generate(input_ids)\n>>> # . To, let\u2019s say you have a dog. To summarize:\n>>> # Since the model has been trained on MLM, this will output gibberish\n```", "```py\n( config: SwitchTransformersConfig )\n```", "```py\n( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None output_router_logits: Optional = True return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.MoEModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoTokenizer, SwitchTransformersEncoderModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/switch-base-8\")\n>>> model = SwitchTransformersEncoderModel.from_pretrained(\"google/switch-base-8\")\n>>> input_ids = tokenizer(\n...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n... ).input_ids  # Batch size 1\n>>> outputs = model(input_ids=input_ids)\n>>> last_hidden_states = outputs.last_hidden_state\n```"]