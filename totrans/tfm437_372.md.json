["```py\nimport av\nimport cv2\nimport numpy as np\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoProcessor, TvpForVideoGrounding\n\ndef pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    Convert the video from its original fps to the target_fps and decode the video with PyAV decoder.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling.\n            If clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly sample from the given video.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video. Return None if the no\n            video stream was found.\n        fps (float): the number of frames per second of the video.\n    '''\n    video = container.streams.video[0]\n    fps = float(video.average_rate)\n    clip_size = sampling_rate * num_frames / target_fps * fps\n    delta = max(num_frames - clip_size, 0)\n    start_idx = delta * clip_idx / num_clips\n    end_idx = start_idx + clip_size - 1\n    timebase = video.duration / num_frames\n    video_start_pts = int(start_idx * timebase)\n    video_end_pts = int(end_idx * timebase)\n    seek_offset = max(video_start_pts - 1024, 0)\n    container.seek(seek_offset, any_frame=False, backward=True, stream=video)\n    frames = {}\n    for frame in container.decode(video=0):\n        if frame.pts < video_start_pts:\n            continue\n        frames[frame.pts] = frame\n        if frame.pts > video_end_pts:\n            break\n    frames = [frames[pts] for pts in sorted(frames)]\n    return frames, fps\n\ndef decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    Decode the video and perform temporal sampling.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling.\n            If clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly sample from the given video.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video.\n    '''\n    assert clip_idx >= -2, \"Not a valied clip_idx {}\".format(clip_idx)\n    frames, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)\n    clip_size = sampling_rate * num_frames / target_fps * fps\n    index = np.linspace(0, clip_size - 1, num_frames)\n    index = np.clip(index, 0, len(frames) - 1).astype(np.int64)\n    frames = np.array([frames[idx].to_rgb().to_ndarray() for idx in index])\n    frames = frames.transpose(0, 3, 1, 2)\n    return frames\n\nfile = hf_hub_download(repo_id=\"Intel/tvp_demo\", filename=\"AK2KG.mp4\", repo_type=\"dataset\")\nmodel = TvpForVideoGrounding.from_pretrained(\"Intel/tvp-base\")\n\ndecoder_kwargs = dict(\n    container=av.open(file, metadata_errors=\"ignore\"),\n    sampling_rate=1,\n    num_frames=model.config.num_frames,\n    clip_idx=0,\n    num_clips=1,\n    target_fps=3,\n)\nraw_sampled_frms = decode(**decoder_kwargs)\n\ntext = \"a person is sitting on a bed.\"\nprocessor = AutoProcessor.from_pretrained(\"Intel/tvp-base\")\nmodel_inputs = processor(\n    text=[text], videos=list(raw_sampled_frms), return_tensors=\"pt\", max_text_length=100#, size=size\n)\n\nmodel_inputs[\"pixel_values\"] = model_inputs[\"pixel_values\"].to(model.dtype)\noutput = model(**model_inputs)\n\ndef get_video_duration(filename):\n    cap = cv2.VideoCapture(filename)\n    if cap.isOpened():\n        rate = cap.get(5)\n        frame_num = cap.get(7)\n        duration = frame_num/rate\n        return duration\n    return -1\n\nduration = get_video_duration(file)\nstart, end = processor.post_process_video_grounding(output.logits, duration)\n\nprint(f\"The time slot of the video corresponding to the text \\\"{text}\\\" is from {start}s to {end}s\")\n```", "```py\n( backbone_config = None distance_loss_weight = 1.0 duration_loss_weight = 0.1 visual_prompter_type = 'framepad' visual_prompter_apply = 'replace' visual_prompt_size = 96 max_img_size = 448 num_frames = 48 vocab_size = 30522 hidden_size = 768 intermediate_size = 3072 num_hidden_layers = 12 num_attention_heads = 12 max_position_embeddings = 512 max_grid_col_position_embeddings = 100 max_grid_row_position_embeddings = 100 hidden_dropout_prob = 0.1 hidden_act = 'gelu' layer_norm_eps = 1e-12 initializer_range = 0.02 attention_probs_dropout_prob = 0.1 **kwargs )\n```", "```py\n( backbone_config: PretrainedConfig **kwargs ) \u2192 export const metadata = 'undefined';TvpConfig\n```", "```py\n( ) \u2192 export const metadata = 'undefined';Dict[str, any]\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_pad: bool = True pad_size: Dict = None constant_values: Union = 0 pad_mode: PaddingMode = <PaddingMode.CONSTANT: 'constant'> do_normalize: bool = True do_flip_channel_order: bool = True image_mean: Union = None image_std: Union = None **kwargs )\n```", "```py\n( videos: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: Dict = None do_rescale: bool = None rescale_factor: float = None do_pad: bool = None pad_size: Dict = None constant_values: Union = None pad_mode: PaddingMode = None do_normalize: bool = None do_flip_channel_order: bool = None image_mean: Union = None image_std: Union = None return_tensors: Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( image_processor = None tokenizer = None **kwargs )\n```", "```py\n( text = None videos = None return_tensors = None **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoConfig, AutoTokenizer, TvpModel\n\n>>> model = TvpModel.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> pixel_values = torch.rand(1, 1, 3, 448, 448)\n>>> text_inputs = tokenizer(\"This is an example input\", return_tensors=\"pt\")\n>>> output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)\n```", "```py\n( config )\n```", "```py\n( input_ids: Optional = None pixel_values: Optional = None attention_mask: Optional = None labels: Tuple = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.tvp.modeling_tvp.TvpVideoGroundingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoConfig, AutoTokenizer, TvpForVideoGrounding\n\n>>> model = TvpForVideoGrounding.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> pixel_values = torch.rand(1, 1, 3, 448, 448)\n>>> text_inputs = tokenizer(\"This is an example input\", return_tensors=\"pt\")\n>>> output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)\n```"]