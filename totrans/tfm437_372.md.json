["```py\nimport av\nimport cv2\nimport numpy as np\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoProcessor, TvpForVideoGrounding\n\ndef pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    Convert the video from its original fps to the target_fps and decode the video with PyAV decoder.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling.\n            If clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly sample from the given video.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video. Return None if the no\n            video stream was found.\n        fps (float): the number of frames per second of the video.\n    '''\n    video = container.streams.video[0]\n    fps = float(video.average_rate)\n    clip_size = sampling_rate * num_frames / target_fps * fps\n    delta = max(num_frames - clip_size, 0)\n    start_idx = delta * clip_idx / num_clips\n    end_idx = start_idx + clip_size - 1\n    timebase = video.duration / num_frames\n    video_start_pts = int(start_idx * timebase)\n    video_end_pts = int(end_idx * timebase)\n    seek_offset = max(video_start_pts - 1024, 0)\n    container.seek(seek_offset, any_frame=False, backward=True, stream=video)\n    frames = {}\n    for frame in container.decode(video=0):\n        if frame.pts < video_start_pts:\n            continue\n        frames[frame.pts] = frame\n        if frame.pts > video_end_pts:\n            break\n    frames = [frames[pts] for pts in sorted(frames)]\n    return frames, fps\n\ndef decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n    '''\n    Decode the video and perform temporal sampling.\n    Args:\n        container (container): pyav container.\n        sampling_rate (int): frame sampling rate (interval between two sampled frames).\n        num_frames (int): number of frames to sample.\n        clip_idx (int): if clip_idx is -1, perform random temporal sampling.\n            If clip_idx is larger than -1, uniformly split the video to num_clips\n            clips, and select the clip_idx-th video clip.\n        num_clips (int): overall number of clips to uniformly sample from the given video.\n        target_fps (int): the input video may have different fps, convert it to\n            the target video fps before frame sampling.\n    Returns:\n        frames (tensor): decoded frames from the video.\n    '''\n    assert clip_idx >= -2, \"Not a valied clip_idx {}\".format(clip_idx)\n    frames, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)\n    clip_size = sampling_rate * num_frames / target_fps * fps\n    index = np.linspace(0, clip_size - 1, num_frames)\n    index = np.clip(index, 0, len(frames) - 1).astype(np.int64)\n    frames = np.array([frames[idx].to_rgb().to_ndarray() for idx in index])\n    frames = frames.transpose(0, 3, 1, 2)\n    return frames\n\nfile = hf_hub_download(repo_id=\"Intel/tvp_demo\", filename=\"AK2KG.mp4\", repo_type=\"dataset\")\nmodel = TvpForVideoGrounding.from_pretrained(\"Intel/tvp-base\")\n\ndecoder_kwargs = dict(\n    container=av.open(file, metadata_errors=\"ignore\"),\n    sampling_rate=1,\n    num_frames=model.config.num_frames,\n    clip_idx=0,\n    num_clips=1,\n    target_fps=3,\n)\nraw_sampled_frms = decode(**decoder_kwargs)\n\ntext = \"a person is sitting on a bed.\"\nprocessor = AutoProcessor.from_pretrained(\"Intel/tvp-base\")\nmodel_inputs = processor(\n    text=[text], videos=list(raw_sampled_frms), return_tensors=\"pt\", max_text_length=100#, size=size\n)\n\nmodel_inputs[\"pixel_values\"] = model_inputs[\"pixel_values\"].to(model.dtype)\noutput = model(**model_inputs)\n\ndef get_video_duration(filename):\n    cap = cv2.VideoCapture(filename)\n    if cap.isOpened():\n        rate = cap.get(5)\n        frame_num = cap.get(7)\n        duration = frame_num/rate\n        return duration\n    return -1\n\nduration = get_video_duration(file)\nstart, end = processor.post_process_video_grounding(output.logits, duration)\n\nprint(f\"The time slot of the video corresponding to the text \\\"{text}\\\" is from {start}s to {end}s\")\n```", "```py\n>>> import torch\n>>> from transformers import AutoConfig, AutoTokenizer, TvpModel\n\n>>> model = TvpModel.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> pixel_values = torch.rand(1, 1, 3, 448, 448)\n>>> text_inputs = tokenizer(\"This is an example input\", return_tensors=\"pt\")\n>>> output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)\n```", "```py\n>>> import torch\n>>> from transformers import AutoConfig, AutoTokenizer, TvpForVideoGrounding\n\n>>> model = TvpForVideoGrounding.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"Jiqing/tiny-random-tvp\")\n\n>>> pixel_values = torch.rand(1, 1, 3, 448, 448)\n>>> text_inputs = tokenizer(\"This is an example input\", return_tensors=\"pt\")\n>>> output = model(text_inputs.input_ids, pixel_values, text_inputs.attention_mask)\n```"]