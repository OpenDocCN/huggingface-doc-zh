- en: Fully Sharded Data Parallel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: To accelerate training huge models on larger batch sizes, we can use a fully
    sharded data parallel model. This type of data parallel paradigm enables fitting
    more data and larger models by sharding the optimizer states, gradients and parameters.
    To read more about it and the benefits, check out the [Fully Sharded Data Parallel
    blog](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).
    We have integrated the latest PyTorchâ€™s Fully Sharded Data Parallel (FSDP) training
    feature. All you need to do is enable it through the config.
  prefs: []
  type: TYPE_NORMAL
- en: How it works out of the box
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On your machine(s) just run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and answer the questions asked. This will generate a config file that will be
    used automatically to properly set the default options when doing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, here is how you would run `examples/nlp_example.py` (from the
    root of the repo) with FSDP enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, `Accelerate` supports the following config through the CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_sharding_strategy`: [1] FULL_SHARD (shards optimizer states, gradients
    and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3]
    NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters
    within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards
    optimizer states and gradients within each node while each node has full copy).
    For more information, please refer the official [PyTorch docs](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy).'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_offload_params` : Decides Whether to offload parameters and gradients
    to CPU'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_auto_wrap_policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3]
    NO_WRAP'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_transformer_layer_cls_to_wrap`: Only applicable for ðŸ¤— Transformers. When
    using `fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP`, a user may provide a comma-separated
    string of transformer layer class names (case-sensitive) to wrap, e.g., `BertLayer`,
    `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`. This is important
    because submodules that share weights (e.g., embedding layers) should not end
    up in different FSDP wrapped units. Using this policy, wrapping happens for each
    block containing Multi-Head Attention followed by a couple of MLP layers. Remaining
    layers including the shared embeddings are conveniently wrapped in same outermost
    FSDP unit. Therefore, use this for transformer-based models. You can use the `model._no_split_modules`
    for ðŸ¤— Transformer models by answering `yes` to `Do you want to use the model''s`
    _no_split_modules`to wrap. It will try to use`model._no_split_modules` when possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_min_num_params`: minimum number of parameters when using `fsdp_auto_wrap_policy=SIZE_BASED_WRAP`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_backward_prefetch_policy`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_forward_prefetch`: if True, then FSDP explicitly prefetches the next
    upcoming all-gather while executing in the forward pass. Should only be used for
    static-graph models since the prefetching follows the first iterationâ€™s execution
    order. i.e., if the sub-modulesâ€™ order changes dynamically during the modelâ€™s
    execution do not enable this feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_state_dict_type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_use_orig_params`: If True, allows non-uniform `requires_grad` during
    init, which means support for interspersed frozen and trainable parameters. This
    setting is useful in cases such as parameter-efficient fine-tuning as discussed
    in [this post](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019).
    This option also allows one to have multiple optimizer param groups. This should
    be `True` when creating an optimizer before preparing/wrapping the model with
    FSDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_cpu_ram_efficient_loading`: Only applicable for ðŸ¤— Transformers models.
    If True, only the first process loads the pretrained model checkpoint while all
    other processes have empty weights. This should be set to False if you experience
    errors when loading the pretrained ðŸ¤— Transformers model via `from_pretrained`
    method. When this setting is True `fsdp_sync_module_states` also must to be True,
    otherwise all the processes except the main process would have random weights
    leading to unexpected behaviour during training. For this to work, make sure the
    distributed process group is initialized before calling Transformers `from_pretrained`
    method. When using ðŸ¤— Trainer API, the distributed process group is initialized
    when you create an instance of `TrainingArguments` class.'
  prefs: []
  type: TYPE_NORMAL
- en: '`fsdp_sync_module_states`: If True, each individually wrapped FSDP unit will
    broadcast module parameters from rank 0.'
  prefs: []
  type: TYPE_NORMAL
- en: For additional and more nuanced control, you can specify other FSDP parameters
    via `FullyShardedDataParallelPlugin`. When creating `FullyShardedDataParallelPlugin`
    object, pass it the parameters that werenâ€™t part of the accelerate config or if
    you want to override them. The FSDP parameters will be picked based on the accelerate
    config file or launch command arguments and other parameters that you will pass
    directly through the `FullyShardedDataParallelPlugin` object will set/override
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Saving and loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The new recommended way of checkpointing when using FSDP models is to use `SHARDED_STATE_DICT`
    as `StateDictType` when setting up the accelerate config. Below is the code snippet
    to save using `save_state` utility of accelerate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspect the checkpoint folder to see model and optimizer as shards per process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To load them back for resuming the training, use the `load_state` utility of
    accelerate
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When using transformers `save_pretrained`, pass `state_dict=accelerator.get_state_dict(model)`
    to save the model state dict. Below is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: State Dict
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`accelerator.get_state_dict` will call the underlying `model.state_dict` implementation
    using `FullStateDictConfig(offload_to_cpu=True, rank0_only=True)` context manager
    to get the state dict only for rank 0 and it will be offloaded to CPU.'
  prefs: []
  type: TYPE_NORMAL
- en: You can then pass `state` into the `save_pretrained` method. There are several
    modes for `StateDictType` and `FullStateDictConfig` that you can use to control
    the behavior of `state_dict`. For more information, see the [PyTorch documentation](https://pytorch.org/docs/stable/fsdp.html).
  prefs: []
  type: TYPE_NORMAL
- en: Mapping between FSDP sharding strategies and DeepSpeed ZeRO Stages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`FULL_SHARD` maps to the DeepSpeed `ZeRO Stage-3`. Shards optimizer states,
    gradients and parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SHARD_GRAD_OP` maps to the DeepSpeed `ZeRO Stage-2`. Shards optimizer states
    and gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NO_SHARD` maps to `ZeRO Stage-0`. No sharding wherein each GPU has full copy
    of model, optimizer states and gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYBRID_SHARD` maps to `ZeRO++ Stage-3` wherein `zero_hpz_partition_size=<num_gpus_per_node>`.
    Here, this will shard optimizer states, gradients and parameters within each node
    while each node has full copy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few caveats to be aware of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In case of multiple models, pass the optimizers to the prepare call in the same
    order as corresponding models else `accelerator.save_state()` and `accelerator.load_state()`
    will result in wrong/unexpected behaviour.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This feature is incompatible with `--predict_with_generate` in the `run_translation.py`
    script of ðŸ¤— `Transformers` library.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more control, users can leverage the `FullyShardedDataParallelPlugin`. After
    creating an instance of this class, users can pass it to the Accelerator class
    instantiation. For more information on these options, please refer to the PyTorch
    [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236)
    code.
  prefs: []
  type: TYPE_NORMAL
