- en: Fully Sharded Data Parallel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fully Sharded Data Parallel
- en: 'Original text: [https://huggingface.co/docs/accelerate/usage_guides/fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/accelerate/usage_guides/fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To accelerate training huge models on larger batch sizes, we can use a fully
    sharded data parallel model. This type of data parallel paradigm enables fitting
    more data and larger models by sharding the optimizer states, gradients and parameters.
    To read more about it and the benefits, check out the [Fully Sharded Data Parallel
    blog](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).
    We have integrated the latest PyTorchâ€™s Fully Sharded Data Parallel (FSDP) training
    feature. All you need to do is enable it through the config.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ é€Ÿåœ¨æ›´å¤§çš„æ‰¹é‡å¤§å°ä¸Šè®­ç»ƒåºå¤§çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®Œå…¨åˆ†ç‰‡çš„æ•°æ®å¹¶è¡Œæ¨¡å‹ã€‚è¿™ç§æ•°æ®å¹¶è¡ŒèŒƒå¼é€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°æ¥å®ç°æ›´å¤šæ•°æ®å’Œæ›´å¤§æ¨¡å‹çš„æ‹Ÿåˆã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯å’Œå¥½å¤„ï¼Œè¯·æŸ¥çœ‹[å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œåšå®¢](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)ã€‚æˆ‘ä»¬å·²ç»é›†æˆäº†æœ€æ–°çš„PyTorchå®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDPï¼‰è®­ç»ƒåŠŸèƒ½ã€‚æ‚¨åªéœ€è¦é€šè¿‡é…ç½®å¯ç”¨å®ƒã€‚
- en: How it works out of the box
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€ç®±å³ç”¨çš„å·¥ä½œåŸç†
- en: 'On your machine(s) just run:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‚¨çš„æœºå™¨ä¸Šè¿è¡Œï¼š
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and answer the questions asked. This will generate a config file that will be
    used automatically to properly set the default options when doing
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å›ç­”æå‡ºçš„é—®é¢˜ã€‚è¿™å°†ç”Ÿæˆä¸€ä¸ªé…ç½®æ–‡ä»¶ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨ä½¿ç”¨ä»¥æ­£ç¡®è®¾ç½®é»˜è®¤é€‰é¡¹
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For instance, here is how you would run `examples/nlp_example.py` (from the
    root of the repo) with FSDP enabled:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¿™æ˜¯å¦‚ä½•åœ¨å¯ç”¨FSDPçš„æƒ…å†µä¸‹è¿è¡Œ`examples/nlp_example.py`ï¼ˆä»å­˜å‚¨åº“çš„æ ¹ç›®å½•ï¼‰çš„æ–¹å¼ï¼š
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Currently, `Accelerate` supports the following config through the CLI:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œ`Accelerate`é€šè¿‡CLIæ”¯æŒä»¥ä¸‹é…ç½®ï¼š
- en: '`fsdp_sharding_strategy`: [1] FULL_SHARD (shards optimizer states, gradients
    and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3]
    NO_SHARD (DDP), [4] HYBRID_SHARD (shards optimizer states, gradients and parameters
    within each node while each node has full copy), [5] HYBRID_SHARD_ZERO2 (shards
    optimizer states and gradients within each node while each node has full copy).
    For more information, please refer the official [PyTorch docs](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_sharding_strategy`ï¼š[1] FULL_SHARDï¼ˆåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼‰ï¼Œ[2] SHARD_GRAD_OPï¼ˆåˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼‰ï¼Œ[3]
    NO_SHARDï¼ˆDDPï¼‰ï¼Œ[4] HYBRID_SHARDï¼ˆåœ¨æ¯ä¸ªèŠ‚ç‚¹å†…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´å‰¯æœ¬ï¼‰ï¼Œ[5] HYBRID_SHARD_ZERO2ï¼ˆåœ¨æ¯ä¸ªèŠ‚ç‚¹å†…åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´å‰¯æœ¬ï¼‰ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…å®˜æ–¹[PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy)ã€‚'
- en: '`fsdp_offload_params` : Decides Whether to offload parameters and gradients
    to CPU'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_offload_params`ï¼šå†³å®šæ˜¯å¦å°†å‚æ•°å’Œæ¢¯åº¦å¸è½½åˆ°CPU'
- en: '`fsdp_auto_wrap_policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3]
    NO_WRAP'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_auto_wrap_policy`ï¼š[1] TRANSFORMER_BASED_WRAPï¼Œ[2] SIZE_BASED_WRAPï¼Œ[3]
    NO_WRAP'
- en: '`fsdp_transformer_layer_cls_to_wrap`: Only applicable for ğŸ¤— Transformers. When
    using `fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP`, a user may provide a comma-separated
    string of transformer layer class names (case-sensitive) to wrap, e.g., `BertLayer`,
    `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`. This is important
    because submodules that share weights (e.g., embedding layers) should not end
    up in different FSDP wrapped units. Using this policy, wrapping happens for each
    block containing Multi-Head Attention followed by a couple of MLP layers. Remaining
    layers including the shared embeddings are conveniently wrapped in same outermost
    FSDP unit. Therefore, use this for transformer-based models. You can use the `model._no_split_modules`
    for ğŸ¤— Transformer models by answering `yes` to `Do you want to use the model''s`
    _no_split_modules`to wrap. It will try to use`model._no_split_modules` when possible.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_transformer_layer_cls_to_wrap`ï¼šä»…é€‚ç”¨äºğŸ¤— Transformersã€‚å½“ä½¿ç”¨`fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP`æ—¶ï¼Œç”¨æˆ·å¯ä»¥æä¾›ä¸€ä¸ªé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œå…¶ä¸­åŒ…å«è¦åŒ…è£…çš„å˜å‹å™¨å±‚ç±»åç§°ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰ï¼Œä¾‹å¦‚`BertLayer`ï¼Œ`GPTJBlock`ï¼Œ`T5Block`ï¼Œ`BertLayerï¼ŒBertEmbeddingsï¼ŒBertSelfOutput`ã€‚è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå…±äº«æƒé‡çš„å­æ¨¡å—ï¼ˆä¾‹å¦‚åµŒå…¥å±‚ï¼‰ä¸åº”è¯¥å‡ºç°åœ¨ä¸åŒçš„FSDPåŒ…è£…å•å…ƒä¸­ã€‚ä½¿ç”¨æ­¤ç­–ç•¥ï¼Œæ¯ä¸ªåŒ…å«å¤šå¤´æ³¨æ„åŠ›å’Œå‡ ä¸ªMLPå±‚çš„å—éƒ½ä¼šè¿›è¡ŒåŒ…è£…ã€‚å…¶ä½™å±‚ï¼ŒåŒ…æ‹¬å…±äº«çš„åµŒå…¥å±‚ï¼Œéƒ½æ–¹ä¾¿åœ°åŒ…è£…åœ¨åŒä¸€ä¸ªæœ€å¤–å±‚çš„FSDPå•å…ƒä¸­ã€‚å› æ­¤ï¼Œå¯¹äºåŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œè¯·ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚æ‚¨å¯ä»¥é€šè¿‡å›ç­”`yes`æ¥ä½¿ç”¨ğŸ¤—
    Transformeræ¨¡å‹çš„`model._no_split_modules`ï¼Œä»¥ä½¿ç”¨`model._no_split_modules`ã€‚åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå®ƒå°†å°è¯•ä½¿ç”¨`model._no_split_modules`ã€‚'
- en: '`fsdp_min_num_params`: minimum number of parameters when using `fsdp_auto_wrap_policy=SIZE_BASED_WRAP`.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_min_num_params`ï¼šåœ¨ä½¿ç”¨`fsdp_auto_wrap_policy=SIZE_BASED_WRAP`æ—¶ï¼Œå‚æ•°æ•°é‡çš„æœ€å°å€¼ã€‚'
- en: '`fsdp_backward_prefetch_policy`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_backward_prefetch_policy`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH'
- en: '`fsdp_forward_prefetch`: if True, then FSDP explicitly prefetches the next
    upcoming all-gather while executing in the forward pass. Should only be used for
    static-graph models since the prefetching follows the first iterationâ€™s execution
    order. i.e., if the sub-modulesâ€™ order changes dynamically during the modelâ€™s
    execution do not enable this feature.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_forward_prefetch`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™FSDPåœ¨å‰å‘ä¼ é€’æ‰§è¡Œæ—¶æ˜¾å¼é¢„å–ä¸‹ä¸€ä¸ªå³å°†åˆ°æ¥çš„å…¨èšåˆã€‚åº”ä»…ç”¨äºé™æ€å›¾æ¨¡å‹ï¼Œå› ä¸ºé¢„å–éµå¾ªç¬¬ä¸€æ¬¡è¿­ä»£çš„æ‰§è¡Œé¡ºåºã€‚å³ï¼Œå¦‚æœå­æ¨¡å—çš„é¡ºåºåœ¨æ¨¡å‹æ‰§è¡Œè¿‡ç¨‹ä¸­åŠ¨æ€å˜åŒ–ï¼Œè¯·ä¸è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚'
- en: '`fsdp_state_dict_type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_state_dict_type`ï¼š[1] FULL_STATE_DICTï¼Œ[2] LOCAL_STATE_DICTï¼Œ[3] SHARDED_STATE_DICT'
- en: '`fsdp_use_orig_params`: If True, allows non-uniform `requires_grad` during
    init, which means support for interspersed frozen and trainable parameters. This
    setting is useful in cases such as parameter-efficient fine-tuning as discussed
    in [this post](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019).
    This option also allows one to have multiple optimizer param groups. This should
    be `True` when creating an optimizer before preparing/wrapping the model with
    FSDP.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_use_orig_params`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™å…è®¸åœ¨åˆå§‹åŒ–æœŸé—´ä½¿ç”¨éå‡åŒ€çš„`requires_grad`ï¼Œè¿™æ„å‘³ç€æ”¯æŒäº¤æ›¿å†»ç»“å’Œå¯è®­ç»ƒå‚æ•°ã€‚åœ¨è¯¸å¦‚å‚æ•°é«˜æ•ˆå¾®è°ƒç­‰æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè®¾ç½®éå¸¸æœ‰ç”¨ï¼Œå¦‚[æ­¤å¸–å­](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)ä¸­æ‰€è®¨è®ºçš„ã€‚æ­¤é€‰é¡¹è¿˜å…è®¸ä¸€ä¸ªæ‹¥æœ‰å¤šä¸ªä¼˜åŒ–å™¨å‚æ•°ç»„ã€‚åœ¨ä½¿ç”¨FSDPä¹‹å‰åˆ›å»ºä¼˜åŒ–å™¨æ—¶ï¼Œåº”å°†æ­¤è®¾ç½®ä¸º`True`ã€‚'
- en: '`fsdp_cpu_ram_efficient_loading`: Only applicable for ğŸ¤— Transformers models.
    If True, only the first process loads the pretrained model checkpoint while all
    other processes have empty weights. This should be set to False if you experience
    errors when loading the pretrained ğŸ¤— Transformers model via `from_pretrained`
    method. When this setting is True `fsdp_sync_module_states` also must to be True,
    otherwise all the processes except the main process would have random weights
    leading to unexpected behaviour during training. For this to work, make sure the
    distributed process group is initialized before calling Transformers `from_pretrained`
    method. When using ğŸ¤— Trainer API, the distributed process group is initialized
    when you create an instance of `TrainingArguments` class.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_cpu_ram_efficient_loading`ï¼šä»…é€‚ç”¨äºğŸ¤— Transformersæ¨¡å‹ã€‚å¦‚æœä¸ºTrueï¼Œåˆ™åªæœ‰ç¬¬ä¸€ä¸ªè¿›ç¨‹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œè€Œæ‰€æœ‰å…¶ä»–è¿›ç¨‹éƒ½å…·æœ‰ç©ºæƒé‡ã€‚å¦‚æœåœ¨é€šè¿‡`from_pretrained`æ–¹æ³•åŠ è½½é¢„è®­ç»ƒğŸ¤—
    Transformersæ¨¡å‹æ—¶é‡åˆ°é”™è¯¯ï¼Œåˆ™åº”å°†å…¶è®¾ç½®ä¸ºFalseã€‚å½“æ­¤è®¾ç½®ä¸ºTrueæ—¶ï¼Œ`fsdp_sync_module_states`ä¹Ÿå¿…é¡»ä¸ºTrueï¼Œå¦åˆ™é™¤ä¸»è¿›ç¨‹å¤–çš„æ‰€æœ‰è¿›ç¨‹éƒ½å°†å…·æœ‰éšæœºæƒé‡ï¼Œå¯¼è‡´è®­ç»ƒæœŸé—´å‡ºç°æ„å¤–è¡Œä¸ºã€‚ä¸ºä½¿å…¶æ­£å¸¸å·¥ä½œï¼Œè¯·ç¡®ä¿åœ¨è°ƒç”¨Transformersçš„`from_pretrained`æ–¹æ³•ä¹‹å‰åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ã€‚åœ¨ä½¿ç”¨ğŸ¤—
    Trainer APIæ—¶ï¼Œå½“æ‚¨åˆ›å»º`TrainingArguments`ç±»çš„å®ä¾‹æ—¶ï¼Œåˆ†å¸ƒå¼è¿›ç¨‹ç»„å°†è¢«åˆå§‹åŒ–ã€‚'
- en: '`fsdp_sync_module_states`: If True, each individually wrapped FSDP unit will
    broadcast module parameters from rank 0.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`fsdp_sync_module_states`ï¼šå¦‚æœä¸ºTrueï¼Œåˆ™æ¯ä¸ªå•ç‹¬åŒ…è£…çš„FSDPå•å…ƒå°†ä»æ’å0å¹¿æ’­æ¨¡å—å‚æ•°ã€‚'
- en: For additional and more nuanced control, you can specify other FSDP parameters
    via `FullyShardedDataParallelPlugin`. When creating `FullyShardedDataParallelPlugin`
    object, pass it the parameters that werenâ€™t part of the accelerate config or if
    you want to override them. The FSDP parameters will be picked based on the accelerate
    config file or launch command arguments and other parameters that you will pass
    directly through the `FullyShardedDataParallelPlugin` object will set/override
    that.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—é¢å¤–å’Œæ›´å¾®å¦™çš„æ§åˆ¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡`FullyShardedDataParallelPlugin`æŒ‡å®šå…¶ä»–FSDPå‚æ•°ã€‚åˆ›å»º`FullyShardedDataParallelPlugin`å¯¹è±¡æ—¶ï¼Œå°†å…¶ä¼ é€’ç»™é‚£äº›ä¸æ˜¯åŠ é€Ÿé…ç½®çš„ä¸€éƒ¨åˆ†æˆ–è€…å¦‚æœæ‚¨æƒ³è¦è¦†ç›–å®ƒä»¬çš„å‚æ•°ã€‚FSDPå‚æ•°å°†æ ¹æ®åŠ é€Ÿé…ç½®æ–‡ä»¶æˆ–å¯åŠ¨å‘½ä»¤å‚æ•°é€‰æ‹©ï¼Œå¹¶ä¸”æ‚¨å°†ç›´æ¥é€šè¿‡`FullyShardedDataParallelPlugin`å¯¹è±¡ä¼ é€’çš„å…¶ä»–å‚æ•°å°†è®¾ç½®/è¦†ç›–å®ƒã€‚
- en: 'Below is an example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Saving and loading
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¿å­˜å’ŒåŠ è½½
- en: The new recommended way of checkpointing when using FSDP models is to use `SHARDED_STATE_DICT`
    as `StateDictType` when setting up the accelerate config. Below is the code snippet
    to save using `save_state` utility of accelerate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨FSDPæ¨¡å‹æ—¶ï¼Œå»ºè®®çš„æ–°çš„æ£€æŸ¥ç‚¹æ–¹å¼æ˜¯åœ¨è®¾ç½®åŠ é€Ÿé…ç½®æ—¶å°†`SHARDED_STATE_DICT`è®¾ç½®ä¸º`StateDictType`ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨åŠ é€Ÿçš„`save_state`å®ç”¨ç¨‹åºä¿å­˜çš„ä»£ç ç‰‡æ®µã€‚
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Inspect the checkpoint folder to see model and optimizer as shards per process:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹ä»¥æŸ¥çœ‹æ¯ä¸ªè¿›ç¨‹çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨ä½œä¸ºåˆ†ç‰‡ï¼š
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To load them back for resuming the training, use the `load_state` utility of
    accelerate
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åŠ è½½å®ƒä»¬ä»¥æ¢å¤è®­ç»ƒï¼Œè¯·ä½¿ç”¨åŠ é€Ÿçš„`load_state`å®ç”¨ç¨‹åº
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When using transformers `save_pretrained`, pass `state_dict=accelerator.get_state_dict(model)`
    to save the model state dict. Below is an example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨transformersçš„`save_pretrained`æ—¶ï¼Œå°†`state_dict=accelerator.get_state_dict(model)`ä¼ é€’ä»¥ä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: State Dict
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: çŠ¶æ€å­—å…¸
- en: '`accelerator.get_state_dict` will call the underlying `model.state_dict` implementation
    using `FullStateDictConfig(offload_to_cpu=True, rank0_only=True)` context manager
    to get the state dict only for rank 0 and it will be offloaded to CPU.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`accelerator.get_state_dict`å°†ä½¿ç”¨`FullStateDictConfig(offload_to_cpu=True, rank0_only=True)`ä¸Šä¸‹æ–‡ç®¡ç†å™¨è°ƒç”¨åº•å±‚çš„`model.state_dict`å®ç°ï¼Œä»…ä¸ºæ’å0è·å–çŠ¶æ€å­—å…¸ï¼Œå¹¶å°†å…¶å¸è½½åˆ°CPUã€‚'
- en: You can then pass `state` into the `save_pretrained` method. There are several
    modes for `StateDictType` and `FullStateDictConfig` that you can use to control
    the behavior of `state_dict`. For more information, see the [PyTorch documentation](https://pytorch.org/docs/stable/fsdp.html).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯ä»¥å°†`state`ä¼ é€’ç»™`save_pretrained`æ–¹æ³•ã€‚æœ‰å‡ ç§`StateDictType`å’Œ`FullStateDictConfig`çš„æ¨¡å¼å¯ç”¨äºæ§åˆ¶`state_dict`çš„è¡Œä¸ºã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/fsdp.html)ã€‚
- en: Mapping between FSDP sharding strategies and DeepSpeed ZeRO Stages
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FSDPåˆ†ç‰‡ç­–ç•¥ä¸DeepSpeed ZeROé˜¶æ®µä¹‹é—´çš„æ˜ å°„
- en: '`FULL_SHARD` maps to the DeepSpeed `ZeRO Stage-3`. Shards optimizer states,
    gradients and parameters.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FULL_SHARD`æ˜ å°„åˆ°DeepSpeedçš„`ZeRO Stage-3`ã€‚åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°ã€‚'
- en: '`SHARD_GRAD_OP` maps to the DeepSpeed `ZeRO Stage-2`. Shards optimizer states
    and gradients.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SHARD_GRAD_OP`æ˜ å°„åˆ°DeepSpeedçš„`ZeRO Stage-2`ã€‚åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦ã€‚'
- en: '`NO_SHARD` maps to `ZeRO Stage-0`. No sharding wherein each GPU has full copy
    of model, optimizer states and gradients.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NO_SHARD`æ˜ å°„åˆ°`ZeRO Stage-0`ã€‚æ²¡æœ‰åˆ†ç‰‡ï¼Œæ¯ä¸ªGPUéƒ½æœ‰æ¨¡å‹ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦çš„å®Œæ•´å‰¯æœ¬ã€‚'
- en: '`HYBRID_SHARD` maps to `ZeRO++ Stage-3` wherein `zero_hpz_partition_size=<num_gpus_per_node>`.
    Here, this will shard optimizer states, gradients and parameters within each node
    while each node has full copy.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HYBRID_SHARD`æ˜ å°„åˆ°`ZeRO++ Stage-3`ï¼Œå…¶ä¸­`zero_hpz_partition_size=<num_gpus_per_node>`ã€‚åœ¨è¿™é‡Œï¼Œè¿™å°†åœ¨æ¯ä¸ªèŠ‚ç‚¹å†…å¯¹ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°è¿›è¡Œåˆ†ç‰‡ï¼Œè€Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰å®Œæ•´çš„å‰¯æœ¬ã€‚'
- en: A few caveats to be aware of
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„ä¸€äº›æ³¨æ„äº‹é¡¹
- en: In case of multiple models, pass the optimizers to the prepare call in the same
    order as corresponding models else `accelerator.save_state()` and `accelerator.load_state()`
    will result in wrong/unexpected behaviour.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¤šä¸ªæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå°†ä¼˜åŒ–å™¨æŒ‰ç…§ç›¸åº”æ¨¡å‹çš„é¡ºåºä¼ é€’ç»™å‡†å¤‡è°ƒç”¨ï¼Œå¦åˆ™`accelerator.save_state()`å’Œ`accelerator.load_state()`å°†å¯¼è‡´é”™è¯¯/æ„å¤–è¡Œä¸ºã€‚
- en: This feature is incompatible with `--predict_with_generate` in the `run_translation.py`
    script of ğŸ¤— `Transformers` library.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­¤åŠŸèƒ½ä¸ğŸ¤—`Transformers`åº“ä¸­`run_translation.py`è„šæœ¬ä¸­çš„`--predict_with_generate`ä¸å…¼å®¹ã€‚
- en: For more control, users can leverage the `FullyShardedDataParallelPlugin`. After
    creating an instance of this class, users can pass it to the Accelerator class
    instantiation. For more information on these options, please refer to the PyTorch
    [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236)
    code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ›´å¤šæ§åˆ¶ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨`FullyShardedDataParallelPlugin`ã€‚åœ¨åˆ›å»ºæ­¤ç±»çš„å®ä¾‹åï¼Œç”¨æˆ·å¯ä»¥å°†å…¶ä¼ é€’ç»™Acceleratorç±»çš„å®ä¾‹åŒ–ã€‚æœ‰å…³è¿™äº›é€‰é¡¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…PyTorch
    [FullyShardedDataParallel](https://github.com/pytorch/pytorch/blob/0df2e863fbd5993a7b9e652910792bd21a516ff3/torch/distributed/fsdp/fully_sharded_data_parallel.py#L236)
    ä»£ç ã€‚
