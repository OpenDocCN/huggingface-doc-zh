# LXMERT

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/lxmert](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/lxmert)

## 概述

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。它是一系列双向Transformer编码器（一个用于视觉模态，一个用于语言模态，然后一个用于融合两种模态），使用一种组合的方法进行预训练，包括遮蔽语言建模、视觉-语言文本对齐、ROI特征回归、遮蔽视觉属性建模、遮蔽视觉对象建模和视觉问题回答目标。预训练包括多个多模态数据集：MSCOCO、Visual-Genome + Visual-Genome Question Answering、VQA 2.0和GQA。

摘要如下：

*视觉与语言推理需要理解视觉概念、语言语义，最重要的是理解这两种模态之间的对齐和关系。因此，我们提出了LXMERT（Learning Cross-Modality Encoder Representations from Transformers）框架来学习这些视觉与语言的连接。在LXMERT中，我们构建了一个大规模的Transformer模型，包括三个编码器：对象关系编码器、语言编码器和跨模态编码器。接下来，为了赋予我们的模型连接视觉和语言语义的能力，我们使用大量的图像和句子对进行预训练，通过五种不同的代表性预训练任务：遮蔽语言建模、遮蔽对象预测（特征回归和标签分类）、跨模态匹配和图像问题回答。这些任务有助于学习模态内部和模态间的关系。在从我们的预训练参数微调后，我们的模型在两个视觉问题回答数据集（即VQA和GQA）上取得了最先进的结果。我们还展示了我们预训练的跨模态模型的泛化能力，通过将其适应具有挑战性的视觉推理任务NLVR，将先前的最佳结果提高了22%绝对值（从54%到76%）。最后，我们进行了详细的消融研究，证明了我们的新颖模型组件和预训练策略对我们强大结果的显著贡献；并展示了不同编码器的几个注意力可视化*

此模型由[eltoto1219](https://huggingface.co/eltoto1219)贡献。原始代码可在[此处](https://github.com/airsplay/lxmert)找到。

## 使用提示

+   在视觉特征嵌入中不必使用边界框，任何类型的视觉空间特征都可以使用。

+   LXMERT输出的语言隐藏状态和视觉隐藏状态都经过了跨模态层，因此它们包含来自两种模态的信息。要访问仅关注自身的模态，请从元组中的第一个输入中选择视觉/语言隐藏状态。

+   双向跨模态编码器注意力仅在语言模态用作输入且视觉模态用作上下文向量时返回注意力值。此外，虽然跨模态编码器包含每个相应模态的自注意力和交叉注意力，但只返回交叉注意力，两个自注意力输出都被忽略。

## 资源

+   [问答任务指南](../tasks/question_answering)

## LxmertConfig

### `class transformers.LxmertConfig`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/configuration_lxmert.py#L29)

```py
( vocab_size = 30522 hidden_size = 768 num_attention_heads = 12 num_qa_labels = 9500 num_object_labels = 1600 num_attr_labels = 400 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 512 type_vocab_size = 2 initializer_range = 0.02 layer_norm_eps = 1e-12 l_layers = 9 x_layers = 5 r_layers = 5 visual_feat_dim = 2048 visual_pos_dim = 4 visual_loss_normalizer = 6.67 task_matched = True task_mask_lm = True task_obj_predict = True task_qa = True visual_obj_loss = True visual_attr_loss = True visual_feat_loss = True **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 30522) — LXMERT 模型的词汇表大小。定义了在调用 [LxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertModel) 或 [TFLxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.TFLxmertModel) 时可以由 `inputs_ids` 表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer 编码器中每个注意力层的注意力头数量。

+   `num_qa_labels` (`int`, *optional*, defaults to 9500) — 这表示不同的问题回答（QA）标签的总数。如果使用多个具有 QA 的数据集，用户需要考虑所有数据集总共拥有的标签数量。

+   `num_object_labels` (`int`, *optional*, defaults to 1600) — 这表示 LXMERT 将能够将池化对象特征分类为所属的语义唯一对象的总数。

+   `num_attr_labels` (`int`, *optional*, defaults to 400) — 这表示 LXMERT 将能够将池化对象特征分类为具有的语义唯一属性的总数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer 编码器中“中间”（通常称为前馈）层的维度。

+   `hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持 `"gelu"`, `"relu"`, `"silu"` 和 `"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — 注意力概率的丢失比率。

+   `max_position_embeddings` (`int`, *optional*, defaults to 512) — 该模型可能会与的最大序列长度。通常将其设置为较大的值以防万一（例如，512、1024 或 2048）。

+   `type_vocab_size` (`int`, *optional*, defaults to 2) — 传递给 [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel) 的 *token_type_ids* 的词汇表大小。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的 epsilon。

+   `l_layers` (`int`, *optional*, defaults to 9) — Transformer 语言编码器中的隐藏层数量。

+   `x_layers` (`int`, *optional*, defaults to 5) — Transformer 跨模态编码器中的隐藏层数量。

+   `r_layers` (`int`, *optional*, defaults to 5) — Transformer 视觉编码器中的隐藏层数量。

+   `visual_feat_dim` (`int`, *optional*, defaults to 2048) — 这表示用作模型输入的池化对象特征的最后维度，表示每个对象特征本身的大小。

+   `visual_pos_dim` (`int`, *optional*, defaults to 4) — 这表示混合到视觉特征中的空间特征的数量。默认设置为 4，因为通常这将表示边界框的位置。即 (x, y, 宽度, 高度)

+   `visual_loss_normalizer` (`float`, *optional*, defaults to 6.67) — 这表示如果在预训练期间决定使用多个基于视觉的损失目标进行训练，则每个视觉损失将乘以的缩放因子。

+   `task_matched` (`bool`, *optional*, defaults to `True`) — 该任务用于句子-图像匹配。如果句子正确描述图像，则标签为 1。如果句子未正确描述图像，则标签为 0。

+   `task_mask_lm` (`bool`, *optional*, defaults to `True`) — 是否添加掩码语言建模（如BERT中使用的）到损失目标中。

+   `task_obj_predict` (`bool`, *optional*, defaults to `True`) — 是否添加对象预测、属性预测和特征回归到损失目标中。

+   `task_qa` (`bool`, *optional*, defaults to `True`) — 是否将问答损失添加到目标中。

+   `visual_obj_loss` (`bool`, *optional*, defaults to `True`) — 是否计算对象预测损失目标

+   `visual_attr_loss` (`bool`, *optional*, defaults to `True`) — 是否计算属性预测损失目标

+   `visual_feat_loss` (`bool`, *optional*, defaults to `True`) — 是否计算特征回归损失目标

这是用于存储[LxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertModel)或[TFLxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.TFLxmertModel)配置的配置类。它用于根据指定的参数实例化一个LXMERT模型，定义模型架构。使用默认值实例化配置将产生与Lxmert [unc-nlp/lxmert-base-uncased](https://huggingface.co/unc-nlp/lxmert-base-uncased)架构类似的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

## LxmertTokenizer

### `class transformers.LxmertTokenizer`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert.py#L67)

```py
( vocab_file do_lower_case = True do_basic_tokenize = True never_split = None unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *optional*, defaults to `True`) — 在标记化时是否将输入转换为小写。

+   `do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — 在WordPiece之前是否进行基本标记化。

+   `never_split` (`Iterable`, *optional*) — 在标记化期间永远不会拆分的标记集合。仅在`do_basic_tokenize=True`时有效。

+   `unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。

+   `sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — 分隔符标记，用于从多个序列构建序列，例如用于序列分类的两个序列或用于文本和问题的问题回答。它还用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — 用于进行序列分类（对整个序列进行分类而不是对每个标记进行分类）时使用的分类器标记。它是使用特殊标记构建的序列的第一个标记。

+   `mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — 是否对中文字符进行标记化。

    这可能应该在日语中停用（请参阅此[问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始Lxmert中相同）。

构建一个Lxmert标记器。基于WordPiece。

此标记器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert.py#L200)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对的可选ID列表。

返回值

`List[int]`

带有适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记构建用于序列分类任务的序列或序列对的模型输入。Lxmert序列的格式如下：

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

`convert_tokens_to_string`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert.py#L195)

```py
( tokens )
```

将一系列标记（字符串）转换为单个字符串。

#### `create_token_type_ids_from_sequences`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert.py#L253)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对的可选ID列表。

返回值

`List[int]`

根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。

从传递的两个序列创建用于序列对分类任务的掩码。一个Lxmert序列

序列对掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。

#### `get_special_tokens_mask`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert.py#L225)

```py
( token_ids_0: List token_ids_1: Optional = None already_has_special_tokens: bool = False ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`, *可选*) — 第二个序列对的可选ID列表。

+   `already_has_special_tokens` (`bool`, *可选*, 默认为 `False`) — 标记列表是否已经使用特殊标记格式化。

返回值

`List[int]`

一个整数列表，范围为[0, 1]：1表示特殊标记，0表示序列标记。

从没有添加特殊标记的标记列表中检索序列ID。在使用标记器的`prepare_for_model`方法添加特殊标记时调用此方法。

## LxmertTokenizerFast

### `class transformers.LxmertTokenizerFast`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert_fast.py#L48)

```py
( vocab_file = None tokenizer_file = None do_lower_case = True unk_token = '[UNK]' sep_token = '[SEP]' pad_token = '[PAD]' cls_token = '[CLS]' mask_token = '[MASK]' tokenize_chinese_chars = True strip_accents = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `do_lower_case` (`bool`, *可选*, 默认为 `True`) — 在标记化时是否将输入转换为小写。

+   `unk_token` (`str`, *可选*, 默认为 `"[UNK]"`) — 未知标记。词汇表中没有的标记无法转换为ID，而是设置为此标记。

+   `sep_token` (`str`, *可选*, 默认为 `"[SEP]"`) — 分隔符标记，在构建多个序列的序列时使用，例如用于序列分类的两个序列或用于文本和问题的问题回答。它也用作使用特殊标记构建的序列的最后一个标记。

+   `pad_token` (`str`, *可选*, 默认为 `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `cls_token` (`str`, *可选*, 默认为 `"[CLS]"`) — 用于进行序列分类（对整个序列进行分类而不是每个标记的分类）时使用的分类器标记。当使用特殊标记构建序列时，它是序列的第一个标记。

+   `mask_token` (`str`, *可选*, 默认为 `"[MASK]"`) — 用于屏蔽值的标记。这是在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。

+   `clean_text` (`bool`, *可选*, 默认为 `True`) — 是否在分词之前清理文本，通过删除所有控制字符并将所有空格替换为经典空格。

+   `tokenize_chinese_chars` (`bool`, *可选*, 默认为 `True`) — 是否对中文字符进行分词。这可能应该在日语中停用（参见[此问题](https://github.com/huggingface/transformers/issues/328)）。

+   `strip_accents` (`bool`, *可选*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始Lxmert中一样）。

+   `wordpieces_prefix` (`str`, *可选*, 默认为 `"##"`) — 用于子词的前缀。

构建一个“快速”Lxmert 分词器（由HuggingFace的 *tokenizers* 库支持）。基于 WordPiece。

这个分词器继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。

#### `build_inputs_with_special_tokens`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert_fast.py#L136)

```py
( token_ids_0 token_ids_1 = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — 将添加特殊标记的ID列表。

+   `token_ids_1` (`List[int]`，*可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

带有适当特殊标记的[输入ID](../glossary#input-ids)列表。

通过连接和添加特殊标记，为序列分类任务从序列或序列对构建模型输入。一个 Lxmert 序列的格式如下：

+   单个序列：`[CLS] X [SEP]`

+   序列对：`[CLS] A [SEP] B [SEP]`

#### `create_token_type_ids_from_sequences`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/tokenization_lxmert_fast.py#L160)

```py
( token_ids_0: List token_ids_1: Optional = None ) → export const metadata = 'undefined';List[int]
```

参数

+   `token_ids_0` (`List[int]`) — ID列表。

+   `token_ids_1` (`List[int]`，*可选*) — 序列对的可选第二个ID列表。

返回

`List[int]`

根据给定序列的[标记类型ID](../glossary#token-type-ids)列表。

从传递的两个序列创建一个用于序列对分类任务的掩码。一个 Lxmert 序列

序列掩码的格式如下：

```py
0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |
```

如果`token_ids_1`为`None`，则此方法仅返回掩码的第一部分（0）。

## Lxmert 特定输出

### `class transformers.models.lxmert.modeling_lxmert.LxmertModelOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L59)

```py
( language_output: Optional = None vision_output: Optional = None pooled_output: Optional = None language_hidden_states: Optional = None vision_hidden_states: Optional = None language_attentions: Optional = None vision_attentions: Optional = None cross_encoder_attentions: Optional = None )
```

参数

+   `language_output` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 语言编码器最后一层的隐藏状态序列。

+   `vision_output` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, hidden_size)`) — 视觉编码器最后一层的隐藏状态序列。

+   `pooled_output` (`torch.FloatTensor`，形状为 `(batch_size, hidden_size)`) — 序列第一个标记（分类，CLS，标记）的最后一层隐藏状态，进一步通过线性层和 Tanh 激活函数处理。线性

+   `language_hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为 `(batch_size, sequence_length, hidden_size)` 的`torch.FloatTensor`元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当`output_hidden_states=True`被传递或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当`output_attentions=True`被传递或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当`output_attentions=True`被传递或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当`output_attentions=True`被传递或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

Lxmert的输出包含语言、视觉和跨模态编码器的最后隐藏状态、汇总输出和注意力概率。（注意：在Lxmert中，视觉编码器被称为“关系-ship”编码器）

### `class transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L145)

```py
( loss: Optional = None prediction_logits: Optional = None cross_relationship_score: Optional = None question_answering_score: Optional = None language_hidden_states: Optional = None vision_hidden_states: Optional = None language_attentions: Optional = None vision_attentions: Optional = None cross_encoder_attentions: Optional = None )
```

参数

+   `loss` (*optional*, 当提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`) — 总损失，作为掩码语言建模损失和下一个序列预测（分类）损失的总和。

+   `prediction_logits` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `cross_relationship_score` (`torch.FloatTensor`，形状为`(batch_size, 2)`) — 文本匹配目标（分类）头的预测分数（SoftMax之前的True/False继续分数）。

+   `question_answering_score` (`torch.FloatTensor`，形状为`(batch_size, n_qa_answers)`) — 问答目标（分类）的预测分数。

+   `language_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当`output_hidden_states=True`被传递或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当`output_hidden_states=True`被传递或者`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当`output_attentions=True`被传递或者`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LxmertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertForPreTraining)的输出类型。

### `class transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L105)

```py
( loss: Optional = None question_answering_score: Optional = None language_hidden_states: Optional = None vision_hidden_states: Optional = None language_attentions: Optional = None vision_attentions: Optional = None cross_encoder_attentions: Optional = None )
```

参数

+   `loss`（*可选*，当提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`）— 总损失，作为掩码语言建模损失和下一个序列预测（分类）损失的总和。

+   `question_answering_score`（形状为`(batch_size, n_qa_answers)`的`torch.FloatTensor`，*可选*）— 问题回答目标（分类）的预测分数。

+   `language_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

+   `vision_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）— 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组。

+   `language_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LxmertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering)的输出类型。

### `class transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L60)

```py
( language_output: tf.Tensor | None = None vision_output: tf.Tensor | None = None pooled_output: tf.Tensor | None = None language_hidden_states: Tuple[tf.Tensor] | None = None vision_hidden_states: Tuple[tf.Tensor] | None = None language_attentions: Tuple[tf.Tensor] | None = None vision_attentions: Tuple[tf.Tensor] | None = None cross_encoder_attentions: Tuple[tf.Tensor] | None = None )
```

参数

+   `language_output`（形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`）— 语言编码器最后一层的隐藏状态序列。

+   `vision_output` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) — 视觉编码器最后一层的隐藏状态序列。

+   `pooled_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — 序列第一个标记的最后一层隐藏状态（分类，CLS，标记），经过线性层和Tanh激活函数进一步处理的隐藏状态。

+   `language_hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions` (`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每个层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

Lxmert的输出包含语言、视觉和跨模态编码器的最后隐藏状态、汇聚输出和注意力概率。（注意：在Lxmert中，视觉编码器被称为“关系-ship”编码器）

### `class transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L106)

```py
( loss: tf.Tensor | None = None prediction_logits: tf.Tensor | None = None cross_relationship_score: tf.Tensor | None = None question_answering_score: tf.Tensor | None = None language_hidden_states: Tuple[tf.Tensor] | None = None vision_hidden_states: Tuple[tf.Tensor] | None = None language_attentions: Tuple[tf.Tensor] | None = None vision_attentions: Tuple[tf.Tensor] | None = None cross_encoder_attentions: Tuple[tf.Tensor] | None = None )
```

参数

+   `loss`（*可选*，在提供`labels`时返回，`tf.Tensor` of shape `(1,)`） — 总损失，作为掩码语言建模损失和下一个序列预测（分类）损失的总和。

+   `prediction_logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `cross_relationship_score` (`tf.Tensor` of shape `(batch_size, 2)`) — 文本匹配目标（分类）头的预测分数（SoftMax之前的True/False连续性分数）。

+   `question_answering_score` (`tf.Tensor` of shape `(batch_size, n_qa_answers)`) — 问答目标（分类）的预测分数。

+   `language_hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。在自注意力头中用于计算加权平均值的注意力权重。

+   `vision_attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。在自注意力头中用于计算加权平均值的注意力权重。

+   `cross_encoder_attentions` (`tuple(tf.Tensor)`, *可选的*, 当传递`output_attentions=True`或者当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。在自注意力头中用于计算加权平均值的注意力权重。

[LxmertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertForPreTraining)的输出类型。

PytorchHide Pytorch内容

## LxmertModel

### `class transformers.LxmertModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L879)

```py
( config )
```

参数

+   `config` ([LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸Lxmert模型变换器输出原始隐藏状态，没有特定的头部。

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。这是一个视觉和语言变换器模型，预训练于包括GQA、VQAv2.0、MSCOCO标题和Visual genome在内的各种多模态数据集，使用掩码语言建模、感兴趣区域特征回归、交叉熵损失用于问题回答属性预测和对象标签预测。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L898)

```py
( input_ids: Optional = None visual_feats: Optional = None visual_pos: Optional = None attention_mask: Optional = None visual_attention_mask: Optional = None token_type_ids: Optional = None inputs_embeds: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.lxmert.modeling_lxmert.LxmertModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。查看[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)获取详细信息。

    [什么是输入ID？](../glossary#input-ids)

+   `visual_feats` (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_feat_dim)`) — 此输入表示视觉特征。它们是使用 faster-RCNN 模型从边界框中 ROI 池化的对象特征）

    目前 transformers 库中没有提供这些。

+   `visual_pos` (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_pos_dim)`) — 此输入表示与它们的相对（通过索引）视觉特征对应的空间特征。预训练的 LXMERT 模型期望这些空间特征是在 0 到

    目前 transformers 库中没有提供这些。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被屏蔽的标记，

    +   0 表示被屏蔽的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充标记索引上执行注意力的掩码。掩码值在 `[0, 1]` 中选择：

    +   1 表示未被屏蔽的标记，

    +   0 表示被屏蔽的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，这将很有用，而不是使用模型的内部嵌入查找矩阵。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回一个 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是一个普通的元组。

返回

[transformers.models.lxmert.modeling_lxmert.LxmertModelOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertModelOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.models.lxmert.modeling_lxmert.LxmertModelOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertModelOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含根据配置（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)）和输入而异的各种元素。

+   `language_output` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 语言编码器最后一层的隐藏状态序列。

+   `vision_output` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 视觉编码器最后一层的隐藏状态序列。

+   `pooled_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) — 序列第一个标记（分类，CLS，标记）的最后一层隐藏状态，进一步由线性层和 Tanh 激活函数处理。线性

+   `language_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征，一个用于每个交叉模态层的输出）。

+   `vision_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征，一个用于每个交叉模态层的输出）。

+   `language_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。在自注意力头中用于计算加权平均值的注意力权重softmax后。

+   `vision_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。在自注意力头中用于计算加权平均值的注意力权重softmax后。

+   `cross_encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。在自注意力头中用于计算加权平均值的注意力权重softmax后。

[LxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, LxmertModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
>>> model = LxmertModel.from_pretrained("unc-nlp/lxmert-base-uncased")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## LxmertForPreTraining

### `class transformers.LxmertForPreTraining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L1017)

```py
( config )
```

参数

+   `config` ([LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

具有指定预训练头的Lxmert模型。

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。它是一个视觉和语言变换器模型，预训练于包括GQA、VQAv2.0、MSCOCO标题和Visual genome在内的各种多模态数据集，使用掩码语言建模、感兴趣区域特征回归、交叉熵损失用于问题回答属性预测和对象标签预测的组合。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

此模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L1150)

```py
( input_ids: Optional = None visual_feats: Optional = None visual_pos: Optional = None attention_mask: Optional = None visual_attention_mask: Optional = None token_type_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None obj_labels: Optional = None matched_label: Optional = None ans: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None **kwargs ) → export const metadata = 'undefined';transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参阅 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode) 和 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入 ID？](../glossary#input-ids)

+   `visual_feats` (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_feat_dim)`) — 这个输入表示视觉特征。它们是通过 faster-RCNN 模型从边界框中ROI池化的对象特征。

    目前这些由 transformers 库不提供。

+   `visual_pos` (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_pos_dim)`) — 这个输入表示与它们的相对（通过索引）视觉特征对应的空间特征。预训练的 LXMERT 模型期望这些空间特征是在 0 到 1 的范围内归一化的边界框。

    目前这些由 transformers 库不提供。

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`：

    +   1 代表未被 `masked` 的标记，

    +   0 代表被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选择在 `[0, 1]`：

    +   1 代表未被 `masked` 的标记，

    +   0 代表被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *可选*) — 段标记索引，用于指示输入的第一部分和第二部分。索引选择在 `[0, 1]`：

    +   0 对应于 *句子 A* 标记，

    +   1 对应于 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将很有用。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通的元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *可选*) — 用于计算掩码语言建模损失的标签。索引应在 `[-100, 0, ..., config.vocab_size]`（参见 `input_ids` 文档字符串）。索引设置为 `-100` 的标记将被忽略（被 `masked`），损失仅计算具有标签在 `[0, ..., config.vocab_size]` 中的标记。

+   `obj_labels` (`Dict[Str -- Tuple[Torch.FloatTensor, Torch.FloatTensor]]`, *可选*): 每个键以视觉损失中的每一个命名，元组的每个元素的形状分别为 `(batch_size, num_features)` 和 `(batch_size, num_features, visual_feature_dim)`，用于标签 ID 和标签分数

+   `matched_label`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*） - 用于计算文本输入是否与图像匹配（分类）损失的标签。输入应为一个序列对（参见`input_ids`文档字符串）。索引应在`[0, 1]`中：

    +   0表示句子与图像不匹配，

    +   1表示句子与图像匹配。

+   `ans`（形状为`(batch_size)`的`Torch.Tensor`，*可选*） - 正确答案的独热表示*可选*

返回

[transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput)或`tuple(torch.FloatTensor)`

一个[transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包括根据配置（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)）和输入的不同元素。

+   `loss`（*可选*，当提供`labels`时返回，形状为`(1,)`的`torch.FloatTensor`） - 作为掩码语言建模损失和下一个序列预测（分类）损失之和的总损失。

+   `prediction_logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`torch.FloatTensor`） - 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `cross_relationship_score`（形状为`(batch_size, 2)`的`torch.FloatTensor`） - 文本匹配目标（分类）头的预测分数（SoftMax之前的True/False继续分数）。

+   `question_answering_score`（形状为`(batch_size, n_qa_answers)`的`torch.FloatTensor`） - 问题回答目标（分类）的预测分数。

+   `language_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征+一个用于每个跨模态层的输出）。

+   `vision_hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） - 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于输入特征+一个用于每个跨模态层的输出）。

+   `language_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LxmertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertForPreTraining)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

## LxmertForQuestionAnswering

### `class transformers.LxmertForQuestionAnswering`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L1285)

```py
( config )
```

参数

+   `config`（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig））—模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有视觉回答头的Lxmert模型，用于下游QA任务

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。这是一个视觉和语言变换器模型，预训练于各种多模态数据集，包括GQA、VQAv2.0、MSCOCO字幕和Visual genome，使用掩码语言建模、感兴趣区域特征回归、交叉熵损失用于问题回答属性预测和对象标签预测的组合。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档，了解库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_lxmert.py#L1381)

```py
( input_ids: Optional = None visual_feats: Optional = None visual_pos: Optional = None attention_mask: Optional = None visual_attention_mask: Optional = None token_type_ids: Optional = None inputs_embeds: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput or tuple(torch.FloatTensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）—输入序列标记在词汇表中的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。

    [什么是输入ID？](../glossary#input-ids)

+   `visual_feats`（形状为`(batch_size, num_visual_features, visual_feat_dim)`的`torch.FloatTensor`）—这个输入表示视觉特征。它们是使用faster-RCNN模型从边界框中ROI池化的对象特征）

    这些目前不是由transformers库提供的。

+   `visual_pos`（形状为`(batch_size, num_visual_features, visual_pos_dim)`的`torch.FloatTensor`）—这个输入表示与它们的相对（通过索引）视觉特征对应的空间特征。预训练的LXMERT模型期望这些空间特征是在0到1的范围内的归一化边界框

    这些目前不是由transformers库提供的。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1表示“未屏蔽”的标记，

    +   0表示“屏蔽”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `visual_attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—避免在填充标记索引上执行注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   1表示“未屏蔽”的标记，

    +   0 对于被 `masked` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 段落标记索引，用于指示输入的第一部分和第二部分。索引在 `[0, 1]` 中选择：

    +   0 对应 *句子 A* 标记，

    +   1 对应 *句子 B* 标记。

    [什么是标记类型 ID？](../glossary#token-type-ids)

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，可以直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，这将非常有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`Torch.Tensor` of shape `(batch_size)`, *optional*) — 正确答案的独热表示

返回

[transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)）和输入的不同元素。

+   `loss` (*optional*, 当提供 `labels` 时返回，形状为 `(1,)` 的 `torch.FloatTensor`) — 作为掩码语言建模损失和下一个序列预测（分类）损失之和的总损失。

+   `question_answering_score` (`torch.FloatTensor` of shape `(batch_size, n_qa_answers)`, *optional*) — 问题回答目标（分类）的预测分数。

+   `language_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于输入特征 + 一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） - 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[LxmertForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

例子：

```py
>>> from transformers import AutoTokenizer, LxmertForQuestionAnswering
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
>>> model = LxmertForQuestionAnswering.from_pretrained("unc-nlp/lxmert-base-uncased")

>>> question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

>>> inputs = tokenizer(question, text, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> answer_start_index = outputs.start_logits.argmax()
>>> answer_end_index = outputs.end_logits.argmax()

>>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]

>>> # target is "nice puppet"
>>> target_start_index = torch.tensor([14])
>>> target_end_index = torch.tensor([15])

>>> outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
>>> loss = outputs.loss
```

隐藏TensorFlow内容

## TFLxmertModel

### `class transformers.TFLxmertModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L1093)

```py
( config *inputs **kwargs )
```

参数

+   `config`（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)） - 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的Lxmert模型变换器，输出原始隐藏状态而没有特定的头部。

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。这是一个视觉和语言变换器模型，预训练于各种多模态数据集，包括GQA、VQAv2.0、MCSCOCO字幕和Visual genome，使用掩码语言建模、感兴趣区域特征回归、交叉熵损失用于问题回答属性预测和对象标签预测。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或者

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，当将输入传递给模型和层时，Keras方法更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以正常工作 - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外使用第二种格式，比如在使用Keras`Functional`API创建自己的层或模型时，有三种可能性可以用来收集所有输入张量放在第一个位置参数中：

+   一个只包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   一个长度不定的列表，其中包含一个或多个按照文档字符串中给定顺序的输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个与文档字符串中给定的输入名称相关联的包含一个或多个输入张量的字典：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L1102)

```py
( input_ids: TFModelInputType | None = None visual_feats: tf.Tensor | None = None visual_pos: tf.Tensor | None = None attention_mask: np.ndarray | tf.Tensor | None = None visual_attention_mask: np.ndarray | tf.Tensor | None = None token_type_ids: np.ndarray | tf.Tensor | None = None inputs_embeds: np.ndarray | tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput or tuple(tf.Tensor)
```

参数

+   `input_ids` (`np.ndarray` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`) — 输入序列标记在词汇表中的索引。

    可以使用 [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer) 获取索引。有关详细信息，请参见 [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__) 和 [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    什么是输入ID？

+   `visual_feats` (`tf.Tensor`，形状为 `(batch_size, num_visual_features, visual_feat_dim)`) — 此输入表示视觉特征。它们是使用 faster-RCNN 模型从边界框中 ROI 池化的对象特征。

    这些目前不由 transformers 库提供。

+   `visual_pos` (`tf.Tensor`，形状为 `(batch_size, num_visual_features, visual_feat_dim)`) — 此输入表示与它们的相对（通过索引）视觉特征对应的空间特征。预训练的 LXMERT 模型期望这些空间特征是在 0 到 之间的归一化边界框

    这些目前不由 transformers 库提供。

+   `attention_mask` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 用于未被掩码的标记，

    +   0 用于被掩码的标记。

    什么是注意力掩码？

+   `visual_attention_mask` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 避免在填充标记索引上执行注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 用于未被掩码的标记，

    +   0 用于被掩码的标记。

    什么是注意力掩码？

+   `token_type_ids` (`tf.Tensor`，形状为 `(batch_size, sequence_length)`，*可选*) — 段标记索引，指示输入的第一部分和第二部分。索引选在 `[0, 1]`：

    +   0 对应于 *句子 A* 的标记，

    +   1 对应于 *句子 B* 的标记。

    什么是标记类型ID？

+   `inputs_embeds` (`tf.Tensor`，形状为 `(batch_size, sequence_length, hidden_size)`，*可选*) — 可选地，您可以选择直接传递嵌入表示，而不是传递 `input_ids`。如果您想要更多控制如何将 `input_ids` 索引转换为相关向量，而不是模型内部的嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的 `attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的 `hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`，*可选*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。此参数可以在急切模式下使用，在图模式下该值将始终设置为 True。

+   `training` (`bool`，*可选*，默认为 `False`) — 是否在训练模式下使用模型（一些模块，如 dropout 模块，在训练和评估之间有不同的行为）。

返回

transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput 或 `tuple(tf.Tensor)`

一个[transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)）和输入的不同元素。

+   `language_output` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 语言编码器最后一层的隐藏状态序列。

+   `vision_output` (`tf.Tensor`，形状为`(batch_size, sequence_length, hidden_size)`) — 视觉编码器最后一层的隐藏状态序列。

+   `pooled_output` (`tf.Tensor`，形状为`(batch_size, hidden_size)`) — 序列第一个标记的最后一层隐藏状态（分类，CLS，标记），经过线性层和Tanh激活函数进一步处理。线性

+   `language_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `vision_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

+   `cross_encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[TFLxmertModel](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.TFLxmertModel)的前向方法覆盖了`__call__`特殊方法。

尽管前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoTokenizer, TFLxmertModel
>>> import tensorflow as tf

>>> tokenizer = AutoTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
>>> model = TFLxmertModel.from_pretrained("unc-nlp/lxmert-base-uncased")

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
>>> outputs = model(inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## TFLxmertForPreTraining

### `class transformers.TFLxmertForPreTraining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L1400)

```py
( config *inputs **kwargs )
```

参数

+   `config` ([LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

在顶部带有`语言建模`头的Lxmert模型。

LXMERT模型是由Hao Tan和Mohit Bansal在[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)中提出的。这是一个视觉和语言变换器模型，使用遮蔽语言建模、感兴趣区域特征回归、交叉熵损失用于问题回答属性预测和对象标签预测，预先在各种多模态数据集上进行了预训练，包括GQA、VQAv2.0、MCSCOCO标题和Visual genome。

此模型还是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有相关信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是，Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递您的输入和标签！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可用于在第一个位置参数中收集所有输入张量：

+   一个仅包含`input_ids`的单个张量，没有其他内容：`model(input_ids)`

+   按照文档字符串中给定的顺序，一个长度可变的列表，包含一个或多个输入张量：`model([input_ids, attention_mask])`或`model([input_ids, attention_mask, token_type_ids])`

+   一个包含一个或多个与文档字符串中给定的输入名称相关联的输入张量的字典：`model({"input_ids": input_ids, "token_type_ids": token_type_ids})`

请注意，在使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像对待任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/lxmert/modeling_tf_lxmert.py#L1501)

```py
( input_ids: TFModelInputType | None = None visual_feats: tf.Tensor | None = None visual_pos: tf.Tensor | None = None attention_mask: tf.Tensor | None = None visual_attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None masked_lm_labels: tf.Tensor | None = None obj_labels: Dict[str, Tuple[tf.Tensor, tf.Tensor]] | None = None matched_label: tf.Tensor | None = None ans: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None training: bool = False ) → export const metadata = 'undefined';transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput or tuple(tf.Tensor)
```

参数

+   `input_ids`（形状为`(batch_size, sequence_length)`的`np.ndarray`或`tf.Tensor`）- 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是input IDs？](../glossary#input-ids)

+   `visual_feats`（形状为`(batch_size, num_visual_features, visual_feat_dim)`的`tf.Tensor`）- 此输入表示视觉特征。它们是使用faster-RCNN模型从边界框中ROI池化的对象特征。

    目前transformers库中没有提供这些。

+   `visual_pos`（形状为`(batch_size, num_visual_features, visual_feat_dim)`的`tf.Tensor`）- 此输入表示与它们的相对（通过索引）视觉特征对应的空间特征。预训练的LXMERT模型期望这些空间特征是在0到1的范围内的归一化边界框。

    目前transformers库中没有提供这些。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`tf.Tensor`，*可选*）- 用于避免在填充标记索引上执行注意力的掩码。选择在`[0, 1]`中的掩码值：

    +   对于未被“masked”（掩盖）的标记，值为1，

    +   对于被`masked`（掩盖）的标记，值为0。

    [什么是attention masks？](../glossary#attention-mask)

+   `visual_attention_mask` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 避免在填充令牌索引上执行注意力的MMask。在`[0, 1]`中选择的掩码值：

    +   1表示未被`masked`的令牌，

    +   0表示被`masked`的令牌。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 段令牌索引，指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0对应于*句子A*令牌，

    +   1对应于*句子B*令牌。

    [什么是令牌类型ID？](../glossary#token-type-ids)

+   `inputs_embeds` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *optional*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training` (`bool`, *optional*, 默认为`False`) — 是否在训练模式下使用模型（某些模块如丢弃模块在训练和评估之间有不同的行为）。

+   `masked_lm_labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`内（请参阅`input_ids`文档字符串）。索引设置为`-100`的令牌将被忽略（掩码），仅对具有标签在`[0, ..., config.vocab_size]`中的令牌计算损失

+   `obj_labels` (`Dict[Str -- Tuple[tf.Tensor, tf.Tensor]]`, *optional*, 默认为`None`): 每个键都以视觉损失中的每个元素命名，元组的每个元素的形状分别为`(batch_size, num_features)`和`(batch_size, num_features, visual_feature_dim)`，用于标签id和标签分数

+   `matched_label` (`tf.Tensor` of shape `(batch_size,)`, *optional*) — 用于计算文本输入是否与图像（分类）损失匹配的标签。输入应为一个序列对（请参阅`input_ids`文档字符串）索引应在`[0, 1]`内：

    +   0表示句子与图像不匹配，

    +   1表示句子与图像匹配。

+   `ans` (`tf.Tensor` of shape `(batch_size)`, *optional*, 默认为`None`) — 正确答案的独热表示*可选*

返回

[transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput)或`tuple(tf.Tensor)`

一个[transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput)或一个`tf.Tensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`时）包括根据配置（[LxmertConfig](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.LxmertConfig)）和输入的不同元素。

+   `loss` (*optional*, 当提供`labels`时返回，形状为`(1,)`) — 总损失，作为掩码语言建模损失和下一个序列预测（分类）损失的总和。

+   `prediction_logits` (`tf.Tensor`，形状为`(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `cross_relationship_score` (`tf.Tensor`，形状为`(batch_size, 2)`) — 文本匹配目标（分类）头的预测分数（SoftMax之前的True/False连续性分数）。

+   `question_answering_score` (`tf.Tensor`，形状为`(batch_size, n_qa_answers)`) — 问答目标（分类）的预测分数。

+   `language_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `vision_hidden_states` (`tuple(tf.Tensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于输入特征，一个用于每个跨模态层的输出）。

+   `language_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在自注意力头中用于计算加权平均的注意力权重softmax之后。

+   `vision_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在自注意力头中用于计算加权平均的注意力权重softmax之后。

+   `cross_encoder_attentions` (`tuple(tf.Tensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组。在自注意力头中用于计算加权平均的注意力权重softmax之后。

[TFLxmertForPreTraining](/docs/transformers/v4.37.2/en/model_doc/lxmert#transformers.TFLxmertForPreTraining)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
