# BERTology

> 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/bertology](https://huggingface.co/docs/transformers/v4.37.2/en/bertology)

有一个不断增长的研究领域，关注调查大规模变压器（如BERT）的内部工作（有些人称之为“BERTology”）。 这个领域的一些很好的例子是：

+   BERT重新发现了经典的NLP流程 作者：Ian Tenney，Dipanjan Das，Ellie Pavlick：[https://arxiv.org/abs/1905.05950](https://arxiv.org/abs/1905.05950)

+   十六个头真的比一个好吗？ 作者：Paul Michel，Omer Levy，Graham Neubig：[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)

+   BERT看什么？ 作者：Kevin Clark，Urvashi Khandelwal，Omer Levy，Christopher D. Manning：[https://arxiv.org/abs/1906.04341](https://arxiv.org/abs/1906.04341)

+   CAT探测：一种基于度量的方法，解释预训练模型如何关注代码结构：[https://arxiv.org/abs/2210.04633](https://arxiv.org/abs/2210.04633)

为了帮助这个新领域发展，我们在BERT/GPT/GPT-2模型中添加了一些额外功能，以帮助人们访问内部表示，主要是从Paul Michel的伟大工作中改编的（[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)）：

+   访问BERT/GPT/GPT-2的所有隐藏状态，

+   访问BERT/GPT/GPT-2每个头的所有注意权重，

+   检索头输出值和梯度，以便计算头重要性分数并修剪头，如[https://arxiv.org/abs/1905.10650](https://arxiv.org/abs/1905.10650)中所解释的。

为了帮助您理解和使用这些功能，我们添加了一个特定的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，同时提取在GLUE上预训练的模型的信息并修剪。
