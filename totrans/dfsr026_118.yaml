- en: AutoencoderKL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/autoencoderkl](https://huggingface.co/docs/diffusers/api/models/autoencoderkl)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The variational autoencoder (VAE) model with KL loss was introduced in [Auto-Encoding
    Variational Bayes](https://arxiv.org/abs/1312.6114v11) by Diederik P. Kingma and
    Max Welling. The model is used in ðŸ¤— Diffusers to encode images into latents and
    to decode latent representations into images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How can we perform efficient inference and learning in directed probabilistic
    models, in the presence of continuous latent variables with intractable posterior
    distributions, and large datasets? We introduce a stochastic variational inference
    and learning algorithm that scales to large datasets and, under some mild differentiability
    conditions, even works in the intractable case. Our contributions are two-fold.
    First, we show that a reparameterization of the variational lower bound yields
    a lower bound estimator that can be straightforwardly optimized using standard
    stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
    latent variables per datapoint, posterior inference can be made especially efficient
    by fitting an approximate inference model (also called a recognition model) to
    the intractable posterior using the proposed lower bound estimator. Theoretical
    advantages are reflected in experimental results.*'
  prefs: []
  type: TYPE_NORMAL
- en: Loading from the original format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default the [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)
    should be loaded with [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained),
    but it can also be loaded from the original format using `FromOriginalVAEMixin.from_single_file`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: AutoencoderKL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.AutoencoderKL`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`in_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    â€” Tuple of downsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    â€” Tuple of upsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) â€” Tuple
    of block output channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) â€” The activation function
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latent_channels` (`int`, *optional*, defaults to 4) â€” Number of channels in
    the latent space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_size` (`int`, *optional*, defaults to `32`) â€” Sample input size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaling_factor` (`float`, *optional*, defaults to 0.18215) â€” The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_upcast` (`bool`, *optional*, default to `True`) â€” If enabled it will
    force the VAE to run in float32 for high image resolution pipelines, such as SD-XL.
    VAE can be fine-tuned / trained to a lower range without loosing too much precision
    in which case `force_upcast` can be set to `False` - see: [https://huggingface.co/madebyollin/sdxl-vae-fp16-fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A VAE model with KL loss for encoding images into latents and decoding latent
    representations into images.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for itâ€™s generic methods implemented for all
    models (such as downloading or saving).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `wrapper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `wrapper`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### `disable_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L152)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `disable_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L138)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_slicing`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L145)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `enable_tiling`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L130)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L423)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sample` (`torch.FloatTensor`) â€” Input sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_posterior` (`bool`, *optional*, defaults to `False`) â€” Whether to sample
    from the posterior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `DecoderOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '#### `fuse_qkv_projections`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L452)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Enables fused QKV projections. For self-attention modules, all projection matrices
    (i.e., query, key, value) are fused. For cross-attention modules, key and value
    projection matrices are fused.
  prefs: []
  type: TYPE_NORMAL
- en: This API is ðŸ§ª experimental.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L185)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) â€”
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sets the attention processor to use to compute attention.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `set_default_attn_processor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L220)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Disables custom attention processors and sets the default attention implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tiled_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L375)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`z` (`torch.FloatTensor`) â€” Input batch of latent vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `~models.vae.DecoderOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`~models.vae.DecoderOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is True, a `~models.vae.DecoderOutput` is returned, otherwise
    a plain `tuple` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Decode a batch of images using a tiled decoder.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `tiled_encode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L321)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`x` (`torch.FloatTensor`) â€” Input batch of images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `~models.autoencoder_kl.AutoencoderKLOutput` instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`~models.autoencoder_kl.AutoencoderKLOutput` or `tuple`'
  prefs: []
  type: TYPE_NORMAL
- en: If return_dict is True, a `~models.autoencoder_kl.AutoencoderKLOutput` is returned,
    otherwise a plain `tuple` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: Encode a batch of images using a tiled encoder.
  prefs: []
  type: TYPE_NORMAL
- en: When this option is enabled, the VAE will split the input tensor into tiles
    to compute encoding in several steps. This is useful to keep memory use constant
    regardless of image size. The end result of tiled encoding is different from non-tiled
    encoding because each tile uses a different encoder. To avoid tiling artifacts,
    the tiles overlap and are blended together to form a smooth output. You may still
    see tile-sized changes in the output, but they should be much less noticeable.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `unfuse_qkv_projections`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L476)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Disables the fused QKV projection if enabled.
  prefs: []
  type: TYPE_NORMAL
- en: This API is ðŸ§ª experimental.
  prefs: []
  type: TYPE_NORMAL
- en: AutoencoderKLOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`latent_dist` (`DiagonalGaussianDistribution`) â€” Encoded outputs of `Encoder`
    represented as the mean and logvar of `DiagonalGaussianDistribution`. `DiagonalGaussianDistribution`
    allows for sampling latents from the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of AutoencoderKL encoding method.
  prefs: []
  type: TYPE_NORMAL
- en: DecoderOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.autoencoders.vae.DecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sample` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” The decoded output sample from the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of decoding method.
  prefs: []
  type: TYPE_NORMAL
- en: FlaxAutoencoderKL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.FlaxAutoencoderKL`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L726)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`in_channels` (`int`, *optional*, defaults to 3) â€” Number of channels in the
    input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`out_channels` (`int`, *optional*, defaults to 3) â€” Number of channels in the
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `(DownEncoderBlock2D)`)
    â€” Tuple of downsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `(UpDecoderBlock2D)`)
    â€” Tuple of upsample block types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`block_out_channels` (`Tuple[str]`, *optional*, defaults to `(64,)`) â€” Tuple
    of block output channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layers_per_block` (`int`, *optional*, defaults to `2`) â€” Number of ResNet
    layer for each block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`act_fn` (`str`, *optional*, defaults to `silu`) â€” The activation function
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`latent_channels` (`int`, *optional*, defaults to `4`) â€” Number of channels
    in the latent space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm_num_groups` (`int`, *optional*, defaults to `32`) â€” The number of groups
    for normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_size` (`int`, *optional*, defaults to 32) â€” Sample input size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaling_factor` (`float`, *optional*, defaults to 0.18215) â€” The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jnp.dtype`, *optional*, defaults to `jnp.float32`) â€” The `dtype`
    of the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flax implementation of a VAE model with KL loss for decoding latent representations.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [FlaxModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.FlaxModelMixin).
    Check the superclass documentation for itâ€™s generic methods implemented for all
    models (such as downloading or saving).
  prefs: []
  type: TYPE_NORMAL
- en: This model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)
    subclass. Use it as a regular Flax Linen module and refer to the Flax documentation
    for all matter related to its general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inherent JAX features such as the following are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FlaxAutoencoderKLOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.vae_flax.FlaxAutoencoderKLOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L47)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`latent_dist` (`FlaxDiagonalGaussianDistribution`) â€” Encoded outputs of `Encoder`
    represented as the mean and logvar of `FlaxDiagonalGaussianDistribution`. `FlaxDiagonalGaussianDistribution`
    allows for sampling latents from the distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of AutoencoderKL encoding method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: â€œReturns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
- en: FlaxDecoderOutput
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class diffusers.models.vae_flax.FlaxDecoderOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L32)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`sample` (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`)
    â€” The decoded output sample from the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype` (`jnp.dtype`, *optional*, defaults to `jnp.float32`) â€” The `dtype`
    of the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of decoding method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `replace`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: â€œReturns a new object replacing the specified fields with new values.
  prefs: []
  type: TYPE_NORMAL
