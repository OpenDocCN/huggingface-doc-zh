- en: AutoencoderKL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoencoderKL
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/autoencoderkl](https://huggingface.co/docs/diffusers/api/models/autoencoderkl)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/diffusers/api/models/autoencoderkl](https://huggingface.co/docs/diffusers/api/models/autoencoderkl)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The variational autoencoder (VAE) model with KL loss was introduced in [Auto-Encoding
    Variational Bayes](https://arxiv.org/abs/1312.6114v11) by Diederik P. Kingma and
    Max Welling. The model is used in ğŸ¤— Diffusers to encode images into latents and
    to decode latent representations into images.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ä¸KLæŸå¤±åœ¨[Diederik P. Kingmaå’ŒMax Wellingçš„ã€Šè‡ªåŠ¨ç¼–ç å˜åˆ†è´å¶æ–¯ã€‹](https://arxiv.org/abs/1312.6114v11)ä¸­å¼•å…¥ã€‚è¯¥æ¨¡å‹ç”¨äºå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨å˜é‡ï¼Œå¹¶å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒã€‚
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š
- en: '*How can we perform efficient inference and learning in directed probabilistic
    models, in the presence of continuous latent variables with intractable posterior
    distributions, and large datasets? We introduce a stochastic variational inference
    and learning algorithm that scales to large datasets and, under some mild differentiability
    conditions, even works in the intractable case. Our contributions are two-fold.
    First, we show that a reparameterization of the variational lower bound yields
    a lower bound estimator that can be straightforwardly optimized using standard
    stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous
    latent variables per datapoint, posterior inference can be made especially efficient
    by fitting an approximate inference model (also called a recognition model) to
    the intractable posterior using the proposed lower bound estimator. Theoretical
    advantages are reflected in experimental results.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*æˆ‘ä»¬å¦‚ä½•åœ¨è¿ç»­æ½œåœ¨å˜é‡å…·æœ‰éš¾ä»¥å¤„ç†çš„åéªŒåˆ†å¸ƒå’Œå¤§å‹æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œåœ¨æœ‰å‘æ¦‚ç‡æ¨¡å‹ä¸­æ‰§è¡Œé«˜æ•ˆçš„æ¨æ–­å’Œå­¦ä¹ ï¼Ÿæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éšæœºå˜åˆ†æ¨æ–­å’Œå­¦ä¹ ç®—æ³•ï¼Œå¯æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨ä¸€äº›æ¸©å’Œçš„å¯å¾®æ¡ä»¶ä¸‹ï¼Œç”šè‡³åœ¨éš¾ä»¥å¤„ç†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œã€‚æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¡¨æ˜å˜åˆ†ä¸‹ç•Œçš„é‡æ–°å‚æ•°åŒ–äº§ç”Ÿäº†ä¸€ä¸ªä¸‹ç•Œä¼°è®¡å™¨ï¼Œå¯ä»¥é€šè¿‡æ ‡å‡†éšæœºæ¢¯åº¦æ–¹æ³•ç›´æ¥ä¼˜åŒ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¡¨æ˜å¯¹äºå…·æœ‰è¿ç»­æ½œåœ¨å˜é‡çš„i.i.d.æ•°æ®é›†ï¼Œé€šè¿‡ä½¿ç”¨æå‡ºçš„ä¸‹ç•Œä¼°è®¡å™¨å°†è¿‘ä¼¼æ¨æ–­æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºè¯†åˆ«æ¨¡å‹ï¼‰æ‹Ÿåˆåˆ°éš¾ä»¥å¤„ç†çš„åéªŒï¼Œå¯ä»¥ä½¿åéªŒæ¨æ–­ç‰¹åˆ«é«˜æ•ˆã€‚ç†è®ºä¼˜åŠ¿åœ¨å®éªŒç»“æœä¸­å¾—åˆ°ä½“ç°ã€‚*'
- en: Loading from the original format
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»åŸå§‹æ ¼å¼åŠ è½½
- en: 'By default the [AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)
    should be loaded with [from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained),
    but it can also be loaded from the original format using `FromOriginalVAEMixin.from_single_file`
    as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œåº”ä½¿ç”¨[from_pretrained()](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)åŠ è½½[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin.from_pretrained)ï¼Œä½†ä¹Ÿå¯ä»¥ä½¿ç”¨`FromOriginalVAEMixin.from_single_file`ä»åŸå§‹æ ¼å¼åŠ è½½ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: AutoencoderKL
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoencoderKL
- en: '### `class diffusers.AutoencoderKL`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.AutoencoderKL`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L35)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L35)'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Parameters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`in_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    input image.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`ï¼ˆintï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰â€” è¾“å…¥å›¾åƒä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_channels` (int, *optional*, defaults to 3) â€” Number of channels in the
    output.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`ï¼ˆintï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰â€” è¾“å‡ºä¸­çš„é€šé“æ•°ã€‚'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `("DownEncoderBlock2D",)`)
    â€” Tuple of downsample block types.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`("DownEncoderBlock2D",)`ï¼‰â€” ä¸‹é‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `("UpDecoderBlock2D",)`)
    â€” Tuple of upsample block types.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`("UpDecoderBlock2D",)`ï¼‰â€” ä¸Šé‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`block_out_channels` (`Tuple[int]`, *optional*, defaults to `(64,)`) â€” Tuple
    of block output channels.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels`ï¼ˆ`Tuple[int]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(64,)`ï¼‰â€” å—è¾“å‡ºé€šé“çš„å…ƒç»„ã€‚'
- en: '`act_fn` (`str`, *optional*, defaults to `"silu"`) â€” The activation function
    to use.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"silu"`ï¼‰â€” è¦ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚'
- en: '`latent_channels` (`int`, *optional*, defaults to 4) â€” Number of channels in
    the latent space.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º4ï¼‰â€” æ½œåœ¨ç©ºé—´ä¸­çš„é€šé“æ•°ã€‚'
- en: '`sample_size` (`int`, *optional*, defaults to `32`) â€” Sample input size.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`32`ï¼‰â€” æ ·æœ¬è¾“å…¥å¤§å°ã€‚'
- en: '`scaling_factor` (`float`, *optional*, defaults to 0.18215) â€” The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.18215ï¼‰â€” ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€æ‰¹è®¡ç®—å‡ºçš„è®­ç»ƒæ½œåœ¨ç©ºé—´çš„åˆ†é‡æ ‡å‡†å·®ã€‚åœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶ï¼Œç”¨äºå°†æ½œåœ¨ç©ºé—´ç¼©æ”¾ä¸ºå•ä½æ–¹å·®ã€‚åœ¨ä¼ é€’ç»™æ‰©æ•£æ¨¡å‹ä¹‹å‰ï¼Œæ½œåœ¨å˜é‡ä½¿ç”¨å…¬å¼`z
    = z * scaling_factor`è¿›è¡Œç¼©æ”¾ã€‚è§£ç æ—¶ï¼Œæ½œåœ¨å˜é‡ä½¿ç”¨å…¬å¼è¿›è¡Œç¼©æ”¾å›åŸå§‹æ¯”ä¾‹ï¼š`z = 1 / scaling_factor * z`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å…·æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](https://arxiv.org/abs/2112.10752)è®ºæ–‡çš„4.3.2å’ŒD.1èŠ‚ã€‚'
- en: '`force_upcast` (`bool`, *optional*, default to `True`) â€” If enabled it will
    force the VAE to run in float32 for high image resolution pipelines, such as SD-XL.
    VAE can be fine-tuned / trained to a lower range without loosing too much precision
    in which case `force_upcast` can be set to `False` - see: [https://huggingface.co/madebyollin/sdxl-vae-fp16-fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`force_upcast`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” å¦‚æœå¯ç”¨ï¼Œå®ƒå°†å¼ºåˆ¶VAEåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç®¡é“ï¼ˆå¦‚SD-XLï¼‰ä¸­ä»¥float32è¿è¡Œã€‚
    VAEå¯ä»¥å¾®è°ƒ/è®­ç»ƒåˆ°è¾ƒä½èŒƒå›´è€Œä¸ä¼šå¤±å»å¤ªå¤šç²¾åº¦ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`force_upcast`å¯ä»¥è®¾ç½®ä¸º`False` - å‚è§ï¼š[https://huggingface.co/madebyollin/sdxl-vae-fp16-fix](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)'
- en: A VAE model with KL loss for encoding images into latents and decoding latent
    representations into images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰KLæŸå¤±çš„VAEæ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨å˜é‡ï¼Œå¹¶å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒã€‚
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for itâ€™s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚
- en: '#### `wrapper`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `wrapper`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `wrapper`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `wrapper`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/utils/accelerate_utils.py#L43)'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#### `disable_slicing`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L152)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L152)'
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_slicing`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚
- en: '#### `disable_tiling`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L138)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L138)'
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this
    method will go back to computing decoding in one step.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨å¹³é“ºVAEè§£ç ã€‚å¦‚æœä¹‹å‰å¯ç”¨äº†`enable_tiling`ï¼Œåˆ™æ­¤æ–¹æ³•å°†è¿”å›åˆ°ä¸€æ­¥è®¡ç®—è§£ç ã€‚
- en: '#### `enable_slicing`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_slicing`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L145)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L145)'
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Enable sliced VAE decoding. When this option is enabled, the VAE will split
    the input tensor in slices to compute decoding in several steps. This is useful
    to save some memory and allow larger batch sizes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨åˆ‡ç‰‡VAEè§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆåˆ‡ç‰‡ï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç ã€‚è¿™å¯¹äºèŠ‚çœä¸€äº›å†…å­˜å¹¶å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°éå¸¸æœ‰ç”¨ã€‚
- en: '#### `enable_tiling`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_tiling`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L130)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L130)'
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enable tiled VAE decoding. When this option is enabled, the VAE will split the
    input tensor into tiles to compute decoding and encoding in several steps. This
    is useful for saving a large amount of memory and to allow processing larger images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨å¹³é“ºVAEè§£ç ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç“¦ç‰‡ï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—è§£ç å’Œç¼–ç ã€‚è¿™å¯¹äºèŠ‚çœå¤§é‡å†…å­˜å¹¶å…è®¸å¤„ç†æ›´å¤§çš„å›¾åƒéå¸¸æœ‰ç”¨ã€‚
- en: '#### `forward`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L423)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L423)'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sample` (`torch.FloatTensor`) â€” Input sample.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`ï¼ˆ`torch.FloatTensor`ï¼‰â€” è¾“å…¥æ ·æœ¬ã€‚'
- en: '`sample_posterior` (`bool`, *optional*, defaults to `False`) â€” Whether to sample
    from the posterior.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_posterior`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ä»åéªŒä¸­æŠ½æ ·ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `DecoderOutput` instead of a plain tuple.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›`DecoderOutput`è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚'
- en: '#### `fuse_qkv_projections`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `fuse_qkv_projections`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L452)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L452)'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Enables fused QKV projections. For self-attention modules, all projection matrices
    (i.e., query, key, value) are fused. For cross-attention modules, key and value
    projection matrices are fused.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨èåˆQKVæŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚
- en: This API is ğŸ§ª experimental.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤APIä¸ºğŸ§ªå®éªŒæ€§è´¨ã€‚
- en: '#### `set_attn_processor`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L185)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L185)'
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) â€”
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`processor`ï¼ˆ`dict`æˆ–ä»…`AttentionProcessor`ï¼‰â€” å®ä¾‹åŒ–çš„å¤„ç†å™¨ç±»æˆ–å°†è®¾ç½®ä¸º**æ‰€æœ‰**`Attention`å±‚çš„å¤„ç†å™¨ç±»å­—å…¸ã€‚'
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¦‚æœ`processor`æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œåˆ™é”®éœ€è¦å®šä¹‰ç›¸åº”äº¤å‰æ³¨æ„åŠ›å¤„ç†å™¨çš„è·¯å¾„ã€‚åœ¨è®¾ç½®å¯è®­ç»ƒçš„æ³¨æ„åŠ›å¤„ç†å™¨æ—¶ï¼Œå¼ºçƒˆå»ºè®®è¿™æ ·åšã€‚
- en: Sets the attention processor to use to compute attention.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®ç”¨äºè®¡ç®—æ³¨æ„åŠ›çš„æ³¨æ„åŠ›å¤„ç†å™¨ã€‚
- en: '#### `set_default_attn_processor`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_default_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L220)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L220)'
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Disables custom attention processors and sets the default attention implementation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦ç”¨è‡ªå®šä¹‰æ³¨æ„åŠ›å¤„ç†å™¨å¹¶è®¾ç½®é»˜è®¤çš„æ³¨æ„åŠ›å®ç°ã€‚
- en: '#### `tiled_decode`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tiled_decode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L375)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L375)'
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`z` (`torch.FloatTensor`) â€” Input batch of latent vectors.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`z`ï¼ˆ`torch.FloatTensor`ï¼‰â€” æ½œåœ¨å‘é‡çš„è¾“å…¥æ‰¹æ¬¡ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `~models.vae.DecoderOutput` instead of a plain tuple.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦è¿”å›`~models.vae.DecoderOutput`è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`~models.vae.DecoderOutput` or `tuple`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`~models.vae.DecoderOutput`æˆ–`tuple`'
- en: If return_dict is True, a `~models.vae.DecoderOutput` is returned, otherwise
    a plain `tuple` is returned.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ`return_dict`ä¸ºTrueï¼Œåˆ™è¿”å›`~models.vae.DecoderOutput`ï¼Œå¦åˆ™è¿”å›æ™®é€šçš„`tuple`ã€‚
- en: Decode a batch of images using a tiled decoder.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¹³é“ºè§£ç å™¨è§£ç ä¸€æ‰¹å›¾åƒã€‚
- en: '#### `tiled_encode`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `tiled_encode`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L321)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L321)'
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`x` (`torch.FloatTensor`) â€” Input batch of images.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`ï¼ˆ`torch.FloatTensor`ï¼‰-å›¾åƒçš„è¾“å…¥æ‰¹æ¬¡ã€‚'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) â€” Whether or not to
    return a `~models.autoencoder_kl.AutoencoderKLOutput` instead of a plain tuple.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰-æ˜¯å¦è¿”å›`~models.autoencoder_kl.AutoencoderKLOutput`è€Œä¸æ˜¯æ™®é€šçš„å…ƒç»„ã€‚'
- en: Returns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿”å›
- en: '`~models.autoencoder_kl.AutoencoderKLOutput` or `tuple`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`~models.autoencoder_kl.AutoencoderKLOutput`æˆ–`tuple`'
- en: If return_dict is True, a `~models.autoencoder_kl.AutoencoderKLOutput` is returned,
    otherwise a plain `tuple` is returned.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœreturn_dictä¸ºTrueï¼Œåˆ™è¿”å›`~models.autoencoder_kl.AutoencoderKLOutput`ï¼Œå¦åˆ™è¿”å›æ™®é€šçš„`tuple`ã€‚
- en: Encode a batch of images using a tiled encoder.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç“¦ç‰‡ç¼–ç æ‰¹é‡å›¾åƒã€‚
- en: When this option is enabled, the VAE will split the input tensor into tiles
    to compute encoding in several steps. This is useful to keep memory use constant
    regardless of image size. The end result of tiled encoding is different from non-tiled
    encoding because each tile uses a different encoder. To avoid tiling artifacts,
    the tiles overlap and are blended together to form a smooth output. You may still
    see tile-sized changes in the output, but they should be much less noticeable.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼ŒVAEå°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç“¦ç‰‡ä»¥åœ¨å‡ ä¸ªæ­¥éª¤ä¸­è®¡ç®—ç¼–ç ã€‚è¿™å¯¹äºä¿æŒå†…å­˜ä½¿ç”¨é‡æ’å®šè€Œä¸å—å›¾åƒå¤§å°å½±å“å¾ˆæœ‰ç”¨ã€‚ç“¦ç‰‡ç¼–ç çš„æœ€ç»ˆç»“æœä¸éç“¦ç‰‡ç¼–ç ä¸åŒï¼Œå› ä¸ºæ¯ä¸ªç“¦ç‰‡ä½¿ç”¨ä¸åŒçš„ç¼–ç å™¨ã€‚ä¸ºäº†é¿å…ç“¦ç‰‡ä¼ªå½±ï¼Œç“¦ç‰‡é‡å å¹¶æ··åˆåœ¨ä¸€èµ·ä»¥å½¢æˆå¹³æ»‘çš„è¾“å‡ºã€‚æ‚¨ä»ç„¶å¯èƒ½çœ‹åˆ°è¾“å‡ºä¸­çš„ç“¦ç‰‡å¤§å°å˜åŒ–ï¼Œä½†å®ƒä»¬åº”è¯¥ä¸å¤ªæ˜æ˜¾ã€‚
- en: '#### `unfuse_qkv_projections`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `unfuse_qkv_projections`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L476)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/autoencoder_kl.py#L476)'
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Disables the fused QKV projection if enabled.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå¯ç”¨ï¼Œç¦ç”¨èåˆçš„QKVæŠ•å½±ã€‚
- en: This API is ğŸ§ª experimental.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤APIä¸ºğŸ§ªå®éªŒæ€§è´¨ã€‚
- en: AutoencoderKLOutput
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨ç¼–ç å™¨KLè¾“å‡º
- en: '### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.modeling_outputs.AutoencoderKLOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/modeling_outputs.py#L6)'
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`latent_dist` (`DiagonalGaussianDistribution`) â€” Encoded outputs of `Encoder`
    represented as the mean and logvar of `DiagonalGaussianDistribution`. `DiagonalGaussianDistribution`
    allows for sampling latents from the distribution.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_dist`ï¼ˆ`DiagonalGaussianDistribution`ï¼‰-`Encoder`çš„ç¼–ç è¾“å‡ºè¡¨ç¤ºä¸º`DiagonalGaussianDistribution`çš„å‡å€¼å’Œlogvarã€‚`DiagonalGaussianDistribution`å…è®¸ä»åˆ†å¸ƒä¸­é‡‡æ ·æ½œåœ¨å˜é‡ã€‚'
- en: Output of AutoencoderKL encoding method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: AutoencoderKLç¼–ç æ–¹æ³•çš„è¾“å‡ºã€‚
- en: DecoderOutput
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DecoderOutput
- en: '### `class diffusers.models.autoencoders.vae.DecoderOutput`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.autoencoders.vae.DecoderOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/autoencoders/vae.py#L33)'
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sample` (`torch.FloatTensor` of shape `(batch_size, num_channels, height,
    width)`) â€” The decoded output sample from the last layer of the model.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`torch.FloatTensor`ï¼‰-æ¨¡å‹æœ€åä¸€å±‚çš„è§£ç è¾“å‡ºæ ·æœ¬ã€‚'
- en: Output of decoding method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç æ–¹æ³•çš„è¾“å‡ºã€‚
- en: FlaxAutoencoderKL
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxAutoencoderKL
- en: '### `class diffusers.FlaxAutoencoderKL`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.FlaxAutoencoderKL`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L726)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L726)'
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`in_channels` (`int`, *optional*, defaults to 3) â€” Number of channels in the
    input image.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰-è¾“å…¥å›¾åƒä¸­çš„é€šé“æ•°ã€‚'
- en: '`out_channels` (`int`, *optional*, defaults to 3) â€” Number of channels in the
    output.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º3ï¼‰-è¾“å‡ºä¸­çš„é€šé“æ•°ã€‚'
- en: '`down_block_types` (`Tuple[str]`, *optional*, defaults to `(DownEncoderBlock2D)`)
    â€” Tuple of downsample block types.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(DownEncoderBlock2D)`ï¼‰-ä¸‹é‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`up_block_types` (`Tuple[str]`, *optional*, defaults to `(UpDecoderBlock2D)`)
    â€” Tuple of upsample block types.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`up_block_types`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(UpDecoderBlock2D)`ï¼‰-ä¸Šé‡‡æ ·å—ç±»å‹çš„å…ƒç»„ã€‚'
- en: '`block_out_channels` (`Tuple[str]`, *optional*, defaults to `(64,)`) â€” Tuple
    of block output channels.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_out_channels`ï¼ˆ`Tuple[str]`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`(64,)`ï¼‰-å—è¾“å‡ºé€šé“çš„å…ƒç»„ã€‚'
- en: '`layers_per_block` (`int`, *optional*, defaults to `2`) â€” Number of ResNet
    layer for each block.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layers_per_block`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`2`ï¼‰-æ¯ä¸ªå—çš„ResNetå±‚æ•°ã€‚'
- en: '`act_fn` (`str`, *optional*, defaults to `silu`) â€” The activation function
    to use.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_fn`ï¼ˆ`str`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`silu`ï¼‰-è¦ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚'
- en: '`latent_channels` (`int`, *optional*, defaults to `4`) â€” Number of channels
    in the latent space.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_channels`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`4`ï¼‰-æ½œåœ¨ç©ºé—´ä¸­çš„é€šé“æ•°ã€‚'
- en: '`norm_num_groups` (`int`, *optional*, defaults to `32`) â€” The number of groups
    for normalization.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm_num_groups`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`32`ï¼‰-è§„èŒƒåŒ–çš„ç»„æ•°ã€‚'
- en: '`sample_size` (`int`, *optional*, defaults to 32) â€” Sample input size.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º32ï¼‰-æ ·æœ¬è¾“å…¥å¤§å°ã€‚'
- en: '`scaling_factor` (`float`, *optional*, defaults to 0.18215) â€” The component-wise
    standard deviation of the trained latent space computed using the first batch
    of the training set. This is used to scale the latent space to have unit variance
    when training the diffusion model. The latents are scaled with the formula `z
    = z * scaling_factor` before being passed to the diffusion model. When decoding,
    the latents are scaled back to the original scale with the formula: `z = 1 / scaling_factor
    * z`. For more details, refer to sections 4.3.2 and D.1 of the [High-Resolution
    Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
    paper.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scaling_factor`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.18215ï¼‰-ä½¿ç”¨è®­ç»ƒé›†çš„ç¬¬ä¸€æ‰¹è®¡ç®—çš„è®­ç»ƒæ½œåœ¨ç©ºé—´çš„åˆ†é‡æ ‡å‡†å·®ã€‚è¿™ç”¨äºåœ¨è®­ç»ƒæ‰©æ•£æ¨¡å‹æ—¶å°†æ½œåœ¨ç©ºé—´ç¼©æ”¾ä¸ºå•ä½æ–¹å·®ã€‚åœ¨ä¼ é€’ç»™æ‰©æ•£æ¨¡å‹ä¹‹å‰ï¼Œæ½œåœ¨ç©ºé—´ä½¿ç”¨å…¬å¼`z
    = z * scaling_factor`è¿›è¡Œç¼©æ”¾ã€‚è§£ç æ—¶ï¼Œæ½œåœ¨å˜é‡ä½¿ç”¨å…¬å¼ç¼©æ”¾å›åŸå§‹æ¯”ä¾‹ï¼š`z = 1 / scaling_factor * z`ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å…·æœ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ](https://arxiv.org/abs/2112.10752)è®ºæ–‡çš„4.3.2å’ŒD.1èŠ‚ã€‚'
- en: '`dtype` (`jnp.dtype`, *optional*, defaults to `jnp.float32`) â€” The `dtype`
    of the parameters.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jnp.dtype`, *å¯é€‰*, é»˜è®¤ä¸º `jnp.float32`) â€” å‚æ•°çš„`dtype`ã€‚'
- en: Flax implementation of a VAE model with KL loss for decoding latent representations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰KLæŸå¤±çš„è§£ç æ½œåœ¨è¡¨ç¤ºçš„Flax VAEæ¨¡å‹çš„å®ç°ã€‚
- en: This model inherits from [FlaxModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.FlaxModelMixin).
    Check the superclass documentation for itâ€™s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[FlaxModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.FlaxModelMixin)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ï¼‰ã€‚
- en: This model is a Flax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)
    subclass. Use it as a regular Flax Linen module and refer to the Flax documentation
    for all matter related to its general usage and behavior.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªFlax Linen [flax.linen.Module](https://flax.readthedocs.io/en/latest/flax.linen.html#module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„Flax
    Linenæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥äº†è§£ä¸å…¶ä¸€èˆ¬ä½¿ç”¨å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰äº‹é¡¹ã€‚
- en: 'Inherent JAX features such as the following are supported:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒä»¥ä¸‹JAXç‰¹æ€§ï¼š
- en: '[Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)'
- en: '[Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)'
- en: '[Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)'
- en: '[Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)'
- en: FlaxAutoencoderKLOutput
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxAutoencoderKLOutput
- en: '### `class diffusers.models.vae_flax.FlaxAutoencoderKLOutput`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.vae_flax.FlaxAutoencoderKLOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L47)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L47)'
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Parameters
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`latent_dist` (`FlaxDiagonalGaussianDistribution`) â€” Encoded outputs of `Encoder`
    represented as the mean and logvar of `FlaxDiagonalGaussianDistribution`. `FlaxDiagonalGaussianDistribution`
    allows for sampling latents from the distribution.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latent_dist` (`FlaxDiagonalGaussianDistribution`) â€” `Encoder`çš„ç¼–ç è¾“å‡ºï¼Œè¡¨ç¤ºä¸º`FlaxDiagonalGaussianDistribution`çš„å‡å€¼å’Œlogvarã€‚`FlaxDiagonalGaussianDistribution`å…è®¸ä»åˆ†å¸ƒä¸­é‡‡æ ·æ½œåœ¨å˜é‡ã€‚'
- en: Output of AutoencoderKL encoding method.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: AutoencoderKLç¼–ç æ–¹æ³•çš„è¾“å‡ºã€‚
- en: '#### `replace`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: â€œReturns a new object replacing the specified fields with new values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
- en: FlaxDecoderOutput
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FlaxDecoderOutput
- en: '### `class diffusers.models.vae_flax.FlaxDecoderOutput`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.vae_flax.FlaxDecoderOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L32)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vae_flax.py#L32)'
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°
- en: '`sample` (`jnp.ndarray` of shape `(batch_size, num_channels, height, width)`)
    â€” The decoded output sample from the last layer of the model.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ ·æœ¬` (`å½¢çŠ¶ä¸º`(batch_size, num_channels, height, width)`çš„`jnp.ndarray`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„è§£ç è¾“å‡ºæ ·æœ¬ã€‚'
- en: '`dtype` (`jnp.dtype`, *optional*, defaults to `jnp.float32`) â€” The `dtype`
    of the parameters.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype` (`jnp.dtype`, *å¯é€‰*, é»˜è®¤ä¸º `jnp.float32`) â€” å‚æ•°çš„`dtype`ã€‚'
- en: Output of decoding method.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç æ–¹æ³•çš„è¾“å‡ºã€‚
- en: '#### `replace`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `replace`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)'
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: â€œReturns a new object replacing the specified fields with new values.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
