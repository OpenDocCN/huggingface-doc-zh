["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\ncd examples/custom_diffusion\npip install -r requirements.txt\npip install clip-retrieval\n```", "```py\naccelerate config\n```", "```py\naccelerate config default\n```", "```py\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "```py\naccelerate launch train_custom_diffusion.py \\\n  --resolution=256\n```", "```py\npython retrieve.py --class_prompt cat --class_data_dir real_reg/samples_cat --num_class_images 200\n```", "```py\naccelerate launch train_custom_diffusion.py \\\n  --with_prior_preservation \\\n  --prior_loss_weight=1.0 \\\n  --class_data_dir=\"./real_reg/samples_cat\" \\\n  --class_prompt=\"cat\" \\\n  --real_prior=True \\\n```", "```py\nparams_to_freeze = itertools.chain(\n    text_encoder.text_model.encoder.parameters(),\n    text_encoder.text_model.final_layer_norm.parameters(),\n    text_encoder.text_model.embeddings.position_embedding.parameters(),\n)\nfreeze_params(params_to_freeze)\n```", "```py\nst = unet.state_dict()\nfor name, _ in unet.attn_processors.items():\n    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n    if name.startswith(\"mid_block\"):\n        hidden_size = unet.config.block_out_channels[-1]\n    elif name.startswith(\"up_blocks\"):\n        block_id = int(name[len(\"up_blocks.\")])\n        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n    elif name.startswith(\"down_blocks\"):\n        block_id = int(name[len(\"down_blocks.\")])\n        hidden_size = unet.config.block_out_channels[block_id]\n    layer_name = name.split(\".processor\")[0]\n    weights = {\n        \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n        \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n    }\n    if train_q_out:\n        weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n        weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n        weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n    if cross_attention_dim is not None:\n        custom_diffusion_attn_procs[name] = attention_class(\n            train_kv=train_kv,\n            train_q_out=train_q_out,\n            hidden_size=hidden_size,\n            cross_attention_dim=cross_attention_dim,\n        ).to(unet.device)\n        custom_diffusion_attn_procs[name].load_state_dict(weights)\n    else:\n        custom_diffusion_attn_procs[name] = attention_class(\n            train_kv=False,\n            train_q_out=False,\n            hidden_size=hidden_size,\n            cross_attention_dim=cross_attention_dim,\n        )\ndel st\nunet.set_attn_processor(custom_diffusion_attn_procs)\ncustom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n```", "```py\noptimizer = optimizer_class(\n    itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())\n    if args.modifier_token is not None\n    else custom_diffusion_layers.parameters(),\n    lr=args.learning_rate,\n    betas=(args.adam_beta1, args.adam_beta2),\n    weight_decay=args.adam_weight_decay,\n    eps=args.adam_epsilon,\n)\n```", "```py\nif args.modifier_token is not None:\n    if accelerator.num_processes > 1:\n        grads_text_encoder = text_encoder.module.get_input_embeddings().weight.grad\n    else:\n        grads_text_encoder = text_encoder.get_input_embeddings().weight.grad\n    index_grads_to_zero = torch.arange(len(tokenizer)) != modifier_token_id[0]\n    for i in range(len(modifier_token_id[1:])):\n        index_grads_to_zero = index_grads_to_zero & (\n            torch.arange(len(tokenizer)) != modifier_token_id[i]\n        )\n    grads_text_encoder.data[index_grads_to_zero, :] = grads_text_encoder.data[\n        index_grads_to_zero, :\n    ].fill_(0)\n```", "```py\nexport MODEL_NAME=\"CompVis/stable-diffusion-v1-4\"\nexport OUTPUT_DIR=\"path-to-save-model\"\nexport INSTANCE_DIR=\"./data/cat\"\n\naccelerate launch train_custom_diffusion.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --class_data_dir=./real_reg/samples_cat/ \\\n  --with_prior_preservation \\\n  --real_prior \\\n  --prior_loss_weight=1.0 \\\n  --class_prompt=\"cat\" \\\n  --num_class_images=200 \\\n  --instance_prompt=\"photo of a <new1> cat\"  \\\n  --resolution=512  \\\n  --train_batch_size=2  \\\n  --learning_rate=1e-5  \\\n  --lr_warmup_steps=0 \\\n  --max_train_steps=250 \\\n  --scale_lr \\\n  --hflip  \\\n  --modifier_token \"<new1>\" \\\n  --validation_prompt=\"<new1> cat sitting in a bucket\" \\\n  --report_to=\"wandb\" \\\n  --push_to_hub\n```", "```py\nimport torch\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16,\n).to(\"cuda\")\npipeline.unet.load_attn_procs(\"path-to-save-model\", weight_name=\"pytorch_custom_diffusion_weights.bin\")\npipeline.load_textual_inversion(\"path-to-save-model\", weight_name=\"<new1>.bin\")\n\nimage = pipeline(\n    \"<new1> cat sitting in a bucket\",\n    num_inference_steps=100,\n    guidance_scale=6.0,\n    eta=1.0,\n).images[0]\nimage.save(\"cat.png\")\n```"]