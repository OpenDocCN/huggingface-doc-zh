- en: ğŸ¤— Optimum
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimum
- en: 'Original text: [https://huggingface.co/docs/optimum/index](https://huggingface.co/docs/optimum/index)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'åŸæ–‡é“¾æ¥: [https://huggingface.co/docs/optimum/index](https://huggingface.co/docs/optimum/index)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ¤— Optimum is an extension of [Transformers](https://huggingface.co/docs/transformers)
    that provides a set of performance optimization tools to train and run models
    on targeted hardware with maximum efficiency.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumæ˜¯[Transformers](https://huggingface.co/docs/transformers)çš„æ‰©å±•ï¼Œæä¾›ä¸€ç»„æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œä»¥æœ€å¤§æ•ˆç‡åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šè®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚
- en: The AI ecosystem evolves quickly, and more and more specialized hardware along
    with their own optimizations are emerging every day. As such, Optimum enables
    developers to efficiently use any of these platforms with the same ease inherent
    to Transformers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿå‘å±•è¿…é€Ÿï¼Œæ¯å¤©éƒ½æœ‰è¶Šæ¥è¶Šå¤šçš„ä¸“ç”¨ç¡¬ä»¶åŠå…¶è‡ªèº«çš„ä¼˜åŒ–å‡ºç°ã€‚å› æ­¤ï¼ŒOptimumä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿé«˜æ•ˆåœ°ä½¿ç”¨ä»»ä½•è¿™äº›å¹³å°ï¼Œä¸Transformerså›ºæœ‰çš„ä¾¿åˆ©æ€§ä¸€æ ·ã€‚
- en: ğŸ¤— Optimum is distributed as a collection of packages - check out the links below
    for an in-depth look at each one.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤— Optimumè¢«åˆ†å‘ä¸ºä¸€ç³»åˆ—è½¯ä»¶åŒ… - è¯·æŸ¥çœ‹ä¸‹é¢çš„é“¾æ¥ï¼Œæ·±å…¥äº†è§£æ¯ä¸ªè½¯ä»¶åŒ…ã€‚
- en: '[Habana'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[Habana'
- en: Maximize training throughput and efficiency with Habana's Gaudi processor](./habana/index)
    [Intel
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡Habanaçš„Gaudiå¤„ç†å™¨æœ€å¤§åŒ–è®­ç»ƒååé‡å’Œæ•ˆç‡](./habana/index) [Intel
- en: Optimize your model to speedup inference with OpenVINO and Neural Compressor](./intel/index)
    [AWS Trainium/Inferentia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡OpenVINOå’Œç¥ç»å‹ç¼©å™¨ä¼˜åŒ–æ‚¨çš„æ¨¡å‹ä»¥åŠ é€Ÿæ¨ç†](./intel/index) [AWS Trainium/Inferentia
- en: Accelerate your training and inference workflows with AWS Trainium and AWS Inferentia](https://huggingface.co/docs/optimum-neuron/index)
    [NVIDIA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨AWS Trainiumå’ŒAWS InferentiaåŠ é€Ÿæ‚¨çš„è®­ç»ƒå’Œæ¨ç†å·¥ä½œæµç¨‹](https://huggingface.co/docs/optimum-neuron/index)
    [NVIDIA
- en: Accelerate inference with NVIDIA TensorRT-LLM on the NVIDIA platform](https://github.com/huggingface/optimum-nvidia)
    [AMD
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨NVIDIAå¹³å°ä¸Šä½¿ç”¨NVIDIA TensorRT-LLMåŠ é€Ÿæ¨ç†](https://github.com/huggingface/optimum-nvidia)
    [AMD
- en: Enable performance optimizations for AMD Instinct GPUs and AMD Ryzen AI NPUs](./amd/index)
    [FuriosaAI
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºAMD Instinct GPUå’ŒAMD Ryzen AI NPUåº”ç”¨æ€§èƒ½ä¼˜åŒ–](./amd/index) [FuriosaAI
- en: Fast and efficient inference on FuriosaAI WARBOY](./furiosa/index) [ONNX Runtime
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨FuriosaAI WARBOYä¸Šè¿›è¡Œå¿«é€Ÿé«˜æ•ˆçš„æ¨ç†](./furiosa/index) [ONNX Runtime
- en: Apply quantization and graph optimization to accelerate Transformers models
    training and inference with ONNX Runtime](./onnxruntime/overview) [BetterTransformer
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨é‡åŒ–å’Œå›¾ä¼˜åŒ–æ¥åŠ é€ŸTransformersæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ï¼Œä½¿ç”¨ONNX Runtime](./onnxruntime/overview) [BetterTransformer
- en: A one-liner integration to use PyTorch's BetterTransformer with Transformers
    models](./bettertransformer/overview)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PyTorchçš„BetterTransformerä¸Transformersæ¨¡å‹çš„ä¸€è¡Œé›†æˆ](./bettertransformer/overview)
