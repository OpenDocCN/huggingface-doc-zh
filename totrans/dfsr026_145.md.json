["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: Optional = None requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union = None image: Union = None num_inference_steps: int = 100 guidance_scale: float = 7.5 image_guidance_scale: float = 1.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) \u2192 export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import PIL\n>>> import requests\n>>> import torch\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionInstructPix2PixPipeline\n\n>>> def download_image(url):\n...     response = requests.get(url)\n...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n>>> img_url = \"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\"\n\n>>> image = download_image(img_url).resize((512, 512))\n\n>>> pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n...     \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> prompt = \"make the mountains snowy\"\n>>> image = pipe(prompt=prompt, image=image).images[0]\n```", "```py\n( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )\n```", "```py\n( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel text_encoder_2: CLIPTextModelWithProjection tokenizer: CLIPTokenizer tokenizer_2: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers force_zeros_for_empty_prompt: bool = True add_watermarker: Optional = None )\n```", "```py\n( prompt: Union = None prompt_2: Union = None image: Union = None height: Optional = None width: Optional = None num_inference_steps: int = 100 denoising_end: Optional = None guidance_scale: float = 5.0 image_guidance_scale: float = 1.5 negative_prompt: Union = None negative_prompt_2: Union = None num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 cross_attention_kwargs: Optional = None guidance_rescale: float = 0.0 original_size: Tuple = None crops_coords_top_left: Tuple = (0, 0) target_size: Tuple = None ) \u2192 export const metadata = 'undefined';~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionXLInstructPix2PixPipeline\n>>> from diffusers.utils import load_image\n\n>>> resolution = 768\n>>> image = load_image(\n...     \"https://hf.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\"\n... ).resize((resolution, resolution))\n>>> edit_instruction = \"Turn sky into a cloudy one\"\n\n>>> pipe = StableDiffusionXLInstructPix2PixPipeline.from_pretrained(\n...     \"diffusers/sdxl-instructpix2pix-768\", torch_dtype=torch.float16\n... ).to(\"cuda\")\n\n>>> edited_image = pipe(\n...     prompt=edit_instruction,\n...     image=image,\n...     height=resolution,\n...     width=resolution,\n...     guidance_scale=3.0,\n...     image_guidance_scale=1.5,\n...     num_inference_steps=30,\n... ).images[0]\n>>> edited_image\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( s1: float s2: float b1: float b2: float )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( prompt: str prompt_2: Optional = None device: Optional = None num_images_per_prompt: int = 1 do_classifier_free_guidance: bool = True negative_prompt: Optional = None negative_prompt_2: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None pooled_prompt_embeds: Optional = None negative_pooled_prompt_embeds: Optional = None lora_scale: Optional = None )\n```"]