- en: UNetMotionModel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UNetMotionModel
- en: 'Original text: [https://huggingface.co/docs/diffusers/api/models/unet-motion](https://huggingface.co/docs/diffusers/api/models/unet-motion)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/diffusers/api/models/unet-motion](https://huggingface.co/docs/diffusers/api/models/unet-motion)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced
    by Ronneberger et al for biomedical image segmentation, but it is also commonly
    used in 🤗 Diffusers because it outputs images that are the same size as the input.
    It is one of the most important components of a diffusion system because it facilitates
    the actual diffusion process. There are several variants of the UNet model in
    🤗 Diffusers, depending on it’s number of dimensions and whether it is a conditional
    model or not. This is a 2D UNet model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[UNet](https://huggingface.co/papers/1505.04597)模型最初由Ronneberger等人引入，用于生物医学图像分割，但它也常用于🤗
    Diffusers，因为它输出与输入相同大小的图像。它是扩散系统中最重要的组件之一，因为它促进了实际的扩散过程。🤗 Diffusers中有几种UNet模型的变体，取决于它的维度数量以及它是否是条件模型。这是一个2D
    UNet模型。'
- en: 'The abstract from the paper is:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 论文摘要如下：
- en: '*There is large consent that successful training of deep networks requires
    many thousand annotated training samples. In this paper, we present a network
    and training strategy that relies on the strong use of data augmentation to use
    the available annotated samples more efficiently. The architecture consists of
    a contracting path to capture context and a symmetric expanding path that enables
    precise localization. We show that such a network can be trained end-to-end from
    very few images and outperforms the prior best method (a sliding-window convolutional
    network) on the ISBI challenge for segmentation of neuronal structures in electron
    microscopic stacks. Using the same network trained on transmitted light microscopy
    images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in
    these categories by a large margin. Moreover, the network is fast. Segmentation
    of a 512x512 image takes less than a second on a recent GPU. The full implementation
    (based on Caffe) and the trained networks are available at [http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net).*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*大多数人一致认为成功训练深度网络需要成千上万个注释训练样本。在本文中，我们提出了一种依赖于数据增强的网络和训练策略，以更有效地利用可用的注释样本。该架构由一个收缩路径和一个对称扩展路径组成，用于捕获上下文和实现精确定位。我们展示了这样一个网络可以从很少的图像端到端训练，并且在ISBI挑战中表现优于先前最佳方法（滑动窗口卷积网络）用于电子显微镜堆栈中神经结构的分割。使用相同的网络在透射光学显微镜图像（相差和DIC）上训练，我们在这些类别上以很大的优势赢得了2015年ISBI细胞跟踪挑战。此外，该网络速度很快。在最新的GPU上，对512x512图像的分割不到一秒。完整的实现（基于Caffe）和训练的网络可在[http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)上获得。*'
- en: UNetMotionModel
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNetMotionModel
- en: '### `class diffusers.UNetMotionModel`'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.UNetMotionModel`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L176)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L176)'
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A modified conditional 2D UNet model that takes a noisy sample, conditional
    state, and a timestep and returns a sample shaped output.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个修改后的条件2D UNet模型，接受嘈杂样本、条件状态和时间步，并返回一个形状输出的样本。
- en: This model inherits from [ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin).
    Check the superclass documentation for it’s generic methods implemented for all
    models (such as downloading or saving).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型继承自[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。检查超类文档以了解为所有模型实现的通用方法（如下载或保存）。
- en: '#### `disable_freeu`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `disable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L695)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L695)'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Disables the FreeU mechanism.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用FreeU机制。
- en: '#### `enable_forward_chunking`'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_forward_chunking`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L608)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L608)'
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Parameters
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`chunk_size` (`int`, *optional*) — The chunk size of the feed-forward layers.
    If not specified, will run feed-forward layer individually over each tensor of
    dim=`dim`.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chunk_size`（`int`，*可选*）— 前馈层的分块大小。如果未指定，将在dim=`dim`的每个张量上单独运行前馈层。'
- en: '`dim` (`int`, *optional*, defaults to `0`) — The dimension over which the feed-forward
    computation should be chunked. Choose between dim=0 (batch) or dim=1 (sequence
    length).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dim`（`int`，*可选*，默认为`0`）— 前馈计算应该在哪个维度上分块。选择dim=0（批处理）或dim=1（序列长度）之间的值。'
- en: Sets the attention processor to use [feed forward chunking](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设置注意力处理器使用[前馈分块](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers)。
- en: '#### `enable_freeu`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `enable_freeu`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L670)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L670)'
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Parameters
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`s1` (`float`) — Scaling factor for stage 1 to attenuate the contributions
    of the skip features. This is done to mitigate the “oversmoothing effect” in the
    enhanced denoising process.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s1`（`float`）— 阶段1的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`s2` (`float`) — Scaling factor for stage 2 to attenuate the contributions
    of the skip features. This is done to mitigate the “oversmoothing effect” in the
    enhanced denoising process.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s2`（`float`）— 阶段2的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。'
- en: '`b1` (`float`) — Scaling factor for stage 1 to amplify the contributions of
    backbone features.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b1`（`float`）— 阶段1的缩放因子，用于放大骨干特征的贡献。'
- en: '`b2` (`float`) — Scaling factor for stage 2 to amplify the contributions of
    backbone features.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b2`（`float`）— 阶段2的缩放因子，用于放大骨干特征的贡献。'
- en: Enables the FreeU mechanism from [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 启用来自 [https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497) 的
    FreeU 机制。
- en: The suffixes after the scaling factors represent the stage blocks where they
    are being applied.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子后的后缀表示它们被应用的阶段块。
- en: Please refer to the [official repository](https://github.com/ChenyangSi/FreeU)
    for combinations of values that are known to work well for different pipelines
    such as Stable Diffusion v1, v2, and Stable Diffusion XL.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同流水线（如 Stable Diffusion
    v1、v2 和 Stable Diffusion XL）的值组合。
- en: '#### `forward`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L703)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L703)'
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor`) — The noisy input tensor with the following
    shape `(batch, num_frames, channel, height, width`.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample` (`torch.FloatTensor`) — 具有以下形状 `(batch, num_frames, channel, height,
    width)` 的嘈杂输入张量。'
- en: '`timestep` (`torch.FloatTensor` or `float` or `int`) — The number of timesteps
    to denoise an input.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestep` (`torch.FloatTensor` 或 `float` 或 `int`) — 对输入进行去噪的时间步数。'
- en: '`encoder_hidden_states` (`torch.FloatTensor`) — The encoder hidden states with
    shape `(batch, sequence_length, feature_dim)`. timestep_cond — (`torch.Tensor`,
    *optional*, defaults to `None`): Conditional embeddings for timestep. If provided,
    the embeddings will be summed with the samples passed through the `self.time_embedding`
    layer to obtain the timestep embeddings.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_hidden_states` (`torch.FloatTensor`) — 具有形状 `(batch, sequence_length,
    feature_dim)` 的编码器隐藏状态。timestep_cond — (`torch.Tensor`, *可选*, 默认为 `None`): 时间步的条件嵌入。如果提供了，这些嵌入将与通过
    `self.time_embedding` 层传递的样本相加，以获得时间步嵌入。'
- en: '`attention_mask` (`torch.Tensor`, *optional*, defaults to `None`) — An attention
    mask of shape `(batch, key_tokens)` is applied to `encoder_hidden_states`. If
    `1` the mask is kept, otherwise if `0` it is discarded. Mask will be converted
    into a bias, which adds large negative values to the attention scores corresponding
    to “discard” tokens.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.Tensor`, *可选*, 默认为 `None`) — 一个形状为 `(batch, key_tokens)`
    的注意力掩码被应用于 `encoder_hidden_states`。如果为 `1`，则保留掩码，否则如果为 `0`，则丢弃。掩码将被转换为偏置，这会在“丢弃”标记对应的注意力分数上添加大的负值。'
- en: '`cross_attention_kwargs` (`dict`, *optional*) — A kwargs dictionary that if
    specified is passed along to the `AttentionProcessor` as defined under `self.processor`
    in [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
    down_block_additional_residuals — (`tuple` of `torch.Tensor`, *optional*): A tuple
    of tensors that if specified are added to the residuals of down unet blocks. mid_block_additional_residual
    — (`torch.Tensor`, *optional*): A tensor that if specified is added to the residual
    of the middle unet block.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cross_attention_kwargs` (`dict`, *可选*) — 如果指定了，将传递给 `AttentionProcessor` 的
    kwargs 字典，如 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)
    中的 `self.processor` 中定义的那样。down_block_additional_residuals — (`torch.Tensor` 的
    `tuple`, *可选*): 如果指定了，将添加到 down unet 块的残差中。mid_block_additional_residual — (`torch.Tensor`,
    *可选*): 如果指定了，将添加到中间 unet 块的残差中。'
- en: '`return_dict` (`bool`, *optional*, defaults to `True`) — Whether or not to
    return a `~models.unet_3d_condition.UNet3DConditionOutput` instead of a plain
    tuple.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 `~models.unet_3d_condition.UNet3DConditionOutput`
    而不是一个普通元组。'
- en: Returns
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`~models.unet_3d_condition.UNet3DConditionOutput` or `tuple`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`~models.unet_3d_condition.UNet3DConditionOutput` 或 `tuple`'
- en: If `return_dict` is True, an `~models.unet_3d_condition.UNet3DConditionOutput`
    is returned, otherwise a `tuple` is returned where the first element is the sample
    tensor.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `return_dict` 为 True，则返回一个 `~models.unet_3d_condition.UNet3DConditionOutput`，否则返回一个元组，其中第一个元素是样本张量。
- en: The [UNetMotionModel](/docs/diffusers/v0.26.3/en/api/models/unet-motion#diffusers.UNetMotionModel)
    forward method.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[UNetMotionModel](/docs/diffusers/v0.26.3/en/api/models/unet-motion#diffusers.UNetMotionModel)
    的前向方法。'
- en: '#### `freeze_unet2d_params`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `freeze_unet2d_params`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L478)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L478)'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Freeze the weights of just the UNet2DConditionModel, and leave the motion modules
    unfrozen for fine tuning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结仅 UNet2DConditionModel 的权重，并保持运动模块未冻结以进行微调。
- en: '#### `set_attn_processor`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L573)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L573)'
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameters
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`processor` (`dict` of `AttentionProcessor` or only `AttentionProcessor`) —
    The instantiated processor class or a dictionary of processor classes that will
    be set as the processor for **all** `Attention` layers.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`processor` (`AttentionProcessor` 的 `dict` 或仅 `AttentionProcessor`) — 实例化的处理器类或处理器类的字典，将被设置为**所有**
    `Attention` 层的处理器。'
- en: If `processor` is a dict, the key needs to define the path to the corresponding
    cross attention processor. This is strongly recommended when setting trainable
    attention processors.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 `processor` 是一个字典，则键需要定义相应的交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。
- en: Sets the attention processor to use to compute attention.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于计算注意力的注意力处理器。
- en: '#### `set_default_attn_processor`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `set_default_attn_processor`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L650)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L650)'
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Disables custom attention processors and sets the default attention implementation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 禁用自定义注意力处理器并设置默认的注意力实现。
- en: UNet3DConditionOutput
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UNet3DConditionOutput
- en: '### `class diffusers.models.unets.unet_3d_condition.UNet3DConditionOutput`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class diffusers.models.unets.unet_3d_condition.UNet3DConditionOutput`'
- en: '[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_3d_condition.py#L51)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_3d_condition.py#L51)'
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Parameters
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`sample` (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels,
    height, width)`) — The hidden states output conditioned on `encoder_hidden_states`
    input. Output of last layer of model.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample`（形状为`(batch_size, num_frames, num_channels, height, width)`的`torch.FloatTensor`）-
    在`encoder_hidden_states`输入条件下的隐藏状态输出。模型的最后一层的输出。'
- en: The output of [UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[UNet3DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet3d-cond#diffusers.UNet3DConditionModel)的输出。'
