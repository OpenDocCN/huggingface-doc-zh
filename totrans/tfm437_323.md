# Wav2Vec2

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)

## æ¦‚è¿°

Wav2Vec2æ¨¡å‹æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliåœ¨[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚

è®ºæ–‡æ‘˜è¦å¦‚ä¸‹ï¼š

*æˆ‘ä»¬é¦–æ¬¡å±•ç¤ºï¼Œä»…é€šè¿‡ä»è¯­éŸ³éŸ³é¢‘ä¸­å­¦ä¹ å¼ºå¤§çš„è¡¨ç¤ºï¼Œç„¶ååœ¨è½¬å½•çš„è¯­éŸ³ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥èƒœè¿‡æœ€ä½³çš„åŠç›‘ç£æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¦‚å¿µä¸Šæ›´ç®€å•ã€‚wav2vec 2.0åœ¨æ½œåœ¨ç©ºé—´ä¸­å±è”½è¯­éŸ³è¾“å…¥ï¼Œå¹¶è§£å†³äº†ä¸€ä¸ªåœ¨è”åˆå­¦ä¹ çš„æ½œåœ¨è¡¨ç¤ºçš„é‡åŒ–ä¸Šå®šä¹‰çš„å¯¹æ¯”ä»»åŠ¡ã€‚ä½¿ç”¨Librispeechçš„æ‰€æœ‰æ ‡è®°æ•°æ®è¿›è¡Œçš„å®éªŒåœ¨å¹²å‡€/å…¶ä»–æµ‹è¯•é›†ä¸Šå®ç°äº†1.8/3.3çš„WERã€‚å½“å°†æ ‡è®°æ•°æ®é‡é™ä½åˆ°ä¸€å°æ—¶æ—¶ï¼Œwav2vec 2.0åœ¨100å°æ—¶å­é›†ä¸Šèƒœè¿‡äº†å…ˆå‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ ‡è®°æ•°æ®é‡å‡å°‘äº†100å€ã€‚ä»…ä½¿ç”¨ååˆ†é’Ÿçš„æ ‡è®°æ•°æ®å¹¶åœ¨53kå°æ—¶çš„æœªæ ‡è®°æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»ç„¶å®ç°äº†4.8/8.2çš„WERã€‚è¿™è¯æ˜äº†åœ¨æœ‰é™çš„æ ‡è®°æ•°æ®é‡ä¸‹è¿›è¡Œè¯­éŸ³è¯†åˆ«çš„å¯è¡Œæ€§ã€‚*

æ­¤æ¨¡å‹ç”±[patrickvonplaten](https://huggingface.co/patrickvonplaten)è´¡çŒ®ã€‚

## ä½¿ç”¨æç¤º

+   Wav2Vec2æ˜¯ä¸€ä¸ªæ¥å—ä¸è¯­éŸ³ä¿¡å·çš„åŸå§‹æ³¢å½¢å¯¹åº”çš„æµ®ç‚¹æ•°ç»„çš„è¯­éŸ³æ¨¡å‹ã€‚

+   Wav2Vec2æ¨¡å‹æ˜¯ä½¿ç”¨è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤æ¨¡å‹è¾“å‡ºå¿…é¡»ä½¿ç”¨[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)è¿›è¡Œè§£ç ã€‚

## èµ„æº

ä¸€ä»½å®˜æ–¹Hugging Faceå’Œç¤¾åŒºï¼ˆç”±ğŸŒè¡¨ç¤ºï¼‰èµ„æºåˆ—è¡¨ï¼Œå¯å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨Wav2Vec2ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£æäº¤èµ„æºä»¥åŒ…å«åœ¨æ­¤å¤„ï¼Œè¯·éšæ—¶æ‰“å¼€Pull Requestï¼Œæˆ‘ä»¬å°†è¿›è¡Œå®¡æŸ¥ï¼èµ„æºåº”è¯¥ç†æƒ³åœ°å±•ç¤ºä¸€äº›æ–°å†…å®¹ï¼Œè€Œä¸æ˜¯é‡å¤ç°æœ‰èµ„æºã€‚

éŸ³é¢‘åˆ†ç±»

+   ä¸€ä¸ªå…³äºå¦‚ä½•[åˆ©ç”¨é¢„è®­ç»ƒçš„Wav2Vec2æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ

+   [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)å—åˆ°è¿™ä¸ª[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)å’Œ[ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)çš„æ”¯æŒã€‚

+   [éŸ³é¢‘åˆ†ç±»ä»»åŠ¡æŒ‡å—](../tasks/audio_classification)

è‡ªåŠ¨è¯­éŸ³è¯†åˆ«

+   ä¸€ç¯‡å…³äº[åœ¨ğŸ¤— Transformersä¸­ä½¿ç”¨n-gramså¢å¼ºWav2Vec2çš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/wav2vec2-with-ngram)ã€‚

+   ä¸€ç¯‡å…³äºå¦‚ä½•[ä½¿ç”¨ğŸ¤— Transformerså¯¹è‹±è¯­ASRè¿›è¡Œå¾®è°ƒçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/fine-tune-wav2vec2-english)ã€‚

+   ä¸€ç¯‡å…³äº[ä½¿ç”¨ğŸ¤— Transformerså¯¹å¤šè¯­è¨€ASRè¿›è¡Œå¾®è°ƒçš„åšå®¢æ–‡ç« ](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)ã€‚

+   ä¸€ä¸ªå…³äºå¦‚ä½•[é€šè¿‡ä½¿ç”¨Wav2Vec2è½¬å½•éŸ³é¢‘ä»ä»»ä½•è§†é¢‘åˆ›å»ºYouTubeå­—å¹•](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)çš„ç¬”è®°æœ¬ã€‚ğŸŒ

+   [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)å—åˆ°ä¸€ç¯‡å…³äº[å¦‚ä½•åœ¨è‹±è¯­ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)å’Œ[å¦‚ä½•åœ¨ä»»ä½•è¯­è¨€ä¸­å¾®è°ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)çš„æ”¯æŒã€‚

+   [è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡æŒ‡å—](../tasks/asr)

ğŸš€ éƒ¨ç½²

+   å…³äºå¦‚ä½•åœ¨[Hugging Faceçš„Transformerså’ŒAmazon SageMakerä¸­éƒ¨ç½²Wav2Vec2è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„åšæ–‡](https://www.philschmid.de/automatic-speech-recognition-sagemaker)ã€‚

## Wav2Vec2Config

### `class transformers.Wav2Vec2Config`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)

```py
( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 feat_quantizer_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_norm = 'group' feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 do_stable_layer_norm = False apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 num_codevectors_per_group = 320 num_codevector_groups = 2 contrastive_logits_temperature = 0.1 num_negatives = 100 codevector_dim = 256 proj_codevector_dim = 256 diversity_loss_weight = 0.1 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None adapter_attn_dim = None **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 32) â€” Wav2Vec2æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)æˆ–[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚æ¨¡å‹çš„è¯æ±‡è¡¨å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)çš„forwardæ–¹æ³•æ—¶å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 768) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) â€” Transformerç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 12) â€” Transformerç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_size` (`int`, *optional*, defaults to 3072) â€” Transformerç¼–ç å™¨ä¸­â€œä¸­é—´â€ï¼ˆå³å‰é¦ˆï¼‰å±‚çš„ç»´åº¦ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`"gelu"`ã€`"relu"`ã€`"selu"`å’Œ`"gelu_new"`ã€‚

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) â€” åµŒå…¥å±‚ã€ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­æ‰€æœ‰å®Œå…¨è¿æ¥å±‚çš„dropoutæ¦‚ç‡ã€‚

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) â€” å®Œå…¨è¿æ¥å±‚å†…æ¿€æ´»çš„dropoutæ¯”ç‡ã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›æ¦‚ç‡çš„dropoutæ¯”ç‡ã€‚

+   `final_dropout` (`float`, *optional*, defaults to 0.1) â€” [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)æœ€ç»ˆæŠ•å½±å±‚çš„dropoutæ¦‚ç‡ã€‚

+   `layerdrop` (`float`, *optional*, defaults to 0.1) â€” LayerDropæ¦‚ç‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[LayerDropè®ºæ–‡](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„epsilonã€‚

+   `feat_extract_norm` (`str`, *optional*, defaults to `"group"`) â€” åº”ç”¨äºç‰¹å¾ç¼–ç å™¨ä¸­1Då·ç§¯å±‚çš„è§„èŒƒåŒ–ã€‚`"group"`è¡¨ç¤ºä»…å¯¹ç¬¬ä¸€ä¸ª1Då·ç§¯å±‚è¿›è¡Œç»„å½’ä¸€åŒ–ï¼Œ`"layer"`è¡¨ç¤ºå¯¹æ‰€æœ‰1Då·ç§¯å±‚è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚

+   `feat_proj_dropout` (`float`, *optional*, defaults to 0.0) â€” ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„dropoutæ¦‚ç‡ã€‚

+   `feat_extract_activation` (`str,` optional`, defaults to` â€œgeluâ€`) -- ç‰¹å¾æå–å™¨ä¸­1Då·ç§¯å±‚çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ”¯æŒ`â€œgeluâ€`ã€`â€œreluâ€`ã€`â€œseluâ€`å’Œ`â€œgelu_newâ€`ã€‚

+   `feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) â€” é‡åŒ–ç‰¹å¾ç¼–ç å™¨çŠ¶æ€çš„dropoutæ¦‚ç‡ã€‚

+   `conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 512, 512, 512)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„è¾“å…¥å’Œè¾“å‡ºé€šé“æ•°ã€‚*conv_dim*çš„é•¿åº¦å®šä¹‰äº†1Då·ç§¯å±‚çš„æ•°é‡ã€‚

+   `conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2, 2, 2, 2, 2, 2)`) â€” åœ¨ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„æ­¥å¹…çš„æ•´æ•°å…ƒç»„ã€‚*conv_stride*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚

+   `conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3, 3, 3, 3, 3, 3)`) â€” åœ¨ç‰¹å¾ç¼–ç å™¨ä¸­æ¯ä¸ª1Då·ç§¯å±‚çš„å·ç§¯æ ¸å¤§å°çš„æ•´æ•°å…ƒç»„ã€‚*conv_kernel*çš„é•¿åº¦å®šä¹‰äº†å·ç§¯å±‚çš„æ•°é‡ï¼Œå¹¶ä¸”å¿…é¡»ä¸*conv_dim*çš„é•¿åº¦åŒ¹é…ã€‚

+   `conv_bias` (`bool`, *optional*, defaults to `False`) â€” 1Då·ç§¯å±‚æ˜¯å¦å…·æœ‰åç½®ã€‚

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) â€” å·ç§¯ä½ç½®åµŒå…¥çš„æ•°é‡ã€‚å®šä¹‰äº†1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„å·ç§¯æ ¸å¤§å°ã€‚

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) â€” 1Då·ç§¯ä½ç½®åµŒå…¥å±‚çš„ç»„æ•°ã€‚

+   `do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) â€” æ˜¯å¦åº”ç”¨Transformerç¼–ç å™¨çš„*stable*å±‚å½’ä¸€åŒ–æ¶æ„ã€‚`do_stable_layer_normä¸ºTrue`è¡¨ç¤ºåœ¨æ³¨æ„åŠ›å±‚ä¹‹å‰åº”ç”¨å±‚å½’ä¸€åŒ–ï¼Œè€Œ`do_stable_layer_normä¸ºFalse`è¡¨ç¤ºåœ¨æ³¨æ„åŠ›å±‚ä¹‹ååº”ç”¨å±‚å½’ä¸€åŒ–ã€‚

+   `apply_spec_augment` (`bool`, *optional*, defaults to `True`) â€” æ˜¯å¦å°†*SpecAugment*æ•°æ®å¢å¼ºåº”ç”¨äºç‰¹å¾ç¼–ç å™¨çš„è¾“å‡ºã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)ã€‚

+   `mask_time_prob` (`float`, *optional*, defaults to 0.05) â€” æ²¿æ—¶é—´è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_time_prob*len(time_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºæ©ç›–çš„å‘é‡è·¨åº¦èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ* mask_time_prob *åº”ä¸º`prob_vector_start*mask_time_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½å®é™…æ©ç›–å‘é‡çš„ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚

+   `mask_time_length` (`int`, *optional*, defaults to 10) â€” æ²¿æ—¶é—´è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_time_min_masks` (`int`, *optional*, defaults to 2), â€” æ²¿æ—¶é—´è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æœ€å°æ©ç æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masksâ€æ—¶ç›¸å…³

+   `mask_feature_prob` (`float`, *optional*, defaults to 0.0) â€” æ²¿ç‰¹å¾è½´çš„æ‰€æœ‰ç‰¹å¾å‘é‡ä¸­å°†è¢«æ©ç›–çš„ç™¾åˆ†æ¯”ï¼ˆä»‹äº0å’Œ1ä¹‹é—´ï¼‰ã€‚æ©ç è¿‡ç¨‹åœ¨è½´ä¸Šç”Ÿæˆâ€mask_feature_prob*len(feature_axis)/mask_time_lengthâ€ä¸ªç‹¬ç«‹çš„æ©ç ã€‚å¦‚æœä»æ¯ä¸ªç‰¹å¾å‘é‡è¢«é€‰æ‹©ä¸ºæ©ç›–çš„å‘é‡è·¨åº¦èµ·å§‹çš„æ¦‚ç‡æ¨ç†ï¼Œ* mask_feature_prob *åº”ä¸º`prob_vector_start*mask_feature_length`ã€‚è¯·æ³¨æ„ï¼Œé‡å å¯èƒ½ä¼šé™ä½å®é™…æ©ç›–å‘é‡çš„ç™¾åˆ†æ¯”ã€‚ä»…åœ¨`apply_spec_augmentä¸ºTrue`æ—¶ç›¸å…³ã€‚

+   `mask_feature_length` (`int`, *optional*, defaults to 10) â€” æ²¿ç‰¹å¾è½´çš„å‘é‡è·¨åº¦é•¿åº¦ã€‚

+   `mask_feature_min_masks` (`int`, *optional*, defaults to 0), â€” æ²¿ç‰¹å¾è½´ç”Ÿæˆçš„é•¿åº¦ä¸º`mask_feature_length`çš„æœ€å°æ©ç æ•°é‡ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ï¼Œä¸`mask_feature_prob`æ— å…³ã€‚ä»…åœ¨â€mask_feature_prob*len(feature_axis)/mask_feature_length < mask_feature_min_masksâ€æ—¶ç›¸å…³

+   `num_codevectors_per_group` (`int`, *optional*, defaults to 320) â€” æ¯ä¸ªé‡åŒ–ç ä¹¦ï¼ˆç»„ï¼‰ä¸­çš„æ¡ç›®æ•°ã€‚

+   `num_codevector_groups` (`int`, *optional*, defaults to 2) â€” äº§å“ç çŸ¢é‡é‡åŒ–çš„ç çŸ¢é‡ç»„æ•°ã€‚

+   `contrastive_logits_temperature` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” å¯¹æ¯”æŸå¤±ä¸­çš„æ¸©åº¦ *kappa*ã€‚

+   `feat_quantizer_dropout` (`float`, *å¯é€‰*, é»˜è®¤ä¸º 0.0) â€” ç”¨äºé‡åŒ–å™¨ä½¿ç”¨çš„ç‰¹å¾ç¼–ç å™¨è¾“å‡ºçš„ä¸¢å¼ƒæ¦‚ç‡ã€‚

+   `num_negatives` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 100) â€” å¯¹æ¯”æŸå¤±çš„è´Ÿæ ·æœ¬æ•°é‡ã€‚

+   `codevector_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” é‡åŒ–ç‰¹å¾å‘é‡çš„ç»´åº¦ã€‚

+   `proj_codevector_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” æœ€ç»ˆæŠ•å½±çš„ç»´åº¦ï¼ŒåŒ…æ‹¬é‡åŒ–ç‰¹å¾å’Œå˜æ¢å™¨ç‰¹å¾ã€‚

+   `diversity_loss_weight` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 0.1) â€” ä»£ç æœ¬å¤šæ ·æ€§æŸå¤±ç»„ä»¶çš„æƒé‡ã€‚

+   `ctc_loss_reduction` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"sum"`) â€” æŒ‡å®šåº”ç”¨äº `torch.nn.CTCLoss` è¾“å‡ºçš„å‡å°‘æ–¹å¼ã€‚ä»…åœ¨è®­ç»ƒ [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `ctc_zero_infinity` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å°† `torch.nn.CTCLoss` çš„æ— é™æŸå¤±å’Œç›¸å…³æ¢¯åº¦ç½®é›¶ã€‚å½“è¾“å…¥å¤ªçŸ­æ— æ³•ä¸ç›®æ ‡å¯¹é½æ—¶ï¼Œä¸»è¦ä¼šå‡ºç°æ— é™æŸå¤±ã€‚ä»…åœ¨è®­ç»ƒ [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `use_weighted_layer_sum` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä½¿ç”¨å…·æœ‰å­¦ä¹ æƒé‡çš„å±‚è¾“å‡ºçš„åŠ æƒå¹³å‡ã€‚ä»…åœ¨ä½¿ç”¨ [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) å®ä¾‹æ—¶ç›¸å…³ã€‚

+   `classifier_proj_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 256) â€” ç”¨äºåˆ†ç±»çš„ä»¤ç‰Œå‡å€¼æ± åŒ–ä¹‹å‰çš„æŠ•å½±ç»´åº¦ã€‚

+   `tdnn_dim` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(512, 512, 512, 512, 1500)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº† *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚*tdnn_dim* çš„é•¿åº¦å®šä¹‰äº† *TDNN* å±‚çš„æ•°é‡ã€‚

+   `tdnn_kernel` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(5, 3, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº† *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚*tdnn_kernel* çš„é•¿åº¦å¿…é¡»ä¸ *tdnn_dim* çš„é•¿åº¦ç›¸åŒ¹é…ã€‚

+   `tdnn_dilation` (`Tuple[int]` æˆ– `List[int]`, *å¯é€‰*, é»˜è®¤ä¸º `(1, 2, 3, 1, 1)`) â€” ä¸€ä¸ªæ•´æ•°å…ƒç»„ï¼Œå®šä¹‰äº† *XVector* æ¨¡å‹ä¸­ *TDNN* æ¨¡å—ä¸­æ¯ä¸ªä¸€ç»´å·ç§¯å±‚çš„è†¨èƒ€å› å­ã€‚*tdnn_dilation* çš„é•¿åº¦å¿…é¡»ä¸ *tdnn_dim* çš„é•¿åº¦ç›¸åŒ¹é…ã€‚

+   `xvector_output_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” *XVector* åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚

+   `add_adapter` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åœ¨ Wav2Vec2 ç¼–ç å™¨é¡¶éƒ¨å †å å·ç§¯ç½‘ç»œã€‚å¯¹äº Warm-starting Wav2Vec2 for SpeechEncoderDecoder æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚

+   `adapter_kernel_size` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„å†…æ ¸å¤§å°ã€‚ä»…åœ¨ `add_adapter` ä¸º True æ—¶ç›¸å…³ã€‚

+   `adapter_stride` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 2) â€” é€‚é…å™¨ç½‘ç»œä¸­å·ç§¯å±‚çš„æ­¥å¹…ã€‚ä»…åœ¨ `add_adapter` ä¸º True æ—¶ç›¸å…³ã€‚

+   `num_adapter_layers` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 3) â€” é€‚é…å™¨ç½‘ç»œä¸­åº”ä½¿ç”¨çš„å·ç§¯å±‚æ•°é‡ã€‚ä»…åœ¨ `add_adapter` ä¸º True æ—¶ç›¸å…³ã€‚

+   `adapter_attn_dim` (`int`, *å¯é€‰*) â€” æ¯ä¸ªæ³¨æ„åŠ›å—ä¸­è¦ä½¿ç”¨çš„æ³¨æ„åŠ›é€‚é…å™¨æƒé‡çš„ç»´åº¦ã€‚ä½¿ç”¨æ³¨æ„åŠ›é€‚é…å™¨çš„æ¨¡å‹ç¤ºä¾‹æ˜¯ [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)ã€‚

+   `output_hidden_size` (`int`, *å¯é€‰*) â€” ç¼–ç å™¨è¾“å‡ºå±‚çš„ç»´åº¦ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™é»˜è®¤ä¸º *hidden-size*ã€‚ä»…åœ¨ `add_adapter` ä¸º True æ—¶ç›¸å…³ã€‚

è¿™æ˜¯ç”¨äºå­˜å‚¨ [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ– Wav2Vec2 æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)ï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import Wav2Vec2Config, Wav2Vec2Model

>>> # Initializing a Wav2Vec2 facebook/wav2vec2-base-960h style configuration
>>> configuration = Wav2Vec2Config()

>>> # Initializing a model (with random weights) from the facebook/wav2vec2-base-960h style configuration
>>> model = Wav2Vec2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Wav2Vec2CTCTokenizer

### `class transformers.Wav2Vec2CTCTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' word_delimiter_token = '|' replace_word_delimiter_char = ' ' do_lower_case = False target_lang = None **kwargs )
```

å‚æ•°

+   `vocab_file` (`str`) â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚

+   `bos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<s>"`) â€” å¥å­å¼€å¤´æ ‡è®°ã€‚

+   `eos_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"</s>"`) â€” å¥å­ç»“æŸæ ‡è®°ã€‚

+   `unk_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<unk>"`) â€” æœªçŸ¥æ ‡è®°ã€‚è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„æ ‡è®°æ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºæ­¤æ ‡è®°ã€‚

+   `pad_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"<pad>"`) â€” ç”¨äºå¡«å……çš„æ ‡è®°ï¼Œä¾‹å¦‚åœ¨æ‰¹å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—æ—¶ä½¿ç”¨ã€‚

+   `word_delimiter_token` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"|"`) â€” ç”¨äºå®šä¹‰å•è¯ç»“å°¾çš„æ ‡è®°ã€‚

+   `do_lower_case` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦æ¥å—å°å†™è¾“å…¥å¹¶åœ¨è§£ç æ—¶å°†è¾“å‡ºè½¬æ¢ä¸ºå°å†™ã€‚

+   `target_lang` (`str`, *å¯é€‰*) â€” åˆ†è¯å™¨åº”é»˜è®¤è®¾ç½®çš„ç›®æ ‡è¯­è¨€ã€‚å¯¹äºå¤šè¯­è¨€ã€åµŒå¥—è¯æ±‡è¡¨ï¼Œå¦‚ [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)ï¼Œå¿…é¡»å®šä¹‰ `target_lang`ã€‚

    **kwargs â€” ä¼ é€’ç»™ [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) çš„é¢å¤–å…³é”®å­—å‚æ•°

æ„å»ºä¸€ä¸ª Wav2Vec2CTC åˆ†è¯å™¨ã€‚

è¿™ä¸ªåˆ†è¯å™¨ç»§æ‰¿è‡ª [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒè¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) â†’ export const metadata = 'undefined';BatchEncoding
```

å‚æ•°

+   `text` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½® `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½® `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *å¯é€‰*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–åºåˆ—æ‰¹æ¬¡ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœæä¾›çš„åºåˆ—æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„åˆ†è¯ï¼‰ï¼Œå¿…é¡»è®¾ç½® `is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤ä¸æ‰¹å¤„ç†åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) â€” è¦ç¼–ç ä¸ºç›®æ ‡æ–‡æœ¬çš„åºåˆ—æˆ–æ‰¹é‡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æˆ–ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²ï¼‰ã€‚å¦‚æœåºåˆ—ä»¥å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆé¢„å…ˆæ ‡è®°åŒ–ï¼‰çš„å½¢å¼æä¾›ï¼Œåˆ™å¿…é¡»è®¾ç½®`is_split_into_words=True`ï¼ˆä»¥æ¶ˆé™¤æ‰¹é‡åºåˆ—çš„æ­§ä¹‰ï¼‰ã€‚

+   `add_special_tokens` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” åœ¨ç¼–ç åºåˆ—æ—¶æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°ã€‚è¿™å°†ä½¿ç”¨åº•å±‚çš„`PretrainedTokenizerBase.build_inputs_with_special_tokens`å‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†è‡ªåŠ¨æ·»åŠ åˆ°è¾“å…¥idçš„æ ‡è®°ã€‚å¦‚æœè¦è‡ªåŠ¨æ·»åŠ `bos`æˆ–`eos`æ ‡è®°ï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `padding` (`bool`, `str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶å¡«å……ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest'`ï¼šå¡«å……åˆ°æ‰¹é‡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸å¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦ä¸åŒçš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `truncation` (`bool`, `str` æˆ– [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, é»˜è®¤ä¸º`False`) â€” æ¿€æ´»å’Œæ§åˆ¶æˆªæ–­ã€‚æ¥å—ä»¥ä¸‹å€¼ï¼š

    +   `True` æˆ– `'longest_first'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™å°†é€æ ‡è®°æˆªæ–­ï¼Œä»ä¸€å¯¹åºåˆ—ä¸­æœ€é•¿çš„åºåˆ—ä¸­åˆ é™¤ä¸€ä¸ªæ ‡è®°ã€‚

    +   `'only_first'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬ä¸€ä¸ªåºåˆ—ã€‚

    +   `'only_second'`ï¼šæˆªæ–­åˆ°ç”±å‚æ•°`max_length`æŒ‡å®šçš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æˆªæ–­åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ï¼ˆå¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼‰ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹åºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰ï¼Œåˆ™åªä¼šæˆªæ–­ç¬¬äºŒä¸ªåºåˆ—ã€‚

    +   `False` æˆ– `'do_not_truncate'`ï¼ˆé»˜è®¤ï¼‰ï¼šä¸æˆªæ–­ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºé•¿åº¦å¤§äºæ¨¡å‹æœ€å¤§å¯æ¥å—è¾“å…¥å¤§å°çš„åºåˆ—æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length` (`int`, *optional*) â€” æ§åˆ¶æˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

    å¦‚æœæœªè®¾ç½®æˆ–è®¾ç½®ä¸º`None`ï¼Œåˆ™å°†ä½¿ç”¨é¢„å®šä¹‰çš„æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚æœæˆªæ–­/å¡«å……å‚æ•°ä¹‹ä¸€éœ€è¦æœ€å¤§é•¿åº¦ï¼‰ã€‚å¦‚æœæ¨¡å‹æ²¡æœ‰ç‰¹å®šçš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚XLNetï¼‰ï¼Œåˆ™å°†ç¦ç”¨æˆªæ–­/å¡«å……åˆ°æœ€å¤§é•¿åº¦ã€‚

+   `stride` (`int`, *optional*, é»˜è®¤ä¸º0) â€” å¦‚æœä¸`max_length`ä¸€èµ·è®¾ç½®ä¸ºä¸€ä¸ªæ•°å­—ï¼Œåˆ™å½“`return_overflowing_tokens=True`æ—¶è¿”å›çš„æº¢å‡ºæ ‡è®°å°†åŒ…å«ä»æˆªæ–­åºåˆ—æœ«å°¾è¿”å›çš„ä¸€äº›æ ‡è®°ï¼Œä»¥æä¾›æˆªæ–­å’Œæº¢å‡ºåºåˆ—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚è¯¥å‚æ•°çš„å€¼å®šä¹‰äº†é‡å æ ‡è®°çš„æ•°é‡ã€‚

+   `is_split_into_words` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” è¾“å…¥æ˜¯å¦å·²ç»é¢„æ ‡è®°åŒ–ï¼ˆä¾‹å¦‚ï¼Œå·²åˆ†å‰²ä¸ºå•è¯ï¼‰ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™åˆ†è¯å™¨ä¼šå‡å®šè¾“å…¥å·²ç»åˆ†å‰²ä¸ºå•è¯ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åœ¨ç©ºæ ¼ä¸Šåˆ†å‰²ï¼‰ï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ ‡è®°åŒ–ã€‚è¿™å¯¹äºå‘½åå®ä½“è¯†åˆ«æˆ–æ ‡è®°åˆ†ç±»å¾ˆæœ‰ç”¨ã€‚

+   `pad_to_multiple_of` (`int`, *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†åºåˆ—å¡«å……åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚éœ€è¦æ¿€æ´»`padding`ã€‚è¿™å¯¹äºåœ¨å…·æœ‰è®¡ç®—èƒ½åŠ›`>= 7.5`ï¼ˆVoltaï¼‰çš„NVIDIAç¡¬ä»¶ä¸Šå¯ç”¨Tensor Coresç‰¹åˆ«æœ‰ç”¨ã€‚

+   `return_tensors` (`str`æˆ–[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯Pythonæ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å›TensorFlow `tf.constant`å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å›PyTorch `torch.Tensor`å¯¹è±¡ã€‚

    +   `'np'`: è¿”å›Numpy `np.ndarray`å¯¹è±¡ã€‚

+   `return_token_type_ids` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›token type IDsã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šæ ‡è®°åŒ–å™¨çš„é»˜è®¤å€¼è¿”å›token type IDsï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚

    [ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤è®¾ç½®ï¼Œå°†æ ¹æ®ç‰¹å®šæ ‡è®°åŒ–å™¨çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ï¼Œç”±`return_outputs`å±æ€§å®šä¹‰ã€‚ 

    [ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›æº¢å‡ºçš„tokenåºåˆ—ã€‚å¦‚æœæä¾›äº†ä¸€å¯¹è¾“å…¥idåºåˆ—ï¼ˆæˆ–ä¸€æ‰¹å¯¹ï¼‰å¹¶ä¸”`truncation_strategy = longest_first`æˆ–`True`ï¼Œåˆ™ä¼šå¼•å‘é”™è¯¯ï¼Œè€Œä¸æ˜¯è¿”å›æº¢å‡ºçš„tokensã€‚

+   `return_special_tokens_mask` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›ç‰¹æ®Štokenæ©ç ä¿¡æ¯ã€‚

+   `return_offsets_mapping` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›æ¯ä¸ªtokençš„`(char_start, char_end)`ã€‚

    è¿™ä»…é€‚ç”¨äºç»§æ‰¿è‡ª[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)çš„å¿«é€Ÿæ ‡è®°åŒ–å™¨ï¼Œå¦‚æœä½¿ç”¨Pythonçš„æ ‡è®°åŒ–å™¨ï¼Œæ­¤æ–¹æ³•å°†å¼•å‘`NotImplementedError`ã€‚

+   `return_length` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¿”å›ç¼–ç è¾“å…¥çš„é•¿åº¦ã€‚

+   `verbose` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `True`) â€” æ˜¯å¦æ‰“å°æ›´å¤šä¿¡æ¯å’Œè­¦å‘Šã€‚**kwargs â€” ä¼ é€’ç»™`self.tokenize()`æ–¹æ³•

è¿”å›

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹å­—æ®µçš„[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)ï¼š

+   `input_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„token idåˆ—è¡¨ã€‚

    [ä»€ä¹ˆæ˜¯input IDsï¼Ÿ](../glossary#input-ids)

+   `token_type_ids` â€” è¦æä¾›ç»™æ¨¡å‹çš„token type idsåˆ—è¡¨ï¼ˆå½“`return_token_type_ids=True`æˆ–*`token_type_ids`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    [ä»€ä¹ˆæ˜¯token type IDsï¼Ÿ](../glossary#token-type-ids)

+   `attention_mask` â€” æŒ‡å®šå“ªäº›tokenåº”è¯¥è¢«æ¨¡å‹å…³æ³¨çš„ç´¢å¼•åˆ—è¡¨ï¼ˆå½“`return_attention_mask=True`æˆ–*`attention_mask`*åœ¨`self.model_input_names`ä¸­æ—¶ï¼‰ã€‚

    [ä»€ä¹ˆæ˜¯attention masksï¼Ÿ](../glossary#attention-mask)

+   `overflowing_tokens` â€” æº¢å‡ºçš„tokenåºåˆ—åˆ—è¡¨ï¼ˆå½“æŒ‡å®šäº†`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `num_truncated_tokens` â€” è¢«æˆªæ–­çš„tokenæ•°é‡ï¼ˆå½“æŒ‡å®šäº†`max_length`å¹¶ä¸”`return_overflowing_tokens=True`æ—¶ï¼‰ã€‚

+   `special_tokens_mask` â€” ç”±0å’Œ1ç»„æˆçš„åˆ—è¡¨ï¼Œå…¶ä¸­1æŒ‡å®šæ·»åŠ çš„ç‰¹æ®Štokenï¼Œ0æŒ‡å®šå¸¸è§„åºåˆ—tokenï¼ˆå½“`add_special_tokens=True`ä¸”`return_special_tokens_mask=True`æ—¶ï¼‰ã€‚

+   `length` â€” è¾“å…¥çš„é•¿åº¦ï¼ˆå½“`return_length=True`æ—¶ï¼‰

å°†ä¸»è¦æ–¹æ³•æ ‡è®°åŒ–å¹¶ä¸ºæ¨¡å‹å‡†å¤‡ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—æˆ–ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—å¯¹ã€‚

#### `save_vocabulary`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) â†’ export const metadata = 'undefined';str or Wav2Vec2CTCTokenizerOutput
```

å‚æ•°

+   `token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`) â€” åˆ†è¯åçš„è¾“å…¥idåˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces` (`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚

+   `output_char_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå­—ç¬¦åç§»é‡ã€‚å­—ç¬¦åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å­—ç¬¦çš„æ—¶é—´æˆ³ã€‚

    è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_char_offsets`ã€‚

+   `output_word_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚

    è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`str`æˆ–`Wav2Vec2CTCTokenizerOutput`

è§£ç åçš„å¥å­åˆ—è¡¨ã€‚å½“`output_char_offsets == True`æˆ–`output_word_offsets == True`æ—¶ï¼Œå°†æ˜¯`Wav2Vec2CTCTokenizerOutput`ã€‚

å°†ä¸€ç³»åˆ—idè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨åˆ†è¯å™¨å’Œè¯æ±‡è¡¨ï¼Œå¯ä»¥é€‰æ‹©åˆ é™¤ç‰¹æ®Šæ ‡è®°å¹¶æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚

ç±»ä¼¼äºæ‰§è¡Œ`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`ã€‚

ç¤ºä¾‹ï¼š

```py
>>> # Let's see how to retrieve time steps for a model
>>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("facebook/wav2vec2-base-960h")
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

>>> # load first sample of English common_voice
>>> dataset = load_dataset("mozilla-foundation/common_voice_11_0", "en", split="train", streaming=True)
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
>>> dataset_iter = iter(dataset)
>>> sample = next(dataset_iter)

>>> # forward sample through model to get greedily predicted transcription ids
>>> input_values = feature_extractor(sample["audio"]["array"], return_tensors="pt").input_values
>>> logits = model(input_values).logits[0]
>>> pred_ids = torch.argmax(logits, axis=-1)

>>> # retrieve word stamps (analogous commands for `output_char_offsets`)
>>> outputs = tokenizer.decode(pred_ids, output_word_offsets=True)
>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
>>> time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate

>>> word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:
>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en
>>> word_offsets[:3]
[{'word': 'THE', 'start_time': 0.7, 'end_time': 0.78}, {'word': 'TRICK', 'start_time': 0.88, 'end_time': 1.08}, {'word': 'APPEARS', 'start_time': 1.2, 'end_time': 1.64}]
```

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) â†’ export const metadata = 'undefined';List[str] or Wav2Vec2CTCTokenizerOutput
```

å‚æ•°

+   `sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`) â€” åˆ†è¯åçš„è¾“å…¥idåˆ—è¡¨ã€‚å¯ä»¥ä½¿ç”¨`__call__`æ–¹æ³•è·å¾—ã€‚

+   `skip_special_tokens` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦åœ¨è§£ç ä¸­åˆ é™¤ç‰¹æ®Šæ ‡è®°ã€‚

+   `clean_up_tokenization_spaces` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦æ¸…ç†åˆ†è¯ç©ºæ ¼ã€‚

+   `output_char_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå­—ç¬¦åç§»é‡ã€‚å­—ç¬¦åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å­—ç¬¦çš„æ—¶é—´æˆ³ã€‚

    è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_char_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚

+   `output_word_offsets` (`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚

    è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚

+   `kwargs`ï¼ˆå…¶ä»–å…³é”®å­—å‚æ•°ï¼Œ*å¯é€‰*ï¼‰ â€” å°†ä¼ é€’ç»™åº•å±‚æ¨¡å‹ç‰¹å®šçš„è§£ç æ–¹æ³•ã€‚

è¿”å›

`List[str]`æˆ–`Wav2Vec2CTCTokenizerOutput`

è§£ç åçš„å¥å­åˆ—è¡¨ã€‚å½“`output_char_offsets == True`æˆ–`output_word_offsets == True`æ—¶ï¼Œå°†æ˜¯`Wav2Vec2CTCTokenizerOutput`ã€‚

é€šè¿‡è°ƒç”¨è§£ç å‡½æ•°ï¼Œå°†ä¸€ç³»åˆ—token idçš„åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚

#### `set_target_lang`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)

```py
( target_lang: str )
```

è®¾ç½®åµŒå¥—å¤šè¯­è¨€å­—å…¸çš„ç›®æ ‡è¯­è¨€

## Wav2Vec2FeatureExtractor

### `class transformers.Wav2Vec2FeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)

```py
( feature_size = 1 sampling_rate = 16000 padding_value = 0.0 return_attention_mask = False do_normalize = True **kwargs )
```

å‚æ•°

+   `feature_size`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º1ï¼‰â€” æå–ç‰¹å¾çš„ç‰¹å¾ç»´åº¦ã€‚

+   `sampling_rate`ï¼ˆ`int`ï¼Œé»˜è®¤ä¸º16000ï¼‰â€” åº”ä»¥èµ«å…¹ï¼ˆHzï¼‰è¡¨ç¤ºçš„éŸ³é¢‘æ–‡ä»¶æ•°å­—åŒ–çš„é‡‡æ ·ç‡ã€‚

+   `padding_value`ï¼ˆ`float`ï¼Œé»˜è®¤ä¸º0.0ï¼‰â€” ç”¨äºå¡«å……å€¼çš„å€¼ã€‚

+   `do_normalize`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€” æ˜¯å¦å¯¹è¾“å…¥è¿›è¡Œé›¶å‡å€¼å•ä½æ–¹å·®å½’ä¸€åŒ–ã€‚å½’ä¸€åŒ–å¯ä»¥å¸®åŠ©ä¸€äº›æ¨¡å‹æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¾‹å¦‚ [wav2vec2-lv60](https://huggingface.co/models?search=lv60)ã€‚

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦ [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__) åº”è¯¥è¿”å› `attention_mask`ã€‚

    è®¾ç½®äº† `config.feat_extract_norm == "group"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œæ²¡æœ‰ä½¿ç”¨ `attention_mask` è¿›è¡Œè®­ç»ƒã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……ï¼Œä¸åº”ä¼ é€’ `attention_mask`ã€‚

    å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "layer"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)ï¼Œåº”è¯¥ä¸ºæ‰¹é‡æ¨æ–­ä¼ é€’ `attention_mask`ã€‚

æ„å»ºä¸€ä¸ª Wav2Vec2 ç‰¹å¾æå–å™¨ã€‚

æ­¤ç‰¹å¾æå–å™¨ç»§æ‰¿è‡ª [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)ï¼Œå…¶ä¸­åŒ…å«å¤§å¤šæ•°ä¸»è¦æ–¹æ³•ã€‚ç”¨æˆ·åº”å‚è€ƒæ­¤è¶…ç±»ä»¥è·å–æœ‰å…³è¿™äº›æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)

```py
( raw_speech: Union padding: Union = False max_length: Optional = None truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None sampling_rate: Optional = None **kwargs )
```

å‚æ•°

+   `raw_speech`ï¼ˆ`np.ndarray`ï¼Œ`List[float]`ï¼Œ`List[np.ndarray]`ï¼Œ`List[List[float]]`ï¼‰â€” è¦å¡«å……çš„åºåˆ—æˆ–æ‰¹æ¬¡åºåˆ—ã€‚æ¯ä¸ªåºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ª numpy æ•°ç»„ï¼Œä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨ï¼Œä¸€ä¸ª numpy æ•°ç»„åˆ—è¡¨æˆ–ä¸€ä¸ªæµ®ç‚¹å€¼åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¿…é¡»æ˜¯å•å£°é“éŸ³é¢‘ï¼Œä¸æ˜¯ç«‹ä½“å£°ï¼Œå³æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚

+   `padding`ï¼ˆ`bool`ï¼Œ`str` æˆ– [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” é€‰æ‹©ä¸€ç§ç­–ç•¥æ¥å¡«å……è¿”å›çš„åºåˆ—ï¼ˆæ ¹æ®æ¨¡å‹çš„å¡«å……æ–¹å‘å’Œå¡«å……ç´¢å¼•ï¼‰ï¼ŒåŒ…æ‹¬ï¼š

    +   `True` æˆ– `'longest'`ï¼šå¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—ï¼ˆå¦‚æœåªæä¾›å•ä¸ªåºåˆ—ï¼Œåˆ™ä¸è¿›è¡Œå¡«å……ï¼‰ã€‚

    +   `'max_length'`ï¼šå¡«å……åˆ°æŒ‡å®šå‚æ•° `max_length` çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…å¦‚æœæœªæä¾›è¯¥å‚æ•°ï¼Œåˆ™å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§å¯æ¥å—è¾“å…¥é•¿åº¦ã€‚

    +   `False` æˆ– `'do_not_pad'`ï¼ˆé»˜è®¤ï¼‰ï¼šæ— å¡«å……ï¼ˆå³ï¼Œå¯ä»¥è¾“å‡ºå…·æœ‰ä¸åŒé•¿åº¦åºåˆ—çš„æ‰¹æ¬¡ï¼‰ã€‚

+   `max_length`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” è¿”å›åˆ—è¡¨çš„æœ€å¤§é•¿åº¦å’Œå¯é€‰å¡«å……é•¿åº¦ï¼ˆè§ä¸Šæ–‡ï¼‰ã€‚

+   `truncation`ï¼ˆ`bool`ï¼‰â€” æ¿€æ´»æˆªæ–­ï¼Œå°†è¾“å…¥åºåˆ—æˆªæ–­ä¸ºæ¯” *max_length* æ›´é•¿çš„åºåˆ—åˆ° *max_length*ã€‚

+   `pad_to_multiple_of`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ï¼Œå°†å¡«å……åºåˆ—åˆ°æä¾›çš„å€¼çš„å€æ•°ã€‚

    è¿™å¯¹äºå¯ç”¨ NVIDIA ç¡¬ä»¶ä¸Šçš„ Tensor Cores ç‰¹åˆ«æœ‰ç”¨ï¼Œå…¶è®¡ç®—èƒ½åŠ› `>= 7.5`ï¼ˆVoltaï¼‰ï¼Œæˆ–è€…å¯¹äºå—ç›Šäºåºåˆ—é•¿åº¦ä¸º 128 çš„å€æ•°çš„ TPUã€‚

+   `return_attention_mask`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ³¨æ„åŠ›æ©ç ã€‚å¦‚æœä¿æŒé»˜è®¤å€¼ï¼Œå°†æ ¹æ®ç‰¹å®š feature_extractor çš„é»˜è®¤å€¼è¿”å›æ³¨æ„åŠ›æ©ç ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "group"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œæ²¡æœ‰ä½¿ç”¨ `attention_mask` è¿›è¡Œè®­ç»ƒã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……ï¼Œä¸åº”ä¼ é€’ `attention_mask`ã€‚

    å¯¹äºè®¾ç½®äº† `config.feat_extract_norm == "layer"` çš„ Wav2Vec2 æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)ï¼Œåº”è¯¥ä¸ºæ‰¹é‡æ¨æ–­ä¼ é€’ `attention_mask`ã€‚

+   `return_tensors` (`str` æˆ– [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *å¯é€‰*) â€” å¦‚æœè®¾ç½®ï¼Œå°†è¿”å›å¼ é‡è€Œä¸æ˜¯ Python æ•´æ•°åˆ—è¡¨ã€‚å¯æ¥å—çš„å€¼ä¸ºï¼š

    +   `'tf'`: è¿”å› TensorFlow `tf.constant` å¯¹è±¡ã€‚

    +   `'pt'`: è¿”å› PyTorch `torch.Tensor` å¯¹è±¡ã€‚

    +   `'np'`: è¿”å› Numpy `np.ndarray` å¯¹è±¡ã€‚

+   `sampling_rate` (`int`, *å¯é€‰*) â€” `raw_speech` è¾“å…¥é‡‡æ ·çš„é‡‡æ ·ç‡ã€‚å¼ºçƒˆå»ºè®®åœ¨å‰å‘è°ƒç”¨æ—¶ä¼ é€’ `sampling_rate` ä»¥é˜²æ­¢é™é»˜é”™è¯¯ã€‚

+   `padding_value` (`float`, é»˜è®¤ä¸º 0.0) â€”

å¯¹ä¸€ä¸ªæˆ–å¤šä¸ªåºåˆ—è¿›è¡Œç‰¹å¾åŒ–å’Œä¸ºæ¨¡å‹å‡†å¤‡çš„ä¸»è¦æ–¹æ³•ã€‚

## Wav2Vec2Processor

### `class transformers.Wav2Vec2Processor`

[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)

```py
( feature_extractor tokenizer )
```

å‚æ•°

+   `feature_extractor` (`Wav2Vec2FeatureExtractor`) â€” [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)) â€” [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ª Wav2Vec2 å¤„ç†å™¨ï¼Œå°† Wav2Vec2 ç‰¹å¾æå–å™¨å’Œ Wav2Vec2 CTC åˆ†è¯å™¨å°è£…æˆä¸€ä¸ªå•ä¸€å¤„ç†å™¨ã€‚

[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) æä¾›äº† [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) å’Œ [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) çš„æ‰€æœ‰åŠŸèƒ½ã€‚æŸ¥çœ‹ [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__) å’Œ [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode) çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `__call__`

[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)

```py
( *args **kwargs )
```

åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° Wav2Vec2FeatureExtractor çš„ [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__) å¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ `as_target_processor()` ä¸­ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ° PreTrainedTokenizer çš„ [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ä¸ªæ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `pad`

[`<æ¥æº>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)

```py
( *args **kwargs )
```

åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)å¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡`as_target_processor()`ä¸­ä½¿ç”¨ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `from_pretrained`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)

```py
( pretrained_model_name_or_path **kwargs )
```

#### `save_pretrained`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)

```py
( save_directory push_to_hub: bool = False **kwargs )
```

å‚æ•°

+   `save_directory`ï¼ˆ`str`æˆ–`os.PathLike`ï¼‰â€” ç‰¹å¾æå–å™¨JSONæ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶å°†ä¿å­˜åœ¨çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™å°†åˆ›å»ºç›®å½•ï¼‰ã€‚

+   `push_to_hub`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦åœ¨ä¿å­˜åå°†æ¨¡å‹æ¨é€åˆ°Hugging Faceæ¨¡å‹ä¸­å¿ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨`repo_id`æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“ï¼ˆå°†é»˜è®¤ä¸ºæ‚¨çš„å‘½åç©ºé—´ä¸­çš„`save_directory`åç§°ï¼‰ã€‚

+   `kwargs`ï¼ˆ`Dict[str, Any]`ï¼Œ*å¯é€‰*ï¼‰â€” ä¼ é€’ç»™[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)æ–¹æ³•çš„é¢å¤–å…³é”®å­—å‚æ•°ã€‚

å°†æ­¤å¤„ç†å™¨çš„å±æ€§ï¼ˆç‰¹å¾æå–å™¨ã€åˆ†è¯å™¨ç­‰ï¼‰ä¿å­˜åœ¨æŒ‡å®šç›®å½•ä¸­ï¼Œä»¥ä¾¿å¯ä»¥ä½¿ç”¨[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)æ–¹æ³•é‡æ–°åŠ è½½ã€‚

è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)å’Œ[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `batch_decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)

```py
( *args **kwargs )
```

æ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°PreTrainedTokenizerçš„[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒæ­¤æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

## Wav2Vec2ProcessorWithLM

### `class transformers.Wav2Vec2ProcessorWithLM`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)

```py
( feature_extractor: FeatureExtractionMixin tokenizer: PreTrainedTokenizerBase decoder: BeamSearchDecoderCTC )
```

å‚æ•°

+   `feature_extractor`ï¼ˆ[Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractorï¼‰ï¼‰â€” [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)çš„ä¸€ä¸ªå®ä¾‹ã€‚ç‰¹å¾æå–å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `tokenizer`ï¼ˆ[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizerï¼‰ï¼‰â€” [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)çš„ä¸€ä¸ªå®ä¾‹ã€‚åˆ†è¯å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

+   `decoder`ï¼ˆ`pyctcdecode.BeamSearchDecoderCTC`ï¼‰â€” `pyctcdecode.BeamSearchDecoderCTC`çš„ä¸€ä¸ªå®ä¾‹ã€‚è§£ç å™¨æ˜¯å¿…éœ€çš„è¾“å…¥ã€‚

æ„å»ºä¸€ä¸ªWav2Vec2å¤„ç†å™¨ï¼Œå°†Wav2Vec2ç‰¹å¾æå–å™¨ã€Wav2Vec2 CTCåˆ†è¯å™¨å’Œå…·æœ‰è¯­è¨€æ¨¡å‹æ”¯æŒçš„è§£ç å™¨åŒ…è£…åˆ°ä¸€ä¸ªå•ä¸€çš„å¤„ç†å™¨ä¸­ï¼Œç”¨äºè¯­è¨€æ¨¡å‹å¢å¼ºçš„è¯­éŸ³è¯†åˆ«è§£ç ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)

```py
( *args **kwargs )
```

åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)ï¼Œå¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨`as_target_processor()`ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2CTCTokenizerçš„[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `pad`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)

```py
( *args **kwargs )
```

åœ¨æ­£å¸¸æ¨¡å¼ä¸‹ä½¿ç”¨æ—¶ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2FeatureExtractorçš„[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)ï¼Œå¹¶è¿”å›å…¶è¾“å‡ºã€‚å¦‚æœåœ¨ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨`as_target_processor()`ï¼Œæ­¤æ–¹æ³•å°†æ‰€æœ‰å‚æ•°è½¬å‘åˆ°Wav2Vec2CTCTokenizerçš„[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)ã€‚è¯·å‚è€ƒä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `from_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)

```py
( pretrained_model_name_or_path **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path` (`str` or `os.PathLike`) â€” å¯ä»¥æ˜¯ï¼š

    +   é¢„è®­ç»ƒç‰¹å¾æå–å™¨çš„*æ¨¡å‹ID*çš„å­—ç¬¦ä¸²ï¼Œæ‰˜ç®¡åœ¨huggingface.coä¸Šçš„æ¨¡å‹å­˜å‚¨åº“ä¸­ã€‚æœ‰æ•ˆçš„æ¨¡å‹IDå¯ä»¥ä½äºæ ¹çº§åˆ«ï¼Œå¦‚`bert-base-uncased`ï¼Œæˆ–è€…åœ¨ç”¨æˆ·æˆ–ç»„ç»‡åç§°ä¸‹å‘½åç©ºé—´åŒ–ï¼Œå¦‚`dbmdz/bert-base-german-cased`ã€‚

    +   ä¸€ä¸ªåŒ…å«ä½¿ç”¨[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)æ–¹æ³•ä¿å­˜çš„ç‰¹å¾æå–å™¨æ–‡ä»¶çš„*ç›®å½•*è·¯å¾„ï¼Œä¾‹å¦‚`./my_model_directory/`ã€‚

    +   ä»é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨JSONæ–‡ä»¶çš„è·¯å¾„æˆ–URLï¼Œä¾‹å¦‚`./my_model_directory/preprocessor_config.json`ã€‚**kwargs â€” ä¼ é€’ç»™[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)å’Œ[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)çš„é¢å¤–å…³é”®å­—å‚æ•°

ä»é¢„è®­ç»ƒçš„Wav2Vec2å¤„ç†å™¨å®ä¾‹åŒ–ä¸€ä¸ª[Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)ã€‚

è¿™ä¸ªç±»æ–¹æ³•åªæ˜¯è°ƒç”¨äº†Wav2Vec2FeatureExtractorçš„[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)ï¼ŒWav2Vec2CTCTokenizerçš„[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)ï¼Œä»¥åŠ`pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`ã€‚

è¯·å‚è€ƒä¸Šè¿°æ–¹æ³•çš„æ–‡æ¡£å­—ç¬¦ä¸²ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

#### `save_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)

```py
( save_directory )
```

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)

```py
( logits: ndarray pool: Optional = None num_processes: Optional = None beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )
```

å‚æ•°

+   `logits` (`np.ndarray`) â€” æ¨¡å‹è¾“å‡ºçš„logitså‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ã€‚

+   `pool` (`multiprocessing.Pool`, *optional*) â€” å¯é€‰çš„ç”¨æˆ·ç®¡ç†çš„æ± ã€‚å¦‚æœæœªè®¾ç½®ï¼Œå°†è‡ªåŠ¨åˆ›å»ºå¹¶å…³é—­ä¸€ä¸ªæ± ã€‚æ± åº”åœ¨`Wav2Vec2ProcessorWithLM`ä¹‹åå®ä¾‹åŒ–ã€‚å¦åˆ™ï¼ŒLMå°†ä¸å¯ç”¨äºæ± çš„å­è¿›ç¨‹ã€‚

    ç›®å‰ï¼Œåªæœ‰ä½¿ç”¨â€œforkâ€ä¸Šä¸‹æ–‡åˆ›å»ºçš„æ± æ‰èƒ½ä½¿ç”¨ã€‚å¦‚æœä¼ é€’äº†â€œspawnâ€æ± ï¼Œå®ƒå°†è¢«å¿½ç•¥ï¼Œè€Œå°†ä½¿ç”¨é¡ºåºè§£ç ã€‚

+   `num_processes` (`int`, *optional*) â€” å¦‚æœæœªè®¾ç½®`pool`ï¼Œåˆ™åº”è¯¥åœ¨å“ªäº›è¿›ç¨‹ä¸Šå¹¶è¡ŒåŒ–å‡½æ•°ã€‚é»˜è®¤ä¸ºå¯ç”¨CPUçš„æ•°é‡ã€‚

+   `beam_width` (`int`, *optional*) â€” è§£ç è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„æœ€å¤§beamæ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_BEAM_WIDTHã€‚

+   `beam_prune_logp` (`int`, *optional*) â€” æ¯”æœ€ä½³beamå·®å¾ˆå¤šçš„beamå°†è¢«ä¿®å‰ªã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_PRUNE_LOGPã€‚

+   `token_min_logp` (`int`, *optional*) â€” ä½äºæ­¤logpçš„æ ‡è®°å°†è¢«è·³è¿‡ï¼Œé™¤éå®ƒä»¬æ˜¯å¸§çš„argmaxã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_MIN_TOKEN_LOGPã€‚

+   `hotwords` (`List[str]`, *optional*) â€” å…·æœ‰é¢å¤–é‡è¦æ€§çš„å•è¯åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯LMçš„OOV

+   `hotword_weight` (`int`, *optional*) â€” çƒ­è¯é‡è¦æ€§çš„æƒé‡å› å­ï¼Œé»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_HOTWORD_WEIGHTã€‚

+   `alpha` (`float`, *optional*) â€” æµ…èåˆæœŸé—´è¯­è¨€æ¨¡å‹çš„æƒé‡

+   `beta` (`float`, *optional*) â€” åœ¨è¯„åˆ†è¿‡ç¨‹ä¸­é•¿åº¦å¾—åˆ†è°ƒæ•´çš„æƒé‡

+   `unk_score_offset` (`float`, *optional*) â€” æœªçŸ¥æ ‡è®°çš„å¯¹æ•°åˆ†æ•°åç§»é‡

+   `lm_score_boundary` (`bool`, *optional*) â€” åœ¨è¯„åˆ†æ—¶æ˜¯å¦è®©kenlmå°Šé‡è¾¹ç•Œ

+   `output_word_offsets` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œä»¥è®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚

+   `n_best` (`int`, *optional*, é»˜è®¤ä¸º`1`) â€” è¦è¿”å›çš„æœ€ä½³å‡è®¾æ•°é‡ã€‚å¦‚æœ`n_best`å¤§äº1ï¼Œåˆ™è¿”å›çš„`text`å°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨çš„åˆ—è¡¨ï¼Œ`logit_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨çš„åˆ—è¡¨ï¼Œ`lm_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨çš„åˆ—è¡¨ï¼Œå¤–éƒ¨åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”æ‰¹æ¬¡å¤§å°ï¼Œå†…éƒ¨åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”è¿”å›çš„å‡è®¾æ•°é‡ã€‚è¯¥å€¼åº” >= 1ã€‚

    è¯·æŸ¥çœ‹[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)ä¸æ‰¹é‡è¾“å‡ºçš„æ–¹å¼ç›¸åŒã€‚

æ‰¹é‡è§£ç è¾“å‡ºlogitsä»¥æ”¯æŒè¯­è¨€æ¨¡å‹çš„éŸ³é¢‘è½¬å½•ã€‚

æ­¤å‡½æ•°åˆ©ç”¨äº†Pythonçš„å¤šè¿›ç¨‹ã€‚ç›®å‰ï¼Œå¤šè¿›ç¨‹ä»…åœ¨Unixç³»ç»Ÿä¸Šå¯ç”¨ï¼ˆè¯·å‚é˜…æ­¤[é—®é¢˜](https://github.com/kensho-technologies/pyctcdecode/issues/65)ï¼‰ã€‚

å¦‚æœæ‚¨æ­£åœ¨è§£ç å¤šä¸ªæ‰¹æ¬¡ï¼Œè¯·è€ƒè™‘åˆ›å»ºä¸€ä¸ª`Pool`å¹¶å°†å…¶ä¼ é€’ç»™`batch_decode`ã€‚å¦åˆ™ï¼Œ`batch_decode`å°†éå¸¸æ…¢ï¼Œå› ä¸ºå®ƒå°†ä¸ºæ¯æ¬¡è°ƒç”¨åˆ›å»ºä¸€ä¸ªæ–°çš„`Pool`ã€‚è¯·å‚è§ä¸‹é¢çš„ç”¨æ³•ç¤ºä¾‹ã€‚

ç¤ºä¾‹ï¼šè¯·å‚è§[è§£ç å¤šä¸ªéŸ³é¢‘](#decoding-multiple-audios)ã€‚

#### `decode`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)

```py
( logits: ndarray beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )
```

å‚æ•°

+   `logits` (`np.ndarray`) â€” ä»£è¡¨æ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡çš„æ¨¡å‹è¾“å‡ºå‘é‡ã€‚

+   `beam_width` (`int`, *optional*) â€” è§£ç è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„æœ€å¤§beamæ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_BEAM_WIDTHã€‚

+   `beam_prune_logp` (`int`, *optional*) â€” ä¸€ä¸ªç”¨äºä¿®å‰ªlog-probså°äºbest_beam_logp + beam_prune_logpçš„é˜ˆå€¼ã€‚è¯¥å€¼åº” <= 0ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_PRUNE_LOGPã€‚

+   `token_min_logp`ï¼ˆ`int`ï¼Œ*optional*ï¼‰ â€” log-probsä½äºtoken_min_logpçš„æ ‡è®°å°†è¢«è·³è¿‡ï¼Œé™¤éå®ƒä»¬æ˜¯è¯è¯­çš„æœ€å¤§log-probã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_MIN_TOKEN_LOGPã€‚

+   `hotwords`ï¼ˆ`List[str]`ï¼Œ*optional*ï¼‰ â€” å…·æœ‰é¢å¤–é‡è¦æ€§çš„å•è¯åˆ—è¡¨ï¼Œå¯èƒ½ä¸åœ¨LMçš„è¯æ±‡è¡¨ä¸­ï¼Œä¾‹å¦‚[â€œhuggingfaceâ€]

+   `hotword_weight`ï¼ˆ`int`ï¼Œ*optional*ï¼‰ â€” å¢å¼ºçƒ­è¯åˆ†æ•°çš„æƒé‡ä¹˜æ•°ã€‚é»˜è®¤ä¸ºpyctcdecodeçš„DEFAULT_HOTWORD_WEIGHTã€‚

+   `alpha`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” æµ…èåˆæœŸé—´è¯­è¨€æ¨¡å‹çš„æƒé‡

+   `beta`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” åœ¨è¯„åˆ†è¿‡ç¨‹ä¸­é•¿åº¦åˆ†æ•°è°ƒæ•´çš„æƒé‡

+   `unk_score_offset`ï¼ˆ`float`ï¼Œ*optional*ï¼‰ â€” æœªçŸ¥æ ‡è®°çš„logåˆ†æ•°åç§»é‡

+   `lm_score_boundary` (`bool`, *optional*) â€” æ˜¯å¦åœ¨è¯„åˆ†æ—¶è®©kenlmå°Šé‡è¾¹ç•Œ

+   `output_word_offsets` (`bool`, *optional*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦è¾“å‡ºå•è¯åç§»é‡ã€‚å•è¯åç§»é‡å¯ä»¥ä¸é‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ç»“åˆä½¿ç”¨ï¼Œè®¡ç®—è½¬å½•å•è¯çš„æ—¶é—´æˆ³ã€‚

+   `n_best`ï¼ˆ`int`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º`1`ï¼‰ â€” è¦è¿”å›çš„æœ€ä½³å‡è®¾æ•°é‡ã€‚å¦‚æœ`n_best`å¤§äº1ï¼Œåˆ™è¿”å›çš„`text`å°†æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œ`logit_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œ`lm_score`å°†æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°åˆ—è¡¨ï¼Œè¿™äº›åˆ—è¡¨çš„é•¿åº¦å°†å¯¹åº”äºè¿”å›çš„å‡è®¾æ•°é‡ã€‚è¯¥å€¼åº”å¤§äºç­‰äº1ã€‚

    è¯·æŸ¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä½¿ç”¨`output_word_offsets`ã€‚

ä½¿ç”¨è¯­è¨€æ¨¡å‹æ”¯æŒå°†è¾“å‡ºé€»è¾‘è§£ç ä¸ºéŸ³é¢‘è½¬å½•ã€‚

ç¤ºä¾‹ï¼š

```py
>>> # Let's see how to retrieve time steps for a model
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # load first sample of English common_voice
>>> dataset = load_dataset("mozilla-foundation/common_voice_11_0", "en", split="train", streaming=True)
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
>>> dataset_iter = iter(dataset)
>>> sample = next(dataset_iter)

>>> # forward sample through model to get greedily predicted transcription ids
>>> input_values = processor(sample["audio"]["array"], return_tensors="pt").input_values
>>> with torch.no_grad():
...     logits = model(input_values).logits[0].cpu().numpy()

>>> # retrieve word stamps (analogous commands for `output_char_offsets`)
>>> outputs = processor.decode(logits, output_word_offsets=True)
>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
>>> time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate

>>> word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:
>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en
>>> word_offsets[:4]
[{'word': 'THE', 'start_time': 0.68, 'end_time': 0.78}, {'word': 'TRACK', 'start_time': 0.88, 'end_time': 1.1}, {'word': 'APPEARS', 'start_time': 1.18, 'end_time': 1.66}, {'word': 'ON', 'start_time': 1.86, 'end_time': 1.92}]
```

### è§£ç å¤šä¸ªéŸ³é¢‘

å¦‚æœæ‚¨è®¡åˆ’è§£ç å¤šæ‰¹éŸ³é¢‘ï¼Œåº”è€ƒè™‘ä½¿ç”¨[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)å¹¶ä¼ é€’ä¸€ä¸ªå®ä¾‹åŒ–çš„`multiprocessing.Pool`ã€‚å¦åˆ™ï¼Œ[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)çš„æ€§èƒ½å°†æ¯”ä¸ºæ¯ä¸ªéŸ³é¢‘å•ç‹¬è°ƒç”¨[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)æ…¢ï¼Œå› ä¸ºå®ƒåœ¨æ¯æ¬¡è°ƒç”¨æ—¶å†…éƒ¨å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„`Pool`ã€‚è¯·å‚é˜…ä¸‹é¢çš„ç¤ºä¾‹ï¼š

```py
>>> # Let's see how to use a user-managed pool for batch decoding multiple audios
>>> from multiprocessing import get_context
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm").to("cuda")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # load example dataset
>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))

>>> def map_to_array(batch):
...     batch["speech"] = batch["audio"]["array"]
...     return batch

>>> # prepare speech data for batch inference
>>> dataset = dataset.map(map_to_array, remove_columns=["audio"])

>>> def map_to_pred(batch, pool):
...     inputs = processor(batch["speech"], sampling_rate=16_000, padding=True, return_tensors="pt")
...     inputs = {k: v.to("cuda") for k, v in inputs.items()}

...     with torch.no_grad():
...         logits = model(**inputs).logits

...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text
...     batch["transcription"] = transcription
...     return batch

>>> # note: pool should be instantiated *after* `Wav2Vec2ProcessorWithLM`.
>>> #       otherwise, the LM won't be available to the pool's sub-processes
>>> # select number of processes and batch_size based on number of CPU cores available and on dataset size
>>> with get_context("fork").Pool(processes=2) as pool:
...     result = dataset.map(
...         map_to_pred, batched=True, batch_size=2, fn_kwargs={"pool": pool}, remove_columns=["speech"]
...     )

>>> result["transcription"][:2]
['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', "NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER"]
```

## Wav2Vec2ç‰¹å®šè¾“å‡º

### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)

```py
( text: Union logit_score: Union = None lm_score: Union = None word_offsets: Union = None )
```

å‚æ•°

+   `text`ï¼ˆ`str`åˆ—è¡¨æˆ–`str`ï¼‰ â€” æ–‡æœ¬ä¸­çš„è§£ç é€»è¾‘ã€‚é€šå¸¸æ˜¯è¯­éŸ³è½¬å½•ã€‚

+   `logit_score`ï¼ˆ`float`åˆ—è¡¨æˆ–`float`ï¼‰ â€” ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„beamçš„æ€»logitåˆ†æ•°ã€‚

+   `lm_score`ï¼ˆ`float`åˆ—è¡¨ï¼‰ â€” ä¸ç”Ÿæˆæ–‡æœ¬ç›¸å…³çš„beamçš„èåˆlm_scoreã€‚

+   `word_offsets`ï¼ˆ`List[Dict[str, Union[int, str]]]`æˆ–`List[Dict[str, Union[int, str]]]`åˆ—è¡¨ â€” è§£ç å•è¯çš„åç§»é‡ã€‚ç»“åˆé‡‡æ ·ç‡å’Œæ¨¡å‹ä¸‹é‡‡æ ·ç‡ï¼Œå•è¯åç§»é‡å¯ç”¨äºè®¡ç®—æ¯ä¸ªå•è¯çš„æ—¶é—´æˆ³ã€‚

`Wav2Vec2DecoderWithLM`çš„è¾“å‡ºç±»å‹ï¼Œå¸¦æœ‰è½¬å½•ã€‚

### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)

```py
( last_hidden_state: FloatTensor = None extract_features: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼‰ â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `extract_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, conv_dim[-1])`çš„`torch.FloatTensor`ï¼‰ â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚çš„æå–ç‰¹å¾å‘é‡åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

ä½¿ç”¨Wav2Vec2æŸå¤±ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹çš„åŸºç±»ã€‚

### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)

```py
( loss: Optional = None projected_states: FloatTensor = None projected_quantized_states: FloatTensor = None codevector_perplexity: FloatTensor = None hidden_states: Optional = None attentions: Optional = None contrastive_loss: Optional = None diversity_loss: Optional = None )
```

å‚æ•°

+   `loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor` of shape `(1,)`) â€” æ€»æŸå¤±ï¼Œç”±å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„å’Œç»„æˆï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚ ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚

+   `projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`) â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚

+   `projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`) â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡åºåˆ—ï¼ŒæŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œä»£è¡¨å¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `contrastive_loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor` of shape `(1,)`) â€” å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚

+   `diversity_loss` (*å¯é€‰*, å½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›, `torch.FloatTensor` of shape `(1,)`) â€” å¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚

[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚

### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)

```py
( last_hidden_state: Array = None extract_features: Array = None hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`) â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ï¼Œå…¶ä¸­`last_conv_dim`æ˜¯æœ€åä¸€ä¸ªå·ç§¯å±‚çš„ç»´åº¦ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWav2Vec2BaseModelOutput`çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚

#### `replace`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚

### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)

```py
( projected_states: Array = None projected_quantized_states: Array = None codevector_perplexity: Array = None hidden_states: Optional = None attentions: Optional = None )
```

å‚æ•°

+   `loss` (*å¯é€‰*, å½“æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`jnp.ndarray`) â€” æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚ (åˆ†ç±») æŸå¤±ã€‚

+   `projected_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰ â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚

+   `projected_quantized_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰ â€” æŠ•å½±åˆ°*config.proj_codevector_dim*çš„é‡åŒ–æå–ç‰¹å¾å‘é‡ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWav2Vec2ForPreTrainingOutput`çš„è¾“å‡ºç±»å‹ï¼Œå…·æœ‰æ½œåœ¨çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›ã€‚

#### `replace`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚

PytorchHide Pytorchå†…å®¹

## Wav2Vec2Model

### `class transformers.Wav2Vec2Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)

```py
( config: Wav2Vec2Config )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ Wav2Vec2 æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡¶éƒ¨å¤´ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ° `List[float]` ç±»å‹çš„æ•°ç»„æˆ– `numpy.ndarray` ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_values`ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º `torch.FloatTensor` ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´:

    +   1 ç”¨äº `æœªè¢«æ©ç ` çš„æ ‡è®°ï¼Œ

    +   0 ç”¨äº `è¢«æ©ç ` çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == False` çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶åº” `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values` åº”ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask` çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—è¾“å‡ºã€‚

+   `extract_features` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, conv_dim[-1])`) â€” æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚æå–çš„ç‰¹å¾å‘é‡åºåˆ—ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰çš„*, å½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰çš„*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯ä¸ªå±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) çš„ forward æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œå‰å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, Wav2Vec2Model
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 292, 768]
```

## Wav2Vec2ForCTC

### `class transformers.Wav2Vec2ForCTC`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)

```py
( config target_lang: Optional = None )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” æ¨¡å‹çš„æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•æ¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `target_lang` (`str`ï¼Œ*å¯é€‰çš„*) â€” é€‚é…å™¨æƒé‡çš„è¯­è¨€ idã€‚é€‚é…å™¨æƒé‡å­˜å‚¨åœ¨æ ¼å¼ä¸º adapter.<lang>.safetensors æˆ– adapter.<lang>.bin çš„æ–‡ä»¶ä¸­ã€‚ä»…åœ¨ä½¿ç”¨å¸¦æœ‰é€‚é…å™¨çš„ [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) å®ä¾‹æ—¶ç›¸å…³ã€‚é»˜è®¤ä½¿ç”¨ â€˜engâ€™ã€‚</lang></lang>

å¸¦æœ‰é¡¶éƒ¨ `è¯­è¨€å»ºæ¨¡` çš„ Wav2Vec2 æ¨¡å‹ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]` æˆ– `numpy.ndarray` çš„æ•°ç»„ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡ soundfile åº“ (`pip install soundfile`)ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä¸º `input_values`ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º `torch.FloatTensor` ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«é®è”½çš„æ ‡è®°ï¼Œä¸º1ï¼Œ

    +   å¯¹äºè¢«é®è”½çš„æ ‡è®°ï¼Œä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹è¿˜ä¼šæ ¹æ®`input_values`æ˜¯å¦å¡«å……è€Œäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸­çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *å¯é€‰*) â€” è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»çš„æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œ`target_length`å¿…é¡»å°äºæˆ–ç­‰äºè¾“å‡ºlogitsçš„åºåˆ—é•¿åº¦ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[-100, 0, ..., config.vocab_size - 1]`ã€‚æ‰€æœ‰è®¾ç½®ä¸º`-100`çš„æ ‡ç­¾éƒ½è¢«å¿½ç•¥ï¼ˆé®è”½ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—æ ‡ç­¾åœ¨`[0, ..., config.vocab_size - 1]`ä¸­çš„æƒ…å†µã€‚

è¿”å›

[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *å¯é€‰*, å½“æä¾›`labels`æ—¶è¿”å›) â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹ï¼‰ã€‚

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_hidden_states=True`æˆ–å½“`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹å…·æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º+æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *å¯é€‰*, å½“ä¼ é€’`output_attentions=True`æˆ–å½“`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoProcessor, Wav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
>>> predicted_ids = torch.argmax(logits, dim=-1)

>>> # transcribe speech
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription[0]
'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'

>>> inputs["labels"] = processor(text=dataset[0]["text"], return_tensors="pt").input_ids

>>> # compute loss
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
53.48
```

#### `load_adapter`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)

```py
( target_lang: str force_load = True **kwargs )
```

å‚æ•°

+   `target_lang` (`str`) â€” å¿…é¡»æ˜¯ç°æœ‰é€‚é…å™¨æƒé‡çš„è¯­è¨€ IDã€‚é€‚é…å™¨æƒé‡å­˜å‚¨åœ¨æ ¼å¼ adapter.<lang>.safetensors æˆ– adapter.<lang>.bin</lang></lang>ã€‚

+   `force_load` (`bool`, é»˜è®¤ä¸º `True`) â€” å³ä½¿ `target_lang` ä¸ `self.target_lang` åŒ¹é…ï¼Œä¹Ÿè¦åŠ è½½æƒé‡ã€‚

+   `cache_dir` (`Union[str, os.PathLike]`, *å¯é€‰*) â€” ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®åº”è¯¥ç¼“å­˜åœ¨å…¶ä¸­çš„ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ä½¿ç”¨æ ‡å‡†ç¼“å­˜ã€‚

+   `force_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦åˆ é™¤æ¥æ”¶ä¸å®Œæ•´çš„æ–‡ä»¶ã€‚å¦‚æœå­˜åœ¨è¿™æ ·çš„æ–‡ä»¶ï¼Œå°†å°è¯•æ¢å¤ä¸‹è½½ã€‚

+   `proxies` (`Dict[str, str]`, *å¯é€‰*) â€” ä¸€ä¸ªæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨çš„ä»£ç†æœåŠ¡å™¨å­—å…¸ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚æ¯ä¸ªè¯·æ±‚éƒ½ä¼šä½¿ç”¨ä»£ç†ã€‚

+   `local_files_only(bool,` *å¯é€‰*, é»˜è®¤ä¸º `False`) â€” æ˜¯å¦ä»…æŸ¥çœ‹æœ¬åœ°æ–‡ä»¶ï¼ˆå³ä¸å°è¯•ä¸‹è½½æ¨¡å‹ï¼‰ã€‚

+   `token` (`str` æˆ– `bool`, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„ HTTP bearer æˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º `True`ï¼Œæˆ–æœªæŒ‡å®šï¼Œå°†ä½¿ç”¨è¿è¡Œ `huggingface-cli login` æ—¶ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨ `~/.huggingface` ä¸­ï¼‰ã€‚

+   `revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º `"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å®ƒå¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°æˆ–æäº¤ IDï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ huggingface.co ä¸Šä½¿ç”¨åŸºäº git çš„ç³»ç»Ÿå­˜å‚¨æ¨¡å‹å’Œå…¶ä»–å·¥ä»¶ï¼Œæ‰€ä»¥ `revision` å¯ä»¥æ˜¯ git å…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

    è¦æµ‹è¯•æ‚¨åœ¨ Hub ä¸Šæäº¤çš„æ‹‰å–è¯·æ±‚ï¼Œå¯ä»¥ä¼ é€’ `revision=â€œrefs/pr/<pr_number>â€œã€‚</pr_number>

+   `mirror` (`str`, *å¯é€‰*) â€” å°†æºé•œåƒåˆ°ä¸­å›½ä»¥åŠ é€Ÿä¸‹è½½ã€‚å¦‚æœæ‚¨æ¥è‡ªä¸­å›½å¹¶ä¸”æœ‰è®¿é—®é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®æ­¤é€‰é¡¹ä»¥è§£å†³é—®é¢˜ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ä¿è¯åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ã€‚è¯·å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

ä»é¢„è®­ç»ƒçš„é€‚é…å™¨æ¨¡å‹åŠ è½½è¯­è¨€é€‚é…å™¨æ¨¡å‹ã€‚

æ¿€æ´»ç‰¹æ®Šçš„[â€œç¦»çº¿æ¨¡å¼â€](https://huggingface.co/transformers/installation.html#offline-mode)ä»¥åœ¨é˜²ç«å¢™ç¯å¢ƒä¸­ä½¿ç”¨æ­¤æ–¹æ³•ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import Wav2Vec2ForCTC, AutoProcessor

>>> ckpt = "facebook/mms-1b-all"
>>> processor = AutoProcessor.from_pretrained(ckpt)
>>> model = Wav2Vec2ForCTC.from_pretrained(ckpt, target_lang="eng")
>>> # set specific language
>>> processor.tokenizer.set_target_lang("spa")
>>> model.load_adapter("spa")
```

## Wav2Vec2ForSequenceClassification

### `class transformers.Wav2Vec2ForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)

```py
( config )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

åœ¨é¡¶éƒ¨æ·»åŠ äº†ä¸€ä¸ªåºåˆ—åˆ†ç±»å¤´çš„ Wav2Vec2 æ¨¡å‹ï¼ˆä¸€ä¸ªçº¿æ€§å±‚åœ¨æ± åŒ–è¾“å‡ºä¸Šæ–¹ï¼‰ç”¨äº SUPERB å…³é”®è¯è¯†åˆ«ç­‰ä»»åŠ¡ã€‚

Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰çš„ä¿¡æ¯ã€‚

è¯¥æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡å£°éŸ³æ–‡ä»¶åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡ä¸º`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`èŒƒå›´å†…ï¼š

    +   å¯¹äºæœªè¢«æ©ç çš„æ ‡è®°ä¸º`1`ï¼Œ

    +   å¯¹äºè¢«æ©ç çš„æ ‡è®°ä¸º`0`ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶æ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹è¿˜ä¼šæ ¹æ®`input_values`æ˜¯å¦å¡«å……è€Œäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `output_attentions` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`ï¼Œ*å¯é€‰*) â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º`(batch_size,)`ï¼Œ*å¯é€‰*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º`(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›) â€” åˆ†ç±»ï¼ˆæˆ–å¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰æŸå¤±ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, config.num_labels)`çš„`torch.FloatTensor`ï¼‰- åˆ†ç±»ï¼ˆå¦‚æœ`config.num_labels==1`åˆ™ä¸ºå›å½’ï¼‰åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸ªå±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("superb/wav2vec2-base-superb-ks")
>>> model = Wav2Vec2ForSequenceClassification.from_pretrained("superb/wav2vec2-base-superb-ks")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'_unknown_'

>>> # compute loss - target_label is e.g. "down"
>>> target_label = model.config.id2label[0]
>>> inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
6.54
```

## Wav2Vec2ForAudioFrameClassification

### `class transformers.Wav2Vec2ForAudioFrameClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚ è¯·æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

å¸¦æœ‰é¡¶éƒ¨å¸§åˆ†ç±»å¤´çš„Wav2Vec2æ¨¡å‹ï¼Œç”¨äºSpeaker Diarizationç­‰ä»»åŠ¡ã€‚

Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliåœ¨[wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚ æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹æ˜¯PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚ å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)

```py
( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚ å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚ é€šè¿‡å£°éŸ³æ–‡ä»¶åº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚ è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å¹¶è½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚ æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚ é€‰æ‹©çš„æ©ç å€¼åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºâ€œæœªå±è”½â€çš„æ ‡è®°ï¼Œ

    +   å¯¹äºè¢« `masked` çš„æ ‡è®°ä¸º 0ã€‚

    [æ³¨æ„åŠ›æ©ç æ˜¯ä»€ä¹ˆï¼Ÿ](../glossary#attention-mask)

    åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == False` çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº” `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask` çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å› [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels` (`torch.LongTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size,)`ï¼Œ*optional*) â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨ `[0, ..., config.num_labels - 1]` èŒƒå›´å†…ã€‚å¦‚æœ `config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ `config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ä¸åŒå…ƒç´ ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*optional*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›) â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.num_labels)`) â€” åˆ†ç±»åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰ï¼‰ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚çš„è¾“å‡ºï¼Œåˆ™ä¸ºåµŒå…¥å±‚çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForAudioFrameClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("anton-l/wav2vec2-base-superb-sd")
>>> model = Wav2Vec2ForAudioFrameClassification.from_pretrained("anton-l/wav2vec2-base-superb-sd")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt", sampling_rate=sampling_rate)
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> probabilities = torch.sigmoid(logits[0])
>>> # labels is a one-hot array of shape (num_frames, num_speakers)
>>> labels = (probabilities > 0.5).long()
>>> labels[0].tolist()
[0, 0]
```

## Wav2Vec2ForXVector

### `class transformers.Wav2Vec2ForXVector`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Wav2Vec2æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰XVectorç‰¹å¾æå–å¤´ï¼Œç”¨äºSpeaker Verificationç­‰ä»»åŠ¡ã€‚

Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliæå‡ºçš„[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯PyTorchçš„[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„PyTorchæ¨¡å—ï¼Œå¹¶å‚è€ƒPyTorchæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.XVectorOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä»¥è·å¾—`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©åœ¨`[0, 1]`èŒƒå›´å†…çš„æ©ç å€¼ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    åªæœ‰åœ¨ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶æ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶åº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”ç®€å•åœ°å¡«å……ä¸º0å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹çš„ç»“æœä¹Ÿä¼šå› `input_values`æ˜¯å¦å¡«å……è€Œç•¥æœ‰ä¸åŒã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size,)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—åºåˆ—åˆ†ç±»/å›å½’æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[0, ..., config.num_labels - 1]`èŒƒå›´å†…ã€‚å¦‚æœ`config.num_labels == 1`ï¼Œåˆ™è®¡ç®—å›å½’æŸå¤±ï¼ˆå‡æ–¹æŸå¤±ï¼‰ï¼Œå¦‚æœ`config.num_labels > 1`ï¼Œåˆ™è®¡ç®—åˆ†ç±»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰ã€‚

è¿”å›

[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput) æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(1,)`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾› `labels` æ—¶è¿”å›ï¼‰â€” åˆ†ç±»æŸå¤±ã€‚

+   `logits` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.xvector_output_dim)`) â€” AMSoftmax ä¹‹å‰çš„åˆ†ç±»éšè—çŠ¶æ€ã€‚

+   `embeddings` (`torch.FloatTensor`ï¼Œå½¢çŠ¶ä¸º `(batch_size, config.xvector_output_dim)`) â€” ç”¨äºåŸºäºå‘é‡ç›¸ä¼¼æ€§çš„æ£€ç´¢çš„è¯è¯­åµŒå…¥ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_hidden_states=True` æˆ–å½“ `config.output_hidden_states=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’ `output_attentions=True` æˆ–å½“ `config.output_attentions=True` æ—¶è¿”å›ï¼‰â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ› softmax ä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[Wav2Vec2ForXVector](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector) çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForXVector
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("anton-l/wav2vec2-base-superb-sv")
>>> model = Wav2Vec2ForXVector.from_pretrained("anton-l/wav2vec2-base-superb-sv")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(
...     [d["array"] for d in dataset[:2]["audio"]], sampling_rate=sampling_rate, return_tensors="pt", padding=True
... )
>>> with torch.no_grad():
...     embeddings = model(**inputs).embeddings

>>> embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()

>>> # the resulting embeddings can be used for cosine similarity-based retrieval
>>> cosine_sim = torch.nn.CosineSimilarity(dim=-1)
>>> similarity = cosine_sim(embeddings[0], embeddings[1])
>>> threshold = 0.7  # the optimal threshold is dataset-dependent
>>> if similarity < threshold:
...     print("Speakers are not the same!")
>>> round(similarity.item(), 2)
0.98
```

## Wav2Vec2ForPreTraining

`transformers.Wav2Vec2ForPreTraining` ç±»

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)

```py
( config: Wav2Vec2Config )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

Wav2Vec2 æ¨¡å‹å¸¦æœ‰é‡åŒ–å™¨å’Œé¡¶éƒ¨çš„ `VQ` å¤´ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª [PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“å®ç°çš„æ‰€æœ‰æ¨¡å‹çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None sampled_negative_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.FloatTensor`ï¼‰â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œ*ä¾‹å¦‚*é€šè¿‡soundfileåº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å‡†å¤‡å¥½æ•°ç»„ä»¥è·å¾—`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`torch.FloatTensor`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ

    +   å¯¹äº`è¢«å±è”½`çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

    åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œæ¯”å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”`ä¸`ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `mask_time_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç›–æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹è¢«æ©ç›–çš„æå–ç‰¹å¾ã€‚

+   `sampled_negative_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, num_negatives)`çš„`torch.BoolTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æŒ‡ç¤ºå“ªäº›é‡åŒ–ç›®æ ‡å‘é‡åœ¨å¯¹æ¯”æŸå¤±ä¸­ç”¨ä½œè´Ÿé‡‡æ ·å‘é‡çš„ç´¢å¼•ã€‚é¢„è®­ç»ƒæ‰€éœ€çš„è¾“å…¥ã€‚

è¿”å›

[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)æˆ–`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–å½“`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥è€Œå¼‚çš„å„ç§å…ƒç´ ã€‚

+   `loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰â€” æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚ ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚

+   `projected_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`çš„`torch.FloatTensor`ï¼‰â€” æ¨¡å‹æŠ•å½±åˆ°*config.proj_codevector_dim*çš„éšè—çŠ¶æ€ï¼Œå¯ç”¨äºé¢„æµ‹è¢«å±è”½çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚

+   `projected_quantized_states`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`çš„`torch.FloatTensor`ï¼‰ â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¯å±‚è¾“å‡ºçš„æ¨¡å‹éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

+   `contrastive_loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰ â€” å¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚

+   `diversity_loss`ï¼ˆ*å¯é€‰*ï¼Œå½“ä¼ é€’`sample_negative_indices`æ—¶è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼‰ â€” å¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰ï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚

[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…ä¼šè´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining
>>> from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
>>> from datasets import load_dataset

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
>>> model = Wav2Vec2ForPreTraining.from_pretrained("facebook/wav2vec2-base")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt").input_values  # Batch size 1

>>> # compute masked indices
>>> batch_size, raw_sequence_length = input_values.shape
>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()
>>> mask_time_indices = _compute_mask_indices(
...     shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2
... )
>>> sampled_negative_indices = _sample_negative_indices(
...     features_shape=(batch_size, sequence_length),
...     num_negatives=model.config.num_negatives,
...     mask_time_indices=mask_time_indices,
... )
>>> mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)
>>> sampled_negative_indices = torch.tensor(
...     data=sampled_negative_indices, device=input_values.device, dtype=torch.long
... )

>>> with torch.no_grad():
...     outputs = model(input_values, mask_time_indices=mask_time_indices)

>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)

>>> # show that cosine similarity is much higher than random
>>> cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5
tensor(True)

>>> # for contrastive loss training model should be put into train mode
>>> model = model.train()
>>> loss = model(
...     input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices
... ).loss
```

TensorFlowéšè—TensorFlowå†…å®¹

## TFWav2Vec2Model

### `class transformers.TFWav2Vec2Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)

```py
( config: Wav2Vec2Config *inputs **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰ â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸TFWav2Vec2æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¯¥æ¨¡å‹ä¹Ÿæ˜¯[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„TF 2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä¼ é€’æ‚¨çš„è¾“å…¥å’Œæ ‡ç­¾ä»¥ä»»ä½•`model.fit()`æ”¯æŒçš„æ ¼å¼ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Keras`Functional` APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ç”¨äºæ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   ä¸€ä¸ªä»…åŒ…å«`input_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_values)`

+   ä¸€ä¸ªé•¿åº¦ä¸åŒçš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æŒ‰ç…§æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºçš„ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥å¼ é‡ï¼š`model([input_values, attention_mask])`æˆ–`model([input_values, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_values": input_values, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒä»»ä½•è¿™äº›ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`({0})`ï¼‰â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   å¯¹äº`æœªå±è”½`çš„æ ‡è®°ï¼Œ

    +   å¯¹äº`è¢«å±è”½`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº*å¥å­A*æ ‡è®°ã€‚

    +   1 å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚æ©ç å€¼åœ¨`[0, 1]`ä¸­é€‰æ‹©ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœª`è¢«å±è”½`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨æ˜¯`å±è”½`çš„ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`({0}, hidden_size)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_values`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°†`input_values`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹å¯ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚

+   `training` (`bool`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º`Falseâ€œ) â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸€äº›æ¨¡å—å¦‚dropoutæ¨¡å—åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

è¿”å›

[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)æˆ–`tuple(tf.Tensor)`

ä¸€ä¸ª[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))å’Œè¾“å…¥çš„å„ç§å…ƒç´ ã€‚

+   `last_hidden_state` (`å½¢çŠ¶ä¸º(batch_size, sequence_length, hidden_size)çš„tf.Tensor`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `hidden_states` (`tuple(tf.FloatTensor)`, *å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(tf.Tensor)`, *å¯é€‰*ï¼Œåœ¨ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹:

```py
>>> from transformers import AutoProcessor, TFWav2Vec2Model
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = TFWav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
>>> hidden_states = model(input_values).last_hidden_state
```

## TFWav2Vec2ForSequenceClassification

### `class transformers.TFWav2Vec2ForSequenceClassification`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)

```py
( config )
```

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: tf.Tensor | None = None training: bool = False )
```

## TFWav2Vec2ForCTC

### `class transformers.TFWav2Vec2ForCTC`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)

```py
( config: Wav2Vec2Config *inputs **kwargs )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

TFWav2Vec2æ¨¡å‹ï¼Œåœ¨Connectionist Temporal Classification (CTC)é¡¶éƒ¨å…·æœ‰`è¯­è¨€å»ºæ¨¡`å¤´ã€‚

è¿™ä¸ªæ¨¡å‹ç»§æ‰¿è‡ª[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ï¼Œäº†è§£åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆå¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

è¿™ä¸ªæ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ª[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„çš„TF 2.0 Kerasæ¨¡å‹ï¼Œå¹¶å‚è€ƒTF 2.0æ–‡æ¡£ä»¥è·å–ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„æ‰€æœ‰å†…å®¹ã€‚

`transformers`ä¸­çš„TensorFlowæ¨¡å‹å’Œå±‚æ¥å—ä¸¤ç§æ ¼å¼çš„è¾“å…¥ï¼š

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºå…³é”®å­—å‚æ•°ï¼ˆç±»ä¼¼äºPyTorchæ¨¡å‹ï¼‰ï¼Œæˆ–è€…

+   å°†æ‰€æœ‰è¾“å…¥ä½œä¸ºåˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­ã€‚

æ”¯æŒç¬¬äºŒç§æ ¼å¼çš„åŸå› æ˜¯Kerasæ–¹æ³•åœ¨å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å’Œå±‚æ—¶æ›´å–œæ¬¢è¿™ç§æ ¼å¼ã€‚ç”±äºæœ‰äº†è¿™ç§æ”¯æŒï¼Œå½“ä½¿ç”¨`model.fit()`ç­‰æ–¹æ³•æ—¶ï¼Œåº”è¯¥å¯ä»¥â€œæ­£å¸¸å·¥ä½œâ€ - åªéœ€ä»¥`model.fit()`æ”¯æŒçš„ä»»ä½•æ ¼å¼ä¼ é€’è¾“å…¥å’Œæ ‡ç­¾å³å¯ï¼ä½†æ˜¯ï¼Œå¦‚æœæ‚¨æƒ³åœ¨Kerasæ–¹æ³•ä¹‹å¤–ï¼ˆå¦‚`fit()`å’Œ`predict()`ï¼‰ä½¿ç”¨ç¬¬äºŒç§æ ¼å¼ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨Keras`Functional` APIåˆ›å»ºè‡ªå·±çš„å±‚æˆ–æ¨¡å‹æ—¶ï¼Œæœ‰ä¸‰ç§å¯èƒ½æ€§å¯ä»¥ç”¨æ¥æ”¶é›†ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°ä¸­çš„æ‰€æœ‰è¾“å…¥å¼ é‡ï¼š

+   ä¸€ä¸ªä»…åŒ…å«`input_values`çš„å•ä¸ªå¼ é‡ï¼Œæ²¡æœ‰å…¶ä»–å†…å®¹ï¼š`model(input_values)`

+   ä¸€ä¸ªé•¿åº¦å¯å˜çš„åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„é¡ºåºç›¸å¯¹åº”çš„è¾“å…¥å¼ é‡ï¼š`model([input_values, attention_mask])`æˆ–`model([input_values, attention_mask, token_type_ids])`

+   ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªä¸æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç»™å®šçš„è¾“å…¥åç§°ç›¸å…³è”çš„è¾“å…¥å¼ é‡ï¼š`model({"input_values": input_values, "token_type_ids": token_type_ids})`

è¯·æ³¨æ„ï¼Œå½“ä½¿ç”¨[å­ç±»åŒ–](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)åˆ›å»ºæ¨¡å‹å’Œå±‚æ—¶ï¼Œæ‚¨æ— éœ€æ‹…å¿ƒè¿™äº›é—®é¢˜ï¼Œå› ä¸ºæ‚¨å¯ä»¥åƒå°†è¾“å…¥ä¼ é€’ç»™ä»»ä½•å…¶ä»–Pythonå‡½æ•°ä¸€æ ·ä¼ é€’è¾“å…¥ï¼

#### `call`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) â†’ export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutput or tuple(tf.Tensor)
```

å‚æ•°

+   `input_values`ï¼ˆ`np.ndarray`ï¼Œ`tf.Tensor`ï¼Œ`List[tf.Tensor]`ï¼Œ`Dict[str, tf.Tensor]`æˆ–`Dict[str, np.ndarray]`ï¼Œæ¯ä¸ªç¤ºä¾‹çš„å½¢çŠ¶å¿…é¡»ä¸º`({0})`ï¼‰â€”è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)è·å–ç´¢å¼•ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)å’Œ[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)ã€‚

    [ä»€ä¹ˆæ˜¯è¾“å…¥IDï¼Ÿ](../glossary#input-ids)

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1è¡¨ç¤ºâ€œæœªè¢«æ©ç â€çš„æ ‡è®°ï¼Œ

    +   0è¡¨ç¤ºâ€œè¢«æ©ç â€çš„æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask)

+   `token_type_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ®µæ ‡è®°ç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   0å¯¹åº”äº*å¥å­A*æ ‡è®°ï¼Œ

    +   1å¯¹åº”äº*å¥å­B*æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ ‡è®°ç±»å‹IDï¼Ÿ](../glossary#token-type-ids)

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`({0})`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

    [ä»€ä¹ˆæ˜¯ä½ç½®IDï¼Ÿ](../glossary#position-ids)

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚é€‰æ‹©çš„æ©ç å€¼ä¸º`[0, 1]`ï¼š

    +   1è¡¨ç¤ºå¤´éƒ¨æ˜¯`not masked`ã€‚

    +   0è¡¨ç¤ºå¤´éƒ¨è¢«`masked`ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`({0}, hidden_size)`çš„`np.ndarray`æˆ–`tf.Tensor`ï¼Œ*å¯é€‰*ï¼‰ â€” å¯é€‰åœ°ï¼Œå¯ä»¥ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_values`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶å¦‚ä½•å°†`input_values`ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µï¼Œåˆ™è¿™å¾ˆæœ‰ç”¨ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚æ­¤å‚æ•°ä»…åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹å°†ä½¿ç”¨é…ç½®ä¸­çš„å€¼ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰ â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚æ­¤å‚æ•°å¯åœ¨æ€¥åˆ‡æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œåœ¨å›¾æ¨¡å¼ä¸‹è¯¥å€¼å°†å§‹ç»ˆè®¾ç½®ä¸ºTrueã€‚

+   `training`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰ â€” æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨æ¨¡å‹ï¼ˆæŸäº›æ¨¡å—ï¼Œå¦‚dropoutæ¨¡å—ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°ä¹‹é—´å…·æœ‰ä¸åŒçš„è¡Œä¸ºï¼‰ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`tf.Tensor`æˆ–`np.ndarray`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºè®¡ç®—æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`èŒƒå›´å†…ï¼ˆå‚è§`input_values`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç ï¼‰ï¼ŒæŸå¤±ä»…è®¡ç®—å…·æœ‰æ ‡ç­¾åœ¨`[0, ..., config.vocab_size]`èŒƒå›´å†…çš„æ ‡è®°ã€‚

è¿”å›

[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput) æˆ– `tuple(tf.Tensor)`

ä¸€ä¸ª[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)æˆ–ä¸€ä¸ª`tf.Tensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…å«å„ç§å…ƒç´ ï¼Œå…·ä½“å–å†³äºé…ç½®ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰å’Œè¾“å…¥ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(n,)`çš„`tf.Tensor`ï¼Œ*å¯é€‰*ï¼Œå…¶ä¸­næ˜¯éæ©ç æ ‡ç­¾çš„æ•°é‡ï¼Œåœ¨æä¾›`labels`æ—¶è¿”å›ï¼‰ â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`tf.Tensor`ï¼‰ â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMaxä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`tf.Tensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€ä»¥åŠåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(tf.Tensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`tf.Tensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›æƒé‡åœ¨æ³¨æ„åŠ›softmaxä¹‹åï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import tensorflow as tf
>>> from transformers import AutoProcessor, TFWav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = TFWav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
>>> logits = model(input_values).logits
>>> predicted_ids = tf.argmax(logits, axis=-1)

>>> transcription = processor.decode(predicted_ids[0])

>>> # compute loss
>>> target_transcription = "A MAN SAID TO THE UNIVERSE SIR I EXIST"

>>> # Pass transcription as `text` to encode labels
>>> labels = processor(text=transcription, return_tensors="tf").input_ids

>>> loss = model(input_values, labels=labels).loss
```

JAXHide JAXå†…å®¹

## FlaxWav2Vec2Model

### `class transformers.FlaxWav2Vec2Model`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config`ï¼ˆ[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)ï¼‰- å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype`ï¼ˆ`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`jax.numpy.float32`ï¼‰- è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚

    è¿™å¯ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šäº†`dtype`ï¼Œåˆ™æ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„dtypeï¼Œä¸å½±å“æ¨¡å‹å‚æ•°çš„dtypeã€‚`

    å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„dtypeï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

è£¸Wav2Vec2æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliåœ¨[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ä¸­æå‡ºçš„ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–åº“ä¸ºå…¶æ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹ä¹Ÿæ˜¯Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰åŠŸèƒ½ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)

```py
( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼‰- è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å€¼å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`jnp.ndarray`ç±»å‹çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º1ï¼Œ

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ï¼Œå€¼ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”`ä¸`ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `mask_time_indices`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰- ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹æ©ç æå–ç‰¹å¾ã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹è¿”å›çš„å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹è¿”å›çš„å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰- æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)æˆ–è€…`tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)æˆ–è€…ä¸€ä¸ª`torch.FloatTensor`çš„å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº†`return_dict=False`æˆ–è€…`config.return_dict=False`ï¼‰åŒ…å«ä¸åŒçš„å…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`ï¼‰å’Œè¾“å…¥ã€‚

+   `last_hidden_state`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`ï¼‰- æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

+   `extract_features`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, last_conv_dim)`çš„`jnp.ndarray`ï¼‰- æ¨¡å‹æœ€åä¸€ä¸ªå·ç§¯å±‚çš„æå–ç‰¹å¾å‘é‡åºåˆ—ï¼Œå…¶ä¸­`last_conv_dim`æ˜¯æœ€åä¸€ä¸ªå·ç§¯å±‚çš„ç»´åº¦ã€‚

+   `hidden_states`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–è€…`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡º + ä¸€ä¸ªç”¨äºæ¯ä¸ªå±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(jnp.ndarray)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–è€…`config.output_attentions=True`æ—¶è¿”å›ï¼‰- å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ›softmaxä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWav2Vec2PreTrainedModel`çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„æ–¹æ³•éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤å¤„è°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoProcessor, FlaxWav2Vec2Model
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-large-lv60")
>>> model = FlaxWav2Vec2Model.from_pretrained("facebook/wav2vec2-large-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(
...     ds["speech"][0], sampling_rate=16_000, return_tensors="np"
... ).input_values  # Batch size 1
>>> hidden_states = model(input_values).last_hidden_state
```

## FlaxWav2Vec2ForCTC

### `class transformers.FlaxWav2Vec2ForCTC`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype`ï¼ˆ`jax.numpy.dtype`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º `jax.numpy.float32`ï¼‰ â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯ `jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨ GPU ä¸Šï¼‰å’Œ `jax.numpy.bfloat16`ï¼ˆåœ¨ TPU ä¸Šï¼‰ä¹‹ä¸€ã€‚

    è¿™å¯ç”¨äºåœ¨ GPU æˆ– TPU ä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„ `dtype` æ‰§è¡Œã€‚

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸ä¼šå½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜… [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) å’Œ [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

Wav2Vec2 æ¨¡å‹åœ¨é¡¶éƒ¨å¸¦æœ‰â€œè¯­è¨€å»ºæ¨¡â€å¤´éƒ¨ï¼Œç”¨äº Connectionist Temporal Classification (CTC)ã€‚Wav2Vec2 æ˜¯ç”± Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auli åœ¨ [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) ä¸­æå‡ºçš„ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯ Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ Flax æ¨¡å—ï¼Œå¹¶å‚è€ƒ Flax æ–‡æ¡£ä»¥äº†è§£æ‰€æœ‰ä¸ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºç›¸å…³çš„äº‹é¡¹ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒ JAX çš„å†…åœ¨ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ (JIT) ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)

```py
( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `jnp.ndarray`ï¼‰ â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°† `.flac` æˆ– `.wav` éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°ç±»å‹ä¸º `List[float]` æˆ– `numpy.ndarray` çš„æ•°ç»„ä¸­è·å¾—ã€‚è¦å°†æ•°ç»„å‡†å¤‡æˆ `input_values`ï¼Œåº”ä½¿ç”¨ [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸ºç±»å‹ä¸º `jnp.ndarray` çš„å¼ é‡ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask`ï¼ˆå½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„ `jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰ â€” ç”¨äºé¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰åœ¨ `[0, 1]`ï¼š

    +   1 ç”¨äºâ€œæœªæ©ç â€æ ‡è®°ï¼Œ

    +   0 ç”¨äºâ€œæ©ç â€æ ‡è®°ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == True` æ—¶æ‰åº”ä¼ é€’ `attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰ `config.return_attention_mask == False` çš„æ¨¡å‹ï¼Œä¾‹å¦‚ [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨ç†æ—¶ï¼Œåº”è¯¥ `ä¸` ä¼ é€’ `attention_mask` ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™æ ·çš„æ¨¡å‹ï¼Œ`input_values` åº”è¯¥ç®€å•åœ°ç”¨ 0 å¡«å……å¹¶åœ¨ä¸ä¼ é€’ `attention_mask` çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ® `input_values` æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `mask_time_indices` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length)`ï¼Œ*å¯é€‰*) â€” ç”¨äºå¯¹æ¯”æŸå¤±æ©ç›–æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨ *config.proj_codevector_dim* ç©ºé—´ä¸­é¢„æµ‹æ©ç›–çš„æå–ç‰¹å¾ã€‚

+   `output_attentions` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *å¯é€‰*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

è¿”å›

[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput) æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’ `return_dict=False` æˆ– `config.return_dict=False`ï¼‰åŒ…å«ä¸åŒå…ƒç´ ï¼Œå–å†³äºé…ç½®ï¼ˆ`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`ï¼‰å’Œè¾“å…¥ã€‚

+   `logits` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º `(batch_size, sequence_length, config.vocab_size)`) â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’ `output_hidden_states=True` æˆ– `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`, *å¯é€‰*, å½“ä¼ é€’ `output_attentions=True` æˆ– `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `jnp.ndarray` å…ƒç»„ï¼ˆæ¯ä¸€å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ› softmax åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

`FlaxWav2Vec2PreTrainedModel` çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„æ­¥éª¤éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨ä¹‹åè°ƒç”¨ `Module` å®ä¾‹è€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…ä¼šå¤„ç†è¿è¡Œå‰åå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import jax.numpy as jnp
>>> from transformers import AutoProcessor, FlaxWav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-large-960h-lv60")
>>> model = FlaxWav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(
...     ds["speech"][0], sampling_rate=16_000, return_tensors="np"
... ).input_values  # Batch size 1
>>> logits = model(input_values).logits
>>> predicted_ids = jnp.argmax(logits, axis=-1)

>>> transcription = processor.decode(predicted_ids[0])
>>> # should give:  "A MAN SAID TO THE UNIVERSE SIR I EXIST"
```

## FlaxWav2Vec2ForPreTraining

### `class transformers.FlaxWav2Vec2ForPreTraining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

å‚æ•°

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) â€” æ¨¡å‹é…ç½®ç±»ï¼ŒåŒ…å«æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹ç›¸å…³çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

+   `dtype` (`jax.numpy.dtype`, *å¯é€‰*, é»˜è®¤ä¸º`jax.numpy.float32`) â€” è®¡ç®—çš„æ•°æ®ç±»å‹ã€‚å¯ä»¥æ˜¯`jax.numpy.float32`ã€`jax.numpy.float16`ï¼ˆåœ¨GPUä¸Šï¼‰å’Œ`jax.numpy.bfloat16`ï¼ˆåœ¨TPUä¸Šï¼‰ä¹‹ä¸€ã€‚

    è¿™å¯ç”¨äºåœ¨GPUæˆ–TPUä¸Šå¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒæˆ–åŠç²¾åº¦æ¨æ–­ã€‚å¦‚æœæŒ‡å®šï¼Œæ‰€æœ‰è®¡ç®—å°†ä½¿ç”¨ç»™å®šçš„`dtype`è¿›è¡Œã€‚ 

    `è¯·æ³¨æ„ï¼Œè¿™ä»…æŒ‡å®šè®¡ç®—çš„æ•°æ®ç±»å‹ï¼Œä¸ä¼šå½±å“æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ã€‚`

    å¦‚æœè¦æ›´æ”¹æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œè¯·å‚é˜…[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)å’Œ[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)ã€‚

å¸¦æœ‰é‡åŒ–å™¨å’Œé¡¶éƒ¨`VQ`å¤´çš„Wav2Vec2æ¨¡å‹ã€‚Wav2Vec2æ˜¯ç”±Alexei Baevskiã€Henry Zhouã€Abdelrahman Mohamedã€Michael Auliæå‡ºçš„[wav2vec 2.0:è‡ªç›‘ç£å­¦ä¹ è¯­éŸ³è¡¨ç¤ºçš„æ¡†æ¶](https://arxiv.org/abs/2006.11477)ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)ã€‚æŸ¥çœ‹è¶…ç±»æ–‡æ¡£ä»¥äº†è§£åº“ä¸ºæ‰€æœ‰æ¨¡å‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸‹è½½æˆ–ä¿å­˜ã€è°ƒæ•´è¾“å…¥åµŒå…¥ã€ä¿®å‰ªå¤´ç­‰ï¼‰ã€‚

æ­¤æ¨¡å‹è¿˜æ˜¯Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„Flaxæ¨¡å—ï¼Œå¹¶å‚è€ƒFlaxæ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

æœ€åï¼Œæ­¤æ¨¡å‹æ”¯æŒJAXçš„å›ºæœ‰ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼š

+   [å³æ—¶ï¼ˆJITï¼‰ç¼–è¯‘](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [è‡ªåŠ¨å¾®åˆ†](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [çŸ¢é‡åŒ–](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [å¹¶è¡ŒåŒ–](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)

```py
( input_values attention_mask = None mask_time_indices = None gumbel_temperature: int = 1 params: dict = None dropout_rng: PRNGKey = None gumbel_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) â€” è¾“å…¥åŸå§‹è¯­éŸ³æ³¢å½¢çš„æµ®ç‚¹å€¼ã€‚å¯ä»¥é€šè¿‡å°†`.flac`æˆ–`.wav`éŸ³é¢‘æ–‡ä»¶åŠ è½½åˆ°`List[float]`ç±»å‹çš„æ•°ç»„æˆ–`numpy.ndarray`ä¸­è·å¾—å€¼ï¼Œä¾‹å¦‚é€šè¿‡soundfileåº“ï¼ˆ`pip install soundfile`ï¼‰ã€‚è¦å°†æ•°ç»„å‡†å¤‡ä¸º`input_values`ï¼Œåº”ä½¿ç”¨[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)è¿›è¡Œå¡«å……å’Œè½¬æ¢ä¸º`jnp.ndarray`ç±»å‹çš„å¼ é‡ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)ã€‚

+   `attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *å¯é€‰*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œå·ç§¯å’Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨`[0, 1]`ä¸­ï¼š

    +   å¯¹äºæœªè¢«`masked`çš„æ ‡è®°ä¸º1ã€‚

    +   å¯¹äºè¢«`masked`çš„æ ‡è®°ä¸º0ã€‚

    [ä»€ä¹ˆæ˜¯æ³¨æ„åŠ›æ©ç ï¼Ÿ](../glossary#attention-mask) .. è­¦å‘Š:: åªæœ‰å½“ç›¸åº”çš„å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == True`æ—¶ï¼Œæ‰åº”ä¼ é€’`attention_mask`ã€‚å¯¹äºæ‰€æœ‰å¤„ç†å™¨å…·æœ‰`config.return_attention_mask == False`çš„æ¨¡å‹ï¼Œä¾‹å¦‚[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)ï¼Œåœ¨è¿›è¡Œæ‰¹é‡æ¨æ–­æ—¶ï¼Œåº”é¿å…ä¼ é€’`attention_mask`ä»¥é¿å…æ€§èƒ½ä¸‹é™ã€‚å¯¹äºè¿™äº›æ¨¡å‹ï¼Œ`input_values`åº”è¯¥ç®€å•åœ°ç”¨0å¡«å……å¹¶åœ¨ä¸ä¼ é€’`attention_mask`çš„æƒ…å†µä¸‹ä¼ é€’ã€‚è¯·æ³¨æ„ï¼Œè¿™äº›æ¨¡å‹æ ¹æ®`input_values`æ˜¯å¦å¡«å……ä¼šäº§ç”Ÿç•¥æœ‰ä¸åŒçš„ç»“æœã€‚

+   `mask_time_indices` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length)`ï¼Œ*optional*ï¼‰ â€” ç”¨äºå¯¹æ¯”æŸå¤±ä¸­æ©ç æå–ç‰¹å¾çš„ç´¢å¼•ã€‚åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹å­¦ä¹ åœ¨*config.proj_codevector_dim*ç©ºé—´ä¸­é¢„æµ‹æ©ç æå–ç‰¹å¾ã€‚

+   `output_attentions` (`bool`, *optional`) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*ï¼‰ â€” æ˜¯å¦è¿”å›[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

è¿”å›

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput) æˆ– `tuple(torch.FloatTensor)`

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput) æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆ`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`ï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss` (*optional*ï¼Œåœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿”å›ï¼Œå½¢çŠ¶ä¸º`(1,)`çš„`jnp.ndarray`) â€” æ€»æŸå¤±ï¼Œä½œä¸ºå¯¹æ¯”æŸå¤±ï¼ˆL_mï¼‰å’Œå¤šæ ·æ€§æŸå¤±ï¼ˆL_dï¼‰çš„æ€»å’Œï¼Œå¦‚[å®˜æ–¹è®ºæ–‡](https://arxiv.org/pdf/2006.11477.pdf)ä¸­æ‰€è¿°ã€‚ ï¼ˆåˆ†ç±»ï¼‰æŸå¤±ã€‚

+   `projected_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰ â€” æ¨¡å‹çš„éšè—çŠ¶æ€æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œå¯ç”¨äºé¢„æµ‹æ©ç çš„æŠ•å½±é‡åŒ–çŠ¶æ€ã€‚

+   `projected_quantized_states` (`jnp.ndarray`ï¼Œå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.proj_codevector_dim)`ï¼‰ â€” é‡åŒ–æå–çš„ç‰¹å¾å‘é‡æŠ•å½±åˆ°*config.proj_codevector_dim*ï¼Œè¡¨ç¤ºå¯¹æ¯”æŸå¤±çš„æ­£ç›®æ ‡å‘é‡ã€‚

+   `hidden_states` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šåˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(jnp.ndarray)`ï¼Œ*optional*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`jnp.ndarray`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    æ³¨æ„åŠ›softmaxåçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

[FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)çš„å‰å‘æ–¹æ³•ï¼Œè¦†ç›–`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ æ’­çš„é…æ–¹éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªå‡½æ•°ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import optax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from transformers import AutoFeatureExtractor, FlaxWav2Vec2ForPreTraining
>>> from transformers.models.wav2vec2.modeling_flax_wav2vec2 import _compute_mask_indices
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-large-lv60")
>>> model = FlaxWav2Vec2ForPreTraining.from_pretrained("facebook/wav2vec2-large-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = feature_extractor(ds["speech"][0], return_tensors="np").input_values  # Batch size 1

>>> # compute masked indices
>>> batch_size, raw_sequence_length = input_values.shape
>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
>>> mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=0.2, mask_length=2)

>>> outputs = model(input_values, mask_time_indices=mask_time_indices)

>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
>>> cosine_sim = optax.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states)

>>> # show that cosine similarity is much higher than random
>>> assert np.asarray(cosine_sim)[mask_time_indices].mean() > 0.5
```
