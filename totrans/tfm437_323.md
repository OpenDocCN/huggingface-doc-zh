# Wav2Vec2

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/wav2vec2)

## 概述

Wav2Vec2模型是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)中提出的。

论文摘要如下：

*我们首次展示，仅通过从语音音频中学习强大的表示，然后在转录的语音上进行微调，可以胜过最佳的半监督方法，同时在概念上更简单。wav2vec 2.0在潜在空间中屏蔽语音输入，并解决了一个在联合学习的潜在表示的量化上定义的对比任务。使用Librispeech的所有标记数据进行的实验在干净/其他测试集上实现了1.8/3.3的WER。当将标记数据量降低到一小时时，wav2vec 2.0在100小时子集上胜过了先前的最先进技术，同时使用的标记数据量减少了100倍。仅使用十分钟的标记数据并在53k小时的未标记数据上进行预训练仍然实现了4.8/8.2的WER。这证明了在有限的标记数据量下进行语音识别的可行性。*

此模型由[patrickvonplaten](https://huggingface.co/patrickvonplaten)贡献。

## 使用提示

+   Wav2Vec2是一个接受与语音信号的原始波形对应的浮点数组的语音模型。

+   Wav2Vec2模型是使用连接主义时间分类（CTC）进行训练的，因此模型输出必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)进行解码。

## 资源

一份官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用Wav2Vec2。如果您有兴趣提交资源以包含在此处，请随时打开Pull Request，我们将进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。

音频分类

+   一个关于如何[利用预训练的Wav2Vec2模型进行情感分类](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb)的笔记本。🌎

+   [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)受到这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification)和[笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)的支持。

+   [音频分类任务指南](../tasks/audio_classification)

自动语音识别

+   一篇关于[在🤗 Transformers中使用n-grams增强Wav2Vec2的博客文章](https://huggingface.co/blog/wav2vec2-with-ngram)。

+   一篇关于如何[使用🤗 Transformers对英语ASR进行微调的博客文章](https://huggingface.co/blog/fine-tune-wav2vec2-english)。

+   一篇关于[使用🤗 Transformers对多语言ASR进行微调的博客文章](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)。

+   一个关于如何[通过使用Wav2Vec2转录音频从任何视频创建YouTube字幕](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb)的笔记本。🌎

+   [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)受到一篇关于[如何在英语中微调语音识别模型的笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)和[如何在任何语言中微调语音识别模型的笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)的支持。

+   [自动语音识别任务指南](../tasks/asr)

🚀 部署

+   关于如何在[Hugging Face的Transformers和Amazon SageMaker中部署Wav2Vec2进行自动语音识别的博文](https://www.philschmid.de/automatic-speech-recognition-sagemaker)。

## Wav2Vec2Config

### `class transformers.Wav2Vec2Config`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/configuration_wav2vec2.py#L32)

```py
( vocab_size = 32 hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 feat_quantizer_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_norm = 'group' feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 do_stable_layer_norm = False apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 num_codevectors_per_group = 320 num_codevector_groups = 2 contrastive_logits_temperature = 0.1 num_negatives = 100 codevector_dim = 256 proj_codevector_dim = 256 diversity_loss_weight = 0.1 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None adapter_attn_dim = None **kwargs )
```

参数

+   `vocab_size` (`int`, *optional*, defaults to 32) — Wav2Vec2模型的词汇表大小。定义了在调用[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)或[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)时可以表示的不同标记数量。模型的词汇表大小。定义了在调用[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model)的forward方法时可以表示的不同标记数量。

+   `hidden_size` (`int`, *optional*, defaults to 768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *optional*, defaults to 12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *optional*, defaults to 12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *optional*, defaults to 3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout` (`float`, *optional*, defaults to 0.1) — 嵌入层、编码器和池化器中所有完全连接层的dropout概率。

+   `activation_dropout` (`float`, *optional*, defaults to 0.1) — 完全连接层内激活的dropout比率。

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) — 注意力概率的dropout比率。

+   `final_dropout` (`float`, *optional*, defaults to 0.1) — [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)最终投影层的dropout概率。

+   `layerdrop` (`float`, *optional*, defaults to 0.1) — LayerDrop概率。有关更多详细信息，请参阅[LayerDrop论文](see [https://arxiv.org/abs/1909.11556](https://arxiv.org/abs/1909.11556))。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。

+   `feat_extract_norm` (`str`, *optional*, defaults to `"group"`) — 应用于特征编码器中1D卷积层的规范化。`"group"`表示仅对第一个1D卷积层进行组归一化，`"layer"`表示对所有1D卷积层进行层归一化。

+   `feat_proj_dropout` (`float`, *optional*, defaults to 0.0) — 特征编码器输出的dropout概率。

+   `feat_extract_activation` (`str,` optional`, defaults to` “gelu”`) -- 特征提取器中1D卷积层的非线性激活函数（函数或字符串）。如果是字符串，支持`“gelu”`、`“relu”`、`“selu”`和`“gelu_new”`。

+   `feat_quantizer_dropout` (`float`, *optional*, defaults to 0.0) — 量化特征编码器状态的dropout概率。

+   `conv_dim` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(512, 512, 512, 512, 512, 512, 512)`) — 一个整数元组，定义特征编码器中每个1D卷积层的输入和输出通道数。*conv_dim*的长度定义了1D卷积层的数量。

+   `conv_stride` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(5, 2, 2, 2, 2, 2, 2)`) — 在特征编码器中每个1D卷积层的步幅的整数元组。*conv_stride*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。

+   `conv_kernel` (`Tuple[int]` or `List[int]`, *optional*, defaults to `(10, 3, 3, 3, 3, 3, 3)`) — 在特征编码器中每个1D卷积层的卷积核大小的整数元组。*conv_kernel*的长度定义了卷积层的数量，并且必须与*conv_dim*的长度匹配。

+   `conv_bias` (`bool`, *optional*, defaults to `False`) — 1D卷积层是否具有偏置。

+   `num_conv_pos_embeddings` (`int`, *optional*, defaults to 128) — 卷积位置嵌入的数量。定义了1D卷积位置嵌入层的卷积核大小。

+   `num_conv_pos_embedding_groups` (`int`, *optional*, defaults to 16) — 1D卷积位置嵌入层的组数。

+   `do_stable_layer_norm` (`bool`, *optional*, defaults to `False`) — 是否应用Transformer编码器的*stable*层归一化架构。`do_stable_layer_norm为True`表示在注意力层之前应用层归一化，而`do_stable_layer_norm为False`表示在注意力层之后应用层归一化。

+   `apply_spec_augment` (`bool`, *optional*, defaults to `True`) — 是否将*SpecAugment*数据增强应用于特征编码器的输出。有关详细信息，请参阅[SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779)。

+   `mask_time_prob` (`float`, *optional*, defaults to 0.05) — 沿时间轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_time_prob*len(time_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度起始的概率推理，* mask_time_prob *应为`prob_vector_start*mask_time_length`。请注意，重叠可能会降低实际掩盖向量的百分比。仅在`apply_spec_augment为True`时相关。

+   `mask_time_length` (`int`, *optional*, defaults to 10) — 沿时间轴的向量跨度长度。

+   `mask_time_min_masks` (`int`, *optional*, defaults to 2), — 沿时间轴生成的长度为`mask_feature_length`的最小掩码数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_time_prob*len(time_axis)/mask_time_length < mask_time_min_masks”时相关

+   `mask_feature_prob` (`float`, *optional*, defaults to 0.0) — 沿特征轴的所有特征向量中将被掩盖的百分比（介于0和1之间）。掩码过程在轴上生成”mask_feature_prob*len(feature_axis)/mask_time_length”个独立的掩码。如果从每个特征向量被选择为掩盖的向量跨度起始的概率推理，* mask_feature_prob *应为`prob_vector_start*mask_feature_length`。请注意，重叠可能会降低实际掩盖向量的百分比。仅在`apply_spec_augment为True`时相关。

+   `mask_feature_length` (`int`, *optional*, defaults to 10) — 沿特征轴的向量跨度长度。

+   `mask_feature_min_masks` (`int`, *optional*, defaults to 0), — 沿特征轴生成的长度为`mask_feature_length`的最小掩码数量，每个时间步，与`mask_feature_prob`无关。仅在”mask_feature_prob*len(feature_axis)/mask_feature_length < mask_feature_min_masks”时相关

+   `num_codevectors_per_group` (`int`, *optional*, defaults to 320) — 每个量化码书（组）中的条目数。

+   `num_codevector_groups` (`int`, *optional*, defaults to 2) — 产品码矢量量化的码矢量组数。

+   `contrastive_logits_temperature` (`float`, *可选*, 默认为 0.1) — 对比损失中的温度 *kappa*。

+   `feat_quantizer_dropout` (`float`, *可选*, 默认为 0.0) — 用于量化器使用的特征编码器输出的丢弃概率。

+   `num_negatives` (`int`, *可选*, 默认为 100) — 对比损失的负样本数量。

+   `codevector_dim` (`int`, *可选*, 默认为 256) — 量化特征向量的维度。

+   `proj_codevector_dim` (`int`, *可选*, 默认为 256) — 最终投影的维度，包括量化特征和变换器特征。

+   `diversity_loss_weight` (`int`, *可选*, 默认为 0.1) — 代码本多样性损失组件的权重。

+   `ctc_loss_reduction` (`str`, *可选*, 默认为 `"sum"`) — 指定应用于 `torch.nn.CTCLoss` 输出的减少方式。仅在训练 [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) 实例时相关。

+   `ctc_zero_infinity` (`bool`, *可选*, 默认为 `False`) — 是否将 `torch.nn.CTCLoss` 的无限损失和相关梯度置零。当输入太短无法与目标对齐时，主要会出现无限损失。仅在训练 [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) 实例时相关。

+   `use_weighted_layer_sum` (`bool`, *可选*, 默认为 `False`) — 是否使用具有学习权重的层输出的加权平均。仅在使用 [Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification) 实例时相关。

+   `classifier_proj_size` (`int`, *可选*, 默认为 256) — 用于分类的令牌均值池化之前的投影维度。

+   `tdnn_dim` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(512, 512, 512, 512, 1500)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个一维卷积层的输出通道数。*tdnn_dim* 的长度定义了 *TDNN* 层的数量。

+   `tdnn_kernel` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(5, 3, 3, 1, 1)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个一维卷积层的内核大小。*tdnn_kernel* 的长度必须与 *tdnn_dim* 的长度相匹配。

+   `tdnn_dilation` (`Tuple[int]` 或 `List[int]`, *可选*, 默认为 `(1, 2, 3, 1, 1)`) — 一个整数元组，定义了 *XVector* 模型中 *TDNN* 模块中每个一维卷积层的膨胀因子。*tdnn_dilation* 的长度必须与 *tdnn_dim* 的长度相匹配。

+   `xvector_output_dim` (`int`, *可选*, 默认为 512) — *XVector* 嵌入向量的维度。

+   `add_adapter` (`bool`, *可选*, 默认为 `False`) — 是否在 Wav2Vec2 编码器顶部堆叠卷积网络。对于 Warm-starting Wav2Vec2 for SpeechEncoderDecoder 模型非常有用。

+   `adapter_kernel_size` (`int`, *可选*, 默认为 3) — 适配器网络中卷积层的内核大小。仅在 `add_adapter` 为 True 时相关。

+   `adapter_stride` (`int`, *可选*, 默认为 2) — 适配器网络中卷积层的步幅。仅在 `add_adapter` 为 True 时相关。

+   `num_adapter_layers` (`int`, *可选*, 默认为 3) — 适配器网络中应使用的卷积层数量。仅在 `add_adapter` 为 True 时相关。

+   `adapter_attn_dim` (`int`, *可选*) — 每个注意力块中要使用的注意力适配器权重的维度。使用注意力适配器的模型示例是 [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)。

+   `output_hidden_size` (`int`, *可选*) — 编码器输出层的维度。如果未定义，则默认为 *hidden-size*。仅在 `add_adapter` 为 True 时相关。

这是用于存储 [Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) 配置的配置类。它用于根据指定的参数实例化 Wav2Vec2 模型，定义模型架构。使用默认值实例化配置将产生类似于 Wav2Vec2 [facebook/wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) 架构的配置。

配置对象继承自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读来自 [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig) 的文档以获取更多信息。

示例：

```py
>>> from transformers import Wav2Vec2Config, Wav2Vec2Model

>>> # Initializing a Wav2Vec2 facebook/wav2vec2-base-960h style configuration
>>> configuration = Wav2Vec2Config()

>>> # Initializing a model (with random weights) from the facebook/wav2vec2-base-960h style configuration
>>> model = Wav2Vec2Model(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## Wav2Vec2CTCTokenizer

### `class transformers.Wav2Vec2CTCTokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L127)

```py
( vocab_file bos_token = '<s>' eos_token = '</s>' unk_token = '<unk>' pad_token = '<pad>' word_delimiter_token = '|' replace_word_delimiter_char = ' ' do_lower_case = False target_lang = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含词汇表的文件。

+   `bos_token` (`str`, *可选*, 默认为 `"<s>"`) — 句子开头标记。

+   `eos_token` (`str`, *可选*, 默认为 `"</s>"`) — 句子结束标记。

+   `unk_token` (`str`, *可选*, 默认为 `"<unk>"`) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。

+   `pad_token` (`str`, *可选*, 默认为 `"<pad>"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。

+   `word_delimiter_token` (`str`, *可选*, 默认为 `"|"`) — 用于定义单词结尾的标记。

+   `do_lower_case` (`bool`, *可选*, 默认为 `False`) — 是否接受小写输入并在解码时将输出转换为小写。

+   `target_lang` (`str`, *可选*) — 分词器应默认设置的目标语言。对于多语言、嵌套词汇表，如 [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all)，必须定义 `target_lang`。

    **kwargs — 传递给 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 的额外关键字参数

构建一个 Wav2Vec2CTC 分词器。

这个分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含一些主要方法。用户应参考超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L2729)

```py
( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) → export const metadata = 'undefined';BatchEncoding
```

参数

+   `text` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置 `is_split_into_words=True`（以消除与批处理序列的歧义）。

+   `text_pair` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置 `is_split_into_words=True`（以消除与批处理序列的歧义）。

+   `text_target` (`str`, `List[str]`, `List[List[str]]`, *可选*) — 要编码为目标文本的序列或序列批次。每个序列可以是字符串或字符串列表（预分词字符串）。如果提供的序列是字符串列表（预分词），必须设置 `is_split_into_words=True`（以消除与批处理序列的歧义）。

+   `text_pair_target` (`str`, `List[str]`, `List[List[str]]`, *optional*) — 要编码为目标文本的序列或批量序列。每个序列可以是一个字符串或一个字符串列表（预先标记化的字符串）。如果序列以字符串列表（预先标记化）的形式提供，则必须设置`is_split_into_words=True`（以消除批量序列的歧义）。

+   `add_special_tokens` (`bool`, *optional*, 默认为`True`) — 在编码序列时是否添加特殊标记。这将使用底层的`PretrainedTokenizerBase.build_inputs_with_special_tokens`函数，该函数定义了自动添加到输入id的标记。如果要自动添加`bos`或`eos`标记，则这很有用。

+   `padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy), *optional*, 默认为`False`) — 激活和控制填充。接受以下值：

    +   `True` 或 `'longest'`：填充到批量中最长的序列（如果只提供单个序列，则不填充）。

    +   `'max_length'`：填充到由参数`max_length`指定的最大长度，或者填充到模型的最大可接受输入长度（如果未提供该参数）。

    +   `False` 或 `'do_not_pad'`（默认）：不填充（即，可以输出长度不同的序列批次）。

+   `truncation` (`bool`, `str` 或 [TruncationStrategy](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy), *optional*, 默认为`False`) — 激活和控制截断。接受以下值：

    +   `True` 或 `'longest_first'`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则将逐标记截断，从一对序列中最长的序列中删除一个标记。

    +   `'only_first'`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则只会截断第一个序列。

    +   `'only_second'`：截断到由参数`max_length`指定的最大长度，或者截断到模型的最大可接受输入长度（如果未提供该参数）。如果提供了一对序列（或一批对），则只会截断第二个序列。

    +   `False` 或 `'do_not_truncate'`（默认）：不截断（即，可以输出长度大于模型最大可接受输入大小的序列批次）。

+   `max_length` (`int`, *optional*) — 控制截断/填充参数之一使用的最大长度。

    如果未设置或设置为`None`，则将使用预定义的模型最大长度（如果截断/填充参数之一需要最大长度）。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。

+   `stride` (`int`, *optional*, 默认为0) — 如果与`max_length`一起设置为一个数字，则当`return_overflowing_tokens=True`时返回的溢出标记将包含从截断序列末尾返回的一些标记，以提供截断和溢出序列之间的一些重叠。该参数的值定义了重叠标记的数量。

+   `is_split_into_words` (`bool`, *optional*, 默认为`False`) — 输入是否已经预标记化（例如，已分割为单词）。如果设置为`True`，则分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行标记化。这对于命名实体识别或标记分类很有用。

+   `pad_to_multiple_of` (`int`, *可选*) — 如果设置，将序列填充到提供的值的倍数。需要激活`padding`。这对于在具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上启用Tensor Cores特别有用。

+   `return_tensors` (`str`或[TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是Python整数列表。可接受的值为：

    +   `'tf'`: 返回TensorFlow `tf.constant`对象。

    +   `'pt'`: 返回PyTorch `torch.Tensor`对象。

    +   `'np'`: 返回Numpy `np.ndarray`对象。

+   `return_token_type_ids` (`bool`, *可选*) — 是否返回token type IDs。如果保持默认设置，将根据特定标记化器的默认值返回token type IDs，由`return_outputs`属性定义。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `return_attention_mask` (`bool`, *可选*) — 是否返回注意力掩码。如果保持默认设置，将根据特定标记化器的默认值返回注意力掩码，由`return_outputs`属性定义。 

    [什么是attention masks？](../glossary#attention-mask)

+   `return_overflowing_tokens` (`bool`, *可选*, 默认为 `False`) — 是否返回溢出的token序列。如果提供了一对输入id序列（或一批对）并且`truncation_strategy = longest_first`或`True`，则会引发错误，而不是返回溢出的tokens。

+   `return_special_tokens_mask` (`bool`, *可选*, 默认为 `False`) — 是否返回特殊token掩码信息。

+   `return_offsets_mapping` (`bool`, *可选*, 默认为 `False`) — 是否返回每个token的`(char_start, char_end)`。

    这仅适用于继承自[PreTrainedTokenizerFast](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)的快速标记化器，如果使用Python的标记化器，此方法将引发`NotImplementedError`。

+   `return_length` (`bool`, *可选*, 默认为 `False`) — 是否返回编码输入的长度。

+   `verbose` (`bool`, *可选*, 默认为 `True`) — 是否打印更多信息和警告。**kwargs — 传递给`self.tokenize()`方法

返回

[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)

一个具有以下字段的[BatchEncoding](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.BatchEncoding)：

+   `input_ids` — 要提供给模型的token id列表。

    [什么是input IDs？](../glossary#input-ids)

+   `token_type_ids` — 要提供给模型的token type ids列表（当`return_token_type_ids=True`或*`token_type_ids`*在`self.model_input_names`中时）。

    [什么是token type IDs？](../glossary#token-type-ids)

+   `attention_mask` — 指定哪些token应该被模型关注的索引列表（当`return_attention_mask=True`或*`attention_mask`*在`self.model_input_names`中时）。

    [什么是attention masks？](../glossary#attention-mask)

+   `overflowing_tokens` — 溢出的token序列列表（当指定了`max_length`并且`return_overflowing_tokens=True`时）。

+   `num_truncated_tokens` — 被截断的token数量（当指定了`max_length`并且`return_overflowing_tokens=True`时）。

+   `special_tokens_mask` — 由0和1组成的列表，其中1指定添加的特殊token，0指定常规序列token（当`add_special_tokens=True`且`return_special_tokens_mask=True`时）。

+   `length` — 输入的长度（当`return_length=True`时）

将主要方法标记化并为模型准备一个或多个序列或一个或多个序列对。

#### `save_vocabulary`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L646)

```py
( save_directory: str filename_prefix: Optional = None )
```

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L541)

```py
( token_ids: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) → export const metadata = 'undefined';str or Wav2Vec2CTCTokenizerOutput
```

参数

+   `token_ids` (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`) — 分词后的输入id列表。可以使用`__call__`方法获得。

+   `skip_special_tokens` (`bool`，*可选*，默认为`False`) — 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces` (`bool`，*可选*） — 是否清理分词空格。

+   `output_char_offsets` (`bool`，*可选*，默认为`False`) — 是否输出字符偏移量。字符偏移量可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。

    请查看下面的示例，以更好地理解如何使用`output_char_offsets`。

+   `output_word_offsets` (`bool`，*可选*，默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。

    请查看下面的示例，以更好地理解如何使用`output_word_offsets`。

+   `kwargs`（其他关键字参数，*可选*） — 将传递给底层模型特定的解码方法。

返回

`str`或`Wav2Vec2CTCTokenizerOutput`

解码后的句子列表。当`output_char_offsets == True`或`output_word_offsets == True`时，将是`Wav2Vec2CTCTokenizerOutput`。

将一系列id转换为字符串，使用分词器和词汇表，可以选择删除特殊标记并清理分词空格。

类似于执行`self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`。

示例：

```py
>>> # Let's see how to retrieve time steps for a model
>>> from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("facebook/wav2vec2-base-960h")
>>> tokenizer = AutoTokenizer.from_pretrained("facebook/wav2vec2-base-960h")
>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

>>> # load first sample of English common_voice
>>> dataset = load_dataset("mozilla-foundation/common_voice_11_0", "en", split="train", streaming=True)
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
>>> dataset_iter = iter(dataset)
>>> sample = next(dataset_iter)

>>> # forward sample through model to get greedily predicted transcription ids
>>> input_values = feature_extractor(sample["audio"]["array"], return_tensors="pt").input_values
>>> logits = model(input_values).logits[0]
>>> pred_ids = torch.argmax(logits, axis=-1)

>>> # retrieve word stamps (analogous commands for `output_char_offsets`)
>>> outputs = tokenizer.decode(pred_ids, output_word_offsets=True)
>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
>>> time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate

>>> word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:
>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en
>>> word_offsets[:3]
[{'word': 'THE', 'start_time': 0.7, 'end_time': 0.78}, {'word': 'TRICK', 'start_time': 0.88, 'end_time': 1.08}, {'word': 'APPEARS', 'start_time': 1.2, 'end_time': 1.64}]
```

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L471)

```py
( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None output_char_offsets: bool = False output_word_offsets: bool = False **kwargs ) → export const metadata = 'undefined';List[str] or Wav2Vec2CTCTokenizerOutput
```

参数

+   `sequences` (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`) — 分词后的输入id列表。可以使用`__call__`方法获得。

+   `skip_special_tokens` (`bool`，*可选*，默认为`False`) — 是否在解码中删除特殊标记。

+   `clean_up_tokenization_spaces` (`bool`，*可选*) — 是否清理分词空格。

+   `output_char_offsets` (`bool`，*可选*，默认为`False`) — 是否输出字符偏移量。字符偏移量可以与采样率和模型下采样率结合使用，计算转录字符的时间戳。

    请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)的示例，以更好地理解如何使用`output_char_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)与批量输出的方式相同。

+   `output_word_offsets` (`bool`，*可选*，默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。

    请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.decode)的示例，以更好地理解如何使用`output_word_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.batch_decode)与批量输出的方式相同。

+   `kwargs`（其他关键字参数，*可选*） — 将传递给底层模型特定的解码方法。

返回

`List[str]`或`Wav2Vec2CTCTokenizerOutput`

解码后的句子列表。当`output_char_offsets == True`或`output_word_offsets == True`时，将是`Wav2Vec2CTCTokenizerOutput`。

通过调用解码函数，将一系列token id的列表转换为字符串列表。

#### `set_target_lang`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/tokenization_wav2vec2.py#L213)

```py
( target_lang: str )
```

设置嵌套多语言字典的目标语言

## Wav2Vec2FeatureExtractor

### `class transformers.Wav2Vec2FeatureExtractor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L31)

```py
( feature_size = 1 sampling_rate = 16000 padding_value = 0.0 return_attention_mask = False do_normalize = True **kwargs )
```

参数

+   `feature_size`（`int`，默认为1）— 提取特征的特征维度。

+   `sampling_rate`（`int`，默认为16000）— 应以赫兹（Hz）表示的音频文件数字化的采样率。

+   `padding_value`（`float`，默认为0.0）— 用于填充值的值。

+   `do_normalize`（`bool`，*可选*，默认为`True`）— 是否对输入进行零均值单位方差归一化。归一化可以帮助一些模型显著提高性能，例如 [wav2vec2-lv60](https://huggingface.co/models?search=lv60)。

+   `return_attention_mask`（`bool`，*可选*，默认为`False`）— 是否 [`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__) 应该返回 `attention_mask`。

    设置了 `config.feat_extract_norm == "group"` 的 Wav2Vec2 模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，没有使用 `attention_mask` 进行训练。对于这样的模型，`input_values` 应该简单地用 0 填充，不应传递 `attention_mask`。

    对于设置了 `config.feat_extract_norm == "layer"` 的 Wav2Vec2 模型，例如 [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)，应该为批量推断传递 `attention_mask`。

构建一个 Wav2Vec2 特征提取器。

此特征提取器继承自 [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)，其中包含大多数主要方法。用户应参考此超类以获取有关这些方法的更多信息。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py#L102)

```py
( raw_speech: Union padding: Union = False max_length: Optional = None truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask: Optional = None return_tensors: Union = None sampling_rate: Optional = None **kwargs )
```

参数

+   `raw_speech`（`np.ndarray`，`List[float]`，`List[np.ndarray]`，`List[List[float]]`）— 要填充的序列或批次序列。每个序列可以是一个 numpy 数组，一个浮点值列表，一个 numpy 数组列表或一个浮点值列表的列表。必须是单声道音频，不是立体声，即每个时间步长一个浮点数。

+   `padding`（`bool`，`str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy)，*可选*，默认为`False`）— 选择一种策略来填充返回的序列（根据模型的填充方向和填充索引），包括：

    +   `True` 或 `'longest'`：填充到批次中最长的序列（如果只提供单个序列，则不进行填充）。

    +   `'max_length'`：填充到指定参数 `max_length` 的最大长度，或者如果未提供该参数，则填充到模型的最大可接受输入长度。

    +   `False` 或 `'do_not_pad'`（默认）：无填充（即，可以输出具有不同长度序列的批次）。

+   `max_length`（`int`，*可选*）— 返回列表的最大长度和可选填充长度（见上文）。

+   `truncation`（`bool`）— 激活截断，将输入序列截断为比 *max_length* 更长的序列到 *max_length*。

+   `pad_to_multiple_of`（`int`，*可选*）— 如果设置，将填充序列到提供的值的倍数。

    这对于启用 NVIDIA 硬件上的 Tensor Cores 特别有用，其计算能力 `>= 7.5`（Volta），或者对于受益于序列长度为 128 的倍数的 TPU。

+   `return_attention_mask`（`bool`，*可选*）— 是否返回注意力掩码。如果保持默认值，将根据特定 feature_extractor 的默认值返回注意力掩码。

    [什么是注意力掩码？](../glossary#attention-mask)

    对于设置了 `config.feat_extract_norm == "group"` 的 Wav2Vec2 模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，没有使用 `attention_mask` 进行训练。对于这样的模型，`input_values` 应该简单地用 0 填充，不应传递 `attention_mask`。

    对于设置了 `config.feat_extract_norm == "layer"` 的 Wav2Vec2 模型，例如 [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self)，应该为批量推断传递 `attention_mask`。

+   `return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType), *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：

    +   `'tf'`: 返回 TensorFlow `tf.constant` 对象。

    +   `'pt'`: 返回 PyTorch `torch.Tensor` 对象。

    +   `'np'`: 返回 Numpy `np.ndarray` 对象。

+   `sampling_rate` (`int`, *可选*) — `raw_speech` 输入采样的采样率。强烈建议在前向调用时传递 `sampling_rate` 以防止静默错误。

+   `padding_value` (`float`, 默认为 0.0) —

对一个或多个序列进行特征化和为模型准备的主要方法。

## Wav2Vec2Processor

### `class transformers.Wav2Vec2Processor`

[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L26)

```py
( feature_extractor tokenizer )
```

参数

+   `feature_extractor` (`Wav2Vec2FeatureExtractor`) — [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) 的一个实例。特征提取器是必需的输入。

+   `tokenizer` ([PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)) — [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 的一个实例。分词器是必需的输入。

构建一个 Wav2Vec2 处理器，将 Wav2Vec2 特征提取器和 Wav2Vec2 CTC 分词器封装成一个单一处理器。

[Wav2Vec2Processor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor) 提供了 [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor) 和 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer) 的所有功能。查看 [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__) 和 [decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.decode) 的文档字符串以获取更多信息。

#### `__call__`

[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L68)

```py
( *args **kwargs )
```

在正常模式下使用时，此方法将所有参数转发到 Wav2Vec2FeatureExtractor 的 [**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__) 并返回其输出。如果在上下文 `as_target_processor()` 中使用此方法，将所有参数转发到 PreTrainedTokenizer 的 [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。请参考上述两个方法的文档字符串以获取更多信息。

#### `pad`

[`<来源>`](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L106)

```py
( *args **kwargs )
```

在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)并返回其输出。如果在上下文`as_target_processor()`中使用，此方法将所有参数转发到PreTrainedTokenizer的[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)。有关更多信息，请参考上述两种方法的文档字符串。

#### `from_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L49)

```py
( pretrained_model_name_or_path **kwargs )
```

#### `save_pretrained`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)

```py
( save_directory push_to_hub: bool = False **kwargs )
```

参数

+   `save_directory`（`str`或`os.PathLike`）— 特征提取器JSON文件和分词器文件将保存在的目录（如果目录不存在，则将创建目录）。

+   `push_to_hub`（`bool`，*可选*，默认为`False`）— 是否在保存后将模型推送到Hugging Face模型中心。您可以使用`repo_id`指定要推送到的存储库（将默认为您的命名空间中的`save_directory`名称）。

+   `kwargs`（`Dict[str, Any]`，*可选*）— 传递给[push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)方法的额外关键字参数。

将此处理器的属性（特征提取器、分词器等）保存在指定目录中，以便可以使用[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)方法重新加载。

这个类方法只是调用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)和[save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained)。有关更多信息，请参考上述方法的文档字符串。

#### `batch_decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L136)

```py
( *args **kwargs )
```

此方法将所有参数转发到PreTrainedTokenizer的[batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode)。有关更多信息，请参考此方法的文档字符串。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/processing_wav2vec2.py#L143)

```py
( *args **kwargs )
```

此方法将所有参数转发到PreTrainedTokenizer的[decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode)。有关更多信息，请参考此方法的文档字符串。

## Wav2Vec2ProcessorWithLM

### `class transformers.Wav2Vec2ProcessorWithLM`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L67)

```py
( feature_extractor: FeatureExtractionMixin tokenizer: PreTrainedTokenizerBase decoder: BeamSearchDecoderCTC )
```

参数

+   `feature_extractor`（[Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor））— [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)的一个实例。特征提取器是必需的输入。

+   `tokenizer`（[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer））— [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)的一个实例。分词器是必需的输入。

+   `decoder`（`pyctcdecode.BeamSearchDecoderCTC`）— `pyctcdecode.BeamSearchDecoderCTC`的一个实例。解码器是必需的输入。

构建一个Wav2Vec2处理器，将Wav2Vec2特征提取器、Wav2Vec2 CTC分词器和具有语言模型支持的解码器包装到一个单一的处理器中，用于语言模型增强的语音识别解码。

#### `__call__`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L215)

```py
( *args **kwargs )
```

在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[**call**()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor.__call__)，并返回其输出。如果在上下文中使用`as_target_processor()`，此方法将所有参数转发到Wav2Vec2CTCTokenizer的[**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。请参考上述两种方法的文档字符串以获取更多信息。

#### `pad`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L254)

```py
( *args **kwargs )
```

在正常模式下使用时，此方法将所有参数转发到Wav2Vec2FeatureExtractor的[pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)，并返回其输出。如果在上下文中使用`as_target_processor()`，此方法将所有参数转发到Wav2Vec2CTCTokenizer的[pad()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.pad)。请参考上述两种方法的文档字符串以获取更多信息。

#### `from_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L113)

```py
( pretrained_model_name_or_path **kwargs )
```

参数

+   `pretrained_model_name_or_path` (`str` or `os.PathLike`) — 可以是：

    +   预训练特征提取器的*模型ID*的字符串，托管在huggingface.co上的模型存储库中。有效的模型ID可以位于根级别，如`bert-base-uncased`，或者在用户或组织名称下命名空间化，如`dbmdz/bert-base-german-cased`。

    +   一个包含使用[save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)方法保存的特征提取器文件的*目录*路径，例如`./my_model_directory/`。

    +   从预训练的特征提取器JSON文件的路径或URL，例如`./my_model_directory/preprocessor_config.json`。**kwargs — 传递给[SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor)和[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)的额外关键字参数

从预训练的Wav2Vec2处理器实例化一个[Wav2Vec2ProcessorWithLM](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM)。

这个类方法只是调用了Wav2Vec2FeatureExtractor的[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)，Wav2Vec2CTCTokenizer的[from_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained)，以及`pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`。

请参考上述方法的文档字符串以获取更多信息。

#### `save_pretrained`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L109)

```py
( save_directory )
```

#### `batch_decode`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L285)

```py
( logits: ndarray pool: Optional = None num_processes: Optional = None beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )
```

参数

+   `logits` (`np.ndarray`) — 模型输出的logits向量，表示每个标记的对数概率。

+   `pool` (`multiprocessing.Pool`, *optional*) — 可选的用户管理的池。如果未设置，将自动创建并关闭一个池。池应在`Wav2Vec2ProcessorWithLM`之后实例化。否则，LM将不可用于池的子进程。

    目前，只有使用“fork”上下文创建的池才能使用。如果传递了“spawn”池，它将被忽略，而将使用顺序解码。

+   `num_processes` (`int`, *optional*) — 如果未设置`pool`，则应该在哪些进程上并行化函数。默认为可用CPU的数量。

+   `beam_width` (`int`, *optional*) — 解码过程中每一步的最大beam数。默认为pyctcdecode的DEFAULT_BEAM_WIDTH。

+   `beam_prune_logp` (`int`, *optional*) — 比最佳beam差很多的beam将被修剪。默认为pyctcdecode的DEFAULT_PRUNE_LOGP。

+   `token_min_logp` (`int`, *optional*) — 低于此logp的标记将被跳过，除非它们是帧的argmax。默认为pyctcdecode的DEFAULT_MIN_TOKEN_LOGP。

+   `hotwords` (`List[str]`, *optional*) — 具有额外重要性的单词列表，可以是LM的OOV

+   `hotword_weight` (`int`, *optional*) — 热词重要性的权重因子，默认为pyctcdecode的DEFAULT_HOTWORD_WEIGHT。

+   `alpha` (`float`, *optional*) — 浅融合期间语言模型的权重

+   `beta` (`float`, *optional*) — 在评分过程中长度得分调整的权重

+   `unk_score_offset` (`float`, *optional*) — 未知标记的对数分数偏移量

+   `lm_score_boundary` (`bool`, *optional*) — 在评分时是否让kenlm尊重边界

+   `output_word_offsets` (`bool`, *optional*, 默认为`False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，以计算转录单词的时间戳。

+   `n_best` (`int`, *optional*, 默认为`1`) — 要返回的最佳假设数量。如果`n_best`大于1，则返回的`text`将是一个字符串列表的列表，`logit_score`将是一个浮点数列表的列表，`lm_score`将是一个浮点数列表的列表，外部列表的长度将对应批次大小，内部列表的长度将对应返回的假设数量。该值应 >= 1。

    请查看[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)的示例，以更好地理解如何使用`output_word_offsets`。[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)与批量输出的方式相同。

批量解码输出logits以支持语言模型的音频转录。

此函数利用了Python的多进程。目前，多进程仅在Unix系统上可用（请参阅此[问题](https://github.com/kensho-technologies/pyctcdecode/issues/65)）。

如果您正在解码多个批次，请考虑创建一个`Pool`并将其传递给`batch_decode`。否则，`batch_decode`将非常慢，因为它将为每次调用创建一个新的`Pool`。请参见下面的用法示例。

示例：请参见[解码多个音频](#decoding-multiple-audios)。

#### `decode`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L470)

```py
( logits: ndarray beam_width: Optional = None beam_prune_logp: Optional = None token_min_logp: Optional = None hotwords: Optional = None hotword_weight: Optional = None alpha: Optional = None beta: Optional = None unk_score_offset: Optional = None lm_score_boundary: Optional = None output_word_offsets: bool = False n_best: int = 1 )
```

参数

+   `logits` (`np.ndarray`) — 代表每个标记的对数概率的模型输出向量。

+   `beam_width` (`int`, *optional*) — 解码过程中每一步的最大beam数。默认为pyctcdecode的DEFAULT_BEAM_WIDTH。

+   `beam_prune_logp` (`int`, *optional*) — 一个用于修剪log-probs小于best_beam_logp + beam_prune_logp的阈值。该值应 <= 0。默认为pyctcdecode的DEFAULT_PRUNE_LOGP。

+   `token_min_logp`（`int`，*optional*） — log-probs低于token_min_logp的标记将被跳过，除非它们是话语的最大log-prob。默认为pyctcdecode的DEFAULT_MIN_TOKEN_LOGP。

+   `hotwords`（`List[str]`，*optional*） — 具有额外重要性的单词列表，可能不在LM的词汇表中，例如[“huggingface”]

+   `hotword_weight`（`int`，*optional*） — 增强热词分数的权重乘数。默认为pyctcdecode的DEFAULT_HOTWORD_WEIGHT。

+   `alpha`（`float`，*optional*） — 浅融合期间语言模型的权重

+   `beta`（`float`，*optional*） — 在评分过程中长度分数调整的权重

+   `unk_score_offset`（`float`，*optional*） — 未知标记的log分数偏移量

+   `lm_score_boundary` (`bool`, *optional*) — 是否在评分时让kenlm尊重边界

+   `output_word_offsets` (`bool`, *optional*, 默认为 `False`) — 是否输出单词偏移量。单词偏移量可以与采样率和模型下采样率结合使用，计算转录单词的时间戳。

+   `n_best`（`int`，*optional*，默认为`1`） — 要返回的最佳假设数量。如果`n_best`大于1，则返回的`text`将是一个字符串列表，`logit_score`将是一个浮点数列表，`lm_score`将是一个浮点数列表，这些列表的长度将对应于返回的假设数量。该值应大于等于1。

    请查看下面的示例，以更好地理解如何使用`output_word_offsets`。

使用语言模型支持将输出逻辑解码为音频转录。

示例：

```py
>>> # Let's see how to retrieve time steps for a model
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # load first sample of English common_voice
>>> dataset = load_dataset("mozilla-foundation/common_voice_11_0", "en", split="train", streaming=True)
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
>>> dataset_iter = iter(dataset)
>>> sample = next(dataset_iter)

>>> # forward sample through model to get greedily predicted transcription ids
>>> input_values = processor(sample["audio"]["array"], return_tensors="pt").input_values
>>> with torch.no_grad():
...     logits = model(input_values).logits[0].cpu().numpy()

>>> # retrieve word stamps (analogous commands for `output_char_offsets`)
>>> outputs = processor.decode(logits, output_word_offsets=True)
>>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
>>> time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate

>>> word_offsets = [
...     {
...         "word": d["word"],
...         "start_time": round(d["start_offset"] * time_offset, 2),
...         "end_time": round(d["end_offset"] * time_offset, 2),
...     }
...     for d in outputs.word_offsets
... ]
>>> # compare word offsets with audio `en_train_0/common_voice_en_19121553.mp3` online on the dataset viewer:
>>> # https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/en
>>> word_offsets[:4]
[{'word': 'THE', 'start_time': 0.68, 'end_time': 0.78}, {'word': 'TRACK', 'start_time': 0.88, 'end_time': 1.1}, {'word': 'APPEARS', 'start_time': 1.18, 'end_time': 1.66}, {'word': 'ON', 'start_time': 1.86, 'end_time': 1.92}]
```

### 解码多个音频

如果您计划解码多批音频，应考虑使用[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)并传递一个实例化的`multiprocessing.Pool`。否则，[batch_decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.batch_decode)的性能将比为每个音频单独调用[decode()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ProcessorWithLM.decode)慢，因为它在每次调用时内部实例化一个新的`Pool`。请参阅下面的示例：

```py
>>> # Let's see how to use a user-managed pool for batch decoding multiple audios
>>> from multiprocessing import get_context
>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
>>> from datasets import load_dataset
>>> import datasets
>>> import torch

>>> # import model, feature extractor, tokenizer
>>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm").to("cuda")
>>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")

>>> # load example dataset
>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))

>>> def map_to_array(batch):
...     batch["speech"] = batch["audio"]["array"]
...     return batch

>>> # prepare speech data for batch inference
>>> dataset = dataset.map(map_to_array, remove_columns=["audio"])

>>> def map_to_pred(batch, pool):
...     inputs = processor(batch["speech"], sampling_rate=16_000, padding=True, return_tensors="pt")
...     inputs = {k: v.to("cuda") for k, v in inputs.items()}

...     with torch.no_grad():
...         logits = model(**inputs).logits

...     transcription = processor.batch_decode(logits.cpu().numpy(), pool).text
...     batch["transcription"] = transcription
...     return batch

>>> # note: pool should be instantiated *after* `Wav2Vec2ProcessorWithLM`.
>>> #       otherwise, the LM won't be available to the pool's sub-processes
>>> # select number of processes and batch_size based on number of CPU cores available and on dataset size
>>> with get_context("fork").Pool(processes=2) as pool:
...     result = dataset.map(
...         map_to_pred, batched=True, batch_size=2, fn_kwargs={"pool": pool}, remove_columns=["speech"]
...     )

>>> result["transcription"][:2]
['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', "NOR IS MISTER COULTER'S MANNER LESS INTERESTING THAN HIS MATTER"]
```

## Wav2Vec2特定输出

### `class transformers.models.wav2vec2_with_lm.processing_wav2vec2_with_lm.Wav2Vec2DecoderWithLMOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py#L44)

```py
( text: Union logit_score: Union = None lm_score: Union = None word_offsets: Union = None )
```

参数

+   `text`（`str`列表或`str`） — 文本中的解码逻辑。通常是语音转录。

+   `logit_score`（`float`列表或`float`） — 与生成文本相关的beam的总logit分数。

+   `lm_score`（`float`列表） — 与生成文本相关的beam的融合lm_score。

+   `word_offsets`（`List[Dict[str, Union[int, str]]]`或`List[Dict[str, Union[int, str]]]`列表 — 解码单词的偏移量。结合采样率和模型下采样率，单词偏移量可用于计算每个单词的时间戳。

`Wav2Vec2DecoderWithLM`的输出类型，带有转录。

### `class transformers.modeling_outputs.Wav2Vec2BaseModelOutput`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/modeling_outputs.py#L1376)

```py
( last_hidden_state: FloatTensor = None extract_features: FloatTensor = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`） — 模型最后一层的隐藏状态序列。

+   `extract_features`（形状为`(batch_size, sequence_length, conv_dim[-1])`的`torch.FloatTensor`） — 模型最后一个卷积层的提取特征向量序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

使用Wav2Vec2损失目标进行训练的模型的基类。

### `class transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L100)

```py
( loss: Optional = None projected_states: FloatTensor = None projected_quantized_states: FloatTensor = None codevector_perplexity: FloatTensor = None hidden_states: Optional = None attentions: Optional = None contrastive_loss: Optional = None diversity_loss: Optional = None )
```

参数

+   `loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor` of shape `(1,)`) — 总损失，由对比损失（L_m）和多样性损失（L_d）的和组成，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。 （分类）损失。

+   `projected_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`) — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。

+   `projected_quantized_states` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`) — 量化提取的特征向量序列，投影到*config.proj_codevector_dim*，代表对比损失的正目标向量。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `contrastive_loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor` of shape `(1,)`) — 对比损失（L_m），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。

+   `diversity_loss` (*可选*, 当传递`sample_negative_indices`时返回, `torch.FloatTensor` of shape `(1,)`) — 多样性损失（L_d），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。

[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)的输出类型，具有潜在的隐藏状态和注意力。

### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L44)

```py
( last_hidden_state: Array = None extract_features: Array = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `last_hidden_state` (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列。

+   `extract_features` (`jnp.ndarray` of shape `(batch_size, sequence_length, last_conv_dim)`) — 模型最后一个卷积层提取的特征向量序列，其中`last_conv_dim`是最后一个卷积层的维度。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxWav2Vec2BaseModelOutput`的输出类型，具有潜在的隐藏状态和注意力。

#### `replace`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。

### `class transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L74)

```py
( projected_states: Array = None projected_quantized_states: Array = None codevector_perplexity: Array = None hidden_states: Optional = None attentions: Optional = None )
```

参数

+   `loss` (*可选*, 当模型处于训练模式时返回，形状为`(1,)`的`jnp.ndarray`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。 (分类) 损失。

+   `projected_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`） — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码投影量化状态。

+   `projected_quantized_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`） — 投影到*config.proj_codevector_dim*的量化提取特征向量，表示对比损失的正目标向量。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxWav2Vec2ForPreTrainingOutput`的输出类型，具有潜在的隐藏状态和注意力。

#### `replace`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。

PytorchHide Pytorch内容

## Wav2Vec2Model

### `class transformers.Wav2Vec2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1440)

```py
( config: Wav2Vec2Config )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的 Wav2Vec2 模型变压器输出原始隐藏状态，没有特定的顶部头。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。

该模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。

该模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1530)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将 `.flac` 或 `.wav` 音频文件加载到 `List[float]` 类型的数组或 `numpy.ndarray` 中获得，例如通过 soundfile 库（`pip install soundfile`）。要将数组准备成 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) 进行填充和转换为 `torch.FloatTensor` 类型的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在 `[0, 1]` 之间:

    +   1 用于 `未被掩码` 的标记，

    +   0 用于 `被掩码` 的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有 `config.return_attention_mask == False` 的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时应 `不` 传递 `attention_mask` 以避免性能下降。对于这些模型，`input_values` 应简单地用 0 填充并在不传递 `attention_mask` 的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

返回

[transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.Wav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Wav2Vec2BaseModelOutput) 或一个 `torch.FloatTensor` 元组（如果传递了 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) — 模型最后一层的隐藏状态序列输出。

+   `extract_features` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, conv_dim[-1])`) — 模型最后一个卷积层提取的特征向量序列。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每个层的输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选的*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每个层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

[Wav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Model) 的 forward 方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是在此处调用，因为前者会负责运行前处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, Wav2Vec2Model
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 292, 768]
```

## Wav2Vec2ForCTC

### `class transformers.Wav2Vec2ForCTC`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1859)

```py
( config target_lang: Optional = None )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法来加载模型权重。

+   `target_lang` (`str`，*可选的*) — 适配器权重的语言 id。适配器权重存储在格式为 adapter.<lang>.safetensors 或 adapter.<lang>.bin 的文件中。仅在使用带有适配器的 [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC) 实例时相关。默认使用 ‘eng’。</lang></lang>

带有顶部 `语言建模` 的 Wav2Vec2 模型，用于 Connectionist Temporal Classification (CTC)。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。

这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存等）。

这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1941)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。值可以通过将 `.flac` 或 `.wav` 音频文件加载到类型为 `List[float]` 或 `numpy.ndarray` 的数组中获得，例如通过 soundfile 库 (`pip install soundfile`)。要准备好数组为 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) 进行填充和转换为 `torch.FloatTensor` 类型的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *可选*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被遮蔽的标记，为1，

    +   对于被遮蔽的标记，为0。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。

+   `labels` (`torch.LongTensor` of shape `(batch_size, target_length)`, *可选*) — 连接主义时间分类的标签。请注意，`target_length`必须小于或等于输出logits的序列长度。索引选择在`[-100, 0, ..., config.vocab_size - 1]`。所有设置为`-100`的标签都被忽略（遮蔽），损失仅计算标签在`[0, ..., config.vocab_size - 1]`中的情况。

返回

[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.CausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入的不同元素。

+   `loss` (`torch.FloatTensor` of shape `(1,)`, *可选*, 当提供`labels`时返回) — 语言建模损失（用于下一个标记的预测）。

+   `logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型具有嵌入层的输出，则为嵌入的输出+每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *可选*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者负责运行前后处理步骤，而后者会默默忽略它们。

示例:

```py
>>> from transformers import AutoProcessor, Wav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

>>> # audio file is decoded on the fly
>>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
>>> predicted_ids = torch.argmax(logits, dim=-1)

>>> # transcribe speech
>>> transcription = processor.batch_decode(predicted_ids)
>>> transcription[0]
'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'

>>> inputs["labels"] = processor(text=dataset[0]["text"], return_tensors="pt").input_ids

>>> # compute loss
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
53.48
```

#### `load_adapter`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1191)

```py
( target_lang: str force_load = True **kwargs )
```

参数

+   `target_lang` (`str`) — 必须是现有适配器权重的语言 ID。适配器权重存储在格式 adapter.<lang>.safetensors 或 adapter.<lang>.bin</lang></lang>。

+   `force_load` (`bool`, 默认为 `True`) — 即使 `target_lang` 与 `self.target_lang` 匹配，也要加载权重。

+   `cache_dir` (`Union[str, os.PathLike]`, *可选*) — 下载的预训练模型配置应该缓存在其中的目录路径，如果不使用标准缓存。

+   `force_download` (`bool`, *可选*, 默认为 `False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *可选*, 默认为 `False`) — 是否删除接收不完整的文件。如果存在这样的文件，将尝试恢复下载。

+   `proxies` (`Dict[str, str]`, *可选*) — 一个按协议或端点使用的代理服务器字典，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。每个请求都会使用代理。

+   `local_files_only(bool,` *可选*, 默认为 `False`) — 是否仅查看本地文件（即不尝试下载模型）。

+   `token` (`str` 或 `bool`, *可选*) — 用作远程文件的 HTTP bearer 授权的令牌。如果为 `True`，或未指定，将使用运行 `huggingface-cli login` 时生成的令牌（存储在 `~/.huggingface` 中）。

+   `revision` (`str`, *可选*, 默认为 `"main"`) — 要使用的特定模型版本。它可以是分支名称、标签名称或提交 ID，因为我们在 huggingface.co 上使用基于 git 的系统存储模型和其他工件，所以 `revision` 可以是 git 允许的任何标识符。

    要测试您在 Hub 上提交的拉取请求，可以传递 `revision=“refs/pr/<pr_number>“。</pr_number>

+   `mirror` (`str`, *可选*) — 将源镜像到中国以加速下载。如果您来自中国并且有访问问题，可以设置此选项以解决问题。请注意，我们不保证及时性或安全性。请参考镜像站点获取更多信息。

从预训练的适配器模型加载语言适配器模型。

激活特殊的[“离线模式”](https://huggingface.co/transformers/installation.html#offline-mode)以在防火墙环境中使用此方法。

示例:

```py
>>> from transformers import Wav2Vec2ForCTC, AutoProcessor

>>> ckpt = "facebook/mms-1b-all"
>>> processor = AutoProcessor.from_pretrained(ckpt)
>>> model = Wav2Vec2ForCTC.from_pretrained(ckpt, target_lang="eng")
>>> # set specific language
>>> processor.tokenizer.set_target_lang("spa")
>>> model.load_adapter("spa")
```

## Wav2Vec2ForSequenceClassification

### `class transformers.Wav2Vec2ForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2021)

```py
( config )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

在顶部添加了一个序列分类头的 Wav2Vec2 模型（一个线性层在池化输出上方）用于 SUPERB 关键词识别等任务。

Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。

该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存等）的信息。

该模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2073)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过声音文件库（`pip install soundfile`）。要将数组准备为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*可选*) — 用于避免在填充标记索引上执行卷积和注意力的掩码。选择的掩码值在`[0, 1]`范围内：

    +   对于未被掩码的标记为`1`，

    +   对于被掩码的标记为`0`。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型还会根据`input_values`是否填充而产生略有不同的结果。

+   `output_attentions` (`bool`，*可选*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`，*可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`，*可选*) — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为`(batch_size,)`，*可选*) — 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或`config.return_dict=False`）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入的不同元素。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`，*可选*，当提供`labels`时返回) — 分类（或如果`config.num_labels==1`则为回归）损失。

+   `logits`（形状为`(batch_size, config.num_labels)`的`torch.FloatTensor`）- 分类（如果`config.num_labels==1`则为回归）分数（SoftMax之前）。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出，如果模型有一个嵌入层，+ 一个用于每个层的输出）。

    模型在每个层的输出的隐藏状态加上可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

[Wav2Vec2ForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForSequenceClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("superb/wav2vec2-base-superb-ks")
>>> model = Wav2Vec2ForSequenceClassification.from_pretrained("superb/wav2vec2-base-superb-ks")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()
>>> predicted_label = model.config.id2label[predicted_class_ids]
>>> predicted_label
'_unknown_'

>>> # compute loss - target_label is e.g. "down"
>>> target_label = model.config.id2label[0]
>>> inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
>>> loss = model(**inputs).loss
>>> round(loss.item(), 2)
6.54
```

## Wav2Vec2ForAudioFrameClassification

### `class transformers.Wav2Vec2ForAudioFrameClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2144)

```py
( config )
```

参数

+   `config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）- 具有模型所有参数的模型配置类。 使用配置文件初始化不会加载与模型关联的权重，只加载配置。 请查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

带有顶部帧分类头的Wav2Vec2模型，用于Speaker Diarization等任务。

Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)中提出的。

此模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。 检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存等）。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。 将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2194)

```py
( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）- 输入原始语音波形的浮点值。 值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如 通过声音文件库（`pip install soundfile`）。 要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充并转换为`torch.FloatTensor`类型的张量。 有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）- 用于避免在填充标记索引上执行卷积和注意力的掩码。 选择的掩码值在`[0, 1]`中：

    +   对于“未屏蔽”的标记，

    +   对于被 `masked` 的标记为 0。

    [注意力掩码是什么？](../glossary#attention-mask)

    只有在相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有 `config.return_attention_mask == False` 的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应 `不` 传递 `attention_mask` 以避免性能下降。对于这些模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask` 的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。

+   `output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *optional*) — 是否返回 [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput) 而不是普通元组。

+   `labels` (`torch.LongTensor`，形状为 `(batch_size,)`，*optional*) — 用于计算序列分类/回归损失的标签。索引应在 `[0, ..., config.num_labels - 1]` 范围内。如果 `config.num_labels == 1`，则计算回归损失（均方损失），如果 `config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.TokenClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入不同元素。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*optional*，当提供 `labels` 时返回) — 分类损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, sequence_length, config.num_labels)`) — 分类分数（SoftMax 之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组（如果模型有嵌入层的输出，则为嵌入层的输出 + 每层的输出）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组（每层一个）。

    在注意力 softmax 之后的注意力权重，用于计算自注意力头中的加权平均值。

[Wav2Vec2ForAudioFrameClassification](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用 `Module` 实例而不是这个，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForAudioFrameClassification
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("anton-l/wav2vec2-base-superb-sd")
>>> model = Wav2Vec2ForAudioFrameClassification.from_pretrained("anton-l/wav2vec2-base-superb-sd")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt", sampling_rate=sampling_rate)
>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> probabilities = torch.sigmoid(logits[0])
>>> # labels is a one-hot array of shape (num_frames, num_speakers)
>>> labels = (probabilities > 0.5).long()
>>> labels[0].tolist()
[0, 0]
```

## Wav2Vec2ForXVector

### `class transformers.Wav2Vec2ForXVector`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2305)

```py
( config )
```

参数

+   `config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

Wav2Vec2模型在顶部具有XVector特征提取头，用于Speaker Verification等任务。

Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli提出的[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)。

这个模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存等）。

这个模型是PyTorch的[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L2373)

```py
( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.XVectorOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，*例如*通过soundfile库（`pip install soundfile`）。要准备好数组以获得`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在填充标记索引上执行卷积和注意力的掩码。选择在`[0, 1]`范围内的掩码值：

    +   对于未被`masked`的标记，值为1，

    +   对于被`masked`的标记，值为0。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有在相应的处理器具有`config.return_attention_mask == True`时才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地填充为0并在不传递`attention_mask`的情况下传递。请注意，这些模型的结果也会因`input_values`是否填充而略有不同。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算序列分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput) 或 `tuple(torch.FloatTensor)`

一个 [transformers.modeling_outputs.XVectorOutput](/docs/transformers/v4.37.2/zh/main_classes/output#transformers.modeling_outputs.XVectorOutput) 或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或当 `config.return_dict=False` 时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。

+   `loss` (`torch.FloatTensor`，形状为 `(1,)`，*可选*，当提供 `labels` 时返回）— 分类损失。

+   `logits` (`torch.FloatTensor`，形状为 `(batch_size, config.xvector_output_dim)`) — AMSoftmax 之前的分类隐藏状态。

+   `embeddings` (`torch.FloatTensor`，形状为 `(batch_size, config.xvector_output_dim)`) — 用于基于向量相似性的检索的话语嵌入。

+   `hidden_states` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_hidden_states=True` 或当 `config.output_hidden_states=True` 时返回）— 形状为 `(batch_size, sequence_length, hidden_size)` 的 `torch.FloatTensor` 元组。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions` (`tuple(torch.FloatTensor)`，*可选*，当传递 `output_attentions=True` 或当 `config.output_attentions=True` 时返回）— 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `torch.FloatTensor` 元组。

    注意力权重在注意力 softmax 之后，用于计算自注意力头中的加权平均值。

[Wav2Vec2ForXVector](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector) 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在此之后调用 `Module` 实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForXVector
>>> from datasets import load_dataset
>>> import torch

>>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
>>> dataset = dataset.sort("id")
>>> sampling_rate = dataset.features["audio"].sampling_rate

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("anton-l/wav2vec2-base-superb-sv")
>>> model = Wav2Vec2ForXVector.from_pretrained("anton-l/wav2vec2-base-superb-sv")

>>> # audio file is decoded on the fly
>>> inputs = feature_extractor(
...     [d["array"] for d in dataset[:2]["audio"]], sampling_rate=sampling_rate, return_tensors="pt", padding=True
... )
>>> with torch.no_grad():
...     embeddings = model(**inputs).embeddings

>>> embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()

>>> # the resulting embeddings can be used for cosine similarity-based retrieval
>>> cosine_sim = torch.nn.CosineSimilarity(dim=-1)
>>> similarity = cosine_sim(embeddings[0], embeddings[1])
>>> threshold = 0.7  # the optimal threshold is dataset-dependent
>>> if similarity < threshold:
...     print("Speakers are not the same!")
>>> round(similarity.item(), 2)
0.98
```

## Wav2Vec2ForPreTraining

`transformers.Wav2Vec2ForPreTraining` 类

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1591)

```py
( config: Wav2Vec2Config )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/zh/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法以加载模型权重。

Wav2Vec2 模型带有量化器和顶部的 `VQ` 头。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。

这个模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/zh/main_classes/model#transformers.PreTrainedModel)。查看超类文档以获取库实现的所有模型的通用方法（如下载或保存等）。

这个模型是 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) 的子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_wav2vec2.py#L1652)

```py
( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None sampled_negative_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`）— 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，*例如*通过soundfile库（`pip install soundfile`）。要准备好数组以获得`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`torch.FloatTensor`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于`未屏蔽`的标记，

    +   对于`被屏蔽`的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

    只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，比如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应`不`传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `mask_time_indices`（形状为`(batch_size, sequence_length)`的`torch.BoolTensor`，*可选*）— 用于对比损失中掩盖提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测被掩盖的提取特征。

+   `sampled_negative_indices`（形状为`(batch_size, sequence_length, num_negatives)`的`torch.BoolTensor`，*可选*）— 指示哪些量化目标向量在对比损失中用作负采样向量的索引。预训练所需的输入。

返回

[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)或`tuple(torch.FloatTensor)`

一个[transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入而异的各种元素。

+   `loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`）— 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。 （分类）损失。

+   `projected_states`（形状为`(batch_size, sequence_length, config.proj_codevector_dim)`的`torch.FloatTensor`）— 模型投影到*config.proj_codevector_dim*的隐藏状态，可用于预测被屏蔽的投影量化状态。

+   `projected_quantized_states`（形状为`(batch_size, sequence_length, config.proj_codevector_dim)`的`torch.FloatTensor`） — 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    每层输出的模型隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

+   `contrastive_loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`） — 对比损失（L_m），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。

+   `diversity_loss`（*可选*，当传递`sample_negative_indices`时返回，形状为`(1,)`的`torch.FloatTensor`） — 多样性损失（L_d），如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。

[Wav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining)的前向方法，覆盖`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import torch
>>> from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining
>>> from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
>>> from datasets import load_dataset

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
>>> model = Wav2Vec2ForPreTraining.from_pretrained("facebook/wav2vec2-base")

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> input_values = feature_extractor(ds[0]["audio"]["array"], return_tensors="pt").input_values  # Batch size 1

>>> # compute masked indices
>>> batch_size, raw_sequence_length = input_values.shape
>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()
>>> mask_time_indices = _compute_mask_indices(
...     shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2
... )
>>> sampled_negative_indices = _sample_negative_indices(
...     features_shape=(batch_size, sequence_length),
...     num_negatives=model.config.num_negatives,
...     mask_time_indices=mask_time_indices,
... )
>>> mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)
>>> sampled_negative_indices = torch.tensor(
...     data=sampled_negative_indices, device=input_values.device, dtype=torch.long
... )

>>> with torch.no_grad():
...     outputs = model(input_values, mask_time_indices=mask_time_indices)

>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)

>>> # show that cosine similarity is much higher than random
>>> cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5
tensor(True)

>>> # for contrastive loss training model should be put into train mode
>>> model = model.train()
>>> loss = model(
...     input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices
... ).loss
```

TensorFlow隐藏TensorFlow内容

## TFWav2Vec2Model

### `class transformers.TFWav2Vec2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1509)

```py
( config: Wav2Vec2Config *inputs **kwargs )
```

参数

+   `config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)） — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸TFWav2Vec2模型变压器输出原始隐藏状态，没有特定的头部。

该模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

该模型也是[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规TF 2.0 Keras模型，并参考TF 2.0文档以获取有关一般用法和行为的所有信息。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需传递您的输入和标签以任何`model.fit()`支持的格式！但是，如果您想在Keras方法之外使用第二种格式，例如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可用于收集第一个位置参数中的所有输入张量：

+   一个仅包含`input_values`的单个张量，没有其他内容：`model(input_values)`

+   一个长度不同的列表，其中包含按照文档字符串中给定的顺序的一个或多个输入张量：`model([input_values, attention_mask])`或`model([input_values, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_values": input_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心任何这些，因为您可以像将输入传递给任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1519)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: bool = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFBaseModelOutput or tuple(tf.Tensor)
```

参数

+   `input_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`({0})`）— 词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 用于避免在填充标记索引上执行注意力的掩码。掩码值在`[0, 1]`中选择：

    +   对于`未屏蔽`的标记，

    +   对于`被屏蔽`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 段标记索引，用于指示输入的第一部分和第二部分。索引在`[0, 1]`中选择：

    +   0 对应于*句子A*标记。

    +   1 对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）— 每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）— 用于使自注意力模块的选定头部无效的掩码。掩码值在`[0, 1]`中选择：

    +   1 表示头部未`被屏蔽`，

    +   0 表示头部是`屏蔽`的。

+   `inputs_embeds`（形状为`({0}, hidden_size)`的`np.ndarray`或`tf.Tensor`，*可选*）— 可选地，您可以选择直接传递嵌入表示，而不是传递`input_values`。如果您想要更多控制权来将`input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下可用，在图模式下将使用配置中的值。

+   `return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training` (`bool`, *可选*，默认为`False“) — 是否在训练模式下使用模型（一些模块如dropout模块在训练和评估之间有不同的行为）。

返回

[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或`tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFBaseModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含根据配置([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config))和输入的各种元素。

+   `last_hidden_state` (`形状为(batch_size, sequence_length, hidden_size)的tf.Tensor`) — 模型最后一层的隐藏状态序列。

+   `hidden_states` (`tuple(tf.FloatTensor)`, *可选*，在传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(tf.Tensor)`, *可选*，在传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[TFWav2Vec2Model](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model)的前向方法，覆盖`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例:

```py
>>> from transformers import AutoProcessor, TFWav2Vec2Model
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = TFWav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
>>> hidden_states = model(input_values).last_hidden_state
```

## TFWav2Vec2ForSequenceClassification

### `class transformers.TFWav2Vec2ForSequenceClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1758)

```py
( config )
```

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1799)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None output_attentions: bool | None = None output_hidden_states: bool | None = None return_dict: bool | None = None labels: tf.Tensor | None = None training: bool = False )
```

## TFWav2Vec2ForCTC

### `class transformers.TFWav2Vec2ForCTC`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1591)

```py
( config: Wav2Vec2Config *inputs **kwargs )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

TFWav2Vec2模型，在Connectionist Temporal Classification (CTC)顶部具有`语言建模`头。

这个模型继承自[TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel)。查看超类文档，了解库为其所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。

这个模型也是一个[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)子类。将其用作常规的TF 2.0 Keras模型，并参考TF 2.0文档以获取与一般用法和行为相关的所有内容。

`transformers`中的TensorFlow模型和层接受两种格式的输入：

+   将所有输入作为关键字参数（类似于PyTorch模型），或者

+   将所有输入作为列表、元组或字典放在第一个位置参数中。

支持第二种格式的原因是Keras方法在将输入传递给模型和层时更喜欢这种格式。由于有了这种支持，当使用`model.fit()`等方法时，应该可以“正常工作” - 只需以`model.fit()`支持的任何格式传递输入和标签即可！但是，如果您想在Keras方法之外（如`fit()`和`predict()`）使用第二种格式，比如在使用Keras`Functional` API创建自己的层或模型时，有三种可能性可以用来收集第一个位置参数中的所有输入张量：

+   一个仅包含`input_values`的单个张量，没有其他内容：`model(input_values)`

+   一个长度可变的列表，其中包含一个或多个与文档字符串中给定的顺序相对应的输入张量：`model([input_values, attention_mask])`或`model([input_values, attention_mask, token_type_ids])`

+   一个字典，其中包含一个或多个与文档字符串中给定的输入名称相关联的输入张量：`model({"input_values": input_values, "token_type_ids": token_type_ids})`

请注意，当使用[子类化](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)创建模型和层时，您无需担心这些问题，因为您可以像将输入传递给任何其他Python函数一样传递输入！

#### `call`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py#L1625)

```py
( input_values: tf.Tensor attention_mask: tf.Tensor | None = None token_type_ids: tf.Tensor | None = None position_ids: tf.Tensor | None = None head_mask: tf.Tensor | None = None inputs_embeds: tf.Tensor | None = None output_attentions: Optional[bool] = None labels: tf.Tensor | None = None output_hidden_states: Optional[bool] = None return_dict: Optional[bool] = None training: Optional[bool] = False ) → export const metadata = 'undefined';transformers.modeling_tf_outputs.TFCausalLMOutput or tuple(tf.Tensor)
```

参数

+   `input_values`（`np.ndarray`，`tf.Tensor`，`List[tf.Tensor]`，`Dict[str, tf.Tensor]`或`Dict[str, np.ndarray]`，每个示例的形状必须为`({0})`）—词汇表中输入序列标记的索引。

    可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)和[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)。

    [什么是输入ID？](../glossary#input-ids)

+   `attention_mask`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—用于避免在填充标记索引上执行注意力的掩码。选择的掩码值为`[0, 1]`：

    +   1表示“未被掩码”的标记，

    +   0表示“被掩码”的标记。

    [什么是注意力掩码？](../glossary#attention-mask)

+   `token_type_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—段标记索引，用于指示输入的第一部分和第二部分。索引选择在`[0, 1]`中：

    +   0对应于*句子A*标记，

    +   1对应于*句子B*标记。

    [什么是标记类型ID？](../glossary#token-type-ids)

+   `position_ids`（形状为`({0})`的`np.ndarray`或`tf.Tensor`，*可选*）—每个输入序列标记在位置嵌入中的位置索引。在范围`[0, config.max_position_embeddings - 1]`中选择。

    [什么是位置ID？](../glossary#position-ids)

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`np.ndarray`或`tf.Tensor`，*可选*）—用于使自注意力模块中选择的头部失效的掩码。选择的掩码值为`[0, 1]`：

    +   1表示头部是`not masked`。

    +   0表示头部被`masked`。

+   `inputs_embeds`（形状为`({0}, hidden_size)`的`np.ndarray`或`tf.Tensor`，*可选*） — 可选地，可以直接传递嵌入表示，而不是传递`input_values`。如果您想要更多控制如何将`input_values`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，则这很有用。

+   `output_attentions`（`bool`，*可选*） — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `output_hidden_states`（`bool`，*可选*） — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。此参数仅在急切模式下使用，在图模式下将使用配置中的值。

+   `return_dict`（`bool`，*可选*） — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。此参数可在急切模式下使用，在图模式下该值将始终设置为True。

+   `training`（`bool`，*可选*，默认为`False`） — 是否在训练模式下使用模型（某些模块，如dropout模块，在训练和评估之间具有不同的行为）。

+   `labels`（形状为`(batch_size, sequence_length)`的`tf.Tensor`或`np.ndarray`，*可选*） — 用于计算掩码语言建模损失的标签。索引应在`[-100, 0, ..., config.vocab_size]`范围内（参见`input_values`文档字符串）。索引设置为`-100`的标记将被忽略（掩码），损失仅计算具有标签在`[0, ..., config.vocab_size]`范围内的标记。

返回

[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput) 或 `tuple(tf.Tensor)`

一个[transformers.modeling_tf_outputs.TFCausalLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutput)或一个`tf.Tensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，具体取决于配置（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）和输入。

+   `loss`（形状为`(n,)`的`tf.Tensor`，*可选*，其中n是非掩码标签的数量，在提供`labels`时返回） — 语言建模损失（用于下一个标记预测）。

+   `logits`（形状为`(batch_size, sequence_length, config.vocab_size)`的`tf.Tensor`） — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。

+   `hidden_states`（`tuple(tf.Tensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`tf.Tensor`元组（一个用于嵌入的输出 + 一个用于每一层的输出）。

    模型在每一层输出的隐藏状态以及初始嵌入输出。

+   `attentions`（`tuple(tf.Tensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`tf.Tensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[TFWav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForCTC)的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者则默默地忽略它们。

示例：

```py
>>> import tensorflow as tf
>>> from transformers import AutoProcessor, TFWav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
>>> model = TFWav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(ds["speech"][0], return_tensors="tf").input_values  # Batch size 1
>>> logits = model(input_values).logits
>>> predicted_ids = tf.argmax(logits, axis=-1)

>>> transcription = processor.decode(predicted_ids[0])

>>> # compute loss
>>> target_transcription = "A MAN SAID TO THE UNIVERSE SIR I EXIST"

>>> # Pass transcription as `text` to encode labels
>>> labels = processor(text=transcription, return_tensors="tf").input_ids

>>> loss = model(input_values, labels=labels).loss
```

JAXHide JAX内容

## FlaxWav2Vec2Model

### `class transformers.FlaxWav2Vec2Model`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1051)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config`（[Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)）- 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为`jax.numpy.float32`）- 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定了`dtype`，则所有计算将使用给定的`dtype`执行。

    `请注意，这仅指定计算的dtype，不影响模型参数的dtype。`

    如果要更改模型参数的dtype，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

裸Wav2Vec2模型变压器输出原始隐藏状态，没有特定的头部。Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli在[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)中提出的。

此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。检查超类文档以获取库为其所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型也是Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。

最后，此模型支持JAX的固有功能，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)

```py
( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`）- 输入原始语音波形的浮点值。值可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得，例如通过soundfile库（`pip install soundfile`）。要将数组准备成`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`jnp.ndarray`类型的张量。有关详细信息，请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）- 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记，值为1，

    +   对于被`masked`的标记，值为0。

    [什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应`不`传递`attention_mask`以避免性能下降。对于这样的模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `mask_time_indices`（形状为`(batch_size, sequence_length)`的`jnp.ndarray`，*可选*）- 用于对比损失中掩码提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测掩码提取特征。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多细节，请查看返回的张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多细节，请查看返回的张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)或者`tuple(torch.FloatTensor)`

一个[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2BaseModelOutput)或者一个`torch.FloatTensor`的元组（如果传递了`return_dict=False`或者`config.return_dict=False`）包含不同的元素，取决于配置（`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`）和输入。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`）- 模型最后一层的隐藏状态序列。

+   `extract_features`（形状为`(batch_size, sequence_length, last_conv_dim)`的`jnp.ndarray`）- 模型最后一个卷积层的提取特征向量序列，其中`last_conv_dim`是最后一个卷积层的维度。

+   `hidden_states`（`tuple(jnp.ndarray)`，*可选*，当传递`output_hidden_states=True`或者`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出 + 一个用于每个层的输出）。

    模型在每一层的输出的隐藏状态加上初始嵌入输出。

+   `attentions`（`tuple(jnp.ndarray)`，*可选*，当传递`output_attentions=True`或者`config.output_attentions=True`时返回）- 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxWav2Vec2PreTrainedModel`的前向方法，覆盖了`__call__`特殊方法。

尽管前向传递的方法需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoProcessor, FlaxWav2Vec2Model
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-large-lv60")
>>> model = FlaxWav2Vec2Model.from_pretrained("facebook/wav2vec2-large-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(
...     ds["speech"][0], sampling_rate=16_000, return_tensors="np"
... ).input_values  # Batch size 1
>>> hidden_states = model(input_values).last_hidden_state
```

## FlaxWav2Vec2ForCTC

### `class transformers.FlaxWav2Vec2ForCTC`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1169)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) 方法以加载模型权重。

+   `dtype`（`jax.numpy.dtype`，*可选*，默认为 `jax.numpy.float32`） — 计算的数据类型。可以是 `jax.numpy.float32`、`jax.numpy.float16`（在 GPU 上）和 `jax.numpy.bfloat16`（在 TPU 上）之一。

    这可用于在 GPU 或 TPU 上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的 `dtype` 执行。

    `请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅 [to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16) 和 [to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

Wav2Vec2 模型在顶部带有“语言建模”头部，用于 Connectionist Temporal Classification (CTC)。Wav2Vec2 是由 Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli 在 [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) 中提出的。

此模型继承自 [FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是 Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) 子类。将其用作常规 Flax 模块，并参考 Flax 文档以了解所有与一般用法和行为相关的事项。

最后，此模型支持 JAX 的内在特性，例如：

+   [即时 (JIT) 编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L888)

```py
( input_values attention_mask = None mask_time_indices = None params: dict = None dropout_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_flax_outputs.FlaxMaskedLMOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values`（形状为 `(batch_size, sequence_length)` 的 `jnp.ndarray`） — 输入原始语音波形的浮点值。可以通过将 `.flac` 或 `.wav` 音频文件加载到类型为 `List[float]` 或 `numpy.ndarray` 的数组中获得。要将数组准备成 `input_values`，应使用 [AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor) 进行填充和转换为类型为 `jnp.ndarray` 的张量。有关详细信息，请参阅 [Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask`（形状为 `(batch_size, sequence_length)` 的 `jnp.ndarray`，*可选*） — 用于避免在填充标记索引上执行卷积和注意力的掩码。掩码值选在 `[0, 1]`：

    +   1 用于“未掩码”标记，

    +   0 用于“掩码”标记。

    [什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有 `config.return_attention_mask == True` 时才应传递 `attention_mask`。对于所有处理器具有 `config.return_attention_mask == False` 的模型，例如 [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推理时，应该 `不` 传递 `attention_mask` 以避免性能下降。对于这样的模型，`input_values` 应该简单地用 0 填充并在不传递 `attention_mask` 的情况下传递。请注意，这些模型根据 `input_values` 是否填充会产生略有不同的结果。

+   `mask_time_indices` (`jnp.ndarray`，形状为 `(batch_size, sequence_length)`，*可选*) — 用于对比损失掩盖提取特征的索引。在训练模式下，模型学习在 *config.proj_codevector_dim* 空间中预测掩盖的提取特征。

+   `output_attentions` (`bool`, *可选*) — 是否返回所有注意力层的注意力张量。有关更多细节，请参阅返回张量下的 `attentions`。

+   `output_hidden_states` (`bool`, *可选*) — 是否返回所有层的隐藏状态。有关更多细节，请参阅返回张量下的 `hidden_states`。

+   `return_dict` (`bool`, *可选*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

返回

[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput) 或 `tuple(torch.FloatTensor)`

一个[transformers.modeling_flax_outputs.FlaxMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput)或一个 `torch.FloatTensor` 元组（如果传递 `return_dict=False` 或 `config.return_dict=False`）包含不同元素，取决于配置（`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`）和输入。

+   `logits` (`jnp.ndarray`，形状为 `(batch_size, sequence_length, config.vocab_size)`) — 语言建模头的预测分数（SoftMax 前每个词汇标记的分数）。

+   `hidden_states` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_hidden_states=True` 或 `config.output_hidden_states=True` 时返回) — 形状为 `(batch_size, sequence_length, hidden_size)` 的 `jnp.ndarray` 元组（一个用于嵌入输出，一个用于每一层的输出）。

    模型在每一层输出处的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`, *可选*, 当传递 `output_attentions=True` 或 `config.output_attentions=True` 时返回) — 形状为 `(batch_size, num_heads, sequence_length, sequence_length)` 的 `jnp.ndarray` 元组（每一层一个）。

    注意力 softmax 后的注意力权重，用于计算自注意力头中的加权平均值。

`FlaxWav2Vec2PreTrainedModel` 的前向方法，覆盖了 `__call__` 特殊方法。

虽然前向传递的步骤需要在这个函数内定义，但应该在之后调用 `Module` 实例而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> import jax.numpy as jnp
>>> from transformers import AutoProcessor, FlaxWav2Vec2ForCTC
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-large-960h-lv60")
>>> model = FlaxWav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = processor(
...     ds["speech"][0], sampling_rate=16_000, return_tensors="np"
... ).input_values  # Batch size 1
>>> logits = model(input_values).logits
>>> predicted_ids = jnp.argmax(logits, axis=-1)

>>> transcription = processor.decode(predicted_ids[0])
>>> # should give:  "A MAN SAID TO THE UNIVERSE SIR I EXIST"
```

## FlaxWav2Vec2ForPreTraining

### `class transformers.FlaxWav2Vec2ForPreTraining`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1318)

```py
( config: Wav2Vec2Config input_shape: Tuple = (1, 1024) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True **kwargs )
```

参数

+   `config` ([Wav2Vec2Config](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Config)) — 模型配置类，包含模型的所有参数。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained)方法以加载模型权重。

+   `dtype` (`jax.numpy.dtype`, *可选*, 默认为`jax.numpy.float32`) — 计算的数据类型。可以是`jax.numpy.float32`、`jax.numpy.float16`（在GPU上）和`jax.numpy.bfloat16`（在TPU上）之一。

    这可用于在GPU或TPU上启用混合精度训练或半精度推断。如果指定，所有计算将使用给定的`dtype`进行。 

    `请注意，这仅指定计算的数据类型，不会影响模型参数的数据类型。`

    如果要更改模型参数的数据类型，请参阅[to_fp16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16)和[to_bf16()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16)。

带有量化器和顶部`VQ`头的Wav2Vec2模型。Wav2Vec2是由Alexei Baevski、Henry Zhou、Abdelrahman Mohamed、Michael Auli提出的[wav2vec 2.0:自监督学习语音表示的框架](https://arxiv.org/abs/2006.11477)。

此模型继承自[FlaxPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.FlaxPreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。

此模型还是Flax Linen [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html)子类。将其用作常规Flax模块，并参考Flax文档以获取有关一般用法和行为的所有相关信息。

最后，此模型支持JAX的固有特性，例如：

+   [即时（JIT）编译](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)

+   [自动微分](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)

+   [矢量化](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)

+   [并行化](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)

#### `__call__`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py#L1322)

```py
( input_values attention_mask = None mask_time_indices = None gumbel_temperature: int = 1 params: dict = None dropout_rng: PRNGKey = None gumbel_rng: PRNGKey = None train: bool = False output_attentions: Optional = None output_hidden_states: Optional = None freeze_feature_encoder: bool = False return_dict: Optional = None ) → export const metadata = 'undefined';transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput or tuple(torch.FloatTensor)
```

参数

+   `input_values` (`jnp.ndarray` of shape `(batch_size, sequence_length)`) — 输入原始语音波形的浮点值。可以通过将`.flac`或`.wav`音频文件加载到`List[float]`类型的数组或`numpy.ndarray`中获得值，例如通过soundfile库（`pip install soundfile`）。要将数组准备为`input_values`，应使用[AutoProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoProcessor)进行填充和转换为`jnp.ndarray`类型的张量。详细信息请参阅[Wav2Vec2Processor.`call`()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor.__call__)。

+   `attention_mask` (`jnp.ndarray` of shape `(batch_size, sequence_length)`, *可选*) — 避免在填充标记索引上执行卷积和注意力的掩码。掩码值选择在`[0, 1]`中：

    +   对于未被`masked`的标记为1。

    +   对于被`masked`的标记为0。

    [什么是注意力掩码？](../glossary#attention-mask) .. 警告:: 只有当相应的处理器具有`config.return_attention_mask == True`时，才应传递`attention_mask`。对于所有处理器具有`config.return_attention_mask == False`的模型，例如[wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h)，在进行批量推断时，应避免传递`attention_mask`以避免性能下降。对于这些模型，`input_values`应该简单地用0填充并在不传递`attention_mask`的情况下传递。请注意，这些模型根据`input_values`是否填充会产生略有不同的结果。

+   `mask_time_indices` (`jnp.ndarray`，形状为`(batch_size, sequence_length)`，*optional*） — 用于对比损失中掩码提取特征的索引。在训练模式下，模型学习在*config.proj_codevector_dim*空间中预测掩码提取特征。

+   `output_attentions` (`bool`, *optional`) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量下的`attentions`。

+   `output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量下的`hidden_states`。

+   `return_dict` (`bool`, *optional*） — 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

返回

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput) 或 `tuple(torch.FloatTensor)`

[transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.models.wav2vec2.modeling_flax_wav2vec2.FlaxWav2Vec2ForPreTrainingOutput) 或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含根据配置（`<class 'transformers.models.wav2vec2.configuration_wav2vec2.Wav2Vec2Config'>`）和输入的不同元素。

+   `loss` (*optional*，在训练模式下返回，形状为`(1,)`的`jnp.ndarray`) — 总损失，作为对比损失（L_m）和多样性损失（L_d）的总和，如[官方论文](https://arxiv.org/pdf/2006.11477.pdf)中所述。 （分类）损失。

+   `projected_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`） — 模型的隐藏状态投影到*config.proj_codevector_dim*，可用于预测掩码的投影量化状态。

+   `projected_quantized_states` (`jnp.ndarray`，形状为`(batch_size, sequence_length, config.proj_codevector_dim)`） — 量化提取的特征向量投影到*config.proj_codevector_dim*，表示对比损失的正目标向量。

+   `hidden_states` (`tuple(jnp.ndarray)`，*optional*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回） — 形状为`(batch_size, sequence_length, hidden_size)`的`jnp.ndarray`元组（一个用于嵌入的输出，一个用于每一层的输出）。

    模型在每一层输出的隐藏状态加上初始嵌入输出。

+   `attentions` (`tuple(jnp.ndarray)`，*optional*，当传递`output_attentions=True`或`config.output_attentions=True`时返回） — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`jnp.ndarray`元组（每层一个）。

    注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。

[FlaxWav2Vec2ForPreTraining](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining)的前向方法，覆盖`__call__`特殊方法。

尽管前向传播的配方需要在这个函数内定义，但应该在此之后调用`Module`实例，而不是这个函数，因为前者负责运行预处理和后处理步骤，而后者则会默默地忽略它们。

示例：

```py
>>> import optax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from transformers import AutoFeatureExtractor, FlaxWav2Vec2ForPreTraining
>>> from transformers.models.wav2vec2.modeling_flax_wav2vec2 import _compute_mask_indices
>>> from datasets import load_dataset
>>> import soundfile as sf

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-large-lv60")
>>> model = FlaxWav2Vec2ForPreTraining.from_pretrained("facebook/wav2vec2-large-lv60")

>>> def map_to_array(batch):
...     speech, _ = sf.read(batch["file"])
...     batch["speech"] = speech
...     return batch

>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
>>> ds = ds.map(map_to_array)

>>> input_values = feature_extractor(ds["speech"][0], return_tensors="np").input_values  # Batch size 1

>>> # compute masked indices
>>> batch_size, raw_sequence_length = input_values.shape
>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
>>> mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=0.2, mask_length=2)

>>> outputs = model(input_values, mask_time_indices=mask_time_indices)

>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
>>> cosine_sim = optax.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states)

>>> # show that cosine similarity is much higher than random
>>> assert np.asarray(cosine_sim)[mask_time_indices].mean() > 0.5
```
