# MMS

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms)

## æ¦‚è¿°

MMS æ¨¡å‹æ˜¯ç”± Vineel Pratapã€Andros Tjandraã€Bowen Shiã€Paden Tomaselloã€Arun Babuã€Sayani Kunduã€Ali Elkahkyã€Zhaoheng Niã€Apoorv Vyasã€Maryam Fazel-Zarandiã€Alexei Baevskiã€Yossi Adiã€Xiaohui Zhangã€Wei-Ning Hsuã€Alexis Conneauã€Michael Auli åœ¨[å°†è¯­éŸ³æŠ€æœ¯æ‰©å±•åˆ° 1000 å¤šç§è¯­è¨€](https://arxiv.org/abs/2305.13516)ä¸­æå‡ºçš„ã€‚

è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š

*æ‰©å¤§è¯­éŸ³æŠ€æœ¯çš„è¯­è¨€è¦†ç›–èŒƒå›´æœ‰å¯èƒ½æé«˜æ›´å¤šäººè·å–ä¿¡æ¯çš„æœºä¼šã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯­éŸ³æŠ€æœ¯ä»…é™äºå¤§çº¦ 100 ç§è¯­è¨€ï¼Œè¿™åªæ˜¯å…¨çƒçº¦ 7000 ç§è¯­è¨€ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚å¤§è§„æ¨¡å¤šè¯­è¨€è¯­éŸ³ï¼ˆMMSï¼‰é¡¹ç›®é€šè¿‡ 10-40 å€å¢åŠ äº†æ”¯æŒçš„è¯­è¨€æ•°é‡ï¼Œå…·ä½“å–å†³äºä»»åŠ¡ã€‚ä¸»è¦æˆåˆ†æ˜¯åŸºäºå…¬å¼€å¯ç”¨å®—æ•™æ–‡æœ¬çš„æ–°æ•°æ®é›†ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ã€‚æˆ‘ä»¬æ„å»ºäº†è¦†ç›– 1406 ç§è¯­è¨€çš„é¢„è®­ç»ƒ wav2vec 2.0 æ¨¡å‹ï¼Œä¸€ç§å•ä¸€çš„æ”¯æŒ 1107 ç§è¯­è¨€çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œä»¥åŠç›¸åŒæ•°é‡è¯­è¨€çš„è¯­éŸ³åˆæˆæ¨¡å‹ï¼Œä»¥åŠæ”¯æŒ 4017 ç§è¯­è¨€çš„è¯­è¨€è¯†åˆ«æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šè¯­è¨€è¯­éŸ³è¯†åˆ«æ¨¡å‹åœ¨ FLEURS åŸºå‡†æµ‹è¯•çš„ 54 ç§è¯­è¨€ä¸Šå°† Whisper çš„è¯é”™è¯¯ç‡å‡å°‘äº†ä¸€åŠä»¥ä¸Šï¼ŒåŒæ—¶åœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨äº†å°‘é‡æ ‡è®°æ•°æ®ã€‚*

ä»¥ä¸‹æ˜¯ MMS é¡¹ç›®ä¸­å¼€æºçš„ä¸åŒæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å’Œä»£ç æœ€åˆå‘å¸ƒåœ¨[è¿™é‡Œ](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)ã€‚æˆ‘ä»¬å·²å°†å®ƒä»¬æ·»åŠ åˆ°`transformers`æ¡†æ¶ä¸­ï¼Œä½¿å…¶æ›´æ˜“äºä½¿ç”¨ã€‚

### è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰

ASR æ¨¡å‹çš„æ£€æŸ¥ç‚¹å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š[mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102)ï¼Œ[mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107)ï¼Œ[mms-1b-all](https://huggingface.co/facebook/mms-1b-all)ã€‚ä¸ºäº†è·å¾—æœ€ä½³å‡†ç¡®æ€§ï¼Œè¯·ä½¿ç”¨`mms-1b-all`æ¨¡å‹ã€‚

æç¤ºï¼š

+   æ‰€æœ‰ ASR æ¨¡å‹éƒ½æ¥å—ä¸€ä¸ªä¸è¯­éŸ³ä¿¡å·çš„åŸå§‹æ³¢å½¢å¯¹åº”çš„æµ®ç‚¹æ•°ç»„ã€‚åŸå§‹æ³¢å½¢åº”è¯¥ä½¿ç”¨ Wav2Vec2FeatureExtractor è¿›è¡Œé¢„å¤„ç†ã€‚

+   è¿™äº›æ¨¡å‹æ˜¯ä½¿ç”¨è¿æ¥ä¸»ä¹‰æ—¶é—´åˆ†ç±»ï¼ˆCTCï¼‰è¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤å¿…é¡»ä½¿ç”¨ Wav2Vec2CTCTokenizer å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè§£ç ã€‚

+   æ‚¨å¯ä»¥é€šè¿‡ load_adapter()ä¸ºä¸åŒè¯­è¨€åŠ è½½ä¸åŒçš„è¯­è¨€é€‚é…å™¨æƒé‡ã€‚è¯­è¨€é€‚é…å™¨ä»…åŒ…å«å¤§çº¦ 200 ä¸‡ä¸ªå‚æ•°ï¼Œå› æ­¤åœ¨éœ€è¦æ—¶å¯ä»¥é«˜æ•ˆåœ°åŠ¨æ€åŠ è½½ã€‚

#### åŠ è½½

é»˜è®¤æƒ…å†µä¸‹ï¼ŒMMS ä¼šåŠ è½½è‹±è¯­çš„é€‚é…å™¨æƒé‡ã€‚å¦‚æœæ‚¨æƒ³åŠ è½½å¦ä¸€ç§è¯­è¨€çš„é€‚é…å™¨æƒé‡ï¼Œè¯·ç¡®ä¿æŒ‡å®š`target_lang=<your-chosen-target-lang>`ä»¥åŠ`"ignore_mismatched_sizes=True`ã€‚å¿…é¡»ä¼ é€’`ignore_mismatched_sizes=True`å…³é”®å­—ï¼Œä»¥å…è®¸è¯­è¨€æ¨¡å‹å¤´æ ¹æ®æŒ‡å®šè¯­è¨€çš„è¯æ±‡é‡æ–°è°ƒæ•´å¤§å°ã€‚åŒæ ·ï¼Œå¤„ç†å™¨åº”è¯¥åŠ è½½ç›¸åŒçš„ç›®æ ‡è¯­è¨€ã€‚

```py
from transformers import Wav2Vec2ForCTC, AutoProcessor

model_id = "facebook/mms-1b-all"
target_lang = "fra"

processor = AutoProcessor.from_pretrained(model_id, target_lang=target_lang)
model = Wav2Vec2ForCTC.from_pretrained(model_id, target_lang=target_lang, ignore_mismatched_sizes=True)
```

æ‚¨å¯ä»¥å®‰å…¨åœ°å¿½ç•¥è­¦å‘Šï¼Œä¾‹å¦‚ï¼š

```py
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:
- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated
- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

å¦‚æœè¦ä½¿ç”¨ ASR æµç¨‹ï¼Œå¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼åŠ è½½æ‰€é€‰çš„ç›®æ ‡è¯­è¨€ï¼š

```py
from transformers import pipeline

model_id = "facebook/mms-1b-all"
target_lang = "fra"

pipe = pipeline(model=model_id, model_kwargs={"target_lang": "fra", "ignore_mismatched_sizes": True})
```

#### æ¨ç†

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨è°ƒç”¨`~PretrainedModel.from_pretrained`ä¹‹åè¿è¡Œ MMS çš„æ¨ç†å¹¶æ›´æ”¹é€‚é…å™¨å±‚ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨[Datasets](https://github.com/huggingface/datasets)åŠ è½½ä¸åŒè¯­è¨€çš„éŸ³é¢‘æ•°æ®ã€‚

```py
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# French
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "fr", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
fr_sample = next(iter(stream_data))["audio"]["array"]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨

```py
from transformers import Wav2Vec2ForCTC, AutoProcessor
import torch

model_id = "facebook/mms-1b-all"

processor = AutoProcessor.from_pretrained(model_id)
model = Wav2Vec2ForCTC.from_pretrained(model_id)
```

ç°åœ¨æˆ‘ä»¬å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå°†å¤„ç†åçš„éŸ³é¢‘æ•°æ®ä¼ é€’ç»™æ¨¡å‹å¹¶è½¬å½•æ¨¡å‹è¾“å‡ºï¼Œå°±åƒæˆ‘ä»¬é€šå¸¸ä¸º Wav2Vec2ForCTC æ‰€åšçš„é‚£æ ·ã€‚

```py
inputs = processor(en_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

ids = torch.argmax(outputs, dim=-1)[0]
transcription = processor.decode(ids)
# 'joe keton disapproved of films and buster also had reservations about the media'
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†ç›¸åŒçš„æ¨¡å‹ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œå¹¶é€šè¿‡è°ƒç”¨æ–¹ä¾¿çš„ load_adapter()å‡½æ•°ä¸ºæ¨¡å‹å’Œ set_target_lang()ä¸ºåˆ†è¯å™¨åˆ‡æ¢è¯­è¨€é€‚é…å™¨ã€‚æˆ‘ä»¬å°†ç›®æ ‡è¯­è¨€ä½œä¸ºè¾“å…¥ä¼ é€’ - `"fra"`è¡¨ç¤ºæ³•è¯­ã€‚

```py
processor.tokenizer.set_target_lang("fra")
model.load_adapter("fra")

inputs = processor(fr_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

ids = torch.argmax(outputs, dim=-1)[0]
transcription = processor.decode(ids)
# "ce dernier est volÃ© tout au long de l'histoire romaine"
```

ä»¥åŒæ ·çš„æ–¹å¼ï¼Œè¯­è¨€å¯ä»¥åˆ‡æ¢ä¸ºæ‰€æœ‰å…¶ä»–æ”¯æŒçš„è¯­è¨€ã€‚è¯·æŸ¥çœ‹ï¼š

```py
processor.tokenizer.vocab.keys()
```

æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„è¯­è¨€ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æé«˜ ASR æ¨¡å‹çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨è¯­è¨€æ¨¡å‹è§£ç ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤å¤„çš„æ–‡æ¡£[here](https://huggingface.co/facebook/mms-1b-all)ã€‚

### è¯­éŸ³åˆæˆï¼ˆTTSï¼‰

MMS-TTS ä½¿ç”¨ä¸ VITS ç›¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨ v4.33 ä¸­æ·»åŠ åˆ°ğŸ¤— Transformers ä¸­ã€‚MMS ä¸ºé¡¹ç›®ä¸­çš„ 1100 å¤šç§è¯­è¨€ä¸­çš„æ¯ç§è¯­è¨€è®­ç»ƒäº†ä¸€ä¸ªå•ç‹¬çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚æ‰€æœ‰å¯ç”¨çš„æ£€æŸ¥ç‚¹éƒ½å¯ä»¥åœ¨ Hugging Face Hub ä¸Šæ‰¾åˆ°ï¼š[facebook/mms-tts](https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts)ï¼Œä»¥åŠ[VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits)ä¸‹çš„æ¨ç†æ–‡æ¡£ã€‚

#### æ¨ç†

è¦ä½¿ç”¨ MMS æ¨¡å‹ï¼Œé¦–å…ˆæ›´æ–°åˆ° Transformers åº“çš„æœ€æ–°ç‰ˆæœ¬ï¼š

```py
pip install --upgrade transformers accelerate
```

ç”±äº VITS ä¸­çš„åŸºäºæµçš„æ¨¡å‹æ˜¯éç¡®å®šæ€§çš„ï¼Œå› æ­¤æœ€å¥½è®¾ç½®ä¸€ä¸ªç§å­ä»¥ç¡®ä¿è¾“å‡ºçš„å¯é‡ç°æ€§ã€‚

+   å¯¹äºå…·æœ‰ç½—é©¬å­—æ¯è¡¨çš„è¯­è¨€ï¼Œä¾‹å¦‚è‹±è¯­æˆ–æ³•è¯­ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨åˆ†è¯å™¨å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„å¤„ç†ã€‚ä»¥ä¸‹ä»£ç ç¤ºä¾‹ä½¿ç”¨ MMS-TTS è‹±è¯­æ£€æŸ¥ç‚¹è¿è¡Œå‰å‘ä¼ é€’ï¼š

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

set_seed(555)  # make deterministic

with torch.no_grad():
   outputs = model(**inputs)

waveform = outputs.waveform[0]
```

ç”Ÿæˆçš„æ³¢å½¢å¯ä»¥ä¿å­˜ä¸º`.wav`æ–‡ä»¶ï¼š

```py
import scipy

scipy.io.wavfile.write("synthesized_speech.wav", rate=model.config.sampling_rate, data=waveform)
```

æˆ–è€…åœ¨ Jupyter Notebook / Google Colab ä¸­æ˜¾ç¤ºï¼š

```py
from IPython.display import Audio

Audio(waveform, rate=model.config.sampling_rate)
```

å¯¹äºå…·æœ‰éç½—é©¬å­—æ¯è¡¨çš„æŸäº›è¯­è¨€ï¼Œä¾‹å¦‚é˜¿æ‹‰ä¼¯è¯­ã€æ™®é€šè¯æˆ–å°åœ°è¯­ï¼Œéœ€è¦ä½¿ç”¨[`uroman`](https://github.com/isi-nlp/uroman) perl åŒ…å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„å¤„ç†ä¸ºç½—é©¬å­—æ¯è¡¨ã€‚

æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥é¢„è®­ç»ƒçš„`tokenizer`çš„`is_uroman`å±æ€§æ¥æ£€æŸ¥æ‚¨çš„è¯­è¨€æ˜¯å¦éœ€è¦`uroman`åŒ…ï¼š

```py
from transformers import VitsTokenizer

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
print(tokenizer.is_uroman)
```

å¦‚æœéœ€è¦ï¼Œåœ¨å°†æ–‡æœ¬è¾“å…¥ä¼ é€’ç»™`VitsTokenizer`ä¹‹å‰ï¼Œåº”è¯¥å°† uroman åŒ…åº”ç”¨äºæ‚¨çš„æ–‡æœ¬è¾“å…¥**ä¹‹å‰**ï¼Œå› ä¸ºç›®å‰åˆ†è¯å™¨ä¸æ”¯æŒæ‰§è¡Œé¢„å¤„ç†ã€‚

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œé¦–å…ˆå°† uroman å­˜å‚¨åº“å…‹éš†åˆ°æœ¬åœ°è®¡ç®—æœºï¼Œå¹¶å°† bash å˜é‡`UROMAN`è®¾ç½®ä¸ºæœ¬åœ°è·¯å¾„ï¼š

```py
git clone https://github.com/isi-nlp/uroman.git
cd uroman
export UROMAN=$(pwd)
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µå¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„å¤„ç†ã€‚æ‚¨å¯ä»¥ä¾èµ–ä½¿ç”¨ bash å˜é‡`UROMAN`æŒ‡å‘ uroman å­˜å‚¨åº“ï¼Œæˆ–è€…å¯ä»¥å°† uroman ç›®å½•ä½œä¸ºå‚æ•°ä¼ é€’ç»™`uromaize`å‡½æ•°ï¼š

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
import os
import subprocess

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-kor")
model = VitsModel.from_pretrained("facebook/mms-tts-kor")

def uromanize(input_string, uroman_path):
    """Convert non-Roman strings to Roman using the `uroman` perl package."""
    script_path = os.path.join(uroman_path, "bin", "uroman.pl")

    command = ["perl", script_path]

    process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    # Execute the perl command
    stdout, stderr = process.communicate(input=input_string.encode())

    if process.returncode != 0:
        raise ValueError(f"Error {process.returncode}: {stderr.decode()}")

    # Return the output as a string and skip the new-line character at the end
    return stdout.decode()[:-1]

text = "ì´ë´ ë¬´ìŠ¨ ì¼ì´ì•¼"
uromaized_text = uromanize(text, uroman_path=os.environ["UROMAN"])

inputs = tokenizer(text=uromaized_text, return_tensors="pt")

set_seed(555)  # make deterministic
with torch.no_grad():
   outputs = model(inputs["input_ids"])

waveform = outputs.waveform[0]
```

**æç¤ºï¼š**

+   MMS-TTS æ£€æŸ¥ç‚¹æ˜¯åœ¨å°å†™ã€æ— æ ‡ç‚¹çš„æ–‡æœ¬ä¸Šè®­ç»ƒçš„ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`VitsTokenizer`é€šè¿‡åˆ é™¤ä»»ä½•å¤§å°å†™å’Œæ ‡ç‚¹æ¥*è§„èŒƒåŒ–*è¾“å…¥ï¼Œä»¥é¿å…å°†è¶…å‡ºè¯æ±‡è¡¨çš„å­—ç¬¦ä¼ é€’ç»™æ¨¡å‹ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯¹å¤§å°å†™å’Œæ ‡ç‚¹ä¸æ•æ„Ÿï¼Œå› æ­¤åº”é¿å…åœ¨æ–‡æœ¬æç¤ºä¸­ä½¿ç”¨å®ƒä»¬ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨åˆ†è¯å™¨æ—¶è®¾ç½®`noramlize=False`æ¥ç¦ç”¨è§„èŒƒåŒ–ï¼Œä½†è¿™å°†å¯¼è‡´æ„å¤–è¡Œä¸ºï¼Œä¸å»ºè®®è¿™æ ·åšã€‚

+   é€šè¿‡å°†å±æ€§`model.speaking_rate`è®¾ç½®ä¸ºæ‰€é€‰å€¼ï¼Œå¯ä»¥æ”¹å˜è¯´è¯é€Ÿåº¦ã€‚åŒæ ·ï¼Œå™ªå£°çš„éšæœºæ€§ç”±`model.noise_scale`æ§åˆ¶ï¼š

```py
import torch
from transformers import VitsTokenizer, VitsModel, set_seed

tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
model = VitsModel.from_pretrained("facebook/mms-tts-eng")

inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

# make deterministic
set_seed(555)  

# make speech faster and more noisy
model.speaking_rate = 1.5
model.noise_scale = 0.8

with torch.no_grad():
   outputs = model(**inputs)
```

### è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰

æ ¹æ®èƒ½å¤Ÿè¯†åˆ«çš„è¯­è¨€æ•°é‡ï¼Œæä¾›ä¸åŒçš„ LID æ¨¡å‹ - [126](https://huggingface.co/facebook/mms-lid-126)ï¼Œ[256](https://huggingface.co/facebook/mms-lid-256)ï¼Œ[512](https://huggingface.co/facebook/mms-lid-512)ï¼Œ[1024](https://huggingface.co/facebook/mms-lid-1024)ï¼Œ[2048](https://huggingface.co/facebook/mms-lid-2048)ï¼Œ[4017](https://huggingface.co/facebook/mms-lid-4017)ã€‚

#### æ¨ç†

é¦–å…ˆï¼Œæˆ‘ä»¬å®‰è£… transformers å’Œå…¶ä»–ä¸€äº›åº“

```py
pip install torch accelerate datasets[audio]
pip install --upgrade transformers
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡`datasets`åŠ è½½ä¸€äº›éŸ³é¢‘æ ·æœ¬ã€‚ç¡®ä¿éŸ³é¢‘æ•°æ®é‡‡æ ·ç‡ä¸º 16000 kHzã€‚

```py
from datasets import load_dataset, Audio

# English
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "en", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
en_sample = next(iter(stream_data))["audio"]["array"]

# Arabic
stream_data = load_dataset("mozilla-foundation/common_voice_13_0", "ar", split="test", streaming=True)
stream_data = stream_data.cast_column("audio", Audio(sampling_rate=16000))
ar_sample = next(iter(stream_data))["audio"]["array"]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨

```py
from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor
import torch

model_id = "facebook/mms-lid-126"

processor = AutoFeatureExtractor.from_pretrained(model_id)
model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)
```

ç°åœ¨æˆ‘ä»¬å¤„ç†éŸ³é¢‘æ•°æ®ï¼Œå°†å¤„ç†åçš„éŸ³é¢‘æ•°æ®ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œåˆ†ç±»ï¼Œå°±åƒæˆ‘ä»¬é€šå¸¸å¯¹ Wav2Vec2 éŸ³é¢‘åˆ†ç±»æ¨¡å‹è¿›è¡Œçš„æ“ä½œä¸€æ ·ï¼Œæ¯”å¦‚[ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)

```py
# English
inputs = processor(en_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

lang_id = torch.argmax(outputs, dim=-1)[0].item()
detected_lang = model.config.id2label[lang_id]
# 'eng'

# Arabic
inputs = processor(ar_sample, sampling_rate=16_000, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits

lang_id = torch.argmax(outputs, dim=-1)[0].item()
detected_lang = model.config.id2label[lang_id]
# 'ara'
```

æŸ¥çœ‹æ£€æŸ¥ç‚¹æ”¯æŒçš„æ‰€æœ‰è¯­è¨€ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ‰“å°è¯­è¨€æ ‡è¯†ç¬¦ï¼š

```py
processor.id2label.values()
```

### éŸ³é¢‘é¢„è®­ç»ƒæ¨¡å‹

é¢„è®­ç»ƒæ¨¡å‹æä¾›ä¸¤ç§ä¸åŒå¤§å°çš„æ¨¡å‹ - [300M](https://huggingface.co/facebook/mms-300m)ï¼Œ[1Bil](https://huggingface.co/facebook/mms-1b)ã€‚

ASR æ¶æ„çš„ MMS åŸºäº Wav2Vec2 æ¨¡å‹ï¼Œè¯·å‚è€ƒ Wav2Vec2 çš„æ–‡æ¡£é¡µé¢ä»¥è·å–æœ‰å…³å¦‚ä½•é’ˆå¯¹å„ç§ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ¨¡å‹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

MMS-TTS ä½¿ç”¨ä¸ VITS ç›¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œè¯·å‚è€ƒ VITS çš„æ–‡æ¡£é¡µé¢è·å– API å‚è€ƒã€‚
