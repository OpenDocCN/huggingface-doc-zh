# 混合视觉Transformer（ViT Hybrid）

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/vit_hybrid)

## 概述

混合视觉Transformer（ViT）模型是由Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby在[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)中提出的。这是第一篇成功在ImageNet上训练Transformer编码器的论文，与熟悉的卷积架构相比取得了非常好的结果。ViT混合是[plain Vision Transformer](vit)的一个轻微变体，通过利用卷积骨干（具体来说是[BiT](bit)）的特征作为Transformer的初始“标记”。

该论文的摘要如下：

*尽管Transformer架构已经成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。在视觉中，注意力要么与卷积网络一起应用，要么用来替换卷积网络的某些组件，同时保持其整体结构不变。我们表明，在图像分类任务中，这种对CNN的依赖是不必要的，直接应用于图像块序列的纯Transformer可以在大量数据上进行预训练，并转移到多个中等或小型图像识别基准（ImageNet、CIFAR-100、VTAB等）时，Vision Transformer（ViT）可以取得与最先进的卷积网络相比非常好的结果，同时需要较少的计算资源来训练。*

这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。原始代码（使用JAX编写）可以在[这里](https://github.com/google-research/vision_transformer)找到。

## 资源

以下是官方Hugging Face和社区（由🌎表示）资源列表，可帮助您开始使用ViT Hybrid。

图像分类

+   [ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification) 是由这个[示例脚本](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) 和 [笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb) 支持的。

+   另请参阅：[图像分类任务指南](../tasks/image_classification)

如果您有兴趣提交资源以包含在此处，请随时打开一个Pull Request，我们将对其进行审查！资源应该理想地展示一些新内容，而不是重复现有资源。

## ViTHybridConfig

### `class transformers.ViTHybridConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/configuration_vit_hybrid.py#L32)

```py
( backbone_config = None hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob = 0.0 attention_probs_dropout_prob = 0.0 initializer_range = 0.02 layer_norm_eps = 1e-12 image_size = 224 patch_size = 1 num_channels = 3 backbone_featmap_shape = [1, 1024, 24, 24] qkv_bias = True **kwargs )
```

参数

+   `backbone_config` (`Union[Dict[str, Any], PretrainedConfig]`, *可选*) — 骨干的配置，可以是字典或骨干的配置对象。

+   `hidden_size` (`int`, *可选*, 默认为768) — 编码器层和池化器层的维度。

+   `num_hidden_layers` (`int`, *可选*, 默认为12) — Transformer编码器中的隐藏层数量。

+   `num_attention_heads` (`int`, *可选*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。

+   `intermediate_size` (`int`, *可选*, 默认为3072) — Transformer编码器中“中间”（即前馈）层的维度。

+   `hidden_act` (`str` or `function`, *可选*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"selu"`和`"gelu_new"`。

+   `hidden_dropout_prob` (`float`, *optional*, defaults to 0.0) — 嵌入、编码器和池化器中所有全连接层的丢失概率。

+   `attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.0) — 注意力概率的丢失比率。

+   `initializer_range` (`float`, *optional*, defaults to 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — 层归一化层使用的epsilon。

+   `image_size` (`int`, *optional*, defaults to 224) — 每个图像的大小（分辨率）。

+   `patch_size` (`int`, *optional*, defaults to 1) — 每个补丁的大小（分辨率）。

+   `num_channels` (`int`, *optional*, defaults to 3) — 输入通道的数量。

+   `backbone_featmap_shape` (`List[int]`, *optional*, defaults to `[1, 1024, 24, 24]`) — 仅用于`hybrid`嵌入类型。主干特征图的形状。

+   `qkv_bias` (`bool`, *optional*, defaults to `True`) — 是否为查询、键和值添加偏置。

这是一个配置类，用于存储[ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel)的配置。根据指定的参数实例化一个ViT Hybrid模型，定义模型架构。使用默认值实例化配置将产生类似于ViT Hybrid [google/vit-hybrid-base-bit-384](https://huggingface.co/google/vit-hybrid-base-bit-384)架构的配置。

配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。

示例：

```py
>>> from transformers import ViTHybridConfig, ViTHybridModel

>>> # Initializing a ViT Hybrid vit-hybrid-base-bit-384 style configuration
>>> configuration = ViTHybridConfig()

>>> # Initializing a model (with random weights) from the vit-hybrid-base-bit-384 style configuration
>>> model = ViTHybridModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## ViTHybridImageProcessor

### `class transformers.ViTHybridImageProcessor`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L50)

```py
( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BICUBIC: 3> do_center_crop: bool = True crop_size: Dict = None do_rescale: bool = True rescale_factor: Union = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None do_convert_rgb: bool = True **kwargs )
```

参数

+   `do_resize` (`bool`, *optional*, defaults to `True`) — 是否将图像的（高度，宽度）尺寸调整为指定的`size`。可以被`preprocess`方法中的`do_resize`覆盖。

+   `size` (`Dict[str, int]` *optional*, defaults to `{"shortest_edge" -- 224}`): 调整大小后的图像尺寸。图像的最短边被调整为size[“shortest_edge”]，最长边被调整以保持输入的长宽比。可以被`preprocess`方法中的`size`覆盖。

+   `resample` (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`) — 如果调整图像大小，要使用的重采样滤波器。可以被`preprocess`方法中的`resample`覆盖。

+   `do_center_crop` (`bool`, *optional*, defaults to `True`) — 是否将图像居中裁剪到指定的`crop_size`。可以被`preprocess`方法中的`do_center_crop`覆盖。

+   `crop_size` (`Dict[str, int]` *optional*, defaults to 224) — 应用`center_crop`后的输出图像大小。可以被`preprocess`方法中的`crop_size`覆盖。

+   `do_rescale` (`bool`, *optional*, defaults to `True`) — 是否按指定比例`rescale_factor`重新缩放图像。可以被`preprocess`方法中的`do_rescale`覆盖。

+   `rescale_factor` (`int` or `float`, *optional*, defaults to `1/255`) — 如果重新调整图像，则使用的比例因子。可以被`preprocess`方法中的`rescale_factor`覆盖。do_normalize — 是否对图像进行归一化。可以被`preprocess`方法中的`do_normalize`覆盖。

+   `image_mean` (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`) — 如果对图像进行归一化，则使用的均值。这是一个浮点数或与图像通道数相同长度的浮点数列表。可以被`preprocess`方法中的`image_mean`参数覆盖。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `IMAGENET_STANDARD_STD`) — 如果对图像进行归一化，则使用的标准差。这是一个浮点数或与图像中通道数相同长度的浮点数列表。可以通过 `preprocess` 方法中的 `image_std` 参数覆盖。可以通过 `preprocess` 方法中的 `image_std` 参数覆盖。

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `True`) — 是否将图像转换为 RGB。

构建一个 ViT Hybrid 图像处理器。

#### `preprocess`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/image_processing_vit_hybrid.py#L174)

```py
( images: Union do_resize: bool = None size: Dict = None resample: Resampling = None do_center_crop: bool = None crop_size: int = None do_rescale: bool = None rescale_factor: float = None do_normalize: bool = None image_mean: Union = None image_std: Union = None do_convert_rgb: bool = None return_tensors: Union = None data_format: Optional = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )
```

参数

+   `images` (`ImageInput`) — 要预处理的图像。期望单个图像或图像批处理，像素值范围为 0 到 255。如果传入像素值在 0 到 1 之间的图像，请设置 `do_rescale=False`。

+   `do_resize` (`bool`, *可选*, 默认为 `self.do_resize`) — 是否调整图像大小。

+   `size` (`Dict[str, int]`, *可选*, 默认为 `self.size`) — 调整大小后的图像尺寸。图像的最短边被调整为 size[“shortest_edge”]，最长边被调整以保持输入的长宽比。

+   `resample` (`int`, *可选*, 默认为 `self.resample`) — 如果调整图像大小，则要使用的重采样滤波器。这可以是枚举 `PILImageResampling` 之一。仅在 `do_resize` 设置为 `True` 时生效。

+   `do_center_crop` (`bool`, *可选*, 默认为 `self.do_center_crop`) — 是否对图像进行中心裁剪。

+   `crop_size` (`Dict[str, int]`, *可选*, 默认为 `self.crop_size`) — 中心裁剪的尺寸。仅在 `do_center_crop` 设置为 `True` 时生效。

+   `do_rescale` (`bool`, *可选*, 默认为 `self.do_rescale`) — 是否重新缩放图像。

+   `rescale_factor` (`float`, *可选*, 默认为 `self.rescale_factor`) — 如果 `do_rescale` 设置为 `True`，则对图像进行重新缩放的缩放因子。

+   `do_normalize` (`bool`, *可选*, 默认为 `self.do_normalize`) — 是否对图像进行归一化。

+   `image_mean` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_mean`) — 用于归一化的图像均值。仅在 `do_normalize` 设置为 `True` 时生效。

+   `image_std` (`float` 或 `List[float]`, *可选*, 默认为 `self.image_std`) — 用于归一化的图像标准差。仅在 `do_normalize` 设置为 `True` 时生效。

+   `do_convert_rgb` (`bool`, *可选*, 默认为 `self.do_convert_rgb`) — 是否将图像转换为 RGB。

+   `return_tensors` (`str` 或 `TensorType`, *可选*) — 要返回的张量类型。可以是以下之一：

    +   未设置：返回一个 `np.ndarray` 列表。

    +   `TensorType.TENSORFLOW` 或 `'tf'`：返回一个 `tf.Tensor` 类型的批处理。

    +   `TensorType.PYTORCH` 或 `'pt'`：返回一个 `torch.Tensor` 类型的批处理。

    +   `TensorType.NUMPY` 或 `'np'`：返回一个 `np.ndarray` 类型的批处理。

    +   `TensorType.JAX` 或 `'jax'`：返回一个 `jax.numpy.ndarray` 类型的批处理。

+   `data_format` (`ChannelDimension` 或 `str`, *可选*, 默认为 `ChannelDimension.FIRST`) — 输出图像的通道维度格式。可以是以下之一：

    +   `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   未设置：默认为输入图像的通道维度格式。

+   `input_data_format` (`ChannelDimension` 或 `str`, *可选*) — 输入图像的通道维度格式。如果未设置，则从输入图像中推断通道维度格式。可以是以下之一：

    +   `"channels_first"` 或 `ChannelDimension.FIRST`: 图像以 (通道数, 高度, 宽度) 格式。

    +   `"channels_last"` 或 `ChannelDimension.LAST`: 图像以 (高度, 宽度, 通道数) 格式。

    +   `"none"` 或 `ChannelDimension.NONE`: 图像以 (高度, 宽度) 格式。

预处理图像或图像批处理。

## ViTHybridModel

### `class transformers.ViTHybridModel`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L518)

```py
( config: ViTHybridConfig add_pooling_layer: bool = True use_mask_token: bool = False )
```

参数

+   `config`（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）- 模型的所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

裸的ViT混合模型变压器输出原始隐藏状态，没有特定的顶部头。此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有事项。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L548)

```py
( pixel_values: Optional = None bool_masked_pos: Optional = None head_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPooling or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）- 像素值。可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取像素值。有关详细信息，请参阅[ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）- 用于使自注意力模块的选定头部无效的掩码。在`[0, 1]`中选择的掩码值：

    +   1表示头部未被掩盖，

    +   0表示头部被掩盖。

+   `output_attentions`（`bool`，*可选*）- 是否返回所有注意力层的注意力张量。有关更多详细信息，请查看返回张量下的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）- 是否返回所有层的隐藏状态。有关更多详细信息，请查看返回张量下的`hidden_states`。

+   `return_dict`（`bool`，*可选*）- 是否返回[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是普通元组。

+   `bool_masked_pos`（形状为`(batch_size, num_patches)`的`torch.BoolTensor`，*可选*）- 布尔掩码位置。指示哪些补丁被掩盖（1）哪些没有（0）。

返回

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`

[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包括根据配置([ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig))和输入的各种元素。

+   `last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）- 模型最后一层的输出的隐藏状态序列。

+   `pooler_output`（形状为`(batch_size, hidden_size)`的`torch.FloatTensor`）- 序列第一个标记（分类标记）的最后一层隐藏状态（经过用于辅助预训练任务的层进一步处理后）。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层权重是从预训练期间的下一个句子预测（分类）目标中训练的。

+   `hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）- 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层的输出一个，+ 每一层的输出一个）。

    模型在每一层输出的隐藏状态以及可选的初始嵌入输出。

+   `attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）— 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。

[ViTHybridModel](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridModel)的前向方法，覆盖了`__call__`特殊方法。

虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ViTHybridModel
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-hybrid-base-bit-384")
>>> model = ViTHybridModel.from_pretrained("google/vit-hybrid-base-bit-384")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> list(last_hidden_states.shape)
[1, 197, 768]
```

## ViTHybridForImageClassification

### `class transformers.ViTHybridForImageClassification`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L634)

```py
( config: ViTHybridConfig )
```

参数

+   `config`（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）— 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。

ViT混合模型变压器，顶部带有图像分类头（在[CLS]标记的最终隐藏状态之上的线性层），例如用于ImageNet。

此模型是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。

#### `forward`

[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py#L655)

```py
( pixel_values: Optional = None head_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None interpolate_pos_encoding: Optional = None return_dict: Optional = None ) → export const metadata = 'undefined';transformers.modeling_outputs.ImageClassifierOutput or tuple(torch.FloatTensor)
```

参数

+   `pixel_values`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 像素值。像素值可以使用[AutoImageProcessor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoImageProcessor)获取。有关详细信息，请参阅[ViTHybridImageProcessor.`call`()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)。

+   `head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）— 用于使自注意力模块中选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：

    +   1表示头部未被屏蔽，

    +   0表示头部被`masked`。

+   `output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参阅返回张量中的`attentions`。

+   `output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参阅返回张量中的`hidden_states`。

+   `return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通的元组。

+   `labels`（形状为`(batch_size,)`的`torch.LongTensor`，*可选*）— 用于计算图像分类/回归损失的标签。索引应在`[0, ..., config.num_labels - 1]`范围内。如果`config.num_labels == 1`，则计算回归损失（均方损失），如果`config.num_labels > 1`，则计算分类损失（交叉熵）。

返回

[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或`tuple(torch.FloatTensor)`

一个[transformers.modeling_outputs.ImageClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`）包含各种元素，取决于配置（[ViTHybridConfig](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridConfig)）和输入。

+   `loss` (`torch.FloatTensor`，形状为`(1,)`, *optional*, 当提供`labels`时返回) — 分类（如果`config.num_labels==1`则为回归）损失。

+   `logits` (`torch.FloatTensor`，形状为`(batch_size, config.num_labels)`) — 分类（如果`config.num_labels==1`则为回归）得分（SoftMax之前）。

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回) — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入层的输出，如果模型有嵌入层，+ 一个用于每个阶段的输出）。模型在每个阶段输出的隐藏状态（也称为特征图）。

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, 当传递`output_attentions=True`或`config.output_attentions=True`时返回) — 形状为`(batch_size, num_heads, patch_size, sequence_length)`的`torch.FloatTensor`元组（每层一个）。

    在自注意力头中使用的注意力softmax之后的注意力权重，用于计算加权平均值。

[ViTHybridForImageClassification](/docs/transformers/v4.37.2/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification)的前向方法重写了`__call__`特殊方法。

虽然前向传递的配方需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者负责运行预处理和后处理步骤，而后者会默默地忽略它们。

示例：

```py
>>> from transformers import AutoImageProcessor, ViTHybridForImageClassification
>>> import torch
>>> from datasets import load_dataset

>>> dataset = load_dataset("huggingface/cats-image")
>>> image = dataset["test"]["image"][0]

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-hybrid-base-bit-384")
>>> model = ViTHybridForImageClassification.from_pretrained("google/vit-hybrid-base-bit-384")

>>> inputs = image_processor(image, return_tensors="pt")

>>> with torch.no_grad():
...     logits = model(**inputs).logits

>>> # model predicts one of the 1000 ImageNet classes
>>> predicted_label = logits.argmax(-1).item()
>>> print(model.config.id2label[predicted_label])
tabby, tabby cat
```
