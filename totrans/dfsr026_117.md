# VQModel

> 原文链接：[https://huggingface.co/docs/diffusers/api/models/vq](https://huggingface.co/docs/diffusers/api/models/vq)

VQ-VAE模型是由Aaron van den Oord、Oriol Vinyals和Koray Kavukcuoglu在[神经离散表示学习](https://huggingface.co/papers/1711.00937)中引入的。该模型用于在🤗 Diffusers中将潜在表示解码为图像。与[AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)不同，[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)在量化的潜在空间中工作。

论文摘要如下：

*在没有监督的情况下学习有用的表示仍然是机器学习中的一个关键挑战。在本文中，我们提出了一个简单而强大的生成模型，用于学习这种离散表示。我们的模型，即矢量量化变分自动编码器（VQ-VAE），与VAE在两个关键方面不同：编码器网络输出离散而不是连续的代码；先验是学习而不是静态的。为了学习离散的潜在表示，我们融入了矢量量化（VQ）的思想。使用VQ方法使模型能够避开“后验坍缩”的问题——即当潜在空间与强大的自回归解码器配对时，通常在VAE框架中观察到的问题。将这些表示与自回归先验配对，模型可以生成高质量的图像、视频和语音，以及进行高质量的说话人转换和无监督学习音素，进一步证明了学习表示的实用性。*

## VQModel

### `class diffusers.VQModel`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L40)

```py
( in_channels: int = 3 out_channels: int = 3 down_block_types: Tuple = ('DownEncoderBlock2D',) up_block_types: Tuple = ('UpDecoderBlock2D',) block_out_channels: Tuple = (64,) layers_per_block: int = 1 act_fn: str = 'silu' latent_channels: int = 3 sample_size: int = 32 num_vq_embeddings: int = 256 norm_num_groups: int = 32 vq_embed_dim: Optional = None scaling_factor: float = 0.18215 norm_type: str = 'group' mid_block_add_attention = True lookup_from_codebook = False force_upcast = False )
```

参数

+   `in_channels`（int，*可选*，默认为3）— 输入图像中的通道数。

+   `out_channels`（int，*可选*，默认为3）— 输出中的通道数。

+   `down_block_types`（`Tuple[str]`，*可选*，默认为`("DownEncoderBlock2D",)`）— 下采样块类型的元组。

+   `up_block_types`（`Tuple[str]`，*可选*，默认为`("UpDecoderBlock2D",)`）— 上采样块类型的元组。

+   `block_out_channels`（`Tuple[int]`，*可选*，默认为`(64,)`）— 块输出通道的元组。

+   `layers_per_block`（`int`，*可选*，默认为`1`）— 每个块的层数。

+   `act_fn`（`str`，*可选*，默认为`"silu"`）— 要使用的激活函数。

+   `latent_channels`（`int`，*可选*，默认为`3`）— 潜在空间中的通道数。

+   `sample_size`（`int`，*可选*，默认为`32`）— 样本输入大小。

+   `num_vq_embeddings`（`int`，*可选*，默认为`256`）— VQ-VAE中码书向量的数量。

+   `norm_num_groups`（`int`，*可选*，默认为`32`）— 规范化层的组数。

+   `vq_embed_dim`（`int`，*可选*）— VQ-VAE中码书向量的隐藏维度。

+   `scaling_factor`（`float`，*可选*，默认为`0.18215`）— 使用训练集的第一批计算出的训练潜在空间的分量标准差。这用于在训练扩散模型时将潜在空间缩放为单位方差。在传递给扩散模型之前，潜在空间会按照公式`z = z * scaling_factor`进行缩放。在解码时，潜在空间会按照公式进行缩放回原始比例：`z = 1 / scaling_factor * z`。有关更多详细信息，请参阅[具有潜在扩散模型的高分辨率图像合成](https://arxiv.org/abs/2112.10752)论文的4.3.2和D.1节。

+   `norm_type`（`str`，*可选*，默认为`"group"`）— 要使用的规范化层类型。可以是`"group"`或`"spatial"`之一。

用于解码潜在表示的VQ-VAE模型。

这个模型继承自[ModelMixin](/docs/diffusers/v0.26.3/en/api/models/overview#diffusers.ModelMixin)。检查超类文档以了解为所有模型实现的通用方法（如下载或保存）。

#### `forward`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L158)

```py
( sample: FloatTensor return_dict: bool = True ) → export const metadata = 'undefined';VQEncoderOutput or tuple
```

参数

+   `sample`（`torch.FloatTensor`）— 输入样本。

+   `return_dict`（`bool`，*可选*，默认为`True`）— 是否返回一个[models.vq_model.VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)而不是一个普通的元组。

返回

[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput) 或 `tuple`

如果return_dict为True，则返回一个[VQEncoderOutput](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.models.vq_model.VQEncoderOutput)，否则返回一个普通的`tuple`。

[VQModel](/docs/diffusers/v0.26.3/en/api/models/vq#diffusers.VQModel)的前向方法。

## VQEncoderOutput

### `class diffusers.models.vq_model.VQEncoderOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/vq_model.py#L27)

```py
( latents: FloatTensor )
```

参数

+   `latents`（形状为`(batch_size, num_channels, height, width)`的`torch.FloatTensor`）— 模型最后一层的编码输出样本。

VQModel编码方法的输出。
