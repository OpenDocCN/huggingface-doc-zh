# UNetMotionModel

> 原始文本：[`huggingface.co/docs/diffusers/api/models/unet-motion`](https://huggingface.co/docs/diffusers/api/models/unet-motion)

[UNet](https://huggingface.co/papers/1505.04597)模型最初由 Ronneberger 等人引入，用于生物医学图像分割，但它也常用于🤗 Diffusers，因为它输出与输入相同大小的图像。它是扩散系统中最重要的组件之一，因为它促进了实际的扩散过程。🤗 Diffusers 中有几种 UNet 模型的变体，取决于它的维度数量以及它是否是条件模型。这是一个 2D UNet 模型。

论文摘要如下：

*大多数人一致认为成功训练深度网络需要成千上万个注释训练样本。在本文中，我们提出了一种依赖于数据增强的网络和训练策略，以更有效地利用可用的注释样本。该架构由一个收缩路径和一个对称扩展路径组成，用于捕获上下文和实现精确定位。我们展示了这样一个网络可以从很少的图像端到端训练，并且在 ISBI 挑战中表现优于先前最佳方法（滑动窗口卷积网络）用于电子显微镜堆栈中神经结构的分割。使用相同的网络在透射光学显微镜图像（相差和 DIC）上训练，我们在这些类别上以很大的优势赢得了 2015 年 ISBI 细胞跟踪挑战。此外，该网络速度很快。在最新的 GPU 上，对 512x512 图像的分割不到一秒。完整的实现（基于 Caffe）和训练的网络可在[`lmb.informatik.uni-freiburg.de/people/ronneber/u-net`](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net)上获得。*

## UNetMotionModel

### `class diffusers.UNetMotionModel`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L176)

```py
( sample_size: Optional = None in_channels: int = 4 out_channels: int = 4 down_block_types: Tuple = ('CrossAttnDownBlockMotion', 'CrossAttnDownBlockMotion', 'CrossAttnDownBlockMotion', 'DownBlockMotion') up_block_types: Tuple = ('UpBlockMotion', 'CrossAttnUpBlockMotion', 'CrossAttnUpBlockMotion', 'CrossAttnUpBlockMotion') block_out_channels: Tuple = (320, 640, 1280, 1280) layers_per_block: int = 2 downsample_padding: int = 1 mid_block_scale_factor: float = 1 act_fn: str = 'silu' norm_num_groups: int = 32 norm_eps: float = 1e-05 cross_attention_dim: int = 1280 use_linear_projection: bool = False num_attention_heads: Union = 8 motion_max_seq_length: int = 32 motion_num_attention_heads: int = 8 use_motion_mid_block: int = True encoder_hid_dim: Optional = None encoder_hid_dim_type: Optional = None )
```

一个修改后的条件 2D UNet 模型，接受嘈杂样本、条件状态和时间步，并返回一个形状输出的样本。

这个模型继承自 ModelMixin。检查超类文档以了解为所有模型实现的通用方法（如下载或保存）。

#### `disable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L695)

```py
( )
```

禁用 FreeU 机制。

#### `enable_forward_chunking`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L608)

```py
( chunk_size: Optional = None dim: int = 0 )
```

参数

+   `chunk_size`（`int`，*可选*）— 前馈层的分块大小。如果未指定，将在 dim=`dim`的每个张量上单独运行前馈层。

+   `dim`（`int`，*可选*，默认为`0`）— 前馈计算应该在哪个维度上分块。选择 dim=0（批处理）或 dim=1（序列长度）之间的值。

设置注意力处理器使用[前馈分块](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers)。

#### `enable_freeu`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L670)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1`（`float`）— 阶段 1 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2`（`float`）— 阶段 2 的缩放因子，用于减弱跳跃特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1`（`float`）— 阶段 1 的缩放因子，用于放大骨干特征的贡献。

+   `b2`（`float`）— 阶段 2 的缩放因子，用于放大骨干特征的贡献。

启用来自 [`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497) 的 FreeU 机制。

缩放因子后的后缀表示它们被应用的阶段块。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)以获取已知适用于不同流水线（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

#### `forward`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L703)

```py
( sample: FloatTensor timestep: Union encoder_hidden_states: Tensor timestep_cond: Optional = None attention_mask: Optional = None cross_attention_kwargs: Optional = None added_cond_kwargs: Optional = None down_block_additional_residuals: Optional = None mid_block_additional_residual: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';~models.unet_3d_condition.UNet3DConditionOutput or tuple
```

参数

+   `sample` (`torch.FloatTensor`) — 具有以下形状 `(batch, num_frames, channel, height, width)` 的嘈杂输入张量。

+   `timestep` (`torch.FloatTensor` 或 `float` 或 `int`) — 对输入进行去噪的时间步数。

+   `encoder_hidden_states` (`torch.FloatTensor`) — 具有形状 `(batch, sequence_length, feature_dim)` 的编码器隐藏状态。timestep_cond — (`torch.Tensor`, *可选*, 默认为 `None`): 时间步的条件嵌入。如果提供了，这些嵌入将与通过 `self.time_embedding` 层传递的样本相加，以获得时间步嵌入。

+   `attention_mask` (`torch.Tensor`, *可选*, 默认为 `None`) — 一个形状为 `(batch, key_tokens)` 的注意力掩码被应用于 `encoder_hidden_states`。如果为 `1`，则保留掩码，否则如果为 `0`，则丢弃。掩码将被转换为偏置，这会在“丢弃”标记对应的注意力分数上添加大的负值。

+   `cross_attention_kwargs` (`dict`, *可选*) — 如果指定了，将传递给 `AttentionProcessor` 的 kwargs 字典，如 [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) 中的 `self.processor` 中定义的那样。down_block_additional_residuals — (`torch.Tensor` 的 `tuple`, *可选*): 如果指定了，将添加到 down unet 块的残差中。mid_block_additional_residual — (`torch.Tensor`, *可选*): 如果指定了，将添加到中间 unet 块的残差中。

+   `return_dict` (`bool`, *可选*, 默认为 `True`) — 是否返回一个 `~models.unet_3d_condition.UNet3DConditionOutput` 而不是一个普通元组。

返回

`~models.unet_3d_condition.UNet3DConditionOutput` 或 `tuple`

如果 `return_dict` 为 True，则返回一个 `~models.unet_3d_condition.UNet3DConditionOutput`，否则返回一个元组，其中第一个元素是样本张量。

UNetMotionModel 的前向方法。

#### `freeze_unet2d_params`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L478)

```py
( )
```

冻结仅 UNet2DConditionModel 的权重，并保持运动模块未冻结以进行微调。

#### `set_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L573)

```py
( processor: Union )
```

参数

+   `processor` (`AttentionProcessor` 的 `dict` 或仅 `AttentionProcessor`) — 实例化的处理器类或处理器类的字典，将被设置为**所有** `Attention` 层的处理器。

    如果 `processor` 是一个字典，则键需要定义相应的交叉注意力处理器的路径。在设置可训练的注意力处理器时，强烈建议这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_motion_model.py#L650)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

## UNet3DConditionOutput

### `class diffusers.models.unets.unet_3d_condition.UNet3DConditionOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/unets/unet_3d_condition.py#L51)

```py
( sample: FloatTensor )
```

参数

+   `sample`（形状为`(batch_size, num_frames, num_channels, height, width)`的`torch.FloatTensor`）- 在`encoder_hidden_states`输入条件下的隐藏状态输出。模型的最后一层的输出。

UNet3DConditionModel 的输出。
