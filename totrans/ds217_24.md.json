["```py\n>>> pip install s3fs\n```", "```py\n>>> storage_options = {\"anon\": True}  # for anonymous connection\n# or use your credentials\n>>> storage_options = {\"key\": aws_access_key_id, \"secret\": aws_secret_access_key}  # for private buckets\n# or use a botocore session\n>>> import aiobotocore.session\n>>> s3_session = aiobotocore.session.AioSession(profile=\"my_profile_name\")\n>>> storage_options = {\"session\": s3_session}\n```", "```py\n>>> import s3fs\n>>> fs = s3fs.S3FileSystem(**storage_options)\n```", "```py\n>>> conda install -c conda-forge gcsfs\n# or install with pip\n>>> pip install gcsfs\n```", "```py\n>>> storage_options={\"token\": \"anon\"}  # for anonymous connection\n# or use your credentials of your default gcloud credentials or from the google metadata service\n>>> storage_options={\"project\": \"my-google-project\"}\n# or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/\n>>> storage_options={\"project\": \"my-google-project\", \"token\": TOKEN}\n```", "```py\n>>> import gcsfs\n>>> fs = gcsfs.GCSFileSystem(**storage_options)\n```", "```py\n>>> conda install -c conda-forge adlfs\n# or install with pip\n>>> pip install adlfs\n```", "```py\n>>> storage_options = {\"anon\": True}  # for anonymous connection\n# or use your credentials\n>>> storage_options = {\"account_name\": ACCOUNT_NAME, \"account_key\": ACCOUNT_KEY}  # gen 2 filesystem\n# or use your credentials with the gen 1 filesystem\n>>> storage_options={\"tenant_id\": TENANT_ID, \"client_id\": CLIENT_ID, \"client_secret\": CLIENT_SECRET}\n```", "```py\n>>> import adlfs\n>>> fs = adlfs.AzureBlobFileSystem(**storage_options)\n```", "```py\n>>> pip install ocifs\n```", "```py\n>>> storage_options = {\"config\": \"~/.oci/config\", \"region\": \"us-ashburn-1\"} \n```", "```py\n>>> import ocifs\n>>> fs = ocifs.OCIFileSystem(**storage_options)\n```", "```py\n>>> output_dir = \"s3://my-bucket/imdb\"\n>>> builder = load_dataset_builder(\"imdb\")\n>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\n```", "```py\n>>> output_dir = \"s3://my-bucket/imdb\"\n>>> builder = load_dataset_builder(\"path/to/local/loading_script/loading_script.py\")\n>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\n```", "```py\n>>> data_files = {\"train\": [\"path/to/train.csv\"]}\n>>> output_dir = \"s3://my-bucket/imdb\"\n>>> builder = load_dataset_builder(\"csv\", data_files=data_files)\n>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\n```", "```py\n>>> builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\", max_shard_size=\"1GB\")\n```", "```py\nimport dask.dataframe as dd\n\ndf = dd.read_parquet(output_dir, storage_options=storage_options)\n\n# or if your dataset is split into train/valid/test\ndf_train = dd.read_parquet(output_dir + f\"/{builder.name}-train-*.parquet\", storage_options=storage_options)\ndf_valid = dd.read_parquet(output_dir + f\"/{builder.name}-validation-*.parquet\", storage_options=storage_options)\ndf_test = dd.read_parquet(output_dir + f\"/{builder.name}-test-*.parquet\", storage_options=storage_options)\n```", "```py\n# saves encoded_dataset to amazon s3\n>>> encoded_dataset.save_to_disk(\"s3://my-private-datasets/imdb/train\", storage_options=storage_options)\n# saves encoded_dataset to google cloud storage\n>>> encoded_dataset.save_to_disk(\"gcs://my-private-datasets/imdb/train\", storage_options=storage_options)\n# saves encoded_dataset to microsoft azure blob/datalake\n>>> encoded_dataset.save_to_disk(\"adl://my-private-datasets/imdb/train\", storage_options=storage_options)\n```", "```py\n>>> fs.ls(\"my-private-datasets/imdb/train\", detail=False)\n[\"dataset_info.json.json\",\"dataset.arrow\",\"state.json\"]\n```", "```py\n>>> from datasets import load_from_disk\n# load encoded_dataset from cloud storage\n>>> dataset = load_from_disk(\"s3://a-public-datasets/imdb/train\", storage_options=storage_options)  \n>>> print(len(dataset))\n25000\n```"]