- en: BridgeTower
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bridgetower](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/bridgetower)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/81.5f8e53f6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The BridgeTower model was proposed in [BridgeTower: Building Bridges Between
    Encoders in Vision-Language Representative Learning](https://arxiv.org/abs/2206.08657)
    by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.
    The goal of this model is to build a bridge between each uni-modal encoder and
    the cross-modal encoder to enable comprehensive and detailed interaction at each
    layer of the cross-modal encoder thus achieving remarkable performance on various
    downstream tasks with almost negligible additional performance and computational
    costs.'
  prefs: []
  type: TYPE_NORMAL
- en: This paper has been accepted to the [AAAI’23](https://aaai.org/Conferences/AAAI-23/)
    conference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vision-Language (VL) models with the TWO-TOWER architecture have dominated
    visual-language representation learning in recent years. Current VL models either
    use lightweight uni-modal encoders and learn to extract, align and fuse both modalities
    simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal
    representations from the deep pre-trained uni-modal encoders into the top cross-modal
    encoder. Both approaches potentially restrict vision-language representation learning
    and limit model performance. In this paper, we propose BRIDGETOWER, which introduces
    multiple bridge layers that build a connection between the top layers of uni-modal
    encoders and each layer of the crossmodal encoder. This enables effective bottom-up
    cross-modal alignment and fusion between visual and textual representations of
    different semantic levels of pre-trained uni-modal encoders in the cross-modal
    encoder. Pre-trained with only 4M images, BRIDGETOWER achieves state-of-the-art
    performance on various downstream vision-language tasks. In particular, on the
    VQAv2 test-std set, BRIDGETOWER achieves an accuracy of 78.73%, outperforming
    the previous state-of-the-art model METER by 1.09% with the same pre-training
    data and almost negligible additional parameters and computational costs. Notably,
    when further scaling the model, BRIDGETOWER achieves an accuracy of 81.15%, surpassing
    models that are pre-trained on orders-of-magnitude larger datasets.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/2cf6cac1e694d76fda85d82f66c9a1aa.png) BridgeTower architecture.
    Taken from the [original paper.](https://arxiv.org/abs/2206.08657)'
  prefs: []
  type: TYPE_IMG
- en: This model was contributed by [Anahita Bhiwandiwalla](https://huggingface.co/anahita-b),
    [Tiep Le](https://huggingface.co/Tile) and [Shaoyen Tseng](https://huggingface.co/shaoyent).
    The original code can be found [here](https://github.com/microsoft/BridgeTower).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips and examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BridgeTower consists of a visual encoder, a textual encoder and cross-modal
    encoder with multiple lightweight bridge layers. The goal of this approach was
    to build a bridge between each uni-modal encoder and the cross-modal encoder to
    enable comprehensive and detailed interaction at each layer of the cross-modal
    encoder. In principle, one can apply any visual, textual or cross-modal encoder
    in the proposed architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The [BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)
    wraps [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    and [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)
    into a single instance to both encode the text and prepare the images respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows how to run contrastive learning using [BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)
    and [BridgeTowerForContrastiveLearning](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The following example shows how to run image-text retrieval using [BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)
    and [BridgeTowerForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The following example shows how to run masked language modeling using [BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)
    and [BridgeTowerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Tips:'
  prefs: []
  type: TYPE_NORMAL
- en: This implementation of BridgeTower uses [RobertaTokenizer](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)
    to generate text embeddings and OpenAI’s CLIP/ViT model to compute visual embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpoints for pre-trained [bridgeTower-base](https://huggingface.co/BridgeTower/bridgetower-base)
    and [bridgetower masked language modeling and image text matching](https://huggingface.co/BridgeTower/bridgetower-base-itm-mlm)
    are released.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please refer to [Table 5](https://arxiv.org/pdf/2206.08657.pdf) for BridgeTower’s
    performance on Image Retrieval and other down stream tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch version of this model is only available in torch 1.10 and higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BridgeTowerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L243)'
  prefs: []
  type: TYPE_NORMAL
- en: ( share_cross_modal_transformer_layers = True hidden_act = 'gelu' hidden_size
    = 768 initializer_factor = 1 layer_norm_eps = 1e-05 share_link_tower_layers =
    False link_tower_type = 'add' num_attention_heads = 12 num_hidden_layers = 6 tie_word_embeddings
    = False init_layernorm_from_vision_encoder = False text_config = None vision_config
    = None **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**share_cross_modal_transformer_layers** (`bool`, *optional*, defaults to `True`)
    — Whether cross modal transformer layers are shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `function`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_factor** (`float`, *optional*, defaults to 1) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**share_link_tower_layers** (`bool`, *optional*, defaults to `False`) — Whether
    the bride/link tower layers are shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**link_tower_type** (`str`, *optional*, defaults to `"add"`) — Type of the
    bridge/link layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 6) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tie_word_embeddings** (`bool`, *optional*, defaults to `False`) — Whether
    to tie input and output embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**init_layernorm_from_vision_encoder** (`bool`, *optional*, defaults to `False`)
    — Whether to init LayerNorm from the vision encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_config** (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [BridgeTowerTextConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerTextConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vision_config** (`dict`, *optional*) — Dictionary of configuration options
    used to initialize [BridgeTowerVisionConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerVisionConfig).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel).
    It is used to instantiate a BridgeTower model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the bridgetower-base [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#### from_text_vision_configs'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L344)'
  prefs: []
  type: TYPE_NORMAL
- en: '( text_config: BridgeTowerTextConfig vision_config: BridgeTowerVisionConfig
    **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a [BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig)
    (or a derived class) from BridgeTower text model configuration. Returns: [BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig):
    An instance of a configuration object'
  prefs: []
  type: TYPE_NORMAL
- en: BridgeTowerTextConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerTextConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L121)'
  prefs: []
  type: TYPE_NORMAL
- en: ( vocab_size = 50265 hidden_size = 768 num_hidden_layers = 12 num_attention_heads
    = 12 initializer_factor = 1 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout_prob
    = 0.1 attention_probs_dropout_prob = 0.1 max_position_embeddings = 514 type_vocab_size
    = 1 layer_norm_eps = 1e-05 pad_token_id = 1 bos_token_id = 0 eos_token_id = 2
    position_embedding_type = 'absolute' use_cache = True **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**vocab_size** (`int`, *optional*, defaults to 50265) — Vocabulary size of
    the text part of the model. Defines the number of different tokens that can be
    represented by the `inputs_ids` passed when calling [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_attention_heads** (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**intermediate_size** (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_act** (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_dropout_prob** (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_probs_dropout_prob** (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_position_embeddings** (`int`, *optional*, defaults to 514) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**type_vocab_size** (`int`, *optional*, defaults to 2) — The vocabulary size
    of the `token_type_ids`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_factor** (`float`, *optional*, defaults to 1) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**position_embedding_type** (`str`, *optional*, defaults to `"absolute"`) —
    Type of position embedding. Choose one of `"absolute"`, `"relative_key"`, `"relative_key_query"`.
    For positional embeddings use `"absolute"`. For more information on `"relative_key"`,
    please refer to [Self-Attention with Relative Position Representations (Shaw et
    al.)](https://arxiv.org/abs/1803.02155). For more information on `"relative_key_query"`,
    please refer to *Method 4* in [Improve Transformer Models with Better Relative
    Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**is_decoder** (`bool`, *optional*, defaults to `False`) — Whether the model
    is used as a decoder or not. If `False`, the model is used as an encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**use_cache** (`bool`, *optional*, defaults to `True`) — Whether or not the
    model should return the last key/values attentions (not used by all models). Only
    relevant if `config.is_decoder=True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the text configuration of a [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel).
    The default values here are copied from RoBERTa. Instantiating a configuration
    with the defaults will yield a similar configuration to that of the bridgetower-base
    [BridegTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: BridgeTowerVisionConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerVisionConfig'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/configuration_bridgetower.py#L34)'
  prefs: []
  type: TYPE_NORMAL
- en: ( hidden_size = 768 num_hidden_layers = 12 num_channels = 3 patch_size = 16
    image_size = 288 initializer_factor = 1 layer_norm_eps = 1e-05 stop_gradient =
    False share_layernorm = True remove_last_layer = False **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**hidden_size** (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_hidden_layers** (`int`, *optional*, defaults to 12) — Number of hidden
    layers in visual encoder model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**patch_size** (`int`, *optional*, defaults to 16) — The size (resolution)
    of each patch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_size** (`int`, *optional*, defaults to 288) — The size (resolution)
    of each image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initializer_factor** (`float`, *optional*, defaults to 1) — A factor for
    initializing all weight matrices (should be kept to 1, used internally for initialization
    testing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**layer_norm_eps** (`float`, *optional*, defaults to 1e-05) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**stop_gradient** (`bool`, *optional*, defaults to `False`) — Whether to stop
    gradient for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**share_layernorm** (`bool`, *optional*, defaults to `True`) — Whether LayerNorm
    layers are shared.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**remove_last_layer** (`bool`, *optional*, defaults to `False`) — Whether to
    remove the last layer from the vision encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the vision configuration of a [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel).
    Instantiating a configuration with the defaults will yield a similar configuration
    to that of the bridgetower-base [BridgeTower/bridgetower-base](https://huggingface.co/BridgeTower/bridgetower-base/)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: BridgeTowerImageProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerImageProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/image_processing_bridgetower.py#L123)'
  prefs: []
  type: TYPE_NORMAL
- en: '( do_resize: bool = True size: Dict = 288 size_divisor: int = 32 resample:
    Resampling = <Resampling.BICUBIC: 3> do_rescale: bool = True rescale_factor: Union
    = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std:
    Union = None do_center_crop: bool = True do_pad: bool = True **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**do_resize** (`bool`, *optional*, defaults to `True`) — Whether to resize
    the image’s (height, width) dimensions to the specified `size`. Can be overridden
    by the `do_resize` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size** (`Dict[str, int]` *optional*, defaults to 288) — Resize the shorter
    side of the input to `size["shortest_edge"]`. The longer side will be limited
    to under `int((1333 / 800) * size["shortest_edge"])` while preserving the aspect
    ratio. Only has an effect if `do_resize` is set to `True`. Can be overridden by
    the `size` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size_divisor** (`int`, *optional*, defaults to 32) — The size by which to
    make sure both the height and width can be divided. Only has an effect if `do_resize`
    is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resample** (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`)
    — Resampling filter to use if resizing the image. Only has an effect if `do_resize`
    is set to `True`. Can be overridden by the `resample` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_rescale** (`bool`, *optional*, defaults to `True`) — Whether to rescale
    the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`
    parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale_factor** (`int` or `float`, *optional*, defaults to `1/255`) — Scale
    factor to use if rescaling the image. Only has an effect if `do_rescale` is set
    to `True`. Can be overridden by the `rescale_factor` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_normalize** (`bool`, *optional*, defaults to `True`) — Whether to normalize
    the image. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method. Can be overridden by the `do_normalize` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_mean** (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`)
    — Mean to use if normalizing the image. This is a float or list of floats the
    length of the number of channels in the image. Can be overridden by the `image_mean`
    parameter in the `preprocess` method. Can be overridden by the `image_mean` parameter
    in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_std** (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`)
    — Standard deviation to use if normalizing the image. This is a float or list
    of floats the length of the number of channels in the image. Can be overridden
    by the `image_std` parameter in the `preprocess` method. Can be overridden by
    the `image_std` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_center_crop** (`bool`, *optional*, defaults to `True`) — Whether to center
    crop the image. Can be overridden by the `do_center_crop` parameter in the `preprocess`
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_pad** (`bool`, *optional*, defaults to `True`) — Whether to pad the image
    to the `(max_height, max_width)` of the images in the batch. Can be overridden
    by the `do_pad` parameter in the `preprocess` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a BridgeTower image processor.
  prefs: []
  type: TYPE_NORMAL
- en: '#### preprocess'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/image_processing_bridgetower.py#L367)'
  prefs: []
  type: TYPE_NORMAL
- en: '( images: Union do_resize: Optional = None size: Optional = None size_divisor:
    Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor:
    Optional = None do_normalize: Optional = None image_mean: Union = None image_std:
    Union = None do_pad: Optional = None do_center_crop: Optional = None return_tensors:
    Union = None data_format: ChannelDimension = <ChannelDimension.FIRST: ''channels_first''>
    input_data_format: Union = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**images** (`ImageInput`) — Image to preprocess. Expects a single or batch
    of images with pixel values ranging from 0 to 255\. If passing in images with
    pixel values between 0 and 1, set `do_rescale=False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_resize** (`bool`, *optional*, defaults to `self.do_resize`) — Whether
    to resize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size** (`Dict[str, int]`, *optional*, defaults to `self.size`) — Controls
    the size of the image after `resize`. The shortest edge of the image is resized
    to `size["shortest_edge"]` whilst preserving the aspect ratio. If the longest
    edge of this resized image is > `int(size["shortest_edge"] * (1333 / 800))`, then
    the image is resized again to make the longest edge equal to `int(size["shortest_edge"]
    * (1333 / 800))`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size_divisor** (`int`, *optional*, defaults to `self.size_divisor`) — The
    image is resized to a size that is a multiple of this value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resample** (`PILImageResampling`, *optional*, defaults to `self.resample`)
    — Resampling filter to use if resizing the image. Only has an effect if `do_resize`
    is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_rescale** (`bool`, *optional*, defaults to `self.do_rescale`) — Whether
    to rescale the image values between [0 - 1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale_factor** (`float`, *optional*, defaults to `self.rescale_factor`)
    — Rescale factor to rescale the image by if `do_rescale` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_normalize** (`bool`, *optional*, defaults to `self.do_normalize`) — Whether
    to normalize the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_mean** (`float` or `List[float]`, *optional*, defaults to `self.image_mean`)
    — Image mean to normalize the image by if `do_normalize` is set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_std** (`float` or `List[float]`, *optional*, defaults to `self.image_std`)
    — Image standard deviation to normalize the image by if `do_normalize` is set
    to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_pad** (`bool`, *optional*, defaults to `self.do_pad`) — Whether to pad
    the image to the (max_height, max_width) in the batch. If `True`, a pixel mask
    is also created and returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**do_center_crop** (`bool`, *optional*, defaults to `self.do_center_crop`)
    — Whether to center crop the image. If the input size is smaller than `crop_size`
    along any edge, the image is padded with 0’s and then center cropped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or `TensorType`, *optional*) — The type of tensors
    to return. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Return a list of `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.TENSORFLOW` or `''tf''`: Return a batch of type `tf.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.PYTORCH` or `''pt''`: Return a batch of type `torch.Tensor`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.NUMPY` or `''np''`: Return a batch of type `np.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TensorType.JAX` or `''jax''`: Return a batch of type `jax.numpy.ndarray`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data_format** (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`)
    — The channel dimension format for the output image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unset: Use the channel dimension format of the input image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**input_data_format** (`ChannelDimension` or `str`, *optional*) — The channel
    dimension format for the input image. If unset, the channel dimension format is
    inferred from the input image. Can be one of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_first"` or `ChannelDimension.FIRST`: image in (num_channels, height,
    width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"channels_last"` or `ChannelDimension.LAST`: image in (height, width, num_channels)
    format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"none"` or `ChannelDimension.NONE`: image in (height, width) format.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess an image or batch of images.
  prefs: []
  type: TYPE_NORMAL
- en: BridgeTowerProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerProcessor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/processing_bridgetower.py#L26)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image_processor tokenizer )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image_processor** (`BridgeTowerImageProcessor`) — An instance of [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor).
    The image processor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tokenizer** (`RobertaTokenizerFast`) — An instance of [‘RobertaTokenizerFast`].
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a BridgeTower processor which wraps a Roberta tokenizer and BridgeTower
    image processor into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[BridgeTowerProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor)
    offers all the functionalities of [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor)
    and [RobertaTokenizerFast](/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizerFast).
    See the docstring of [**call**()](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerProcessor.__call__)
    and `decode()` for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### __call__'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/processing_bridgetower.py#L49)'
  prefs: []
  type: TYPE_NORMAL
- en: '( images text: Union = None add_special_tokens: bool = True padding: Union
    = False truncation: Union = None max_length: Optional = None stride: int = 0 pad_to_multiple_of:
    Optional = None return_token_type_ids: Optional = None return_attention_mask:
    Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask:
    bool = False return_offsets_mapping: bool = False return_length: bool = False
    verbose: bool = True return_tensors: Union = None **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: This method uses [BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    method to prepare image(s) for the model, and [RobertaTokenizerFast.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    to prepare text for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the docstring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: BridgeTowerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerModel'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1197)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare BridgeTower Model transformer outputting BridgeTowerModelOutput object
    without any specific head on top. This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1265)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None token_type_ids:
    Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask:
    Optional = None inputs_embeds: Optional = None image_embeds: Optional = None image_token_type_idx:
    Optional = None output_attentions: Optional = None output_hidden_states: Optional
    = None return_dict: Optional = None labels: Optional = None ) → `transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor).
    See [BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_mask** (`torch.LongTensor` of shape `(batch_size, height, width)`,
    *optional*) — Mask to avoid performing attention on padding pixel values. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. **not masked**),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. **masked**). `What are attention masks?
    <../glossary.html#attention-mask>`__
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_embeds** (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_token_type_idx** (`int`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token type ids for images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — If set to `True`, hidden states
    are returned as a list containing the hidden states of text, image, and cross-modal
    components respectively. i.e. `(hidden_states_text, hidden_states_image, hidden_states_cross_modal)`
    where each element is a list of the hidden states of the corresponding modality.
    `hidden_states_txt/img` are a list of tensors corresponding to unimodal hidden
    states and `hidden_states_cross_modal` is a list of tuples containing `cross_modal_text_hidden_states`
    and `cross_modal_image_hidden_states` of each brdige layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    are currently not supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**text_features** (`torch.FloatTensor` of shape `(batch_size, text_sequence_length,
    hidden_size)`) — Sequence of hidden-states at the text output of the last layer
    of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_features** (`torch.FloatTensor` of shape `(batch_size, image_sequence_length,
    hidden_size)`) — Sequence of hidden-states at the image output of the last layer
    of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size x
    2)`) — Concatenation of last layer hidden-state of the first token of the text
    and image sequence (classification token), respectively, after further processing
    through layers used for auxiliary pretraining tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BridgeTowerModel](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: BridgeTowerForContrastiveLearning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerForContrastiveLearning'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1761)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BridgeTower Model with a image-text contrastive head on top computing image-text
    contrastive loss.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1781)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None token_type_ids:
    Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask:
    Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = True return_dict: Optional =
    None return_loss: Optional = None ) → `transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor).
    See [BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_mask** (`torch.LongTensor` of shape `(batch_size, height, width)`,
    *optional*) — Mask to avoid performing attention on padding pixel values. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. **not masked**),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. **masked**). `What are attention masks?
    <../glossary.html#attention-mask>`__
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_embeds** (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_token_type_idx** (`int`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token type ids for images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_loss** (`bool`, *optional*) — Whether or not to return the contrastive
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss`
    is `True` — Image-text contrastive loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_embeds** (`torch.FloatTensor)`, *optional*, returned when model is initialized
    with `with_projection=True`) — The text embeddings obtained by applying the projection
    layer to the pooler_output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_embeds** (`torch.FloatTensor)`, *optional*, returned when model is
    initialized with `with_projection=True`) — The image embeddings obtained by applying
    the projection layer to the pooler_output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cross_embeds** (`torch.FloatTensor)`, *optional*, returned when model is
    initialized with `with_projection=True`) — The text-image cross-modal embeddings
    obtained by applying the projection layer to the pooler_output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.
    Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [BridgeTowerForContrastiveLearning](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: BridgeTowerForMaskedLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerForMaskedLM'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1541)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BridgeTower Model with a language modeling head on top as done during pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1565)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None token_type_ids:
    Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask:
    Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None labels: Optional = None ) → [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`)
    — Indices of input sequence tokens in the vocabulary. Indices can be obtained
    using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Segment token indices to indicate first and second portions of the
    inputs. Indices are selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor).
    See [BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_mask** (`torch.LongTensor` of shape `(batch_size, height, width)`,
    *optional*) — Mask to avoid performing attention on padding pixel values. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. **not masked**),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. **masked**). `What are attention masks?
    <../glossary.html#attention-mask>`__
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_embeds** (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_token_type_idx** (`int`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token type ids for images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BridgeTowerForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: BridgeTowerForImageAndTextRetrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BridgeTowerForImageAndTextRetrieval'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1649)'
  prefs: []
  type: TYPE_NORMAL
- en: ( config )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**config** ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BridgeTower Model transformer with a classifier head on top (a linear layer
    on top of the final hidden state of the [CLS] token) for image-to-text matching.
  prefs: []
  type: TYPE_NORMAL
- en: This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### forward'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/bridgetower/modeling_bridgetower.py#L1667)'
  prefs: []
  type: TYPE_NORMAL
- en: '( input_ids: Optional = None attention_mask: Optional = None token_type_ids:
    Optional = None pixel_values: Optional = None pixel_mask: Optional = None head_mask:
    Optional = None inputs_embeds: Optional = None image_embeds: Optional = None output_attentions:
    Optional = None output_hidden_states: Optional = None return_dict: Optional =
    None labels: Optional = None ) → [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**input_ids** (`torch.LongTensor` of shape `({0})`) — Indices of input sequence
    tokens in the vocabulary. Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details. [What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**attention_mask** (`torch.FloatTensor` of shape `({0})`, *optional*) — Mask
    to avoid performing attention on padding token indices. Mask values selected in
    `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are **masked**. [What are attention masks?](../glossary#attention-mask)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token_type_ids** (`torch.LongTensor` of shape `({0})`, *optional*) — Segment
    token indices to indicate first and second portions of the inputs. Indices are
    selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 corresponds to a *sentence A* token,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 corresponds to a *sentence B* token. [What are token type IDs?](../glossary#token-type-ids)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_values** (`torch.FloatTensor` of shape `(batch_size, num_channels,
    height, width)`) — Pixel values. Pixel values can be obtained using [BridgeTowerImageProcessor](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor).
    See [BridgeTowerImageProcessor.**call**()](/docs/transformers/v4.37.2/en/model_doc/glpn#transformers.GLPNFeatureExtractor.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pixel_mask** (`torch.LongTensor` of shape `(batch_size, height, width)`,
    *optional*) — Mask to avoid performing attention on padding pixel values. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for pixels that are real (i.e. **not masked**),
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for pixels that are padding (i.e. **masked**). `What are attention masks?
    <../glossary.html#attention-mask>`__
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**head_mask** (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is **not masked**,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is **masked**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**inputs_embeds** (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*)
    — Optionally, instead of passing `input_ids` you can choose to directly pass an
    embedded representation. This is useful if you want more control over how to convert
    `input_ids` indices into associated vectors than the model’s internal embedding
    lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_embeds** (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`,
    *optional*) — Optionally, instead of passing `pixel_values`, you can choose to
    directly pass an embedded representation. This is useful if you want more control
    over how to convert `pixel_values` into patch embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**image_token_type_idx** (`int`, *optional*) —'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The token type ids for images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_attentions** (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_hidden_states** (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**return_dict** (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**labels** (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*) — Labels
    for computing the image-text matching loss. 0 means the pairs don’t match and
    1 means they match. The pairs with 0 will be skipped for calculation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([BridgeTowerConfig](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '**loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`)
    — Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [BridgeTowerForImageAndTextRetrieval](/docs/transformers/v4.37.2/en/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
