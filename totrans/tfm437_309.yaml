- en: MMS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MMS
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/mms)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516)
    by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani
    Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski,
    Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MMS模型是由Vineel Pratap、Andros Tjandra、Bowen Shi、Paden Tomasello、Arun Babu、Sayani
    Kundu、Ali Elkahky、Zhaoheng Ni、Apoorv Vyas、Maryam Fazel-Zarandi、Alexei Baevski、Yossi
    Adi、Xiaohui Zhang、Wei-Ning Hsu、Alexis Conneau、Michael Auli在[将语音技术扩展到1000多种语言](https://arxiv.org/abs/2305.13516)中提出的。
- en: 'The abstract from the paper is the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的摘要如下：
- en: '*Expanding the language coverage of speech technology has the potential to
    improve access to information for many more people. However, current speech technology
    is restricted to about one hundred languages which is a small fraction of the
    over 7,000 languages spoken around the world. The Massively Multilingual Speech
    (MMS) project increases the number of supported languages by 10-40x, depending
    on the task. The main ingredients are a new dataset based on readings of publicly
    available religious texts and effectively leveraging self-supervised learning.
    We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual
    automatic speech recognition model for 1,107 languages, speech synthesis models
    for the same number of languages, as well as a language identification model for
    4,017 languages. Experiments show that our multilingual speech recognition model
    more than halves the word error rate of Whisper on 54 languages of the FLEURS
    benchmark while being trained on a small fraction of the labeled data.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*扩大语音技术的语言覆盖范围有可能提高更多人获取信息的机会。然而，当前的语音技术仅限于大约100种语言，这只是全球约7000种语言中的一小部分。大规模多语言语音（MMS）项目通过10-40倍增加了支持的语言数量，具体取决于任务。主要成分是基于公开可用宗教文本的新数据集，并有效地利用自监督学习。我们构建了覆盖1406种语言的预训练wav2vec
    2.0模型，一种单一的支持1107种语言的多语言自动语音识别模型，以及相同数量语言的语音合成模型，以及支持4017种语言的语言识别模型。实验表明，我们的多语言语音识别模型在FLEURS基准测试的54种语言上将Whisper的词错误率减少了一半以上，同时在训练时仅使用了少量标记数据。*'
- en: Here are the different models open sourced in the MMS project. The models and
    code are originally released [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).
    We have add them to the `transformers` framework, making them easier to use.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是MMS项目中开源的不同模型。这些模型和代码最初发布在[这里](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)。我们已将它们添加到`transformers`框架中，使其更易于使用。
- en: Automatic Speech Recognition (ASR)
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）
- en: 'The ASR model checkpoints can be found here : [mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102),
    [mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107), [mms-1b-all](https://huggingface.co/facebook/mms-1b-all).
    For best accuracy, use the `mms-1b-all` model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ASR模型的检查点可以在这里找到：[mms-1b-fl102](https://huggingface.co/facebook/mms-1b-fl102)，[mms-1b-l1107](https://huggingface.co/facebook/mms-1b-l1107)，[mms-1b-all](https://huggingface.co/facebook/mms-1b-all)。为了获得最佳准确性，请使用`mms-1b-all`模型。
- en: 'Tips:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：
- en: All ASR models accept a float array corresponding to the raw waveform of the
    speech signal. The raw waveform should be pre-processed with [Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有ASR模型都接受一个与语音信号的原始波形对应的浮点数组。原始波形应该使用[Wav2Vec2FeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor)进行预处理。
- en: The models were trained using connectionist temporal classification (CTC) so
    the model output has to be decoded using [Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型是使用连接主义时间分类（CTC）进行训练的，因此必须使用[Wav2Vec2CTCTokenizer](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer)对模型输出进行解码。
- en: You can load different language adapter weights for different languages via
    [load_adapter()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load_adapter).
    Language adapters only consists of roughly 2 million parameters and can therefore
    be efficiently loaded on the fly when needed.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过[load_adapter()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load_adapter)为不同语言加载不同的语言适配器权重。语言适配器仅包含大约200万个参数，因此在需要时可以高效地动态加载。
- en: Loading
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加载
- en: By default MMS loads adapter weights for English. If you want to load adapter
    weights of another language make sure to specify `target_lang=<your-chosen-target-lang>`
    as well as `"ignore_mismatched_sizes=True`. The `ignore_mismatched_sizes=True`
    keyword has to be passed to allow the language model head to be resized according
    to the vocabulary of the specified language. Similarly, the processor should be
    loaded with the same target language
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，MMS会加载英语的适配器权重。如果您想加载另一种语言的适配器权重，请确保指定`target_lang=<your-chosen-target-lang>`以及`"ignore_mismatched_sizes=True`。必须传递`ignore_mismatched_sizes=True`关键字，以允许语言模型头根据指定语言的词汇重新调整大小。同样，处理器应该加载相同的目标语言。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can safely ignore a warning such as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以安全地忽略警告，例如：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to use the ASR pipeline, you can load your chosen target language
    as such:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用ASR流程，可以按如下方式加载所选的目标语言：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Inference
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理
- en: Next, let’s look at how we can run MMS in inference and change adapter layers
    after having called `~PretrainedModel.from_pretrained` First, we load audio data
    in different languages using the [Datasets](https://github.com/huggingface/datasets).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何在调用`~PretrainedModel.from_pretrained`之后运行MMS的推理并更改适配器层。首先，我们使用[Datasets](https://github.com/huggingface/datasets)加载不同语言的音频数据。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we load the model and processor
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载模型和处理器
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we process the audio data, pass the processed audio data to the model and
    transcribe the model output, just like we usually do for [Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处理音频数据，将处理后的音频数据传递给模型并转录模型输出，就像我们通常为[Wav2Vec2ForCTC](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC)所做的那样。
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now keep the same model in memory and simply switch out the language
    adapters by calling the convenient [load_adapter()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load_adapter)
    function for the model and [set_target_lang()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.set_target_lang)
    for the tokenizer. We pass the target language as an input - `"fra"` for French.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将相同的模型保留在内存中，并通过调用方便的[load_adapter()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC.load_adapter)函数为模型和[set_target_lang()](/docs/transformers/v4.37.2/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer.set_target_lang)为分词器切换语言适配器。我们将目标语言作为输入传递
    - `"fra"`表示法语。
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the same way the language can be switched out for all other supported languages.
    Please have a look at:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，语言可以切换为所有其他支持的语言。请查看：
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: to see all supported languages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 查看所有支持的语言。
- en: To further improve performance from ASR models, language model decoding can
    be used. See the documentation [here](https://huggingface.co/facebook/mms-1b-all)
    for further details.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高ASR模型的性能，可以使用语言模型解码。有关更多详细信息，请参阅此处的文档[here](https://huggingface.co/facebook/mms-1b-all)。
- en: Speech Synthesis (TTS)
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音合成（TTS）
- en: 'MMS-TTS uses the same model architecture as VITS, which was added to 🤗 Transformers
    in v4.33\. MMS trains a separate model checkpoint for each of the 1100+ languages
    in the project. All available checkpoints can be found on the Hugging Face Hub:
    [facebook/mms-tts](https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts),
    and the inference documentation under [VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MMS-TTS使用与VITS相同的模型架构，该架构在v4.33中添加到🤗 Transformers中。MMS为项目中的1100多种语言中的每种语言训练了一个单独的模型检查点。所有可用的检查点都可以在Hugging
    Face Hub上找到：[facebook/mms-tts](https://huggingface.co/models?sort=trending&search=facebook%2Fmms-tts)，以及[VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits)下的推理文档。
- en: Inference
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理
- en: 'To use the MMS model, first update to the latest version of the Transformers
    library:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用MMS模型，首先更新到Transformers库的最新版本：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since the flow-based model in VITS is non-deterministic, it is good practice
    to set a seed to ensure reproducibility of the outputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于VITS中的基于流的模型是非确定性的，因此最好设置一个种子以确保输出的可重现性。
- en: 'For languages with a Roman alphabet, such as English or French, the tokenizer
    can be used directly to pre-process the text inputs. The following code example
    runs a forward pass using the MMS-TTS English checkpoint:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有罗马字母表的语言，例如英语或法语，可以直接使用分词器对文本输入进行预处理。以下代码示例使用MMS-TTS英语检查点运行前向传递：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting waveform can be saved as a `.wav` file:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的波形可以保存为`.wav`文件：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Or displayed in a Jupyter Notebook / Google Colab:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在Jupyter Notebook / Google Colab中显示：
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For certain languages with non-Roman alphabets, such as Arabic, Mandarin or
    Hindi, the [`uroman`](https://github.com/isi-nlp/uroman) perl package is required
    to pre-process the text inputs to the Roman alphabet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有非罗马字母表的某些语言，例如阿拉伯语、普通话或印地语，需要使用[`uroman`](https://github.com/isi-nlp/uroman)
    perl包对文本输入进行预处理为罗马字母表。
- en: 'You can check whether you require the `uroman` package for your language by
    inspecting the `is_uroman` attribute of the pre-trained `tokenizer`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查预训练的`tokenizer`的`is_uroman`属性来检查您的语言是否需要`uroman`包：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If required, you should apply the uroman package to your text inputs **prior**
    to passing them to the `VitsTokenizer`, since currently the tokenizer does not
    support performing the pre-processing itself.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，在将文本输入传递给`VitsTokenizer`之前，应该将uroman包应用于您的文本输入**之前**，因为目前分词器不支持执行预处理。
- en: 'To do this, first clone the uroman repository to your local machine and set
    the bash variable `UROMAN` to the local path:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，首先将uroman存储库克隆到本地计算机，并将bash变量`UROMAN`设置为本地路径：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can then pre-process the text input using the following code snippet. You
    can either rely on using the bash variable `UROMAN` to point to the uroman repository,
    or you can pass the uroman directory as an argument to the `uromaize` function:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码片段对文本输入进行预处理。您可以依赖使用bash变量`UROMAN`指向uroman存储库，或者可以将uroman目录作为参数传递给`uromaize`函数：
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Tips:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：**'
- en: The MMS-TTS checkpoints are trained on lower-cased, un-punctuated text. By default,
    the `VitsTokenizer` *normalizes* the inputs by removing any casing and punctuation,
    to avoid passing out-of-vocabulary characters to the model. Hence, the model is
    agnostic to casing and punctuation, so these should be avoided in the text prompt.
    You can disable normalisation by setting `noramlize=False` in the call to the
    tokenizer, but this will lead to un-expected behaviour and is discouraged.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMS-TTS检查点是在小写、无标点的文本上训练的。默认情况下，`VitsTokenizer`通过删除任何大小写和标点来*规范化*输入，以避免将超出词汇表的字符传递给模型。因此，模型对大小写和标点不敏感，因此应避免在文本提示中使用它们。您可以通过在调用分词器时设置`noramlize=False`来禁用规范化，但这将导致意外行为，不建议这样做。
- en: 'The speaking rate can be varied by setting the attribute `model.speaking_rate`
    to a chosen value. Likewise, the randomness of the noise is controlled by `model.noise_scale`:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将属性`model.speaking_rate`设置为所选值，可以改变说话速度。同样，噪声的随机性由`model.noise_scale`控制：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Language Identification (LID)
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言识别（LID）
- en: Different LID models are available based on the number of languages they can
    recognize - [126](https://huggingface.co/facebook/mms-lid-126), [256](https://huggingface.co/facebook/mms-lid-256),
    [512](https://huggingface.co/facebook/mms-lid-512), [1024](https://huggingface.co/facebook/mms-lid-1024),
    [2048](https://huggingface.co/facebook/mms-lid-2048), [4017](https://huggingface.co/facebook/mms-lid-4017).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据能够识别的语言数量，提供不同的LID模型 - [126](https://huggingface.co/facebook/mms-lid-126)，[256](https://huggingface.co/facebook/mms-lid-256)，[512](https://huggingface.co/facebook/mms-lid-512)，[1024](https://huggingface.co/facebook/mms-lid-1024)，[2048](https://huggingface.co/facebook/mms-lid-2048)，[4017](https://huggingface.co/facebook/mms-lid-4017)。
- en: Inference
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理
- en: First, we install transformers and some other libraries
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们安装transformers和其他一些库
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, we load a couple of audio samples via `datasets`. Make sure that the audio
    data is sampled to 16000 kHz.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过`datasets`加载一些音频样本。确保音频数据采样率为16000 kHz。
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we load the model and processor
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载模型和处理器
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we process the audio data, pass the processed audio data to the model to
    classify it into a language, just like we usually do for Wav2Vec2 audio classification
    models such as [ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处理音频数据，将处理后的音频数据传递给模型进行分类，就像我们通常对Wav2Vec2音频分类模型进行的操作一样，比如[ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To see all the supported languages of a checkpoint, you can print out the language
    ids as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 查看检查点支持的所有语言，可以按照以下方式打印语言标识符：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Audio Pretrained Models
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频预训练模型
- en: Pretrained models are available for two different sizes - [300M](https://huggingface.co/facebook/mms-300m)
    , [1Bil](https://huggingface.co/facebook/mms-1b).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型提供两种不同大小的模型 - [300M](https://huggingface.co/facebook/mms-300m)，[1Bil](https://huggingface.co/facebook/mms-1b)。
- en: The MMS for ASR architecture is based on the Wav2Vec2 model, refer to [Wav2Vec2’s
    documentation page](wav2vec2) for further details on how to finetune with models
    for various downstream tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ASR架构的MMS基于Wav2Vec2模型，请参考[Wav2Vec2的文档页面](wav2vec2)以获取有关如何针对各种下游任务微调模型的更多详细信息。
- en: MMS-TTS uses the same model architecture as VITS, refer to [VITS’s documentation
    page](vits) for API reference.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MMS-TTS使用与VITS相同的模型架构，请参考[VITS的文档页面](vits)获取API参考。
