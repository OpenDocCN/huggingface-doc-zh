["```py\npip install -q datasets transformers evaluate timm albumentations\n```", "```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```", "```py\n>>> from datasets import load_dataset\n\n>>> cppe5 = load_dataset(\"cppe-5\")\n>>> cppe5\nDatasetDict({\n    train: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['image_id', 'image', 'width', 'height', 'objects'],\n        num_rows: 29\n    })\n})\n```", "```py\n>>> cppe5[\"train\"][0]\n{'image_id': 15,\n 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10>,\n 'width': 943,\n 'height': 663,\n 'objects': {'id': [114, 115, 116, 117],\n  'area': [3796, 1596, 152768, 81002],\n  'bbox': [[302.0, 109.0, 73.0, 52.0],\n   [810.0, 100.0, 57.0, 28.0],\n   [160.0, 31.0, 248.0, 616.0],\n   [741.0, 68.0, 202.0, 401.0]],\n  'category': [4, 4, 0, 0]}}\n```", "```py\n>>> import numpy as np\n>>> import os\n>>> from PIL import Image, ImageDraw\n\n>>> image = cppe5[\"train\"][0][\"image\"]\n>>> annotations = cppe5[\"train\"][0][\"objects\"]\n>>> draw = ImageDraw.Draw(image)\n\n>>> categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n\n>>> id2label = {index: x for index, x in enumerate(categories, start=0)}\n>>> label2id = {v: k for k, v in id2label.items()}\n\n>>> for i in range(len(annotations[\"id\"])):\n...     box = annotations[\"bbox\"][i]\n...     class_idx = annotations[\"category\"][i]\n...     x, y, w, h = tuple(box)\n...     # Check if coordinates are normalized or not\n...     if max(box) > 1.0:\n...         # Coordinates are un-normalized, no need to re-scale them\n...         x1, y1 = int(x), int(y)\n...         x2, y2 = int(x + w), int(y + h)\n...     else:\n...         # Coordinates are normalized, re-scale them\n...         x1 = int(x * width)\n...         y1 = int(y * height)\n...         x2 = int((x + w) * width)\n...         y2 = int((y + h) * height)\n...     draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n...     draw.text((x, y), id2label[class_idx], fill=\"white\")\n\n>>> image\n```", "```py\n>>> remove_idx = [590, 821, 822, 875, 876, 878, 879]\n>>> keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]\n>>> cppe5[\"train\"] = cppe5[\"train\"].select(keep)\n```", "```py\n>>> from transformers import AutoImageProcessor\n\n>>> checkpoint = \"facebook/detr-resnet-50\"\n>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n```", "```py\n>>> import albumentations\n>>> import numpy as np\n>>> import torch\n\n>>> transform = albumentations.Compose(\n...     [\n...         albumentations.Resize(480, 480),\n...         albumentations.HorizontalFlip(p=1.0),\n...         albumentations.RandomBrightnessContrast(p=1.0),\n...     ],\n...     bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n... )\n```", "```py\n>>> def formatted_anns(image_id, category, area, bbox):\n...     annotations = []\n...     for i in range(0, len(category)):\n...         new_ann = {\n...             \"image_id\": image_id,\n...             \"category_id\": category[i],\n...             \"isCrowd\": 0,\n...             \"area\": area[i],\n...             \"bbox\": list(bbox[i]),\n...         }\n...         annotations.append(new_ann)\n\n...     return annotations\n```", "```py\n>>> # transforming a batch\n>>> def transform_aug_ann(examples):\n...     image_ids = examples[\"image_id\"]\n...     images, bboxes, area, categories = [], [], [], []\n...     for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n...         image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n...         out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n\n...         area.append(objects[\"area\"])\n...         images.append(out[\"image\"])\n...         bboxes.append(out[\"bboxes\"])\n...         categories.append(out[\"category\"])\n\n...     targets = [\n...         {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n...         for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n...     ]\n\n...     return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n```", "```py\n>>> cppe5[\"train\"] = cppe5[\"train\"].with_transform(transform_aug_ann)\n>>> cppe5[\"train\"][15]\n{'pixel_values': tensor([[[ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n          [ 0.9132,  0.9132,  0.9132,  ..., -1.9809, -1.9809, -1.9809],\n          [ 0.9132,  0.9132,  0.9132,  ..., -1.9638, -1.9638, -1.9638],\n          ...,\n          [-1.5699, -1.5699, -1.5699,  ..., -1.9980, -1.9980, -1.9980],\n          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809],\n          [-1.5528, -1.5528, -1.5528,  ..., -1.9980, -1.9809, -1.9809]],\n\n         [[ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n          [ 1.3081,  1.3081,  1.3081,  ..., -1.8431, -1.8431, -1.8431],\n          [ 1.3081,  1.3081,  1.3081,  ..., -1.8256, -1.8256, -1.8256],\n          ...,\n          [-1.3179, -1.3179, -1.3179,  ..., -1.8606, -1.8606, -1.8606],\n          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431],\n          [-1.3004, -1.3004, -1.3004,  ..., -1.8606, -1.8431, -1.8431]],\n\n         [[ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n          [ 1.4200,  1.4200,  1.4200,  ..., -1.6476, -1.6476, -1.6476],\n          [ 1.4200,  1.4200,  1.4200,  ..., -1.6302, -1.6302, -1.6302],\n          ...,\n          [-1.0201, -1.0201, -1.0201,  ..., -1.5604, -1.5604, -1.5604],\n          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430],\n          [-1.0027, -1.0027, -1.0027,  ..., -1.5604, -1.5430, -1.5430]]]),\n 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         ...,\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 1, 1, 1]]),\n 'labels': {'size': tensor([800, 800]), 'image_id': tensor([756]), 'class_labels': tensor([4]), 'boxes': tensor([[0.7340, 0.6986, 0.3414, 0.5944]]), 'area': tensor([519544.4375]), 'iscrowd': tensor([0]), 'orig_size': tensor([480, 480])}}\n```", "```py\n>>> def collate_fn(batch):\n...     pixel_values = [item[\"pixel_values\"] for item in batch]\n...     encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n...     labels = [item[\"labels\"] for item in batch]\n...     batch = {}\n...     batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n...     batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n...     batch[\"labels\"] = labels\n...     return batch\n```", "```py\n>>> from transformers import AutoModelForObjectDetection\n\n>>> model = AutoModelForObjectDetection.from_pretrained(\n...     checkpoint,\n...     id2label=id2label,\n...     label2id=label2id,\n...     ignore_mismatched_sizes=True,\n... )\n```", "```py\n>>> from transformers import TrainingArguments\n\n>>> training_args = TrainingArguments(\n...     output_dir=\"detr-resnet-50_finetuned_cppe5\",\n...     per_device_train_batch_size=8,\n...     num_train_epochs=10,\n...     fp16=True,\n...     save_steps=200,\n...     logging_steps=50,\n...     learning_rate=1e-5,\n...     weight_decay=1e-4,\n...     save_total_limit=2,\n...     remove_unused_columns=False,\n...     push_to_hub=True,\n... )\n```", "```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     data_collator=collate_fn,\n...     train_dataset=cppe5[\"train\"],\n...     tokenizer=image_processor,\n... )\n\n>>> trainer.train()\n```", "```py\n>>> trainer.push_to_hub()\n```", "```py\n>>> import json\n\n>>> # format annotations the same as for training, no need for data augmentation\n>>> def val_formatted_anns(image_id, objects):\n...     annotations = []\n...     for i in range(0, len(objects[\"id\"])):\n...         new_ann = {\n...             \"id\": objects[\"id\"][i],\n...             \"category_id\": objects[\"category\"][i],\n...             \"iscrowd\": 0,\n...             \"image_id\": image_id,\n...             \"area\": objects[\"area\"][i],\n...             \"bbox\": objects[\"bbox\"][i],\n...         }\n...         annotations.append(new_ann)\n\n...     return annotations\n\n>>> # Save images and annotations into the files torchvision.datasets.CocoDetection expects\n>>> def save_cppe5_annotation_file_images(cppe5):\n...     output_json = {}\n...     path_output_cppe5 = f\"{os.getcwd()}/cppe5/\"\n\n...     if not os.path.exists(path_output_cppe5):\n...         os.makedirs(path_output_cppe5)\n\n...     path_anno = os.path.join(path_output_cppe5, \"cppe5_ann.json\")\n...     categories_json = [{\"supercategory\": \"none\", \"id\": id, \"name\": id2label[id]} for id in id2label]\n...     output_json[\"images\"] = []\n...     output_json[\"annotations\"] = []\n...     for example in cppe5:\n...         ann = val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n...         output_json[\"images\"].append(\n...             {\n...                 \"id\": example[\"image_id\"],\n...                 \"width\": example[\"image\"].width,\n...                 \"height\": example[\"image\"].height,\n...                 \"file_name\": f\"{example['image_id']}.png\",\n...             }\n...         )\n...         output_json[\"annotations\"].extend(ann)\n...     output_json[\"categories\"] = categories_json\n\n...     with open(path_anno, \"w\") as file:\n...         json.dump(output_json, file, ensure_ascii=False, indent=4)\n\n...     for im, img_id in zip(cppe5[\"image\"], cppe5[\"image_id\"]):\n...         path_img = os.path.join(path_output_cppe5, f\"{img_id}.png\")\n...         im.save(path_img)\n\n...     return path_output_cppe5, path_anno\n```", "```py\n>>> import torchvision\n\n>>> class CocoDetection(torchvision.datasets.CocoDetection):\n...     def __init__(self, img_folder, image_processor, ann_file):\n...         super().__init__(img_folder, ann_file)\n...         self.image_processor = image_processor\n\n...     def __getitem__(self, idx):\n...         # read in PIL image and target in COCO format\n...         img, target = super(CocoDetection, self).__getitem__(idx)\n\n...         # preprocess image and target: converting target to DETR format,\n...         # resizing + normalization of both image and target)\n...         image_id = self.ids[idx]\n...         target = {\"image_id\": image_id, \"annotations\": target}\n...         encoding = self.image_processor(images=img, annotations=target, return_tensors=\"pt\")\n...         pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n...         target = encoding[\"labels\"][0]  # remove batch dimension\n\n...         return {\"pixel_values\": pixel_values, \"labels\": target}\n\n>>> im_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n\n>>> path_output_cppe5, path_anno = save_cppe5_annotation_file_images(cppe5[\"test\"])\n>>> test_ds_coco_format = CocoDetection(path_output_cppe5, im_processor, path_anno)\n```", "```py\n>>> import evaluate\n>>> from tqdm import tqdm\n\n>>> model = AutoModelForObjectDetection.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n>>> module = evaluate.load(\"ybelkada/cocoevaluate\", coco=test_ds_coco_format.coco)\n>>> val_dataloader = torch.utils.data.DataLoader(\n...     test_ds_coco_format, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn\n... )\n\n>>> with torch.no_grad():\n...     for idx, batch in enumerate(tqdm(val_dataloader)):\n...         pixel_values = batch[\"pixel_values\"]\n...         pixel_mask = batch[\"pixel_mask\"]\n\n...         labels = [\n...             {k: v for k, v in t.items()} for t in batch[\"labels\"]\n...         ]  # these are in DETR format, resized + normalized\n\n...         # forward pass\n...         outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n\n...         orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n...         results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to Pascal VOC format (xmin, ymin, xmax, ymax)\n\n...         module.add(prediction=results, reference=labels)\n...         del batch\n\n>>> results = module.compute()\n>>> print(results)\nAccumulating evaluation results...\nDONE (t=0.08s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.352\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.681\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.292\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.168\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.208\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.429\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.274\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.484\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.501\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590\n```", "```py\n>>> from transformers import pipeline\n>>> import requests\n\n>>> url = \"https://i.imgur.com/2lnWoly.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> obj_detector = pipeline(\"object-detection\", model=\"devonho/detr-resnet-50_finetuned_cppe5\")\n>>> obj_detector(image)\n```", "```py\n>>> image_processor = AutoImageProcessor.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n>>> model = AutoModelForObjectDetection.from_pretrained(\"devonho/detr-resnet-50_finetuned_cppe5\")\n\n>>> with torch.no_grad():\n...     inputs = image_processor(images=image, return_tensors=\"pt\")\n...     outputs = model(**inputs)\n...     target_sizes = torch.tensor([image.size[::-1]])\n...     results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     print(\n...         f\"Detected {model.config.id2label[label.item()]} with confidence \"\n...         f\"{round(score.item(), 3)} at location {box}\"\n...     )\nDetected Coverall with confidence 0.566 at location [1215.32, 147.38, 4401.81, 3227.08]\nDetected Mask with confidence 0.584 at location [2449.06, 823.19, 3256.43, 1413.9]\n```", "```py\n>>> draw = ImageDraw.Draw(image)\n\n>>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n...     box = [round(i, 2) for i in box.tolist()]\n...     x, y, x2, y2 = tuple(box)\n...     draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n...     draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n\n>>> image\n```"]