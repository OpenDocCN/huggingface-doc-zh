["```py\nfrom transformers import SwinConfig, UperNetConfig, UperNetForSemanticSegmentation\n\nbackbone_config = SwinConfig(out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n\nconfig = UperNetConfig(backbone_config=backbone_config)\nmodel = UperNetForSemanticSegmentation(config)\n```", "```py\nfrom transformers import ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation\n\nbackbone_config = ConvNextConfig(out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"])\n\nconfig = UperNetConfig(backbone_config=backbone_config)\nmodel = UperNetForSemanticSegmentation(config)\n```", "```py\n( backbone_config = None hidden_size = 512 initializer_range = 0.02 pool_scales = [1, 2, 3, 6] use_auxiliary_head = True auxiliary_loss_weight = 0.4 auxiliary_in_channels = 384 auxiliary_channels = 256 auxiliary_num_convs = 1 auxiliary_concat_input = False loss_ignore_index = 255 **kwargs )\n```", "```py\n>>> from transformers import UperNetConfig, UperNetForSemanticSegmentation\n\n>>> # Initializing a configuration\n>>> configuration = UperNetConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = UperNetForSemanticSegmentation(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( config )\n```", "```py\n( pixel_values: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None labels: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SemanticSegmenterOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n>>> from PIL import Image\n>>> from huggingface_hub import hf_hub_download\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-tiny\")\n>>> model = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-tiny\")\n\n>>> filepath = hf_hub_download(\n...     repo_id=\"hf-internal-testing/fixtures_ade20k\", filename=\"ADE_val_00000001.jpg\", repo_type=\"dataset\"\n... )\n>>> image = Image.open(filepath).convert(\"RGB\")\n\n>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits  # shape (batch_size, num_labels, height, width)\n>>> list(logits.shape)\n[1, 150, 512, 512]\n```"]