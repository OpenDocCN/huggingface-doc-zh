# LLMsçš„ç”Ÿæˆ

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)

LLMsï¼Œæˆ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„ä»¶ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä»¬ç”±å¤§å‹é¢„è®­ç»ƒçš„å˜å‹å™¨æ¨¡å‹ç»„æˆï¼Œè®­ç»ƒç”¨äºé¢„æµ‹ç»™å®šä¸€äº›è¾“å…¥æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªå•è¯ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼Œä»¤ç‰Œï¼‰ã€‚ç”±äºå®ƒä»¬ä¸€æ¬¡é¢„æµ‹ä¸€ä¸ªä»¤ç‰Œï¼Œå› æ­¤æ‚¨éœ€è¦åšä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…æ¥ç”Ÿæˆæ–°çš„å¥å­ï¼Œè€Œä¸ä»…ä»…æ˜¯è°ƒç”¨æ¨¡å‹ - æ‚¨éœ€è¦è¿›è¡Œè‡ªå›å½’ç”Ÿæˆã€‚

è‡ªå›å½’ç”Ÿæˆæ˜¯åœ¨æ¨ç†æ—¶è¿­ä»£è°ƒç”¨æ¨¡å‹ä»¥ç”Ÿæˆè¾“å‡ºçš„è¿‡ç¨‹ï¼Œç»™å®šä¸€äº›åˆå§‹è¾“å…¥ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”±[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•å¤„ç†ï¼Œé€‚ç”¨äºæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹ã€‚

æœ¬æ•™ç¨‹å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š

+   ä½¿ç”¨LLMç”Ÿæˆæ–‡æœ¬

+   é¿å…å¸¸è§é™·é˜±

+   å¸®åŠ©æ‚¨å……åˆ†åˆ©ç”¨LLMçš„ä¸‹ä¸€æ­¥

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š

```py
pip install transformers bitsandbytes>=0.39.0 -q
```

## ç”Ÿæˆæ–‡æœ¬

è¿›è¡Œ[å› æœè¯­è¨€å»ºæ¨¡](tasks/language_modeling)è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†æ–‡æœ¬ä»¤ç‰Œåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ¦‚ç‡åˆ†å¸ƒã€‚

<https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>

"LLMçš„å‰å‘ä¼ é€’"

LLMsè¿›è¡Œè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¦‚ä½•ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚åœ¨è¿™ä¸€æ­¥ä¸­å¯ä»¥é‡‡å–ä»»ä½•æ–¹æ³•ï¼Œåªè¦æœ€ç»ˆå¾—åˆ°ä¸‹ä¸€æ¬¡è¿­ä»£çš„ä»¤ç‰Œå³å¯ã€‚è¿™æ„å‘³ç€å®ƒå¯ä»¥ç®€å•åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œä¹Ÿå¯ä»¥åœ¨ä»ç»“æœåˆ†å¸ƒä¸­æŠ½æ ·ä¹‹å‰åº”ç”¨åå‡ ç§è½¬æ¢ã€‚

<https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>

"è‡ªå›å½’ç”Ÿæˆé€šè¿‡ä»æ¦‚ç‡åˆ†å¸ƒä¸­è¿­ä»£é€‰æ‹©ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¥ç”Ÿæˆæ–‡æœ¬"

ä¸Šè¿°è¿‡ç¨‹ä¼šé‡å¤è¿­ä»£ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œè¯¥æ¨¡å‹åº”è¯¥å­¦ä¼šä½•æ—¶è¾“å‡ºä¸€ä¸ªç»ˆæ­¢åºåˆ—ï¼ˆ`EOS`ï¼‰ä»¤ç‰Œã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œå½“è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œç”Ÿæˆä¼šåœæ­¢ã€‚

æ­£ç¡®è®¾ç½®ä»¤ç‰Œé€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹äºä½¿æ‚¨çš„æ¨¡å‹åœ¨ä»»åŠ¡ä¸Šè¡¨ç°å¦‚æ‚¨æœŸæœ›çš„æ–¹å¼è‡³å…³é‡è¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ºæ¯ä¸ªæ¨¡å‹å…³è”ä¸€ä¸ª[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªè‰¯å¥½çš„é»˜è®¤ç”Ÿæˆå‚æ•°è®¾ç½®ï¼Œå¹¶ä¸”ä¸æ‚¨çš„æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚

è®©æˆ‘ä»¬è°ˆè°ˆä»£ç ï¼

å¦‚æœæ‚¨å¯¹åŸºæœ¬LLMç”¨æ³•æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬çš„é«˜çº§[`Pipeline`](pipeline_tutorial)æ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMsé€šå¸¸éœ€è¦é«˜çº§åŠŸèƒ½ï¼Œå¦‚é‡åŒ–å’Œå¯¹ä»¤ç‰Œé€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶ï¼Œæœ€å¥½é€šè¿‡[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ¥å®ç°ã€‚LLMsçš„è‡ªå›å½’ç”Ÿæˆä¹Ÿéœ€è¦å¤§é‡èµ„æºï¼Œå¹¶ä¸”åº”è¯¥åœ¨GPUä¸Šæ‰§è¡Œä»¥è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚

é¦–å…ˆï¼Œæ‚¨éœ€è¦åŠ è½½æ¨¡å‹ã€‚

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

æ‚¨ä¼šæ³¨æ„åˆ°`from_pretrained`è°ƒç”¨ä¸­æœ‰ä¸¤ä¸ªæ ‡å¿—ï¼š

+   `device_map`ç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°æ‚¨çš„GPUä¸Š

+   `load_in_4bit`åº”ç”¨[4ä½åŠ¨æ€é‡åŒ–](main_classes/quantization)ä»¥å¤§å¹…å‡å°‘èµ„æºéœ€æ±‚

è¿˜æœ‰å…¶ä»–åˆå§‹åŒ–æ¨¡å‹çš„æ–¹æ³•ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åŸºå‡†ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨LLMã€‚

æ¥ä¸‹æ¥ï¼Œæ‚¨éœ€è¦ä½¿ç”¨[tokenizer](tokenizer_summary)å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œé¢„å¤„ç†ã€‚

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

`model_inputs`å˜é‡ä¿å­˜äº†æ ‡è®°åŒ–çš„æ–‡æœ¬è¾“å…¥ï¼Œä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚è™½ç„¶[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ä¼šå°½åŠ›æ¨æ–­æ³¨æ„åŠ›æ©ç ï¼Œä½†æˆ‘ä»¬å»ºè®®å°½å¯èƒ½åœ¨ç”Ÿæˆæ—¶ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚

åœ¨å¯¹è¾“å…¥è¿›è¡Œæ ‡è®°åŒ–åï¼Œæ‚¨å¯ä»¥è°ƒç”¨[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)æ–¹æ³•è¿”å›ç”Ÿæˆçš„æ ‡è®°ã€‚ç„¶ååº”å°†ç”Ÿæˆçš„æ ‡è®°è½¬æ¢ä¸ºæ–‡æœ¬åæ‰“å°ã€‚

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

æœ€åï¼Œæ‚¨ä¸éœ€è¦ä¸€æ¬¡å¤„ç†ä¸€ä¸ªåºåˆ—ï¼æ‚¨å¯ä»¥å¯¹è¾“å…¥è¿›è¡Œæ‰¹å¤„ç†ï¼Œè¿™å°†å¤§å¤§æé«˜ååé‡ï¼ŒåŒæ—¶å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬å¾ˆå°ã€‚æ‚¨åªéœ€è¦ç¡®ä¿æ­£ç¡®å¡«å……è¾“å…¥å³å¯ï¼ˆä¸‹æ–‡æœ‰æ›´å¤šä¿¡æ¯ï¼‰ã€‚

```py
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

å°±æ˜¯è¿™æ ·ï¼åœ¨å‡ è¡Œä»£ç ä¸­ï¼Œæ‚¨å°±å¯ä»¥åˆ©ç”¨LLMçš„å¼ºå¤§åŠŸèƒ½ã€‚

## å¸¸è§é™·é˜±

æœ‰è®¸å¤š[ç”Ÿæˆç­–ç•¥](generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç”¨ä¾‹ã€‚å¦‚æœæ‚¨çš„è¾“å‡ºä¸æ‚¨çš„é¢„æœŸä¸ç¬¦ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªå…³äºæœ€å¸¸è§é™·é˜±ä»¥åŠå¦‚ä½•é¿å…å®ƒä»¬çš„åˆ—è¡¨ã€‚

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

### ç”Ÿæˆçš„è¾“å‡ºè¿‡çŸ­/è¿‡é•¿

å¦‚æœåœ¨[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æœªæŒ‡å®šï¼Œ`generate`é»˜è®¤è¿”å›æœ€å¤š20ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®åœ¨`generate`è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®`max_new_tokens`ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°æ ‡è®°æ•°é‡ã€‚è¯·è®°ä½ï¼ŒLLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…è§£ç å™¨æ¨¡å‹ï¼‰è¿˜ä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚

```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### ç”Ÿæˆæ¨¡å¼ä¸æ­£ç¡®

é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)æ–‡ä»¶ä¸­æŒ‡å®šï¼Œ`generate`åœ¨æ¯æ¬¡è¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„æ ‡è®°ï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œè¿™å¯èƒ½æ˜¯ä¸å¸Œæœ›çš„ï¼›åƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡å—ç›ŠäºæŠ½æ ·ã€‚å¦ä¸€æ–¹é¢ï¼ŒåƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„è¾“å…¥é©±åŠ¨ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚é€šè¿‡`do_sample=True`å¯ç”¨æŠ½æ ·ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤[åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªä¸»é¢˜çš„ä¿¡æ¯ã€‚

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

### å¡«å……æ–¹å‘é”™è¯¯

LLMsæ˜¯ä»…è§£ç å™¨æ¶æ„ï¼Œæ„å‘³ç€å®ƒä»¬ä¼šç»§ç»­è¿­ä»£æ‚¨çš„è¾“å…¥æç¤ºã€‚å¦‚æœæ‚¨çš„è¾“å…¥é•¿åº¦ä¸åŒï¼Œå°±éœ€è¦è¿›è¡Œå¡«å……ã€‚ç”±äºLLMsæ²¡æœ‰ç»è¿‡è®­ç»ƒä»¥ä»å¡«å……æ ‡è®°ç»§ç»­ï¼Œå› æ­¤æ‚¨çš„è¾“å…¥éœ€è¦è¿›è¡Œå·¦å¡«å……ã€‚ç¡®ä¿ä¸è¦å¿˜è®°ä¼ é€’æ³¨æ„åŠ›æ©ç ä»¥ç”Ÿæˆï¼

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

### æç¤ºé”™è¯¯

ä¸€äº›æ¨¡å‹å’Œä»»åŠ¡æœŸæœ›ç‰¹å®šçš„è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å¦‚æœæœªåº”ç”¨æ­¤æ ¼å¼ï¼Œæ‚¨å°†è·å¾—æ²‰é»˜çš„æ€§èƒ½ä¸‹é™ï¼šæ¨¡å‹å¯èƒ½ä¼šè¿è¡Œï¼Œä½†ä¸å¦‚æŒ‰ç…§é¢„æœŸæç¤ºé‚£æ ·å¥½ã€‚æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œå¯åœ¨æ­¤[æŒ‡å—](tasks/prompting)ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªèŠå¤©LLMçš„ç¤ºä¾‹ï¼Œå®ƒä½¿ç”¨èŠå¤©æ¨¡æ¿ï¼š

```py
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style ğŸ˜
```

## æ›´å¤šèµ„æº

è™½ç„¶è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä½†å……åˆ†åˆ©ç”¨æ‚¨çš„LLMå¯èƒ½æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠªåŠ›ï¼Œå› ä¸ºå…¶ä¸­æœ‰è®¸å¤šè¦ç´ ã€‚ä¸ºäº†å¸®åŠ©æ‚¨æ·±å…¥äº†è§£LLMçš„ä½¿ç”¨å’Œç†è§£ï¼Œè¯·ç»§ç»­ä»¥ä¸‹æ­¥éª¤ï¼š

### é«˜çº§ç”Ÿæˆç”¨æ³•

1.  [æŒ‡å—](generation_strategies)å…³äºå¦‚ä½•æ§åˆ¶ä¸åŒçš„ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚ä½•è®¾ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œä»¥åŠå¦‚ä½•æµå¼ä¼ è¾“è¾“å‡ºï¼›

1.  [æŒ‡å—](chat_templating)å…³äºèŠå¤©LLMçš„æç¤ºæ¨¡æ¿ï¼›

1.  [æŒ‡å—](tasks/prompting)å¦‚ä½•å……åˆ†åˆ©ç”¨æç¤ºè®¾è®¡ï¼›

1.  [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) ä¸Šçš„ API å‚è€ƒï¼Œ[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)ï¼Œä»¥åŠ [generate-related classes](internal/generation_utils)ã€‚å¤§å¤šæ•°ç±»ï¼ŒåŒ…æ‹¬ logits å¤„ç†å™¨ï¼Œéƒ½æœ‰ä½¿ç”¨ç¤ºä¾‹ï¼

### LLM æ’è¡Œæ¦œ

1.  [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)ï¼Œä¸“æ³¨äºå¼€æºæ¨¡å‹çš„è´¨é‡ï¼›

1.  [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)ï¼Œä¸“æ³¨äº LLM ååé‡ã€‚

### å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡

1.  [Guide](llm_tutorial_optimization) å¦‚ä½•ä¼˜åŒ– LLMs çš„é€Ÿåº¦å’Œå†…å­˜ï¼›

1.  [Guide](main_classes/quantization) å…³äºé‡åŒ–ï¼Œå¦‚ bitsandbytes å’Œ autogptqï¼Œå±•ç¤ºäº†å¦‚ä½•å¤§å¹…å‡å°‘å†…å­˜éœ€æ±‚ã€‚

### ç›¸å…³åº“

1.  [`text-generation-inference`](https://github.com/huggingface/text-generation-inference)ï¼Œä¸€ä¸ªä¸º LLMs å‡†å¤‡çš„ç”Ÿäº§å°±ç»ªæœåŠ¡å™¨ï¼›

1.  [`optimum`](https://github.com/huggingface/optimum)ï¼Œä¸€ä¸ª ğŸ¤— Transformers çš„æ‰©å±•ï¼Œé’ˆå¯¹ç‰¹å®šç¡¬ä»¶è®¾å¤‡è¿›è¡Œä¼˜åŒ–ã€‚
