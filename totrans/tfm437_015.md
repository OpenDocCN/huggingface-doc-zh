# LLMs的生成

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial](https://huggingface.co/docs/transformers/v4.37.2/en/llm_tutorial)

LLMs，或大型语言模型，是文本生成背后的关键组件。简而言之，它们由大型预训练的变压器模型组成，训练用于预测给定一些输入文本的下一个单词（或更准确地说，令牌）。由于它们一次预测一个令牌，因此您需要做一些更复杂的事情来生成新的句子，而不仅仅是调用模型 - 您需要进行自回归生成。

自回归生成是在推理时迭代调用模型以生成输出的过程，给定一些初始输入。在🤗 Transformers中，这由[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法处理，适用于所有具有生成能力的模型。

本教程将向您展示如何：

+   使用LLM生成文本

+   避免常见陷阱

+   帮助您充分利用LLM的下一步

在开始之前，请确保您已安装所有必要的库：

```py
pip install transformers bitsandbytes>=0.39.0 -q
```

## 生成文本

进行[因果语言建模](tasks/language_modeling)训练的语言模型将文本令牌序列作为输入，并返回下一个令牌的概率分布。

<https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov>

"LLM的前向传递"

LLMs进行自回归生成的一个关键方面是如何从这个概率分布中选择下一个令牌。在这一步中可以采取任何方法，只要最终得到下一次迭代的令牌即可。这意味着它可以简单地从概率分布中选择最可能的令牌，也可以在从结果分布中抽样之前应用十几种转换。

<https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov>

"自回归生成通过从概率分布中迭代选择下一个令牌来生成文本"

上述过程会重复迭代，直到达到某个停止条件。理想情况下，停止条件由模型决定，该模型应该学会何时输出一个终止序列（`EOS`）令牌。如果不是这种情况，当达到某个预定义的最大长度时，生成会停止。

正确设置令牌选择步骤和停止条件对于使您的模型在任务上表现如您期望的方式至关重要。这就是为什么我们为每个模型关联一个[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件，其中包含一个良好的默认生成参数设置，并且与您的模型一起加载。

让我们谈谈代码！

如果您对基本LLM用法感兴趣，我们的高级[`Pipeline`](pipeline_tutorial)接口是一个很好的起点。然而，LLMs通常需要高级功能，如量化和对令牌选择步骤的精细控制，最好通过[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)来实现。LLMs的自回归生成也需要大量资源，并且应该在GPU上执行以获得足够的吞吐量。

首先，您需要加载模型。

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

您会注意到`from_pretrained`调用中有两个标志：

+   `device_map`确保模型被移动到您的GPU上

+   `load_in_4bit`应用[4位动态量化](main_classes/quantization)以大幅减少资源需求

还有其他初始化模型的方法，但这是一个很好的基准，可以开始使用LLM。

接下来，您需要使用[tokenizer](tokenizer_summary)对文本输入进行预处理。

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

`model_inputs`变量保存了标记化的文本输入，以及注意力掩码。虽然[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)会尽力推断注意力掩码，但我们建议尽可能在生成时传递它以获得最佳结果。

在对输入进行标记化后，您可以调用[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)方法返回生成的标记。然后应将生成的标记转换为文本后打印。

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

最后，您不需要一次处理一个序列！您可以对输入进行批处理，这将大大提高吞吐量，同时延迟和内存成本很小。您只需要确保正确填充输入即可（下文有更多信息）。

```py
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

就是这样！在几行代码中，您就可以利用LLM的强大功能。

## 常见陷阱

有许多[生成策略](generation_strategies)，有时默认值可能不适合您的用例。如果您的输出与您的预期不符，我们已经创建了一个关于最常见陷阱以及如何避免它们的列表。

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

### 生成的输出过短/过长

如果在[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件中未指定，`generate`默认返回最多20个标记。我们强烈建议在`generate`调用中手动设置`max_new_tokens`以控制它可以返回的最大新标记数量。请记住，LLMs（更准确地说，仅解码器模型）还会将输入提示作为输出的一部分返回。

```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### 生成模式不正确

默认情况下，除非在[GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig)文件中指定，`generate`在每次迭代中选择最可能的标记（贪婪解码）。根据您的任务，这可能是不希望的；像聊天机器人或写作文章这样的创造性任务受益于抽样。另一方面，像音频转录或翻译这样的输入驱动任务受益于贪婪解码。通过`do_sample=True`启用抽样，您可以在此[博客文章](https://huggingface.co/blog/how-to-generate)中了解更多关于这个主题的信息。

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

### 填充方向错误

LLMs是仅解码器架构，意味着它们会继续迭代您的输入提示。如果您的输入长度不同，就需要进行填充。由于LLMs没有经过训练以从填充标记继续，因此您的输入需要进行左填充。确保不要忘记传递注意力掩码以生成！

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

### 提示错误

一些模型和任务期望特定的输入提示格式才能正常工作。如果未应用此格式，您将获得沉默的性能下降：模型可能会运行，但不如按照预期提示那样好。有关提示的更多信息，包括哪些模型和任务需要小心，可在此[指南](tasks/prompting)中找到。让我们看一个聊天LLM的示例，它使用聊天模板：

```py
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style 😎
```

## 更多资源

虽然自回归生成过程相对简单，但充分利用您的LLM可能是一项具有挑战性的努力，因为其中有许多要素。为了帮助您深入了解LLM的使用和理解，请继续以下步骤：

### 高级生成用法

1.  [指南](generation_strategies)关于如何控制不同的生成方法，如何设置生成配置文件，以及如何流式传输输出；

1.  [指南](chat_templating)关于聊天LLM的提示模板；

1.  [指南](tasks/prompting)如何充分利用提示设计；

1.  [GenerationConfig](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationConfig) 上的 API 参考，[generate()](/docs/transformers/v4.37.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)，以及 [generate-related classes](internal/generation_utils)。大多数类，包括 logits 处理器，都有使用示例！

### LLM 排行榜

1.  [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，专注于开源模型的质量；

1.  [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)，专注于 LLM 吞吐量。

### 延迟、吞吐量和内存利用率

1.  [Guide](llm_tutorial_optimization) 如何优化 LLMs 的速度和内存；

1.  [Guide](main_classes/quantization) 关于量化，如 bitsandbytes 和 autogptq，展示了如何大幅减少内存需求。

### 相关库

1.  [`text-generation-inference`](https://github.com/huggingface/text-generation-inference)，一个为 LLMs 准备的生产就绪服务器；

1.  [`optimum`](https://github.com/huggingface/optimum)，一个 🤗 Transformers 的扩展，针对特定硬件设备进行优化。
