# 导出为ONNX

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/serialization](https://huggingface.co/docs/transformers/v4.37.2/en/serialization)

在生产环境中部署🤗 Transformers模型通常需要将模型导出为可以在专用运行时和硬件上加载和执行的序列化格式。

🤗 Optimum是Transformers的扩展，通过其`exporters`模块使得可以将模型从PyTorch或TensorFlow导出为ONNX和TFLite等序列化格式。🤗 Optimum还提供了一套性能优化工具，以在目标硬件上以最大效率训练和运行模型。

本指南演示了如何使用🤗 Optimum将🤗 Transformers模型导出为ONNX，有关将模型导出为TFLite的指南，请参考[导出到TFLite页面](tflite)。

## 导出为ONNX

[ONNX（Open Neural Network eXchange）](http://onnx.ai)是一个开放标准，定义了一组通用操作符和一种通用文件格式，用于在各种框架中表示深度学习模型，包括PyTorch和TensorFlow。当模型导出为ONNX格式时，这些操作符用于构建计算图（通常称为*中间表示*），表示数据通过神经网络的流动。

通过公开具有标准化操作符和数据类型的图，ONNX使得在不同框架之间轻松切换变得容易。例如，在PyTorch中训练的模型可以导出为ONNX格式，然后在TensorFlow中导入（反之亦然）。

将模型导出为ONNX格式后，可以：

+   通过诸如[图优化](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)和[量化](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)等技术进行推理优化。

+   使用ONNX Runtime运行，通过[`ORTModelForXXX`类](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)，其遵循与🤗 Transformers中您习惯的`AutoModel` API相同。

+   使用[优化推理流水线](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)运行，其API与🤗 Transformers中的[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)函数相同。

🤗 Optimum通过利用配置对象提供对ONNX导出的支持。这些配置对象已经为许多模型架构准备好，并且设计为易于扩展到其他架构。

有关现成配置列表，请参阅[🤗 Optimum文档](https://huggingface.co/docs/optimum/exporters/onnx/overview)。

有两种将🤗 Transformers模型导出为ONNX的方法，这里我们展示两种：

+   通过CLI使用🤗 Optimum导出。

+   使用`optimum.onnxruntime`与🤗 Optimum导出。

### 使用CLI将🤗 Transformers模型导出为ONNX

要将🤗 Transformers模型导出为ONNX，首先安装额外的依赖项：

```py
pip install optimum[exporters]
```

要查看所有可用参数，请参考[🤗 Optimum文档](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)，或在命令行中查看帮助：

```py
optimum-cli export onnx --help
```

要从🤗 Hub导出模型的检查点，例如`distilbert-base-uncased-distilled-squad`，请运行以下命令：

```py
optimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

您应该看到指示进度并显示结果`model.onnx`保存位置的日志，如下所示：

```py
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[✓] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

上面的示例说明了从 🤗 Hub 导出检查点。在导出本地模型时，首先确保您将模型的权重和分词器文件保存在同一个目录中（`local_path`）。在使用 CLI 时，将 `local_path` 传递给 `model` 参数，而不是在 🤗 Hub 上提供检查点名称，并提供 `--task` 参数。您可以在 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/task_manager) 中查看支持的任务列表。如果未提供 `task` 参数，它将默认为没有任何特定任务头的模型架构。

```py
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

导出的 `model.onnx` 文件可以在支持ONNX标准的 [许多加速器](https://onnx.ai/supported-tools.html#deployModel) 中运行。例如，我们可以使用 [ONNX Runtime](https://onnxruntime.ai/) 加载和运行模型如下：

```py
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

这个过程对于 Hub 上的 TensorFlow 检查点是相同的。例如，这里是如何从 [Keras 组织](https://huggingface.co/keras-io) 导出一个纯 TensorFlow 检查点的：

```py
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### 使用 optimum.onnxruntime 将 🤗 Transformers 模型导出到 ONNX

与 CLI 的替代方法是，您可以像这样以编程方式将 🤗 Transformers 模型导出到 ONNX：

```py
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### 将模型导出到不受支持的架构

如果您希望通过为当前无法导出的模型添加支持来做出贡献，您应该首先检查它是否在 [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) 中受支持，如果不是，可以直接 [为 🤗 Optimum 做出贡献](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)。

### 使用 transformers.onnx 导出模型

`tranformers.onnx` 不再维护，请按照上述使用 🤗 Optimum 导出模型的方法。这一部分将在未来版本中被移除。

要使用 `transformers.onnx` 将 🤗 Transformers 模型导出到 ONNX，需要安装额外的依赖：

```py
pip install transformers[onnx]
```

使用 `transformers.onnx` 包作为 Python 模块，使用现成的配置导出检查点：

```py
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

这将导出由 `--model` 参数定义的检查点的 ONNX 图。传递任何在 🤗 Hub 上或本地存储的检查点。导出的 `model.onnx` 文件可以在支持ONNX标准的许多加速器中运行。例如，加载并使用 ONNX Runtime 运行模型如下：

```py
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

所需的输出名称（如 `["last_hidden_state"]`）可以通过查看每个模型的 ONNX 配置来获得。例如，对于 DistilBERT，我们有：

```py
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

这个过程对于 Hub 上的 TensorFlow 检查点是相同的。例如，像这样导出一个纯 TensorFlow 检查点：

```py
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

要导出存储在本地的模型，请将模型的权重和分词器文件保存在同一个目录中（例如 `local-pt-checkpoint`），然后通过将 `transformers.onnx` 包的 `--model` 参数指向所需目录来将其导出到 ONNX：

```py
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```
