# å¯¼å‡ºä¸ºONNX

> åŸå§‹æ–‡æœ¬ï¼š[https://huggingface.co/docs/transformers/v4.37.2/en/serialization](https://huggingface.co/docs/transformers/v4.37.2/en/serialization)

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ğŸ¤— Transformersæ¨¡å‹é€šå¸¸éœ€è¦å°†æ¨¡å‹å¯¼å‡ºä¸ºå¯ä»¥åœ¨ä¸“ç”¨è¿è¡Œæ—¶å’Œç¡¬ä»¶ä¸ŠåŠ è½½å’Œæ‰§è¡Œçš„åºåˆ—åŒ–æ ¼å¼ã€‚

ğŸ¤— Optimumæ˜¯Transformersçš„æ‰©å±•ï¼Œé€šè¿‡å…¶`exporters`æ¨¡å—ä½¿å¾—å¯ä»¥å°†æ¨¡å‹ä»PyTorchæˆ–TensorFlowå¯¼å‡ºä¸ºONNXå’ŒTFLiteç­‰åºåˆ—åŒ–æ ¼å¼ã€‚ğŸ¤— Optimumè¿˜æä¾›äº†ä¸€å¥—æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œä»¥åœ¨ç›®æ ‡ç¡¬ä»¶ä¸Šä»¥æœ€å¤§æ•ˆç‡è®­ç»ƒå’Œè¿è¡Œæ¨¡å‹ã€‚

æœ¬æŒ‡å—æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ğŸ¤— Optimumå°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXï¼Œæœ‰å…³å°†æ¨¡å‹å¯¼å‡ºä¸ºTFLiteçš„æŒ‡å—ï¼Œè¯·å‚è€ƒ[å¯¼å‡ºåˆ°TFLiteé¡µé¢](tflite)ã€‚

## å¯¼å‡ºä¸ºONNX

[ONNXï¼ˆOpen Neural Network eXchangeï¼‰](http://onnx.ai)æ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨æ“ä½œç¬¦å’Œä¸€ç§é€šç”¨æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºåœ¨å„ç§æ¡†æ¶ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬PyTorchå’ŒTensorFlowã€‚å½“æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼æ—¶ï¼Œè¿™äº›æ“ä½œç¬¦ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¡¨ç¤ºæ•°æ®é€šè¿‡ç¥ç»ç½‘ç»œçš„æµåŠ¨ã€‚

é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–æ“ä½œç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNXä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´è½»æ¾åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œåœ¨PyTorchä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œç„¶ååœ¨TensorFlowä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

å°†æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼åï¼Œå¯ä»¥ï¼š

+   é€šè¿‡è¯¸å¦‚[å›¾ä¼˜åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)å’Œ[é‡åŒ–](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)ç­‰æŠ€æœ¯è¿›è¡Œæ¨ç†ä¼˜åŒ–ã€‚

+   ä½¿ç”¨ONNX Runtimeè¿è¡Œï¼Œé€šè¿‡[`ORTModelForXXX`ç±»](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort)ï¼Œå…¶éµå¾ªä¸ğŸ¤— Transformersä¸­æ‚¨ä¹ æƒ¯çš„`AutoModel` APIç›¸åŒã€‚

+   ä½¿ç”¨[ä¼˜åŒ–æ¨ç†æµæ°´çº¿](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)è¿è¡Œï¼Œå…¶APIä¸ğŸ¤— Transformersä¸­çš„[pipeline()](/docs/transformers/v4.37.2/en/main_classes/pipelines#transformers.pipeline)å‡½æ•°ç›¸åŒã€‚

ğŸ¤— Optimumé€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡æä¾›å¯¹ONNXå¯¼å‡ºçš„æ”¯æŒã€‚è¿™äº›é…ç½®å¯¹è±¡å·²ç»ä¸ºè®¸å¤šæ¨¡å‹æ¶æ„å‡†å¤‡å¥½ï¼Œå¹¶ä¸”è®¾è®¡ä¸ºæ˜“äºæ‰©å±•åˆ°å…¶ä»–æ¶æ„ã€‚

æœ‰å…³ç°æˆé…ç½®åˆ—è¡¨ï¼Œè¯·å‚é˜…[ğŸ¤— Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã€‚

æœ‰ä¸¤ç§å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXçš„æ–¹æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å±•ç¤ºä¸¤ç§ï¼š

+   é€šè¿‡CLIä½¿ç”¨ğŸ¤— Optimumå¯¼å‡ºã€‚

+   ä½¿ç”¨`optimum.onnxruntime`ä¸ğŸ¤— Optimumå¯¼å‡ºã€‚

### ä½¿ç”¨CLIå°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNX

è¦å°†ğŸ¤— Transformersæ¨¡å‹å¯¼å‡ºä¸ºONNXï¼Œé¦–å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š

```py
pip install optimum[exporters]
```

è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œè¯·å‚è€ƒ[ğŸ¤— Optimumæ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ï¼Œæˆ–åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š

```py
optimum-cli export onnx --help
```

è¦ä»ğŸ¤— Hubå¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚`distilbert-base-uncased-distilled-squad`ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```py
optimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

æ‚¨åº”è¯¥çœ‹åˆ°æŒ‡ç¤ºè¿›åº¦å¹¶æ˜¾ç¤ºç»“æœ`model.onnx`ä¿å­˜ä½ç½®çš„æ—¥å¿—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output "start_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output "end_logits":
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx
```

ä¸Šé¢çš„ç¤ºä¾‹è¯´æ˜äº†ä» ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹ã€‚åœ¨å¯¼å‡ºæœ¬åœ°æ¨¡å‹æ—¶ï¼Œé¦–å…ˆç¡®ä¿æ‚¨å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ï¼ˆ`local_path`ï¼‰ã€‚åœ¨ä½¿ç”¨ CLI æ—¶ï¼Œå°† `local_path` ä¼ é€’ç»™ `model` å‚æ•°ï¼Œè€Œä¸æ˜¯åœ¨ ğŸ¤— Hub ä¸Šæä¾›æ£€æŸ¥ç‚¹åç§°ï¼Œå¹¶æä¾› `--task` å‚æ•°ã€‚æ‚¨å¯ä»¥åœ¨ [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/task_manager) ä¸­æŸ¥çœ‹æ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ã€‚å¦‚æœæœªæä¾› `task` å‚æ•°ï¼Œå®ƒå°†é»˜è®¤ä¸ºæ²¡æœ‰ä»»ä½•ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹æ¶æ„ã€‚

```py
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

å¯¼å‡ºçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒONNXæ ‡å‡†çš„ [è®¸å¤šåŠ é€Ÿå™¨](https://onnx.ai/supported-tools.html#deployModel) ä¸­è¿è¡Œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [ONNX Runtime](https://onnxruntime.ai/) åŠ è½½å’Œè¿è¡Œæ¨¡å‹å¦‚ä¸‹ï¼š

```py
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")
>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
>>> outputs = model(**inputs)
```

è¿™ä¸ªè¿‡ç¨‹å¯¹äº Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å¦‚ä½•ä» [Keras ç»„ç»‡](https://huggingface.co/keras-io) å¯¼å‡ºä¸€ä¸ªçº¯ TensorFlow æ£€æŸ¥ç‚¹çš„ï¼š

```py
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### ä½¿ç”¨ optimum.onnxruntime å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNX

ä¸ CLI çš„æ›¿ä»£æ–¹æ³•æ˜¯ï¼Œæ‚¨å¯ä»¥åƒè¿™æ ·ä»¥ç¼–ç¨‹æ–¹å¼å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼š

```py
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert_base_uncased_squad"
>>> save_directory = "onnx/"

>>> # Load a model from transformers and export it to ONNX
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

>>> # Save the onnx model and tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)
```

### å°†æ¨¡å‹å¯¼å‡ºåˆ°ä¸å—æ”¯æŒçš„æ¶æ„

å¦‚æœæ‚¨å¸Œæœ›é€šè¿‡ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒæ¥åšå‡ºè´¡çŒ®ï¼Œæ‚¨åº”è¯¥é¦–å…ˆæ£€æŸ¥å®ƒæ˜¯å¦åœ¨ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) ä¸­å—æ”¯æŒï¼Œå¦‚æœä¸æ˜¯ï¼Œå¯ä»¥ç›´æ¥ [ä¸º ğŸ¤— Optimum åšå‡ºè´¡çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚

### ä½¿ç”¨ transformers.onnx å¯¼å‡ºæ¨¡å‹

`tranformers.onnx` ä¸å†ç»´æŠ¤ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°ä½¿ç”¨ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹çš„æ–¹æ³•ã€‚è¿™ä¸€éƒ¨åˆ†å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚

è¦ä½¿ç”¨ `transformers.onnx` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼Œéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–ï¼š

```py
pip install transformers[onnx]
```

ä½¿ç”¨ `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ï¼Œä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š

```py
python -m transformers.onnx --model=distilbert-base-uncased onnx/
```

è¿™å°†å¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ é€’ä»»ä½•åœ¨ ğŸ¤— Hub ä¸Šæˆ–æœ¬åœ°å­˜å‚¨çš„æ£€æŸ¥ç‚¹ã€‚å¯¼å‡ºçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒONNXæ ‡å‡†çš„è®¸å¤šåŠ é€Ÿå™¨ä¸­è¿è¡Œã€‚ä¾‹å¦‚ï¼ŒåŠ è½½å¹¶ä½¿ç”¨ ONNX Runtime è¿è¡Œæ¨¡å‹å¦‚ä¸‹ï¼š

```py
>>> from transformers import AutoTokenizer
>>> from onnxruntime import InferenceSession

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
>>> session = InferenceSession("onnx/model.onnx")
>>> # ONNX Runtime expects NumPy arrays as input
>>> inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
>>> outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))
```

æ‰€éœ€çš„è¾“å‡ºåç§°ï¼ˆå¦‚ `["last_hidden_state"]`ï¼‰å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®æ¥è·å¾—ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œæˆ‘ä»¬æœ‰ï¼š

```py
>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

>>> config = DistilBertConfig()
>>> onnx_config = DistilBertOnnxConfig(config)
>>> print(list(onnx_config.outputs.keys()))
["last_hidden_state"]
```

è¿™ä¸ªè¿‡ç¨‹å¯¹äº Hub ä¸Šçš„ TensorFlow æ£€æŸ¥ç‚¹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œåƒè¿™æ ·å¯¼å‡ºä¸€ä¸ªçº¯ TensorFlow æ£€æŸ¥ç‚¹ï¼š

```py
python -m transformers.onnx --model=keras-io/transformers-qa onnx/
```

è¦å¯¼å‡ºå­˜å‚¨åœ¨æœ¬åœ°çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx` åŒ…çš„ `--model` å‚æ•°æŒ‡å‘æ‰€éœ€ç›®å½•æ¥å°†å…¶å¯¼å‡ºåˆ° ONNXï¼š

```py
python -m transformers.onnx --model=local-pt-checkpoint onnx/
```
