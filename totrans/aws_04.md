# å¿«é€Ÿå…¥é—¨

> [https://huggingface.co/docs/optimum-neuron/quickstart](https://huggingface.co/docs/optimum-neuron/quickstart)

ğŸ¤— Optimum Neuronçš„è®¾è®¡ç›®æ ‡æ˜¯ï¼š**ä½¿ä»»ä½•ğŸ¤— Transformersç”¨æˆ·èƒ½å¤Ÿè½»æ¾è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶åˆ©ç”¨AWSåŠ é€Ÿå™¨çš„å…¨éƒ¨åŠŸèƒ½**ã€‚

## è®­ç»ƒ

æœ‰ä¸¤ä¸ªä¸»è¦çš„ç±»éœ€è¦äº†è§£ï¼š

+   NeuronArgumentParserï¼šç»§æ‰¿äº†Transformersä¸­åŸå§‹çš„[HfArgumentParser](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.HfArgumentParser)ï¼Œå¹¶å¯¹å‚æ•°å€¼è¿›è¡Œé¢å¤–æ£€æŸ¥ï¼Œä»¥ç¡®ä¿å®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°ä¸AWS Trainiumå®ä¾‹é…åˆä½¿ç”¨ã€‚

+   [NeuronTrainer](https://huggingface.co/docs/optimum/neuron/package_reference/trainer)ï¼šè´Ÿè´£ç¼–è¯‘å’Œåˆ†å‘æ¨¡å‹ä»¥åœ¨TrainiumèŠ¯ç‰‡ä¸Šè¿è¡Œï¼Œå¹¶æ‰§è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„è®­ç»ƒå™¨ç±»ã€‚

NeuronTraineréå¸¸ç±»ä¼¼äºğŸ¤— Transformers Trainerï¼Œå¹¶ä¸”é€šè¿‡ä½¿ç”¨NeuronTraineræ¥è°ƒæ•´è„šæœ¬ä»¥ä½¿å…¶ä¸Trainiumä¸€èµ·å·¥ä½œï¼Œä¸»è¦æ˜¯ç®€å•åœ°å°†`Trainer`ç±»æ›¿æ¢ä¸º`NeuronTrainer`ç±»ã€‚è¿™å°±æ˜¯å¤§å¤šæ•°[ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/optimum-neuron/tree/main/examples)æ˜¯å¦‚ä½•ä»å®ƒä»¬çš„[åŸå§‹å¯¹åº”ç‰©](https://github.com/huggingface/transformers/tree/main/examples/pytorch)è¿›è¡Œè°ƒæ•´çš„ã€‚

ä¿®æ”¹ï¼š

```py
from transformers import TrainingArguments
-from transformers import Trainer
+from optimum.neuron import NeuronTrainer as Trainer
training_args = TrainingArguments(
  # training arguments...
)
# A lot of code here
# Initialize our Trainer
trainer = Trainer(
    model=model,
    args=training_args,  # Original training arguments.
    train_dataset=train_dataset if training_args.do_train else None,
    eval_dataset=eval_dataset if training_args.do_eval else None,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
```

æ‰€æœ‰Trainiumå®ä¾‹è‡³å°‘é…å¤‡2ä¸ªNeuronæ ¸å¿ƒã€‚ä¸ºäº†åˆ©ç”¨è¿™äº›æ ¸å¿ƒï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`torchrun`å¯åŠ¨è®­ç»ƒã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨`trn1.2xlarge`å®ä¾‹ä¸Šä½¿ç”¨`bert-base-uncased`æ¨¡å‹å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚

```py
torchrun --nproc_per_node=2 huggingface-neuron-samples/text-classification/run_glue.py \
--model_name_or_path bert-base-uncased \
--dataset_name philschmid/emotion \
--do_train \
--do_eval \
--bf16 True \
--per_device_train_batch_size 16 \
--learning_rate 5e-5 \
--num_train_epochs 3 \
--output_dir ./bert-emotion
```

## æ¨ç†

æ‚¨å¯ä»¥åœ¨Neuronè®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†ä¹‹å‰ï¼Œå°†ğŸ¤— Transformersæ¨¡å‹ç¼–è¯‘å¹¶å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼ï¼š

```py
optimum-cli export neuron 
  --model distilbert-base-uncased-finetuned-sst-2-english \
  --batch_size 1 \
  --sequence_length 32 \
  --auto_cast matmul \
  --auto_cast_type bf16 \
  distilbert_base_uncased_finetuned_sst2_english_neuron/
```

ä¸Šé¢çš„å‘½ä»¤å°†ä½¿ç”¨é™æ€å½¢çŠ¶å¯¼å‡º`distilbert-base-uncased-finetuned-sst-2-english`ï¼š`batch_size=1`å’Œ`sequence_length=32`ï¼Œå¹¶å°†æ‰€æœ‰`matmul`æ“ä½œä»FP32è½¬æ¢ä¸ºBF16ã€‚æŸ¥çœ‹[å¯¼å‡ºå™¨æŒ‡å—](https://huggingface.co/docs/optimum-neuron/guides/export_model#exporting-a-model-to-neuron-using-the-cli)ä»¥è·å–æ›´å¤šç¼–è¯‘é€‰é¡¹ã€‚

ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç±»ä¼¼äºğŸ¤— Transformersä¸­çš„`AutoModelForXXX`ç±»çš„`NeuronModelForXXX`ç±»åœ¨Neuronè®¾å¤‡ä¸Šè¿è¡Œå¯¼å‡ºçš„Neuronæ¨¡å‹ï¼š

```py
from transformers import AutoTokenizer
-from transformers import AutoModelForSequenceClassification
+from optimum.neuron import NeuronModelForSequenceClassification

# PyTorch checkpoint
-model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
+model = NeuronModelForSequenceClassification.from_pretrained("distilbert_base_uncased_finetuned_sst2_english_neuron")

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
inputs = tokenizer("Hamilton is considered to be the best musical of past years.", return_tensors="pt")

logits = model(**inputs).logits
print(model.config.id2label[logits.argmax().item()])
# 'POSITIVE'
```
