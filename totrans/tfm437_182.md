# GPT-Sw3

> 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt-sw3)

## 概述

GPT-Sw3 模型首次提出于 [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) 由 Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren 撰写。

自第一篇论文以来，作者已经扩展了他们的工作，并在他们的新 1.2TB 语料库上训练了新模型，名为 The Nordic Pile。

GPT-Sw3 是由 AI Sweden 与 RISE 和 WASP WARA for Media and Language 合作开发的一组大型仅解码器预训练的变压器语言模型。GPT-Sw3 在包含 320B 个瑞典语、挪威语、丹麦语、冰岛语、英语和编程代码的数据集上进行了训练。该模型使用了因果语言建模（CLM）目标进行预训练，利用了 NeMo Megatron GPT 实现。

此模型由 [AI Sweden Models](https://huggingface.co/AI-Sweden-Models) 贡献。

## 使用示例

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-356m")
>>> model = AutoModelForCausalLM.from_pretrained("AI-Sweden-Models/gpt-sw3-356m")

>>> input_ids = tokenizer("Träd är fina för att", return_tensors="pt")["input_ids"]

>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]

>>> print(tokenizer.decode(generated_token_ids))
Träd är fina för att de är färgstarka. Men ibland är det fint
```

## 资源

+   [文本分类任务指南](../tasks/sequence_classification)

+   [标记分类任务指南](../tasks/token_classification)

+   [因果语言建模任务指南](../tasks/language_modeling)

该实现使用 `GPT2Model` 与我们的 `GPTSw3Tokenizer` 结合。请参考 [GPT2Model 文档](gpt2) 获取 API 参考和示例。

请注意，使用我们的标记器需要 sentencepiece，并可通过 `pip install transformers[sentencepiece]` 或 `pip install sentencepiece` 进行安装。

## GPTSw3Tokenizer

### `class transformers.GPTSw3Tokenizer`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L45)

```py
( vocab_file do_lower_case = False remove_space = False keep_accents = False pad_token = None unk_token = None eos_token = None bos_token = None sp_model_kwargs: Optional = None **kwargs )
```

参数

+   `vocab_file` (`str`) — 包含实例化标记器所需词汇表的 [SentencePiece](https://github.com/google/sentencepiece) 文件（通常具有 *.spm* 扩展名）。

+   `do_lower_case` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否将输入转换为小写。

+   `remove_space` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否去除文本（删除字符串前后的多余空格）。

+   `keep_accents` (`bool`, *optional*, 默认为 `False`) — 在标记化时是否保留重音。

+   `pad_token` (`str`, *optional*) — 用于填充的标记，例如在批处理不同长度的序列时使用。如果未提供，默认为 ’<pad>’ 或 ’<unk>’，取决于模型大小。</unk></pad>

+   `unk_token` (`str`, *optional*) — 未知标记。词汇表中不存在的标记无法转换为 ID，而是设置为此标记。如果未提供，默认为 ’<unk>‘。</unk>

+   `eos_token`（`str`，*可选*）--在预训练过程中看到的序列结束标记。如果未提供，则默认为`<|endoftext|>`

+   `bos_token`（`str`，*optional*）--可用于下游任务的序列标记的开头，在预训练过程中没有看到。如果未提供，将默认为`~~`或`<|endoftext|>`，具体取决于模型大小

+   `sp_model_kwargs` (`dict`, *optional*) — 将传递给 `SentencePieceProcessor.__init__()` 方法的参数。[SentencePiece 的 Python 封装](https://github.com/google/sentencepiece/tree/master/python) 可以用来设置：

    +   `enable_sampling`: 启用子词规范化。

    +   `nbest_size`: unigram 的抽样参数。对于 BPE-Dropout 无效。

        +   `nbest_size = {0,1}`: 不执行抽样。

        +   `nbest_size > 1`: 从 nbest_size 结果中进行抽样。

        +   `nbest_size < 0`: 假设 nbest_size 为无限，并使用前向过滤和后向抽样算法从所有假设（格）中进行抽样。

    +   `alpha`: unigram 抽样的平滑参数，以及 BPE-Dropout 合并操作的丢弃概率。

+   `sp_model` (`SentencePieceProcessor`) — 用于每次转换（字符串、标记和 ID）的 *SentencePiece* 处理器。

+   `whitespaces` (`set`) — 在预处理中进行空格规范化时替换的空格。

+   `non_printing_characters_re` (`Pattern`) — 编译后的正则表达式，用于在预处理中删除非打印字符。

构建一个 GPTSw3 分词器。基于 [SentencePiece](https://github.com/google/sentencepiece)。

这个分词器继承自 [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大多数主要方法。用户应参考这个超类以获取有关这些方法的更多信息。

示例用法：

```py
>>> from transformers import GPTSw3Tokenizer

>>> tokenizer = GPTSw3Tokenizer.from_pretrained("AI-Sweden-Models/gpt-sw3-126m")
>>> tokenizer("Svenska är kul!")["input_ids"]
[1814, 377, 3617, 63504]
```

#### `save_vocabulary`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py#L258)

```py
( save_directory: str filename_prefix: Optional = None )
```
