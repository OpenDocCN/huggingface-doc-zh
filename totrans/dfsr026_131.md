# AudioLDM

> 原始文本：[https://huggingface.co/docs/diffusers/api/pipelines/audioldm](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)

Haohe Liu等人在[AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://huggingface.co/papers/2301.12503)中提出了AudioLDM。受到[Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview)的启发，AudioLDM是一个文本到音频的*潜在扩散模型（LDM）*，从[CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)潜在中学习连续音频表示。AudioLDM以文本提示作为输入，并预测相应的音频。它可以生成文本条件的音效、人类语音和音乐。

该论文的摘要是：

*最近，文本到音频（TTA）系统因其根据文本描述合成一般音频的能力而受到关注。然而，以往的TTA研究在高计算成本下限制了生成质量。在本研究中，我们提出了AudioLDM，这是一个建立在潜在空间上的TTA系统，从对比语言-音频预训练（CLAP）潜在中学习连续音频表示。预训练的CLAP模型使我们能够训练具有音频嵌入的LDM，并在采样过程中提供文本嵌入作为条件。通过学习音频信号及其组成的潜在表示，而不对跨模态关系进行建模，AudioLDM在生成质量和计算效率方面具有优势。在单个GPU上训练的AudioLDM在客观和主观指标（例如，frechet距离）方面实现了最先进的TTA性能。此外，AudioLDM是第一个以零样式进行各种文本引导音频操作（例如，风格转移）的TTA系统。我们的实现和演示可在[此https URL](https://audioldm.github.io/)上找到。*

原始代码库可在[haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM)找到。

## 提示

在构建提示时，请记住：

+   描述性提示输入效果最佳；您可以使用形容词描述声音（例如，“高质量”或“清晰”），并使提示具体化（例如，“森林中的水流”而不是“流水”）。

+   最好使用一般术语，如“猫”或“狗”，而不是特定名称或模型可能不熟悉的抽象对象。

推理过程中：

+   预测音频样本的*质量*可以通过`num_inference_steps`参数进行控制；更高的步骤会提供更高质量的音频，但推理速度会变慢。

+   预测音频样本的*长度*可以通过变化`audio_length_in_s`参数进行控制。

确保查看调度器[指南](../../using-diffusers/schedulers)以了解如何探索调度器速度和质量之间的权衡，并查看[跨管道重用组件](../../using-diffusers/loading#reuse-components-across-pipelines)部分，以了解如何有效地将相同组件加载到多个管道中。

## AudioLDMPipeline

### `class diffusers.AudioLDMPipeline`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L52)

```py
( vae: AutoencoderKL text_encoder: ClapTextModelWithProjection tokenizer: Union unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers vocoder: SpeechT5HifiGan )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码，并转换为潜在表示。

+   `text_encoder` ([ClapTextModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clap#transformers.ClapTextModelWithProjection)) — 冻结的文本编码器（`ClapTextModelWithProjection`，具体是[laion/clap-htsat-unfused](https://huggingface.co/laion/clap-htsat-unfused)变体。

+   `tokenizer` (`PreTrainedTokenizer`) — 用于对文本进行标记化的[RobertaTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/roberta#transformers.RobertaTokenizer)。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪编码音频潜变量的 `UNet2DConditionModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 用于与 `unet` 结合使用以去噪编码音频潜变量的调度程序。可以是[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler)或[PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler)之一。

+   `vocoder` ([SpeechT5HifiGan](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speecht5#transformers.SpeechT5HifiGan)) — 类 `SpeechT5HifiGan` 的声码器。

用于使用 AudioLDM 进行文本到音频生成的管道。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L367)

```py
( prompt: Union = None audio_length_in_s: Optional = None num_inference_steps: int = 10 guidance_scale: float = 2.5 negative_prompt: Union = None num_waveforms_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None return_dict: bool = True callback: Optional = None callback_steps: Optional = 1 cross_attention_kwargs: Optional = None output_type: Optional = 'np' ) → export const metadata = 'undefined';AudioPipelineOutput or tuple
```

参数

+   `prompt` (`str` or `List[str]`, *optional*) — 用于引导音频生成的提示或提示。如果未定义，则需要传递 `prompt_embeds`。

+   `audio_length_in_s` (`int`, *optional*, defaults to 5.12) — 生成音频样本的长度（秒）。

+   `num_inference_steps` (`int`, *optional*, defaults to 10) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的音频，但会降低推理速度。

+   `guidance_scale` (`float`, *optional*, defaults to 2.5) — 更高的引导比例值鼓励模型生成与文本 `prompt` 密切相关的音频，但会降低声音质量。当 `guidance_scale > 1` 时启用引导比例。

+   `negative_prompt` (`str` or `List[str]`, *optional*) — 用于指导音频生成中不包括什么的提示或提示。如果未定义，则需要传递 `negative_prompt_embeds`。在不使用引导时（`guidance_scale < 1`）会被忽略。

+   `num_waveforms_per_prompt` (`int`, *optional*, defaults to 1) — 每个提示生成的波形数量。

+   `eta` (`float`, *optional*, defaults to 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` or `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *optional*) — 从高斯分布中采样的预生成嘈杂潜变量，用作图像生成的输入。可用于使用不同提示调整相同生成。如果未提供，将使用提供的随机 `generator` 进行采样生成潜变量张量。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，文本嵌入将从 `prompt` 输入参数生成。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，`negative_prompt_embeds` 将从 `negative_prompt` 输入参数生成。

+   `return_dict` (`bool`, *optional*, defaults to `True`) — 是否返回[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)而不是普通元组。

+   `callback` (`Callable`, *可选*) — 一个在推断过程中每`callback_steps`步调用一次的函数。该函数接受以下参数：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为1) — 调用`callback`函数的频率。如果未指定，则在每个步骤中调用回调。

+   `cross_attention_kwargs` (`dict`, *可选*) — 一个kwargs字典，如果指定，则传递给`AttentionProcessor`，如[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的那样。

+   `output_type` (`str`, *可选*, 默认为`"np"`) — 生成图像的输出格式。选择在`"np"`之间返回一个NumPy `np.ndarray`或`"pt"`返回一个PyTorch `torch.Tensor`对象。

返回

[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput) 或 `tuple`

如果`return_dict`为`True`，则返回[AudioPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/audioldm#diffusers.AudioPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成的音频的列表。

用于生成的管道的调用函数。

示例：

```py
>>> from diffusers import AudioLDMPipeline
>>> import torch
>>> import scipy

>>> repo_id = "cvssp/audioldm-s-full-v2"
>>> pipe = AudioLDMPipeline.from_pretrained(repo_id, torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")

>>> prompt = "Techno music with a strong, upbeat tempo and high melodic riffs"
>>> audio = pipe(prompt, num_inference_steps=10, audio_length_in_s=5.0).audios[0]

>>> # save the audio sample as a .wav file
>>> scipy.io.wavfile.write("techno.wav", rate=16000, data=audio)
```

#### `disable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L108)

```py
( )
```

禁用切片VAE解码。如果之前启用了`enable_vae_slicing`，则此方法将返回到在一个步骤中计算解码。

`enable_vae_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/audioldm/pipeline_audioldm.py#L100)

```py
( )
```

启用切片VAE解码。当启用此选项时，VAE将分割输入张量以在几个步骤中计算解码。这对于节省一些内存并允许更大的批量大小非常有用。

## AudioPipelineOutput

### `class diffusers.AudioPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L130)

```py
( audios: ndarray )
```

参数

+   `audios` (`np.ndarray`) — 一个形状为`(batch_size, num_channels, sample_rate)`的NumPy数组的去噪音频样本列表。

音频管道的输出类。
