- en: How do Unity ML-Agents work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works](https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Before training our agent, we need to understand **what ML-Agents is and how
    it works**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: What is Unity ML-Agents?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents) is a toolkit
    for the game engine Unity that **allows us to create environments using Unity
    or use pre-made environments to train our agents**.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s developed by [Unity Technologies](https://unity.com/), the developers
    of Unity, one of the most famous Game Engines used by the creators of Firewatch,
    Cuphead, and Cities: Skylines.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![Firewatch](../Images/56b87aa0405b4b90defdc70caa6f9571.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
- en: Firewatch was made with Unity
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The six components
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Unity ML-Agents, you have six essential components:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![MLAgents](../Images/f1083a6f889f83b0d7ba8f82fe1c00da.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The first is the *Learning Environment*, which contains **the Unity scene (the
    environment) and the environment elements** (game characters).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is the *Python Low-level API*, which contains **the low-level Python
    interface for interacting and manipulating the environment**. It’s the API we
    use to launch the training.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we have the *External Communicator* that **connects the Learning Environment
    (made with C#) with the low level Python API (Python)**.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Python trainers*: the **Reinforcement algorithms made with PyTorch (PPO,
    SAC…)**.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Gym wrapper*: to encapsulate the RL environment in a gym wrapper.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *PettingZoo wrapper*: PettingZoo is the multi-agents version of the gym
    wrapper.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the Learning Component
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inside the Learning Component, we have **two important elements**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The first is the *agent component*, the actor of the scene. We’ll **train the
    agent by optimizing its policy** (which will tell us what action to take in each
    state). The policy is called the *Brain*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there is the *Academy*. This component **orchestrates agents and their
    decision-making processes**. Think of this Academy as a teacher who handles Python
    API requests.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To better understand its role, let’s remember the RL process. This can be modeled
    as a loop that works like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'The RL Process: a loop of state, action, reward and next state'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [Reinforcement Learning: An Introduction, Richard Sutton and Andrew
    G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s imagine an agent learning to play a platform game. The RL process
    looks like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Our Agent receives **state <math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​** from the **Environment** —
    we receive the first frame of our game (Environment).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on that **state<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​,** the Agent
    takes **action<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0​** — our Agent
    will move to the right.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment goes to a **new** **state<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1​** — new frame.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives some **reward<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1​** to the Agent
    — we’re not dead *(Positive Reward +1)*.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This RL loop outputs a sequence of **state, action, reward and next state.**
    The goal of the agent is to **maximize the expected cumulative reward**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The Academy will be the one that will **send the order to our Agents and ensure
    that agents are in sync**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 学院将会**向我们的智能体发送指令，并确保智能体同步**：
- en: Collect Observations
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集观察数据
- en: Select your action using your policy
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你的策略选择动作
- en: Take the Action
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行动作
- en: Reset if you reached the max step or if you’re done.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你达到了最大步数或者已经完成了，就重置。
- en: '![The MLAgents Academy](../Images/ce5e350e3e33e7b48aee5a99e9c2666d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![MLAgents学院](../Images/ce5e350e3e33e7b48aee5a99e9c2666d.png)'
- en: Now that we understand how ML-Agents works, **we’re ready to train our agents.**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了ML-Agents的工作原理，**我们准备好训练我们的智能体了。**
