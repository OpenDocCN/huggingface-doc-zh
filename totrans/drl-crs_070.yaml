- en: How do Unity ML-Agents work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works](https://huggingface.co/learn/deep-rl-course/unit5/how-mlagents-works)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Before training our agent, we need to understand **what ML-Agents is and how
    it works**.
  prefs: []
  type: TYPE_NORMAL
- en: What is Unity ML-Agents?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents) is a toolkit
    for the game engine Unity that **allows us to create environments using Unity
    or use pre-made environments to train our agents**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s developed by [Unity Technologies](https://unity.com/), the developers
    of Unity, one of the most famous Game Engines used by the creators of Firewatch,
    Cuphead, and Cities: Skylines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Firewatch](../Images/56b87aa0405b4b90defdc70caa6f9571.png)'
  prefs: []
  type: TYPE_IMG
- en: Firewatch was made with Unity
  prefs: []
  type: TYPE_NORMAL
- en: The six components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Unity ML-Agents, you have six essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MLAgents](../Images/f1083a6f889f83b0d7ba8f82fe1c00da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Unity ML-Agents Documentation](https://unity-technologies.github.io/ml-agents/)'
  prefs: []
  type: TYPE_NORMAL
- en: The first is the *Learning Environment*, which contains **the Unity scene (the
    environment) and the environment elements** (game characters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second is the *Python Low-level API*, which contains **the low-level Python
    interface for interacting and manipulating the environment**. It’s the API we
    use to launch the training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we have the *External Communicator* that **connects the Learning Environment
    (made with C#) with the low level Python API (Python)**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Python trainers*: the **Reinforcement algorithms made with PyTorch (PPO,
    SAC…)**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Gym wrapper*: to encapsulate the RL environment in a gym wrapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *PettingZoo wrapper*: PettingZoo is the multi-agents version of the gym
    wrapper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the Learning Component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inside the Learning Component, we have **two important elements**:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is the *agent component*, the actor of the scene. We’ll **train the
    agent by optimizing its policy** (which will tell us what action to take in each
    state). The policy is called the *Brain*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there is the *Academy*. This component **orchestrates agents and their
    decision-making processes**. Think of this Academy as a teacher who handles Python
    API requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To better understand its role, let’s remember the RL process. This can be modeled
    as a loop that works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RL Process: a loop of state, action, reward and next state'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [Reinforcement Learning: An Introduction, Richard Sutton and Andrew
    G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s imagine an agent learning to play a platform game. The RL process
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  prefs: []
  type: TYPE_IMG
- en: Our Agent receives **state <math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​** from the **Environment** —
    we receive the first frame of our game (Environment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on that **state<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​,** the Agent
    takes **action<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0​** — our Agent
    will move to the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment goes to a **new** **state<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1​** — new frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives some **reward<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1​** to the Agent
    — we’re not dead *(Positive Reward +1)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This RL loop outputs a sequence of **state, action, reward and next state.**
    The goal of the agent is to **maximize the expected cumulative reward**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Academy will be the one that will **send the order to our Agents and ensure
    that agents are in sync**:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect Observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select your action using your policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the Action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reset if you reached the max step or if you’re done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The MLAgents Academy](../Images/ce5e350e3e33e7b48aee5a99e9c2666d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand how ML-Agents works, **we’re ready to train our agents.**
  prefs: []
  type: TYPE_NORMAL
