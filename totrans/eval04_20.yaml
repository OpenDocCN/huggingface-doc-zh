- en: Main classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/evaluate/package_reference/main_classes](https://huggingface.co/docs/evaluate/package_reference/main_classes)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: EvaluationModuleInfo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class `EvaluationModuleInfo` implements a the logic for the subclasses
    `MetricInfo`, `ComparisonInfo`, and `MeasurementInfo`.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.EvaluationModuleInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L35)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Base class to store fnformation about an evaluation used for `MetricInfo`, `ComparisonInfo`,
    and `MeasurementInfo`.
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo` documents an evaluation, including its name, version,
    and features. See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L82)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Create EvaluationModuleInfo from the JSON file in `metric_info_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `write_to_directory`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L72)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Write `EvaluationModuleInfo` as JSON to `metric_info_dir`. Also save the license
    separately in LICENCE.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.MetricInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L105)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Information about a metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo` documents a metric, including its name, version, and
    features. See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.ComparisonInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Information about a comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo` documents a comparison, including its name, version,
    and features. See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.MeasurementInfo`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/info.py#L131)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Information about a measurement.
  prefs: []
  type: TYPE_NORMAL
- en: '`EvaluationModuleInfo` documents a measurement, including its name, version,
    and features. See the constructor arguments and properties for a full list.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Not all fields are known on construction and may be updated later.'
  prefs: []
  type: TYPE_NORMAL
- en: EvaluationModule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The base class `EvaluationModule` implements a the logic for the subclasses
    `Metric`, `Comparison`, and `Measurement`.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.EvaluationModule`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L145)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a module
    computation script and prevents the module’s data to be overridden when the module
    loading script is modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute module in distributed setups (in particular
    non-additive modules like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute module in distributed
    setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [evaluate.EvaluationModule.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)
    is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    module in distributed setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hash` (`str`) — Used to identify the evaluation module according to the hashed
    file contents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent module cache
    files (default 10000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A EvaluationModule is the base class and common API for metrics, comparisons,
    and measurements.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L514)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reference` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add one prediction and reference for the evaluation module’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L465)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a batch of predictions and references for the evaluation module’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L401)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (optional) — Keyword arguments that will be forwarded to the evaluation
    module `_compute` method (see details in the docstring).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the evaluation module.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of positional arguments is not allowed to prevent mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `download_and_prepare`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L633)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`download_config` (`DownloadConfig`, optional) — Specific download configuration
    parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dl_manager` (`DownloadManager`, optional) — Specific download manager to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads and prepares dataset for reading.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.Metric`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L730)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a metric
    computation script and prevents the metric’s data to be overridden when the metric
    loading script is modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute metrics in distributed setups (in particular
    non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute metrics in distributed
    setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [evaluate.Metric.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)
    is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    metrics in distributed setups (in particular non-additive metrics like F1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent metric cache
    files (default 10000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Metric is the base class and common API for all metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.Comparison`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L751)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a comparison
    computation script and prevents the comparison’s data to be overridden when the
    comparison loading script is modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute comparisons in distributed setups (in particular
    non-additive comparisons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute comparisons in distributed
    setups (in particular non-additive comparisons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [evaluate.Comparison.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)
    is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    comparisons in distributed setups (in particular non-additive comparisons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent comparison
    cache files (default 10000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Comparison is the base class and common API for all comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: '### `class evaluate.Measurement`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L772)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config_name` (`str`) — This is used to define a hash specific to a measurement
    computation script and prevents the measurement’s data to be overridden when the
    measurement loading script is modified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_in_memory` (`bool`) — keep all predictions and references in memory.
    Not possible in distributed settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cache_dir` (`str`) — Path to a directory in which temporary prediction/references
    data will be stored. The data directory should be located on a shared file-system
    in distributed setups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_process` (`int`) — specify the total number of nodes in a distributed
    settings. This is useful to compute measurements in distributed setups (in particular
    non-additive measurements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process_id` (`int`) — specify the id of the current process in a distributed
    setup (between 0 and num_process-1) This is useful to compute measurements in
    distributed setups (in particular non-additive measurements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` (`int`, optional) — If specified, this will temporarily set numpy’s
    random seed when [evaluate.Measurement.compute()](/docs/evaluate/v0.4.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute)
    is run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`experiment_id` (`str`) — A specific experiment id. This is used if several
    distributed evaluations share the same file system. This is useful to compute
    measurements in distributed setups (in particular non-additive measurements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_concurrent_cache_files` (`int`) — Max number of concurrent measurement
    cache files (default 10000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeout` (`Union[int, float]`) — Timeout in second for distributed setting
    synchronization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Measurement is the base class and common API for all measurements.
  prefs: []
  type: TYPE_NORMAL
- en: CombinedEvaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `combine` function allows to combine multiple `EvaluationModule`s into a
    single `CombinedEvaluations`.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `evaluate.combine`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L891)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`evaluations` (`Union[list, dict]`) — A list or dictionary of evaluation modules.
    The modules can either be passed as strings or loaded *EvaluationModule*s. If
    a dictionary is passed its keys are the names used and the values the modules.
    The names are used as prefix in case there are name overlaps in the returned results
    of each module or if *force_prefix=True*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`force_prefix` (`bool`, optional, defaults to *False*) — If *True* all scores
    from the modules are prefixed with their name. If a dictionary is passed the keys
    are used as name otherwise the module’s name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combines several metrics, comparisons, or measurements into a single *CombinedEvaluations*
    object that can be used like a single evaluation module.
  prefs: []
  type: TYPE_NORMAL
- en: If two scores have the same name, then they are prefixed with their module names.
    And if two modules have the same name, please use a dictionary to give them different
    names, otherwise an integer id is appended to the prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'clf_metrics = combine([“accuracy”, “f1”, “precision”,“recall”]) clf_metrics.compute(predictions=[0,1],
    references=[1,1]) {‘accuracy’: 0.5, ‘f1’: 0.66, ‘precision’: 1.0, ‘recall’: 0.5}'
  prefs:
  - PREF_BQ
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: '### `class evaluate.CombinedEvaluations`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L793)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#### `add`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L816)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reference` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add one prediction and reference for each evaluation module’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `add_batch`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L828)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a batch of predictions and references for each evaluation module’s stack.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `compute`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/evaluate/blob/v0.4.0/src/evaluate/module.py#L840)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`predictions` (list/array/tensor, optional) — Predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`references` (list/array/tensor, optional) — References.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*`*kwargs` (optional) — Keyword arguments that will be forwarded to the evaluation
    module `_compute` method (see details in the docstring).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute each evaluation module.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of positional arguments is not allowed to prevent mistakes.
  prefs: []
  type: TYPE_NORMAL
