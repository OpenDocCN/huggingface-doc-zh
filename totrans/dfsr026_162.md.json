["```py\n>>> import requests\n>>> import torch\n>>> from PIL import Image\n>>> from io import BytesIO\n\n>>> from diffusers import StableDiffusionImg2ImgPipeline\n\n>>> device = \"cuda\"\n>>> model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n>>> pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n>>> pipe = pipe.to(device)\n\n>>> url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\n>>> response = requests.get(url)\n>>> init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n>>> init_image = init_image.resize((768, 512))\n\n>>> prompt = \"A fantasy landscape, trending on artstation\"\n\n>>> images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n>>> images[0].save(\"fantasy_landscape.png\")\n```", "```py\n>>> import torch\n>>> from diffusers import StableDiffusionPipeline\n\n>>> pipe = StableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\",\n...     torch_dtype=torch.float16,\n...     use_safetensors=True,\n... )\n\n>>> prompt = \"a photo of an astronaut riding a horse on mars\"\n>>> pipe.enable_attention_slicing()\n>>> image = pipe(prompt).images[0]\n```", "```py\n>>> import torch\n>>> from diffusers import DiffusionPipeline\n>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n\n>>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n>>> pipe = pipe.to(\"cuda\")\n>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n>>> # Workaround for not accepting attention shape using VAE for Flash Attention\n>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n\nprompt = \"A <cat-toy> backpack\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"cat-backpack.png\")\n```", "```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n\npipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n\nprompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n\nimage = pipe(prompt, num_inference_steps=50).images[0]\nimage.save(\"character.png\")\n```", "```py\n>>> from diffusers import StableDiffusionPipeline\n\n>>> # Download pipeline from huggingface.co and cache.\n>>> pipeline = StableDiffusionPipeline.from_single_file(\n...     \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n... )\n\n>>> # Download pipeline from local file\n>>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt\n>>> pipeline = StableDiffusionPipeline.from_single_file(\"./v1-5-pruned-emaonly\")\n\n>>> # Enable float16 and move to GPU\n>>> pipeline = StableDiffusionPipeline.from_single_file(\n...     \"https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt\",\n...     torch_dtype=torch.float16,\n... )\n>>> pipeline.to(\"cuda\")\n```", "```py\n>>> import jax\n>>> import numpy as np\n>>> import jax.numpy as jnp\n>>> from flax.jax_utils import replicate\n>>> from flax.training.common_utils import shard\n>>> import requests\n>>> from io import BytesIO\n>>> from PIL import Image\n>>> from diffusers import FlaxStableDiffusionImg2ImgPipeline\n\n>>> def create_key(seed=0):\n...     return jax.random.PRNGKey(seed)\n\n>>> rng = create_key(0)\n\n>>> url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n>>> response = requests.get(url)\n>>> init_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n>>> init_img = init_img.resize((768, 512))\n\n>>> prompts = \"A fantasy landscape, trending on artstation\"\n\n>>> pipeline, params = FlaxStableDiffusionImg2ImgPipeline.from_pretrained(\n...     \"CompVis/stable-diffusion-v1-4\",\n...     revision=\"flax\",\n...     dtype=jnp.bfloat16,\n... )\n\n>>> num_samples = jax.device_count()\n>>> rng = jax.random.split(rng, jax.device_count())\n>>> prompt_ids, processed_image = pipeline.prepare_inputs(\n...     prompt=[prompts] * num_samples, image=[init_img] * num_samples\n... )\n>>> p_params = replicate(params)\n>>> prompt_ids = shard(prompt_ids)\n>>> processed_image = shard(processed_image)\n\n>>> output = pipeline(\n...     prompt_ids=prompt_ids,\n...     image=processed_image,\n...     params=p_params,\n...     prng_seed=rng,\n...     strength=0.75,\n...     num_inference_steps=50,\n...     jit=True,\n...     height=512,\n...     width=768,\n... ).images\n\n>>> output_images = pipeline.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))\n```"]