- en: The Reinforcement Learning Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/rl-framework](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: The RL Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![The RL process](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RL Process: a loop of state, action, reward and next state'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [Reinforcement Learning: An Introduction, Richard Sutton and Andrew
    G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the RL process, let’s imagine an agent learning to play a platform
    game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  prefs: []
  type: TYPE_IMG
- en: Our Agent receives **state <math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​** from the **Environment** —
    we receive the first frame of our game (Environment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on that **state<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​,** the Agent
    takes **action<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0​** — our Agent
    will move to the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment goes to a **new** **state<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1​** — new frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment gives some **reward<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1​** to the Agent
    — we’re not dead *(Positive Reward +1)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This RL loop outputs a sequence of **state, action, reward and next state.**
  prefs: []
  type: TYPE_NORMAL
- en: '![State, Action, Reward, Next State](../Images/1f2f9ef9ca66384a7f30cb01df0cc998.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent’s goal is to *maximize* its cumulative reward, **called the expected
    return.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The reward hypothesis: the central idea of Reinforcement Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ⇒ Why is the goal of the agent to maximize the expected return?
  prefs: []
  type: TYPE_NORMAL
- en: Because RL is based on the **reward hypothesis**, which is that all goals can
    be described as the **maximization of the expected return** (expected cumulative
    reward).
  prefs: []
  type: TYPE_NORMAL
- en: That’s why in Reinforcement Learning, **to have the best behavior,** we aim
    to learn to take actions that **maximize the expected cumulative reward.**
  prefs: []
  type: TYPE_NORMAL
- en: Markov Property
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In papers, you’ll see that the RL process is called a **Markov Decision Process** (MDP).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll talk again about the Markov Property in the following units. But if you
    need to remember something today about it, it’s this: the Markov Property implies
    that our agent needs **only the current state to decide** what action to take
    and **not the history of all the states and actions** they took before.'
  prefs: []
  type: TYPE_NORMAL
- en: Observations/States Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Observations/States are the **information our agent gets from the environment.** In
    the case of a video game, it can be a frame (a screenshot). In the case of the
    trading agent, it can be the value of a certain stock, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a differentiation to make between *observation* and *state*, however:'
  prefs: []
  type: TYPE_NORMAL
- en: '*State s*: is **a complete description of the state of the world** (there is
    no hidden information). In a fully observed environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Chess](../Images/4abc718a19af159c3fcd1e3d1eb9daf8.png)'
  prefs: []
  type: TYPE_IMG
- en: In chess game, we receive a state from the environment since we have access
    to the whole check board information.
  prefs: []
  type: TYPE_NORMAL
- en: In a chess game, we have access to the whole board information, so we receive
    a state from the environment. In other words, the environment is fully observed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Observation o*: is a **partial description of the state.** In a partially
    observed environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mario](../Images/f7007613a2e88444f687ee5cdbd82e16.png)'
  prefs: []
  type: TYPE_IMG
- en: In Super Mario Bros, we only see the part of the level close to the player,
    so we receive an observation.
  prefs: []
  type: TYPE_NORMAL
- en: In Super Mario Bros, we only see the part of the level close to the player,
    so we receive an observation.
  prefs: []
  type: TYPE_NORMAL
- en: In Super Mario Bros, we are in a partially observed environment. We receive
    an observation **since we only see a part of the level.**
  prefs: []
  type: TYPE_NORMAL
- en: In this course, we use the term "state" to denote both state and observation,
    but we will make the distinction in implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obs space recap](../Images/e7cafbe776324bbcf985e2d4ff4a87f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Action Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Action space is the set of **all possible actions in an environment.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The actions can come from a *discrete* or *continuous space*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Discrete space*: the number of possible actions is **finite**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mario](../Images/f7007613a2e88444f687ee5cdbd82e16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Super Mario Bros, we have only 4 possible actions: left, right, up (jumping)
    and down (crouching).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, in Super Mario Bros, we have a finite set of actions since we have only
    4 directions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Continuous space*: the number of possible actions is **infinite**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Self Driving Car](../Images/97ce48e0905f0673081c9ed15db623c3.png)'
  prefs: []
  type: TYPE_IMG
- en: A Self Driving Car agent has an infinite number of possible actions since it
    can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Action space recap](../Images/bbdf7bd1acf438cc4f239937493ab96a.png)'
  prefs: []
  type: TYPE_IMG
- en: Taking this information into consideration is crucial because it will **have
    importance when choosing the RL algorithm in the future.**
  prefs: []
  type: TYPE_NORMAL
- en: Rewards and the discounting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reward is fundamental in RL because it’s **the only feedback** for the agent.
    Thanks to it, our agent knows **if the action taken was good or not.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The cumulative reward at each time step **t** can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/d78f5d51ac167cb76cb42cfa4b19d3ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The cumulative reward equals the sum of all rewards in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Which is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/ebfd3c4b30393947bc58c81d77d53858.png)'
  prefs: []
  type: TYPE_IMG
- en: The cumulative reward = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1
    = rt+2) + ...
  prefs: []
  type: TYPE_NORMAL
- en: However, in reality, **we can’t just add them like that.** The rewards that
    come sooner (at the beginning of the game) **are more likely to happen** since
    they are more predictable than the long-term future reward.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say your agent is this tiny mouse that can move one tile each time step,
    and your opponent is the cat (that can move too). The mouse’s goal is **to eat
    the maximum amount of cheese before being eaten by the cat.**
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/b3b52f9a5548289b29a0f8b1ec21468c.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the diagram, **it’s more probable to eat the cheese near us
    than the cheese close to the cat** (the closer we are to the cat, the more dangerous
    it is).
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, **the reward near the cat, even if it is bigger (more cheese),
    will be more discounted** since we’re not really sure we’ll be able to eat it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To discount the rewards, we proceed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: We define a discount rate called gamma. **It must be between 0 and 1.** Most
    of the time between **0.95 and 0.99**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger the gamma, the smaller the discount. This means our agent **cares
    more about the long-term reward.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the smaller the gamma, the bigger the discount. This means
    our **agent cares more about the short term reward (the nearest cheese).**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2. Then, each reward will be discounted by gamma to the exponent of the time
    step. As the time step increases, the cat gets closer to us, **so the future reward
    is less and less likely to happen.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Our discounted expected cumulative reward is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/95b51ffb9bff4a5e5be3a023ab4ec8f4.png)'
  prefs: []
  type: TYPE_IMG
