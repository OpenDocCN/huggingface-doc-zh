- en: The Reinforcement Learning Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习框架
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unit1/rl-framework](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/learn/deep-rl-course/unit1/rl-framework](https://huggingface.co/learn/deep-rl-course/unit1/rl-framework)'
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The RL Process
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RL过程
- en: '![The RL process](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![RL过程](../Images/018079078cf4ad9c782cc74fc0ce7a20.png)'
- en: 'The RL Process: a loop of state, action, reward and next state'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: RL过程：一个状态、动作、奖励和下一个状态的循环
- en: 'Source: [Reinforcement Learning: An Introduction, Richard Sutton and Andrew
    G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[强化学习：一种介绍，Richard Sutton和Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)
- en: 'To understand the RL process, let’s imagine an agent learning to play a platform
    game:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解RL过程，让我们想象一个学习玩平台游戏的Agent：
- en: '![The RL process](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![RL过程](../Images/79d6e90ecca40e7412a5ae37c07bf478.png)'
- en: Our Agent receives **state <math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​** from the **Environment** —
    we receive the first frame of our game (Environment).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的Agent从**环境**接收**状态<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0** — 我们接收游戏的第一帧（环境）。
- en: Based on that **state<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0​,** the Agent
    takes **action<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0​** — our Agent
    will move to the right.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于那个**状态<math><semantics><mrow><msub><mi>S</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">S_0</annotation></semantics></math>S0**，Agent采取**动作<math><semantics><mrow><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation
    encoding="application/x-tex">A_0</annotation></semantics></math>A0** — 我们的Agent将向右移动。
- en: The environment goes to a **new** **state<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1​** — new frame.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境进入一个**新的**状态<math><semantics><mrow><msub><mi>S</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">S_1</annotation></semantics></math>S1** — 新的帧。
- en: The environment gives some **reward<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1​** to the Agent
    — we’re not dead *(Positive Reward +1)*.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境给Agent一些**奖励<math><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation
    encoding="application/x-tex">R_1</annotation></semantics></math>R1** — 我们还活着（正面奖励+1）。
- en: This RL loop outputs a sequence of **state, action, reward and next state.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RL循环输出一系列**状态、动作、奖励和下一个状态。**
- en: '![State, Action, Reward, Next State](../Images/1f2f9ef9ca66384a7f30cb01df0cc998.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![状态、动作、奖励、下一个状态](../Images/1f2f9ef9ca66384a7f30cb01df0cc998.png)'
- en: The agent’s goal is to *maximize* its cumulative reward, **called the expected
    return.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Agent的目标是*最大化*其累积奖励，**称为预期回报。**
- en: 'The reward hypothesis: the central idea of Reinforcement Learning'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励假设：强化学习的核心思想
- en: ⇒ Why is the goal of the agent to maximize the expected return?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ⇒ 为什么Agent的目标是最大化预期回报？
- en: Because RL is based on the **reward hypothesis**, which is that all goals can
    be described as the **maximization of the expected return** (expected cumulative
    reward).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因为RL基于**奖励假设**，即所有目标都可以描述为**最大化预期回报**（预期累积奖励）。
- en: That’s why in Reinforcement Learning, **to have the best behavior,** we aim
    to learn to take actions that **maximize the expected cumulative reward.**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在强化学习中，**为了有最佳行为**，我们的目标是学会采取能够**最大化预期累积奖励**的行动。
- en: Markov Property
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫性质
- en: In papers, you’ll see that the RL process is called a **Markov Decision Process** (MDP).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，你会看到RL过程被称为**马尔可夫决策过程**（MDP）。
- en: 'We’ll talk again about the Markov Property in the following units. But if you
    need to remember something today about it, it’s this: the Markov Property implies
    that our agent needs **only the current state to decide** what action to take
    and **not the history of all the states and actions** they took before.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的单元再次讨论马尔可夫性质。但是如果你今天需要记住一些关于它的东西，那就是：马尔可夫性质意味着我们的Agent只需要**当前状态来决定**采取什么行动，而**不需要所有之前状态和行动的历史**。
- en: Observations/States Space
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察/状态空间
- en: Observations/States are the **information our agent gets from the environment.** In
    the case of a video game, it can be a frame (a screenshot). In the case of the
    trading agent, it can be the value of a certain stock, etc.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 观察/状态是**Agent从环境中获取的信息**。在视频游戏中，它可以是一帧（一张截图）。在交易Agent的情况下，它可以是某只股票的价值等。
- en: 'There is a differentiation to make between *observation* and *state*, however:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要区分*观察*和*状态*：
- en: '*State s*: is **a complete description of the state of the world** (there is
    no hidden information). In a fully observed environment.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*状态s*：是**世界状态的完整描述**（没有隐藏信息）。在完全观察的环境中。'
- en: '![Chess](../Images/4abc718a19af159c3fcd1e3d1eb9daf8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![国际象棋](../Images/4abc718a19af159c3fcd1e3d1eb9daf8.png)'
- en: In chess game, we receive a state from the environment since we have access
    to the whole check board information.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际象棋游戏中，我们从环境中接收一个状态，因为我们可以访问整个棋盘信息。
- en: In a chess game, we have access to the whole board information, so we receive
    a state from the environment. In other words, the environment is fully observed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在国际象棋游戏中，我们可以访问整个棋盘信息，因此我们从环境中接收一个状态。换句话说，环境是完全观察的。
- en: '*Observation o*: is a **partial description of the state.** In a partially
    observed environment.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*观察o*：是**状态的部分描述**。在部分观察的环境中。'
- en: '![Mario](../Images/f7007613a2e88444f687ee5cdbd82e16.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![Mario](../Images/f7007613a2e88444f687ee5cdbd82e16.png)'
- en: In Super Mario Bros, we only see the part of the level close to the player,
    so we receive an observation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在超级马里奥兄弟中，我们只能看到靠近玩家的部分关卡，因此我们接收一个观察。
- en: In Super Mario Bros, we only see the part of the level close to the player,
    so we receive an observation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在超级马里奥兄弟中，我们只能看到靠近玩家的部分关卡，因此我们接收一个观察。
- en: In Super Mario Bros, we are in a partially observed environment. We receive
    an observation **since we only see a part of the level.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In this course, we use the term "state" to denote both state and observation,
    but we will make the distinction in implementations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![Obs space recap](../Images/e7cafbe776324bbcf985e2d4ff4a87f7.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Action Space
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Action space is the set of **all possible actions in an environment.**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'The actions can come from a *discrete* or *continuous space*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*Discrete space*: the number of possible actions is **finite**.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Mario](../Images/f7007613a2e88444f687ee5cdbd82e16.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'In Super Mario Bros, we have only 4 possible actions: left, right, up (jumping)
    and down (crouching).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Again, in Super Mario Bros, we have a finite set of actions since we have only
    4 directions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Continuous space*: the number of possible actions is **infinite**.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Self Driving Car](../Images/97ce48e0905f0673081c9ed15db623c3.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: A Self Driving Car agent has an infinite number of possible actions since it
    can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![Action space recap](../Images/bbdf7bd1acf438cc4f239937493ab96a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Taking this information into consideration is crucial because it will **have
    importance when choosing the RL algorithm in the future.**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Rewards and the discounting
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reward is fundamental in RL because it’s **the only feedback** for the agent.
    Thanks to it, our agent knows **if the action taken was good or not.**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'The cumulative reward at each time step **t** can be written as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/d78f5d51ac167cb76cb42cfa4b19d3ba.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: The cumulative reward equals the sum of all rewards in the sequence.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Which is equivalent to:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/ebfd3c4b30393947bc58c81d77d53858.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: The cumulative reward = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1
    = rt+2) + ...
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: However, in reality, **we can’t just add them like that.** The rewards that
    come sooner (at the beginning of the game) **are more likely to happen** since
    they are more predictable than the long-term future reward.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say your agent is this tiny mouse that can move one tile each time step,
    and your opponent is the cat (that can move too). The mouse’s goal is **to eat
    the maximum amount of cheese before being eaten by the cat.**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/b3b52f9a5548289b29a0f8b1ec21468c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: As we can see in the diagram, **it’s more probable to eat the cheese near us
    than the cheese close to the cat** (the closer we are to the cat, the more dangerous
    it is).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, **the reward near the cat, even if it is bigger (more cheese),
    will be more discounted** since we’re not really sure we’ll be able to eat it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'To discount the rewards, we proceed like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: We define a discount rate called gamma. **It must be between 0 and 1.** Most
    of the time between **0.95 and 0.99**.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The larger the gamma, the smaller the discount. This means our agent **cares
    more about the long-term reward.**
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the smaller the gamma, the bigger the discount. This means
    our **agent cares more about the short term reward (the nearest cheese).**
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2. Then, each reward will be discounted by gamma to the exponent of the time
    step. As the time step increases, the cat gets closer to us, **so the future reward
    is less and less likely to happen.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Our discounted expected cumulative reward is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![Rewards](../Images/95b51ffb9bff4a5e5be3a023ab4ec8f4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
