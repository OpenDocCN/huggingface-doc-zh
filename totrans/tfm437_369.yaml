- en: TAPAS
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TAPAS
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The TAPAS model was proposed in [TAPAS: Weakly Supervised Table Parsing via
    Pre-training](https://www.aclweb.org/anthology/2020.acl-main.398) by Jonathan
    Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin
    Eisenschlos. It’s a BERT-based model specifically designed (and pre-trained) for
    answering questions about tabular data. Compared to BERT, TAPAS uses relative
    position embeddings and has 7 token types that encode tabular structure. TAPAS
    is pre-trained on the masked language modeling (MLM) objective on a large dataset
    comprising millions of tables from English Wikipedia and corresponding texts.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: TAPAS模型是由Jonathan Herzig、Paweł Krzysztof Nowak、Thomas Müller、Francesco Piccinno和Julian
    Martin Eisenschlos在[通过预训练实现弱监督表格解析的TAPAS](https://www.aclweb.org/anthology/2020.acl-main.398)中提出的。这是一种基于BERT的模型，专门设计（和预训练）用于回答关于表格数据的问题。与BERT相比，TAPAS使用相对位置嵌入，并具有7种编码表格结构的标记类型。TAPAS在大型数据集上进行了掩码语言建模（MLM）目标的预训练，该数据集包含来自英文维基百科和相应文本的数百万个表格。
- en: 'For question answering, TAPAS has 2 heads on top: a cell selection head and
    an aggregation head, for (optionally) performing aggregations (such as counting
    or summing) among selected cells. TAPAS has been fine-tuned on several datasets:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问题回答，TAPAS在顶部有2个头：一个单元选择头和一个聚合头，用于（可选地）在选定的单元之间执行聚合（例如计数或求和）。TAPAS已在多个数据集上进行了微调：
- en: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential
    Question Answering by Microsoft)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253)（由微软提供的顺序问答）'
- en: '[WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions
    by Stanford University)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WTQ](https://github.com/ppasupat/WikiTableQuestions)（由斯坦福大学提供的维基表问题）'
- en: '[WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce).'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[WikiSQL](https://github.com/salesforce/WikiSQL)（由Salesforce提供）'
- en: It achieves state-of-the-art on both SQA and WTQ, while having comparable performance
    to SOTA on WikiSQL, with a much simpler architecture.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它在SQA和WTQ上取得了最新的成果，同时在WikiSQL上的表现与SOTA相当，但结构更简单。
- en: 'The abstract from the paper is the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*Answering natural language questions over tables is usually seen as a semantic
    parsing task. To alleviate the collection cost of full logical forms, one popular
    approach focuses on weak supervision consisting of denotations instead of logical
    forms. However, training semantic parsers from weak supervision poses difficulties,
    and in addition, the generated logical forms are only used as an intermediate
    step prior to retrieving the denotation. In this paper, we present TAPAS, an approach
    to question answering over tables without generating logical forms. TAPAS trains
    from weak supervision, and predicts the denotation by selecting table cells and
    optionally applying a corresponding aggregation operator to such selection. TAPAS
    extends BERT’s architecture to encode tables as input, initializes from an effective
    joint pre-training of text segments and tables crawled from Wikipedia, and is
    trained end-to-end. We experiment with three different semantic parsing datasets,
    and find that TAPAS outperforms or rivals semantic parsing models by improving
    state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with
    the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture.
    We additionally find that transfer learning, which is trivial in our setting,
    from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*对表格进行自然语言问题回答通常被视为语义解析任务。为了减轻完整逻辑形式的收集成本，一种流行的方法是专注于弱监督，包括指示而不是逻辑形式。然而，从弱监督训练语义解析器存在困难，并且生成的逻辑形式仅用作检索指示之前的中间步骤。在本文中，我们提出了TAPAS，一种在表格上回答问题而无需生成逻辑形式的方法。TAPAS从弱监督中训练，并通过选择表格单元并可选地应用相应的聚合运算符来预测指示。TAPAS扩展了BERT的架构以对表格进行编码，从维基百科爬取的文本段和表格的有效联合预训练进行初始化，并进行端到端训练。我们在三个不同的语义解析数据集上进行实验，发现TAPAS在SQA上的最新准确率从55.1提高到67.2，与WIKISQL和WIKITQ上的最新技术相媲美，但模型结构更简单。我们还发现，在我们的设置中，从WIKISQL到WIKITQ的转移学习，可以获得48.7的准确率，比最新技术高出4.2个百分点。*'
- en: 'In addition, the authors have further pre-trained TAPAS to recognize **table
    entailment**, by creating a balanced dataset of millions of automatically created
    training examples which are learned in an intermediate step prior to fine-tuning.
    The authors of TAPAS call this further pre-training intermediate pre-training
    (since TAPAS is first pre-trained on MLM, and then on another dataset). They found
    that intermediate pre-training further improves performance on SQA, achieving
    a new state-of-the-art as well as state-of-the-art on [TabFact](https://github.com/wenhuchen/Table-Fact-Checking),
    a large-scale dataset with 16k Wikipedia tables for table entailment (a binary
    classification task). For more details, see their follow-up paper: [Understanding
    tables with intermediate pre-training](https://www.aclweb.org/anthology/2020.findings-emnlp.27/)
    by Julian Martin Eisenschlos, Syrine Krichene and Thomas Müller.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者进一步对TAPAS进行了预训练，以识别**表格蕴涵**，通过创建一个平衡的数据集，其中包含数百万个自动创建的训练示例，这些示例在微调之前的中间步骤中学习。TAPAS的作者将这种进一步的预训练称为中间预训练（因为TAPAS首先在MLM上进行预训练，然后在另一个数据集上进行预训练）。他们发现中间预训练进一步提高了在SQA上的性能，实现了新的最新技术，以及在[TabFact](https://github.com/wenhuchen/Table-Fact-Checking)上的最新技术，这是一个包含16k维基百科表格的大规模数据集，用于表格蕴涵（二元分类任务）。有关更多详细信息，请参阅他们的后续论文：[通过中间预训练理解表格](https://www.aclweb.org/anthology/2020.findings-emnlp.27/)，作者为Julian
    Martin Eisenschlos、Syrine Krichene和Thomas Müller。
- en: '![drawing](../Images/df7807a77dc97c048e8b8965efaa87b6.png) TAPAS architecture.
    Taken from the [original blog post](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![drawing](../Images/df7807a77dc97c048e8b8965efaa87b6.png) TAPAS架构。摘自[原始博客文章](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html)。'
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The Tensorflow
    version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj).
    The original code can be found [here](https://github.com/google-research/tapas).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是由[nielsr](https://huggingface.co/nielsr)贡献的。这个模型的Tensorflow版本是由[kamalkraj](https://huggingface.co/kamalkraj)贡献的。原始代码可以在[这里](https://github.com/google-research/tapas)找到。
- en: Usage tips
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用提示
- en: TAPAS is a model that uses relative position embeddings by default (restarting
    the position embeddings at every cell of the table). Note that this is something
    that was added after the publication of the original TAPAS paper. According to
    the authors, this usually results in a slightly better performance, and allows
    you to encode longer sequences without running out of embeddings. This is reflected
    in the `reset_position_index_per_cell` parameter of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    which is set to `True` by default. The default versions of the models available
    on the [hub](https://huggingface.co/models?search=tapas) all use relative position
    embeddings. You can still use the ones with absolute position embeddings by passing
    in an additional argument `revision="no_reset"` when calling the `from_pretrained()`
    method. Note that it’s usually advised to pad the inputs on the right rather than
    the left.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPAS默认使用相对位置嵌入（在表格的每个单元格重新启动位置嵌入）。请注意，这是在原始TAPAS论文发表后添加的内容。根据作者的说法，这通常会导致略微更好的性能，并且允许您在不耗尽嵌入的情况下编码更长的序列。这反映在[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)的`reset_position_index_per_cell`参数中，默认设置为`True`。[hub](https://huggingface.co/models?search=tapas)上提供的默认版本的模型都使用相对位置嵌入。您仍然可以通过在调用`from_pretrained()`方法时传入额外参数`revision="no_reset"`来使用绝对位置嵌入的模型。通常建议在右侧而不是左侧填充输入。
- en: TAPAS is based on BERT, so `TAPAS-base` for example corresponds to a `BERT-base`
    architecture. Of course, `TAPAS-large` will result in the best performance (the
    results reported in the paper are from `TAPAS-large`). Results of the various
    sized models are shown on the [original GitHub repository](https://github.com/google-research/tapas).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPAS基于BERT，因此例如`TAPAS-base`对应于`BERT-base`架构。当然，`TAPAS-large`将获得最佳性能（论文中报告的结果来自`TAPAS-large`）。各种大小模型的结果显示在[原始GitHub存储库](https://github.com/google-research/tapas)上。
- en: 'TAPAS has checkpoints fine-tuned on SQA, which are capable of answering questions
    related to a table in a conversational set-up. This means that you can ask follow-up
    questions such as “what is his age?” related to the previous question. Note that
    the forward pass of TAPAS is a bit different in case of a conversational set-up:
    in that case, you have to feed every table-question pair one by one to the model,
    such that the `prev_labels` token type ids can be overwritten by the predicted
    `labels` of the model to the previous question. See “Usage” section for more info.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPAS有在SQA上微调的检查点，能够回答与表格相关的问题，这意味着您可以提出后续问题，比如“他多大了？”与之前的问题相关。请注意，在对话设置中，TAPAS的前向传递有点不同：在这种情况下，您必须逐个向模型提供每个表格-问题对，以便`prev_labels`令牌类型id可以被模型的预测`labels`覆盖到前一个问题。查看“用法”部分获取更多信息。
- en: TAPAS is similar to BERT and therefore relies on the masked language modeling
    (MLM) objective. It is therefore efficient at predicting masked tokens and at
    NLU in general, but is not optimal for text generation. Models trained with a
    causal language modeling (CLM) objective are better in that regard. Note that
    TAPAS can be used as an encoder in the EncoderDecoderModel framework, to combine
    it with an autoregressive text decoder such as GPT-2.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAPAS类似于BERT，因此依赖于掩码语言建模（MLM）目标。因此，它在预测掩码标记和NLU方面效率很高，但不适用于文本生成。使用因果语言建模（CLM）目标训练的模型在这方面更好。请注意，TAPAS可以作为EncoderDecoderModel框架中的编码器使用，将其与自回归文本解码器（如GPT-2）结合使用。
- en: 'Usage: fine-tuning'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法：微调
- en: Here we explain how you can fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    on your own dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们解释了如何在自己的数据集上微调[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)。
- en: '**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：选择三种使用TAPAS的方式之一 - 或者进行实验**'
- en: 'Basically, there are 3 different ways in which one can fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering),
    corresponding to the different datasets on which Tapas was fine-tuned:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，有三种不同的方式可以微调[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)，对应于TAPAS进行微调的不同数据集：
- en: 'SQA: if you’re interested in asking follow-up questions related to a table,
    in a conversational set-up. For example if you first ask “what’s the name of the
    first actor?” then you can ask a follow-up question such as “how old is he?“.
    Here, questions do not involve any aggregation (all questions are cell selection
    questions).'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SQA：如果您对在对话设置中与表相关的后续问题感兴趣。例如，如果您首先问“第一个演员的名字是什么？”然后您可以问一个后续问题，比如“他多大了？”。在这里，问题不涉及任何聚合（所有问题都是单元格选择问题）。
- en: 'WTQ: if you’re not interested in asking questions in a conversational set-up,
    but rather just asking questions related to a table, which might involve aggregation,
    such as counting a number of rows, summing up cell values or averaging cell values.
    You can then for example ask “what’s the total number of goals Cristiano Ronaldo
    made in his career?“. This case is also called **weak supervision**, since the
    model itself must learn the appropriate aggregation operator (SUM/COUNT/AVERAGE/NONE)
    given only the answer to the question as supervision.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WTQ：如果您对在对话设置中提问不感兴趣，而只是提问与表相关的问题，可能涉及聚合，比如计算行数、求和单元格值或平均单元格值。然后，例如您可以问“Cristiano
    Ronaldo在他的职业生涯中进球总数是多少？”。这种情况也被称为**弱监督**，因为模型本身必须仅根据问题的答案学习适当的聚合操作符（SUM/COUNT/AVERAGE/NONE）。
- en: 'WikiSQL-supervised: this dataset is based on WikiSQL with the model being given
    the ground truth aggregation operator during training. This is also called **strong
    supervision**. Here, learning the appropriate aggregation operator is much easier.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'WikiSQL-supervised: 这个数据集基于WikiSQL，模型在训练过程中会得到真实的聚合操作符。这也被称为**强监督**。在这里，学习适当的聚合操作符要容易得多。'
- en: 'To summarize:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：
- en: '| **Task** | **Example dataset** | **Description** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **示例数据集** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Conversational | SQA | Conversational, only cell selection questions |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | SQA | 仅单元选择问题的对话 |'
- en: '| Weak supervision for aggregation | WTQ | Questions might involve aggregation,
    and the model must learn this given only the answer as supervision |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 聚合的弱监督 | WTQ | 问题可能涉及聚合，模型必须仅根据答案作为监督来学习这一点 |'
- en: '| Strong supervision for aggregation | WikiSQL-supervised | Questions might
    involve aggregation, and the model must learn this given the gold aggregation
    operator |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 聚合的强监督 | WikiSQL-supervised | 问题可能涉及聚合，模型必须学习这一点，给出黄金聚合操作符 |'
- en: PytorchHide Pytorch content
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: Initializing a model with a pre-trained base and randomly initialized classification
    heads from the hub can be done as shown below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练基础模型和从hub随机初始化分类头初始化模型可以按照下面所示进行。
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Of course, you don’t necessarily have to follow one of these three ways in
    which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters
    you want when initializing [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    and then create a [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    based on that configuration. For example, if you have a dataset that has both
    conversational questions and questions that might involve aggregation, then you
    can do it this way. Here’s an example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不一定要遵循TAPAS微调的这三种方式之一。您还可以通过定义初始化[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)时想要的任何超参数来进行实验，然后基于该配置创建[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)。例如，如果您有一个包含对话问题和可能涉及聚合的问题的数据集，那么您可以这样做。这里是一个例子：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TensorFlowHide TensorFlow content
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow内容
- en: 'Initializing a model with a pre-trained base and randomly initialized classification
    heads from the hub can be done as shown below. Be sure to have installed the [tensorflow_probability](https://github.com/tensorflow/probability)
    dependency:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练基础模型和从hub随机初始化分类头初始化模型可以按照下面所示进行。请确保已安装[tensorflow_probability](https://github.com/tensorflow/probability)依赖项：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Of course, you don’t necessarily have to follow one of these three ways in
    which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters
    you want when initializing [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    and then create a [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    based on that configuration. For example, if you have a dataset that has both
    conversational questions and questions that might involve aggregation, then you
    can do it this way. Here’s an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您不一定要遵循TAPAS微调的这三种方式之一。您还可以通过定义初始化[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)时想要的任何超参数来进行实验，然后基于该配置创建[TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)。例如，如果您有一个包含对话问题和可能涉及聚合的问题的数据集，那么您可以这样做。这里是一个例子：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: What you can also do is start from an already fine-tuned checkpoint. A note
    here is that the already fine-tuned checkpoint on WTQ has some issues due to the
    L2-loss which is somewhat brittle. See [here](https://github.com/google-research/tapas/issues/91#issuecomment-735719340)
    for more info.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从已经微调的检查点开始。这里需要注意的是，WTQ上已经微调的检查点由于L2损失有些脆弱。有关更多信息，请参见[这里](https://github.com/google-research/tapas/issues/91#issuecomment-735719340)。
- en: For a list of all pre-trained and fine-tuned TAPAS checkpoints available on
    HuggingFace’s hub, see [here](https://huggingface.co/models?search=tapas).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看HuggingFace的hub上可用的所有预训练和微调的TAPAS检查点，请参见[这里](https://huggingface.co/models?search=tapas)。
- en: '**STEP 2: Prepare your data in the SQA format**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：准备您的数据以SQA格式**'
- en: 'Second, no matter what you picked above, you should prepare your dataset in
    the [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) format.
    This format is a TSV/CSV file with the following columns:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，无论您选择了什么，都应该准备好您的数据集以[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253)格式。这种格式是一个带有以下列的TSV/CSV文件：
- en: '`id`: optional, id of the table-question pair, for bookkeeping purposes.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`id`：可选，表-问题对的ID，用于记录目的。'
- en: '`annotator`: optional, id of the person who annotated the table-question pair,
    for bookkeeping purposes.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annotator`：可选，注释表-问题对的人的ID，用于记录目的。'
- en: '`position`: integer indicating if the question is the first, second, third,…
    related to the table. Only required in case of conversational setup (SQA). You
    don’t need this column in case you’re going for WTQ/WikiSQL-supervised.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position`: 整数，指示问题是与表相关的第一个、第二个、第三个等等。只有在对话设置（SQA）的情况下才需要（在进行WTQ/WikiSQL监督时不需要此列）。'
- en: '`question`: string'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`question`: 字符串'
- en: '`table_file`: string, name of a csv file containing the tabular data'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table_file`: 字符串，包含表格数据的csv文件的名称'
- en: '`answer_coordinates`: list of one or more tuples (each tuple being a cell coordinate,
    i.e. row, column pair that is part of the answer)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_coordinates`: 一个或多个元组的列表（每个元组都是答案的单元格坐标，即行列对）'
- en: '`answer_text`: list of one or more strings (each string being a cell value
    that is part of the answer)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_text`: 一个或多个字符串的列表（每个字符串都是答案的一部分单元格值）'
- en: '`aggregation_label`: index of the aggregation operator. Only required in case
    of strong supervision for aggregation (the WikiSQL-supervised case)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregation_label`: 聚合运算符的索引。只有在强监督聚合的情况下才需要（WikiSQL监督的情况）。'
- en: '`float_answer`: the float answer to the question, if there is one (np.nan if
    there isn’t). Only required in case of weak supervision for aggregation (such
    as WTQ and WikiSQL)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`float_answer`: 问题的浮点答案，如果有的话（如果没有则为np.nan）。只有在弱监督聚合的情况下才需要（例如WTQ和WikiSQL）。'
- en: The tables themselves should be present in a folder, each table being a separate
    csv file. Note that the authors of the TAPAS algorithm used conversion scripts
    with some automated logic to convert the other datasets (WTQ, WikiSQL) into the
    SQA format. The author explains this [here](https://github.com/google-research/tapas/issues/50#issuecomment-705465960).
    A conversion of this script that works with HuggingFace’s implementation can be
    found [here](https://github.com/NielsRogge/tapas_utils). Interestingly, these
    conversion scripts are not perfect (the `answer_coordinates` and `float_answer`
    fields are populated based on the `answer_text`), meaning that WTQ and WikiSQL
    results could actually be improved.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表格本身应该存在于一个文件夹中，每个表格都是一个单独的csv文件。请注意，TAPAS算法的作者使用具有一些自动逻辑的转换脚本将其他数据集（WTQ、WikiSQL）转换为SQA格式。作者在[这里](https://github.com/google-research/tapas/issues/50#issuecomment-705465960)解释了这一点。可以在[这里](https://github.com/NielsRogge/tapas_utils)找到一个适用于HuggingFace实现的此脚本的转换。有趣的是，这些转换脚本并不完美（`answer_coordinates`和`float_answer`字段是基于`answer_text`填充的），这意味着WTQ和WikiSQL的结果实际上可能会得到改善。
- en: '**STEP 3: Convert your data into tensors using TapasTokenizer**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：使用TapasTokenizer将数据转换为张量**'
- en: PytorchHide Pytorch content
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏Pytorch内容
- en: 'Third, given that you’ve prepared your data in this TSV/CSV format (and corresponding
    CSV files containing the tabular data), you can then use [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids`
    and so on. Again, based on which of the three cases you picked above, [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    requires different inputs to be fine-tuned:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，假设您已经准备好了以TSV/CSV格式（以及包含表格数据的相应CSV文件）的数据，那么您可以使用[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)将表格问题对转换为`input_ids`、`attention_mask`、`token_type_ids`等。再次根据您选择的三种情况中的哪一种，[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)需要不同的输入进行微调：
- en: '| **Task** | **Required inputs** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **所需输入** |'
- en: '| --- | --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Conversational | `input_ids`, `attention_mask`, `token_type_ids`, `labels`
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | `input_ids`，`attention_mask`，`token_type_ids`，`labels` |'
- en: '| Weak supervision for aggregation | `input_ids`, `attention_mask`, `token_type_ids`,
    `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督聚合 | `input_ids`，`attention_mask`，`token_type_ids`，`labels`，`numeric_values`，`numeric_values_scale`，`float_answer`
    |'
- en: '| Strong supervision for aggregation | `input ids`, `attention mask`, `token
    type ids`, `labels`, `aggregation_labels` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 强监督聚合 | `input ids`，`attention mask`，`token type ids`，`labels`，`aggregation_labels`
    |'
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates the `labels`, `numeric_values` and `numeric_values_scale` based on the
    `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer`
    and `aggregation_labels` are already in the TSV file of step 2\. Here’s an example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    根据TSV文件的`answer_coordinates`和`answer_text`列创建`labels`、`numeric_values`和`numeric_values_scale`。第2步的TSV文件中已经包含了`float_answer`和`aggregation_labels`。以下是一个示例：'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    expects the data of the table to be **text-only**. You can use `.astype(str)`
    on a dataframe to turn it into text-only data. Of course, this only shows how
    to encode a single training example. It is advised to create a dataloader to iterate
    over batches:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)期望表格数据为**仅文本**。您可以在数据框上使用`.astype(str)`将其转换为仅文本数据。当然，这只是如何对单个训练示例进行编码的示例。建议创建一个数据加载器以迭代处理批次：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: TensorFlowHide TensorFlow content
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏TensorFlow内容
- en: 'Third, given that you’ve prepared your data in this TSV/CSV format (and corresponding
    CSV files containing the tabular data), you can then use [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids`
    and so on. Again, based on which of the three cases you picked above, [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    requires different inputs to be fine-tuned:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，假设您已经准备好了以TSV/CSV格式（以及包含表格数据的相应CSV文件）的数据，那么您可以使用[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)将表格问题对转换为`input_ids`、`attention_mask`、`token_type_ids`等。再次根据您选择的三种情况中的哪一种，[TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)需要不同的输入进行微调：
- en: '| **Task** | **Required inputs** |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **任务** | **所需输入** |'
- en: '| --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Conversational | `input_ids`, `attention_mask`, `token_type_ids`, `labels`
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | `input_ids`，`attention_mask`，`token_type_ids`，`labels` |'
- en: '| Weak supervision for aggregation | `input_ids`, `attention_mask`, `token_type_ids`,
    `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 聚合的弱监督 | `input_ids`，`attention_mask`，`token_type_ids`，`labels`，`numeric_values`，`numeric_values_scale`，`float_answer`
    |'
- en: '| Strong supervision for aggregation | `input ids`, `attention mask`, `token
    type ids`, `labels`, `aggregation_labels` |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 聚合的强监督 | `input ids`，`attention mask`，`token type ids`，`labels`，`aggregation_labels`
    |'
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates the `labels`, `numeric_values` and `numeric_values_scale` based on the
    `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer`
    and `aggregation_labels` are already in the TSV file of step 2\. Here’s an example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)根据TSV文件的`answer_coordinates`和`answer_text`列创建`labels`，`numeric_values`和`numeric_values_scale`。第2步的TSV文件中已经包含了`float_answer`和`aggregation_labels`。以下是一个示例：'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    expects the data of the table to be **text-only**. You can use `.astype(str)`
    on a dataframe to turn it into text-only data. Of course, this only shows how
    to encode a single training example. It is advised to create a dataloader to iterate
    over batches:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)期望表格数据为**仅文本**。您可以在数据框上使用`.astype(str)`将其转换为仅文本数据。当然，这仅显示了如何对单个训练示例进行编码。建议创建数据加载器以迭代处理批次：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that here, we encode each table-question pair independently. This is fine
    as long as your dataset is **not conversational**. In case your dataset involves
    conversational questions (such as in SQA), then you should first group together
    the `queries`, `answer_coordinates` and `answer_text` per table (in the order
    of their `position` index) and batch encode each table with its questions. This
    will make sure that the `prev_labels` token types (see docs of [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    for more info. See [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    for more info regarding using the TensorFlow model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，我们独立对每个表格-问题对进行编码。只要您的数据集**不是会话式**，这样做就没问题。如果您的数据集涉及会话式问题（例如SQA），那么您应该首先按表格（按其`position`索引顺序）将`queries`，`answer_coordinates`和`answer_text`分组在一起，并批量对每个表格及其问题进行编码。这将确保`prev_labels`标记类型（请参阅[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)文档）被正确设置。有关更多信息，请参阅[此笔记本](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)。有关使用TensorFlow模型的更多信息，请参阅[此笔记本](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)。
- en: '**STEP 4: Train (fine-tune) the model'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：训练（微调）模型'
- en: PytorchHide Pytorch content
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: 'You can then fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    as follows (shown here for the weak supervision for aggregation case):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以按照以下方式对[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)进行微调（这里展示了聚合案例的弱监督）：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: TensorFlowHide TensorFlow content
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow内容
- en: 'You can then fine-tune [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    as follows (shown here for the weak supervision for aggregation case):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以按照以下方式对[TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)进行微调（这里展示了聚合案例的弱监督）：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Usage: inference'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用法：推断
- en: PytorchHide Pytorch content
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch隐藏 Pytorch内容
- en: Here we explain how you can use [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    or [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    for inference (i.e. making predictions on new data). For inference, only `input_ids`,
    `attention_mask` and `token_type_ids` (which you can obtain using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    have to be provided to the model to obtain the logits. Next, you can use the handy
    `~models.tapas.tokenization_tapas.convert_logits_to_predictions` method to convert
    these into predicted coordinates and optional aggregation indices.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们解释了如何使用[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)或[TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)进行推断（即对新数据进行预测）。对于推断，只需向模型提供`input_ids`，`attention_mask`和`token_type_ids`（您可以使用[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)获取），即可获得logits。接下来，您可以使用方便的`~models.tapas.tokenization_tapas.convert_logits_to_predictions`方法将其转换为预测坐标和可选的聚合索引。
- en: 'However, note that inference is **different** depending on whether or not the
    setup is conversational. In a non-conversational set-up, inference can be done
    in parallel on all table-question pairs of a batch. Here’s an example of that:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，推断取决于设置是否为会话式**不同**。在非会话式设置中，可以并行对批次中的所有表格-问题对进行推断。以下是一个示例：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: TensorFlowHide TensorFlow content
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow隐藏 TensorFlow内容
- en: Here we explain how you can use [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    for inference (i.e. making predictions on new data). For inference, only `input_ids`,
    `attention_mask` and `token_type_ids` (which you can obtain using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    have to be provided to the model to obtain the logits. Next, you can use the handy
    `~models.tapas.tokenization_tapas.convert_logits_to_predictions` method to convert
    these into predicted coordinates and optional aggregation indices.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们解释了如何使用[TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)进行推断（即在新数据上进行预测）。对于推断，只需要向模型提供`input_ids`、`attention_mask`和`token_type_ids`（可以使用[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)获得）即可获得logits。接下来，您可以使用方便的`~models.tapas.tokenization_tapas.convert_logits_to_predictions`方法将其转换为预测的坐标和可选的聚合索引。
- en: 'However, note that inference is **different** depending on whether or not the
    setup is conversational. In a non-conversational set-up, inference can be done
    in parallel on all table-question pairs of a batch. Here’s an example of that:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，推断取决于设置是否是对话式。在非对话式设置中，可以并行对批处理中的所有表格-问题对进行推断。以下是一个示例：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In case of a conversational set-up, then each table-question pair must be provided
    **sequentially** to the model, such that the `prev_labels` token types can be
    overwritten by the predicted `labels` of the previous table-question pair. Again,
    more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    (for PyTorch) and [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    (for TensorFlow).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话式设置中，每个表格-问题对必须**顺序**提供给模型，以便前一个表格-问题对的`prev_labels`标记类型可以被前一个表格-问题对的预测`labels`覆盖。再次，更多信息可以在[此笔记本](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)（适用于PyTorch）和[此笔记本](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)（适用于TensorFlow）中找到。
- en: Resources
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[Text classification task guide](../tasks/sequence_classification)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本分类任务指南](../tasks/sequence_classification)'
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掩码语言建模任务指南](../tasks/masked_language_modeling)'
- en: TAPAS specific outputs
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TAPAS特定输出
- en: '### `class transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L97)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L97)'
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    (and possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，当提供`labels`（可能还有`answer`、`aggregation_labels`、`numeric_values`和`numeric_values_scale`）时返回）
    — 作为分层单元选择对数似然损失和（可选）半监督回归损失以及（可选）聚合的监督损失的总损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`） — 每个标记的单元选择头的预测分数。'
- en: '`logits_aggregation` (`torch.FloatTensor`, *optional*, of shape `(batch_size,
    num_aggregation_labels)`) — Prediction scores of the aggregation head, for every
    aggregation operator.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_aggregation`（`torch.FloatTensor`，*可选*，形状为`(batch_size, num_aggregation_labels)`）
    — 每个聚合运算符的聚合头的预测分数。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回）
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每层的输出）。模型在每一层的输出的隐藏状态加上初始嵌入输出。'
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或当`config.output_attentions=True`时返回）
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。'
- en: Output type of [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)的输出类型。'
- en: TapasConfig
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapasConfig
- en: '### `class transformers.TapasConfig`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapasConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/configuration_tapas.py#L45)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/configuration_tapas.py#L45)'
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    TAPAS model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`（`int`，*可选*，默认为30522） — TAPAS模型的词汇表大小。定义了在调用[TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)时可以表示的不同标记的数量。'
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为768) — 编码器层和池化器层的维度。'
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_hidden_layers` (`int`, *optional*, 默认为12) — Transformer编码器中的隐藏层数量。'
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_attention_heads` (`int`, *optional*, 默认为12) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`intermediate_size` (`int`, *optional*, 默认为3072) — Transformer编码器中“中间”（通常称为前馈）层的维度。'
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"swish"` and `"gelu_new"` are supported.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_act` (`str` 或 `Callable`, *optional*, 默认为`"gelu"`) — 编码器和池化器中的非线性激活函数（函数或字符串）。如果是字符串，支持`"gelu"`、`"relu"`、`"swish"`和`"gelu_new"`。'
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_dropout_prob` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的dropout概率。'
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_probs_dropout_prob` (`float`, *optional*, 默认为0.1) — 注意力概率的dropout比率。'
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_position_embeddings` (`int`, *optional*, 默认为1024) — 该模型可能使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`type_vocab_sizes` (`List[int]`, *optional*, defaults to `[3, 256, 256, 2,
    256, 256, 10]`) — The vocabulary sizes of the `token_type_ids` passed when calling
    [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type_vocab_sizes` (`List[int]`, *optional*, 默认为`[3, 256, 256, 2, 256, 256,
    10]`) — 在调用[TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)时传递的`token_type_ids`的词汇大小。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *optional*, 默认为0.02) — 用于初始化所有权重矩阵的truncated_normal_initializer的标准差。'
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_eps` (`float`, *optional*, 默认为1e-12) — 层归一化层使用的epsilon。'
- en: '`positive_label_weight` (`float`, *optional*, defaults to 10.0) — Weight for
    positive labels.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`positive_label_weight` (`float`, *optional*, 默认为10.0) — 正标签的权重。'
- en: '`num_aggregation_labels` (`int`, *optional*, defaults to 0) — The number of
    aggregation operators to predict.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_aggregation_labels` (`int`, *optional*, 默认为0) — 要预测的聚合运算符数量。'
- en: '`aggregation_loss_weight` (`float`, *optional*, defaults to 1.0) — Importance
    weight for the aggregation loss.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregation_loss_weight` (`float`, *optional*, 默认为1.0) — 聚合损失的重要性权重。'
- en: '`use_answer_as_supervision` (`bool`, *optional*) — Whether to use the answer
    as the only supervision for aggregation examples.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_answer_as_supervision` (`bool`, *optional*) — 是否将答案作为聚合示例的唯一监督。'
- en: '`answer_loss_importance` (`float`, *optional*, defaults to 1.0) — Importance
    weight for the regression loss.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_loss_importance` (`float`, *optional*, 默认为1.0) — 回归损失的重要性权重。'
- en: '`use_normalized_answer_loss` (`bool`, *optional*, defaults to `False`) — Whether
    to normalize the answer loss by the maximum of the predicted and expected value.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_normalized_answer_loss` (`bool`, *optional*, 默认为`False`) — 是否通过预测值和期望值的最大值对答案损失进行归一化。'
- en: '`huber_loss_delta` (`float`, *optional*) — Delta parameter used to calculate
    the regression loss.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`huber_loss_delta` (`float`, *optional*) — 用于计算回归损失的Delta参数。'
- en: '`temperature` (`float`, *optional*, defaults to 1.0) — Value used to control
    (OR change) the skewness of cell logits probabilities.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` (`float`, *optional*, 默认为1.0) — 用于控制（或改变）单元格logits概率的偏斜度的值。'
- en: '`aggregation_temperature` (`float`, *optional*, defaults to 1.0) — Scales aggregation
    logits to control the skewness of probabilities.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregation_temperature` (`float`, *optional*, 默认为1.0) — 缩放聚合logits以控制概率的偏斜度。'
- en: '`use_gumbel_for_cells` (`bool`, *optional*, defaults to `False`) — Whether
    to apply Gumbel-Softmax to cell selection.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_gumbel_for_cells` (`bool`, *optional*, 默认为`False`) — 是否对单元格选择应用Gumbel-Softmax。'
- en: '`use_gumbel_for_aggregation` (`bool`, *optional*, defaults to `False`) — Whether
    to apply Gumbel-Softmax to aggregation selection.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_gumbel_for_aggregation` (`bool`, *optional*, 默认为`False`) — 是否对聚合选择应用Gumbel-Softmax。'
- en: '`average_approximation_function` (`string`, *optional*, defaults to `"ratio"`)
    — Method to calculate the expected average of cells in the weak supervision case.
    One of `"ratio"`, `"first_order"` or `"second_order"`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`average_approximation_function` (`string`, *optional*, 默认为`"ratio"`) — 计算弱监督情况下单元格期望平均值的方法。可以选择`"ratio"`、`"first_order"`或`"second_order"`之一。'
- en: '`cell_selection_preference` (`float`, *optional*) — Preference for cell selection
    in ambiguous cases. Only applicable in case of weak supervision for aggregation
    (WTQ, WikiSQL). If the total mass of the aggregation probabilities (excluding
    the “NONE” operator) is higher than this hyperparameter, then aggregation is predicted
    for an example.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cell_selection_preference` (`float`, *optional*) — 在模糊情况下对单元格选择的偏好。仅适用于聚合的弱监督情况（WTQ、WikiSQL）。如果聚合概率的总质量（不包括“NONE”运算符）高于此超参数，则对示例进行聚合预测。'
- en: '`answer_loss_cutoff` (`float`, *optional*) — Ignore examples with answer loss
    larger than cutoff.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_loss_cutoff` (`float`, *optional*) — 忽略答案损失大于截断值的示例。'
- en: '`max_num_rows` (`int`, *optional*, defaults to 64) — Maximum number of rows.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_num_rows` (`int`, *optional*, 默认为64) — 最大行数。'
- en: '`max_num_columns` (`int`, *optional*, defaults to 32) — Maximum number of columns.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_num_columns` (`int`, *optional*, 默认为32) — 最大列数。'
- en: '`average_logits_per_cell` (`bool`, *optional*, defaults to `False`) — Whether
    to average logits per cell.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`average_logits_per_cell` (`bool`, *optional*, defaults to `False`) — 是否对每个单元格的logits进行平均。'
- en: '`select_one_column` (`bool`, *optional*, defaults to `True`) — Whether to constrain
    the model to only select cells from a single column.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select_one_column` (`bool`, *optional*, defaults to `True`) — 是否限制模型只选择来自单个列的单元格。'
- en: '`allow_empty_column_selection` (`bool`, *optional*, defaults to `False`) —
    Whether to allow not to select any column.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`allow_empty_column_selection` (`bool`, *optional*, defaults to `False`) —
    是否允许不选择任何列。'
- en: '`init_cell_selection_weights_to_zero` (`bool`, *optional*, defaults to `False`)
    — Whether to initialize cell selection weights to 0 so that the initial probabilities
    are 50%.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_cell_selection_weights_to_zero` (`bool`, *optional*, defaults to `False`)
    — 是否将单元格选择权重初始化为0，以便初始概率为50%。'
- en: '`reset_position_index_per_cell` (`bool`, *optional*, defaults to `True`) —
    Whether to restart position indexes at every cell (i.e. use relative position
    embeddings).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reset_position_index_per_cell` (`bool`, *optional*, defaults to `True`) —
    是否在每个单元格重新开始位置索引（即使用相对位置嵌入）。'
- en: '`disable_per_token_loss` (`bool`, *optional*, defaults to `False`) — Whether
    to disable any (strong or weak) supervision on cells.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`disable_per_token_loss` (`bool`, *optional*, defaults to `False`) — 是否禁用对单元格的任何（强或弱）监督。'
- en: '`aggregation_labels` (`Dict[int, label]`, *optional*) — The aggregation labels
    used to aggregate the results. For example, the WTQ models have the following
    aggregation labels: `{0: "NONE", 1: "SUM", 2: "AVERAGE", 3: "COUNT"}`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregation_labels` (`Dict[int, label]`, *optional*) — 用于聚合结果的聚合标签。例如，WTQ模型具有以下聚合标签：`{0:
    "NONE", 1: "SUM", 2: "AVERAGE", 3: "COUNT"}`'
- en: '`no_aggregation_label_index` (`int`, *optional*) — If the aggregation labels
    are defined and one of these labels represents “No aggregation”, this should be
    set to its index. For example, the WTQ models have the “NONE” aggregation label
    at index 0, so that value should be set to 0 for these models.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_aggregation_label_index` (`int`, *optional*) — 如果定义了聚合标签，并且其中一个标签表示“无聚合”，则应将其设置为其索引。例如，WTQ模型在索引0处具有“NONE”聚合标签，因此对于这些模型，应将该值设置为0。'
- en: This is the configuration class to store the configuration of a [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).
    It is used to instantiate a TAPAS model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the TAPAS [google/tapas-base-finetuned-sqa](https://huggingface.co/google/tapas-base-finetuned-sqa)
    architecture.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)配置的配置类。它用于根据指定的参数实例化TAPAS模型，定义模型架构。使用默认值实例化配置将产生类似于TAPAS
    [google/tapas-base-finetuned-sqa](https://huggingface.co/google/tapas-base-finetuned-sqa)架构的配置。
- en: Configuration objects inherit from `PreTrainedConfig` and can be used to control
    the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自`PreTrainedConfig`，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)中的文档以获取更多信息。
- en: Hyperparameters additional to BERT are taken from run_task_main.py and hparam_utils.py
    of the original implementation. Original implementation available at [https://github.com/google-research/tapas/tree/master](https://github.com/google-research/tapas/tree/master).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除BERT外的超参数取自原始实现的run_task_main.py和hparam_utils.py。原始实现可在[https://github.com/google-research/tapas/tree/master](https://github.com/google-research/tapas/tree/master)找到。
- en: 'Example:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: TapasTokenizer
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapasTokenizer
- en: '### `class transformers.TapasTokenizer`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapasTokenizer`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L237)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L237)'
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Parameters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_file` (`str`) — 包含词汇表的文件。'
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — 在标记化时是否将输入转换为小写。'
- en: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — Whether or not
    to do basic tokenization before WordPiece.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — 在WordPiece之前是否进行基本标记化。'
- en: '`never_split` (`Iterable`, *optional*) — Collection of tokens which will never
    be split during tokenization. Only has an effect when `do_basic_tokenize=True`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`never_split` (`Iterable`, *optional*) — 在标记化期间永远不会分割的标记集合。仅在`do_basic_tokenize=True`时才有效。'
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — 未知标记。词汇表中不存在的标记无法转换为ID，而是设置为此标记。'
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — 分隔符标记，在构建来自多个序列的序列时使用，例如，用于序列分类的两个序列或用于问题回答的文本和问题。它还用作使用特殊标记构建的序列的最后一个标记。'
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — 用于填充的标记，例如在批处理不同长度的序列时使用。'
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — 分类器标记，在进行序列分类（对整个序列进行分类而不是每个标记的分类）时使用。它是使用特殊标记构建的序列的第一个标记。'
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mask_token` (`str`, *optional*, 默认为 `"[MASK]"`) — 用于屏蔽值的标记。在使用掩码语言建模训练此模型时使用的标记。这是模型将尝试预测的标记。'
- en: '`empty_token` (`str`, *optional*, defaults to `"[EMPTY]"`) — The token used
    for empty cell values in a table. Empty cell values include "", “n/a”, “nan” and
    ”?“.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`empty_token` (`str`, *optional*, 默认为 `"[EMPTY]"`) — 用于表格中空单元格值的标记。空单元格值包括""、"n/a"、"nan"和"?"。'
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters. This should likely be deactivated for Japanese
    (see this [issue](https://github.com/huggingface/transformers/issues/328)).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenize_chinese_chars` (`bool`, *optional*, 默认为 `True`) — 是否对中文字符进行标记。这对于日语可能应该被停用（参见此[问题](https://github.com/huggingface/transformers/issues/328)）。'
- en: '`strip_accents` (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_accents` (`bool`, *optional*) — 是否去除所有重音符号。如果未指定此选项，则将由`lowercase`的值确定（与原始BERT相同）。'
- en: '`cell_trim_length` (`int`, *optional*, defaults to -1) — If > 0: Trim cells
    so that the length is <= this value. Also disables further cell trimming, should
    thus be used with `truncation` set to `True`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cell_trim_length` (`int`, *optional*, 默认为 -1) — 如果 > 0：修剪单元格，使长度 <= 此值。还会禁用进一步的单元格修剪，因此应该与`truncation`设置为`True`一起使用。'
- en: '`max_column_id` (`int`, *optional*) — Max column id to extract.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_column_id` (`int`, *optional*) — 要提取的最大列id。'
- en: '`max_row_id` (`int`, *optional*) — Max row id to extract.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_row_id` (`int`, *optional*) — 要提取的最大行id。'
- en: '`strip_column_names` (`bool`, *optional*, defaults to `False`) — Whether to
    add empty strings instead of column names.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strip_column_names` (`bool`, *optional*, 默认为 `False`) — 是否添加空字符串而不是列名。'
- en: '`update_answer_coordinates` (`bool`, *optional*, defaults to `False`) — Whether
    to recompute the answer coordinates from the answer text.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_answer_coordinates` (`bool`, *optional*, 默认为 `False`) — 是否重新计算答案文本的答案坐标。'
- en: '`min_question_length` (`int`, *optional*) — Minimum length of each question
    in terms of tokens (will be skipped otherwise).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_question_length` (`int`, *optional*) — 每个问题的最小长度，以标记为单位（否则将被跳过）。'
- en: '`max_question_length` (`int`, *optional*) — Maximum length of each question
    in terms of tokens (will be skipped otherwise).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_question_length` (`int`, *optional*) — 每个问题的最大长度，以标记为单位（否则将被跳过）。'
- en: Construct a TAPAS tokenizer. Based on WordPiece. Flattens a table and one or
    more related sentences to be used by TAPAS models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个TAPAS分词器。基于WordPiece。将表格和一个或多个相关句子展平，以供TAPAS模型使用。
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods. [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates several token type ids to encode tabular structure. To be more precise,
    it adds 7 token type ids, in the following order: `segment_ids`, `column_ids`,
    `row_ids`, `prev_labels`, `column_ranks`, `inv_column_ranks` and `numeric_relations`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分词器继承自[PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)，其中包含大部分主要方法。用户应该参考这个超类以获取有关这些方法的更多信息。[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)创建了几个标记类型id来编码表格结构。更准确地说，它按照以下顺序添加了7个标记类型id：`segment_ids`、`column_ids`、`row_ids`、`prev_labels`、`column_ranks`、`inv_column_ranks`和`numeric_relations`：
- en: 'segment_ids: indicate whether a token belongs to the question (0) or the table
    (1). 0 for special tokens and padding.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'segment_ids: 指示一个标记属于问题（0）还是表格（1）。对于特殊标记和填充，值为0。'
- en: 'column_ids: indicate to which column of the table a token belongs (starting
    from 1). Is 0 for all question tokens, special tokens and padding.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'column_ids: 指示一个标记属于表格的哪一列（从1开始）。对于所有问题标记、特殊标记和填充，值为0。'
- en: 'row_ids: indicate to which row of the table a token belongs (starting from
    1). Is 0 for all question tokens, special tokens and padding. Tokens of column
    headers are also 0.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'row_ids: 指示一个标记属于表格的哪一行（从1开始）。对于所有问题标记、特殊标记和填充，值为0。列标题的标记也为0。'
- en: 'prev_labels: indicate whether a token was (part of) an answer to the previous
    question (1) or not (0). Useful in a conversational setup (such as SQA).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'prev_labels: 指示一个标记是否是前一个问题的答案的一部分（1）还是不是（0）。在对话设置中很有用（如SQA）。'
- en: 'column_ranks: indicate the rank of a table token relative to a column, if applicable.
    For example, if you have a column “number of movies” with values 87, 53 and 69,
    then the column ranks of these tokens are 3, 1 and 2 respectively. 0 for all question
    tokens, special tokens and padding.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'column_ranks: 指示表格标记相对于列的排名，如果适用的话。例如，如果你有一个列“电影数量”，值为87、53和69，则这些标记的列排名分别为3、1和2。对于所有问题标记、特殊标记和填充，值为0。'
- en: 'inv_column_ranks: indicate the inverse rank of a table token relative to a
    column, if applicable. For example, if you have a column “number of movies” with
    values 87, 53 and 69, then the inverse column ranks of these tokens are 1, 3 and
    2 respectively. 0 for all question tokens, special tokens and padding.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'inv_column_ranks: 指示表格标记相对于列的逆序排名，如果适用的话。例如，如果你有一个列“电影数量”，值为87、53和69，则这些标记的逆序列排名分别为1、3和2。对于所有问题标记、特殊标记和填充，值为0。'
- en: 'numeric_relations: indicate numeric relations between the question and the
    tokens of the table. 0 for all question tokens, special tokens and padding.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'numeric_relations: 指示问题和表格标记之间的数值关系。对于所有问题标记、特殊标记和填充，值为0。'
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    runs end-to-end tokenization on a table and associated sentences: punctuation
    splitting and wordpiece.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)在表格和相关句子上运行端到端的分词：标点符号拆分和wordpiece。'
- en: '#### `__call__`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `__call__`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L588)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L588)'
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Parameters
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`table` (`pd.DataFrame`) — Table containing tabular data. Note that all cell
    values must be text. Use *.astype(str)* on a Pandas dataframe to convert it to
    string.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table` (`pd.DataFrame`) — 包含表格数据的表格。请注意，所有单元格的值必须是文本。在Pandas数据帧上使用*.astype(str)*将其转换为字符串。'
- en: '`queries` (`str` or `List[str]`) — Question or batch of questions related to
    a table to be encoded. Note that in case of a batch, all questions must refer
    to the **same** table.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`queries` (`str` 或 `List[str]`) — 与要编码的表格相关的问题或问题批次。请注意，在批处理的情况下，所有问题必须引用**相同**的表格。'
- en: '`answer_coordinates` (`List[Tuple]` or `List[List[Tuple]]`, *optional*) — Answer
    coordinates of each table-question pair in the batch. In case only a single table-question
    pair is provided, then the answer_coordinates must be a single list of one or
    more tuples. Each tuple must be a (row_index, column_index) pair. The first data
    row (not the column header row) has index 0\. The first column has index 0\. In
    case a batch of table-question pairs is provided, then the answer_coordinates
    must be a list of lists of tuples (each list corresponding to a single table-question
    pair).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_coordinates` (`List[Tuple]` 或 `List[List[Tuple]]`, *可选*) — 批次中每个表格-问题对的答案坐标。如果只提供单个表格-问题对，则answer_coordinates必须是一个包含一个或多个元组的列表。每个元组必须是（行索引，列索引）对。第一行数据行（而不是列标题行）的索引为0。第一列的索引为0。如果提供了一个表格-问题对批次，则answer_coordinates必须是一个包含元组列表的列表（每个列表对应一个单个表格-问题对）。'
- en: '`answer_text` (`List[str]` or `List[List[str]]`, *optional*) — Answer text
    of each table-question pair in the batch. In case only a single table-question
    pair is provided, then the answer_text must be a single list of one or more strings.
    Each string must be the answer text of a corresponding answer coordinate. In case
    a batch of table-question pairs is provided, then the answer_coordinates must
    be a list of lists of strings (each list corresponding to a single table-question
    pair).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`answer_text` (`List[str]` 或 `List[List[str]]`, *可选*) — 批次中每个表格-问题对的答案文本。如果只提供单个表格-问题对，则answer_text必须是一个包含一个或多个字符串的列表。每个字符串必须是相应答案坐标的答案文本。如果提供了一个表格-问题对批次，则answer_coordinates必须是一个包含元组列表的列表（每个列表对应一个单个表格-问题对）。'
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens` (`bool`, *可选*, 默认为`True`) — 是否对序列进行编码，相对于其模型的特殊标记。'
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` (`bool`, `str` 或 [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *可选*, 默认为`False`) — 激活和控制填充。接受以下值：'
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''longest''`：填充到批次中最长的序列（如果只提供单个序列，则不填充）。'
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''max_length''`：填充到指定长度的最大长度，使用参数`max_length`，或者如果未提供该参数，则填充到模型的最大可接受输入长度。'
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_pad''`（默认）：不填充（即，可以输出具有不同长度序列的批次）。'
- en: '`truncation` (`bool`, `str` or `TapasTruncationStrategy`, *optional*, defaults
    to `False`) — Activates and controls truncation. Accepts the following values:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncation` (`bool`, `str` 或 `TapasTruncationStrategy`, *可选*, 默认为`False`)
    — 激活和控制截断。接受以下值：'
- en: '`True` or `''drop_rows_to_fit''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate row by row, removing rows
    from the table.'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 或 `''drop_rows_to_fit''`：截断到指定长度的最大长度，或者如果未提供该参数，则截断到模型的最大可接受输入长度。这将逐行截断，从表中删除行。'
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 或 `''do_not_truncate''`（默认）：不截断（即，可以输出批次，其序列长度大于模型最大可接受的输入大小）。'
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length` (`int`, *可选*) — 控制截断/填充参数使用的最大长度。'
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果未设置或设置为`None`，则如果截断/填充参数需要最大长度，则将使用预定义的模型最大长度。如果模型没有特定的最大输入长度（如XLNet），则将禁用截断/填充到最大长度。
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_split_into_words` (`bool`, *可选*, 默认为`False`) — 输入是否已经预分词（例如，已分割为单词）。如果设置为`True`，分词器会假定输入已经分割为单词（例如，通过在空格上分割），然后对其进行分词。这对于NER或标记分类很有用。'
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_multiple_of` (`int`, *可选*) — 如果设置，将填充序列到提供的值的倍数。这对于启用具有计算能力`>= 7.5`（Volta）的NVIDIA硬件上的Tensor
    Cores特别有用。'
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors` (`str` 或 [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *可选*) — 如果设置，将返回张量而不是 Python 整数列表。可接受的值为：'
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''tf''`: 返回 TensorFlow `tf.constant` 对象。'
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''pt''`: 返回 PyTorch `torch.Tensor` 对象。'
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''np''`: 返回 Numpy `np.ndarray` 对象。'
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    related to a table.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 用于对一个或多个与表格相关的序列进行标记化和准备模型的主要方法。
- en: '#### `convert_logits_to_predictions`'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `convert_logits_to_predictions`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L1951)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L1951)'
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Parameters
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`data` (`dict`) — Dictionary mapping features to actual values. Should be created
    using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data` (`dict`) — 将特征映射到实际值的字典。应使用 [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    创建。'
- en: '`logits` (`torch.Tensor` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Tensor containing the logits at the token level.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.Tensor` 或 `tf.Tensor`，形状为 `(batch_size, sequence_length)`)
    — 包含标记级别上的 logits 的张量。'
- en: '`logits_agg` (`torch.Tensor` or `tf.Tensor` of shape `(batch_size, num_aggregation_labels)`,
    *optional*) — Tensor containing the aggregation logits.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits_agg` (`torch.Tensor` 或 `tf.Tensor`，形状为 `(batch_size, num_aggregation_labels)`，*可选*)
    — 包含聚合 logits 的张量。'
- en: '`cell_classification_threshold` (`float`, *optional*, defaults to 0.5) — Threshold
    to be used for cell selection. All table cells for which their probability is
    larger than this threshold will be selected.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cell_classification_threshold` (`float`，*可选*，默认为0.5) — 用于单元格选择的阈值。所有概率大于此阈值的表格单元格将被选择。'
- en: Returns
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`tuple` comprising various elements depending on the inputs'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 包含各种元素的元组，取决于输入
- en: 'predicted_answer_coordinates (`List[List[[tuple]]` of length `batch_size`):
    Predicted answer coordinates as a list of lists of tuples. Each element in the
    list contains the predicted answer coordinates of a single example in the batch,
    as a list of tuples. Each tuple is a cell, i.e. (row index, column index).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: predicted_answer_coordinates (`List[List[[tuple]]`，长度为 `batch_size`)：预测的答案坐标，作为元组列表的列表。列表中的每个元素包含批次中单个示例的预测答案坐标，作为元组列表。每个元组是一个单元格，即
    (行索引，列索引)。
- en: 'predicted_aggregation_indices (`List[int]`of length `batch_size`, *optional*,
    returned when `logits_aggregation` is provided): Predicted aggregation operator
    indices of the aggregation head.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'predicted_aggregation_indices (`List[int]`，长度为 `batch_size`，*可选*，当提供 `logits_aggregation`
    时返回): 预测的聚合运算符索引。'
- en: Converts logits of [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    to actual predicted answer coordinates and optional aggregation indices.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    的 logits 转换为实际预测的答案坐标和可选的聚合索引。
- en: The original implementation, on which this function is based, can be found [here](https://github.com/google-research/tapas/blob/4908213eb4df7aa988573350278b44c4dbe3f71b/tapas/experiments/prediction_utils.py#L288).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此函数的原始实现可在 [此处](https://github.com/google-research/tapas/blob/4908213eb4df7aa988573350278b44c4dbe3f71b/tapas/experiments/prediction_utils.py#L288)
    找到。
- en: '#### `save_vocabulary`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `save_vocabulary`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L456)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L456)'
- en: '[PRE18]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: PytorchHide Pytorch content
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: PytorchHide Pytorch content
- en: TapasModel
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapasModel
- en: '### `class transformers.TapasModel`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapasModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L835)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L835)'
- en: '[PRE19]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Parameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只会加载配置。查看 [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    方法以加载模型权重。'
- en: The bare Tapas Model transformer outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 裸的 Tapas 模型变换器输出原始隐藏状态，没有特定的头部。此模型继承自 [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个 PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    子类。将其用作常规的 PyTorch 模块，并参考 PyTorch 文档以获取有关一般用法和行为的所有信息。
- en: This class is a small change compared to [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel),
    taking into account the additional token type ids.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类与 [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel)
    相比有一点小改变，考虑了额外的标记类型 id。
- en: The model can behave as an encoder (with only self-attention) as well as a decoder,
    in which case a layer of cross-attention is added between the self-attention layers,
    following the architecture described in [Attention is all you need](https://arxiv.org/abs/1706.03762)
    by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser and Illia Polosukhin.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可以作为编码器（仅具有自注意力）以及解码器运行，此时在自注意力层之间添加了一层交叉注意力，遵循[Ashish Vaswani, Noam Shazeer,
    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser和Illia
    Polosukhin](https://arxiv.org/abs/1706.03762)描述的架构。
- en: '#### `forward`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L876)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L876)'
- en: '[PRE20]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Parameters
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`) — 词汇表中输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1表示`未被掩码`的标记，
- en: 0 for tokens that are `masked`.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0表示`被掩码`的标记。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length, 7)`，*optional*)
    — 编码表格结构的标记索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关更多信息，请参见此类。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是标记类型ID？](../glossary#token-type-ids)'
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids` (`torch.LongTensor`，形状为`(batch_size, sequence_length)`，*optional*)
    — 每个输入序列标记在位置嵌入中的位置索引。如果[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)的`reset_position_index_per_cell`设置为`True`，则将使用相对位置嵌入。选在范围`[0,
    config.max_position_embeddings - 1]`内。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是位置ID？](../glossary#position-ids)'
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask` (`torch.FloatTensor`，形状为`(num_heads,)`或`(num_layers, num_heads)`，*optional*)
    — 用于使自注意力模块中选择的头部失效的掩码。掩码值选在`[0, 1]`范围内：- 1表示头部**未被掩码**，- 0表示头部**被掩码**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds` (`torch.FloatTensor`，形状为`(batch_size, sequence_length, hidden_size)`，*optional*)
    — 可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您想要更多控制如何将`input_ids`索引转换为相关向量，这将非常有用，而不是使用模型的内部嵌入查找矩阵。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions` (`bool`, *optional*) — 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states` (`bool`, *optional*) — 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict` (`bool`, *optional*) — 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: Returns
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)）和输入的不同元素。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — 模型最后一层的隐藏状态序列。'
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — 经过用于辅助预训练任务的层进一步处理后的序列的第一个标记（分类标记）的最后一层隐藏状态。例如，对于BERT系列模型，这将返回经过线性层和tanh激活函数处理后的分类标记。线性层的权重是在预训练期间从下一个句子预测（分类）目标中训练的。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_hidden_states=True`或当`config.output_hidden_states=True`时返回)
    — 形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（如果模型有嵌入层，则为嵌入输出的一个+每层输出的一个）。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *可选的*, 当传递`output_attentions=True`或当`config.output_attentions=True`时返回)
    — 形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力权重在注意力softmax之后，用于计算自注意力头中的加权平均值。
- en: The [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)
    forward method, overrides the `__call__` special method.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE21]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: TapasForMaskedLM
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapasForMaskedLM
- en: '### `class transformers.TapasForMaskedLM`'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapasForMaskedLM`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L991)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L991)'
- en: '[PRE22]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Parameters
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Tapas Model with a `language modeling` head on top. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部带有`语言建模`头的Tapas模型。该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。检查超类文档以获取库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型也是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1012)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1012)'
- en: '[PRE23]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Parameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`）— 词汇表中输入序列令牌的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参见[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是输入ID？
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, sequence_length)`的`torch.FloatTensor`，*可选*）—
    用于避免在填充令牌索引上执行注意力。掩码值选定在`[0, 1]`范围内：'
- en: 1 for tokens that are `not masked`,
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`未被掩码`的令牌为1，
- en: 0 for tokens that are `masked`.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`被掩码`的令牌为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是注意力掩码？
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`（形状为`(batch_size, sequence_length, 7)`的`torch.LongTensor`，*可选*）—
    编码表格结构的令牌索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关更多信息，请参见此类。'
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是令牌类型ID？
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`position_ids`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）—
    每个输入序列令牌在位置嵌入中的位置索引。如果[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)的`reset_position_index_per_cell`设置为`True`，将使用相对位置嵌入。在范围`[0,
    config.max_position_embeddings - 1]`中选择。'
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 什么是位置ID？
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head_mask`（形状为`(num_heads,)`或`(num_layers, num_heads)`的`torch.FloatTensor`，*可选*）—
    用于使自注意力模块中的选定头部失效的掩码。掩码值选定在`[0, 1]`范围内：- 1表示头部**未被掩码**，- 0表示头部**被掩码**。'
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs_embeds`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`，*可选*）—
    可选地，您可以选择直接传递嵌入表示而不是传递`input_ids`。如果您希望更多地控制如何将`input_ids`索引转换为相关向量，而不是使用模型的内部嵌入查找矩阵，这将非常有用。'
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_attentions`（`bool`，*可选*）— 是否返回所有注意力层的注意力张量。有关更多详细信息，请参见返回张量下的`attentions`。'
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_hidden_states`（`bool`，*可选*）— 是否返回所有层的隐藏状态。有关更多详细信息，请参见返回张量下的`hidden_states`。'
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_dict`（`bool`，*可选*）— 是否返回一个[ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)而不是一个普通元组。'
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`（形状为`(batch_size, sequence_length)`的`torch.LongTensor`，*可选*）— 用于计算掩码语言建模损失的标签。索引应在`[-100,
    0, ..., config.vocab_size]`范围内（参见`input_ids`文档字符串）。将索引设置为`-100`的令牌将被忽略（掩码），损失仅计算具有标签在`[0,
    ..., config.vocab_size]`范围内的令牌。'
- en: Returns
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或`tuple(torch.FloatTensor)`'
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一个[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)或一个`torch.FloatTensor`元组（如果传递了`return_dict=False`或当`config.return_dict=False`时）包含根据配置（[TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)）和输入的各种元素。
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`（形状为`(1,)`的`torch.FloatTensor`，*可选*，在提供`labels`时返回）— 掩码语言建模（MLM）损失。'
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — 语言建模头的预测分数（SoftMax之前每个词汇标记的分数）。'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`。'
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层的输出的隐藏状态以及可选的初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在注意力softmax之后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [TapasForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[TapasForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForMaskedLM)的前向方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是在此处调用，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE24]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: TapasForSequenceClassification
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TapasForSequenceClassification
- en: '### `class transformers.TapasForSequenceClassification`'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.TapasForSequenceClassification`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1446)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1446)'
- en: '[PRE25]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Parameters
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — 包含模型所有参数的模型配置类。使用配置文件初始化不会加载与模型相关的权重，只加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: Tapas Model with a sequence classification head on top (a linear layer on top
    of the pooled output), e.g. for table entailment tasks, such as TabFact (Chen
    et al., 2020).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部具有序列分类头的Tapas模型（在池化输出的顶部有一个线性层），例如用于表格推理任务的TabFact（Chen等，2020）。
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型继承自[PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel)。查看超类文档以了解库为所有模型实现的通用方法（例如下载或保存、调整输入嵌入、修剪头等）。
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型也是PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规PyTorch模块，并参考PyTorch文档以获取有关一般用法和行为的所有相关信息。
- en: '#### `forward`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1465)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1465)'
- en: '[PRE26]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Parameters
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    词汇表中输入序列标记的索引。可以使用[AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer)获取索引。有关详细信息，请参阅[PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)和[PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)。'
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是输入ID？](../glossary#input-ids)'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — 用于避免在填充标记索引上执行注意力的掩码。掩码值选在`[0, 1]`之间：'
- en: 1 for tokens that are `not masked`,
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于未被掩盖的标记，值为1，
- en: 0 for tokens that are `masked`.
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于被掩盖的标记，值为0。
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[什么是注意力掩码？](../glossary#attention-mask)'
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy). Note: this is called “classification_class_index”
    in the original implementation.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TapasForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: TapasForQuestionAnswering
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasForQuestionAnswering`'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1100)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parameters
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a cell selection head and optional aggregation head on top
    for question-answering tasks on tables (linear layers on top of the hidden-states
    output to compute `logits` and optional `logits_aggregation`), e.g. for SQA, WTQ
    or WikiSQL-supervised tasks.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1143)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Parameters
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_mask` (`torch.LongTensor` of shape `(batch_size, seq_length)`, *optional*)
    — Mask for the table. Indicates which tokens belong to the table (1). Question
    tokens, table headers and padding are 0.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, seq_length)`, *optional*)
    — Labels per token for computing the hierarchical cell selection loss. This encodes
    the positions of the answer appearing in the table. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `part of the answer`,
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not part of the answer`.
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_labels` (`torch.LongTensor` of shape `(batch_size, )`, *optional*)
    — Aggregation function index for every example in the batch for computing the
    aggregation loss. Indices should be in `[0, ..., config.num_aggregation_labels
    - 1]`. Only required in case of strong supervision for aggregation (WikiSQL-supervised).'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_answer` (`torch.FloatTensor` of shape `(batch_size, )`, *optional*)
    — Float answer for every example in the batch. Set to *float(‘nan’)* for cell
    selection questions. Only required in case of weak supervision (WTQ) to calculate
    the aggregate mask and regression loss.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values` (`torch.FloatTensor` of shape `(batch_size, seq_length)`,
    *optional*) — Numeric values of every token, NaN for tokens which are not numeric
    values. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values_scale` (`torch.FloatTensor` of shape `(batch_size, seq_length)`,
    *optional*) — Scale of the numeric values of every token. Can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput)
    or `tuple(torch.FloatTensor)`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    (and possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_aggregation` (`torch.FloatTensor`, *optional*, of shape `(batch_size,
    num_aggregation_labels)`) — Prediction scores of the aggregation head, for every
    aggregation operator.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: TensorFlowHide TensorFlow content
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: TFTapasModel
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasModel`'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1108)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Parameters
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Tapas Model transformer outputting raw hidden-states without any specific
    head on top.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1118)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Parameters
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasModel)
    forward method, overrides the `__call__` special method.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: TFTapasForMaskedLM
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForMaskedLM`'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1183)'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Parameters
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a `language modeling` head on top.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1200)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Parameters
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-489
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the masked language modeling loss. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) — Masked language modeling (MLM) loss.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForMaskedLM)
    forward method, overrides the `__call__` special method.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: TFTapasForSequenceClassification
  id: totrans-517
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForSequenceClassification`'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1730)'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Parameters
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a sequence classification head on top (a linear layer on top
    of the pooled output), e.g. for table entailment tasks, such as TabFact (Chen
    et al., 2020).
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1749)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Parameters
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_choices, sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-542
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-543
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length, 7)`, *optional*) — Token indices that encode tabular structure.
    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Indices of positions of each input sequence tokens
    in the position embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy). Note: this is called “classification_class_index”
    in the original implementation.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-566
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForSequenceClassification)
    forward method, overrides the `__call__` special method.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: TFTapasForQuestionAnswering
  id: totrans-571
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForQuestionAnswering`'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1387)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Parameters
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a cell selection head and optional aggregation head on top
    for question-answering tasks on tables (linear layers on top of the hidden-states
    output to compute `logits` and optional `logits_aggregation`), e.g. for SQA, WTQ
    or WikiSQL-supervised tasks.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1417)'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-590
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Parameters
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  id: totrans-596
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  id: totrans-597
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_mask` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Mask for the table. Indicates which tokens belong to the table (1). Question
    tokens, table headers and padding are 0.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*) — Labels
    per token for computing the hierarchical cell selection loss. This encodes the
    positions of the answer appearing in the table. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `part of the answer`,
  id: totrans-613
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not part of the answer`.
  id: totrans-614
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_labels` (`tf.Tensor` of shape `(batch_size, )`, *optional*) —
    Aggregation function index for every example in the batch for computing the aggregation
    loss. Indices should be in `[0, ..., config.num_aggregation_labels - 1]`. Only
    required in case of strong supervision for aggregation (WikiSQL-supervised).'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_answer` (`tf.Tensor` of shape `(batch_size, )`, *optional*) — Float
    answer for every example in the batch. Set to *float(‘nan’)* for cell selection
    questions. Only required in case of weak supervision (WTQ) to calculate the aggregate
    mask and regression loss.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Numeric values of every token, NaN for tokens which are not numeric values.
    Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values_scale` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Scale of the numeric values of every token. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.tapas.modeling_tf_tapas.TFTableQuestionAnsweringOutput`
    or `tuple(tf.Tensor)`'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.tapas.modeling_tf_tapas.TFTableQuestionAnsweringOutput`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` (and
    possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_aggregation` (`tf.Tensor`, *optional*, of shape `(batch_size, num_aggregation_labels)`)
    — Prediction scores of the aggregation head, for every aggregation operator.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-630
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
