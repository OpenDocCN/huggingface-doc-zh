- en: TAPAS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/tapas)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/261.f9caca2d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TAPAS model was proposed in [TAPAS: Weakly Supervised Table Parsing via
    Pre-training](https://www.aclweb.org/anthology/2020.acl-main.398) by Jonathan
    Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin
    Eisenschlos. It’s a BERT-based model specifically designed (and pre-trained) for
    answering questions about tabular data. Compared to BERT, TAPAS uses relative
    position embeddings and has 7 token types that encode tabular structure. TAPAS
    is pre-trained on the masked language modeling (MLM) objective on a large dataset
    comprising millions of tables from English Wikipedia and corresponding texts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For question answering, TAPAS has 2 heads on top: a cell selection head and
    an aggregation head, for (optionally) performing aggregations (such as counting
    or summing) among selected cells. TAPAS has been fine-tuned on several datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential
    Question Answering by Microsoft)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions
    by Stanford University)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It achieves state-of-the-art on both SQA and WTQ, while having comparable performance
    to SOTA on WikiSQL, with a much simpler architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Answering natural language questions over tables is usually seen as a semantic
    parsing task. To alleviate the collection cost of full logical forms, one popular
    approach focuses on weak supervision consisting of denotations instead of logical
    forms. However, training semantic parsers from weak supervision poses difficulties,
    and in addition, the generated logical forms are only used as an intermediate
    step prior to retrieving the denotation. In this paper, we present TAPAS, an approach
    to question answering over tables without generating logical forms. TAPAS trains
    from weak supervision, and predicts the denotation by selecting table cells and
    optionally applying a corresponding aggregation operator to such selection. TAPAS
    extends BERT’s architecture to encode tables as input, initializes from an effective
    joint pre-training of text segments and tables crawled from Wikipedia, and is
    trained end-to-end. We experiment with three different semantic parsing datasets,
    and find that TAPAS outperforms or rivals semantic parsing models by improving
    state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with
    the state-of-the-art on WIKISQL and WIKITQ, but with a simpler model architecture.
    We additionally find that transfer learning, which is trivial in our setting,
    from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the authors have further pre-trained TAPAS to recognize **table
    entailment**, by creating a balanced dataset of millions of automatically created
    training examples which are learned in an intermediate step prior to fine-tuning.
    The authors of TAPAS call this further pre-training intermediate pre-training
    (since TAPAS is first pre-trained on MLM, and then on another dataset). They found
    that intermediate pre-training further improves performance on SQA, achieving
    a new state-of-the-art as well as state-of-the-art on [TabFact](https://github.com/wenhuchen/Table-Fact-Checking),
    a large-scale dataset with 16k Wikipedia tables for table entailment (a binary
    classification task). For more details, see their follow-up paper: [Understanding
    tables with intermediate pre-training](https://www.aclweb.org/anthology/2020.findings-emnlp.27/)
    by Julian Martin Eisenschlos, Syrine Krichene and Thomas Müller.'
  prefs: []
  type: TYPE_NORMAL
- en: '![drawing](../Images/df7807a77dc97c048e8b8965efaa87b6.png) TAPAS architecture.
    Taken from the [original blog post](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [nielsr](https://huggingface.co/nielsr). The Tensorflow
    version of this model was contributed by [kamalkraj](https://huggingface.co/kamalkraj).
    The original code can be found [here](https://github.com/google-research/tapas).
  prefs: []
  type: TYPE_NORMAL
- en: Usage tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TAPAS is a model that uses relative position embeddings by default (restarting
    the position embeddings at every cell of the table). Note that this is something
    that was added after the publication of the original TAPAS paper. According to
    the authors, this usually results in a slightly better performance, and allows
    you to encode longer sequences without running out of embeddings. This is reflected
    in the `reset_position_index_per_cell` parameter of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    which is set to `True` by default. The default versions of the models available
    on the [hub](https://huggingface.co/models?search=tapas) all use relative position
    embeddings. You can still use the ones with absolute position embeddings by passing
    in an additional argument `revision="no_reset"` when calling the `from_pretrained()`
    method. Note that it’s usually advised to pad the inputs on the right rather than
    the left.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TAPAS is based on BERT, so `TAPAS-base` for example corresponds to a `BERT-base`
    architecture. Of course, `TAPAS-large` will result in the best performance (the
    results reported in the paper are from `TAPAS-large`). Results of the various
    sized models are shown on the [original GitHub repository](https://github.com/google-research/tapas).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TAPAS has checkpoints fine-tuned on SQA, which are capable of answering questions
    related to a table in a conversational set-up. This means that you can ask follow-up
    questions such as “what is his age?” related to the previous question. Note that
    the forward pass of TAPAS is a bit different in case of a conversational set-up:
    in that case, you have to feed every table-question pair one by one to the model,
    such that the `prev_labels` token type ids can be overwritten by the predicted
    `labels` of the model to the previous question. See “Usage” section for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TAPAS is similar to BERT and therefore relies on the masked language modeling
    (MLM) objective. It is therefore efficient at predicting masked tokens and at
    NLU in general, but is not optimal for text generation. Models trained with a
    causal language modeling (CLM) objective are better in that regard. Note that
    TAPAS can be used as an encoder in the EncoderDecoderModel framework, to combine
    it with an autoregressive text decoder such as GPT-2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage: fine-tuning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we explain how you can fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    on your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, there are 3 different ways in which one can fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering),
    corresponding to the different datasets on which Tapas was fine-tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SQA: if you’re interested in asking follow-up questions related to a table,
    in a conversational set-up. For example if you first ask “what’s the name of the
    first actor?” then you can ask a follow-up question such as “how old is he?“.
    Here, questions do not involve any aggregation (all questions are cell selection
    questions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'WTQ: if you’re not interested in asking questions in a conversational set-up,
    but rather just asking questions related to a table, which might involve aggregation,
    such as counting a number of rows, summing up cell values or averaging cell values.
    You can then for example ask “what’s the total number of goals Cristiano Ronaldo
    made in his career?“. This case is also called **weak supervision**, since the
    model itself must learn the appropriate aggregation operator (SUM/COUNT/AVERAGE/NONE)
    given only the answer to the question as supervision.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'WikiSQL-supervised: this dataset is based on WikiSQL with the model being given
    the ground truth aggregation operator during training. This is also called **strong
    supervision**. Here, learning the appropriate aggregation operator is much easier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Example dataset** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Conversational | SQA | Conversational, only cell selection questions |'
  prefs: []
  type: TYPE_TB
- en: '| Weak supervision for aggregation | WTQ | Questions might involve aggregation,
    and the model must learn this given only the answer as supervision |'
  prefs: []
  type: TYPE_TB
- en: '| Strong supervision for aggregation | WikiSQL-supervised | Questions might
    involve aggregation, and the model must learn this given the gold aggregation
    operator |'
  prefs: []
  type: TYPE_TB
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a model with a pre-trained base and randomly initialized classification
    heads from the hub can be done as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you don’t necessarily have to follow one of these three ways in
    which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters
    you want when initializing [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    and then create a [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    based on that configuration. For example, if you have a dataset that has both
    conversational questions and questions that might involve aggregation, then you
    can do it this way. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'Initializing a model with a pre-trained base and randomly initialized classification
    heads from the hub can be done as shown below. Be sure to have installed the [tensorflow_probability](https://github.com/tensorflow/probability)
    dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, you don’t necessarily have to follow one of these three ways in
    which TAPAS was fine-tuned. You can also experiment by defining any hyperparameters
    you want when initializing [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig),
    and then create a [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    based on that configuration. For example, if you have a dataset that has both
    conversational questions and questions that might involve aggregation, then you
    can do it this way. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: What you can also do is start from an already fine-tuned checkpoint. A note
    here is that the already fine-tuned checkpoint on WTQ has some issues due to the
    L2-loss which is somewhat brittle. See [here](https://github.com/google-research/tapas/issues/91#issuecomment-735719340)
    for more info.
  prefs: []
  type: TYPE_NORMAL
- en: For a list of all pre-trained and fine-tuned TAPAS checkpoints available on
    HuggingFace’s hub, see [here](https://huggingface.co/models?search=tapas).
  prefs: []
  type: TYPE_NORMAL
- en: '**STEP 2: Prepare your data in the SQA format**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, no matter what you picked above, you should prepare your dataset in
    the [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) format.
    This format is a TSV/CSV file with the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: optional, id of the table-question pair, for bookkeeping purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`annotator`: optional, id of the person who annotated the table-question pair,
    for bookkeeping purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`position`: integer indicating if the question is the first, second, third,…
    related to the table. Only required in case of conversational setup (SQA). You
    don’t need this column in case you’re going for WTQ/WikiSQL-supervised.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`question`: string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_file`: string, name of a csv file containing the tabular data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_coordinates`: list of one or more tuples (each tuple being a cell coordinate,
    i.e. row, column pair that is part of the answer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_text`: list of one or more strings (each string being a cell value
    that is part of the answer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_label`: index of the aggregation operator. Only required in case
    of strong supervision for aggregation (the WikiSQL-supervised case)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_answer`: the float answer to the question, if there is one (np.nan if
    there isn’t). Only required in case of weak supervision for aggregation (such
    as WTQ and WikiSQL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tables themselves should be present in a folder, each table being a separate
    csv file. Note that the authors of the TAPAS algorithm used conversion scripts
    with some automated logic to convert the other datasets (WTQ, WikiSQL) into the
    SQA format. The author explains this [here](https://github.com/google-research/tapas/issues/50#issuecomment-705465960).
    A conversion of this script that works with HuggingFace’s implementation can be
    found [here](https://github.com/NielsRogge/tapas_utils). Interestingly, these
    conversion scripts are not perfect (the `answer_coordinates` and `float_answer`
    fields are populated based on the `answer_text`), meaning that WTQ and WikiSQL
    results could actually be improved.
  prefs: []
  type: TYPE_NORMAL
- en: '**STEP 3: Convert your data into tensors using TapasTokenizer**'
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, given that you’ve prepared your data in this TSV/CSV format (and corresponding
    CSV files containing the tabular data), you can then use [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids`
    and so on. Again, based on which of the three cases you picked above, [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    requires different inputs to be fine-tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Required inputs** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Conversational | `input_ids`, `attention_mask`, `token_type_ids`, `labels`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weak supervision for aggregation | `input_ids`, `attention_mask`, `token_type_ids`,
    `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |'
  prefs: []
  type: TYPE_TB
- en: '| Strong supervision for aggregation | `input ids`, `attention mask`, `token
    type ids`, `labels`, `aggregation_labels` |'
  prefs: []
  type: TYPE_TB
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates the `labels`, `numeric_values` and `numeric_values_scale` based on the
    `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer`
    and `aggregation_labels` are already in the TSV file of step 2\. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    expects the data of the table to be **text-only**. You can use `.astype(str)`
    on a dataframe to turn it into text-only data. Of course, this only shows how
    to encode a single training example. It is advised to create a dataloader to iterate
    over batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, given that you’ve prepared your data in this TSV/CSV format (and corresponding
    CSV files containing the tabular data), you can then use [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    to convert table-question pairs into `input_ids`, `attention_mask`, `token_type_ids`
    and so on. Again, based on which of the three cases you picked above, [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    requires different inputs to be fine-tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Task** | **Required inputs** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Conversational | `input_ids`, `attention_mask`, `token_type_ids`, `labels`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Weak supervision for aggregation | `input_ids`, `attention_mask`, `token_type_ids`,
    `labels`, `numeric_values`, `numeric_values_scale`, `float_answer` |'
  prefs: []
  type: TYPE_TB
- en: '| Strong supervision for aggregation | `input ids`, `attention mask`, `token
    type ids`, `labels`, `aggregation_labels` |'
  prefs: []
  type: TYPE_TB
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates the `labels`, `numeric_values` and `numeric_values_scale` based on the
    `answer_coordinates` and `answer_text` columns of the TSV file. The `float_answer`
    and `aggregation_labels` are already in the TSV file of step 2\. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    expects the data of the table to be **text-only**. You can use `.astype(str)`
    on a dataframe to turn it into text-only data. Of course, this only shows how
    to encode a single training example. It is advised to create a dataloader to iterate
    over batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that here, we encode each table-question pair independently. This is fine
    as long as your dataset is **not conversational**. In case your dataset involves
    conversational questions (such as in SQA), then you should first group together
    the `queries`, `answer_coordinates` and `answer_text` per table (in the order
    of their `position` index) and batch encode each table with its questions. This
    will make sure that the `prev_labels` token types (see docs of [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    are set correctly. See [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    for more info. See [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    for more info regarding using the TensorFlow model.
  prefs: []
  type: TYPE_NORMAL
- en: '**STEP 4: Train (fine-tune) the model'
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then fine-tune [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    as follows (shown here for the weak supervision for aggregation case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then fine-tune [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    as follows (shown here for the weak supervision for aggregation case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Usage: inference'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: Here we explain how you can use [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    or [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    for inference (i.e. making predictions on new data). For inference, only `input_ids`,
    `attention_mask` and `token_type_ids` (which you can obtain using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    have to be provided to the model to obtain the logits. Next, you can use the handy
    `~models.tapas.tokenization_tapas.convert_logits_to_predictions` method to convert
    these into predicted coordinates and optional aggregation indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, note that inference is **different** depending on whether or not the
    setup is conversational. In a non-conversational set-up, inference can be done
    in parallel on all table-question pairs of a batch. Here’s an example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: Here we explain how you can use [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    for inference (i.e. making predictions on new data). For inference, only `input_ids`,
    `attention_mask` and `token_type_ids` (which you can obtain using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer))
    have to be provided to the model to obtain the logits. Next, you can use the handy
    `~models.tapas.tokenization_tapas.convert_logits_to_predictions` method to convert
    these into predicted coordinates and optional aggregation indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, note that inference is **different** depending on whether or not the
    setup is conversational. In a non-conversational set-up, inference can be done
    in parallel on all table-question pairs of a batch. Here’s an example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In case of a conversational set-up, then each table-question pair must be provided
    **sequentially** to the model, such that the `prev_labels` token types can be
    overwritten by the predicted `labels` of the previous table-question pair. Again,
    more info can be found in [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    (for PyTorch) and [this notebook](https://github.com/kamalkraj/Tapas-Tutorial/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)
    (for TensorFlow).
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text classification task guide](../tasks/sequence_classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Masked language modeling task guide](../tasks/masked_language_modeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TAPAS specific outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L97)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    (and possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_aggregation` (`torch.FloatTensor`, *optional*, of shape `(batch_size,
    num_aggregation_labels)`) — Prediction scores of the aggregation head, for every
    aggregation operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output type of [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering).
  prefs: []
  type: TYPE_NORMAL
- en: TapasConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/configuration_tapas.py#L45)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 30522) — Vocabulary size of the
    TAPAS model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 768) — Dimensionality of the
    encoder layers and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_hidden_layers` (`int`, *optional*, defaults to 12) — Number of hidden
    layers in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_attention_heads` (`int`, *optional*, defaults to 12) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intermediate_size` (`int`, *optional*, defaults to 3072) — Dimensionality
    of the “intermediate” (often named feed-forward) layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_act` (`str` or `Callable`, *optional*, defaults to `"gelu"`) — The
    non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"swish"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_dropout_prob` (`float`, *optional*, defaults to 0.1) — The dropout
    probability for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_probs_dropout_prob` (`float`, *optional*, defaults to 0.1) — The
    dropout ratio for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_position_embeddings` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model might ever be used with. Typically set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type_vocab_sizes` (`List[int]`, *optional*, defaults to `[3, 256, 256, 2,
    256, 256, 10]`) — The vocabulary sizes of the `token_type_ids` passed when calling
    [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_eps` (`float`, *optional*, defaults to 1e-12) — The epsilon used
    by the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`positive_label_weight` (`float`, *optional*, defaults to 10.0) — Weight for
    positive labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_aggregation_labels` (`int`, *optional*, defaults to 0) — The number of
    aggregation operators to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_loss_weight` (`float`, *optional*, defaults to 1.0) — Importance
    weight for the aggregation loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_answer_as_supervision` (`bool`, *optional*) — Whether to use the answer
    as the only supervision for aggregation examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_loss_importance` (`float`, *optional*, defaults to 1.0) — Importance
    weight for the regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_normalized_answer_loss` (`bool`, *optional*, defaults to `False`) — Whether
    to normalize the answer loss by the maximum of the predicted and expected value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huber_loss_delta` (`float`, *optional*) — Delta parameter used to calculate
    the regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature` (`float`, *optional*, defaults to 1.0) — Value used to control
    (OR change) the skewness of cell logits probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_temperature` (`float`, *optional*, defaults to 1.0) — Scales aggregation
    logits to control the skewness of probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_gumbel_for_cells` (`bool`, *optional*, defaults to `False`) — Whether
    to apply Gumbel-Softmax to cell selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_gumbel_for_aggregation` (`bool`, *optional*, defaults to `False`) — Whether
    to apply Gumbel-Softmax to aggregation selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`average_approximation_function` (`string`, *optional*, defaults to `"ratio"`)
    — Method to calculate the expected average of cells in the weak supervision case.
    One of `"ratio"`, `"first_order"` or `"second_order"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cell_selection_preference` (`float`, *optional*) — Preference for cell selection
    in ambiguous cases. Only applicable in case of weak supervision for aggregation
    (WTQ, WikiSQL). If the total mass of the aggregation probabilities (excluding
    the “NONE” operator) is higher than this hyperparameter, then aggregation is predicted
    for an example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_loss_cutoff` (`float`, *optional*) — Ignore examples with answer loss
    larger than cutoff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_num_rows` (`int`, *optional*, defaults to 64) — Maximum number of rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_num_columns` (`int`, *optional*, defaults to 32) — Maximum number of columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`average_logits_per_cell` (`bool`, *optional*, defaults to `False`) — Whether
    to average logits per cell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`select_one_column` (`bool`, *optional*, defaults to `True`) — Whether to constrain
    the model to only select cells from a single column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`allow_empty_column_selection` (`bool`, *optional*, defaults to `False`) —
    Whether to allow not to select any column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_cell_selection_weights_to_zero` (`bool`, *optional*, defaults to `False`)
    — Whether to initialize cell selection weights to 0 so that the initial probabilities
    are 50%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reset_position_index_per_cell` (`bool`, *optional*, defaults to `True`) —
    Whether to restart position indexes at every cell (i.e. use relative position
    embeddings).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`disable_per_token_loss` (`bool`, *optional*, defaults to `False`) — Whether
    to disable any (strong or weak) supervision on cells.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_labels` (`Dict[int, label]`, *optional*) — The aggregation labels
    used to aggregate the results. For example, the WTQ models have the following
    aggregation labels: `{0: "NONE", 1: "SUM", 2: "AVERAGE", 3: "COUNT"}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`no_aggregation_label_index` (`int`, *optional*) — If the aggregation labels
    are defined and one of these labels represents “No aggregation”, this should be
    set to its index. For example, the WTQ models have the “NONE” aggregation label
    at index 0, so that value should be set to 0 for these models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel).
    It is used to instantiate a TAPAS model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the TAPAS [google/tapas-base-finetuned-sqa](https://huggingface.co/google/tapas-base-finetuned-sqa)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from `PreTrainedConfig` and can be used to control
    the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters additional to BERT are taken from run_task_main.py and hparam_utils.py
    of the original implementation. Original implementation available at [https://github.com/google-research/tapas/tree/master](https://github.com/google-research/tapas/tree/master).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: TapasTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L237)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_lower_case` (`bool`, *optional*, defaults to `True`) — Whether or not to
    lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_basic_tokenize` (`bool`, *optional*, defaults to `True`) — Whether or not
    to do basic tokenization before WordPiece.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`never_split` (`Iterable`, *optional*) — Collection of tokens which will never
    be split during tokenization. Only has an effect when `do_basic_tokenize=True`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"[UNK]"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep_token` (`str`, *optional*, defaults to `"[SEP]"`) — The separator token,
    which is used when building a sequence from multiple sequences, e.g. two sequences
    for sequence classification or for a text and a question for question answering.
    It is also used as the last token of a sequence built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"[PAD]"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cls_token` (`str`, *optional*, defaults to `"[CLS]"`) — The classifier token
    which is used when doing sequence classification (classification of the whole
    sequence instead of per-token classification). It is the first token of the sequence
    when built with special tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask_token` (`str`, *optional*, defaults to `"[MASK]"`) — The token used for
    masking values. This is the token used when training this model with masked language
    modeling. This is the token which the model will try to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`empty_token` (`str`, *optional*, defaults to `"[EMPTY]"`) — The token used
    for empty cell values in a table. Empty cell values include "", “n/a”, “nan” and
    ”?“.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenize_chinese_chars` (`bool`, *optional*, defaults to `True`) — Whether
    or not to tokenize Chinese characters. This should likely be deactivated for Japanese
    (see this [issue](https://github.com/huggingface/transformers/issues/328)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strip_accents` (`bool`, *optional*) — Whether or not to strip all accents.
    If this option is not specified, then it will be determined by the value for `lowercase`
    (as in the original BERT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cell_trim_length` (`int`, *optional*, defaults to -1) — If > 0: Trim cells
    so that the length is <= this value. Also disables further cell trimming, should
    thus be used with `truncation` set to `True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_column_id` (`int`, *optional*) — Max column id to extract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_row_id` (`int`, *optional*) — Max row id to extract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strip_column_names` (`bool`, *optional*, defaults to `False`) — Whether to
    add empty strings instead of column names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_answer_coordinates` (`bool`, *optional*, defaults to `False`) — Whether
    to recompute the answer coordinates from the answer text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_question_length` (`int`, *optional*) — Minimum length of each question
    in terms of tokens (will be skipped otherwise).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_question_length` (`int`, *optional*) — Maximum length of each question
    in terms of tokens (will be skipped otherwise).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a TAPAS tokenizer. Based on WordPiece. Flattens a table and one or
    more related sentences to be used by TAPAS models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods. [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    creates several token type ids to encode tabular structure. To be more precise,
    it adds 7 token type ids, in the following order: `segment_ids`, `column_ids`,
    `row_ids`, `prev_labels`, `column_ranks`, `inv_column_ranks` and `numeric_relations`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'segment_ids: indicate whether a token belongs to the question (0) or the table
    (1). 0 for special tokens and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'column_ids: indicate to which column of the table a token belongs (starting
    from 1). Is 0 for all question tokens, special tokens and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'row_ids: indicate to which row of the table a token belongs (starting from
    1). Is 0 for all question tokens, special tokens and padding. Tokens of column
    headers are also 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'prev_labels: indicate whether a token was (part of) an answer to the previous
    question (1) or not (0). Useful in a conversational setup (such as SQA).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'column_ranks: indicate the rank of a table token relative to a column, if applicable.
    For example, if you have a column “number of movies” with values 87, 53 and 69,
    then the column ranks of these tokens are 3, 1 and 2 respectively. 0 for all question
    tokens, special tokens and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'inv_column_ranks: indicate the inverse rank of a table token relative to a
    column, if applicable. For example, if you have a column “number of movies” with
    values 87, 53 and 69, then the inverse column ranks of these tokens are 1, 3 and
    2 respectively. 0 for all question tokens, special tokens and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'numeric_relations: indicate numeric relations between the question and the
    tokens of the table. 0 for all question tokens, special tokens and padding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer)
    runs end-to-end tokenization on a table and associated sentences: punctuation
    splitting and wordpiece.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L588)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`table` (`pd.DataFrame`) — Table containing tabular data. Note that all cell
    values must be text. Use *.astype(str)* on a Pandas dataframe to convert it to
    string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`queries` (`str` or `List[str]`) — Question or batch of questions related to
    a table to be encoded. Note that in case of a batch, all questions must refer
    to the **same** table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_coordinates` (`List[Tuple]` or `List[List[Tuple]]`, *optional*) — Answer
    coordinates of each table-question pair in the batch. In case only a single table-question
    pair is provided, then the answer_coordinates must be a single list of one or
    more tuples. Each tuple must be a (row_index, column_index) pair. The first data
    row (not the column header row) has index 0\. The first column has index 0\. In
    case a batch of table-question pairs is provided, then the answer_coordinates
    must be a list of lists of tuples (each list corresponding to a single table-question
    pair).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`answer_text` (`List[str]` or `List[List[str]]`, *optional*) — Answer text
    of each table-question pair in the batch. In case only a single table-question
    pair is provided, then the answer_text must be a single list of one or more strings.
    Each string must be the answer text of a corresponding answer coordinate. In case
    a batch of table-question pairs is provided, then the answer_coordinates must
    be a list of lists of strings (each list corresponding to a single table-question
    pair).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens` (`bool`, *optional*, defaults to `True`) — Whether or
    not to encode the sequences with the special tokens relative to their model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `False`) — Activates and controls padding. Accepts the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`, `str` or `TapasTruncationStrategy`, *optional*, defaults
    to `False`) — Activates and controls truncation. Accepts the following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''drop_rows_to_fit''`: Truncate to a maximum length specified with
    the argument `max_length` or to the maximum acceptable input length for the model
    if that argument is not provided. This will truncate row by row, removing rows
    from the table.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_truncate''` (default): No truncation (i.e., can output
    batch with sequence lengths greater than the model maximum admissible input size).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Controls the maximum length to use by one
    of the truncation/padding parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If left unset or set to `None`, this will use the predefined model maximum length
    if a maximum length is required by one of the truncation/padding parameters. If
    the model has no specific maximum input length (like XLNet) truncation/padding
    to a maximum length will be deactivated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`is_split_into_words` (`bool`, *optional*, defaults to `False`) — Whether or
    not the input is already pre-tokenized (e.g., split into words). If set to `True`,
    the tokenizer assumes the input is already split into words (for instance, by
    splitting it on whitespace) which it will tokenize. This is useful for NER or
    token classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value. This is especially useful to enable the use
    of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to tokenize and prepare for the model one or several sequence(s)
    related to a table.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `convert_logits_to_predictions`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L1951)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`data` (`dict`) — Dictionary mapping features to actual values. Should be created
    using [TapasTokenizer](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.Tensor` or `tf.Tensor` of shape `(batch_size, sequence_length)`)
    — Tensor containing the logits at the token level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_agg` (`torch.Tensor` or `tf.Tensor` of shape `(batch_size, num_aggregation_labels)`,
    *optional*) — Tensor containing the aggregation logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cell_classification_threshold` (`float`, *optional*, defaults to 0.5) — Threshold
    to be used for cell selection. All table cells for which their probability is
    larger than this threshold will be selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`tuple` comprising various elements depending on the inputs'
  prefs: []
  type: TYPE_NORMAL
- en: 'predicted_answer_coordinates (`List[List[[tuple]]` of length `batch_size`):
    Predicted answer coordinates as a list of lists of tuples. Each element in the
    list contains the predicted answer coordinates of a single example in the batch,
    as a list of tuples. Each tuple is a cell, i.e. (row index, column index).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'predicted_aggregation_indices (`List[int]`of length `batch_size`, *optional*,
    returned when `logits_aggregation` is provided): Predicted aggregation operator
    indices of the aggregation head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts logits of [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    to actual predicted answer coordinates and optional aggregation indices.
  prefs: []
  type: TYPE_NORMAL
- en: The original implementation, on which this function is based, can be found [here](https://github.com/google-research/tapas/blob/4908213eb4df7aa988573350278b44c4dbe3f71b/tapas/experiments/prediction_utils.py#L288).
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/tokenization_tapas.py#L456)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: TapasModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L835)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Tapas Model transformer outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: This class is a small change compared to [BertModel](/docs/transformers/v4.37.2/en/model_doc/bert#transformers.BertModel),
    taking into account the additional token type ids.
  prefs: []
  type: TYPE_NORMAL
- en: The model can behave as an encoder (with only self-attention) as well as a decoder,
    in which case a layer of cross-attention is added between the self-attention layers,
    following the architecture described in [Attention is all you need](https://arxiv.org/abs/1706.03762)
    by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser and Illia Polosukhin.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L876)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.BaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`torch.FloatTensor` of shape `(batch_size, hidden_size)`)
    — Last layer hidden-state of the first token of the sequence (classification token)
    after further processing through the layers used for the auxiliary pretraining
    task. E.g. for BERT-family of models, this returns the classification token after
    processing through a linear layer and a tanh activation function. The linear layer
    weights are trained from the next sentence prediction (classification) objective
    during pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: TapasForMaskedLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasForMaskedLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L991)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a `language modeling` head on top. This model inherits from
    [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1012)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should be in
    `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices
    set to `-100` are ignored (masked), the loss is only computed for the tokens with
    labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.MaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Masked language modeling (MLM) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TapasForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForMaskedLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: TapasForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasForSequenceClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1446)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a sequence classification head on top (a linear layer on top
    of the pooled output), e.g. for table entailment tasks, such as TabFact (Chen
    et al., 2020).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1465)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy). Note: this is called “classification_class_index”
    in the original implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.SequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) —
    Classification (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings, if the model has an embedding layer, +
    one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the optional initial
    embedding outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TapasForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: TapasForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TapasForQuestionAnswering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1100)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a cell selection head and optional aggregation head on top
    for question-answering tasks on tables (linear layers on top of the hidden-states
    output to compute `logits` and optional `logits_aggregation`), e.g. for SQA, WTQ
    or WikiSQL-supervised tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its models (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tapas.py#L1143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) —
    Indices of input sequence tokens in the vocabulary. Indices can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`,
    *optional*) — Mask to nullify selected heads of the self-attention modules. Mask
    values selected in `[0, 1]`: - 1 indicates the head is **not masked**, - 0 indicates
    the head is **masked**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_mask` (`torch.LongTensor` of shape `(batch_size, seq_length)`, *optional*)
    — Mask for the table. Indicates which tokens belong to the table (1). Question
    tokens, table headers and padding are 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, seq_length)`, *optional*)
    — Labels per token for computing the hierarchical cell selection loss. This encodes
    the positions of the answer appearing in the table. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `part of the answer`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not part of the answer`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_labels` (`torch.LongTensor` of shape `(batch_size, )`, *optional*)
    — Aggregation function index for every example in the batch for computing the
    aggregation loss. Indices should be in `[0, ..., config.num_aggregation_labels
    - 1]`. Only required in case of strong supervision for aggregation (WikiSQL-supervised).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_answer` (`torch.FloatTensor` of shape `(batch_size, )`, *optional*)
    — Float answer for every example in the batch. Set to *float(‘nan’)* for cell
    selection questions. Only required in case of weak supervision (WTQ) to calculate
    the aggregate mask and regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values` (`torch.FloatTensor` of shape `(batch_size, seq_length)`,
    *optional*) — Numeric values of every token, NaN for tokens which are not numeric
    values. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values_scale` (`torch.FloatTensor` of shape `(batch_size, seq_length)`,
    *optional*) — Scale of the numeric values of every token. Can be obtained using
    [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.models.tapas.modeling_tapas.TableQuestionAnsweringOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    (and possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_aggregation` (`torch.FloatTensor`, *optional*, of shape `(batch_size,
    num_aggregation_labels)`) — Prediction scores of the aggregation head, for every
    aggregation operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFTapasModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1108)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Tapas Model transformer outputting raw hidden-states without any specific
    head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1118)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pooler_output` (`tf.Tensor` of shape `(batch_size, hidden_size)`) — Last layer
    hidden-state of the first token of the sequence (classification token) further
    processed by a Linear layer and a Tanh activation function. The Linear layer weights
    are trained from the next sentence prediction (classification) objective during
    pretraining.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This output is usually *not* a good summary of the semantic content of the input,
    you’re often better with averaging or pooling the sequence of hidden-states for
    the whole input sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasModel](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: TFTapasForMaskedLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForMaskedLM`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1183)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a `language modeling` head on top.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1200)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`,
    *optional*) — Labels for computing the masked language modeling loss. Indices
    should be in `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFMaskedLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) — Masked language modeling (MLM) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasForMaskedLM](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForMaskedLM)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: TFTapasForSequenceClassification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForSequenceClassification`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1730)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a sequence classification head on top (a linear layer on top
    of the pooled output), e.g. for table entailment tasks, such as TabFact (Chen
    et al., 2020).
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1749)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    num_choices, sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Mask to avoid performing attention on padding
    token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length, 7)`, *optional*) — Token indices that encode tabular structure.
    Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length)`, *optional*) — Indices of positions of each input sequence tokens
    in the position embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, num_choices,
    sequence_length, hidden_size)`, *optional*) — Optionally, instead of passing `input_ids`
    you can choose to directly pass an embedded representation. This is useful if
    you want more control over how to convert `input_ids` indices into associated
    vectors than the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size,)`, *optional*) — Labels
    for computing the sequence classification/regression loss. Indices should be in
    `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss
    is computed (Mean-Square loss), If `config.num_labels > 1` a classification loss
    is computed (Cross-Entropy). Note: this is called “classification_class_index”
    in the original implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFSequenceClassifierOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(batch_size, )`, *optional*, returned when `labels`
    is provided) — Classification (or regression if config.num_labels==1) loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, config.num_labels)`) — Classification
    (or regression if config.num_labels==1) scores (before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFTapasForSequenceClassification](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForSequenceClassification)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: TFTapasForQuestionAnswering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFTapasForQuestionAnswering`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1387)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tapas Model with a cell selection head and optional aggregation head on top
    for question-answering tasks on tables (linear layers on top of the hidden-states
    output to compute `logits` and optional `logits_aggregation`), e.g. for SQA, WTQ
    or WikiSQL-supervised tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/tapas/modeling_tf_tapas.py#L1417)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_ids` (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]`
    or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size,
    sequence_length)`) — Indices of input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    and [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are input IDs?](../glossary#input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attention_mask` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing attention on padding token indices. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`token_type_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    7)`, *optional*) — Token indices that encode tabular structure. Indices can be
    obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    See this class for more info.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are token type IDs?](../glossary#token-type-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`position_ids` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Indices of positions of each input sequence tokens in the position
    embeddings. If `reset_position_index_per_cell` of [TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig)
    is set to `True`, relative position embeddings will be used. Selected in the range
    `[0, config.max_position_embeddings - 1]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are position IDs?](../glossary#position-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers,
    num_heads)`, *optional*) — Mask to nullify selected heads of the self-attention
    modules. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs_embeds` (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `input_ids` you can
    choose to directly pass an embedded representation. This is useful if you want
    more control over how to convert `input_ids` indices into associated vectors than
    the model’s internal embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False“) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table_mask` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Mask for the table. Indicates which tokens belong to the table (1). Question
    tokens, table headers and padding are 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*) — Labels
    per token for computing the hierarchical cell selection loss. This encodes the
    positions of the answer appearing in the table. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `part of the answer`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `not part of the answer`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregation_labels` (`tf.Tensor` of shape `(batch_size, )`, *optional*) —
    Aggregation function index for every example in the batch for computing the aggregation
    loss. Indices should be in `[0, ..., config.num_aggregation_labels - 1]`. Only
    required in case of strong supervision for aggregation (WikiSQL-supervised).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float_answer` (`tf.Tensor` of shape `(batch_size, )`, *optional*) — Float
    answer for every example in the batch. Set to *float(‘nan’)* for cell selection
    questions. Only required in case of weak supervision (WTQ) to calculate the aggregate
    mask and regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Numeric values of every token, NaN for tokens which are not numeric values.
    Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numeric_values_scale` (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*)
    — Scale of the numeric values of every token. Can be obtained using [AutoTokenizer](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer).
    Only required in case of weak supervision for aggregation (WTQ) to calculate the
    regression loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.tapas.modeling_tf_tapas.TFTableQuestionAnsweringOutput`
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.tapas.modeling_tf_tapas.TFTableQuestionAnsweringOutput`
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([TapasConfig](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TapasConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` (and
    possibly `answer`, `aggregation_labels`, `numeric_values` and `numeric_values_scale`
    are provided)) — Total loss as the sum of the hierarchical cell selection log-likelihood
    loss and (optionally) the semi-supervised regression loss and (optionally) supervised
    loss for aggregations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length)`) — Prediction
    scores of the cell selection head, for every token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits_aggregation` (`tf.Tensor`, *optional*, of shape `(batch_size, num_aggregation_labels)`)
    — Prediction scores of the aggregation head, for every aggregation operator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the
    output of each layer plus the initial embedding outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.
    Attentions weights after the attention softmax, used to compute the weighted average
    in the self-attention heads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [TFTapasForQuestionAnswering](/docs/transformers/v4.37.2/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
