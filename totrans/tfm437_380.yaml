- en: Decision Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Decision Transformer
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原始文本：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer)
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: 'The Decision Transformer model was proposed in [Decision Transformer: Reinforcement
    Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 'Decision Transformer模型提出于[Decision Transformer: Reinforcement Learning via
    Sequence Modeling](https://arxiv.org/abs/2106.01345)'
- en: by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
    Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作者为Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
    Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch。
- en: 'The abstract from the paper is the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文的摘要如下：
- en: '*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence
    modeling problem. This allows us to draw upon the simplicity and scalability of
    the Transformer architecture, and associated advances in language modeling such
    as GPT-x and BERT. In particular, we present Decision Transformer, an architecture
    that casts the problem of RL as conditional sequence modeling. Unlike prior approaches
    to RL that fit value functions or compute policy gradients, Decision Transformer
    simply outputs the optimal actions by leveraging a causally masked Transformer.
    By conditioning an autoregressive model on the desired return (reward), past states,
    and actions, our Decision Transformer model can generate future actions that achieve
    the desired return. Despite its simplicity, Decision Transformer matches or exceeds
    the performance of state-of-the-art model-free offline RL baselines on Atari,
    OpenAI Gym, and Key-to-Door tasks.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们引入了一个将强化学习（RL）抽象为序列建模问题的框架。这使我们能够利用Transformer架构的简单性和可扩展性，以及与语言建模相关的进展，如GPT-x和BERT。特别是，我们提出了Decision
    Transformer，这是一个将RL问题转化为条件序列建模的架构。与先前拟合值函数或计算策略梯度的RL方法不同，Decision Transformer通过利用因果屏蔽的Transformer简单地输出最佳动作。通过将自回归模型条件于期望的回报（奖励）、过去状态和动作，我们的Decision
    Transformer模型可以生成实现期望回报的未来动作。尽管简单，Decision Transformer在Atari、OpenAI Gym和Key-to-Door任务上与最先进的无模型离线RL基线模型的性能相匹配或超越。*'
- en: This version of the model is for tasks where the state is a vector.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型版本适用于状态为向量的任务。
- en: This model was contributed by [edbeeching](https://huggingface.co/edbeeching).
    The original code can be found [here](https://github.com/kzl/decision-transformer).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由[edbeeching](https://huggingface.co/edbeeching)贡献。原始代码可在[此处](https://github.com/kzl/decision-transformer)找到。
- en: DecisionTransformerConfig
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DecisionTransformerConfig
- en: '### `class transformers.DecisionTransformerConfig`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DecisionTransformerConfig`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/configuration_decision_transformer.py#L31)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/configuration_decision_transformer.py#L31)'
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Parameters
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`state_dim` (`int`, *optional*, defaults to 17) — The state size for the RL
    environment'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_dim` (`int`, *optional*, 默认为17) — RL环境的状态大小'
- en: '`act_dim` (`int`, *optional*, defaults to 4) — The size of the output action
    space'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act_dim` (`int`, *optional*, 默认为4) — 输出动作空间的大小'
- en: '`hidden_size` (`int`, *optional*, defaults to 128) — The size of the hidden
    layers'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size` (`int`, *optional*, 默认为128) — 隐藏层的大小'
- en: '`max_ep_len` (`int`, *optional*, defaults to 4096) — The maximum length of
    an episode in the environment'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_ep_len` (`int`, *optional*, 默认为4096) — 环境中一个episode的最大长度'
- en: '`action_tanh` (`bool`, *optional*, defaults to True) — Whether to use a tanh
    activation on action prediction'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action_tanh` (`bool`, *optional*, 默认为True) — 是否在动作预测上使用tanh激活'
- en: '`vocab_size` (`int`, *optional*, defaults to 50257) — Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size` (`int`, *optional*, 默认为50257) — GPT-2模型的词汇大小。定义了在调用[DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel)时可以表示的不同标记数量。'
- en: '`n_positions` (`int`, *optional*, defaults to 1024) — The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_positions` (`int`, *optional*, 默认为1024) — 该模型可能会使用的最大序列长度。通常将其设置为较大的值以防万一（例如512、1024或2048）。'
- en: '`n_layer` (`int`, *optional*, defaults to 3) — Number of hidden layers in the
    Transformer encoder.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_layer` (`int`, *optional*, 默认为3) — Transformer编码器中隐藏层的数量。'
- en: '`n_head` (`int`, *optional*, defaults to 1) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_head` (`int`, *optional*, 默认为1) — Transformer编码器中每个注意力层的注意力头数。'
- en: '`n_inner` (`int`, *optional*) — Dimensionality of the inner feed-forward layers.
    If unset, will default to 4 times `n_embd`.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_inner` (`int`, *optional*) — 内部前馈层的维度。如果未设置，将默认为`n_embd`的4倍。'
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) — Activation
    function, to be selected in the list `["relu", "silu", "gelu", "tanh", "gelu_new"]`.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation_function` (`str`, *optional*, 默认为`"gelu"`) — 激活函数，可在列表`["relu",
    "silu", "gelu", "tanh", "gelu_new"]`中选择。'
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resid_pdrop` (`float`, *optional*, 默认为0.1) — 嵌入层、编码器和池化器中所有全连接层的丢失概率。'
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — The dropout ratio for the
    embeddings.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embd_pdrop` (`int`, *optional*, 默认为0.1) — 嵌入的丢失比例。'
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the attention.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attn_pdrop` (`float`, *optional*, 默认为0.1) — 注意力的丢失比例。'
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-5) — The epsilon
    to use in the layer normalization layers.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_norm_epsilon` (`float`, *optional*, 默认为1e-5) — 在层归一化层中使用的epsilon值。'
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initializer_range` (`float`, *可选*, 默认为 0.02) — 用于初始化所有权重矩阵的截断正态初始化器的标准差。'
- en: '`scale_attn_weights` (`bool`, *optional*, defaults to `True`) — Scale attention
    weights by dividing by sqrt(hidden_size)..'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_weights` (`bool`, *可选*, 默认为 `True`) — 通过除以sqrt(hidden_size)来缩放注意力权重。'
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_cache` (`bool`, *可选*, 默认为 `True`) — 模型是否应返回最后的键/值注意力（并非所有模型都使用）。'
- en: '`scale_attn_by_inverse_layer_idx` (`bool`, *optional*, defaults to `False`)
    — Whether to additionally scale attention weights by `1 / layer_idx + 1`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale_attn_by_inverse_layer_idx` (`bool`, *可选*, 默认为 `False`) — 是否通过`1 / layer_idx
    + 1`额外缩放注意力权重。'
- en: '`reorder_and_upcast_attn` (`bool`, *optional*, defaults to `False`) — Whether
    to scale keys (K) prior to computing attention (dot-product) and upcast attention
    dot-product/softmax to float() when training with mixed precision.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reorder_and_upcast_attn` (`bool`, *可选*, 默认为 `False`) — 是否在计算注意力（点积）之前缩放键（K），并在使用混合精度训练时将注意力点积/softmax向上转换为float()。'
- en: This is the configuration class to store the configuration of a [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel).
    It is used to instantiate a Decision Transformer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the standard DecisionTransformer
    architecture. Many of the config options are used to instatiate the GPT2 model
    that is used as part of the architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于存储[DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel)配置的配置类。它用于根据指定的参数实例化一个决策变换器模型，定义模型架构。使用默认值实例化配置将产生类似于标准DecisionTransformer架构的配置。许多配置选项用于实例化作为架构一部分使用的GPT2模型。
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象继承自[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)，可用于控制模型输出。阅读[PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)的文档以获取更多信息。
- en: 'Example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DecisionTransformerGPT2Model
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DecisionTransformerGPT2Model
- en: '### `class transformers.DecisionTransformerGPT2Model`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DecisionTransformerGPT2Model`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L473)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L473)'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#### `forward`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L503)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L503)'
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: DecisionTransformerModel
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DecisionTransformerModel
- en: '### `class transformers.DecisionTransformerModel`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '### `class transformers.DecisionTransformerModel`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L783)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L783)'
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Parameters
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`config` ([~DecisionTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config` ([~DecisionTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig))
    — 具有模型所有参数的模型配置类。使用配置文件初始化不会加载与模型关联的权重，只会加载配置。查看[from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)方法以加载模型权重。'
- en: The Decision Transformer Model This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 决策变换器模型 这个模型是一个PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)子类。将其用作常规的PyTorch模块，并参考PyTorch文档以获取与一般用法和行为相关的所有内容。
- en: 'The model builds upon the GPT2 architecture to perform autoregressive prediction
    of actions in an offline RL setting. Refer to the paper for more details: [https://arxiv.org/abs/2106.01345](https://arxiv.org/abs/2106.01345)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于GPT2架构，在离线RL设置中执行动作的自回归预测。更多细节请参考论文：[https://arxiv.org/abs/2106.01345](https://arxiv.org/abs/2106.01345)
- en: '#### `forward`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#### `forward`'
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L817)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[<来源>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L817)'
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 参数
- en: '`states` (`torch.FloatTensor` of shape `(batch_size, episode_length, state_dim)`)
    — The states for each step in the trajectory'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`states` (`torch.FloatTensor`，形状为`(batch_size, episode_length, state_dim)`)
    — 轨迹中每个步骤的状态'
- en: '`actions` (`torch.FloatTensor` of shape `(batch_size, episode_length, act_dim)`)
    — The actions taken by the “expert” policy for the current state, these are masked
    for auto regressive prediction'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`actions` (`torch.FloatTensor`，形状为`(batch_size, episode_length, act_dim)`)
    — “专家”策略对当前状态采取的动作，这些动作用于自回归预测'
- en: '`rewards` (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`)
    — The rewards for each state, action'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewards` (`torch.FloatTensor`，形状为`(batch_size, episode_length, 1)`) — 每个状态、动作的奖励'
- en: '`returns_to_go` (`torch.FloatTensor` of shape `(batch_size, episode_length,
    1)`) — The returns for each state in the trajectory'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`returns_to_go`（形状为`(batch_size, episode_length, 1)`的`torch.FloatTensor`）-
    轨迹中每个状态的回报'
- en: '`timesteps` (`torch.LongTensor` of shape `(batch_size, episode_length)`) —
    The timestep for each step in the trajectory'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timesteps`（形状为`(batch_size, episode_length)`的`torch.LongTensor`）- 轨迹中每一步的时间步长'
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, episode_length)`)
    — Masking, used to mask the actions when performing autoregressive prediction'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`（形状为`(batch_size, episode_length)`的`torch.FloatTensor`）- 掩码，用于在执行自回归预测时屏蔽动作'
- en: Returns
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 返回
- en: '`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`
    or `tuple(torch.FloatTensor)`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`或`tuple(torch.FloatTensor)`'
- en: A `transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DecisionTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig))
    and inputs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`或一个`torch.FloatTensor`元组（如果传递`return_dict=False`或`config.return_dict=False`时）包含各种元素，取决于配置（`DecisionTransformerConfig`）和输入。
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`last_hidden_state`（形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`）-
    模型最后一层的隐藏状态序列。'
- en: '`state_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    state_dim)`) — Environment state predictions'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state_preds`（形状为`(batch_size, sequence_length, state_dim)`的`torch.FloatTensor`）-
    环境状态预测'
- en: '`action_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    action_dim)`) — Model action predictions'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action_preds`（形状为`(batch_size, sequence_length, action_dim)`的`torch.FloatTensor`）-
    模型动作预测'
- en: '`return_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    1)`) — Predicted returns for each state'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_preds`（形状为`(batch_size, sequence_length, 1)`的`torch.FloatTensor`）-
    每个状态的预测回报'
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_states`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_hidden_states=True`或`config.output_hidden_states=True`时返回）-
    形状为`(batch_size, sequence_length, hidden_size)`的`torch.FloatTensor`元组（一个用于嵌入的输出
    + 一个用于每一层的输出）。'
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型在每一层输出的隐藏状态加上初始嵌入输出。
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attentions`（`tuple(torch.FloatTensor)`，*可选*，当传递`output_attentions=True`或`config.output_attentions=True`时返回）-
    形状为`(batch_size, num_heads, sequence_length, sequence_length)`的`torch.FloatTensor`元组（每层一个）。'
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力softmax后的注意力权重，用于计算自注意力头中的加权平均值。
- en: The [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel)
    forward method, overrides the `__call__` special method.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTransformerModel`的`forward`方法，覆盖了`__call__`特殊方法。'
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前向传递的步骤需要在此函数内定义，但应该在此之后调用`Module`实例，而不是这个，因为前者会处理运行前后处理步骤，而后者会默默地忽略它们。
- en: 'Examples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
