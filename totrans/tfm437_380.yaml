- en: Decision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/decision_transformer)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/104.b6d0b693.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Decision Transformer model was proposed in [Decision Transformer: Reinforcement
    Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)'
  prefs: []
  type: TYPE_NORMAL
- en: by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
    Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The abstract from the paper is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence
    modeling problem. This allows us to draw upon the simplicity and scalability of
    the Transformer architecture, and associated advances in language modeling such
    as GPT-x and BERT. In particular, we present Decision Transformer, an architecture
    that casts the problem of RL as conditional sequence modeling. Unlike prior approaches
    to RL that fit value functions or compute policy gradients, Decision Transformer
    simply outputs the optimal actions by leveraging a causally masked Transformer.
    By conditioning an autoregressive model on the desired return (reward), past states,
    and actions, our Decision Transformer model can generate future actions that achieve
    the desired return. Despite its simplicity, Decision Transformer matches or exceeds
    the performance of state-of-the-art model-free offline RL baselines on Atari,
    OpenAI Gym, and Key-to-Door tasks.*'
  prefs: []
  type: TYPE_NORMAL
- en: This version of the model is for tasks where the state is a vector.
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [edbeeching](https://huggingface.co/edbeeching).
    The original code can be found [here](https://github.com/kzl/decision-transformer).
  prefs: []
  type: TYPE_NORMAL
- en: DecisionTransformerConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DecisionTransformerConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/configuration_decision_transformer.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`state_dim` (`int`, *optional*, defaults to 17) — The state size for the RL
    environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`act_dim` (`int`, *optional*, defaults to 4) — The size of the output action
    space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_size` (`int`, *optional*, defaults to 128) — The size of the hidden
    layers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_ep_len` (`int`, *optional*, defaults to 4096) — The maximum length of
    an episode in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action_tanh` (`bool`, *optional*, defaults to True) — Whether to use a tanh
    activation on action prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 50257) — Vocabulary size of the
    GPT-2 model. Defines the number of different tokens that can be represented by
    the `inputs_ids` passed when calling [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_positions` (`int`, *optional*, defaults to 1024) — The maximum sequence
    length that this model might ever be used with. Typically set this to something
    large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_layer` (`int`, *optional*, defaults to 3) — Number of hidden layers in the
    Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_head` (`int`, *optional*, defaults to 1) — Number of attention heads for
    each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_inner` (`int`, *optional*) — Dimensionality of the inner feed-forward layers.
    If unset, will default to 4 times `n_embd`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str`, *optional*, defaults to `"gelu"`) — Activation
    function, to be selected in the list `["relu", "silu", "gelu", "tanh", "gelu_new"]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resid_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embd_pdrop` (`int`, *optional*, defaults to 0.1) — The dropout ratio for the
    embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attn_pdrop` (`float`, *optional*, defaults to 0.1) — The dropout ratio for
    the attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_norm_epsilon` (`float`, *optional*, defaults to 1e-5) — The epsilon
    to use in the layer normalization layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`initializer_range` (`float`, *optional*, defaults to 0.02) — The standard
    deviation of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_attn_weights` (`bool`, *optional*, defaults to `True`) — Scale attention
    weights by dividing by sqrt(hidden_size)..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether or not the model
    should return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_attn_by_inverse_layer_idx` (`bool`, *optional*, defaults to `False`)
    — Whether to additionally scale attention weights by `1 / layer_idx + 1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reorder_and_upcast_attn` (`bool`, *optional*, defaults to `False`) — Whether
    to scale keys (K) prior to computing attention (dot-product) and upcast attention
    dot-product/softmax to float() when training with mixed precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel).
    It is used to instantiate a Decision Transformer model according to the specified
    arguments, defining the model architecture. Instantiating a configuration with
    the defaults will yield a similar configuration to that of the standard DecisionTransformer
    architecture. Many of the config options are used to instatiate the GPT2 model
    that is used as part of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: DecisionTransformerGPT2Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DecisionTransformerGPT2Model`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L473)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L503)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: DecisionTransformerModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.DecisionTransformerModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L783)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([~DecisionTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Decision Transformer Model This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model builds upon the GPT2 architecture to perform autoregressive prediction
    of actions in an offline RL setting. Refer to the paper for more details: [https://arxiv.org/abs/2106.01345](https://arxiv.org/abs/2106.01345)'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/decision_transformer/modeling_decision_transformer.py#L817)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`states` (`torch.FloatTensor` of shape `(batch_size, episode_length, state_dim)`)
    — The states for each step in the trajectory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`actions` (`torch.FloatTensor` of shape `(batch_size, episode_length, act_dim)`)
    — The actions taken by the “expert” policy for the current state, these are masked
    for auto regressive prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewards` (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`)
    — The rewards for each state, action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`returns_to_go` (`torch.FloatTensor` of shape `(batch_size, episode_length,
    1)`) — The returns for each state in the trajectory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timesteps` (`torch.LongTensor` of shape `(batch_size, episode_length)`) —
    The timestep for each step in the trajectory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.FloatTensor` of shape `(batch_size, episode_length)`)
    — Masking, used to mask the actions when performing autoregressive prediction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A `transformers.models.decision_transformer.modeling_decision_transformer.DecisionTransformerOutput`
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([DecisionTransformerConfig](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`) — Sequence of hidden-states at the output of the last layer of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`state_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    state_dim)`) — Environment state predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`action_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    action_dim)`) — Model action predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_preds` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    1)`) — Predicted returns for each state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `torch.FloatTensor`
    (one for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the model at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights after the attention softmax, used to compute the weighted
    average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [DecisionTransformerModel](/docs/transformers/v4.37.2/en/model_doc/decision_transformer#transformers.DecisionTransformerModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
