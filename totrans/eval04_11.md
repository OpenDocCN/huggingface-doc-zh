# 创建一个 EvaluationSuite

> 原文链接：[`huggingface.co/docs/evaluate/evaluation_suite`](https://huggingface.co/docs/evaluate/evaluation_suite)

在各种不同任务上评估模型可以帮助我们了解它们的下游性能。在几种类型的任务上评估模型可以揭示在某些方面性能差距。例如，在训练语言模型时，通常有必要在一个领域语料库上测量困惑度，但同时也要在测试一般语言能力的任务上进行评估，比如自然语言蕴涵或问答，或者旨在探测模型在公平性和偏见维度上的任务。

`EvaluationSuite`提供了一种将任意数量的(evaluator, dataset, metric)元组组合为一个 SubTask，以便在多个评估任务集合上评估模型的方法。请参阅 evaluator 文档以获取当前支持的任务列表。

一个新的`EvaluationSuite`由一系列定义评估任务的`SubTask`类组成。包含定义的 Python 文件可以上传到 Hugging Face Hub 上的一个 Space 中，以便与社区共享，或者保存/加载为一个 Python 脚本。

有些数据集在传递给`Evaluator`之前需要额外的预处理。您可以为每个`SubTask`设置一个`data_preprocessor`，通过`datasets`库使用`map`操作应用它。`Evaluator`的关键参数可以通过`args_for_task`属性传递下来。

`task_type`映射到 Evaluator 当前支持的任务。

一个新的`SubTask`的必需属性是`task_type`和`data`。

1.  | 0.4 | 1.67552 | 5.9683 | 0.167552 | glue/rte |

1.  `data`可以是一个实例化的 Hugging Face 数据集对象，也可以是一个数据集的名称。

1.  `subset`和`split`可以用来定义数据集的哪个名称和分割应该用于评估。

1.  `args_for_task`应该是一个包含要传递给 Evaluator 的关键字参数的字典。

```py
import evaluate
from evaluate.evaluation_suite import SubTask

class Suite(evaluate.EvaluationSuite):

    def __init__(self, name):
        super().__init__(name)
        self.preprocessor = lambda x: {"text": x["text"].lower()}
        self.suite = [
            SubTask(
                task_type="text-classification",
                data="glue",
                subset="sst2",
                split="validation[:10]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0.0,
                        "LABEL_1": 1.0
                    }
                }
            ),
            SubTask(
                task_type="text-classification",
                data="glue",
                subset="rte",
                split="validation[:10]",
                args_for_task={
                    "metric": "accuracy",
                    "input_column": "sentence1",
                    "second_input_column": "sentence2",
                    "label_column": "label",
                    "label_mapping": {
                        "LABEL_0": 0,
                        "LABEL_1": 1
                    }
                }
            )
        ]
```

可以通过名称从 Hugging Face Hub 加载`EvaluationSuite`，或者通过提供路径本地加载，并使用`run(model_or_pipeline)`方法运行。评估结果将与任务名称以及通过管道获取预测所需时间的信息一起返回。这些结果可以很容易地用`pandas.DataFrame`显示出来：

```py
>>> from evaluate import EvaluationSuite
>>> suite = EvaluationSuite.load('mathemakitten/glue-evaluation-suite')
>>> results = suite.run("gpt2")
```

| accuracy | total_time_in_seconds | samples_per_second | latency_in_seconds | task_name |
| --- | --- | --- | --- | --- |
| 要创建一个新的`EvaluationSuite`，请创建一个与 Space 名称匹配的.py 文件的[新 Space](https://huggingface.co/new-space)，将下面的模板添加到一个 Python 文件中，并填写一个新任务的属性。 |
| 0.5 | 0.740811 | 13.4987 | 0.0740811 | glue/sst2 |
| --: | --: | --: | --: | :-- |
