- en: Feature Extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/feature_extractor](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/feature_extractor)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/42.ce16b701.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: A feature extractor is in charge of preparing input features for audio or vision
    models. This includes feature extraction from sequences, e.g., pre-processing
    audio files to generate Log-Mel Spectrogram features, feature extraction from
    images, e.g., cropping image files, but also padding, normalization, and conversion
    to NumPy, PyTorch, and TensorFlow tensors.
  prefs: []
  type: TYPE_NORMAL
- en: FeatureExtractionMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.FeatureExtractionMixin'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L240)'
  prefs: []
  type: TYPE_NORMAL
- en: ( **kwargs )
  prefs: []
  type: TYPE_NORMAL
- en: This is a feature extraction mixin used to provide saving/loading functionality
    for sequential and image feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: '#### from_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L264)'
  prefs: []
  type: TYPE_NORMAL
- en: '( pretrained_model_name_or_path: Union cache_dir: Union = None force_download:
    bool = False local_files_only: bool = False token: Union = None revision: str
    = ''main'' **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**pretrained_model_name_or_path** (`str` or `os.PathLike`) — This can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cache_dir** (`str` or `os.PathLike`, *optional*) — Path to a directory in
    which a downloaded pretrained model feature extractor should be cached if the
    standard cache should not be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**force_download** (`bool`, *optional*, defaults to `False`) — Whether or not
    to force to (re-)download the feature extractor files and override the cached
    versions if they exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**resume_download** (`bool`, *optional*, defaults to `False`) — Whether or
    not to delete incompletely received file. Attempts to resume the download if such
    a file exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**proxies** (`Dict[str, str]`, *optional*) — A dictionary of proxy servers
    to use by protocol or endpoint, e.g., `{''http'': ''foo.bar:3128'', ''http://hostname'':
    ''foo.bar:4012''}.` The proxies are used on each request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token** (`str` or `bool`, *optional*) — The token to use as HTTP bearer authorization
    for remote files. If `True`, or not specified, will use the token generated when
    running `huggingface-cli login` (stored in `~/.huggingface`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**revision** (`str`, *optional*, defaults to `"main"`) — The specific model
    version to use. It can be a branch name, a tag name, or a commit id, since we
    use a git-based system for storing models and other artifacts on huggingface.co,
    so `revision` can be any identifier allowed by git.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a type of [FeatureExtractionMixin](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin)
    from a feature extractor, *e.g.* a derived class of [SequenceFeatureExtractor](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor).
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#### save_pretrained'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L377)'
  prefs: []
  type: TYPE_NORMAL
- en: '( save_directory: Union push_to_hub: bool = False **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**save_directory** (`str` or `os.PathLike`) — Directory where the feature extractor
    JSON file will be saved (will be created if it does not exist).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**push_to_hub** (`bool`, *optional*, defaults to `False`) — Whether or not
    to push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save a feature_extractor object to the directory `save_directory`, so that it
    can be re-loaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)
    class method.
  prefs: []
  type: TYPE_NORMAL
- en: SequenceFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.SequenceFeatureExtractor'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_sequence_utils.py#L29)'
  prefs: []
  type: TYPE_NORMAL
- en: '( feature_size: int sampling_rate: int padding_value: float **kwargs )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**feature_size** (`int`) — The feature dimension of the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sampling_rate** (`int`) — The sampling rate at which the audio files should
    be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**padding_value** (`float`) — The value that is used to fill the padding values
    / vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a general feature extraction class for speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: '#### pad'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_sequence_utils.py#L52)'
  prefs: []
  type: TYPE_NORMAL
- en: '( processed_features: Union padding: Union = True max_length: Optional = None
    truncation: bool = False pad_to_multiple_of: Optional = None return_attention_mask:
    Optional = None return_tensors: Union = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**processed_features** ([BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature),
    list of [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature),
    `Dict[str, List[float]]`, `Dict[str, List[List[float]]` or `List[Dict[str, List[float]]]`)
    — Processed inputs. Can represent one input ([BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
    or `Dict[str, List[float]]`) or a batch of input values / vectors (list of [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature),
    *Dict[str, List[List[float]]]* or *List[Dict[str, List[float]]]*) so you can use
    this method during preprocessing as well as in a PyTorch Dataloader collate function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of `List[float]` you can have tensors (numpy arrays, PyTorch tensors
    or TensorFlow tensors), see the note above for the return type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_length** (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**truncation** (`bool`) — Activates truncation to cut input sequences longer
    than `max_length` to `max_length`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pad_to_multiple_of** (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_attention_mask** (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**return_tensors** (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad input values / input vectors or a batch of input values / input vectors
    up to predefined length or to the max sequence length in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: Padding side (left/right) padding values are defined at the feature extractor
    level (with `self.padding_side`, `self.padding_value`)
  prefs: []
  type: TYPE_NORMAL
- en: If the `processed_features` passed are dictionary of numpy arrays, PyTorch tensors
    or TensorFlow tensors, the result will use the same type unless you provide a
    different tensor type with `return_tensors`. In the case of PyTorch tensors, you
    will lose the specific device of your tensors however.
  prefs: []
  type: TYPE_NORMAL
- en: BatchFeature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.BatchFeature'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L61)'
  prefs: []
  type: TYPE_NORMAL
- en: '( data: Optional = None tensor_type: Union = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**data** (`dict`, *optional*) — Dictionary of lists/arrays/tensors returned
    by the **call**/pad methods (‘input_values’, ‘attention_mask’, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tensor_type** (`Union[None, str, TensorType]`, *optional*) — You can give
    a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy
    Tensors at initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holds the output of the [pad()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad)
    and feature extractor specific `__call__` methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class is derived from a python dictionary and can be used as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '#### convert_to_tensors'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L164)'
  prefs: []
  type: TYPE_NORMAL
- en: '( tensor_type: Union = None )'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**tensor_type** (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — The type of tensors to use. If `str`, should be one of the values
    of the enum [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType).
    If `None`, no modification is done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the inner content to tensors.
  prefs: []
  type: TYPE_NORMAL
- en: '#### to'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/feature_extraction_utils.py#L195)'
  prefs: []
  type: TYPE_NORMAL
- en: ( *args **kwargs ) → [BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**args** (`Tuple`) — Will be passed to the `to(...)` function of the tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs** (`Dict`, *optional*) — Will be passed to the `to(...)` function
    of the tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[BatchFeature](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.BatchFeature)'
  prefs: []
  type: TYPE_NORMAL
- en: The same instance after modification.
  prefs: []
  type: TYPE_NORMAL
- en: Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only).
    This should support casting in different `dtypes` and sending the `BatchFeature`
    to a different `device`.
  prefs: []
  type: TYPE_NORMAL
- en: ImageFeatureExtractionMixin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### class transformers.ImageFeatureExtractionMixin'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L341)'
  prefs: []
  type: TYPE_NORMAL
- en: ( )
  prefs: []
  type: TYPE_NORMAL
- en: Mixin that contain utilities for preparing image features.
  prefs: []
  type: TYPE_NORMAL
- en: '#### center_crop'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L569)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image size ) → new_image
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor` of shape (n_channels,
    height, width) or (height, width, n_channels)) — The image to resize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size** (`int` or `Tuple[int, int]`) — The size to which crop the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: new_image
  prefs: []
  type: TYPE_NORMAL
- en: 'A center cropped `PIL.Image.Image` or `np.ndarray` or `torch.Tensor` of shape:
    (n_channels, height, width).'
  prefs: []
  type: TYPE_NORMAL
- en: Crops `image` to the given size using a center crop. Note that if the image
    is too small to be cropped to the size given, it will be padded (so the returned
    result has the size asked).
  prefs: []
  type: TYPE_NORMAL
- en: '#### convert_rgb'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L383)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image`) — The image to convert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts `PIL.Image.Image` to RGB format.
  prefs: []
  type: TYPE_NORMAL
- en: '#### expand_dims'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L436)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    to expand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expands 2-dimensional `image` to 3 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '#### flip_channel_order'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L644)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    whose color channels to flip. If `np.ndarray` or `torch.Tensor`, the channel dimension
    should be first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flips the channel order of `image` from RGB to BGR, or vice versa. Note that
    this will trigger a conversion of `image` to a NumPy array if it’s a PIL Image.
  prefs: []
  type: TYPE_NORMAL
- en: '#### normalize'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L456)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image mean std rescale = False )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    to normalize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mean** (`List[float]` or `np.ndarray` or `torch.Tensor`) — The mean (per
    channel) to use for normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**std** (`List[float]` or `np.ndarray` or `torch.Tensor`) — The standard deviation
    (per channel) to use for normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale** (`bool`, *optional*, defaults to `False`) — Whether or not to rescale
    the image to be between 0 and 1\. If a PIL image is provided, scaling will happen
    automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizes `image` with `mean` and `std`. Note that this will trigger a conversion
    of `image` to a NumPy array if it’s a PIL Image.
  prefs: []
  type: TYPE_NORMAL
- en: '#### rescale'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L397)'
  prefs: []
  type: TYPE_NORMAL
- en: '( image: ndarray scale: Union )'
  prefs: []
  type: TYPE_NORMAL
- en: Rescale a numpy image by scale amount
  prefs: []
  type: TYPE_NORMAL
- en: '#### resize'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L502)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image size resample = None default_to_square = True max_size = None ) → image
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    to resize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size** (`int` or `Tuple[int, int]`) — The size to use for resizing the image.
    If `size` is a sequence like (h, w), output size will be matched to this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `size` is an int and `default_to_square` is `True`, then image will be resized
    to (size, size). If `size` is an int and `default_to_square` is `False`, then
    smaller edge of the image will be matched to this number. i.e, if height > width,
    then image will be rescaled to (size * height / width, size).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**resample** (`int`, *optional*, defaults to `PILImageResampling.BILINEAR`)
    — The filter to user for resampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**default_to_square** (`bool`, *optional*, defaults to `True`) — How to convert
    `size` when it is a single int. If set to `True`, the `size` will be converted
    to a square (`size`,`size`). If set to `False`, will replicate [`torchvision.transforms.Resize`](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize)
    with support for resizing only the smallest edge and providing an optional `max_size`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_size** (`int`, *optional*, defaults to `None`) — The maximum allowed
    for the longer edge of the resized image: if the longer edge of the image is greater
    than `max_size` after being resized according to `size`, then the image is resized
    again so that the longer edge is equal to `max_size`. As a result, `size` might
    be overruled, i.e the smaller edge may be shorter than `size`. Only used if `default_to_square`
    is `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: image
  prefs: []
  type: TYPE_NORMAL
- en: A resized `PIL.Image.Image`.
  prefs: []
  type: TYPE_NORMAL
- en: Resizes `image`. Enforces conversion of input to PIL.Image.
  prefs: []
  type: TYPE_NORMAL
- en: '#### rotate'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L661)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image angle resample = None expand = 0 center = None translate = None fillcolor
    = None ) → image
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    to rotate. If `np.ndarray` or `torch.Tensor`, will be converted to `PIL.Image.Image`
    before rotating.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: image
  prefs: []
  type: TYPE_NORMAL
- en: A rotated `PIL.Image.Image`.
  prefs: []
  type: TYPE_NORMAL
- en: Returns a rotated copy of `image`. This method returns a copy of `image`, rotated
    the given number of degrees counter clockwise around its centre.
  prefs: []
  type: TYPE_NORMAL
- en: '#### to_numpy_array'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L404)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image rescale = None channel_first = True )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) — The image
    to convert to a NumPy array.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale** (`bool`, *optional*) — Whether or not to apply the scaling factor
    (to make pixel values floats between 0\. and 1.). Will default to `True` if the
    image is a PIL Image or an array/tensor of integers, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**channel_first** (`bool`, *optional*, defaults to `True`) — Whether or not
    to permute the dimensions of the image to put the channel dimension first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts `image` to a numpy array. Optionally rescales it and puts the channel
    dimension as the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '#### to_pil_image'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/image_utils.py#L353)'
  prefs: []
  type: TYPE_NORMAL
- en: ( image rescale = None )
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '**image** (`PIL.Image.Image` or `numpy.ndarray` or `torch.Tensor`) — The image
    to convert to the PIL Image format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rescale** (`bool`, *optional*) — Whether or not to apply the scaling factor
    (to make pixel values integers between 0 and 255). Will default to `True` if the
    image type is a floating type, `False` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts `image` to a PIL Image. Optionally rescales it and puts the channel
    dimension back as the last axis if needed.
  prefs: []
  type: TYPE_NORMAL
