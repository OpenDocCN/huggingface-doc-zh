# CPU推理

> 原文链接：[https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_cpu](https://huggingface.co/docs/transformers/v4.37.2/en/perf_infer_cpu)

通过一些优化，可以在CPU上高效运行大型模型推理。其中一种优化技术涉及将PyTorch代码编译成高性能环境（如C++）的中间格式。另一种技术是将多个操作融合成一个内核，以减少单独运行每个操作的开销。

您将学习如何使用[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/)进行更快的推理，以及如何将您的PyTorch代码转换为[TorchScript](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)。如果您使用Intel CPU，还可以使用[Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/index.html)中的[图优化](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#graph-optimization)来进一步提高推理速度。最后，学习如何使用🤗 Optimum来加速使用ONNX Runtime或OpenVINO进行推理（如果您使用Intel CPU）。

## BetterTransformer

BetterTransformer通过其快速路径（Transformer函数的本机PyTorch专用实现）执行加速推理。快速路径执行中的两个优化是：

1.  融合，将多个顺序操作组合成一个“内核”，以减少计算步骤的数量

1.  跳过填充令牌的固有稀疏性，以避免与嵌套张量一起进行不必要的计算

BetterTransformer还将所有注意力操作转换为更节省内存的[缩放点积注意力](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention)。

并非所有模型都支持BetterTransformer。查看此[列表](https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models)以查看模型是否支持BetterTransformer。

在开始之前，请确保您已经安装了🤗 Optimum [installed](https://huggingface.co/docs/optimum/installation)。

使用[PreTrainedModel.to_bettertransformer()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer)方法启用BetterTransformer：

```py
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("bigcode/starcoder")
model.to_bettertransformer()
```

## TorchScript

TorchScript是一个中间PyTorch模型表示，可以在性能重要的生产环境中运行。您可以在PyTorch中训练模型，然后将其导出到TorchScript中，以解放模型免受Python性能约束。PyTorch[跟踪](https://pytorch.org/docs/stable/generated/torch.jit.trace.html)一个模型以返回一个经过即时编译（JIT）优化的`ScriptFunction`。与默认的急切模式相比，PyTorch中的JIT模式通常通过操作融合等优化技术为推理提供更好的性能。

有关TorchScript的简要介绍，请参阅[PyTorch TorchScript简介](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html)教程。

使用[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类，您可以通过设置`--jit_mode_eval`标志为CPU推理启用JIT模式：

```py
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--jit_mode_eval
```

对于PyTorch >= 1.14.0，JIT模式可以使任何模型受益于预测和评估，因为`jit.trace`支持字典输入。

对于PyTorch < 1.14.0，如果模型的前向参数顺序与`jit.trace`中的元组输入顺序匹配，例如问答模型，JIT模式可以使模型受益。如果前向参数顺序与`jit.trace`中的元组输入顺序不匹配，例如文本分类模型，`jit.trace`将失败，我们在此处捕获此异常以使其回退。使用日志记录通知用户。

## IPEX图优化

Intel® Extension for PyTorch (IPEX)为Intel CPU的JIT模式提供进一步优化，并建议将其与TorchScript结合使用以获得更快的性能。IPEX [图优化](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html)融合了多头注意力、Concat Linear、Linear + Add、Linear + Gelu、Add + LayerNorm等操作。

要利用这些图优化，请确保已安装IPEX [installed](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html)。

```py
pip install intel_extension_for_pytorch
```

在[Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)类中设置`--use_ipex`和`--jit_mode_eval`标志以启用带有图优化的JIT模式：

```py
python run_qa.py \
--model_name_or_path csarron/bert-base-uncased-squad-v1 \
--dataset_name squad \
--do_eval \
--max_seq_length 384 \
--doc_stride 128 \
--output_dir /tmp/ \
--no_cuda \
--use_ipex \
--jit_mode_eval
```

## 🤗 Optimum

在[Optimum Inference with ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models)指南中了解有关使用ORT与🤗 Optimum的更多详细信息。本节仅提供了一个简短且简单的示例。

ONNX Runtime (ORT)是一个模型加速器，默认情况下在CPU上运行推理。ORT受🤗 Optimum支持，可以在🤗 Transformers中使用，而无需对您的代码进行太多更改。您只需要将🤗 Transformers的`AutoClass`替换为其等效的[ORTModel](https://huggingface.co/docs/optimum/v1.16.2/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)以解决您正在解决的任务，并加载一个ONNX格式的检查点。

例如，如果您正在运行问题回答任务的推理，加载包含`model.onnx`文件的[optimum/roberta-base-squad2](https://huggingface.co/optimum/roberta-base-squad2)检查点：

```py
from transformers import AutoTokenizer, pipeline
from optimum.onnxruntime import ORTModelForQuestionAnswering

model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")
tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

onnx_qa = pipeline("question-answering", model=model, tokenizer=tokenizer)

question = "What's my name?"
context = "My name is Philipp and I live in Nuremberg."
pred = onnx_qa(question, context)
```

如果您有Intel CPU，请查看🤗 [Optimum Intel](https://huggingface.co/docs/optimum/intel/index)，该支持各种压缩技术（量化、剪枝、知识蒸馏）和将模型转换为[OpenVINO](https://huggingface.co/docs/optimum/intel/inference)格式以获得更高性能推理的工具。
