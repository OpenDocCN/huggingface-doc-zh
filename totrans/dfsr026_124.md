# Prior Transformer

> 原始文本：[https://huggingface.co/docs/diffusers/api/models/prior_transformer](https://huggingface.co/docs/diffusers/api/models/prior_transformer)

Prior Transformer最初是由Ramesh等人在[具有CLIP潜在特征的分层文本条件图像生成](https://huggingface.co/papers/2204.06125)中引入的。它用于从CLIP文本嵌入中预测CLIP图像嵌入；图像嵌入通过去噪扩散过程进行预测。

论文摘要如下：

*像CLIP这样的对比模型已被证明能够学习捕捉图像的语义和风格的稳健表示。为了利用这些表示来进行图像生成，我们提出了一个两阶段模型：一个先验模型，根据文本标题生成CLIP图像嵌入，以及一个解码器，根据图像嵌入生成图像。我们展示了明确生成图像表示可以提高图像的多样性，同时最小化逼真度和标题相似性的损失。我们的解码器根据图像表示可以产生保留其语义和风格的图像变体，同时变化非必要的细节，这些细节在图像表示中不存在。此外，CLIP的联合嵌入空间使得可以以零样本的方式进行语言引导的图像操作。我们使用扩散模型进行解码器，并尝试先验模型的自回归和扩散模型，发现后者在计算上更有效，并产生更高质量的样本。*

## PriorTransformer

### `class diffusers.PriorTransformer`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L36)

```py
( num_attention_heads: int = 32 attention_head_dim: int = 64 num_layers: int = 20 embedding_dim: int = 768 num_embeddings = 77 additional_embeddings = 4 dropout: float = 0.0 time_embed_act_fn: str = 'silu' norm_in_type: Optional = None embedding_proj_norm_type: Optional = None encoder_hid_proj_type: Optional = 'linear' added_emb_type: Optional = 'prd' time_embed_dim: Optional = None embedding_proj_dim: Optional = None clip_embed_dim: Optional = None )
```

参数

+   `num_attention_heads` (`int`, *optional*, 默认为32) — 用于多头注意力的头数。

+   `attention_head_dim` (`int`, *optional*, 默认为64) — 每个头中的通道数。

+   `num_layers` (`int`, *optional*, 默认为20) — 要使用的Transformer块层数。

+   `embedding_dim` (`int`, *optional*, 默认为768) — 模型输入`hidden_states`的维度

+   `num_embeddings` (`int`, *optional*, 默认为77) — 模型输入`hidden_states`的嵌入数量

+   `additional_embeddings` (`int`, *optional*, 默认为4) — 附加到投影的`hidden_states`的令牌数量。使用的`hidden_states`的实际长度为`num_embeddings + additional_embeddings`。

+   `dropout` (`float`, *optional*, 默认为0.0) — 要使用的dropout概率。

+   `time_embed_act_fn` (`str`, *optional*, 默认为'silu') — 用于创建时间步嵌入的激活函数。

+   `norm_in_type` (`str`, *optional*, 默认为None) — 在传递给Transformer块之前对隐藏状态应用的规范化层。如果不需要规范化，请将其设置为`None`。

+   `embedding_proj_norm_type` (`str`, *optional*, 默认为None) — 应用于输入`proj_embedding`的规范化层。如果不需要规范化，请将其设置为`None`。

+   `encoder_hid_proj_type` (`str`, *optional*, 默认为`linear`) — 应用于输入`encoder_hidden_states`的投影层。如果`encoder_hidden_states`为`None`，则将其设置为`None`。

+   `added_emb_type` (`str`, *optional*, 默认为`prd`) — 用于条件模型的附加嵌入。从`prd`或`None`中选择。如果选择`prd`，它将在文本嵌入和图像嵌入之间插入一个表示（量化）点积的标记，如在unclip论文中提出的[https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125)。如果是`None`，则不会添加额外的嵌入。

+   `time_embed_dim` (`int, *optional*, 默认为None) -- 时间步嵌入的维度。如果为None，则将设置为`num_attention_heads * attention_head_dim`

+   `embedding_proj_dim` (`int`, *optional*, 默认为None) — `proj_embedding`的维度。如果为None，则将设置为`embedding_dim`。

+   `clip_embed_dim`（`int`，*可选*，默认为None）- 输出的维度。如果为None，则设置为`embedding_dim`。

一个先验Transformer模型。

#### `forward`

[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L245)

```py
( hidden_states timestep: Union proj_embedding: FloatTensor encoder_hidden_states: Optional = None attention_mask: Optional = None return_dict: bool = True ) → export const metadata = 'undefined';~models.prior_transformer.PriorTransformerOutput or tuple
```

参数

+   `hidden_states`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）- 当前预测的图像嵌入。

+   `timestep`（`torch.LongTensor`）- 当前去噪步骤。

+   `proj_embedding`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）- 去噪过程所依赖的投影嵌入向量。

+   `encoder_hidden_states`（形状为`(batch_size, num_embeddings, embedding_dim)`的`torch.FloatTensor`）- 文本嵌入的隐藏状态，去噪过程是基于这些隐藏状态进行的。

+   `attention_mask`（形状为`(batch_size, num_embeddings)`的`torch.BoolTensor`）- 文本嵌入的文本掩码。

+   `return_dict`（`bool`，*可选*，默认为`True`）- 是否返回`~models.prior_transformer.PriorTransformerOutput`而不是普通元组。

返回

`~models.prior_transformer.PriorTransformerOutput`或`tuple`

如果`return_dict`为True，则返回`~models.prior_transformer.PriorTransformerOutput`，否则返回一个元组，其中第一个元素是样本张量。

[先验Transformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)的前向方法。

#### `set_attn_processor`

[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L195)

```py
( processor: Union )
```

参数

+   `processor`（`AttentionProcessor`的字典或仅`AttentionProcessor`）- 实例化的处理器类或将设置为**所有**`Attention`层的处理器的处理器类字典。

    如果`processor`是一个字典，则键需要定义到相应交叉注意力处理器的路径。在设置可训练的注意力处理器时强烈推荐这样做。

设置用于计算注意力的注意力处理器。

#### `set_default_attn_processor`

[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L230)

```py
( )
```

禁用自定义注意力处理器并设置默认的注意力实现。

## PriorTransformerOutput

### `class diffusers.models.transformers.prior_transformer.PriorTransformerOutput`

[源代码](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/models/transformers/prior_transformer.py#L23)

```py
( predicted_image_embedding: FloatTensor )
```

参数

+   `predicted_image_embedding`（形状为`(batch_size, embedding_dim)`的`torch.FloatTensor`）- 在CLIP文本嵌入输入条件下预测的CLIP图像嵌入。

[先验Transformer](/docs/diffusers/v0.26.3/en/api/models/prior_transformer#diffusers.PriorTransformer)的输出。
