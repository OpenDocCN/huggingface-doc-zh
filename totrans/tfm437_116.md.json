["```py\n( config: PretrainedConfig *inputs **kwargs )\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( tags: Union )\n```", "```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\nmodel.add_model_tags([\"custom\", \"custom-bert\"])\n\n# Push the model to your namespace with the name \"my-custom-bert\".\nmodel.push_to_hub(\"my-custom-bert\")\n```", "```py\n( ) \u2192 export const metadata = 'undefined';bool\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( pretrained_model_name_or_path: Union *model_args config: Union = None cache_dir: Union = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' use_safetensors: bool = None **kwargs )\n```", "```py\n>>> from transformers import BertConfig, BertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Update configuration during loading.\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n>>> assert model.config.output_attentions == True\n>>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n>>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n>>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n```", "```py\n( ) \u2192 export const metadata = 'undefined';nn.Module\n```", "```py\n( return_buffers = True )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';nn.Module\n```", "```py\n( )\n```", "```py\n( gradient_checkpointing_kwargs = None )\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( heads_to_prune: Dict )\n```", "```py\n( auto_class = 'AutoModel' )\n```", "```py\n( new_num_tokens: Optional = None pad_to_multiple_of: Optional = None ) \u2192 export const metadata = 'undefined';torch.nn.Embedding\n```", "```py\n( ) \u2192 export const metadata = 'undefined';PreTrainedModel\n```", "```py\n( save_directory: Union is_main_process: bool = True state_dict: Optional = None save_function: Callable = <function save at 0x7f3c5f9b0160> push_to_hub: bool = False max_shard_size: Union = '5GB' safe_serialization: bool = True variant: Optional = None token: Union = None save_peft_format: bool = True **kwargs )\n```", "```py\n( value: Module )\n```", "```py\n( )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';PreTrainedModel\n```", "```py\n( input_ids attention_mask )\n```", "```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```", "```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n```", "```py\nt0pp.hf_device_map\n```", "```py\n{'shared': 0,\n 'decoder.embed_tokens': 0,\n 'encoder': 0,\n 'decoder.block.0': 0,\n 'decoder.block.1': 1,\n 'decoder.block.2': 1,\n 'decoder.block.3': 1,\n 'decoder.block.4': 1,\n 'decoder.block.5': 1,\n 'decoder.block.6': 1,\n 'decoder.block.7': 1,\n 'decoder.block.8': 1,\n 'decoder.block.9': 1,\n 'decoder.block.10': 1,\n 'decoder.block.11': 1,\n 'decoder.block.12': 1,\n 'decoder.block.13': 1,\n 'decoder.block.14': 1,\n 'decoder.block.15': 1,\n 'decoder.block.16': 1,\n 'decoder.block.17': 1,\n 'decoder.block.18': 1,\n 'decoder.block.19': 1,\n 'decoder.block.20': 1,\n 'decoder.block.21': 1,\n 'decoder.block.22': 'cpu',\n 'decoder.block.23': 'cpu',\n 'decoder.final_layer_norm': 'cpu',\n 'decoder.dropout': 'cpu',\n 'lm_head': 'cpu'}\n```", "```py\ndevice_map = {\"shared\": 0, \"encoder\": 0, \"decoder\": 1, \"lm_head\": 1}\n```", "```py\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=torch.float16)\n```", "```py\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=\"auto\")\n```", "```py\nconfig = T5Config.from_pretrained(\"t5\")\nmodel = AutoModel.from_config(config)\n```", "```py\n( )\n```", "```py\n( )\n```", "```py\n( input_dict: Dict ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( input_dict: Dict exclude_embeddings: bool = True ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( attention_mask: Tensor input_shape: Tuple device: device = None dtype: torch.float32 = None )\n```", "```py\n( head_mask: Optional num_hidden_layers: int is_attention_chunked: bool = False )\n```", "```py\n( encoder_attention_mask: Tensor ) \u2192 export const metadata = 'undefined';torch.Tensor\n```", "```py\n( only_trainable: bool = False exclude_embeddings: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( )\n```", "```py\n( config *inputs **kwargs )\n```", "```py\n( repo_id: str use_temp_dir: Optional[bool] = None commit_message: Optional[str] = None private: Optional[bool] = None max_shard_size: Optional[Union[int, str]] = '10GB' token: Optional[Union[bool, str]] = None use_auth_token: Optional[Union[bool, str]] = None create_pr: bool = False **base_model_card_args )\n```", "```py\nfrom transformers import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( ) \u2192 export const metadata = 'undefined';bool\n```", "```py\n( optimizer = 'rmsprop' loss = 'auto_with_warning' metrics = None loss_weights = None weighted_metrics = None run_eagerly = None steps_per_execution = None **kwargs )\n```", "```py\n( output_dir model_name: str language: Optional[str] = None license: Optional[str] = None tags: Optional[str] = None finetuned_from: Optional[str] = None tasks: Optional[str] = None dataset_tags: Optional[Union[str, List[str]]] = None dataset: Optional[Union[str, List[str]]] = None dataset_args: Optional[Union[str, List[str]]] = None )\n```", "```py\n( inputs )\n```", "```py\n( pretrained_model_name_or_path: Optional[Union[str, os.PathLike]] *model_args config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None cache_dir: Optional[Union[str, os.PathLike]] = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Optional[Union[str, bool]] = None revision: str = 'main' use_safetensors: bool = None **kwargs )\n```", "```py\n>>> from transformers import BertConfig, TFBertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = TFBertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Update configuration during loading.\n>>> model = TFBertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n>>> assert model.config.output_attentions == True\n>>> # Loading from a Pytorch model file instead of a TensorFlow checkpoint (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./pt_model/my_pt_model_config.json\")\n>>> model = TFBertModel.from_pretrained(\"./pt_model/my_pytorch_model.bin\", from_pt=True, config=config)\n```", "```py\n( ) \u2192 export const metadata = 'undefined';tf.Variable\n```", "```py\n( head_mask: tf.Tensor | None num_hidden_layers: int )\n```", "```py\n( ) \u2192 export const metadata = 'undefined';tf.Variable\n```", "```py\n( ) \u2192 export const metadata = 'undefined';tf.keras.layers.Layer\n```", "```py\n( ) \u2192 export const metadata = 'undefined';tf.Variable\n```", "```py\n( ) \u2192 export const metadata = 'undefined';tf.keras.layers.Layer\n```", "```py\n( ) \u2192 export const metadata = 'undefined';str\n```", "```py\n( repo_path_or_name ) \u2192 export const metadata = 'undefined';dict\n```", "```py\n( dataset: 'datasets.Dataset' batch_size: int = 8 shuffle: bool = True tokenizer: Optional['PreTrainedTokenizerBase'] = None collate_fn: Optional[Callable] = None collate_fn_args: Optional[Dict[str, Any]] = None drop_remainder: Optional[bool] = None prefetch: bool = True ) \u2192 export const metadata = 'undefined';Dataset\n```", "```py\n( heads_to_prune )\n```", "```py\n( auto_class = 'TFAutoModel' )\n```", "```py\n( new_num_tokens: Optional[int] = None ) \u2192 export const metadata = 'undefined';tf.Variable or tf.keras.layers.Embedding\n```", "```py\n( save_directory saved_model = False version = 1 push_to_hub = False signatures = None max_shard_size: Union[int, str] = '10GB' create_pr: bool = False safe_serialization: bool = False token: Optional[Union[str, bool]] = None **kwargs )\n```", "```py\n( inputs )\n```", "```py\n( output )\n```", "```py\n( value )\n```", "```py\n( value )\n```", "```py\n( value )\n```", "```py\n( data )\n```", "```py\n( data )\n```", "```py\n( )\n```", "```py\n( only_trainable: bool = False ) \u2192 export const metadata = 'undefined';int\n```", "```py\n( config: PretrainedConfig module: Module input_shape: Tuple = (1, 1) seed: int = 0 dtype: dtype = <class 'jax.numpy.float32'> _do_init: bool = True )\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import FlaxAutoModel\n\nmodel = FlaxAutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( )\n```", "```py\n( pretrained_model_name_or_path: Union dtype: dtype = <class 'jax.numpy.float32'> *model_args config: Union = None cache_dir: Union = None ignore_mismatched_sizes: bool = False force_download: bool = False local_files_only: bool = False token: Union = None revision: str = 'main' **kwargs )\n```", "```py\n>>> from transformers import BertConfig, FlaxBertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = FlaxBertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Loading from a PyTorch checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./pt_model/config.json\")\n>>> model = FlaxBertModel.from_pretrained(\"./pt_model/pytorch_model.bin\", from_pt=True, config=config)\n```", "```py\n( shard_files ) \u2192 export const metadata = 'undefined';Dict\n```", "```py\n( auto_class = 'FlaxAutoModel' )\n```", "```py\n( save_directory: Union params = None push_to_hub = False max_shard_size = '10GB' token: Union = None safe_serialization: bool = False **kwargs )\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # load model\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n>>> model.params = model.to_bf16(model.params)\n>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> flat_params = traverse_util.flatten_dict(model.params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> model.params = model.to_bf16(model.params, mask)\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # load model\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model params will be in fp32, to cast these to float16\n>>> model.params = model.to_fp16(model.params)\n>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> flat_params = traverse_util.flatten_dict(model.params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> model.params = model.to_fp16(model.params, mask)\n```", "```py\n( params: Union mask: Any = None )\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # Download model and configuration from huggingface.co\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model params will be in fp32, to illustrate the use of this method,\n>>> # we'll first cast to fp16 and back to fp32\n>>> model.params = model.to_f16(model.params)\n>>> # now cast back to fp32\n>>> model.params = model.to_fp32(model.params)\n```", "```py\n( )\n```", "```py\n( repo_id: str use_temp_dir: Optional = None commit_message: Optional = None private: Optional = None token: Union = None max_shard_size: Union = '5GB' create_pr: bool = False safe_serialization: bool = True revision: str = None commit_description: str = None tags: Optional = None **deprecated_kwargs )\n```", "```py\nfrom transformers import {object_class}\n\n{object} = {object_class}.from_pretrained(\"bert-base-cased\")\n\n# Push the {object} to your namespace with the name \"my-finetuned-bert\".\n{object}.push_to_hub(\"my-finetuned-bert\")\n\n# Push the {object} to an organization with the name \"my-finetuned-bert\".\n{object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n( model folder strict = True prefer_safe = True ) \u2192 export const metadata = 'undefined';NamedTuple\n```"]