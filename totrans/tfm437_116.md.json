["```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n\nmodel.add_model_tags([\"custom\", \"custom-bert\"])\n\n# Push the model to your namespace with the name \"my-custom-bert\".\nmodel.push_to_hub(\"my-custom-bert\")\n```", "```py\n>>> from transformers import BertConfig, BertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Update configuration during loading.\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n>>> assert model.config.output_attentions == True\n>>> # Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./tf_model/my_tf_model_config.json\")\n>>> model = BertModel.from_pretrained(\"./tf_model/my_tf_checkpoint.ckpt.index\", from_tf=True, config=config)\n>>> # Loading from a Flax checkpoint file instead of a PyTorch model (slower)\n>>> model = BertModel.from_pretrained(\"bert-base-uncased\", from_flax=True)\n```", "```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```", "```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n```", "```py\nt0pp.hf_device_map\n```", "```py\n{'shared': 0,\n 'decoder.embed_tokens': 0,\n 'encoder': 0,\n 'decoder.block.0': 0,\n 'decoder.block.1': 1,\n 'decoder.block.2': 1,\n 'decoder.block.3': 1,\n 'decoder.block.4': 1,\n 'decoder.block.5': 1,\n 'decoder.block.6': 1,\n 'decoder.block.7': 1,\n 'decoder.block.8': 1,\n 'decoder.block.9': 1,\n 'decoder.block.10': 1,\n 'decoder.block.11': 1,\n 'decoder.block.12': 1,\n 'decoder.block.13': 1,\n 'decoder.block.14': 1,\n 'decoder.block.15': 1,\n 'decoder.block.16': 1,\n 'decoder.block.17': 1,\n 'decoder.block.18': 1,\n 'decoder.block.19': 1,\n 'decoder.block.20': 1,\n 'decoder.block.21': 1,\n 'decoder.block.22': 'cpu',\n 'decoder.block.23': 'cpu',\n 'decoder.final_layer_norm': 'cpu',\n 'decoder.dropout': 'cpu',\n 'lm_head': 'cpu'}\n```", "```py\ndevice_map = {\"shared\": 0, \"encoder\": 0, \"decoder\": 1, \"lm_head\": 1}\n```", "```py\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=torch.float16)\n```", "```py\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=\"auto\")\n```", "```py\nconfig = T5Config.from_pretrained(\"t5\")\nmodel = AutoModel.from_config(config)\n```", "```py\nfrom transformers import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n>>> from transformers import BertConfig, TFBertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = TFBertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Update configuration during loading.\n>>> model = TFBertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n>>> assert model.config.output_attentions == True\n>>> # Loading from a Pytorch model file instead of a TensorFlow checkpoint (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./pt_model/my_pt_model_config.json\")\n>>> model = TFBertModel.from_pretrained(\"./pt_model/my_pytorch_model.bin\", from_pt=True, config=config)\n```", "```py\nfrom transformers import FlaxAutoModel\n\nmodel = FlaxAutoModel.from_pretrained(\"bert-base-cased\")\n\n# Push the model to your namespace with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"my-finetuned-bert\")\n\n# Push the model to an organization with the name \"my-finetuned-bert\".\nmodel.push_to_hub(\"huggingface/my-finetuned-bert\")\n```", "```py\n>>> from transformers import BertConfig, FlaxBertModel\n\n>>> # Download model and configuration from huggingface.co and cache.\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n>>> model = FlaxBertModel.from_pretrained(\"./test/saved_model/\")\n>>> # Loading from a PyTorch checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).\n>>> config = BertConfig.from_json_file(\"./pt_model/config.json\")\n>>> model = FlaxBertModel.from_pretrained(\"./pt_model/pytorch_model.bin\", from_pt=True, config=config)\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # load model\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision\n>>> model.params = model.to_bf16(model.params)\n>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> flat_params = traverse_util.flatten_dict(model.params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> model.params = model.to_bf16(model.params, mask)\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # load model\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model params will be in fp32, to cast these to float16\n>>> model.params = model.to_fp16(model.params)\n>>> # If you want don't want to cast certain parameters (for example layer norm bias and scale)\n>>> # then pass the mask as follows\n>>> from flax import traverse_util\n\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> flat_params = traverse_util.flatten_dict(model.params)\n>>> mask = {\n...     path: (path[-2] != (\"LayerNorm\", \"bias\") and path[-2:] != (\"LayerNorm\", \"scale\"))\n...     for path in flat_params\n... }\n>>> mask = traverse_util.unflatten_dict(mask)\n>>> model.params = model.to_fp16(model.params, mask)\n```", "```py\n>>> from transformers import FlaxBertModel\n\n>>> # Download model and configuration from huggingface.co\n>>> model = FlaxBertModel.from_pretrained(\"bert-base-cased\")\n>>> # By default, the model params will be in fp32, to illustrate the use of this method,\n>>> # we'll first cast to fp16 and back to fp32\n>>> model.params = model.to_f16(model.params)\n>>> # now cast back to fp32\n>>> model.params = model.to_fp32(model.params)\n```", "```py\nfrom transformers import {object_class}\n\n{object} = {object_class}.from_pretrained(\"bert-base-cased\")\n\n# Push the {object} to your namespace with the name \"my-finetuned-bert\".\n{object}.push_to_hub(\"my-finetuned-bert\")\n\n# Push the {object} to an organization with the name \"my-finetuned-bert\".\n{object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n```"]