- en: (Automatic) Curriculum Learning for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning](https://huggingface.co/learn/deep-rl-course/unitbonus3/curriculum-learning)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: 'While most of the RL methods seen in this course work well in practice, there
    are some cases where using them alone fails. This can happen, for instance, when:'
  prefs: []
  type: TYPE_NORMAL
- en: the task to learn is hard and requires an **incremental acquisition of skills**
    (for instance when one wants to make a bipedal agent learn to go through hard
    obstacles, it must first learn to stand, then walk, then maybe jumpâ€¦)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: there are variations in the environment (that affect the difficulty) and one
    wants its agent to be **robust** to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Bipedal](../Images/e675d3cbd1fb1e896c36e92b88b2a7dd.png) ![Movable creepers](../Images/515895e0a85621d86aab110f7a7f5398.png)'
  prefs: []
  type: TYPE_IMG
- en: '[TeachMyAgent](https://developmentalsystems.org/TeachMyAgent/)'
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, it seems needed to propose different tasks to our RL agent and
    organize them such that the agent progressively acquires skills. This approach
    is called **Curriculum Learning** and usually implies a hand-designed curriculum
    (or set of tasks organized in a specific order). In practice, one can, for instance,
    control the generation of the environment, the initial states, or use Self-Play
    and control the level of opponents proposed to the RL agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'As designing such a curriculum is not always trivial, the field of **Automatic
    Curriculum Learning (ACL) proposes to design approaches that learn to create such
    an organization of tasks in order to maximize the RL agentâ€™s performances**. Portelas
    et al. proposed to define ACL as:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¦ a family of mechanisms that automatically adapt the distribution of training
    data by learning to adjust the selection of learning situations to the capabilities
    of RL agents.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an example, OpenAI used **Domain Randomization** (they applied random variations
    on the environment) to make a robot hand solve Rubikâ€™s Cubes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dr](../Images/375a72dcd7269a70641cfd815a1e9467.png)'
  prefs: []
  type: TYPE_IMG
- en: '[OpenAI - Solving Rubikâ€™s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can play with the robustness of agents trained in the [TeachMyAgent](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)
    benchmark by controlling environment variations or even drawing the terrain ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '![Demo](../Images/5c2270050486e329736af710094b7c28.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo](https://huggingface.co/spaces/flowers-team/Interactive_DeepRL_Demo)'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, we recommend that you check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the field
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Automatic Curriculum Learning For Deep RL: A Short Survey](https://arxiv.org/pdf/2003.04664.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Curriculum for Reinforcement Learning](https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recent methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Evolving Curricula with Regret-Based Environment Design](https://arxiv.org/abs/2203.01302)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Curriculum Reinforcement Learning via Constrained Optimal Transport](https://proceedings.mlr.press/v162/klink22a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Prioritized Level Replay](https://arxiv.org/abs/2010.03934)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section was written by [ClÃ©ment Romac](https://twitter.com/ClementRomac)
  prefs: []
  type: TYPE_NORMAL
