["```py\n( vocab_size = None hidden_size = 768 num_hidden_layers = 12 num_attention_heads = 12 intermediate_size = 3072 hidden_act = 'gelu' hidden_dropout = 0.1 activation_dropout = 0.1 attention_dropout = 0.1 feat_proj_dropout = 0.0 feat_quantizer_dropout = 0.0 final_dropout = 0.1 layerdrop = 0.1 initializer_range = 0.02 layer_norm_eps = 1e-05 feat_extract_norm = 'group' feat_extract_activation = 'gelu' conv_dim = (512, 512, 512, 512, 512, 512, 512) conv_stride = (5, 2, 2, 2, 2, 2, 2) conv_kernel = (10, 3, 3, 3, 3, 2, 2) conv_bias = False num_conv_pos_embeddings = 128 num_conv_pos_embedding_groups = 16 apply_spec_augment = True mask_time_prob = 0.05 mask_time_length = 10 mask_time_min_masks = 2 mask_feature_prob = 0.0 mask_feature_length = 10 mask_feature_min_masks = 0 num_codevectors_per_group = 320 num_codevector_groups = 2 contrastive_logits_temperature = 0.1 num_negatives = 100 codevector_dim = 256 proj_codevector_dim = 256 diversity_loss_weight = 0.1 ctc_loss_reduction = 'sum' ctc_zero_infinity = False use_weighted_layer_sum = False classifier_proj_size = 256 tdnn_dim = (512, 512, 512, 512, 1500) tdnn_kernel = (5, 3, 3, 1, 1) tdnn_dilation = (1, 2, 3, 1, 1) xvector_output_dim = 512 pad_token_id = 0 bos_token_id = 1 eos_token_id = 2 add_adapter = False adapter_kernel_size = 3 adapter_stride = 2 num_adapter_layers = 3 output_hidden_size = None position_embeddings_type = 'relative' rotary_embedding_base = 10000 max_source_positions = 5000 conv_depthwise_kernel_size = 31 conformer_conv_dropout = 0.1 **kwargs )\n```", "```py\n>>> from transformers import Wav2Vec2ConformerConfig, Wav2Vec2ConformerModel\n\n>>> # Initializing a Wav2Vec2Conformer facebook/wav2vec2-conformer-rel-pos-large style configuration\n>>> configuration = Wav2Vec2ConformerConfig()\n\n>>> # Initializing a model (with random weights) from the facebook/wav2vec2-conformer-rel-pos-large style configuration\n>>> model = Wav2Vec2ConformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( loss: Optional = None projected_states: FloatTensor = None projected_quantized_states: FloatTensor = None codevector_perplexity: FloatTensor = None hidden_states: Optional = None attentions: Optional = None contrastive_loss: Optional = None diversity_loss: Optional = None )\n```", "```py\n( config: Wav2Vec2ConformerConfig )\n```", "```py\n( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.Wav2Vec2BaseModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Wav2Vec2ConformerModel\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n>>> model = Wav2Vec2ConformerModel.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 292, 1024]\n```", "```py\n( config target_lang: Optional = None )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoProcessor, Wav2Vec2ConformerForCTC\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n>>> model = Wav2Vec2ConformerForCTC.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n\n>>> # transcribe speech\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription[0]\n'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\n\n>>> inputs[\"labels\"] = processor(text=dataset[0][\"text\"], return_tensors=\"pt\").input_ids\n\n>>> # compute loss\n>>> loss = model(**inputs).loss\n>>> round(loss.item(), 2)\n64.21\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.SequenceClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForSequenceClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n>>> model = Wav2Vec2ConformerForSequenceClassification.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> predicted_class_ids = torch.argmax(logits, dim=-1).item()\n>>> predicted_label = model.config.id2label[predicted_class_ids]\n\n>>> # compute loss - target_label is e.g. \"down\"\n>>> target_label = model.config.id2label[0]\n>>> inputs[\"labels\"] = torch.tensor([model.config.label2id[target_label]])\n>>> loss = model(**inputs).loss\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None labels: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.TokenClassifierOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForAudioFrameClassification\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n>>> model = Wav2Vec2ConformerForAudioFrameClassification.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], return_tensors=\"pt\", sampling_rate=sampling_rate)\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> probabilities = torch.sigmoid(logits[0])\n>>> # labels is a one-hot array of shape (num_frames, num_speakers)\n>>> labels = (probabilities > 0.5).long()\n```", "```py\n( config )\n```", "```py\n( input_values: Optional attention_mask: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None labels: Optional = None ) \u2192 export const metadata = 'undefined';transformers.modeling_outputs.XVectorOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForXVector\n>>> from datasets import load_dataset\n>>> import torch\n\n>>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n>>> dataset = dataset.sort(\"id\")\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n>>> model = Wav2Vec2ConformerForXVector.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n\n>>> # audio file is decoded on the fly\n>>> inputs = feature_extractor(\n...     [d[\"array\"] for d in dataset[:2][\"audio\"]], sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True\n... )\n>>> with torch.no_grad():\n...     embeddings = model(**inputs).embeddings\n\n>>> embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n\n>>> # the resulting embeddings can be used for cosine similarity-based retrieval\n>>> cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n>>> similarity = cosine_sim(embeddings[0], embeddings[1])\n>>> threshold = 0.7  # the optimal threshold is dataset-dependent\n>>> if similarity < threshold:\n...     print(\"Speakers are not the same!\")\n```", "```py\n( config: Wav2Vec2ConformerConfig )\n```", "```py\n( input_values: Optional attention_mask: Optional = None mask_time_indices: Optional = None sampled_negative_indices: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from transformers import AutoFeatureExtractor, Wav2Vec2ConformerForPreTraining\n>>> from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import _compute_mask_indices, _sample_negative_indices\n>>> from datasets import load_dataset\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large\")\n>>> model = Wav2Vec2ConformerForPreTraining.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large\")\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n\n>>> # compute masked indices\n>>> batch_size, raw_sequence_length = input_values.shape\n>>> sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n>>> mask_time_indices = _compute_mask_indices(\n...     shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n... )\n>>> sampled_negative_indices = _sample_negative_indices(\n...     features_shape=(batch_size, sequence_length),\n...     num_negatives=model.config.num_negatives,\n...     mask_time_indices=mask_time_indices,\n... )\n>>> mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n>>> sampled_negative_indices = torch.tensor(\n...     data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n... )\n\n>>> with torch.no_grad():\n...     outputs = model(input_values, mask_time_indices=mask_time_indices)\n\n>>> # compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)\n>>> cosine_sim = torch.cosine_similarity(outputs.projected_states, outputs.projected_quantized_states, dim=-1)\n\n>>> # show that cosine similarity is much higher than random\n>>> cosine_sim[mask_time_indices.to(torch.bool)].mean() > 0.5\ntensor(True)\n\n>>> # for contrastive loss training model should be put into train mode\n>>> model = model.train()\n>>> loss = model(\n...     input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n... ).loss\n```"]