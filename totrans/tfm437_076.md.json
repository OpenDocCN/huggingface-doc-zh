["```py\nnvidia-smi topo -m\n```", "```py\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      NV2     0-23            N/A\nGPU1    NV2      X      0-23            N/A\n```", "```py\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      PHB     0-11            N/A\nGPU1    PHB      X      0-11            N/A\n```", "```py\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n```", "```py\n# DDP w/ NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 torchrun \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}\n\n# DDP w/o NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 torchrun \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}\n```"]