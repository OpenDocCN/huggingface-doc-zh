- en: Preparing the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model](https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/text-generation-inference/main/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/start.96d64f85.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/scheduler.9680c161.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/singletons.5632daf5.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.9d57cde4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/paths.5eca520f.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/entry/app.48a2a24c.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/index.38d74ee1.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/0.c01ff294.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/nodes/6.d64cb595.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Tip.9bb23095.js">
    <link rel="modulepreload" href="/docs/text-generation-inference/main/en/_app/immutable/chunks/Heading.74c51a96.js">
  prefs: []
  type: TYPE_NORMAL
- en: Text Generation Inference improves the model in several aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TGI supports [bits-and-bytes](https://github.com/TimDettmers/bitsandbytes#bitsandbytes),
    [GPT-Q](https://arxiv.org/abs/2210.17323) and [AWQ](https://arxiv.org/abs/2306.00978)
    quantization. To speed up inference with quantization, simply set `quantize` flag
    to `bitsandbytes`, `gptq` or `awq` depending on the quantization technique you
    wish to use. When using GPT-Q quantization, you need to point to one of the models
    [here](https://huggingface.co/models?search=gptq) when using AWQ quantization,
    you need to point to one of the models [here](https://huggingface.co/models?search=awq).
    To get more information about quantization, please refer to [quantization guide](./../conceptual/quantization)
  prefs: []
  type: TYPE_NORMAL
- en: RoPE Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RoPE scaling can be used to increase the sequence length of the model during
    the inference time without necessarily fine-tuning it. To enable RoPE scaling,
    simply pass `--rope-scaling`, `--max-input-length` and `--rope-factors` flags
    when running through CLI. `--rope-scaling` can take the values `linear` or `dynamic`.
    If your model is not fine-tuned to a longer sequence length, use `dynamic`. `--rope-factor`
    is the ratio between the intended max sequence length and the model’s original
    max sequence length. Make sure to pass `--max-input-length` to provide maximum
    input length for extension.
  prefs: []
  type: TYPE_NORMAL
- en: We recommend using `dynamic` RoPE scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Safetensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Safetensors](https://github.com/huggingface/safetensors) is a fast and safe
    persistence format for deep learning models, and is required for tensor parallelism.
    TGI supports `safetensors` model loading under the hood. By default, given a repository
    with `safetensors` and `pytorch` weights, TGI will always load `safetensors`.
    If there’s no `pytorch` weights, TGI will convert the weights to `safetensors`
    format.'
  prefs: []
  type: TYPE_NORMAL
