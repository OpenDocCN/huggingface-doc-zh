["```py\nfrom accelerate import Accelerator\nfrom accelerate.utils import LoggerType\n\naccelerator = Accelerator(log_with=\"all\")  # For all available trackers in the environment\naccelerator = Accelerator(log_with=\"wandb\")\naccelerator = Accelerator(log_with=[\"wandb\", LoggerType.TENSORBOARD])\n```", "```py\nhps = {\"num_iterations\": 5, \"learning_rate\": 1e-2}\naccelerator.init_trackers(\"my_project\", config=hps)\n```", "```py\naccelerator.log({\"train_loss\": 1.12, \"valid_loss\": 0.8}, step=1)\n```", "```py\naccelerator.end_training()\n```", "```py\nfrom accelerate import Accelerator\n\naccelerator = Accelerator(log_with=\"all\")\nconfig = {\n    \"num_iterations\": 5,\n    \"learning_rate\": 1e-2,\n    \"loss_function\": str(my_loss_function),\n}\n\naccelerator.init_trackers(\"example_project\", config=config)\n\nmy_model, my_optimizer, my_training_dataloader = accelerate.prepare(my_model, my_optimizer, my_training_dataloader)\ndevice = accelerator.device\nmy_model.to(device)\n\nfor iteration in config[\"num_iterations\"]:\n    for step, batch in my_training_dataloader:\n        my_optimizer.zero_grad()\n        inputs, targets = batch\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = my_model(inputs)\n        loss = my_loss_function(outputs, targets)\n        accelerator.backward(loss)\n        my_optimizer.step()\n        accelerator.log({\"training_loss\": loss}, step=step)\naccelerator.end_training()\n```", "```py\naccelerator = Accelerator(log_with=\"tensorboard\", project_dir=\".\")\n\n# use with ProjectConfiguration\nconfig = ProjectConfiguration(project_dir=\".\", logging_dir=\"another/directory\")\naccelerator = Accelerator(log_with=\"tensorboard\", project_config=config)\n```", "```py\nfrom accelerate.tracking import GeneralTracker, on_main_process\nfrom typing import Optional\n\nimport wandb\n\nclass MyCustomTracker(GeneralTracker):\n    name = \"wandb\"\n    requires_logging_directory = False\n\n @on_main_process\n    def __init__(self, run_name: str):\n        self.run_name = run_name\n        run = wandb.init(self.run_name)\n\n @property\n    def tracker(self):\n        return self.run.run\n\n @on_main_process\n    def store_init_configuration(self, values: dict):\n        wandb.config(values)\n\n @on_main_process\n    def log(self, values: dict, step: Optional[int] = None):\n        wandb.log(values, step=step)\n```", "```py\ntracker = MyCustomTracker(\"some_run_name\")\naccelerator = Accelerator(log_with=tracker)\n```", "```py\ntracker = MyCustomTracker(\"some_run_name\")\naccelerator = Accelerator(log_with=[tracker, \"all\"])\n```", "```py\nwandb_tracker = accelerator.get_tracker(\"wandb\")\n```", "```py\nwandb_run.log_artifact(some_artifact_to_log)\n```", "```py\nwandb_tracker = accelerator.get_tracker(\"wandb\", unwrap=True)\nwith accelerator.on_main_process:\n    wandb_tracker.log_artifact(some_artifact_to_log)\n```", "```py\n  from accelerate import Accelerator\n+ import neptune.new as neptune\n\n  accelerator = Accelerator()\n+ run = neptune.init(...)\n\n  my_model, my_optimizer, my_training_dataloader = accelerate.prepare(my_model, my_optimizer, my_training_dataloader)\n  device = accelerator.device\n  my_model.to(device)\n\n  for iteration in config[\"num_iterations\"]:\n      for batch in my_training_dataloader:\n          my_optimizer.zero_grad()\n          inputs, targets = batch\n          inputs = inputs.to(device)\n          targets = targets.to(device)\n          outputs = my_model(inputs)\n          loss = my_loss_function(outputs, targets)\n          total_loss += loss\n          accelerator.backward(loss)\n          my_optimizer.step()\n+         if accelerator.is_main_process:\n+             run[\"logs/training/batch/loss\"].log(loss)\n```"]