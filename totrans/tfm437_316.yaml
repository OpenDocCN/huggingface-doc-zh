- en: Speech2Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech_to_text](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/speech_to_text)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/transformers/v4.37.2/en/_app/immutable/assets/0.e3b0c442.css"
    rel="modulepreload"> <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/start.1af50ed5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/scheduler.9bc65507.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/singletons.a2d7fdf1.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.3b203c72.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/paths.b8f1dad4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/entry/app.59e74a31.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/index.707bf1b6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/0.dbd8cc12.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/nodes/248.15d6ca35.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Docstring.17db21ae.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/globals.7f7f1b26.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Heading.342b1fa6.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/Markdown.fef84341.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/stores.c16bc1a5.js">
    <link rel="modulepreload" href="/docs/transformers/v4.37.2/en/_app/immutable/chunks/ExampleCodeBlock.4f515aa9.js">
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling
    with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai
    Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It’s a transformer-based seq2seq (encoder-decoder)
    model designed for end-to-end Automatic Speech Recognition (ASR) and Speech Translation
    (ST). It uses a convolutional downsampler to reduce the length of speech inputs
    by 3/4th before they are fed into the encoder. The model is trained with standard
    autoregressive cross-entropy loss and generates the transcripts/translations autoregressively.
    Speech2Text has been fine-tuned on several datasets for ASR and ST: [LibriSpeech](http://www.openslr.org/12),
    [CoVoST 2](https://github.com/facebookresearch/covost), [MuST-C](https://ict.fbk.eu/must-c/).'
  prefs: []
  type: TYPE_NORMAL
- en: This model was contributed by [valhalla](https://huggingface.co/valhalla). The
    original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text).
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Speech2Text is a speech model that accepts a float tensor of log-mel filter-bank
    features extracted from the speech signal. It’s a transformer-based seq2seq model,
    so the transcripts/translations are generated autoregressively. The `generate()`
    method can be used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: The [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    class is responsible for extracting the log-mel filter-bank features. The [Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    wraps [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    and [Speech2TextTokenizer](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer)
    into a single instance to both extract the input features and decode the predicted
    token ids.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature extractor depends on `torchaudio` and the tokenizer depends on
    `sentencepiece` so be sure to install those packages before running the examples.
    You could either install those as extra speech dependencies with `pip install
    transformers"[speech, sentencepiece]"` or install the packages separately with
    `pip install torchaudio sentencepiece`. Also `torchaudio` requires the development
    version of the [libsndfile](http://www.mega-nerd.com/libsndfile/) package which
    can be installed via a system package manager. On Ubuntu it can be installed as
    follows: `apt install libsndfile1-dev`'
  prefs: []
  type: TYPE_NORMAL
- en: ASR and Speech Translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Multilingual speech translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multilingual speech translation models, `eos_token_id` is used as the `decoder_start_token_id`
    and the target language id is forced as the first generated token. To force the
    target language id as the first generated token, pass the `forced_bos_token_id`
    parameter to the `generate()` method. The following example shows how to transate
    English speech to French text using the *facebook/s2t-medium-mustc-multilingual-st*
    checkpoint.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: See the [model hub](https://huggingface.co/models?filter=speech_to_text) to
    look for Speech2Text checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Speech2TextConfig
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextConfig`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/configuration_speech_to_text.py#L31)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_size` (`int`, *optional*, defaults to 10000) — Vocabulary size of the
    Speech2Text model. Defines the number of different tokens that can be represented
    by the `inputs_ids` passed when calling [Speech2TextModel](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_layers` (`int`, *optional*, defaults to 12) — Number of encoder layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_ffn_dim` (`int`, *optional*, defaults to 2048) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layers` (`int`, *optional*, defaults to 6) — Number of decoder layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_ffn_dim` (`int`, *optional*, defaults to 2048) — Dimensionality of
    the “intermediate” (often named feed-forward) layer in decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_attention_heads` (`int`, *optional*, defaults to 4) — Number of attention
    heads for each attention layer in the Transformer decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the encoder. See the [LayerDrop paper](https://arxiv.org/abs/1909.11556)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_layerdrop` (`float`, *optional*, defaults to 0.0) — The LayerDrop
    probability for the decoder. See the [LayerDrop paper](https://arxiv.org/abs/1909.11556)
    for more details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*, defaults to `True`) — Whether the model should
    return the last key/values attentions (not used by all models).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_encoder_decoder` (`bool`, *optional*, defaults to `True`) — Whether the
    model is set up as an encoder-decoder architecture for sequence-to-sequence tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_function` (`str` or `function`, *optional*, defaults to `"relu"`)
    — The non-linear activation function (function or string) in the encoder and pooler.
    If string, `"gelu"`, `"relu"`, `"silu"` and `"gelu_new"` are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`d_model` (`int`, *optional*, defaults to 256) — Dimensionality of the layers
    and the pooler layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout` (`float`, *optional*, defaults to 0.1) — The dropout probability
    for all fully connected layers in the embeddings, encoder, and pooler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for the attention probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation_dropout` (`float`, *optional*, defaults to 0.0) — The dropout ratio
    for activations inside the fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`init_std` (`float`, *optional*, defaults to 0.02) — The standard deviation
    of the truncated_normal_initializer for initializing all weight matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_start_token_id` (`int`, *optional*, defaults to 2) — The initial token
    ID of the decoder when decoding sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scale_embedding` (`bool`, *optional*, defaults to `True`) — Whether the embeddings
    are scaled by the square root of `d_model`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token_id` (`int`, *optional*, defaults to 1) — Padding token id.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token_id` (`int`, *optional*, defaults to 0) — The id of the beginning-of-sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id` (`int`, *optional*, defaults to 2) — The id of the end-of-sequence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_source_positions` (`int`, *optional*, defaults to 6000) — The maximum
    sequence length of log-mel filter-bank features that this model might ever be
    used with.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_target_positions` (`int`, *optional*, defaults to 1024) — The maximum
    sequence length that this model might ever be used with. Typically, set this to
    something large just in case (e.g., 512 or 1024 or 2048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_conv_layers` (`int`, *optional*, defaults to 2) — Number of 1D convolutional
    layers in the conv module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_kernel_sizes` (`Tuple[int]`, *optional*, defaults to `(5, 5)`) — A tuple
    of integers defining the kernel size of each 1D convolutional layer in the conv
    module. The length of `conv_kernel_sizes` has to match `num_conv_layers`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conv_channels` (`int`, *optional*, defaults to 1024) — An integer defining
    the number of output channels of each convolution layers except the final one
    in the conv module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_feat_per_channel` (`int`, *optional*, defaults to 80) — An integer specifying
    the size of feature vector. This is also the dimensions of log-mel filter-bank
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`input_channels` (`int`, *optional*, defaults to 1) — An integer specifying
    number of input channels of the input feature vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the configuration class to store the configuration of a [Speech2TextModel](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextModel).
    It is used to instantiate a Speech2Text model according to the specified arguments,
    defining the model architecture. Instantiating a configuration with the defaults
    will yield a similar configuration to that of the Speech2Text [facebook/s2t-small-librispeech-asr](https://huggingface.co/facebook/s2t-small-librispeech-asr)
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration objects inherit from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    and can be used to control the model outputs. Read the documentation from [PretrainedConfig](/docs/transformers/v4.37.2/en/main_classes/configuration#transformers.PretrainedConfig)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Speech2TextTokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextTokenizer`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`vocab_file` (`str`) — File containing the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spm_file` (`str`) — Path to the [SentencePiece](https://github.com/google/sentencepiece)
    model file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bos_token` (`str`, *optional*, defaults to `"<s>"`) — The beginning of sentence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token` (`str`, *optional*, defaults to `"</s>"`) — The end of sentence
    token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unk_token` (`str`, *optional*, defaults to `"<unk>"`) — The unknown token.
    A token that is not in the vocabulary cannot be converted to an ID and is set
    to be this token instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_token` (`str`, *optional*, defaults to `"<pad>"`) — The token used for
    padding, for example when batching sequences of different lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_upper_case` (`bool`, *optional*, defaults to `False`) — Whether or not
    to uppercase the output when decoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_lower_case` (`bool`, *optional*, defaults to `False`) — Whether or not
    to lowercase the input when tokenizing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tgt_lang` (`str`, *optional*) — A string representing the target language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sp_model_kwargs` (`dict`, *optional*) — Will be passed to the `SentencePieceProcessor.__init__()`
    method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python)
    can be used, among other things, to set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enable_sampling`: Enable subword regularization.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size = {0,1}`: No sampling is performed.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size > 1`: samples from the nbest_size results.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nbest_size < 0`: assuming that nbest_size is infinite and samples from the
    all hypothesis (lattice) using forward-filtering-and-backward-sampling algorithm.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Smoothing parameter for unigram sampling, and dropout probability
    of merge operations for BPE-dropout.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**kwargs — Additional keyword arguments passed along to [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Construct an Speech2Text tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: This tokenizer inherits from [PreTrainedTokenizer](/docs/transformers/v4.37.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer)
    which contains some of the main methods. Users should refer to the superclass
    for more information regarding such methods.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `build_inputs_with_special_tokens`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L217)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Build model inputs from a sequence by appending eos_token_id.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `get_special_tokens_mask`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L224)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — List of IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — Optional second list of IDs for sequence
    pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`already_has_special_tokens` (`bool`, *optional*, defaults to `False`) — Whether
    or not the token list is already formatted with special tokens for the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence
    token.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve sequence ids from a token list that has no special tokens added. This
    method is called when adding special tokens using the tokenizer `prepare_for_model`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `create_token_type_ids_from_sequences`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/tokenization_utils_base.py#L3302)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`token_ids_0` (`List[int]`) — The first tokenized sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token_ids_1` (`List[int]`, *optional*) — The second tokenized sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '`List[int]`'
  prefs: []
  type: TYPE_NORMAL
- en: The token type ids.
  prefs: []
  type: TYPE_NORMAL
- en: Create the token type IDs corresponding to the sequences passed. [What are token
    type IDs?](../glossary#token-type-ids)
  prefs: []
  type: TYPE_NORMAL
- en: Should be overridden in a subclass if the model has a special way of building
    those.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_vocabulary`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L268)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Speech2TextFeatureExtractor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextFeatureExtractor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_size` (`int`, *optional*, defaults to 80) — The feature dimension
    of the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*, defaults to 16000) — The sampling rate
    at which the audio files should be digitalized expressed in hertz (Hz).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_mel_bins` (`int`, *optional*, defaults to 80) — Number of Mel-frequency
    bins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, *optional*, defaults to 0.0) — The value that is
    used to fill the padding vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_ceptral_normalize` (`bool`, *optional*, defaults to `True`) — Whether or
    not to apply utterance-level cepstral mean and variance normalization to extracted
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_means` (`bool`, *optional*, defaults to `True`) — Whether or not
    to zero-mean normalize the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize_vars` (`bool`, *optional*, defaults to `True`) — Whether or not
    to unit-variance normalize the extracted features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Speech2Text feature extractor.
  prefs: []
  type: TYPE_NORMAL
- en: This feature extractor inherits from [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    which contains most of the main methods. Users should refer to this superclass
    for more information regarding those methods.
  prefs: []
  type: TYPE_NORMAL
- en: This class extracts mel-filter bank features from raw speech using TorchAudio
    if installed or using numpy otherwise, and applies utterance-level cepstral mean
    and variance normalization to the extracted features.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L162)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`raw_speech` (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`)
    — The sequence or batch of sequences to be padded. Each sequence can be a numpy
    array, a list of float values, a list of numpy arrays or a list of list of float
    values. Must be mono channel audio, not stereo, i.e. single float per timestep.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding` (`bool`, `str` or [PaddingStrategy](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.utils.PaddingStrategy),
    *optional*, defaults to `True`) — Select a strategy to pad the returned sequences
    (according to the model’s padding side and padding index) among:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True` or `''longest''`: Pad to the longest sequence in the batch (or no padding
    if only a single sequence if provided).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''max_length''`: Pad to a maximum length specified with the argument `max_length`
    or to the maximum acceptable input length for the model if that argument is not
    provided.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False` or `''do_not_pad''` (default): No padding (i.e., can output a batch
    with sequences of different lengths).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` (`int`, *optional*) — Maximum length of the returned list and
    optionally padding length (see above).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation` (`bool`) — Activates truncation to cut input sequences longer
    than *max_length* to *max_length*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_multiple_of` (`int`, *optional*) — If set will pad the sequence to
    a multiple of the provided value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is especially useful to enable the use of Tensor Cores on NVIDIA hardware
    with compute capability `>= 7.5` (Volta), or on TPUs which benefit from having
    sequence lengths be a multiple of 128.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_attention_mask` (`bool`, *optional*) — Whether to return the attention
    mask. If left to the default, will return the attention mask according to the
    specific feature_extractor’s default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For Speech2TextTransformer models, `attention_mask` should always be passed
    for batched inference, to avoid subtle bugs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return_tensors` (`str` or [TensorType](/docs/transformers/v4.37.2/en/internal/file_utils#transformers.TensorType),
    *optional*) — If set, will return tensors instead of list of python integers.
    Acceptable values are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''tf''`: Return TensorFlow `tf.constant` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''pt''`: Return PyTorch `torch.Tensor` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''np''`: Return Numpy `np.ndarray` objects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_rate` (`int`, *optional*) — The sampling rate at which the `raw_speech`
    input was sampled. It is strongly recommended to pass `sampling_rate` at the forward
    call to prevent silent errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding_value` (`float`, defaults to 0.0) — The value that is used to fill
    the padding values / vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main method to featurize and prepare for the model one or several sequence(s).
  prefs: []
  type: TYPE_NORMAL
- en: Speech2TextProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextProcessor`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/processing_speech_to_text.py#L24)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`feature_extractor` (`Speech2TextFeatureExtractor`) — An instance of [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor).
    The feature extractor is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tokenizer` (`Speech2TextTokenizer`) — An instance of [Speech2TextTokenizer](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer).
    The tokenizer is a required input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructs a Speech2Text processor which wraps a Speech2Text feature extractor
    and a Speech2Text tokenizer into a single processor.
  prefs: []
  type: TYPE_NORMAL
- en: '[Speech2TextProcessor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor)
    offers all the functionalities of [Speech2TextFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor)
    and [Speech2TextTokenizer](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer).
    See the [**call**()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.__call__)
    and [decode()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.decode)
    for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: '#### `__call__`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/processing_speech_to_text.py#L48)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor’s
    [**call**()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)
    and returns its output. If used in the context `as_target_processor()` this method
    forwards all its arguments to Speech2TextTokenizer’s [**call**()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__).
    Please refer to the doctsring of the above two methods for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `from_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L406)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path` (`str` or `os.PathLike`) — This can be either:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a string, the *model id* of a pretrained feature_extractor hosted inside a model
    repo on huggingface.co. Valid model ids can be located at the root-level, like
    `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path to a *directory* containing a feature extractor file saved using the
    [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    method, e.g., `./my_model_directory/`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: a path or url to a saved feature extractor JSON *file*, e.g., `./my_model_directory/preprocessor_config.json`.
    **kwargs — Additional keyword arguments passed along to both [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained)
    and `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a processor associated with a pretrained model.
  prefs: []
  type: TYPE_NORMAL
- en: This class method is simply calling the feature extractor [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained),
    image processor [ImageProcessingMixin](/docs/transformers/v4.37.2/en/internal/image_processing_utils#transformers.ImageProcessingMixin)
    and the tokenizer `~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`
    methods. Please refer to the docstrings of the methods above for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `save_pretrained`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/processing_utils.py#L167)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`save_directory` (`str` or `os.PathLike`) — Directory where the feature extractor
    JSON file and the tokenizer files will be saved (directory will be created if
    it does not exist).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`push_to_hub` (`bool`, *optional*, defaults to `False`) — Whether or not to
    push your model to the Hugging Face model hub after saving it. You can specify
    the repository you want to push to with `repo_id` (will default to the name of
    `save_directory` in your namespace).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kwargs` (`Dict[str, Any]`, *optional*) — Additional key word arguments passed
    along to the [push_to_hub()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.push_to_hub)
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves the attributes of this processor (feature extractor, tokenizer…) in the
    specified directory so that it can be reloaded using the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: This class method is simply calling [save_pretrained()](/docs/transformers/v4.37.2/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained)
    and [save_pretrained()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained).
    Please refer to the docstrings of the methods above for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `batch_decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/processing_speech_to_text.py#L87)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to Speech2TextTokenizer’s [batch_decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `decode`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/processing_speech_to_text.py#L94)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This method forwards all its arguments to Speech2TextTokenizer’s [decode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode).
    Please refer to the docstring of this method for more information.
  prefs: []
  type: TYPE_NORMAL
- en: PytorchHide Pytorch content
  prefs: []
  type: TYPE_NORMAL
- en: Speech2TextModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1081)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Speech2Text Model outputting raw hidden-states without any specific
    head on top. This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1107)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    feature_size)`) — Float values of fbank features extracted from the raw speech
    waveform. Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio
    file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the
    soundfile library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using `SpeechToTextTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_speech_to_text._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Speech2TextModel](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Speech2TextForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.Speech2TextForConditionalGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1212)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Speech2Text Model with a language modeling head. Can be used for summarization.
    This model inherits from [PreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '#### `forward`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1240)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    feature_size)`) — Float values of fbank features extracted from the raw speech
    waveform. Raw speech waveform can be obtained by loading a `.flac` or `.wav` audio
    file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the
    soundfile library (`pip install soundfile`). To prepare the array into `input_features`,
    the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of type `torch.FloatTensor`. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`torch.Tensor` of shape `(batch_size, sequence_length)`,
    *optional*) — Mask to avoid performing convolution and attention on padding token
    indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using `SpeechToTextTokenizer`. See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`.
    Causal mask will also be used by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to change padding behavior, you should read `modeling_speech_to_text._prepare_decoder_attention_mask`
    and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461)
    for more information on the default strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`head_mask` (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tuple(tuple(torch.FloatTensor)`, *optional*) — Tuple consists
    of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)
    `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*)
    is a sequence of hidden-states at the output of the last layer of the encoder.
    Used in the cross-attention of the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids`
    (those that don’t have their past key value states given to this model) of shape
    `(batch_size, 1)` instead of all `decoder_input_ids` of shape `(batch_size, sequence_length)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the language modeling loss. Indices should either be in
    `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with
    indices set to `-100` are ignored (masked), the loss is only computed for the
    tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or `tuple(torch.FloatTensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_outputs.Seq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput)
    or a tuple of `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels`
    is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, returned
    when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of `tuple(torch.FloatTensor)`
    of length `config.n_layers`, with each tuple having 2 tensors of shape `(batch_size,
    num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
    shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the self-attention blocks
    and in the cross-attention blocks) that can be used (see `past_key_values` input)
    to speed up sequential decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `torch.FloatTensor`
    (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_hidden_states=True` is passed or when `config.output_hidden_states=True`)
    — Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model
    has an embedding layer, + one for the output of each layer) of shape `(batch_size,
    sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(torch.FloatTensor)`, *optional*, returned when
    `output_attentions=True` is passed or when `config.output_attentions=True`) —
    Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads,
    sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [Speech2TextForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlowHide TensorFlow content
  prefs: []
  type: TYPE_NORMAL
- en: TFSpeech2TextModel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFSpeech2TextModel`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py#L1322)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bare Speech2Text Model outputting raw hidden-states without any specific
    head on top. This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py#L1338)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`tf.Tensor` of shape `(batch_size, sequence_length, feature_size)`)
    — Float values of fbank features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of floats. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `({0})`, *optional*) — Mask to avoid
    performing attention on padding token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [Speech2TextTokenizer](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — will be made by default and ignore pad tokens. It is not recommended
    to set this for most use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tf.FloatTensor`, *optional*) — hidden states at the output
    of the last layer of the encoder. Used in the cross-attention of the decoder.
    of shape `(batch_size, sequence_length, hidden_size)` is a sequence of'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`) —
    contains precomputed key and value hidden states of the attention blocks. Can
    be used to speed up decoding. If `past_key_values` are used, the user can optionally
    input only the last `decoder_input_ids` (those that don’t have their past key
    value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`
    of shape `(batch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`tf.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFSeq2SeqModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`)
    — Sequence of hidden-states at the output of the last layer of the decoder of
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `past_key_values` is used only the last hidden-state of the sequences of
    shape `(batch_size, 1, hidden_size)` is output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFSpeech2TextModel](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: TFSpeech2TextForConditionalGeneration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '### `class transformers.TFSpeech2TextForConditionalGeneration`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py#L1412)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`config` ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    — Model configuration class with all the parameters of the model. Initializing
    with a config file does not load the weights associated with the model, only the
    configuration. Check out the [from_pretrained()](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained)
    method to load the model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Speech2Text Model with a language modeling head. Can be used for summarization.
    This model inherits from [TFPreTrainedModel](/docs/transformers/v4.37.2/en/main_classes/model#transformers.TFPreTrainedModel).
    Check the superclass documentation for the generic methods the library implements
    for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)
  prefs: []
  type: TYPE_NORMAL
- en: This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)
    subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation
    for all matter related to general usage and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow models and layers in `transformers` accept two formats as input:'
  prefs: []
  type: TYPE_NORMAL
- en: having all inputs as keyword arguments (like PyTorch models), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: having all inputs as a list, tuple or dict in the first positional argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason the second format is supported is that Keras methods prefer this
    format when passing inputs to models and layers. Because of this support, when
    using methods like `model.fit()` things should “just work” for you - just pass
    your inputs and labels in any format that `model.fit()` supports! If, however,
    you want to use the second format outside of Keras methods like `fit()` and `predict()`,
    such as when creating your own layers or models with the Keras `Functional` API,
    there are three possibilities you can use to gather all the input Tensors in the
    first positional argument:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a single Tensor with `input_ids` only and nothing else: `model(input_ids)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a list of varying length with one or several input Tensors IN THE ORDER given
    in the docstring: `model([input_ids, attention_mask])` or `model([input_ids, attention_mask,
    token_type_ids])`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'a dictionary with one or several input Tensors associated to the input names
    given in the docstring: `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when creating models and layers with [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)
    then you don’t need to worry about any of this, as you can just pass inputs like
    you would to any other Python function!
  prefs: []
  type: TYPE_NORMAL
- en: '#### `call`'
  prefs: []
  type: TYPE_NORMAL
- en: '[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py#L1441)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Parameters
  prefs: []
  type: TYPE_NORMAL
- en: '`input_features` (`tf.Tensor` of shape `(batch_size, sequence_length, feature_size)`)
    — Float values of fbank features extracted from the raw speech waveform. Raw speech
    waveform can be obtained by loading a `.flac` or `.wav` audio file into an array
    of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip
    install soundfile`). To prepare the array into `input_features`, the [AutoFeatureExtractor](/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoFeatureExtractor)
    should be used for extracting the fbank features, padding and conversion into
    a tensor of floats. See [`call`()](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attention_mask` (`tf.Tensor` of shape `({0})`, *optional*) — Mask to avoid
    performing attention on padding token indices. Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 for tokens that are `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 for tokens that are `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are attention masks?](../glossary#attention-mask)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_input_ids` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — Indices of decoder input sequence tokens in the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indices can be obtained using [Speech2TextTokenizer](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer).
    See [PreTrainedTokenizer.encode()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode)
    and [PreTrainedTokenizer.`call`()](/docs/transformers/v4.37.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)
    for details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What are decoder input IDs?](../glossary#decoder-input-ids)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SpeechToText uses the `eos_token_id` as the starting token for `decoder_input_ids`
    generation. If `past_key_values` is used, optionally only the last `decoder_input_ids`
    have to be input (see `past_key_values`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For translation and summarization training, `decoder_input_ids` should be provided.
    If no `decoder_input_ids` is provided, the model will create this tensor by shifting
    the `input_ids` to the right for denoising pre-training following the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attention_mask` (`tf.Tensor` of shape `(batch_size, target_sequence_length)`,
    *optional*) — will be made by default and ignore pad tokens. It is not recommended
    to set this for most use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`head_mask` (`tf.Tensor` of shape `(encoder_layers, encoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the encoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the attention modules in the decoder.
    Mask values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cross_attn_head_mask` (`tf.Tensor` of shape `(decoder_layers, decoder_attention_heads)`,
    *optional*) — Mask to nullify selected heads of the cross-attention modules. Mask
    values selected in `[0, 1]`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 indicates the head is `not masked`,
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicates the head is `masked`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_outputs` (`tf.FloatTensor`, *optional*) — hidden states at the output
    of the last layer of the encoder. Used in the cross-attention of the decoder.
    of shape `(batch_size, sequence_length, hidden_size)` is a sequence of'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`) —
    contains precomputed key and value hidden states of the attention blocks. Can
    be used to speed up decoding. If `past_key_values` are used, the user can optionally
    input only the last `decoder_input_ids` (those that don’t have their past key
    value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids`
    of shape `(batch_size, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`decoder_inputs_embeds` (`tf.FloatTensor` of shape `(batch_size, target_sequence_length,
    hidden_size)`, *optional*) — Optionally, instead of passing `decoder_input_ids`
    you can choose to directly pass an embedded representation. If `past_key_values`
    is used, optionally only the last `decoder_inputs_embeds` have to be input (see
    `past_key_values`). This is useful if you want more control over how to convert
    `decoder_input_ids` indices into associated vectors than the model’s internal
    embedding lookup matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cache` (`bool`, *optional*) — If set to `True`, `past_key_values` key
    value states are returned and can be used to speed up decoding (see `past_key_values`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_attentions` (`bool`, *optional*) — Whether or not to return the attentions
    tensors of all attention layers. See `attentions` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_hidden_states` (`bool`, *optional*) — Whether or not to return the
    hidden states of all layers. See `hidden_states` under returned tensors for more
    detail. This argument can be used only in eager mode, in graph mode the value
    in the config will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_dict` (`bool`, *optional*) — Whether or not to return a [ModelOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.utils.ModelOutput)
    instead of a plain tuple. This argument can be used in eager mode, in graph mode
    the value will always be set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training` (`bool`, *optional*, defaults to `False`) — Whether or not to use
    the model in training mode (some modules like dropout modules have different behaviors
    between training and evaluation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels` (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*)
    — Labels for computing the masked language modeling loss. Indices should either
    be in `[0, ..., config.vocab_size]` or -100 (see `input_ids` docstring). Tokens
    with indices set to `-100` are ignored (masked), the loss is only computed for
    the tokens with labels in `[0, ..., config.vocab_size]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns
  prefs: []
  type: TYPE_NORMAL
- en: '[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or `tuple(tf.Tensor)`'
  prefs: []
  type: TYPE_NORMAL
- en: A [transformers.modeling_tf_outputs.TFSeq2SeqLMOutput](/docs/transformers/v4.37.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput)
    or a tuple of `tf.Tensor` (if `return_dict=False` is passed or when `config.return_dict=False`)
    comprising various elements depending on the configuration ([Speech2TextConfig](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.Speech2TextConfig))
    and inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` (`tf.Tensor` of shape `(n,)`, *optional*, where n is the number of non-masked
    labels, returned when `labels` is provided) — Language modeling loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logits` (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`)
    — Prediction scores of the language modeling head (scores for each vocabulary
    token before SoftMax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`past_key_values` (`List[tf.Tensor]`, *optional*, returned when `use_cache=True`
    is passed or when `config.use_cache=True`) — List of `tf.Tensor` of length `config.n_layers`,
    with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains pre-computed hidden-states (key and values in the attention blocks)
    of the decoder that can be used (see `past_key_values` input) to speed up sequential
    decoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the decoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`decoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`cross_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the decoder’s cross-attention layer, after the attention
    softmax, used to compute the weighted average in the cross-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_last_hidden_state` (`tf.Tensor` of shape `(batch_size, sequence_length,
    hidden_size)`, *optional*) — Sequence of hidden-states at the output of the last
    layer of the encoder of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encoder_hidden_states` (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True`
    is passed or when `config.output_hidden_states=True`) — Tuple of `tf.Tensor` (one
    for the output of the embeddings + one for the output of each layer) of shape
    `(batch_size, sequence_length, hidden_size)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden-states of the encoder at the output of each layer plus the initial embedding
    outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`encoder_attentions` (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True`
    is passed or when `config.output_attentions=True`) — Tuple of `tf.Tensor` (one
    for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attentions weights of the encoder, after the attention softmax, used to compute
    the weighted average in the self-attention heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The [TFSpeech2TextForConditionalGeneration](/docs/transformers/v4.37.2/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration)
    forward method, overrides the `__call__` special method.
  prefs: []
  type: TYPE_NORMAL
- en: Although the recipe for forward pass needs to be defined within this function,
    one should call the `Module` instance afterwards instead of this since the former
    takes care of running the pre and post processing steps while the latter silently
    ignores them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
