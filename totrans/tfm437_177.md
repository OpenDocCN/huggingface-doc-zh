# GPT-NeoX-Japanese

> åŸå§‹æ–‡æœ¬ï¼š[`huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt_neox_japanese`](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/gpt_neox_japanese)

## æ¦‚è¿°

æˆ‘ä»¬ä»‹ç»äº† GPT-NeoX-Japaneseï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¥è¯­çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæ˜¯åœ¨[`github.com/EleutherAI/gpt-neox`](https://github.com/EleutherAI/gpt-neox)çš„åŸºç¡€ä¸Šè®­ç»ƒçš„ã€‚æ—¥è¯­æ˜¯ä¸€ç§å…·æœ‰å¤§é‡è¯æ±‡å’Œå¹³å‡åã€ç‰‡å‡åå’Œæ±‰å­—ä¹¦å†™ç³»ç»Ÿç»„åˆçš„ç‹¬ç‰¹è¯­è¨€ã€‚ä¸ºäº†è§£å†³æ—¥è¯­è¿™ç§ç‹¬ç‰¹ç»“æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[ç‰¹æ®Šçš„å­è¯æ ‡è®°å™¨](https://github.com/tanreinama/Japanese-BPEEncoder_V2)ã€‚æˆ‘ä»¬éå¸¸æ„Ÿè°¢*tanreinama*å¼€æºäº†è¿™ä¸ªéå¸¸æœ‰å¸®åŠ©çš„æ ‡è®°å™¨ã€‚æ ¹æ®è°·æ­Œå…³äº[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)çš„ç ”ç©¶å»ºè®®ï¼Œæˆ‘ä»¬ä»å˜å‹å™¨å—ä¸­åˆ é™¤äº†åç½®å‚æ•°ï¼Œå®ç°äº†æ›´å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚è¯·è¯¦ç»†å‚é˜…[æ­¤æ–‡ç« ](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4)ã€‚

è¯¥æ¨¡å‹çš„å¼€å‘ç”±[Shinya Otani](https://github.com/SO0529)ã€[Takayoshi Makabe](https://github.com/spider-man-tm)ã€[Anuj Arora](https://github.com/Anuj040)å’Œ[Kyo Hattori](https://github.com/go5paopao)é¢†å¯¼ï¼Œæ¥è‡ª[ABEJA, Inc.](https://www.abejainc.com/)ã€‚æœ‰å…³æ­¤æ¨¡å‹æ„å»ºæ´»åŠ¨çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ­¤å¤„ï¼ˆjaï¼‰](https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207)ã€‚

### ä½¿ç”¨ç¤ºä¾‹

`generate()`æ–¹æ³•å¯ç”¨äºä½¿ç”¨ GPT NeoX Japanese æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚

```py
>>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")

>>> prompt = "äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€"

>>> input_ids = tokenizer(prompt, return_tensors="pt").input_ids

>>> gen_tokens = model.generate(
...     input_ids,
...     do_sample=True,
...     temperature=0.9,
...     max_length=100,
... )
>>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]

>>> print(gen_text)
äººã¨AIãŒå”èª¿ã™ã‚‹ãŸã‚ã«ã¯ã€AIã¨äººãŒå…±å­˜ã—ã€AIã‚’æ­£ã—ãç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
```

## èµ„æº

+   å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡æŒ‡å—

## GPTNeoXJapaneseConfig

### `class transformers.GPTNeoXJapaneseConfig`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py#L28)

```py
( vocab_size = 32000 hidden_size = 2560 num_hidden_layers = 32 num_attention_heads = 32 intermediate_multiple_size = 4 hidden_act = 'gelu' rotary_pct = 1.0 rotary_emb_base = 10000 max_position_embeddings = 2048 initializer_range = 0.02 layer_norm_eps = 1e-05 use_cache = True bos_token_id = 31996 eos_token_id = 31999 attention_dropout = 0.1 hidden_dropout = 0.0 **kwargs )
```

å‚æ•°

+   `vocab_size` (`int`, *optional*, defaults to 32000) â€” GPTNeoXJapanese æ¨¡å‹çš„è¯æ±‡å¤§å°ã€‚å®šä¹‰äº†åœ¨è°ƒç”¨`GPTNeoXJapanese`æ—¶ä¼ é€’çš„`inputs_ids`å¯ä»¥è¡¨ç¤ºçš„ä¸åŒæ ‡è®°æ•°é‡ã€‚

+   `hidden_size` (`int`, *optional*, defaults to 2560) â€” ç¼–ç å™¨å±‚å’Œæ± åŒ–å™¨å±‚çš„ç»´åº¦ã€‚

+   `num_hidden_layers` (`int`, *optional*, defaults to 32) â€” Transformer ç¼–ç å™¨ä¸­çš„éšè—å±‚æ•°é‡ã€‚

+   `num_attention_heads` (`int`, *optional*, defaults to 32) â€” Transformer ç¼–ç å™¨ä¸­æ¯ä¸ªæ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¤´æ•°ã€‚

+   `intermediate_multiple_size` (`int`, *optional*, defaults to 4) â€” Transformer ç¼–ç å™¨ä¸­â€œä¸­é—´â€å±‚çš„ç»´åº¦ç”±`hidden_size * intermediate_multiple_size`è®¡ç®—ã€‚

+   `hidden_act` (`str` or `function`, *optional*, defaults to `"gelu"`) â€” ç¼–ç å™¨å’Œæ± åŒ–å™¨ä¸­çš„éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå‡½æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚

+   `rotary_pct` (`float`, *optional*, defaults to 1.00) â€” éšè—ç»´åº¦åˆ†é…ç»™æ—‹è½¬åµŒå…¥çš„ç™¾åˆ†æ¯”ã€‚

+   `rotary_emb_base` (`int`, *optional*, defaults to 10000) â€” ç”¨äºè®¡ç®—æ—‹è½¬åµŒå…¥é¢‘ç‡çš„åŸºæ•°ã€‚

+   `max_position_embeddings` (`int`, *optional*, defaults to 2048) â€” è¯¥æ¨¡å‹å¯èƒ½ä½¿ç”¨çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚

+   `initializer_range` (`float`, *optional*, defaults to 0.02) â€” ç”¨äºåˆå§‹åŒ–æ‰€æœ‰æƒé‡çŸ©é˜µçš„æˆªæ–­æ­£æ€åˆå§‹åŒ–å™¨çš„æ ‡å‡†å·®ã€‚

+   `layer_norm_eps` (`float`, *optional*, defaults to 1e-5) â€” å±‚å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„ epsilonã€‚

+   `use_cache` (`bool`, *optional*, defaults to `True`) â€” æ¨¡å‹æ˜¯å¦åº”è¿”å›æœ€åçš„é”®/å€¼æ³¨æ„åŠ›ï¼ˆä¸æ˜¯æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ï¼‰ã€‚ä»…åœ¨`config.is_decoder=True`æ—¶ç›¸å…³ã€‚

+   `attention_dropout` (`float`, *optional*, defaults to 0.1) â€” æ³¨æ„åŠ›çš„ dropout æ¯”ç‡ã€‚

+   `hidden_dropout` (`float`, *optional*, defaults to 0.0) â€” éšè—å±‚çš„ dropout æ¯”ç‡ã€‚ç¤ºä¾‹ â€”

è¿™æ˜¯ç”¨äºå­˜å‚¨`GPTNeoXModelJapanese`é…ç½®çš„é…ç½®ç±»ã€‚å®ƒç”¨äºæ ¹æ®æŒ‡å®šçš„å‚æ•°å®ä¾‹åŒ–ä¸€ä¸ª GPTNeoX æ¨¡å‹ï¼Œå®šä¹‰æ¨¡å‹æ¶æ„ã€‚ä½¿ç”¨é»˜è®¤å€¼å®ä¾‹åŒ–é…ç½®å°†äº§ç”Ÿç±»ä¼¼äº GPTNeoXJapanese [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b)æ¶æ„çš„é…ç½®ã€‚

é…ç½®å¯¹è±¡ç»§æ‰¿è‡ª PretrainedConfigï¼Œå¯ç”¨äºæ§åˆ¶æ¨¡å‹è¾“å‡ºã€‚é˜…è¯»æ¥è‡ª PretrainedConfig çš„æ–‡æ¡£ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚é»˜è®¤é…ç½®è®¾ç½®ä¸º 2.7B æ¨¡å‹

```py
>>> from transformers import GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel

>>> # Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration
>>> configuration = GPTNeoXJapaneseConfig()

>>> # Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration
>>> model = GPTNeoXJapaneseModel(configuration)

>>> # Accessing the model configuration
>>> configuration = model.config
```

## GPTNeoXJapaneseTokenizer

### `class transformers.GPTNeoXJapaneseTokenizer`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L66)

```py
( vocab_file emoji_file unk_token = '<|endoftext|>' pad_token = '<|endoftext|>' bos_token = '<|startoftext|>' eos_token = '<|endoftext|>' do_clean_text = False **kwargs )
```

å‚æ•°

+   `vocab_file`ï¼ˆ`str`ï¼‰â€” åŒ…å«è¯æ±‡è¡¨çš„æ–‡ä»¶ã€‚

+   `emoji_file`ï¼ˆ`str`ï¼‰â€” åŒ…å«è¡¨æƒ…ç¬¦å·çš„æ–‡ä»¶ã€‚

+   `unk_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--æœªçŸ¥ä»¤ç‰Œã€‚è¯æ±‡è¡¨ä¸­æ²¡æœ‰çš„ä»¤ç‰Œæ— æ³•è½¬æ¢ä¸º IDï¼Œè€Œæ˜¯è®¾ç½®ä¸ºè¯¥ä»¤ç‰Œã€‚

+   `pad_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--ç”¨äºå¡«å……çš„ä»¤ç‰Œ

+   `bos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|startoftext|>"``ï¼‰--åºåˆ—æ ‡è®°çš„å¼€å¤´ã€‚

+   `eos_token`ï¼ˆ`str`ï¼Œ*optional*ï¼Œé»˜è®¤ä¸º``"<|endoftext|>"``ï¼‰--åºåˆ—ç»“æŸæ ‡è®°ã€‚

+   `do_clean_text`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`False`ï¼‰â€” æ˜¯å¦æ¸…ç†æ–‡æœ¬ä»¥ç”¨äº URLã€EMAILã€TELã€æ—¥æœ¬æ—¥æœŸå’Œæ—¥æœ¬ä»·æ ¼ã€‚

æ­¤åˆ†è¯å™¨ç»§æ‰¿è‡ª PreTrainedTokenizerï¼ŒåŸºäºç”¨äºæ­¤å­˜å‚¨åº“çš„æ—¥è¯­ç‰¹æ®Šå­è¯ç¼–ç ï¼ˆ[`github.com/tanreinama/Japanese-BPEEncoder_V2`](https://github.com/tanreinama/Japanese-BPEEncoder_V2)ï¼‰ã€‚æŸ¥çœ‹å­˜å‚¨åº“ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚æ—¥è¯­å…·æœ‰ç›¸å¯¹è¾ƒå¤§çš„è¯æ±‡é‡ï¼Œå•è¯ä¹‹é—´æ²¡æœ‰åˆ†éš”ã€‚æ­¤å¤–ï¼Œè¯¥è¯­è¨€æ˜¯å¹³å‡åã€ç‰‡å‡åå’Œæ±‰å­—çš„ç»„åˆï¼Œç»å¸¸ä½¿ç”¨å˜ä½“å¦‚â€œ1â€å’Œâ€œâ‘ â€ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æƒ…å†µï¼Œæ­¤åˆ†è¯å™¨å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹

+   é€å­—åˆ†å‰²ï¼Œä»‹äºå­—èŠ‚å­—ç¬¦ä¸²å’Œå½¢æ€åˆ†æä¹‹é—´ã€‚

+   ä¸ºæ¯ä¸ªæ±‰å­—ã€å¹³å‡åå’Œç‰‡å‡åå­—ç¬¦åˆ›å»º BPEï¼Œå¹¶ä¸”æ²¡æœ‰è·¨å­—ç¬¦ç±»å‹çš„ BPEï¼Œä¾‹å¦‚æ±‰å­—+å¹³å‡åæˆ–å¹³å‡å+ç‰‡å‡åã€‚

+   ä¸éœ€è¦<unk>çš„å…¨å­—èŠ‚ç¼–ç ã€‚

+   ç‹¬ç«‹äº 2 å­—èŠ‚å’Œ 3 å­—èŠ‚å­—ç¬¦ç­‰ UTF ä»£ç 

+   å¼‚ä½“å­—è½¬æ¢ä¸ºç›¸åŒçš„æ ‡è®° ID

+   è¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·è¢«åˆ†ä¸º 12 ç§ç±»å‹ä½œä¸ºç‰¹æ®Šæ ‡ç­¾ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import GPTNeoXJapaneseTokenizer

>>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> # You can confirm both æ…¶å¿œ and æ…¶æ‡‰ are encoded to 17749
>>> tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"]
[30014, 26883, 26638, 27228, 25, 26650, 31732, 31679, 27809, 26638, 17749, 31592, 17749, 31593, 321, 1281]

>>> # Both æ…¶å¿œ and æ…¶æ‡‰ are decoded to æ…¶å¿œ
>>> tokenizer.decode(tokenizer("å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶æ‡‰)å¤§å­¦å‡ºèº«")["input_ids"])
'å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ğŸ¯ã€‚å®Ÿã¯æ…¶å¿œ(æ…¶å¿œ)å¤§å­¦å‡ºèº«'
```

#### `convert_tokens_to_string`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L173)

```py
( tokens )
```

å°†ä¸€ç³»åˆ—æ ‡è®°ï¼ˆå­—ç¬¦ä¸²ï¼‰è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ä¸²ã€‚

## GPTNeoXJapaneseModel

### `class transformers.GPTNeoXJapaneseModel`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L438)

```py
( config )
```

å‚æ•°

+   `config`ï¼ˆ~GPTNeoXJapaneseConfigï¼‰â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼Œåªä¼šåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained()æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

è£¸çš„ GPTNeoXJapanese æ¨¡å‹å˜å‹å™¨è¾“å‡ºåŸå§‹éšè—çŠ¶æ€ï¼Œæ²¡æœ‰ç‰¹å®šçš„å¤´éƒ¨ã€‚æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

#### `forward`

[<æ¥æº>](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L462)

```py
( input_ids: Optional = None attention_mask: Optional = None head_mask: Optional = None inputs_embeds: Optional = None past_key_values: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.BaseModelOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—æ ‡è®°çš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” é¿å…åœ¨å¡«å……æ ‡è®°ç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¸­ï¼š

    +   1 è¡¨ç¤ºæœªè¢« `masked` çš„æ ‡è®°ï¼Œ

    +   0 è¡¨ç¤ºè¢« `masked` çš„æ ‡è®°ã€‚

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ®µæ ‡è®°ç´¢å¼•ï¼ŒæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* æ ‡è®°ï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* æ ‡è®°ã€‚

+   `position_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´ `[0, config.max_position_embeddings - 1]` ä¸­é€‰æ‹©ã€‚

+   `head_mask` (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*) â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é€‰æ‹©çš„å¤´éƒ¨å¤±æ•ˆçš„æ©ç ã€‚æ©ç å€¼é€‰æ‹©åœ¨ `[0, 1]` ä¹‹é—´ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢« `masked`ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢« `masked`ã€‚

+   `inputs_embeds` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºè€Œä¸æ˜¯ä¼ é€’ `input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒæ¥å°† *input_ids* ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `attentions`ã€‚

+   `output_hidden_states` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è¿”å›çš„å¼ é‡ä¸‹çš„ `hidden_states`ã€‚

+   `return_dict` (`bool`, *optional*) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª ModelOutput è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šå…ƒç»„ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`ï¼Œé•¿åº¦ä¸º `config.n_layers`ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 4 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)` çš„å¼ é‡ï¼‰ â€” åŒ…å«æ³¨æ„åŠ›å—çš„é¢„è®¡ç®—é”®å’Œå€¼éšè—çŠ¶æ€ã€‚å¯ç”¨äºåŠ é€Ÿè§£ç ã€‚å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„ `decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å®ƒä»¬çš„è¿‡å»é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹çš„ï¼‰çš„å½¢çŠ¶ä¸º `(batch_size, 1)` çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º `(batch_size, sequence_length)` çš„æ‰€æœ‰ `decoder_input_ids`ã€‚

+   `use_cache` (`bool`, *optional*) â€” å¦‚æœè®¾ç½®ä¸º `True`ï¼Œåˆ™è¿”å› `past_key_values` é”®å€¼çŠ¶æ€ï¼Œå¹¶å¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆå‚è§ `past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.BaseModelOutputWithPast æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.BaseModelOutputWithPast æˆ–ä¸€ä¸ª `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœä¼ é€’äº† `return_dict=False` æˆ–å½“ `config.return_dict=False` æ—¶ï¼‰åŒ…å«æ ¹æ®é…ç½®ï¼ˆGPTNeoXJapaneseConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `last_hidden_state` (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) â€” æ¨¡å‹æœ€åä¸€å±‚çš„éšè—çŠ¶æ€åºåˆ—ã€‚

    å¦‚æœä½¿ç”¨äº† `past_key_values`ï¼Œåˆ™ä»…è¾“å‡ºå½¢çŠ¶ä¸º `(batch_size, 1, hidden_size)` çš„åºåˆ—çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚

+   `past_key_values` (`tuple(tuple(torch.FloatTensor))`, *optional*, å½“ä¼ é€’ `use_cache=True` æˆ–è€…å½“ `config.use_cache=True` æ—¶è¿”å›) â€” é•¿åº¦ä¸º `config.n_layers` çš„ `tuple(torch.FloatTensor)` å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, embed_size_per_head)` çš„å¼ é‡ï¼Œå¦‚æœ `config.is_encoder_decoder=True` è¿˜æœ‰ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)` çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œä»¥åŠåœ¨äº¤å‰æ³¨æ„åŠ›å—ä¸­å¯é€‰åœ°å¦‚æœ `config.is_encoder_decoder=True`ï¼‰å¯ä»¥ä½¿ç”¨ï¼ˆè¯·å‚è§ `past_key_values` è¾“å…¥ï¼‰ä»¥åŠ å¿«é¡ºåºè§£ç ã€‚

+   `hidden_states` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_hidden_states=True` æˆ–è€…å½“ `config.output_hidden_states=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, sequence_length, hidden_size)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆå¦‚æœæ¨¡å‹æœ‰åµŒå…¥å±‚ï¼Œåˆ™ä¸ºåµŒå…¥çš„è¾“å‡º + æ¯å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºå¤„çš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions` (`tuple(torch.FloatTensor)`, *optional*, å½“ä¼ é€’ `output_attentions=True` æˆ–è€…å½“ `config.output_attentions=True` æ—¶è¿”å›) â€” å½¢çŠ¶ä¸º `(batch_size, num_heads, sequence_length, sequence_length)` çš„ `torch.FloatTensor` å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

GPTNeoXJapaneseModel å‰å‘æ–¹æ³•ï¼Œè¦†ç›–äº† `__call__` ç‰¹æ®Šæ–¹æ³•ã€‚

è™½ç„¶å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨ `Module` å®ä¾‹ï¼Œè€Œä¸æ˜¯åœ¨æ­¤ä¹‹åè°ƒç”¨ï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™ä¼šé»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, GPTNeoXJapaneseModel
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> model = GPTNeoXJapaneseModel.from_pretrained("abeja/gpt-neox-japanese-2.7b")

>>> inputs = tokenizer("æ—¥æœ¬èªã®GPT-neoxãŒHugging Faceã§ä½¿ãˆã¾ã™ğŸ˜€", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
```

## GPTNeoXJapaneseForCausalLM

### `class transformers.GPTNeoXJapaneseForCausalLM`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L593)

```py
( config )
```

å‚æ•°

+   `config` (~GPTNeoXJapaneseConfig) â€” å…·æœ‰æ¨¡å‹æ‰€æœ‰å‚æ•°çš„æ¨¡å‹é…ç½®ç±»ã€‚ä½¿ç”¨é…ç½®æ–‡ä»¶åˆå§‹åŒ–ä¸ä¼šåŠ è½½ä¸æ¨¡å‹å…³è”çš„æƒé‡ï¼ŒåªåŠ è½½é…ç½®ã€‚æŸ¥çœ‹ from_pretrained() æ–¹æ³•ä»¥åŠ è½½æ¨¡å‹æƒé‡ã€‚

GPTNeoXJapanese æ¨¡å‹åœ¨é¡¶éƒ¨å…·æœ‰ç”¨äºåˆ†ç±»æ¨¡å‹å¾®è°ƒçš„ `è¯­è¨€å»ºæ¨¡` å¤´ã€‚æ­¤æ¨¡å‹æ˜¯ PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) çš„å­ç±»ã€‚å°†å…¶ç”¨ä½œå¸¸è§„ PyTorch æ¨¡å—ï¼Œå¹¶å‚è€ƒ PyTorch æ–‡æ¡£ä»¥è·å–æœ‰å…³ä¸€èˆ¬ç”¨æ³•å’Œè¡Œä¸ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚

#### `forward`

[< source >](https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L616)

```py
( input_ids: Optional = None attention_mask: Optional = None inputs_embeds: Optional = None head_mask: Optional = None past_key_values: Optional = None labels: Optional = None use_cache: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None return_dict: Optional = None ) â†’ export const metadata = 'undefined';transformers.modeling_outputs.CausalLMOutputWithPast or tuple(torch.FloatTensor)
```

å‚æ•°

+   `input_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`) â€” è¯æ±‡è¡¨ä¸­è¾“å…¥åºåˆ—ä»¤ç‰Œçš„ç´¢å¼•ã€‚

    å¯ä»¥ä½¿ç”¨ AutoTokenizer è·å–ç´¢å¼•ã€‚

+   `attention_mask` (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” ç”¨äºé¿å…åœ¨å¡«å……ä»¤ç‰Œç´¢å¼•ä¸Šæ‰§è¡Œæ³¨æ„åŠ›çš„æ©ç ã€‚æ©ç å€¼åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   1 ç”¨äº `not masked` çš„ä»¤ç‰Œï¼Œ

    +   0 ç”¨äº `masked` çš„ä»¤ç‰Œã€‚

+   `token_type_ids` (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*) â€” æ®µè½ä»¤ç‰Œç´¢å¼•ï¼Œç”¨äºæŒ‡ç¤ºè¾“å…¥çš„ç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†ã€‚ç´¢å¼•åœ¨ `[0, 1]` ä¸­é€‰æ‹©ï¼š

    +   0 å¯¹åº”äº *å¥å­ A* ä»¤ç‰Œï¼Œ

    +   1 å¯¹åº”äº *å¥å­ B* ä»¤ç‰Œã€‚

+   `position_ids`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” æ¯ä¸ªè¾“å…¥åºåˆ—æ ‡è®°åœ¨ä½ç½®åµŒå…¥ä¸­çš„ä½ç½®ç´¢å¼•ã€‚åœ¨èŒƒå›´`[0, config.max_position_embeddings - 1]`ä¸­é€‰æ‹©ã€‚

+   `head_mask`ï¼ˆå½¢çŠ¶ä¸º`(num_heads,)`æˆ–`(num_layers, num_heads)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºä½¿è‡ªæ³¨æ„åŠ›æ¨¡å—çš„é€‰å®šå¤´éƒ¨æ— æ•ˆçš„æ©ç ã€‚åœ¨`[0, 1]`ä¸­é€‰æ‹©çš„æ©ç å€¼ï¼š

    +   1 è¡¨ç¤ºå¤´éƒ¨æœªè¢«æ©ç›–ï¼Œ

    +   0 è¡¨ç¤ºå¤´éƒ¨è¢«æ©ç›–ã€‚

+   `inputs_embeds`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€” å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ç›´æ¥ä¼ é€’åµŒå…¥è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ä¼ é€’`input_ids`ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´å¤šæ§åˆ¶æƒï¼Œä»¥ä¾¿å°†*input_ids*ç´¢å¼•è½¬æ¢ä¸ºç›¸å…³å‘é‡ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å†…éƒ¨åµŒå…¥æŸ¥æ‰¾çŸ©é˜µã€‚

+   `output_attentions`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æ³¨æ„åŠ›å¼ é‡ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`attentions`ã€‚

+   `output_hidden_states`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§è¿”å›å¼ é‡ä¸‹çš„`hidden_states`ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” æ˜¯å¦è¿”å› ModelOutput è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡å’Œ 2 ä¸ªé¢å¤–çš„å½¢çŠ¶ä¸º`(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`çš„å¼ é‡ã€‚å½“æ¨¡å‹ç”¨ä½œåºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­çš„è§£ç å™¨æ—¶ï¼Œåªæœ‰åœ¨éœ€è¦æ—¶æ‰éœ€è¦è¿™ä¸¤ä¸ªé¢å¤–çš„å¼ é‡ã€‚

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆè¯·å‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

    å¦‚æœä½¿ç”¨äº†`past_key_values`ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä»…è¾“å…¥æœ€åçš„`decoder_input_ids`ï¼ˆè¿™äº›æ²¡æœ‰å°†å…¶è¿‡å»çš„é”®å€¼çŠ¶æ€æä¾›ç»™æ­¤æ¨¡å‹ï¼‰çš„å½¢çŠ¶ä¸º`(batch_size, 1)`çš„å¼ é‡ï¼Œè€Œä¸æ˜¯å½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„æ‰€æœ‰`decoder_input_ids`ã€‚

+   `labels`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length)`çš„`torch.LongTensor`ï¼Œ*å¯é€‰*ï¼‰â€” ç”¨äºè®¡ç®—ä»å·¦åˆ°å³çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆä¸‹ä¸€ä¸ªè¯é¢„æµ‹ï¼‰çš„æ ‡ç­¾ã€‚ç´¢å¼•åº”åœ¨`[-100, 0, ..., config.vocab_size]`ï¼ˆè¯·å‚è§`input_ids`æ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ä¸­ã€‚ç´¢å¼•è®¾ç½®ä¸º`-100`çš„æ ‡è®°å°†è¢«å¿½ç•¥ï¼ˆæ©ç›–ï¼‰ï¼Œä»…å¯¹æ ‡ç­¾ä¸º`[0, ..., config.vocab_size]`çš„æ ‡è®°è®¡ç®—æŸå¤±ã€‚

+   `use_cache`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼‰â€” å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™è¿”å›`past_key_values`é”®å€¼çŠ¶æ€ï¼Œå¯ç”¨äºåŠ é€Ÿè§£ç ï¼ˆè¯·å‚è§`past_key_values`ï¼‰ã€‚

è¿”å›

transformers.modeling_outputs.CausalLMOutputWithPast æˆ– `tuple(torch.FloatTensor)`

ä¸€ä¸ª transformers.modeling_outputs.CausalLMOutputWithPast æˆ–ä¸€ä¸ª`torch.FloatTensor`å…ƒç»„ï¼ˆå¦‚æœä¼ é€’`return_dict=False`æˆ–`config.return_dict=False`æ—¶ï¼‰åŒ…æ‹¬æ ¹æ®é…ç½®ï¼ˆGPTNeoXJapaneseConfigï¼‰å’Œè¾“å…¥çš„ä¸åŒå…ƒç´ ã€‚

+   `loss`ï¼ˆå½¢çŠ¶ä¸º`(1,)`çš„`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼Œå½“æä¾›`labels`æ—¶è¿”å›ï¼‰â€” è¯­è¨€å»ºæ¨¡æŸå¤±ï¼ˆç”¨äºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼‰ã€‚

+   `logits`ï¼ˆå½¢çŠ¶ä¸º`(batch_size, sequence_length, config.vocab_size)`çš„`torch.FloatTensor`ï¼‰â€” è¯­è¨€å»ºæ¨¡å¤´çš„é¢„æµ‹åˆ†æ•°ï¼ˆSoftMax ä¹‹å‰æ¯ä¸ªè¯æ±‡æ ‡è®°çš„åˆ†æ•°ï¼‰ã€‚

+   `past_key_values`ï¼ˆ`tuple(tuple(torch.FloatTensor))`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`use_cache=True`æˆ–`config.use_cache=True`æ—¶è¿”å›ï¼‰ â€” é•¿åº¦ä¸º`config.n_layers`çš„`tuple(torch.FloatTensor)`å…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„æœ‰ 2 ä¸ªå½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, embed_size_per_head)`çš„å¼ é‡ï¼‰

    åŒ…å«é¢„å…ˆè®¡ç®—çš„éšè—çŠ¶æ€ï¼ˆè‡ªæ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼‰ï¼Œå¯ç”¨äºåŠ é€Ÿé¡ºåºè§£ç ï¼ˆå‚è§`past_key_values`è¾“å…¥ï¼‰ã€‚

+   `hidden_states`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_hidden_states=True`æˆ–`config.output_hidden_states=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, sequence_length, hidden_size)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆä¸€ä¸ªç”¨äºåµŒå…¥çš„è¾“å‡ºï¼Œå¦‚æœæ¨¡å‹æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œ+ ä¸€ä¸ªç”¨äºæ¯ä¸€å±‚çš„è¾“å‡ºï¼‰ã€‚

    æ¨¡å‹åœ¨æ¯ä¸€å±‚è¾“å‡ºçš„éšè—çŠ¶æ€åŠ ä¸Šå¯é€‰çš„åˆå§‹åµŒå…¥è¾“å‡ºã€‚

+   `attentions`ï¼ˆ`tuple(torch.FloatTensor)`ï¼Œ*å¯é€‰*ï¼Œå½“ä¼ é€’`output_attentions=True`æˆ–`config.output_attentions=True`æ—¶è¿”å›ï¼‰ â€” å½¢çŠ¶ä¸º`(batch_size, num_heads, sequence_length, sequence_length)`çš„`torch.FloatTensor`å…ƒç»„ï¼ˆæ¯å±‚ä¸€ä¸ªï¼‰ã€‚

    åœ¨æ³¨æ„åŠ› softmax ä¹‹åçš„æ³¨æ„åŠ›æƒé‡ï¼Œç”¨äºè®¡ç®—è‡ªæ³¨æ„åŠ›å¤´ä¸­çš„åŠ æƒå¹³å‡å€¼ã€‚

GPTNeoXJapaneseForCausalLM çš„å‰å‘æ–¹æ³•é‡å†™äº†`__call__`ç‰¹æ®Šæ–¹æ³•ã€‚

å°½ç®¡å‰å‘ä¼ é€’çš„é…æ–¹éœ€è¦åœ¨æ­¤å‡½æ•°å†…å®šä¹‰ï¼Œä½†åº”è¯¥åœ¨æ­¤ä¹‹åè°ƒç”¨`Module`å®ä¾‹ï¼Œè€Œä¸æ˜¯è¿™ä¸ªï¼Œå› ä¸ºå‰è€…è´Ÿè´£è¿è¡Œé¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œè€Œåè€…åˆ™é»˜é»˜åœ°å¿½ç•¥å®ƒä»¬ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from transformers import AutoTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> config = GPTNeoXJapaneseConfig.from_pretrained("abeja/gpt-neox-japanese-2.7b")
>>> config.is_decoder = True
>>> model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b", config=config)

>>> inputs = tokenizer("æ—¥æœ¬èªã®GPT-neoxãŒHugging Faceã§ä½¿ãˆã¾ã™ğŸ˜€", return_tensors="pt")
>>> outputs = model(**inputs)

>>> prediction_logits = outputs.logits
```
