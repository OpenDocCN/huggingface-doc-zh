["```py\n>>> from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n>>> import torch\n\n>>> src_text = [\n...     \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n... ]\n\n... model_name = \"google/pegasus-xsum\"\n... device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n... tokenizer = PegasusTokenizer.from_pretrained(model_name)\n... model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n... batch = tokenizer(src_text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n... translated = model.generate(**batch)\n... tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n... assert (\n...     tgt_text[0]\n...     == \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n... )\n```", "```py\n>>> from transformers import PegasusConfig, PegasusModel\n\n>>> # Initializing a PEGASUS google/pegasus-large style configuration\n>>> configuration = PegasusConfig()\n\n>>> # Initializing a model (with random weights) from the google/pegasus-large style configuration\n>>> model = PegasusModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n>>> from transformers import AutoTokenizer, PegasusModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n>>> model = PegasusModel.from_pretrained(\"google/pegasus-large\")\n\n>>> inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n>>> decoder_inputs = tokenizer(\"Studies show that\", return_tensors=\"pt\")\n>>> outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n\n>>> last_hidden_states = outputs.last_hidden_state\n>>> list(last_hidden_states.shape)\n[1, 4, 1024]\n```", "```py\n>>> from transformers import AutoTokenizer, PegasusForConditionalGeneration\n\n>>> model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs[\"input_ids\"])\n>>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n\"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n```", "```py\n>>> from transformers import AutoTokenizer, PegasusForCausalLM\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n>>> model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)\n>>> assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n\n>>> logits = outputs.logits\n>>> expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n>>> list(logits.shape) == expected_shape\nTrue\n```", "```py\n>>> from transformers import AutoTokenizer, TFPegasusModel\n>>> import tensorflow as tf\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n>>> model = TFPegasusModel.from_pretrained(\"google/pegasus-large\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n>>> outputs = model(inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, TFPegasusForConditionalGeneration\n\n>>> model = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n\n>>> ARTICLE_TO_SUMMARIZE = (\n...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n... )\n>>> inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"tf\")\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(input_ids)\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxPegasusModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n>>> model = FlaxPegasusModel.from_pretrained(\"google/pegasus-large\")\n\n>>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n>>> outputs = model(**inputs)\n\n>>> last_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> last_decoder_hidden_states = outputs.last_hidden_state\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n>>> tokenizer = AutoTokenizer.from_pretrained('google/pegasus-large')\n\n>>> ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='np')\n\n>>> # Generate Summary\n>>> summary_ids = model.generate(inputs['input_ids']).sequences\n>>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n>>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n>>> input_ids = tokenizer([TXT], return_tensors=\"np\")[\"input_ids\"]\n>>> logits = model(input_ids).logits\n\n>>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n>>> probs = jax.nn.softmax(logits[0, masked_index], axis=0)\n>>> values, predictions = jax.lax.top_k(probs)\n\n>>> tokenizer.decode(predictions).split()\n```", "```py\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n```", "```py\n>>> import jax.numpy as jnp\n>>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n\n>>> model = FlaxPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-large\")\n\n>>> text = \"My friends are cool but they eat too many carbs.\"\n>>> inputs = tokenizer(text, max_length=1024, return_tensors=\"np\")\n>>> encoder_outputs = model.encode(**inputs)\n\n>>> decoder_start_token_id = model.config.decoder_start_token_id\n>>> decoder_input_ids = jnp.ones((inputs.input_ids.shape[0], 1), dtype=\"i4\") * decoder_start_token_id\n\n>>> outputs = model.decode(decoder_input_ids, encoder_outputs)\n>>> logits = outputs.logits\n```"]