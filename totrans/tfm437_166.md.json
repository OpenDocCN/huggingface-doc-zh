["```py\npip install --upgrade pip\npip install --upgrade transformers g2p-en\n```", "```py\n\nfrom transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerModel, FastSpeech2ConformerHifiGan\nimport soundfile as sf\n\ntokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\ninputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\n\nmodel = FastSpeech2ConformerModel.from_pretrained(\"espnet/fastspeech2_conformer\")\noutput_dict = model(input_ids, return_dict=True)\nspectrogram = output_dict[\"spectrogram\"]\n\nhifigan = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\nwaveform = hifigan(spectrogram)\n\nsf.write(\"speech.wav\", waveform.squeeze().detach().numpy(), samplerate=22050)\n```", "```py\nfrom transformers import FastSpeech2ConformerTokenizer, FastSpeech2ConformerWithHifiGan\nimport soundfile as sf\n\ntokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\ninputs = tokenizer(\"Hello, my dog is cute.\", return_tensors=\"pt\")\ninput_ids = inputs[\"input_ids\"]\n\nmodel = FastSpeech2ConformerWithHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_with_hifigan\")\noutput_dict = model(input_ids, return_dict=True)\nwaveform = output_dict[\"waveform\"]\n\nsf.write(\"speech.wav\", waveform.squeeze().detach().numpy(), samplerate=22050)\n```", "```py\nfrom transformers import pipeline, FastSpeech2ConformerHifiGan\nimport soundfile as sf\n\nvocoder = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\nsynthesiser = pipeline(model=\"espnet/fastspeech2_conformer\", vocoder=vocoder)\n\nspeech = synthesiser(\"Hello, my dog is cooler than you!\")\n\nsf.write(\"speech.wav\", speech[\"audio\"].squeeze(), samplerate=speech[\"sampling_rate\"])\n```", "```py\n( hidden_size = 384 vocab_size = 78 num_mel_bins = 80 encoder_num_attention_heads = 2 encoder_layers = 4 encoder_linear_units = 1536 decoder_layers = 4 decoder_num_attention_heads = 2 decoder_linear_units = 1536 speech_decoder_postnet_layers = 5 speech_decoder_postnet_units = 256 speech_decoder_postnet_kernel = 5 positionwise_conv_kernel_size = 3 encoder_normalize_before = False decoder_normalize_before = False encoder_concat_after = False decoder_concat_after = False reduction_factor = 1 speaking_speed = 1.0 use_macaron_style_in_conformer = True use_cnn_in_conformer = True encoder_kernel_size = 7 decoder_kernel_size = 31 duration_predictor_layers = 2 duration_predictor_channels = 256 duration_predictor_kernel_size = 3 energy_predictor_layers = 2 energy_predictor_channels = 256 energy_predictor_kernel_size = 3 energy_predictor_dropout = 0.5 energy_embed_kernel_size = 1 energy_embed_dropout = 0.0 stop_gradient_from_energy_predictor = False pitch_predictor_layers = 5 pitch_predictor_channels = 256 pitch_predictor_kernel_size = 5 pitch_predictor_dropout = 0.5 pitch_embed_kernel_size = 1 pitch_embed_dropout = 0.0 stop_gradient_from_pitch_predictor = True encoder_dropout_rate = 0.2 encoder_positional_dropout_rate = 0.2 encoder_attention_dropout_rate = 0.2 decoder_dropout_rate = 0.2 decoder_positional_dropout_rate = 0.2 decoder_attention_dropout_rate = 0.2 duration_predictor_dropout_rate = 0.2 speech_decoder_postnet_dropout = 0.5 max_source_positions = 5000 use_masking = True use_weighted_masking = False num_speakers = None num_languages = None speaker_embed_dim = None is_encoder_decoder = True **kwargs )\n```", "```py\n>>> from transformers import FastSpeech2ConformerModel, FastSpeech2ConformerConfig\n\n>>> # Initializing a FastSpeech2Conformer style configuration\n>>> configuration = FastSpeech2ConformerConfig()\n\n>>> # Initializing a model from the FastSpeech2Conformer style configuration\n>>> model = FastSpeech2ConformerModel(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( model_in_dim = 80 upsample_initial_channel = 512 upsample_rates = [8, 8, 2, 2] upsample_kernel_sizes = [16, 16, 4, 4] resblock_kernel_sizes = [3, 7, 11] resblock_dilation_sizes = [[1, 3, 5], [1, 3, 5], [1, 3, 5]] initializer_range = 0.01 leaky_relu_slope = 0.1 normalize_before = True **kwargs )\n```", "```py\n>>> from transformers import FastSpeech2ConformerHifiGan, FastSpeech2ConformerHifiGanConfig\n\n>>> # Initializing a FastSpeech2ConformerHifiGan configuration\n>>> configuration = FastSpeech2ConformerHifiGanConfig()\n\n>>> # Initializing a model (with random weights) from the configuration\n>>> model = FastSpeech2ConformerHifiGan(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( model_config: Dict = None vocoder_config: Dict = None **kwargs )\n```", "```py\n>>> from transformers import (\n...     FastSpeech2ConformerConfig,\n...     FastSpeech2ConformerHifiGanConfig,\n...     FastSpeech2ConformerWithHifiGanConfig,\n...     FastSpeech2ConformerWithHifiGan,\n... )\n\n>>> # Initializing FastSpeech2ConformerWithHifiGan sub-modules configurations.\n>>> model_config = FastSpeech2ConformerConfig()\n>>> vocoder_config = FastSpeech2ConformerHifiGanConfig()\n\n>>> # Initializing a FastSpeech2ConformerWithHifiGan module style configuration\n>>> configuration = FastSpeech2ConformerWithHifiGanConfig(model_config.to_dict(), vocoder_config.to_dict())\n\n>>> # Initializing a model (with random weights)\n>>> model = FastSpeech2ConformerWithHifiGan(configuration)\n\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( vocab_file bos_token = '<sos/eos>' eos_token = '<sos/eos>' pad_token = '<blank>' unk_token = '<unk>' should_strip_spaces = False **kwargs )\n```", "```py\n( text: Union = None text_pair: Union = None text_target: Union = None text_pair_target: Union = None add_special_tokens: bool = True padding: Union = False truncation: Union = None max_length: Optional = None stride: int = 0 is_split_into_words: bool = False pad_to_multiple_of: Optional = None return_tensors: Union = None return_token_type_ids: Optional = None return_attention_mask: Optional = None return_overflowing_tokens: bool = False return_special_tokens_mask: bool = False return_offsets_mapping: bool = False return_length: bool = False verbose: bool = True **kwargs ) \u2192 export const metadata = 'undefined';BatchEncoding\n```", "```py\n( save_directory: str filename_prefix: Optional = None ) \u2192 export const metadata = 'undefined';Tuple(str)\n```", "```py\n( token_ids **kwargs )\n```", "```py\n( sequences: Union skip_special_tokens: bool = False clean_up_tokenization_spaces: bool = None **kwargs ) \u2192 export const metadata = 'undefined';List[str]\n```", "```py\n( config: FastSpeech2ConformerConfig )\n```", "```py\n( input_ids: LongTensor attention_mask: Optional = None spectrogram_labels: Optional = None duration_labels: Optional = None pitch_labels: Optional = None energy_labels: Optional = None speaker_ids: Optional = None lang_ids: Optional = None speaker_embedding: Optional = None return_dict: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import (\n...     FastSpeech2ConformerTokenizer,\n...     FastSpeech2ConformerModel,\n...     FastSpeech2ConformerHifiGan,\n... )\n\n>>> tokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\n>>> inputs = tokenizer(\"some text to convert to speech\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n\n>>> model = FastSpeech2ConformerModel.from_pretrained(\"espnet/fastspeech2_conformer\")\n>>> output_dict = model(input_ids, return_dict=True)\n>>> spectrogram = output_dict[\"spectrogram\"]\n\n>>> vocoder = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\n>>> waveform = vocoder(spectrogram)\n>>> print(waveform.shape)\ntorch.Size([1, 49664])\n```", "```py\n( config: FastSpeech2ConformerHifiGanConfig )\n```", "```py\n( spectrogram: FloatTensor ) \u2192 export const metadata = 'undefined';torch.FloatTensor\n```", "```py\n( config: FastSpeech2ConformerWithHifiGanConfig )\n```", "```py\n( input_ids: LongTensor attention_mask: Optional = None spectrogram_labels: Optional = None duration_labels: Optional = None pitch_labels: Optional = None energy_labels: Optional = None speaker_ids: Optional = None lang_ids: Optional = None speaker_embedding: Optional = None return_dict: Optional = None output_attentions: Optional = None output_hidden_states: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.fastspeech2_conformer.modeling_fastspeech2_conformer.FastSpeech2ConformerWithHifiGanOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import (\n...     FastSpeech2ConformerTokenizer,\n...     FastSpeech2ConformerWithHifiGan,\n... )\n\n>>> tokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\n>>> inputs = tokenizer(\"some text to convert to speech\", return_tensors=\"pt\")\n>>> input_ids = inputs[\"input_ids\"]\n\n>>> model = FastSpeech2ConformerWithHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_with_hifigan\")\n>>> output_dict = model(input_ids, return_dict=True)\n>>> waveform = output_dict[\"waveform\"]\n>>> print(waveform.shape)\ntorch.Size([1, 49664])\n```"]