["```py\ndef compute_metrics(params):\n    if params.metric == \"custom\":\n        metric_file = hf_hub_download(\n            repo_id=params.competition_id,\n            filename=\"metric.py\",\n            token=params.token,\n            repo_type=\"dataset\",\n        )\n        sys.path.append(os.path.dirname(metric_file))\n        metric = importlib.import_module(\"metric\")\n        evaluation = metric.compute(params)\n    .\n    .\n    .\n```", "```py\nclass EvalParams(BaseModel):\n    competition_id: str\n    competition_type: str\n    metric: str\n    token: str\n    team_id: str\n    submission_id: str\n    submission_id_col: str\n    submission_cols: List[str]\n    submission_rows: int\n    output_path: str\n    submission_repo: str\n    time_limit: int\n    dataset: str  # private test dataset, used only for script competitions\n```", "```py\n{\n    \"public_score\": {\n        \"metric1\": metric_value,\n    },,\n    \"private_score\": {\n        \"metric1\": metric_value,\n    },,\n}\n```", "```py\n{\n    \"public_score\": {\n        \"metric1\": metric_value,\n        \"metric2\": metric_value,\n    },\n    \"private_score\": {\n        \"metric1\": metric_value,\n        \"metric2\": metric_value,\n    },\n}\n```", "```py\nimport pandas as pd\nfrom huggingface_hub import hf_hub_download\n\ndef compute(params):\n    solution_file = hf_hub_download(\n        repo_id=params.competition_id,\n        filename=\"solution.csv\",\n        token=params.token,\n        repo_type=\"dataset\",\n    )\n\n    solution_df = pd.read_csv(solution_file)\n\n    submission_filename = f\"submissions/{params.team_id}-{params.submission_id}.csv\"\n    submission_file = hf_hub_download(\n        repo_id=params.competition_id,\n        filename=submission_filename,\n        token=params.token,\n        repo_type=\"dataset\",\n    )\n    submission_df = pd.read_csv(submission_file)\n\n    public_ids = solution_df[solution_df.split == \"public\"][params.submission_id_col].values\n    private_ids = solution_df[solution_df.split == \"private\"][params.submission_id_col].values\n\n    public_solution_df = solution_df[solution_df[params.submission_id_col].isin(public_ids)]\n    public_submission_df = submission_df[submission_df[params.submission_id_col].isin(public_ids)]\n\n    private_solution_df = solution_df[solution_df[params.submission_id_col].isin(private_ids)]\n    private_submission_df = submission_df[submission_df[params.submission_id_col].isin(private_ids)]\n\n    public_solution_df = public_solution_df.sort_values(params.submission_id_col).reset_index(drop=True)\n    public_submission_df = public_submission_df.sort_values(params.submission_id_col).reset_index(drop=True)\n\n    private_solution_df = private_solution_df.sort_values(params.submission_id_col).reset_index(drop=True)\n    private_submission_df = private_submission_df.sort_values(params.submission_id_col).reset_index(drop=True)\n\n    # CALCULATE METRICS HERE.......\n    # _metric = SOME METRIC FUNCTION\n    target_cols = [col for col in solution_df.columns if col not in [params.submission_id_col, \"split\"]]\n    public_score = _metric(public_solution_df[target_cols], public_submission_df[target_cols])\n    private_score = _metric(private_solution_df[target_cols], private_submission_df[target_cols])\n\n    evaluation = {\n        \"public_score\": {\n            \"metric1\": public_score,\n        },\n        \"private_score\": {\n            \"metric1\": public_score,\n        }\n    }\n    return evaluation\n```"]