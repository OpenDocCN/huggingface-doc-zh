["```py\n( encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )\n```", "```py\n( loss: Optional = None class_queries_logits: FloatTensor = None masks_queries_logits: FloatTensor = None auxiliary_predictions: List = None encoder_hidden_states: Optional = None pixel_decoder_hidden_states: Optional = None transformer_decoder_hidden_states: Optional = None transformer_decoder_object_queries: FloatTensor = None transformer_decoder_contrastive_queries: Optional = None transformer_decoder_mask_predictions: FloatTensor = None transformer_decoder_class_predictions: FloatTensor = None transformer_decoder_auxiliary_predictions: Optional = None text_queries: Optional = None task_token: FloatTensor = None attentions: Optional = None )\n```", "```py\n( backbone_config: Optional = None ignore_value: int = 255 num_queries: int = 150 no_object_weight: int = 0.1 class_weight: float = 2.0 mask_weight: float = 5.0 dice_weight: float = 5.0 contrastive_weight: float = 0.5 contrastive_temperature: float = 0.07 train_num_points: int = 12544 oversample_ratio: float = 3.0 importance_sample_ratio: float = 0.75 init_std: float = 0.02 init_xavier_std: float = 1.0 layer_norm_eps: float = 1e-05 is_training: bool = False use_auxiliary_loss: bool = True output_auxiliary_logits: bool = True strides: Optional = [4, 8, 16, 32] task_seq_len: int = 77 text_encoder_width: int = 256 text_encoder_context_length: int = 77 text_encoder_num_layers: int = 6 text_encoder_vocab_size: int = 49408 text_encoder_proj_layers: int = 2 text_encoder_n_ctx: int = 16 conv_dim: int = 256 mask_dim: int = 256 hidden_dim: int = 256 encoder_feedforward_dim: int = 1024 norm: str = 'GN' encoder_layers: int = 6 decoder_layers: int = 10 use_task_norm: bool = True num_attention_heads: int = 8 dropout: float = 0.1 dim_feedforward: int = 2048 pre_norm: bool = False enforce_input_proj: bool = False query_dec_layers: int = 2 common_stride: int = 4 **kwargs )\n```", "```py\n>>> from transformers import OneFormerConfig, OneFormerModel\n\n>>> # Initializing a OneFormer shi-labs/oneformer_ade20k_swin_tiny configuration\n>>> configuration = OneFormerConfig()\n>>> # Initializing a model (with random weights) from the shi-labs/oneformer_ade20k_swin_tiny style configuration\n>>> model = OneFormerModel(configuration)\n>>> # Accessing the model configuration\n>>> configuration = model.config\n```", "```py\n( do_resize: bool = True size: Dict = None resample: Resampling = <Resampling.BILINEAR: 2> do_rescale: bool = True rescale_factor: float = 0.00392156862745098 do_normalize: bool = True image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: bool = False repo_path: Optional = 'shi-labs/oneformer_demo' class_info_file: str = None num_text: Optional = None **kwargs )\n```", "```py\n( images: Union task_inputs: Optional = None segmentation_maps: Union = None instance_id_to_semantic_id: Optional = None do_resize: Optional = None size: Optional = None resample: Resampling = None do_rescale: Optional = None rescale_factor: Optional = None do_normalize: Optional = None image_mean: Union = None image_std: Union = None ignore_index: Optional = None do_reduce_labels: Optional = None return_tensors: Union = None data_format: Union = <ChannelDimension.FIRST: 'channels_first'> input_data_format: Union = None **kwargs )\n```", "```py\n( pixel_values_list: List task_inputs: List segmentation_maps: Union = None instance_id_to_semantic_id: Union = None ignore_index: Optional = None reduce_labels: bool = False return_tensors: Union = None input_data_format: Union = None ) \u2192 export const metadata = 'undefined';BatchFeature\n```", "```py\n( outputs target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[torch.Tensor]\n```", "```py\n( outputs task_type: str = 'instance' is_demo: bool = True threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 target_sizes: Optional = None return_coco_annotation: Optional = False ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( outputs threshold: float = 0.5 mask_threshold: float = 0.5 overlap_mask_area_threshold: float = 0.8 label_ids_to_fuse: Optional = None target_sizes: Optional = None ) \u2192 export const metadata = 'undefined';List[Dict]\n```", "```py\n( image_processor = None tokenizer = None max_seq_length: int = 77 task_seq_length: int = 77 **kwargs )\n```", "```py\n( images = None task_inputs = None segmentation_maps = None **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( *args **kwargs )\n```", "```py\n( config: OneFormerConfig )\n```", "```py\n( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None pixel_mask: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerModelOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> import torch\n>>> from PIL import Image\n>>> import requests\n>>> from transformers import OneFormerProcessor, OneFormerModel\n\n>>> # download texting image\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # load processor for preprocessing the inputs\n>>> processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n>>> model = OneFormerModel.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n>>> inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n\n>>> mask_predictions = outputs.transformer_decoder_mask_predictions\n>>> class_predictions = outputs.transformer_decoder_class_predictions\n\n>>> f\"\ud83d\udc49 Mask Predictions Shape: {list(mask_predictions.shape)}, Class Predictions Shape: {list(class_predictions.shape)}\"\n'\ud83d\udc49 Mask Predictions Shape: [1, 150, 128, 171], Class Predictions Shape: [1, 150, 151]'\n```", "```py\n( config: OneFormerConfig )\n```", "```py\n( pixel_values: Tensor task_inputs: Tensor text_inputs: Optional = None mask_labels: Optional = None class_labels: Optional = None pixel_mask: Optional = None output_auxiliary_logits: Optional = None output_hidden_states: Optional = None output_attentions: Optional = None return_dict: Optional = None ) \u2192 export const metadata = 'undefined';transformers.models.oneformer.modeling_oneformer.OneFormerForUniversalSegmentationOutput or tuple(torch.FloatTensor)\n```", "```py\n>>> from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n>>> from PIL import Image\n>>> import requests\n>>> import torch\n\n>>> # load OneFormer fine-tuned on ADE20k for universal segmentation\n>>> processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n>>> model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n\n>>> url = (\n...     \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n... )\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> # Semantic Segmentation\n>>> inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # you can pass them to processor for semantic postprocessing\n>>> predicted_semantic_map = processor.post_process_semantic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0]\n>>> f\"\ud83d\udc49 Semantic Predictions Shape: {list(predicted_semantic_map.shape)}\"\n'\ud83d\udc49 Semantic Predictions Shape: [512, 683]'\n\n>>> # Instance Segmentation\n>>> inputs = processor(image, [\"instance\"], return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # you can pass them to processor for instance postprocessing\n>>> predicted_instance_map = processor.post_process_instance_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0][\"segmentation\"]\n>>> f\"\ud83d\udc49 Instance Predictions Shape: {list(predicted_instance_map.shape)}\"\n'\ud83d\udc49 Instance Predictions Shape: [512, 683]'\n\n>>> # Panoptic Segmentation\n>>> inputs = processor(image, [\"panoptic\"], return_tensors=\"pt\")\n\n>>> with torch.no_grad():\n...     outputs = model(**inputs)\n>>> # model predicts class_queries_logits of shape `(batch_size, num_queries)`\n>>> # and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n>>> class_queries_logits = outputs.class_queries_logits\n>>> masks_queries_logits = outputs.masks_queries_logits\n\n>>> # you can pass them to processor for panoptic postprocessing\n>>> predicted_panoptic_map = processor.post_process_panoptic_segmentation(\n...     outputs, target_sizes=[image.size[::-1]]\n... )[0][\"segmentation\"]\n>>> f\"\ud83d\udc49 Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}\"\n'\ud83d\udc49 Panoptic Predictions Shape: [512, 683]'\n```"]