- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://huggingface.co/docs/sagemaker/reference](https://huggingface.co/docs/sagemaker/reference)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/sagemaker/main/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/entry/start.599a0c37.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/scheduler.389d799c.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/singletons.68d051b6.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/paths.050ac67a.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/entry/app.66c68f8e.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/index.8f81d18f.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/nodes/0.a9101d2f.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/nodes/5.6c73bf68.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/CodeBlock.3845caa1.js">
    <link rel="modulepreload" href="/docs/sagemaker/main/en/_app/immutable/chunks/Heading.41733039.js">
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Container
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below you can find a version table of currently available Hugging Face DLCs.
    The table doesnâ€™t include the full `image_uri` here are two examples on how to
    construct those if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Manually construction the `image_uri`**'
  prefs: []
  type: TYPE_NORMAL
- en: '`{dlc-aws-account-id}.dkr.ecr.{region}.amazonaws.com/huggingface-{framework}-{(training
    | inference)}:{framework-version}-transformers{transformers-version}-{device}-{python-version}-{device-tag}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dlc-aws-account-id`: The AWS account ID of the account that owns the ECR repository.
    You can find them in the [here](https://github.com/aws/sagemaker-python-sdk/blob/e0b9d38e1e3b48647a02af23c4be54980e53dc61/src/sagemaker/image_uri_config/huggingface.json#L21)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`region`: The AWS region where you want to use it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework`: The framework you want to use, either `pytorch` or `tensorflow`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(training | inference)`: The training or inference mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`framework-version`: The version of the framework you want to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers-version`: The version of the transformers library you want to
    use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`: The device you want to use, either `cpu` or `gpu`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`python-version`: The version of the python of the DLC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device-tag`: The device tag you want to use. The device tag can include os
    version and cuda version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 1: PyTorch Training:** `763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.6.0-transformers4.4.2-gpu-py36-cu110-ubuntu18.04`
    **Example 2: Tensorflow Inference:** `763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-tensorflow-inference:2.4.1-transformers4.6.1-cpu-py37-ubuntu18.04`'
  prefs: []
  type: TYPE_NORMAL
- en: Training DLC Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Training DLC overview includes all released and available Hugging Face Training
    DLCs. It includes PyTorch and TensorFlow flavored versions for GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '| ðŸ¤— Transformers version | ðŸ¤— Datasets version | PyTorch/TensorFlow version
    | type | device | Python Version |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4.4.2 | 1.5.0 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.4.2 | 1.5.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.5.0 | 1.5.0 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.5.0 | 1.5.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | 1.6.2 | PyTorch 1.6.0 | training | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | 1.6.2 | PyTorch 1.7.1 | training | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | 1.6.2 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | 1.11.0 | PyTorch 1.8.1 | training | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | 1.11.0 | PyTorch 1.9.0 | training | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | 1.11.0 | TensorFlow 2.4.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | 1.11.0 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | 1.12.1 | PyTorch 1.9.0 | training | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | 1.12.1 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | 1.15.1 | PyTorch 1.9.1 | training | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | 1.15.1 | TensorFlow 2.5.1 | training | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | 1.18.4 | PyTorch 1.10.2 | training | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | 1.18.4 | TensorFlow 2.6.3 | training | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.26.0 | 2.9.0 | PyTorch 1.13.1 | training | GPU | 3.9 |'
  prefs: []
  type: TYPE_TB
- en: Inference DLC Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Inference DLC overview includes all released and available Hugging Face
    Inference DLCs. It includes PyTorch and TensorFlow flavored versions for CPU,
    GPU & AWS Inferentia.
  prefs: []
  type: TYPE_NORMAL
- en: '| ðŸ¤— Transformers version | PyTorch/TensorFlow version | type | device | Python
    Version |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | PyTorch 1.7.1 | inference | CPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | PyTorch 1.7.1 | inference | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | TensorFlow 2.4.1 | inference | CPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.6.1 | TensorFlow 2.4.1 | inference | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | PyTorch 1.8.1 | inference | GPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | PyTorch 1.9.0 | inference | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | TensorFlow 2.4.1 | inference | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | PyTorch 1.8.1 | inference | CPU | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | PyTorch 1.9.0 | inference | CPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | TensorFlow 2.4.1 | inference | CPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.10.2 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | PyTorch 1.9.0 | inference | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | PyTorch 1.9.0 | inference | CPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.11.0 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | TensorFlow 2.5.1 | inference | GPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | CPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | TensorFlow 2.5.1 | inference | CPU | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.12.3 | PyTorch 1.9.1 | inference | Inferentia | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | PyTorch 1.10.2 | inference | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | TensorFlow 2.6.3 | inference | GPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | PyTorch 1.10.2 | inference | CPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.17.0 | TensorFlow 2.6.3 | inference | CPU | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.26.0 | PyTorch 1.13.1 | inference | CPU | 3.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 4.26.0 | PyTorch 1.13.1 | inference | GPU | 3.9 |'
  prefs: []
  type: TYPE_TB
- en: Hugging Face Transformers Amazon SageMaker Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Example Jupyter notebooks that demonstrate how to build, train, and deploy [Hugging
    Face Transformers](https://github.com/huggingface/transformers) using [Amazon
    SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html) and the
    [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: '| Notebook | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [01 Getting started with PyTorch](https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb)
    | Training | Getting started end-to-end example on how to fine-tune a pre-trained
    Hugging Face Transformer for Text-Classification using PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| [02 getting started with TensorFlow](https://github.com/huggingface/notebooks/blob/main/sagemaker/02_getting_started_tensorflow/sagemaker-notebook.ipynb)
    | Training | Getting started end-to-end example on how to fine-tune a pre-trained
    Hugging Face Transformer for Text-Classification using TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| [03 Distributed Training: Data Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/03_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use distributed training with data-parallelism
    strategy for fine-tuning a pre-trained Hugging Face Transformer for Question-Answering
    using Amazon SageMaker Data Parallelism |'
  prefs: []
  type: TYPE_TB
- en: '| [04 Distributed Training: Model Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use distributed training with model-parallelism
    strategy to pre-trained Hugging Face Transformer using Amazon SageMaker Model
    Parallelism |'
  prefs: []
  type: TYPE_TB
- en: '| [05 How to use Spot Instances & Checkpointing](https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use Spot Instances and Checkpointing
    to reduce training cost |'
  prefs: []
  type: TYPE_TB
- en: '| [06 Experiment Tracking with SageMaker Metrics](https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use SageMaker metrics to track your
    experiments and training jobs |'
  prefs: []
  type: TYPE_TB
- en: '| [07 Distributed Training: Data Parallelism](https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to use Amazon SageMaker Data Parallelism
    with TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '| [08 Distributed Training: Summarization with T5/BART](https://github.com/huggingface/notebooks/blob/main/sagemaker/08_distributed_summarization_bart_t5/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to fine-tune BART/T5 for Summarization
    using Amazon SageMaker Data Parallelism |'
  prefs: []
  type: TYPE_TB
- en: '| [09 Vision: Fine-tune ViT](https://github.com/huggingface/notebooks/blob/main/sagemaker/09_image_classification_vision_transformer/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to fine-tune Vision Transformer for Image-Classification
    |'
  prefs: []
  type: TYPE_TB
- en: '| [10 Deploy HF Transformer from Amazon S3](https://github.com/huggingface/notebooks/blob/main/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb)
    | Inference | End-to-end example on how to deploy a model from Amazon S3 |'
  prefs: []
  type: TYPE_TB
- en: '| [11 Deploy HF Transformer from Hugging Face Hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)
    | Inference | End-to-end example on how to deploy a model from the Hugging Face
    Hub |'
  prefs: []
  type: TYPE_TB
- en: '| [12 Batch Processing with Amazon SageMaker Batch Transform](https://github.com/huggingface/notebooks/blob/main/sagemaker/12_batch_transform_inference/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do batch processing with Amazon SageMaker
    Batch Transform |'
  prefs: []
  type: TYPE_TB
- en: '| [13 Autoscaling SageMaker Endpoints](https://github.com/huggingface/notebooks/blob/main/sagemaker/13_deploy_and_autoscaling_transformers/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do use autoscaling for a HF Endpoint
    |'
  prefs: []
  type: TYPE_TB
- en: '| [14 Fine-tune and push to Hub](https://github.com/huggingface/notebooks/blob/main/sagemaker/14_train_and_push_to_hub/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to do use the Hugging Face Hub as MLOps
    backend for saving checkpoints during training |'
  prefs: []
  type: TYPE_TB
- en: '| [15 Training Compiler](https://github.com/huggingface/notebooks/blob/main/sagemaker/15_training_compiler/sagemaker-notebook.ipynb)
    | Training | End-to-end example on how to do use Amazon SageMaker Training Compiler
    to speed up training time |'
  prefs: []
  type: TYPE_TB
- en: '| [16 Asynchronous Inference](https://github.com/huggingface/notebooks/blob/main/sagemaker/16_async_inference_hf_hub/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to do use Amazon SageMaker Asynchronous
    Inference endpoints with Hugging Face Transformers |'
  prefs: []
  type: TYPE_TB
- en: '| [17 Custom inference.py script](https://github.com/huggingface/notebooks/blob/main/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to create a custom inference.py for Sentence
    Transformers and sentence embeddings |'
  prefs: []
  type: TYPE_TB
- en: '| [18 AWS Inferentia](https://github.com/huggingface/notebooks/blob/main/sagemaker/18_inferentia_inference/sagemaker-notebook.ipynb)
    | Inference | End-to-end example on how to AWS Inferentia to speed up inference
    time |'
  prefs: []
  type: TYPE_TB
- en: Inference Toolkit API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Inference Toolkit accepts inputs in the `inputs` key, and supports additional
    [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines)
    parameters in the `parameters` key. You can provide any of the supported `kwargs`
    from `pipelines` as `parameters`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tasks supported by the Inference Toolkit API include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`text-classification`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`sentiment-analysis`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`token-classification`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`feature-extraction`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`fill-mask`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`summarization`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`translation_xx_to_yy`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`text2text-generation`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`text-generation`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`audio-classificatin`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`automatic-speech-recognition`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`conversational`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`image-classification`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`image-segmentation`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`object-detection`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`table-question-answering`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`zero-shot-classification`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`zero-shot-image-classification`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the following request examples for some of the tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`text-classification`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**`sentiment-analysis`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**`token-classification`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**`question-answering`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**`zero-shot-classification`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**`table-question-answering`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**`parameterized-request`**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Inference Toolkit environment variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Inference Toolkit implements various additional environment variables to
    simplify deployment. A complete list of Hugging Face specific environment variables
    is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`HF_TASK`**'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_TASK` defines the task for the ðŸ¤— Transformers pipeline used . See [here](https://huggingface.co/docs/transformers/main_classes/pipelines)
    for a complete list of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**`HF_MODEL_ID`**'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_MODEL_ID` defines the model ID which is automatically loaded from [hf.co/models](https://huggingface.co/models)
    when creating a SageMaker endpoint. All of the ðŸ¤— Hubâ€™s 10,000+ models are available
    through this environment variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**`HF_MODEL_REVISION`**'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_MODEL_REVISION` is an extension to `HF_MODEL_ID` and allows you to define
    or pin a model revision to make sure you always load the same model on your SageMaker
    endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**`HF_API_TOKEN`**'
  prefs: []
  type: TYPE_NORMAL
- en: '`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_TOKEN`
    is used as a HTTP bearer authorization for remote files like private models. You
    can find your token under [Settings](https://huggingface.co/settings/tokens) of
    your Hugging Face account.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
