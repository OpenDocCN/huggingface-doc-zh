# 图像变化

> 原文链接：[`huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation`](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation)

Stable Diffusion 模型还可以从输入图像生成变化。它使用了 [Justin Pinkney](https://www.justinpinkney.com/) 在 [Lambda](https://lambdalabs.com/) 上的 Stable Diffusion 模型的经过微调的版本。

原始代码库可在 [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) 找到，图像变化的额外官方检查点可在 [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers) 找到。

确保查看 Stable Diffusion Tips 部分，了解如何探索调度器速度和质量之间的权衡，以及如何高效地重用管道组件！

## StableDiffusionImageVariationPipeline

### `class diffusers.StableDiffusionImageVariationPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py#L37)

```py
( vae: AutoencoderKL image_encoder: CLIPVisionModelWithProjection unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )
```

参数

+   `vae` (AutoencoderKL) — 变分自动编码器（VAE）模型，用于将图像编码和解码为潜在表示。

+   `image_encoder` ([CLIPVisionModelWithProjection](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPVisionModelWithProjection)) — 冻结的 CLIP 图像编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` (UNet2DConditionModel) — 用于去噪编码图像潜在表示的 `UNet2DConditionModel`。

+   `scheduler` (SchedulerMixin) — 用于与 `unet` 结合使用以去噪编码图像潜在表示的调度器。可以是 DDIMScheduler、LMSDiscreteScheduler 或 PNDMScheduler 中的一个。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 用于估计生成的图像是否可能被视为具有冒犯性或有害的分类模块。请参考 [模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5) 以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入使用。

使用 Stable Diffusion 生成输入图像的图像变化管道。

此模型继承自 DiffusionPipeline。查看超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py#L271)

```py
( image: Union height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 num_images_per_prompt: Optional = 1 eta: float = 0.0 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `image` (`PIL.Image.Image` 或 `List[PIL.Image.Image]` 或 `torch.FloatTensor`) — 用于指导图像生成的图像。如果提供张量，则需要与[`CLIPImageProcessor`](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json)兼容。

+   `height` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素高度。

+   `width` (`int`, *可选*, 默认为`self.unet.config.sample_size * self.vae_scale_factor`) — 生成图像的像素宽度。

+   `num_inference_steps` (`int`, *可选*, 默认为 50) — 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。该参数由`strength`调节。

+   `guidance_scale` (`float`, *可选*, 默认为 7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `num_images_per_prompt` (`int`, *可选*, 默认为 1) — 每个提示生成的图像数量。

+   `eta` (`float`, *可选*, 默认为 0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数 eta (η)。仅适用于 DDIMScheduler，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *可选*) — 用于使生成具有确定性的[`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `latents` (`torch.FloatTensor`, *可选*) — 从高斯分布中采样的预生成噪声潜变量，用作图像生成的输入。可用于使用不同提示微调相同生成。如果未提供，则通过使用提供的随机`generator`进行采样生成潜变量张量。

+   `output_type` (`str`, *可选*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间。

+   `return_dict` (`bool`, *可选*, 默认为`True`) — 是否返回 StableDiffusionPipelineOutput 而不是普通的 tuple。

+   `callback` (`Callable`, *可选*) — 推理过程中每`callback_steps`步调用的函数。该函数使用以下参数调用：`callback(step: int, timestep: int, latents: torch.FloatTensor)`。

+   `callback_steps` (`int`, *可选*, 默认为 1) — 调用`callback`函数的频率。如果未指定，则在每一步调用回调。

返回

StableDiffusionPipelineOutput 或 `tuple`

如果`return_dict`为`True`，则返回 StableDiffusionPipelineOutput，否则返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”(nsfw)内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
from diffusers import StableDiffusionImageVariationPipeline
from PIL import Image
from io import BytesIO
import requests

pipe = StableDiffusionImageVariationPipeline.from_pretrained(
    "lambdalabs/sd-image-variations-diffusers", revision="v2.0"
)
pipe = pipe.to("cuda")

url = "https://lh3.googleusercontent.com/y-iFOHfLTwkuQSUegpwDdgKmOjRSTvPxat63dQLB25xkTs4lhIbRUFeNBWZzYf370g=s1200"

response = requests.get(url)
image = Image.open(BytesIO(response.content)).convert("RGB")

out = pipe(image, num_images_per_prompt=3, guidance_scale=15)
out["images"][0].save("result.jpg")
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size` (`str`或`int`，*可选*，默认为`"auto"`) — 当为`"auto"`时，将输入减半到注意力头，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。当启用此选项时，注意力模块将输入张量分割成多个步骤计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于节省一些内存以换取一点速度降低很有用。

⚠️ 如果您已经在使用 PyTorch 2.0 或 xFormers 中的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在 SDPA 或 xFormers 中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将在一个步骤中计算。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op` (`Callable`，*可选*) — 用作 xFormers 的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数的默认`None`操作符。

启用来自[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。当启用此选项时，您应该观察到较低的 GPU 内存使用量，并且在推断期间可能会加速。训练期间的加速不被保证。

⚠️ 当启用内存高效注意力和切片注意力时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用来自[xFormers](https://facebookresearch.github.io/xformers/)的内存高效注意力。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py#L267)

```py
( )
```

如果已启用，则禁用 FreeU 机制。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py#L244)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1` (`float`) — 阶段 1 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2` (`float`) — 阶段 2 的缩放因子，用于减弱跳过特征的贡献。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1` (`float`) — 阶段 1 的缩放因子，用于放大主干特征的贡献。

+   `b2` (`float`) — 阶段 2 的缩放因子，用于放大主干特征的贡献。

启用 FreeU 机制，如[`arxiv.org/abs/2309.11497`](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如 Stable Diffusion v1、v2 和 Stable Diffusion XL）的值组合。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不安全内容”（nsfw），如果无法执行安全检查，则为 `None`。

稳定扩散管道的输出类。
