["```py\ndevice = \"cuda\"\nmodel.to(device)\n\ngradient_accumulation_steps = 2\n\nfor index, batch in enumerate(training_dataloader):\n    inputs, targets = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss = loss / gradient_accumulation_steps\n    loss.backward()\n    if (index + 1) % gradient_accumulation_steps == 0:\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n```", "```py\n+ from accelerate import Accelerator\n+ accelerator = Accelerator()\n\n+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n+     model, optimizer, training_dataloader, scheduler\n+ )\n\n  for index, batch in enumerate(training_dataloader):\n      inputs, targets = batch\n-     inputs = inputs.to(device)\n-     targets = targets.to(device)\n      outputs = model(inputs)\n      loss = loss_function(outputs, targets)\n      loss = loss / gradient_accumulation_steps\n+     accelerator.backward(loss)\n      if (index+1) % gradient_accumulation_steps == 0:\n          optimizer.step()\n          scheduler.step()\n```", "```py\n+local_sgd_steps=8\n\n+with LocalSGD(accelerator=accelerator, model=model, local_sgd_steps=8, enabled=True) as local_sgd:\n    for batch in training_dataloader:\n        with accelerator.accumulate(model):\n            inputs, targets = batch\n            outputs = model(inputs)\n            loss = loss_function(outputs, targets)\n            accelerator.backward(loss)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n+           local_sgd.step()\n```"]