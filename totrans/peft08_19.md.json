["```py\npython -m pip install -U <package_name>\n```", "```py\npython -m pip install git+https://github.com/huggingface/peft\n```", "```py\npeft_model = get_peft_model(...)\n\n# add this:\nfor param in model.parameters():\n    if param.requires_grad:\n        param.data = param.data.float()\n\n# proceed as usual\ntrainer = Trainer(model=peft_model, fp16=True, ...)\ntrainer.train()\n```", "```py\nfrom peft import cast_mixed_precision_params\n\npeft_model = get_peft_model(...)\ncast_mixed_precision_params(peft_model, dtype=torch.float16)\n\n# proceed as usual\ntrainer = Trainer(model=peft_model, fp16=True, ...)\ntrainer.train()\n```", "```py\nfrom peft import PeftModel, PeftConfig\n\nbase_model = ...  # to load the base model, use the same code as when you trained it\nconfig = PeftConfig.from_pretrained(peft_model_id)\npeft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n```", "```py\nSome weights of <MODEL> were not initialized from the model checkpoint at <ID> and are newly initialized: [<LAYER_NAMES>].\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n```", "```py\nconfig = LoraConfig(..., target_modules=[\"embed_tokens\", \"lm_head\", \"q_proj\", \"v_proj\"])\n```", "```py\nmodel = get_peft_model(...)\n# train the model\nmodel.save_adapter(\"my_adapter\", save_embedding_layers=True)\n```"]