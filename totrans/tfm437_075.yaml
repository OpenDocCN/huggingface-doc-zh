- en: PyTorch training on Apple silicon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special](https://huggingface.co/docs/transformers/v4.37.2/en/perf_train_special)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en:  
    
    
    
    
    
    
    
    
    
    
    
    
  prefs: []
  type: TYPE_NORMAL
- en: Previously, training models on a Mac was limited to the CPU only. With the release
    of PyTorch v1.12, you can take advantage of training models with Apple’s silicon
    GPUs for significantly faster performance and training. This is powered in PyTorch
    by integrating Apple’s Metal Performance Shaders (MPS) as a backend. The [MPS
    backend](https://pytorch.org/docs/stable/notes/mps.html) implements PyTorch operations
    as custom Metal shaders and places these modules on a `mps` device.
  prefs: []
  type: TYPE_NORMAL
- en: Some PyTorch operations are not implemented in MPS yet and will throw an error.
    To avoid this, you should set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1`
    to use the CPU kernels instead (you’ll still see a `UserWarning`).
  prefs: []
  type: TYPE_NORMAL
- en: If you run into any other errors, please open an issue in the [PyTorch](https://github.com/pytorch/pytorch/issues)
    repository because the [Trainer](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    only integrates the MPS backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `mps` device set, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: train larger networks or batch sizes locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reduce data retrieval latency because the GPU’s unified memory architecture
    allows direct access to the full memory store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reduce costs because you don’t need to train on cloud-based GPUs or add additional
    local GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get started by making sure you have PyTorch installed. MPS acceleration is supported
    on macOS 12.3+.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[TrainingArguments](/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.TrainingArguments)
    uses the `mps` device by default if it’s available which means you don’t need
    to explicitly set the device. For example, you can run the [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)
    script with the MPS backend automatically enabled without making any changes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Backends for [distributed setups](https://pytorch.org/docs/stable/distributed.html#backends)
    like `gloo` and `nccl` are not supported by the `mps` device which means you can
    only train on a single GPU with the MPS backend.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about the MPS backend in the [Introducing Accelerated PyTorch
    Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)
    blog post.
  prefs: []
  type: TYPE_NORMAL
