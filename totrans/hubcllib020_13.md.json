["```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n\n>>> image = client.text_to_image(\"An astronaut riding a horse on the moon.\")\n>>> image.save(\"astronaut.png\")\n```", "```py\n>>> from huggingface_hub import InferenceClient\n# Initialize client for a specific model\n>>> client = InferenceClient(model=\"prompthero/openjourney-v4\")\n>>> client.text_to_image(...)\n# Or use a generic client but pass your model as an argument\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"prompthero/openjourney-v4\")\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n# or\n>>> client = InferenceClient()\n>>> client.text_to_image(..., model=\"https://uu149rez6gw9ehej.eu-west-1.aws.endpoints.huggingface.cloud/deepfloyd-if\")\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient(token=\"hf_***\")\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> response = client.post(json={\"inputs\": \"An astronaut riding a horse on the moon.\"}, model=\"stabilityai/stable-diffusion-2-1\")\n>>> response.content # raw bytes\nb'...'\n```", "```py\npip install --upgrade huggingface_hub[inference]\n# or\n# pip install aiohttp\n```", "```py\n# Code must be run in a asyncio concurrent context.\n# $ python -m asyncio\n>>> from huggingface_hub import AsyncInferenceClient\n>>> client = AsyncInferenceClient()\n\n>>> image = await client.text_to_image(\"An astronaut riding a horse on the moon.\")\n>>> image.save(\"astronaut.png\")\n\n>>> async for token in await client.text_generation(\"The Huggingface Hub is\", stream=True):\n...     print(token, end=\"\")\n a platform for sharing and discussing ML-related content.\n```", "```py\n>>> from huggingface_hub import InferenceClient, InferenceTimeoutError\n>>> client = InferenceClient(timeout=30)\n>>> try:\n...     client.text_to_image(...)\n... except InferenceTimeoutError:\n...     print(\"Inference timed out after 30s.\")\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> client.image_classification(\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/320px-Cute_dog.jpg\")\n[{'score': 0.9779096841812134, 'label': 'Blenheim spaniel'}, ...]\n```", "```py\n>>> from huggingface_hub import InferenceApi\n>>> inference = InferenceApi(repo_id=\"bert-base-uncased\", token=API_TOKEN)\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> inference = InferenceClient(model=\"bert-base-uncased\", token=API_TOKEN)\n```", "```py\n>>> from huggingface_hub import InferenceApi\n>>> inference = InferenceApi(repo_id=\"paraphrase-xlm-r-multilingual-v1\", task=\"feature-extraction\")\n>>> inference(...)\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> inference = InferenceClient()\n>>> inference.feature_extraction(..., model=\"paraphrase-xlm-r-multilingual-v1\")\n```", "```py\n>>> from huggingface_hub import InferenceApi\n>>> inference = InferenceApi(repo_id=\"bert-base-uncased\")\n>>> inference(inputs=\"The goal of life is [MASK].\")\n[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> response = client.post(json={\"inputs\": \"The goal of life is [MASK].\"}, model=\"bert-base-uncased\")\n>>> response.json()\n[{'sequence': 'the goal of life is life.', 'score': 0.10933292657136917, 'token': 2166, 'token_str': 'life'}]\n```", "```py\n>>> from huggingface_hub import InferenceApi\n>>> inference = InferenceApi(repo_id=\"typeform/distilbert-base-uncased-mnli\")\n>>> inputs = \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\"\n>>> params = {\"candidate_labels\":[\"refund\", \"legal\", \"faq\"]}\n>>> inference(inputs, params)\n{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}\n```", "```py\n>>> from huggingface_hub import InferenceClient\n>>> client = InferenceClient()\n>>> inputs = \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\"\n>>> params = {\"candidate_labels\":[\"refund\", \"legal\", \"faq\"]}\n>>> response = client.post(json={\"inputs\": inputs, \"parameters\": params}, model=\"typeform/distilbert-base-uncased-mnli\")\n>>> response.json()\n{'sequence': 'Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!', 'labels': ['refund', 'faq', 'legal'], 'scores': [0.9378499388694763, 0.04914155602455139, 0.013008488342165947]}\n```"]