# 图像到图像

> 原文链接：[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img)

稳定扩散模型也可以应用于图像到图像的生成，通过传递文本提示和初始图像来调节新图像的生成。

[StableDiffusionImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) 使用了 [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://huggingface.co/papers/2108.01073) 中提出的扩散去噪机制，作者为 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon。

该论文的摘要是：

*引导图像合成使普通用户能够以最小的努力创建和编辑逼真的图像。关键挑战在于平衡对用户输入（例如手绘的彩色笔画）的忠实性和合成图像的逼真性。现有基于GAN的方法尝试使用条件GAN或GAN反演来实现这种平衡，这是具有挑战性的，通常需要额外的训练数据或损失函数来适用于各个应用。为了解决这些问题，我们引入了一种新的图像合成和编辑方法，基于扩散模型生成先验的随机微分编辑（SDEdit），通过随机微分方程（SDE）迭代去噪来合成逼真的图像。给定任何类型的用户指导的输入图像，SDEdit首先向输入图像添加噪声，然后通过SDE先验逐步去噪结果图像以增加其逼真性。SDEdit不需要任务特定的训练或反演，可以自然地实现逼真性和忠实性之间的平衡。根据人类感知研究，在多个任务上，包括基于笔画的图像合成和编辑以及图像合成，SDEdit在逼真性和整体满意度得分上显著优于最先进的基于GAN的方法，最高可达98.09%和91.72%。*

请务必查看稳定扩散 [提示](overview#tips) 部分，了解如何探索调度器速度和质量之间的权衡，以及如何高效地重用管道组件！

## StableDiffusionImg2ImgPipeline

### `class diffusers.StableDiffusionImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L158)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

参数

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码到潜在表示。

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) — 用于去噪编码图像潜变量的 `UNet2DConditionModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与 `unet` 结合使用的调度器，用于去噪编码图像潜变量。可以是 [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)、[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) 或 [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler) 中的一个。

+   `safety_checker` (`StableDiffusionSafetyChecker`) — 估计生成的图像是否可能被视为具有冒犯性或有害性的分类模块。有关模型潜在危害的更多详细信息，请参阅[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的`CLIPImageProcessor`；作为`safety_checker`的输入。

使用稳定扩散进行文本引导的图像到图像生成的流程。

该模型继承自[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。检查超类文档以获取为所有流程实现的通用方法（下载、保存、在特定设备上运行等）。

该流程还继承了以下加载方法：

+   加载文本反转嵌入的[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) 用于加载LoRA权重

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) 用于保存LoRA权重

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) 用于加载`.ckpt`文件

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) 用于加载IP适配器

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L868)

```py
( prompt: Union = None image: Union = None strength: float = 0.8 num_inference_steps: Optional = 50 timesteps: List = None guidance_scale: Optional = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: Optional = 0.0 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: int = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) → export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

参数

+   `prompt` (`str` 或 `List[str]`, *可选*) — 用于指导图像生成的提示或提示。如果未定义，则需要传递`prompt_embeds`。

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, 或 `List[np.ndarray]`) — 用作起点的图像批次的`Image`、numpy数组或张量表示。对于numpy数组和pytorch张量，期望的值范围在`[0, 1]`之间。如果是张量或张量列表，则期望的形状应为`(B, C, H, W)`或`(C, H, W)`。如果是numpy数组或数组列表，则期望的形状应为`(B, H, W, C)`或`(H, W, C)`。它还可以接受图像潜变量作为`image`，但如果直接传递潜变量，则不会再次编码。

+   `strength` (`float`, *可选*，默认为0.8) — 表示转换参考`image`的程度。必须在0和1之间。`image`被用作起点，`strength`越高，添加的噪音就越多。去噪步骤的数量取决于最初添加的噪音量。当`strength`为1时，添加的噪音是最大的，去噪过程将运行指定的`num_inference_steps`的完整迭代次数。值为1基本上忽略了`image`。

+   `num_inference_steps` (`int`, *可选*，默认为50) — 去噪步骤的数量。更多的去噪步骤通常会导致更高质量的图像，但会降低推理速度。该参数受`strength`调节。

+   `timesteps` (`List[int]`, *可选*) — 用于具有支持`timesteps`参数的调度器的去噪过程的自定义时间步长，在其`set_timesteps`方法中。如果未定义，则将使用传递`num_inference_steps`时的默认行为。必须按降序排列。

+   `guidance_scale` (`float`, *可选*，默认为7.5) — 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `negative_prompt` (`str` 或 `List[str]`, *optional*) — 指导图像生成中不包括的提示或提示。如果未定义，则需要传递`negative_prompt_embeds`。在不使用指导时（`guidance_scale < 1`）将被忽略。

+   `num_images_per_prompt` (`int`, *optional*, 默认为1) — 每个提示生成的图像数量。

+   `eta` (`float`, *optional*, 默认为0.0) — 对应于[DDIM](https://arxiv.org/abs/2010.02502)论文中的参数eta (η)。仅适用于[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)，在其他调度程序中被忽略。

+   `generator` (`torch.Generator` 或 `List[torch.Generator]`, *optional*) — 用于使生成过程确定性的 [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)。

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) — 预生成的负文本嵌入。可用于轻松调整文本输入（提示加权）。如果未提供，则从`negative_prompt`输入参数生成`negative_prompt_embeds`。ip_adapter_image — (`PipelineImageInput`, *optional*): 与IP适配器一起使用的可选图像输入。

+   `output_type` (`str`, *optional*, 默认为`"pil"`) — 生成图像的输出格式。选择`PIL.Image`或`np.array`之间的一个。

+   `return_dict` (`bool`, *optional*, 默认为`True`) — 是否返回一个[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)而不是一个普通的元组。

+   `cross_attention_kwargs` (`dict`, *optional*) — 一个kwargs字典，如果指定，则传递给[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)中定义的`AttentionProcessor`。

+   `clip_skip` (`int`, *optional*) — 在计算提示嵌入时要从CLIP跳过的层数。值为1意味着将使用预最终层的输出来计算提示嵌入。

+   `callback_on_step_end` (`Callable`, *optional*) — 在推断过程中每个去噪步骤结束时调用的函数。该函数将使用以下参数调用：`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`。`callback_kwargs`将包含由`callback_on_step_end_tensor_inputs`指定的所有张量的列表。

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) — `callback_on_step_end`函数的张量输入列表。列表中指定的张量将作为`callback_kwargs`参数传递。您只能包含在管道类的`._callback_tensor_inputs`属性中列出的变量。

返回

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) 或 `tuple`

如果`return_dict`为`True`，将返回[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)，否则将返回一个`tuple`，其中第一个元素是生成的图像列表，第二个元素是一个`bool`列表，指示相应生成的图像是否包含“不安全内容”（nsfw）。

用于生成的管道的调用函数。

示例：

```py
>>> import requests
>>> import torch
>>> from PIL import Image
>>> from io import BytesIO

>>> from diffusers import StableDiffusionImg2ImgPipeline

>>> device = "cuda"
>>> model_id_or_path = "runwayml/stable-diffusion-v1-5"
>>> pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
>>> pipe = pipe.to(device)

>>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

>>> response = requests.get(url)
>>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
>>> init_image = init_image.resize((768, 512))

>>> prompt = "A fantasy landscape, trending on artstation"

>>> images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images
>>> images[0].save("fantasy_landscape.png")
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

参数

+   `slice_size`（`str`或`int`，*可选*，默认为`"auto"`）—当为`"auto"`时，将输入减半给注意力头，因此注意力将在两个步骤中计算。如果为`"max"`，将通过一次只运行一个切片来节省最大内存量。如果提供了一个数字，则使用`attention_head_dim // slice_size`个切片。在这种情况下，`attention_head_dim`必须是`slice_size`的倍数。

启用切片注意力计算。当启用此选项时，注意力模块将输入张量分割成片段，以便在多个步骤中计算注意力。对于多个注意力头，计算将按顺序在每个头上执行。这对于在节省一些内存的同时换取一点速度降低是有用的。

⚠️ 如果您已经在使用PyTorch 2.0或xFormers中的`scaled_dot_product_attention`（SDPA），请不要启用注意力切片。这些注意力计算已经非常高效，因此您不需要启用此功能。如果您在SDPA或xFormers中启用了注意力切片，可能会导致严重减速！

示例：

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

禁用切片注意力计算。如果之前调用了`enable_attention_slicing`，则注意力将在一步中计算。

#### `enable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

参数

+   `attention_op`（`Callable`，*可选*）—覆盖默认的`None`运算符，用作xFormers的[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)函数的`op`参数。

从[xFormers](https://facebookresearch.github.io/xformers/)启用内存高效注意力。当启用此选项时，您应该观察到更低的GPU内存使用量，并且在推理过程中可能会加速。训练过程中的加速不被保证。

⚠️ 当内存高效注意力和切片注意力都启用时，内存高效注意力优先。

示例：

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

禁用[xFormers](https://facebookresearch.github.io/xformers/)中的内存高效注意力。

#### `load_textual_inversion`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

参数

+   `pretrained_model_name_or_path`（`str`或`os.PathLike`或`List[str或os.PathLike]`或`Dict`或`List[Dict]`）—可以是以下之一或其中的一个列表：

    +   一个字符串，预训练模型的*模型ID*（例如`sd-concepts-library/low-poly-hd-logos-icons`），托管在Hub上。

    +   包含文本反转权重的*目录*路径（例如`./my_text_inversion_directory/`）。

    +   包含文本反转权重的*文件*路径（例如`./my_text_inversions.pt`）。

    +   一个[torch状态字典](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)。

+   `token`（`str`或`List[str]`，*可选*）—覆盖用于文本反转权重的令牌。如果`pretrained_model_name_or_path`是一个列表，则`token`也必须是相同长度的列表。

+   `text_encoder`（[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)，*可选*）—冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。如果未指定，函数将使用self.tokenizer。

+   `tokenizer`（[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)，*可选*）—用于对文本进行标记化的`CLIPTokenizer`。如果未指定，函数将使用self.tokenizer。

+   `weight_name` (`str`, *optional*) — 自定义权重文件的名称。在以下情况下应使用此选项：

    +   保存的文本反转文件采用🤗 Diffusers格式，但保存在特定权重名称下，例如`text_inv.bin`。

    +   保存的文本反转文件采用Automatic1111格式。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 如果不使用标准缓存，则缓存下载的预训练模型配置的目录路径。

+   `force_download` (`bool`, *optional*, 默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `resume_download` (`bool`, *optional*, 默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。这些代理在每个请求中使用。

+   `local_files_only` (`bool`, *optional*, 默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则不会从Hub下载模型。

+   `token` (`str`或*bool*, *optional*) — 用作远程文件HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *optional*, 默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `subfolder` (`str`, *optional*, 默认为`""`) — 模型文件在Hub或本地较大模型存储库中的子文件夹位置。

+   `mirror` (`str`, *optional*) — 如果您在中国下载模型时遇到可访问性问题，可以使用镜像源解决。我们不保证源的及时性或安全性，您应参考镜像站点获取更多信息。

将文本反转嵌入加载到[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)的文本编码器中（支持🤗 Diffusers和Automatic1111格式）。

示例：

要加载🤗 Diffusers格式的文本反转嵌入向量：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

要加载Automatic1111格式的文本反转嵌入向量，请确保首先下载向量（例如从[civitAI](https://civitai.com/models/3036?modelVersionId=9857)），然后加载向量

本地：

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `from_single_file`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/single_file.py#L141)

```py
( pretrained_model_link_or_path **kwargs )
```

参数

+   `pretrained_model_link_or_path` (`str`或`os.PathLike`, *optional*) — 可以是：

    +   Hub上`.ckpt`文件的链接（例如`"https://huggingface.co/<repo_id>/blob/main/<path_to_file>.ckpt"`）。

    +   包含所有管道权重的*文件*的路径。

+   `torch_dtype` (`str`或`torch.dtype`, *optional*) — 覆盖默认的`torch.dtype`并使用另一种dtype加载模型。

+   `force_download` (`bool`, *optional*, 默认为`False`) — 是否强制（重新）下载模型权重和配置文件，覆盖缓存版本（如果存在）。

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) — 如果不使用标准缓存，则缓存下载的预训练模型配置的目录路径。

+   `resume_download` (`bool`, *optional*, 默认为`False`) — 是否恢复下载模型权重和配置文件。如果设置为`False`，则删除任何未完全下载的文件。

+   `proxies` (`Dict[str, str]`, *optional*) — 一个代理服务器字典，按协议或端点使用，例如，`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`。这些代理在每个请求中使用。

+   `local_files_only` (`bool`, *可选*, 默认为`False`) — 是否仅加载本地模型权重和配置文件。如果设置为`True`，则模型不会从Hub下载。

+   `token` (`str` 或 *bool*, *可选*) — 用作远程文件的HTTP bearer授权的令牌。如果为`True`，则使用从`diffusers-cli login`生成的令牌（存储在`~/.huggingface`）。

+   `revision` (`str`, *可选*, 默认为`"main"`) — 要使用的特定模型版本。可以是分支名称、标签名称、提交ID或Git允许的任何标识符。

+   `use_safetensors` (`bool`, *可选*, 默认为`None`) — 如果设置为`None`，则会下载safetensors权重（如果可用）**并且**如果已安装safetensors库。如果设置为`True`，则将强制从safetensors权重加载模型。如果设置为`False`，则不会加载safetensors权重。

从以`.ckpt`或`.safetensors`格式保存的预训练管道权重实例化[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)。默认情况下，管道设置为评估模式（`model.eval()`）。

示例：

```py
>>> from diffusers import StableDiffusionPipeline

>>> # Download pipeline from huggingface.co and cache.
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors"
... )

>>> # Download pipeline from local file
>>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt
>>> pipeline = StableDiffusionPipeline.from_single_file("./v1-5-pruned-emaonly")

>>> # Enable float16 and move to GPU
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt",
...     torch_dtype=torch.float16,
... )
>>> pipeline.to("cuda")
```

#### `load_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

参数

+   `pretrained_model_name_or_path_or_dict` (`str` 或 `os.PathLike` 或 `dict`) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `kwargs` (`dict`, *可选*) — 查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)。

+   `adapter_name` (`str`, *可选*) — 用于引用加载的适配器模型的适配器名称。如果未指定，将使用`default_{i}`，其中i是正在加载的适配器总数。

将`pretrained_model_name_or_path_or_dict`中指定的LoRA权重加载到`self.unet`和`self.text_encoder`中。

所有kwargs都将转发到`self.lora_state_dict`。

查看[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)以获取有关如何加载状态字典的更多详细信息。

查看[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)以获取有关如何将状态字典加载到`self.unet`中的更多详细信息。

查看[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)以获取有关如何将状态字典加载到`self.text_encoder`中的更多详细信息。

#### `save_lora_weights`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

参数

+   `save_directory` (`str` 或 `os.PathLike`) — 保存LoRA参数的目录。如果不存在，将会创建。

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]` 或 `Dict[str, torch.Tensor]`) — 与`unet`对应的LoRA层的状态字典。

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` 或 `Dict[str, torch.Tensor]`) — 与`text_encoder`对应的LoRA层的状态字典。必须显式传递文本编码器LoRA状态字典，因为它来自🤗 Transformers。

+   `is_main_process` (`bool`, *可选*, 默认为`True`) — 调用此函数的进程是否为主进程。在分布式训练期间，当您需要在所有进程上调用此函数时，这很有用。在这种情况下，仅在主进程上设置`is_main_process=True`以避免竞争条件。

+   `save_function` (`Callable`) — 用于保存状态字典的函数。在分布式训练期间，当您需要用另一种方法替换`torch.save`时，这很有用。可以使用环境变量`DIFFUSERS_SAVE_MODE`进行配置。

+   `safe_serialization` (`bool`, *可选*, 默认为`True`) — 是否使用`safetensors`保存模型，还是使用传统的PyTorch方式保存模型。

保存与UNet和文本编码器对应的LoRA参数。

#### `disable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L747)

```py
( )
```

如果启用，则禁用FreeU机制。

#### `enable_freeu`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L724)

```py
( s1: float s2: float b1: float b2: float )
```

参数

+   `s1`（`float`）—用于减弱跳跃特征贡献的阶段1的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `s2`（`float`）—用于减弱跳跃特征贡献的阶段2的缩放因子。这样做是为了减轻增强去噪过程中的“过度平滑效应”。

+   `b1`（`float`）—用于放大骨干特征贡献的阶段1的缩放因子。

+   `b2`（`float`）—用于放大骨干特征贡献的阶段2的缩放因子。

启用FreeU机制，如[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)中所述。

缩放因子后缀表示它们被应用的阶段。

请参考[官方存储库](https://github.com/ChenyangSi/FreeU)，了解已知适用于不同管道（如Stable Diffusion v1、v2和Stable Diffusion XL）的值组合。

#### `encode_prompt`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L325)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

参数

+   `prompt`（`str`或`List[str]`，*可选*）—要编码的提示设备—（`torch.device`）：torch设备

+   `num_images_per_prompt`（`int`）—应该为每个提示生成的图像数量

+   `do_classifier_free_guidance`（`bool`）—是否使用分类器自由指导

+   `negative_prompt`（`str`或`List[str]`，*可选*）—不用于指导图像生成的提示或提示。如果未定义，则必须传递`negative_prompt_embeds`。如果不使用指导（即，如果`guidance_scale`小于`1`，则忽略）。

+   `prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`prompt`输入参数生成文本嵌入。

+   `negative_prompt_embeds`（`torch.FloatTensor`，*可选*）—预生成的负文本嵌入。可用于轻松调整文本输入，例如提示加权。如果未提供，将从`negative_prompt`输入参数生成negative_prompt_embeds。

+   `lora_scale`（`float`，*可选*）—如果加载了LoRA层，则将应用于文本编码器的所有LoRA层的LoRA比例。

+   `clip_skip`（`int`，*可选*）—在计算提示嵌入时要从CLIP跳过的层数。值为1表示将使用预最终层的输出来计算提示嵌入。

将提示编码为文本编码器隐藏状态。

#### `fuse_qkv_projections`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L752)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet`（`bool`，默认为`True`）—在UNet上应用融合。

+   `vae`（`bool`，默认为`True`）—在VAE上应用融合。

启用融合的QKV投影。对于自注意力模块，所有投影矩阵（即查询、键、值）都被融合。对于交叉注意力模块，键和值投影矩阵被融合。

此API为🧪实验性质。

#### `get_guidance_scale_embedding`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L813)

```py
( w embedding_dim = 512 dtype = torch.float32 ) → export const metadata = 'undefined';torch.FloatTensor
```

参数

+   `timesteps`（`torch.Tensor`）—在这些时间步生成嵌入向量

+   `embedding_dim` (`int`, *可选*, 默认为 512) — 生成嵌入的维度 dtype — 生成嵌入的数据类型

返回

`torch.FloatTensor`

形状为 `(len(timesteps), embedding_dim)` 的嵌入向量

查看 [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L784)

```py
( unet: bool = True vae: bool = True )
```

参数

+   `unet` (`bool`, 默认为 `True`) — 对 UNet 应用融合。

+   `vae` (`bool`, 默认为 `True`) — 对 VAE 应用融合。

如果启用，则禁用 QKV 投影融合。

此 API 是 🧪 实验性的。

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

参数

+   `images` (`List[PIL.Image.Image]` 或 `np.ndarray`) — 长度为 `batch_size` 的去噪 PIL 图像列表或形状为 `(batch_size, height, width, num_channels)` 的 NumPy 数组。

+   `nsfw_content_detected` (`List[bool]`) — 列表指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，如果无法执行安全检查，则为 `None`。

稳定扩散管道的输出类。

## FlaxStableDiffusionImg2ImgPipeline

### `class diffusers.FlaxStableDiffusionImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py#L105)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )
```

参数

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) — 变分自动编码器（VAE）模型，用于对图像进行编码和解码以及从潜在表示到图像的解码。

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) — 冻结的文本编码器（[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)）。

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) — 用于对文本进行标记化的 `CLIPTokenizer`。

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) — 用于去噪编码图像潜变量的 `FlaxUNet2DConditionModel`。

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) — 与 `unet` 结合使用以去噪编码图像潜变量的调度器。可以是 `FlaxDDIMScheduler`、`FlaxLMSDiscreteScheduler`、`FlaxPNDMScheduler` 或 `FlaxDPMSolverMultistepScheduler` 中的一个。

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) — 用于估计生成图像是否可能被认为具有冒犯性或有害的分类模块。请参考 [模型卡片](https://huggingface.co/runwayml/stable-diffusion-v1-5) 以获取有关模型潜在危害的更多详细信息。

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) — 用于从生成的图像中提取特征的 `CLIPImageProcessor`；作为 `safety_checker` 的输入。

基于 Flax 的文本引导图像到图像生成的稳定扩散管道。

此模型继承自 [FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)。检查超类文档以获取所有管道实现的通用方法（下载、保存、在特定设备上运行等）。

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py#L337)

```py
( prompt_ids: Array image: Array params: Union prng_seed: Array strength: float = 0.8 num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 noise: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) → export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

参数

+   `prompt_ids`（`jnp.ndarray`） - 用于指导图像生成的提示或提示。

+   `image`（`jnp.ndarray`） - 表示要用作起点的图像批处理的数组。

+   `params`（`Dict`或`FrozenDict`） - 包含模型参数/权重的字典。

+   `prng_seed`（`jax.Array`或`jax.Array`） - 包含随机数生成器密钥的数组。

+   `strength`（`float`，*可选*，默认为0.8） - 指示转换参考`image`的程度。必须介于0和1之间。`image`用作起点，`strength`越高，添加的噪声越多。降噪步骤的数量取决于最初添加的噪声量。当`strength`为1时，添加的噪声最大，并且降噪过程将运行指定的`num_inference_steps`的完整迭代次数。值为1基本上忽略`image`。

+   `num_inference_steps`（`int`，*可选*，默认为50） - 降噪步骤的数量。更多的降噪步骤通常会导致更高质量的图像，但会降低推理速度。此参数由`strength`调节。

+   `height`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`） - 生成图像的像素高度。

+   `width`（`int`，*可选*，默认为`self.unet.config.sample_size * self.vae_scale_factor`） - 生成图像的像素宽度。

+   `guidance_scale`（`float`，*可选*，默认为7.5） - 更高的引导比例值鼓励模型生成与文本`prompt`紧密相关的图像，但会降低图像质量。当`guidance_scale > 1`时启用引导比例。

+   `noise`（`jnp.ndarray`，*可选*） - 从高斯分布中采样的预生成嘈杂潜在变量，用作图像生成的输入。可用于使用不同提示调整相同生成。该数组是通过使用提供的随机`generator`进行采样生成的。

+   `return_dict`（`bool`，*可选*，默认为`True`） - 是否返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)而不是普通元组。

+   `jit`（`bool`，默认为`False`） - 是否运行生成和安全评分函数的`pmap`版本。

    存在此参数是因为`__call__`尚不是端到端的pmap-able。它将在将来的版本中被移除。

返回

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)或`tuple`

如果`return_dict`为`True`，则返回[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)，否则返回一个`tuple`，其中第一个元素是生成图像的列表，第二个元素是一个包含相应生成图像是否包含“不适宜工作”（nsfw）内容的`bool`列表。

用于生成的管道的调用函数。

示例：

```py
>>> import jax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> import requests
>>> from io import BytesIO
>>> from PIL import Image
>>> from diffusers import FlaxStableDiffusionImg2ImgPipeline

>>> def create_key(seed=0):
...     return jax.random.PRNGKey(seed)

>>> rng = create_key(0)

>>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
>>> response = requests.get(url)
>>> init_img = Image.open(BytesIO(response.content)).convert("RGB")
>>> init_img = init_img.resize((768, 512))

>>> prompts = "A fantasy landscape, trending on artstation"

>>> pipeline, params = FlaxStableDiffusionImg2ImgPipeline.from_pretrained(
...     "CompVis/stable-diffusion-v1-4",
...     revision="flax",
...     dtype=jnp.bfloat16,
... )

>>> num_samples = jax.device_count()
>>> rng = jax.random.split(rng, jax.device_count())
>>> prompt_ids, processed_image = pipeline.prepare_inputs(
...     prompt=[prompts] * num_samples, image=[init_img] * num_samples
... )
>>> p_params = replicate(params)
>>> prompt_ids = shard(prompt_ids)
>>> processed_image = shard(processed_image)

>>> output = pipeline(
...     prompt_ids=prompt_ids,
...     image=processed_image,
...     params=p_params,
...     prng_seed=rng,
...     strength=0.75,
...     num_inference_steps=50,
...     jit=True,
...     height=512,
...     width=768,
... ).images

>>> output_images = pipeline.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
```

## FlaxStableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

参数

+   `images`（`np.ndarray`） - 形状为`(batch_size, height, width, num_channels)`的去噪图像数组。

+   `nsfw_content_detected`（`List[bool]`） - 列表指示相应生成的图像是否包含“不适宜工作”（nsfw）内容，或者如果无法执行安全检查，则为`None`。

基于Flax的稳定扩散管道的输出类。

#### `replace`

[<来源>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

“返回一个用新值替换指定字段的新对象。
