# å›¾åƒåˆ°å›¾åƒ

> åŸæ–‡é“¾æ¥ï¼š[https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img)

ç¨³å®šæ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥åº”ç”¨äºå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆï¼Œé€šè¿‡ä¼ é€’æ–‡æœ¬æç¤ºå’Œåˆå§‹å›¾åƒæ¥è°ƒèŠ‚æ–°å›¾åƒçš„ç”Ÿæˆã€‚

[StableDiffusionImg2ImgPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) ä½¿ç”¨äº† [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://huggingface.co/papers/2108.01073) ä¸­æå‡ºçš„æ‰©æ•£å»å™ªæœºåˆ¶ï¼Œä½œè€…ä¸º Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermonã€‚

è¯¥è®ºæ–‡çš„æ‘˜è¦æ˜¯ï¼š

*å¼•å¯¼å›¾åƒåˆæˆä½¿æ™®é€šç”¨æˆ·èƒ½å¤Ÿä»¥æœ€å°çš„åŠªåŠ›åˆ›å»ºå’Œç¼–è¾‘é€¼çœŸçš„å›¾åƒã€‚å…³é”®æŒ‘æˆ˜åœ¨äºå¹³è¡¡å¯¹ç”¨æˆ·è¾“å…¥ï¼ˆä¾‹å¦‚æ‰‹ç»˜çš„å½©è‰²ç¬”ç”»ï¼‰çš„å¿ å®æ€§å’Œåˆæˆå›¾åƒçš„é€¼çœŸæ€§ã€‚ç°æœ‰åŸºäºGANçš„æ–¹æ³•å°è¯•ä½¿ç”¨æ¡ä»¶GANæˆ–GANåæ¼”æ¥å®ç°è¿™ç§å¹³è¡¡ï¼Œè¿™æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œé€šå¸¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æŸå¤±å‡½æ•°æ¥é€‚ç”¨äºå„ä¸ªåº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒåˆæˆå’Œç¼–è¾‘æ–¹æ³•ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹ç”Ÿæˆå…ˆéªŒçš„éšæœºå¾®åˆ†ç¼–è¾‘ï¼ˆSDEditï¼‰ï¼Œé€šè¿‡éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰è¿­ä»£å»å™ªæ¥åˆæˆé€¼çœŸçš„å›¾åƒã€‚ç»™å®šä»»ä½•ç±»å‹çš„ç”¨æˆ·æŒ‡å¯¼çš„è¾“å…¥å›¾åƒï¼ŒSDEdité¦–å…ˆå‘è¾“å…¥å›¾åƒæ·»åŠ å™ªå£°ï¼Œç„¶åé€šè¿‡SDEå…ˆéªŒé€æ­¥å»å™ªç»“æœå›¾åƒä»¥å¢åŠ å…¶é€¼çœŸæ€§ã€‚SDEditä¸éœ€è¦ä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒæˆ–åæ¼”ï¼Œå¯ä»¥è‡ªç„¶åœ°å®ç°é€¼çœŸæ€§å’Œå¿ å®æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚æ ¹æ®äººç±»æ„ŸçŸ¥ç ”ç©¶ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬åŸºäºç¬”ç”»çš„å›¾åƒåˆæˆå’Œç¼–è¾‘ä»¥åŠå›¾åƒåˆæˆï¼ŒSDEditåœ¨é€¼çœŸæ€§å’Œæ•´ä½“æ»¡æ„åº¦å¾—åˆ†ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºäºGANçš„æ–¹æ³•ï¼Œæœ€é«˜å¯è¾¾98.09%å’Œ91.72%ã€‚*

è¯·åŠ¡å¿…æŸ¥çœ‹ç¨³å®šæ‰©æ•£ [æç¤º](overview#tips) éƒ¨åˆ†ï¼Œäº†è§£å¦‚ä½•æ¢ç´¢è°ƒåº¦å™¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆåœ°é‡ç”¨ç®¡é“ç»„ä»¶ï¼

## StableDiffusionImg2ImgPipeline

### `class diffusers.StableDiffusionImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L158)

```py
( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor image_encoder: CLIPVisionModelWithProjection = None requires_safety_checker: bool = True )
```

å‚æ•°

+   `vae` ([AutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.AutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç åˆ°æ½œåœ¨è¡¨ç¤ºã€‚

+   `text_encoder` ([CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `unet` ([UNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.UNet2DConditionModel)) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„ `UNet2DConditionModel`ã€‚

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) â€” ä¸ `unet` ç»“åˆä½¿ç”¨çš„è°ƒåº¦å™¨ï¼Œç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡ã€‚å¯ä»¥æ˜¯ [DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ã€[LMSDiscreteScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) æˆ– [PNDMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/pndm#diffusers.PNDMScheduler) ä¸­çš„ä¸€ä¸ªã€‚

+   `safety_checker` (`StableDiffusionSafetyChecker`) â€” ä¼°è®¡ç”Ÿæˆçš„å›¾åƒæ˜¯å¦å¯èƒ½è¢«è§†ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„åˆ†ç±»æ¨¡å—ã€‚æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[model card](https://huggingface.co/runwayml/stable-diffusion-v1-5)ã€‚

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) â€” ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„`CLIPImageProcessor`ï¼›ä½œä¸º`safety_checker`çš„è¾“å…¥ã€‚

ä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„æµç¨‹ã€‚

è¯¥æ¨¡å‹ç»§æ‰¿è‡ª[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–ä¸ºæ‰€æœ‰æµç¨‹å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

è¯¥æµç¨‹è¿˜ç»§æ‰¿äº†ä»¥ä¸‹åŠ è½½æ–¹æ³•ï¼š

+   åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥çš„[load_textual_inversion()](/docs/diffusers/v0.26.3/en/api/loaders/textual_inversion#diffusers.loaders.TextualInversionLoaderMixin.load_textual_inversion)

+   [load_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_weights) ç”¨äºåŠ è½½LoRAæƒé‡

+   [save_lora_weights()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.save_lora_weights) ç”¨äºä¿å­˜LoRAæƒé‡

+   [from_single_file()](/docs/diffusers/v0.26.3/en/api/loaders/single_file#diffusers.loaders.FromSingleFileMixin.from_single_file) ç”¨äºåŠ è½½`.ckpt`æ–‡ä»¶

+   [load_ip_adapter()](/docs/diffusers/v0.26.3/en/api/loaders/ip_adapter#diffusers.loaders.IPAdapterMixin.load_ip_adapter) ç”¨äºåŠ è½½IPé€‚é…å™¨

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L868)

```py
( prompt: Union = None image: Union = None strength: float = 0.8 num_inference_steps: Optional = 50 timesteps: List = None guidance_scale: Optional = 7.5 negative_prompt: Union = None num_images_per_prompt: Optional = 1 eta: Optional = 0.0 generator: Union = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None ip_adapter_image: Union = None output_type: Optional = 'pil' return_dict: bool = True cross_attention_kwargs: Optional = None clip_skip: int = None callback_on_step_end: Optional = None callback_on_step_end_tensor_inputs: List = ['latents'] **kwargs ) â†’ export const metadata = 'undefined';StableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt` (`str` æˆ– `List[str]`, *å¯é€‰*) â€” ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`prompt_embeds`ã€‚

+   `image` (`torch.FloatTensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, æˆ– `List[np.ndarray]`) â€” ç”¨ä½œèµ·ç‚¹çš„å›¾åƒæ‰¹æ¬¡çš„`Image`ã€numpyæ•°ç»„æˆ–å¼ é‡è¡¨ç¤ºã€‚å¯¹äºnumpyæ•°ç»„å’Œpytorchå¼ é‡ï¼ŒæœŸæœ›çš„å€¼èŒƒå›´åœ¨`[0, 1]`ä¹‹é—´ã€‚å¦‚æœæ˜¯å¼ é‡æˆ–å¼ é‡åˆ—è¡¨ï¼Œåˆ™æœŸæœ›çš„å½¢çŠ¶åº”ä¸º`(B, C, H, W)`æˆ–`(C, H, W)`ã€‚å¦‚æœæ˜¯numpyæ•°ç»„æˆ–æ•°ç»„åˆ—è¡¨ï¼Œåˆ™æœŸæœ›çš„å½¢çŠ¶åº”ä¸º`(B, H, W, C)`æˆ–`(H, W, C)`ã€‚å®ƒè¿˜å¯ä»¥æ¥å—å›¾åƒæ½œå˜é‡ä½œä¸º`image`ï¼Œä½†å¦‚æœç›´æ¥ä¼ é€’æ½œå˜é‡ï¼Œåˆ™ä¸ä¼šå†æ¬¡ç¼–ç ã€‚

+   `strength` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8) â€” è¡¨ç¤ºè½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»åœ¨0å’Œ1ä¹‹é—´ã€‚`image`è¢«ç”¨ä½œèµ·ç‚¹ï¼Œ`strength`è¶Šé«˜ï¼Œæ·»åŠ çš„å™ªéŸ³å°±è¶Šå¤šã€‚å»å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªéŸ³é‡ã€‚å½“`strength`ä¸º1æ—¶ï¼Œæ·»åŠ çš„å™ªéŸ³æ˜¯æœ€å¤§çš„ï¼Œå»å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å®Œæ•´è¿­ä»£æ¬¡æ•°ã€‚å€¼ä¸º1åŸºæœ¬ä¸Šå¿½ç•¥äº†`image`ã€‚

+   `num_inference_steps` (`int`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º50) â€” å»å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„å»å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚è¯¥å‚æ•°å—`strength`è°ƒèŠ‚ã€‚

+   `timesteps` (`List[int]`, *å¯é€‰*) â€” ç”¨äºå…·æœ‰æ”¯æŒ`timesteps`å‚æ•°çš„è°ƒåº¦å™¨çš„å»å™ªè¿‡ç¨‹çš„è‡ªå®šä¹‰æ—¶é—´æ­¥é•¿ï¼Œåœ¨å…¶`set_timesteps`æ–¹æ³•ä¸­ã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å°†ä½¿ç”¨ä¼ é€’`num_inference_steps`æ—¶çš„é»˜è®¤è¡Œä¸ºã€‚å¿…é¡»æŒ‰é™åºæ’åˆ—ã€‚

+   `guidance_scale` (`float`, *å¯é€‰*ï¼Œé»˜è®¤ä¸º7.5) â€” æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `negative_prompt` (`str` æˆ– `List[str]`, *optional*) â€” æŒ‡å¯¼å›¾åƒç”Ÿæˆä¸­ä¸åŒ…æ‹¬çš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™éœ€è¦ä¼ é€’`negative_prompt_embeds`ã€‚åœ¨ä¸ä½¿ç”¨æŒ‡å¯¼æ—¶ï¼ˆ`guidance_scale < 1`ï¼‰å°†è¢«å¿½ç•¥ã€‚

+   `num_images_per_prompt` (`int`, *optional*, é»˜è®¤ä¸º1) â€” æ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡ã€‚

+   `eta` (`float`, *optional*, é»˜è®¤ä¸º0.0) â€” å¯¹åº”äº[DDIM](https://arxiv.org/abs/2010.02502)è®ºæ–‡ä¸­çš„å‚æ•°eta (Î·)ã€‚ä»…é€‚ç”¨äº[DDIMScheduler](/docs/diffusers/v0.26.3/en/api/schedulers/ddim#diffusers.DDIMScheduler)ï¼Œåœ¨å…¶ä»–è°ƒåº¦ç¨‹åºä¸­è¢«å¿½ç•¥ã€‚

+   `generator` (`torch.Generator` æˆ– `List[torch.Generator]`, *optional*) â€” ç”¨äºä½¿ç”Ÿæˆè¿‡ç¨‹ç¡®å®šæ€§çš„ [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html)ã€‚

+   `prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds` (`torch.FloatTensor`, *optional*) â€” é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼ˆæç¤ºåŠ æƒï¼‰ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆ`negative_prompt_embeds`ã€‚ip_adapter_image â€” (`PipelineImageInput`, *optional*): ä¸IPé€‚é…å™¨ä¸€èµ·ä½¿ç”¨çš„å¯é€‰å›¾åƒè¾“å…¥ã€‚

+   `output_type` (`str`, *optional*, é»˜è®¤ä¸º`"pil"`) â€” ç”Ÿæˆå›¾åƒçš„è¾“å‡ºæ ¼å¼ã€‚é€‰æ‹©`PIL.Image`æˆ–`np.array`ä¹‹é—´çš„ä¸€ä¸ªã€‚

+   `return_dict` (`bool`, *optional*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦è¿”å›ä¸€ä¸ª[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)è€Œä¸æ˜¯ä¸€ä¸ªæ™®é€šçš„å…ƒç»„ã€‚

+   `cross_attention_kwargs` (`dict`, *optional*) â€” ä¸€ä¸ªkwargså­—å…¸ï¼Œå¦‚æœæŒ‡å®šï¼Œåˆ™ä¼ é€’ç»™[`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py)ä¸­å®šä¹‰çš„`AttentionProcessor`ã€‚

+   `clip_skip` (`int`, *optional*) â€” åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPè·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1æ„å‘³ç€å°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

+   `callback_on_step_end` (`Callable`, *optional*) â€” åœ¨æ¨æ–­è¿‡ç¨‹ä¸­æ¯ä¸ªå»å™ªæ­¥éª¤ç»“æŸæ—¶è°ƒç”¨çš„å‡½æ•°ã€‚è¯¥å‡½æ•°å°†ä½¿ç”¨ä»¥ä¸‹å‚æ•°è°ƒç”¨ï¼š`callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`ã€‚`callback_kwargs`å°†åŒ…å«ç”±`callback_on_step_end_tensor_inputs`æŒ‡å®šçš„æ‰€æœ‰å¼ é‡çš„åˆ—è¡¨ã€‚

+   `callback_on_step_end_tensor_inputs` (`List`, *optional*) â€” `callback_on_step_end`å‡½æ•°çš„å¼ é‡è¾“å…¥åˆ—è¡¨ã€‚åˆ—è¡¨ä¸­æŒ‡å®šçš„å¼ é‡å°†ä½œä¸º`callback_kwargs`å‚æ•°ä¼ é€’ã€‚æ‚¨åªèƒ½åŒ…å«åœ¨ç®¡é“ç±»çš„`._callback_tensor_inputs`å±æ€§ä¸­åˆ—å‡ºçš„å˜é‡ã€‚

è¿”å›

[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput) æˆ– `tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œå°†è¿”å›[StableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/upscale#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput)ï¼Œå¦åˆ™å°†è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª`bool`åˆ—è¡¨ï¼ŒæŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸å®‰å…¨å†…å®¹â€ï¼ˆnsfwï¼‰ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import requests
>>> import torch
>>> from PIL import Image
>>> from io import BytesIO

>>> from diffusers import StableDiffusionImg2ImgPipeline

>>> device = "cuda"
>>> model_id_or_path = "runwayml/stable-diffusion-v1-5"
>>> pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)
>>> pipe = pipe.to(device)

>>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"

>>> response = requests.get(url)
>>> init_image = Image.open(BytesIO(response.content)).convert("RGB")
>>> init_image = init_image.resize((768, 512))

>>> prompt = "A fantasy landscape, trending on artstation"

>>> images = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images
>>> images[0].save("fantasy_landscape.png")
```

#### `enable_attention_slicing`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2063)

```py
( slice_size: Union = 'auto' )
```

å‚æ•°

+   `slice_size`ï¼ˆ`str`æˆ–`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`"auto"`ï¼‰â€”å½“ä¸º`"auto"`æ—¶ï¼Œå°†è¾“å…¥å‡åŠç»™æ³¨æ„åŠ›å¤´ï¼Œå› æ­¤æ³¨æ„åŠ›å°†åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­è®¡ç®—ã€‚å¦‚æœä¸º`"max"`ï¼Œå°†é€šè¿‡ä¸€æ¬¡åªè¿è¡Œä¸€ä¸ªåˆ‡ç‰‡æ¥èŠ‚çœæœ€å¤§å†…å­˜é‡ã€‚å¦‚æœæä¾›äº†ä¸€ä¸ªæ•°å­—ï¼Œåˆ™ä½¿ç”¨`attention_head_dim // slice_size`ä¸ªåˆ‡ç‰‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`attention_head_dim`å¿…é¡»æ˜¯`slice_size`çš„å€æ•°ã€‚

å¯ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆç‰‡æ®µï¼Œä»¥ä¾¿åœ¨å¤šä¸ªæ­¥éª¤ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚å¯¹äºå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œè®¡ç®—å°†æŒ‰é¡ºåºåœ¨æ¯ä¸ªå¤´ä¸Šæ‰§è¡Œã€‚è¿™å¯¹äºåœ¨èŠ‚çœä¸€äº›å†…å­˜çš„åŒæ—¶æ¢å–ä¸€ç‚¹é€Ÿåº¦é™ä½æ˜¯æœ‰ç”¨çš„ã€‚

âš ï¸ å¦‚æœæ‚¨å·²ç»åœ¨ä½¿ç”¨PyTorch 2.0æˆ–xFormersä¸­çš„`scaled_dot_product_attention`ï¼ˆSDPAï¼‰ï¼Œè¯·ä¸è¦å¯ç”¨æ³¨æ„åŠ›åˆ‡ç‰‡ã€‚è¿™äº›æ³¨æ„åŠ›è®¡ç®—å·²ç»éå¸¸é«˜æ•ˆï¼Œå› æ­¤æ‚¨ä¸éœ€è¦å¯ç”¨æ­¤åŠŸèƒ½ã€‚å¦‚æœæ‚¨åœ¨SDPAæˆ–xFormersä¸­å¯ç”¨äº†æ³¨æ„åŠ›åˆ‡ç‰‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡å‡é€Ÿï¼

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import StableDiffusionPipeline

>>> pipe = StableDiffusionPipeline.from_pretrained(
...     "runwayml/stable-diffusion-v1-5",
...     torch_dtype=torch.float16,
...     use_safetensors=True,
... )

>>> prompt = "a photo of an astronaut riding a horse on mars"
>>> pipe.enable_attention_slicing()
>>> image = pipe(prompt).images[0]
```

#### `disable_attention_slicing`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2103)

```py
( )
```

ç¦ç”¨åˆ‡ç‰‡æ³¨æ„åŠ›è®¡ç®—ã€‚å¦‚æœä¹‹å‰è°ƒç”¨äº†`enable_attention_slicing`ï¼Œåˆ™æ³¨æ„åŠ›å°†åœ¨ä¸€æ­¥ä¸­è®¡ç®—ã€‚

#### `enable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2002)

```py
( attention_op: Optional = None )
```

å‚æ•°

+   `attention_op`ï¼ˆ`Callable`ï¼Œ*å¯é€‰*ï¼‰â€”è¦†ç›–é»˜è®¤çš„`None`è¿ç®—ç¬¦ï¼Œç”¨ä½œxFormersçš„[`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)å‡½æ•°çš„`op`å‚æ•°ã€‚

ä»[xFormers](https://facebookresearch.github.io/xformers/)å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚å½“å¯ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œæ‚¨åº”è¯¥è§‚å¯Ÿåˆ°æ›´ä½çš„GPUå†…å­˜ä½¿ç”¨é‡ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½ä¼šåŠ é€Ÿã€‚è®­ç»ƒè¿‡ç¨‹ä¸­çš„åŠ é€Ÿä¸è¢«ä¿è¯ã€‚

âš ï¸ å½“å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›å’Œåˆ‡ç‰‡æ³¨æ„åŠ›éƒ½å¯ç”¨æ—¶ï¼Œå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ä¼˜å…ˆã€‚

ç¤ºä¾‹ï¼š

```py
>>> import torch
>>> from diffusers import DiffusionPipeline
>>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp

>>> pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16)
>>> pipe = pipe.to("cuda")
>>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)
>>> # Workaround for not accepting attention shape using VAE for Flash Attention
>>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)
```

#### `disable_xformers_memory_efficient_attention`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/pipeline_utils.py#L2037)

```py
( )
```

ç¦ç”¨[xFormers](https://facebookresearch.github.io/xformers/)ä¸­çš„å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›ã€‚

#### `load_textual_inversion`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/textual_inversion.py#L265)

```py
( pretrained_model_name_or_path: Union token: Union = None tokenizer: Optional = None text_encoder: Optional = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path`ï¼ˆ`str`æˆ–`os.PathLike`æˆ–`List[stræˆ–os.PathLike]`æˆ–`Dict`æˆ–`List[Dict]`ï¼‰â€”å¯ä»¥æ˜¯ä»¥ä¸‹ä¹‹ä¸€æˆ–å…¶ä¸­çš„ä¸€ä¸ªåˆ—è¡¨ï¼š

    +   ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„*æ¨¡å‹ID*ï¼ˆä¾‹å¦‚`sd-concepts-library/low-poly-hd-logos-icons`ï¼‰ï¼Œæ‰˜ç®¡åœ¨Hubä¸Šã€‚

    +   åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*ç›®å½•*è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversion_directory/`ï¼‰ã€‚

    +   åŒ…å«æ–‡æœ¬åè½¬æƒé‡çš„*æ–‡ä»¶*è·¯å¾„ï¼ˆä¾‹å¦‚`./my_text_inversions.pt`ï¼‰ã€‚

    +   ä¸€ä¸ª[torchçŠ¶æ€å­—å…¸](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)ã€‚

+   `token`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦†ç›–ç”¨äºæ–‡æœ¬åè½¬æƒé‡çš„ä»¤ç‰Œã€‚å¦‚æœ`pretrained_model_name_or_path`æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ™`token`ä¹Ÿå¿…é¡»æ˜¯ç›¸åŒé•¿åº¦çš„åˆ—è¡¨ã€‚

+   `text_encoder`ï¼ˆ[CLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTextModel)ï¼Œ*å¯é€‰*ï¼‰â€”å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `tokenizer`ï¼ˆ[CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)ï¼Œ*å¯é€‰*ï¼‰â€”ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„`CLIPTokenizer`ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå‡½æ•°å°†ä½¿ç”¨self.tokenizerã€‚

+   `weight_name` (`str`, *optional*) â€” è‡ªå®šä¹‰æƒé‡æ–‡ä»¶çš„åç§°ã€‚åœ¨ä»¥ä¸‹æƒ…å†µä¸‹åº”ä½¿ç”¨æ­¤é€‰é¡¹ï¼š

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨ğŸ¤— Diffusersæ ¼å¼ï¼Œä½†ä¿å­˜åœ¨ç‰¹å®šæƒé‡åç§°ä¸‹ï¼Œä¾‹å¦‚`text_inv.bin`ã€‚

    +   ä¿å­˜çš„æ–‡æœ¬åè½¬æ–‡ä»¶é‡‡ç”¨Automatic1111æ ¼å¼ã€‚

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” å¦‚æœä¸ä½¿ç”¨æ ‡å‡†ç¼“å­˜ï¼Œåˆ™ç¼“å­˜ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ã€‚

+   `force_download` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `resume_download` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™åˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies` (`Dict[str, str]`, *optional*) â€” ä¸€ä¸ªä»£ç†æœåŠ¡å™¨å­—å…¸ï¼ŒæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚è¿™äº›ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸­ä½¿ç”¨ã€‚

+   `local_files_only` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™ä¸ä¼šä»Hubä¸‹è½½æ¨¡å‹ã€‚

+   `token` (`str`æˆ–*bool*, *optional*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ï¼‰ã€‚

+   `revision` (`str`, *optional*, é»˜è®¤ä¸º`"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `subfolder` (`str`, *optional*, é»˜è®¤ä¸º`""`) â€” æ¨¡å‹æ–‡ä»¶åœ¨Hubæˆ–æœ¬åœ°è¾ƒå¤§æ¨¡å‹å­˜å‚¨åº“ä¸­çš„å­æ–‡ä»¶å¤¹ä½ç½®ã€‚

+   `mirror` (`str`, *optional*) â€” å¦‚æœæ‚¨åœ¨ä¸­å›½ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ°å¯è®¿é—®æ€§é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨é•œåƒæºè§£å†³ã€‚æˆ‘ä»¬ä¸ä¿è¯æºçš„åŠæ—¶æ€§æˆ–å®‰å…¨æ€§ï¼Œæ‚¨åº”å‚è€ƒé•œåƒç«™ç‚¹è·å–æ›´å¤šä¿¡æ¯ã€‚

å°†æ–‡æœ¬åè½¬åµŒå…¥åŠ è½½åˆ°[StableDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.StableDiffusionPipeline)çš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼ˆæ”¯æŒğŸ¤— Diffuserså’ŒAutomatic1111æ ¼å¼ï¼‰ã€‚

ç¤ºä¾‹ï¼š

è¦åŠ è½½ğŸ¤— Diffusersæ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("sd-concepts-library/cat-toy")

prompt = "A <cat-toy> backpack"

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("cat-backpack.png")
```

è¦åŠ è½½Automatic1111æ ¼å¼çš„æ–‡æœ¬åè½¬åµŒå…¥å‘é‡ï¼Œè¯·ç¡®ä¿é¦–å…ˆä¸‹è½½å‘é‡ï¼ˆä¾‹å¦‚ä»[civitAI](https://civitai.com/models/3036?modelVersionId=9857)ï¼‰ï¼Œç„¶ååŠ è½½å‘é‡

æœ¬åœ°ï¼š

```py
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to("cuda")

pipe.load_textual_inversion("./charturnerv2.pt", token="charturnerv2")

prompt = "charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details."

image = pipe(prompt, num_inference_steps=50).images[0]
image.save("character.png")
```

#### `from_single_file`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/single_file.py#L141)

```py
( pretrained_model_link_or_path **kwargs )
```

å‚æ•°

+   `pretrained_model_link_or_path` (`str`æˆ–`os.PathLike`, *optional*) â€” å¯ä»¥æ˜¯ï¼š

    +   Hubä¸Š`.ckpt`æ–‡ä»¶çš„é“¾æ¥ï¼ˆä¾‹å¦‚`"https://huggingface.co/<repo_id>/blob/main/<path_to_file>.ckpt"`ï¼‰ã€‚

    +   åŒ…å«æ‰€æœ‰ç®¡é“æƒé‡çš„*æ–‡ä»¶*çš„è·¯å¾„ã€‚

+   `torch_dtype` (`str`æˆ–`torch.dtype`, *optional*) â€” è¦†ç›–é»˜è®¤çš„`torch.dtype`å¹¶ä½¿ç”¨å¦ä¸€ç§dtypeåŠ è½½æ¨¡å‹ã€‚

+   `force_download` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦å¼ºåˆ¶ï¼ˆé‡æ–°ï¼‰ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ï¼Œè¦†ç›–ç¼“å­˜ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

+   `cache_dir` (`Union[str, os.PathLike]`, *optional*) â€” å¦‚æœä¸ä½¿ç”¨æ ‡å‡†ç¼“å­˜ï¼Œåˆ™ç¼“å­˜ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®çš„ç›®å½•è·¯å¾„ã€‚

+   `resume_download` (`bool`, *optional*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦æ¢å¤ä¸‹è½½æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™åˆ é™¤ä»»ä½•æœªå®Œå…¨ä¸‹è½½çš„æ–‡ä»¶ã€‚

+   `proxies` (`Dict[str, str]`, *optional*) â€” ä¸€ä¸ªä»£ç†æœåŠ¡å™¨å­—å…¸ï¼ŒæŒ‰åè®®æˆ–ç«¯ç‚¹ä½¿ç”¨ï¼Œä¾‹å¦‚ï¼Œ`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`ã€‚è¿™äº›ä»£ç†åœ¨æ¯ä¸ªè¯·æ±‚ä¸­ä½¿ç”¨ã€‚

+   `local_files_only` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`False`) â€” æ˜¯å¦ä»…åŠ è½½æœ¬åœ°æ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™æ¨¡å‹ä¸ä¼šä»Hubä¸‹è½½ã€‚

+   `token` (`str` æˆ– *bool*, *å¯é€‰*) â€” ç”¨ä½œè¿œç¨‹æ–‡ä»¶çš„HTTP beareræˆæƒçš„ä»¤ç‰Œã€‚å¦‚æœä¸º`True`ï¼Œåˆ™ä½¿ç”¨ä»`diffusers-cli login`ç”Ÿæˆçš„ä»¤ç‰Œï¼ˆå­˜å‚¨åœ¨`~/.huggingface`ï¼‰ã€‚

+   `revision` (`str`, *å¯é€‰*, é»˜è®¤ä¸º`"main"`) â€” è¦ä½¿ç”¨çš„ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬ã€‚å¯ä»¥æ˜¯åˆ†æ”¯åç§°ã€æ ‡ç­¾åç§°ã€æäº¤IDæˆ–Gitå…è®¸çš„ä»»ä½•æ ‡è¯†ç¬¦ã€‚

+   `use_safetensors` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`None`) â€” å¦‚æœè®¾ç½®ä¸º`None`ï¼Œåˆ™ä¼šä¸‹è½½safetensorsæƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰**å¹¶ä¸”**å¦‚æœå·²å®‰è£…safetensorsåº“ã€‚å¦‚æœè®¾ç½®ä¸º`True`ï¼Œåˆ™å°†å¼ºåˆ¶ä»safetensorsæƒé‡åŠ è½½æ¨¡å‹ã€‚å¦‚æœè®¾ç½®ä¸º`False`ï¼Œåˆ™ä¸ä¼šåŠ è½½safetensorsæƒé‡ã€‚

ä»ä»¥`.ckpt`æˆ–`.safetensors`æ ¼å¼ä¿å­˜çš„é¢„è®­ç»ƒç®¡é“æƒé‡å®ä¾‹åŒ–[DiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.DiffusionPipeline)ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œç®¡é“è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆ`model.eval()`ï¼‰ã€‚

ç¤ºä¾‹ï¼š

```py
>>> from diffusers import StableDiffusionPipeline

>>> # Download pipeline from huggingface.co and cache.
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors"
... )

>>> # Download pipeline from local file
>>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt
>>> pipeline = StableDiffusionPipeline.from_single_file("./v1-5-pruned-emaonly")

>>> # Enable float16 and move to GPU
>>> pipeline = StableDiffusionPipeline.from_single_file(
...     "https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt",
...     torch_dtype=torch.float16,
... )
>>> pipeline.to("cuda")
```

#### `load_lora_weights`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L83)

```py
( pretrained_model_name_or_path_or_dict: Union adapter_name = None **kwargs )
```

å‚æ•°

+   `pretrained_model_name_or_path_or_dict` (`str` æˆ– `os.PathLike` æˆ– `dict`) â€” æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚

+   `kwargs` (`dict`, *å¯é€‰*) â€” æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ã€‚

+   `adapter_name` (`str`, *å¯é€‰*) â€” ç”¨äºå¼•ç”¨åŠ è½½çš„é€‚é…å™¨æ¨¡å‹çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨`default_{i}`ï¼Œå…¶ä¸­iæ˜¯æ­£åœ¨åŠ è½½çš„é€‚é…å™¨æ€»æ•°ã€‚

å°†`pretrained_model_name_or_path_or_dict`ä¸­æŒ‡å®šçš„LoRAæƒé‡åŠ è½½åˆ°`self.unet`å’Œ`self.text_encoder`ä¸­ã€‚

æ‰€æœ‰kwargséƒ½å°†è½¬å‘åˆ°`self.lora_state_dict`ã€‚

æŸ¥çœ‹[lora_state_dict()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.lora_state_dict)ä»¥è·å–æœ‰å…³å¦‚ä½•åŠ è½½çŠ¶æ€å­—å…¸çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

æŸ¥çœ‹[load_lora_into_unet()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_unet)ä»¥è·å–æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.unet`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

æŸ¥çœ‹[load_lora_into_text_encoder()](/docs/diffusers/v0.26.3/en/api/loaders/lora#diffusers.loaders.LoraLoaderMixin.load_lora_into_text_encoder)ä»¥è·å–æœ‰å…³å¦‚ä½•å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°`self.text_encoder`ä¸­çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

#### `save_lora_weights`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/loaders/lora.py#L869)

```py
( save_directory: Union unet_lora_layers: Dict = None text_encoder_lora_layers: Dict = None transformer_lora_layers: Dict = None is_main_process: bool = True weight_name: str = None save_function: Callable = None safe_serialization: bool = True )
```

å‚æ•°

+   `save_directory` (`str` æˆ– `os.PathLike`) â€” ä¿å­˜LoRAå‚æ•°çš„ç›®å½•ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œå°†ä¼šåˆ›å»ºã€‚

+   `unet_lora_layers` (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`) â€” ä¸`unet`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚

+   `text_encoder_lora_layers` (`Dict[str, torch.nn.Module]` æˆ– `Dict[str, torch.Tensor]`) â€” ä¸`text_encoder`å¯¹åº”çš„LoRAå±‚çš„çŠ¶æ€å­—å…¸ã€‚å¿…é¡»æ˜¾å¼ä¼ é€’æ–‡æœ¬ç¼–ç å™¨LoRAçŠ¶æ€å­—å…¸ï¼Œå› ä¸ºå®ƒæ¥è‡ªğŸ¤— Transformersã€‚

+   `is_main_process` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” è°ƒç”¨æ­¤å‡½æ•°çš„è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´ï¼Œå½“æ‚¨éœ€è¦åœ¨æ‰€æœ‰è¿›ç¨‹ä¸Šè°ƒç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»…åœ¨ä¸»è¿›ç¨‹ä¸Šè®¾ç½®`is_main_process=True`ä»¥é¿å…ç«äº‰æ¡ä»¶ã€‚

+   `save_function` (`Callable`) â€” ç”¨äºä¿å­˜çŠ¶æ€å­—å…¸çš„å‡½æ•°ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒæœŸé—´ï¼Œå½“æ‚¨éœ€è¦ç”¨å¦ä¸€ç§æ–¹æ³•æ›¿æ¢`torch.save`æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚å¯ä»¥ä½¿ç”¨ç¯å¢ƒå˜é‡`DIFFUSERS_SAVE_MODE`è¿›è¡Œé…ç½®ã€‚

+   `safe_serialization` (`bool`, *å¯é€‰*, é»˜è®¤ä¸º`True`) â€” æ˜¯å¦ä½¿ç”¨`safetensors`ä¿å­˜æ¨¡å‹ï¼Œè¿˜æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„PyTorchæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚

ä¿å­˜ä¸UNetå’Œæ–‡æœ¬ç¼–ç å™¨å¯¹åº”çš„LoRAå‚æ•°ã€‚

#### `disable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L747)

```py
( )
```

å¦‚æœå¯ç”¨ï¼Œåˆ™ç¦ç”¨FreeUæœºåˆ¶ã€‚

#### `enable_freeu`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L724)

```py
( s1: float s2: float b1: float b2: float )
```

å‚æ•°

+   `s1`ï¼ˆ`float`ï¼‰â€”ç”¨äºå‡å¼±è·³è·ƒç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ1çš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `s2`ï¼ˆ`float`ï¼‰â€”ç”¨äºå‡å¼±è·³è·ƒç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ2çš„ç¼©æ”¾å› å­ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†å‡è½»å¢å¼ºå»å™ªè¿‡ç¨‹ä¸­çš„â€œè¿‡åº¦å¹³æ»‘æ•ˆåº”â€ã€‚

+   `b1`ï¼ˆ`float`ï¼‰â€”ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ1çš„ç¼©æ”¾å› å­ã€‚

+   `b2`ï¼ˆ`float`ï¼‰â€”ç”¨äºæ”¾å¤§éª¨å¹²ç‰¹å¾è´¡çŒ®çš„é˜¶æ®µ2çš„ç¼©æ”¾å› å­ã€‚

å¯ç”¨FreeUæœºåˆ¶ï¼Œå¦‚[https://arxiv.org/abs/2309.11497](https://arxiv.org/abs/2309.11497)ä¸­æ‰€è¿°ã€‚

ç¼©æ”¾å› å­åç¼€è¡¨ç¤ºå®ƒä»¬è¢«åº”ç”¨çš„é˜¶æ®µã€‚

è¯·å‚è€ƒ[å®˜æ–¹å­˜å‚¨åº“](https://github.com/ChenyangSi/FreeU)ï¼Œäº†è§£å·²çŸ¥é€‚ç”¨äºä¸åŒç®¡é“ï¼ˆå¦‚Stable Diffusion v1ã€v2å’ŒStable Diffusion XLï¼‰çš„å€¼ç»„åˆã€‚

#### `encode_prompt`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L325)

```py
( prompt device num_images_per_prompt do_classifier_free_guidance negative_prompt = None prompt_embeds: Optional = None negative_prompt_embeds: Optional = None lora_scale: Optional = None clip_skip: Optional = None )
```

å‚æ•°

+   `prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”è¦ç¼–ç çš„æç¤ºè®¾å¤‡â€”ï¼ˆ`torch.device`ï¼‰ï¼štorchè®¾å¤‡

+   `num_images_per_prompt`ï¼ˆ`int`ï¼‰â€”åº”è¯¥ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆçš„å›¾åƒæ•°é‡

+   `do_classifier_free_guidance`ï¼ˆ`bool`ï¼‰â€”æ˜¯å¦ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼

+   `negative_prompt`ï¼ˆ`str`æˆ–`List[str]`ï¼Œ*å¯é€‰*ï¼‰â€”ä¸ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚å¦‚æœæœªå®šä¹‰ï¼Œåˆ™å¿…é¡»ä¼ é€’`negative_prompt_embeds`ã€‚å¦‚æœä¸ä½¿ç”¨æŒ‡å¯¼ï¼ˆå³ï¼Œå¦‚æœ`guidance_scale`å°äº`1`ï¼Œåˆ™å¿½ç•¥ï¼‰ã€‚

+   `prompt_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¢„ç”Ÿæˆçš„æ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`prompt`è¾“å…¥å‚æ•°ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚

+   `negative_prompt_embeds`ï¼ˆ`torch.FloatTensor`ï¼Œ*å¯é€‰*ï¼‰â€”é¢„ç”Ÿæˆçš„è´Ÿæ–‡æœ¬åµŒå…¥ã€‚å¯ç”¨äºè½»æ¾è°ƒæ•´æ–‡æœ¬è¾“å…¥ï¼Œä¾‹å¦‚æç¤ºåŠ æƒã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä»`negative_prompt`è¾“å…¥å‚æ•°ç”Ÿæˆnegative_prompt_embedsã€‚

+   `lora_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼‰â€”å¦‚æœåŠ è½½äº†LoRAå±‚ï¼Œåˆ™å°†åº”ç”¨äºæ–‡æœ¬ç¼–ç å™¨çš„æ‰€æœ‰LoRAå±‚çš„LoRAæ¯”ä¾‹ã€‚

+   `clip_skip`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼‰â€”åœ¨è®¡ç®—æç¤ºåµŒå…¥æ—¶è¦ä»CLIPè·³è¿‡çš„å±‚æ•°ã€‚å€¼ä¸º1è¡¨ç¤ºå°†ä½¿ç”¨é¢„æœ€ç»ˆå±‚çš„è¾“å‡ºæ¥è®¡ç®—æç¤ºåµŒå…¥ã€‚

å°†æç¤ºç¼–ç ä¸ºæ–‡æœ¬ç¼–ç å™¨éšè—çŠ¶æ€ã€‚

#### `fuse_qkv_projections`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L752)

```py
( unet: bool = True vae: bool = True )
```

å‚æ•°

+   `unet`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”åœ¨UNetä¸Šåº”ç”¨èåˆã€‚

+   `vae`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`True`ï¼‰â€”åœ¨VAEä¸Šåº”ç”¨èåˆã€‚

å¯ç”¨èåˆçš„QKVæŠ•å½±ã€‚å¯¹äºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œæ‰€æœ‰æŠ•å½±çŸ©é˜µï¼ˆå³æŸ¥è¯¢ã€é”®ã€å€¼ï¼‰éƒ½è¢«èåˆã€‚å¯¹äºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé”®å’Œå€¼æŠ•å½±çŸ©é˜µè¢«èåˆã€‚

æ­¤APIä¸ºğŸ§ªå®éªŒæ€§è´¨ã€‚

#### `get_guidance_scale_embedding`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L813)

```py
( w embedding_dim = 512 dtype = torch.float32 ) â†’ export const metadata = 'undefined';torch.FloatTensor
```

å‚æ•°

+   `timesteps`ï¼ˆ`torch.Tensor`ï¼‰â€”åœ¨è¿™äº›æ—¶é—´æ­¥ç”ŸæˆåµŒå…¥å‘é‡

+   `embedding_dim` (`int`, *å¯é€‰*, é»˜è®¤ä¸º 512) â€” ç”ŸæˆåµŒå…¥çš„ç»´åº¦ dtype â€” ç”ŸæˆåµŒå…¥çš„æ•°æ®ç±»å‹

è¿”å›

`torch.FloatTensor`

å½¢çŠ¶ä¸º `(len(timesteps), embedding_dim)` çš„åµŒå…¥å‘é‡

æŸ¥çœ‹ [https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298](https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298)

#### `unfuse_qkv_projections`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L784)

```py
( unet: bool = True vae: bool = True )
```

å‚æ•°

+   `unet` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹ UNet åº”ç”¨èåˆã€‚

+   `vae` (`bool`, é»˜è®¤ä¸º `True`) â€” å¯¹ VAE åº”ç”¨èåˆã€‚

å¦‚æœå¯ç”¨ï¼Œåˆ™ç¦ç”¨ QKV æŠ•å½±èåˆã€‚

æ­¤ API æ˜¯ ğŸ§ª å®éªŒæ€§çš„ã€‚

## StableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L10)

```py
( images: Union nsfw_content_detected: Optional )
```

å‚æ•°

+   `images` (`List[PIL.Image.Image]` æˆ– `np.ndarray`) â€” é•¿åº¦ä¸º `batch_size` çš„å»å™ª PIL å›¾åƒåˆ—è¡¨æˆ–å½¢çŠ¶ä¸º `(batch_size, height, width, num_channels)` çš„ NumPy æ•°ç»„ã€‚

+   `nsfw_content_detected` (`List[bool]`) â€” åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹ï¼Œå¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º `None`ã€‚

ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

## FlaxStableDiffusionImg2ImgPipeline

### `class diffusers.FlaxStableDiffusionImg2ImgPipeline`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py#L105)

```py
( vae: FlaxAutoencoderKL text_encoder: FlaxCLIPTextModel tokenizer: CLIPTokenizer unet: FlaxUNet2DConditionModel scheduler: Union safety_checker: FlaxStableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor dtype: dtype = <class 'jax.numpy.float32'> )
```

å‚æ•°

+   `vae` ([FlaxAutoencoderKL](/docs/diffusers/v0.26.3/en/api/models/autoencoderkl#diffusers.FlaxAutoencoderKL)) â€” å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œç¼–ç å’Œè§£ç ä»¥åŠä»æ½œåœ¨è¡¨ç¤ºåˆ°å›¾åƒçš„è§£ç ã€‚

+   `text_encoder` ([FlaxCLIPTextModel](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.FlaxCLIPTextModel)) â€” å†»ç»“çš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆ[clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)ï¼‰ã€‚

+   `tokenizer` ([CLIPTokenizer](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPTokenizer)) â€” ç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–çš„ `CLIPTokenizer`ã€‚

+   `unet` ([FlaxUNet2DConditionModel](/docs/diffusers/v0.26.3/en/api/models/unet2d-cond#diffusers.FlaxUNet2DConditionModel)) â€” ç”¨äºå»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„ `FlaxUNet2DConditionModel`ã€‚

+   `scheduler` ([SchedulerMixin](/docs/diffusers/v0.26.3/en/api/schedulers/overview#diffusers.SchedulerMixin)) â€” ä¸ `unet` ç»“åˆä½¿ç”¨ä»¥å»å™ªç¼–ç å›¾åƒæ½œå˜é‡çš„è°ƒåº¦å™¨ã€‚å¯ä»¥æ˜¯ `FlaxDDIMScheduler`ã€`FlaxLMSDiscreteScheduler`ã€`FlaxPNDMScheduler` æˆ– `FlaxDPMSolverMultistepScheduler` ä¸­çš„ä¸€ä¸ªã€‚

+   `safety_checker` (`FlaxStableDiffusionSafetyChecker`) â€” ç”¨äºä¼°è®¡ç”Ÿæˆå›¾åƒæ˜¯å¦å¯èƒ½è¢«è®¤ä¸ºå…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„åˆ†ç±»æ¨¡å—ã€‚è¯·å‚è€ƒ [æ¨¡å‹å¡ç‰‡](https://huggingface.co/runwayml/stable-diffusion-v1-5) ä»¥è·å–æœ‰å…³æ¨¡å‹æ½œåœ¨å±å®³çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

+   `feature_extractor` ([CLIPImageProcessor](https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/clip#transformers.CLIPImageProcessor)) â€” ç”¨äºä»ç”Ÿæˆçš„å›¾åƒä¸­æå–ç‰¹å¾çš„ `CLIPImageProcessor`ï¼›ä½œä¸º `safety_checker` çš„è¾“å…¥ã€‚

åŸºäº Flax çš„æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„ç¨³å®šæ‰©æ•£ç®¡é“ã€‚

æ­¤æ¨¡å‹ç»§æ‰¿è‡ª [FlaxDiffusionPipeline](/docs/diffusers/v0.26.3/en/api/pipelines/overview#diffusers.FlaxDiffusionPipeline)ã€‚æ£€æŸ¥è¶…ç±»æ–‡æ¡£ä»¥è·å–æ‰€æœ‰ç®¡é“å®ç°çš„é€šç”¨æ–¹æ³•ï¼ˆä¸‹è½½ã€ä¿å­˜ã€åœ¨ç‰¹å®šè®¾å¤‡ä¸Šè¿è¡Œç­‰ï¼‰ã€‚

#### `__call__`

[< source >](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py#L337)

```py
( prompt_ids: Array image: Array params: Union prng_seed: Array strength: float = 0.8 num_inference_steps: int = 50 height: Optional = None width: Optional = None guidance_scale: Union = 7.5 noise: Array = None neg_prompt_ids: Array = None return_dict: bool = True jit: bool = False ) â†’ export const metadata = 'undefined';FlaxStableDiffusionPipelineOutput or tuple
```

å‚æ•°

+   `prompt_ids`ï¼ˆ`jnp.ndarray`ï¼‰ - ç”¨äºæŒ‡å¯¼å›¾åƒç”Ÿæˆçš„æç¤ºæˆ–æç¤ºã€‚

+   `image`ï¼ˆ`jnp.ndarray`ï¼‰ - è¡¨ç¤ºè¦ç”¨ä½œèµ·ç‚¹çš„å›¾åƒæ‰¹å¤„ç†çš„æ•°ç»„ã€‚

+   `params`ï¼ˆ`Dict`æˆ–`FrozenDict`ï¼‰ - åŒ…å«æ¨¡å‹å‚æ•°/æƒé‡çš„å­—å…¸ã€‚

+   `prng_seed`ï¼ˆ`jax.Array`æˆ–`jax.Array`ï¼‰ - åŒ…å«éšæœºæ•°ç”Ÿæˆå™¨å¯†é’¥çš„æ•°ç»„ã€‚

+   `strength`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º0.8ï¼‰ - æŒ‡ç¤ºè½¬æ¢å‚è€ƒ`image`çš„ç¨‹åº¦ã€‚å¿…é¡»ä»‹äº0å’Œ1ä¹‹é—´ã€‚`image`ç”¨ä½œèµ·ç‚¹ï¼Œ`strength`è¶Šé«˜ï¼Œæ·»åŠ çš„å™ªå£°è¶Šå¤šã€‚é™å™ªæ­¥éª¤çš„æ•°é‡å–å†³äºæœ€åˆæ·»åŠ çš„å™ªå£°é‡ã€‚å½“`strength`ä¸º1æ—¶ï¼Œæ·»åŠ çš„å™ªå£°æœ€å¤§ï¼Œå¹¶ä¸”é™å™ªè¿‡ç¨‹å°†è¿è¡ŒæŒ‡å®šçš„`num_inference_steps`çš„å®Œæ•´è¿­ä»£æ¬¡æ•°ã€‚å€¼ä¸º1åŸºæœ¬ä¸Šå¿½ç•¥`image`ã€‚

+   `num_inference_steps`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º50ï¼‰ - é™å™ªæ­¥éª¤çš„æ•°é‡ã€‚æ›´å¤šçš„é™å™ªæ­¥éª¤é€šå¸¸ä¼šå¯¼è‡´æ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚æ­¤å‚æ•°ç”±`strength`è°ƒèŠ‚ã€‚

+   `height`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰ - ç”Ÿæˆå›¾åƒçš„åƒç´ é«˜åº¦ã€‚

+   `width`ï¼ˆ`int`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`self.unet.config.sample_size * self.vae_scale_factor`ï¼‰ - ç”Ÿæˆå›¾åƒçš„åƒç´ å®½åº¦ã€‚

+   `guidance_scale`ï¼ˆ`float`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º7.5ï¼‰ - æ›´é«˜çš„å¼•å¯¼æ¯”ä¾‹å€¼é¼“åŠ±æ¨¡å‹ç”Ÿæˆä¸æ–‡æœ¬`prompt`ç´§å¯†ç›¸å…³çš„å›¾åƒï¼Œä½†ä¼šé™ä½å›¾åƒè´¨é‡ã€‚å½“`guidance_scale > 1`æ—¶å¯ç”¨å¼•å¯¼æ¯”ä¾‹ã€‚

+   `noise`ï¼ˆ`jnp.ndarray`ï¼Œ*å¯é€‰*ï¼‰ - ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·çš„é¢„ç”Ÿæˆå˜ˆæ‚æ½œåœ¨å˜é‡ï¼Œç”¨ä½œå›¾åƒç”Ÿæˆçš„è¾“å…¥ã€‚å¯ç”¨äºä½¿ç”¨ä¸åŒæç¤ºè°ƒæ•´ç›¸åŒç”Ÿæˆã€‚è¯¥æ•°ç»„æ˜¯é€šè¿‡ä½¿ç”¨æä¾›çš„éšæœº`generator`è¿›è¡Œé‡‡æ ·ç”Ÿæˆçš„ã€‚

+   `return_dict`ï¼ˆ`bool`ï¼Œ*å¯é€‰*ï¼Œé»˜è®¤ä¸º`True`ï¼‰ - æ˜¯å¦è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)è€Œä¸æ˜¯æ™®é€šå…ƒç»„ã€‚

+   `jit`ï¼ˆ`bool`ï¼Œé»˜è®¤ä¸º`False`ï¼‰ - æ˜¯å¦è¿è¡Œç”Ÿæˆå’Œå®‰å…¨è¯„åˆ†å‡½æ•°çš„`pmap`ç‰ˆæœ¬ã€‚

    å­˜åœ¨æ­¤å‚æ•°æ˜¯å› ä¸º`__call__`å°šä¸æ˜¯ç«¯åˆ°ç«¯çš„pmap-ableã€‚å®ƒå°†åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­è¢«ç§»é™¤ã€‚

è¿”å›

[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)æˆ–`tuple`

å¦‚æœ`return_dict`ä¸º`True`ï¼Œåˆ™è¿”å›[FlaxStableDiffusionPipelineOutput](/docs/diffusers/v0.26.3/en/api/pipelines/stable_diffusion/text2img#diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput)ï¼Œå¦åˆ™è¿”å›ä¸€ä¸ª`tuple`ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ç”Ÿæˆå›¾åƒçš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªåŒ…å«ç›¸åº”ç”Ÿæˆå›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹çš„`bool`åˆ—è¡¨ã€‚

ç”¨äºç”Ÿæˆçš„ç®¡é“çš„è°ƒç”¨å‡½æ•°ã€‚

ç¤ºä¾‹ï¼š

```py
>>> import jax
>>> import numpy as np
>>> import jax.numpy as jnp
>>> from flax.jax_utils import replicate
>>> from flax.training.common_utils import shard
>>> import requests
>>> from io import BytesIO
>>> from PIL import Image
>>> from diffusers import FlaxStableDiffusionImg2ImgPipeline

>>> def create_key(seed=0):
...     return jax.random.PRNGKey(seed)

>>> rng = create_key(0)

>>> url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
>>> response = requests.get(url)
>>> init_img = Image.open(BytesIO(response.content)).convert("RGB")
>>> init_img = init_img.resize((768, 512))

>>> prompts = "A fantasy landscape, trending on artstation"

>>> pipeline, params = FlaxStableDiffusionImg2ImgPipeline.from_pretrained(
...     "CompVis/stable-diffusion-v1-4",
...     revision="flax",
...     dtype=jnp.bfloat16,
... )

>>> num_samples = jax.device_count()
>>> rng = jax.random.split(rng, jax.device_count())
>>> prompt_ids, processed_image = pipeline.prepare_inputs(
...     prompt=[prompts] * num_samples, image=[init_img] * num_samples
... )
>>> p_params = replicate(params)
>>> prompt_ids = shard(prompt_ids)
>>> processed_image = shard(processed_image)

>>> output = pipeline(
...     prompt_ids=prompt_ids,
...     image=processed_image,
...     params=p_params,
...     prng_seed=rng,
...     strength=0.75,
...     num_inference_steps=50,
...     jit=True,
...     height=512,
...     width=768,
... ).images

>>> output_images = pipeline.numpy_to_pil(np.asarray(output.reshape((num_samples,) + output.shape[-3:])))
```

## FlaxStableDiffusionPipelineOutput

### `class diffusers.pipelines.stable_diffusion.FlaxStableDiffusionPipelineOutput`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/diffusers/pipelines/stable_diffusion/pipeline_output.py#L31)

```py
( images: ndarray nsfw_content_detected: List )
```

å‚æ•°

+   `images`ï¼ˆ`np.ndarray`ï¼‰ - å½¢çŠ¶ä¸º`(batch_size, height, width, num_channels)`çš„å»å™ªå›¾åƒæ•°ç»„ã€‚

+   `nsfw_content_detected`ï¼ˆ`List[bool]`ï¼‰ - åˆ—è¡¨æŒ‡ç¤ºç›¸åº”ç”Ÿæˆçš„å›¾åƒæ˜¯å¦åŒ…å«â€œä¸é€‚å®œå·¥ä½œâ€ï¼ˆnsfwï¼‰å†…å®¹ï¼Œæˆ–è€…å¦‚æœæ— æ³•æ‰§è¡Œå®‰å…¨æ£€æŸ¥ï¼Œåˆ™ä¸º`None`ã€‚

åŸºäºFlaxçš„ç¨³å®šæ‰©æ•£ç®¡é“çš„è¾“å‡ºç±»ã€‚

#### `replace`

[<æ¥æº>](https://github.com/huggingface/diffusers/blob/v0.26.3/src/flax/struct.py#L111)

```py
( **updates )
```

â€œè¿”å›ä¸€ä¸ªç”¨æ–°å€¼æ›¿æ¢æŒ‡å®šå­—æ®µçš„æ–°å¯¹è±¡ã€‚
