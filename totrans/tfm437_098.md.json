["```py\n[\"Don't\", \"you\", \"love\", \"\ud83e\udd17\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n```", "```py\n[\"Don\", \"'\", \"t\", \"you\", \"love\", \"\ud83e\udd17\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```", "```py\n[\"Do\", \"n't\", \"you\", \"love\", \"\ud83e\udd17\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```", "```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n>>> tokenizer.tokenize(\"I have a new GPU!\")\n[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]\n```", "```py\n>>> from transformers import XLNetTokenizer\n\n>>> tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n>>> tokenizer.tokenize(\"Don't you love \ud83e\udd17 Transformers? We sure do.\")\n[\"\u2581Don\", \"'\", \"t\", \"\u2581you\", \"\u2581love\", \"\u2581\", \"\ud83e\udd17\", \"\u2581\", \"Transform\", \"ers\", \"?\", \"\u2581We\", \"\u2581sure\", \"\u2581do\", \".\"]\n```", "```py\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```", "```py\n(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n```", "```py\n(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n```", "```py\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n```", "```py\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n```"]