- en: PEFT configurations and models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Original text: [https://huggingface.co/docs/peft/tutorial/peft_model_config](https://huggingface.co/docs/peft/tutorial/peft_model_config)'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: null
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The sheer size of today’s large pretrained models - which commonly have billions
    of parameters - present a significant training challenge because they require
    more storage space and more computational power to crunch all those calculations.
    You’ll need access to powerful GPUs or TPUs to train these large pretrained models
    which is expensive, not widely accessible to everyone, not environmentally friendly,
    and not very practical. PEFT methods address many of these challenges. There are
    several types of PEFT methods (soft prompting, matrix decomposition, adapters),
    but they all focus on the same thing, reduce the number of trainable parameters.
    This makes it more accessible to train and store large models on consumer hardware.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The PEFT library is designed to help you quickly train large models on free
    or low-cost GPUs, and in this tutorial, you’ll learn how to setup a configuration
    to apply a PEFT method to a pretrained base model for training. Once the PEFT
    configuration is setup, you can use any training framework you like (Transformer’s
    [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, [Accelerate](https://hf.co/docs/accelerate), a custom PyTorch training
    loop).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: PEFT configurations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learn more about the parameters you can configure for each PEFT method in their
    respective API reference page.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: A configuration stores important parameters that specify how a particular PEFT
    method should be applied.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: For example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json)
    for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json)
    for applying p-tuning (these configuration files are already JSON-serialized).
    Whenever you load a PEFT adapter, it is a good idea to check whether it has an
    associated adapter_config.json file which is required.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: LoraConfigPromptEncoderConfig
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can create your own configuration for training by initializing a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PEFT models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a PEFT configuration in hand, you can now apply it to any pretrained model
    to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers)
    library, a custom model, and even new and unsupported transformer architectures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)
    model to finetune.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Use the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    from the base facebook/opt-350m model and the `lora_config` you created earlier.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now you can train the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    with your preferred training framework! After training, you can save your model
    locally with [save_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.save_pretrained)
    or upload it to the Hub with the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
    method.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    for inference, you’ll need to provide the [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    used to create it and the base model it was trained from.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: By default, the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    is set for inference, but if you’d like to train the adapter some more you can
    set `is_trainable=True`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The [PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
    method is the most flexible way to load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    because it doesn’t matter what model framework was used (Transformers, timm, a
    generic PyTorch model). Other classes, like [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel),
    are just a convenient wrapper around the base [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel),
    and makes it easier to load PEFT models directly from the Hub or locally where
    the PEFT weights are stored.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)方法是加载[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)最灵活的方式，因为不管使用了什么模型框架（Transformers、timm、通用的PyTorch模型）都可以。其他类，如[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)，只是基本[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)的一个方便包装器，使得可以更容易地直接从Hub或本地加载PEFT模型，其中存储了PEFT权重。'
- en: '[PRE7]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Take a look at the [AutoPeftModel](package_reference/auto_class) API reference
    to learn more about the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    classes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[AutoPeftModel](package_reference/auto_class) API参考，了解更多关于[AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)类的信息。
- en: Next steps
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步
- en: 'With the appropriate [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig),
    you can apply it to any pretrained model to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    and train large powerful models faster on freely available GPUs! To learn more
    about PEFT configurations and models, the following guide may be helpful:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过适当的[PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)，您可以将其应用于任何预训练模型，创建一个[PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)，并在免费提供的GPU上更快地训练大型强大模型！要了解更多关于PEFT配置和模型的信息，以下指南可能会有所帮助：
- en: Learn how to configure a PEFT method for models that aren’t from Transformers
    in the [Working with custom models](../developer_guides/custom_models) guide.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何为不来自Transformers的模型配置PEFT方法，可以查看[使用自定义模型](../developer_guides/custom_models)指南。
