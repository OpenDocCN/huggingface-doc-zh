- en: PEFT configurations and models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://huggingface.co/docs/peft/tutorial/peft_model_config](https://huggingface.co/docs/peft/tutorial/peft_model_config)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: <link href="/docs/peft/v0.8.2/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/start.c9bed6ec.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/scheduler.d627b047.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/singletons.95cf6adf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.a57a1c33.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/paths.5d07c46f.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/entry/app.72c78cae.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/index.d48c4817.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/0.aa346fde.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/each.e59479a4.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/nodes/39.f0120790.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Tip.9bd3babf.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/CodeBlock.5da89496.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/Heading.47e562a9.js">
    <link rel="modulepreload" href="/docs/peft/v0.8.2/en/_app/immutable/chunks/HfOption.84c24d39.js">
  prefs: []
  type: TYPE_NORMAL
- en: The sheer size of today’s large pretrained models - which commonly have billions
    of parameters - present a significant training challenge because they require
    more storage space and more computational power to crunch all those calculations.
    You’ll need access to powerful GPUs or TPUs to train these large pretrained models
    which is expensive, not widely accessible to everyone, not environmentally friendly,
    and not very practical. PEFT methods address many of these challenges. There are
    several types of PEFT methods (soft prompting, matrix decomposition, adapters),
    but they all focus on the same thing, reduce the number of trainable parameters.
    This makes it more accessible to train and store large models on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The PEFT library is designed to help you quickly train large models on free
    or low-cost GPUs, and in this tutorial, you’ll learn how to setup a configuration
    to apply a PEFT method to a pretrained base model for training. Once the PEFT
    configuration is setup, you can use any training framework you like (Transformer’s
    [Trainer](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/trainer#transformers.Trainer)
    class, [Accelerate](https://hf.co/docs/accelerate), a custom PyTorch training
    loop).
  prefs: []
  type: TYPE_NORMAL
- en: PEFT configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learn more about the parameters you can configure for each PEFT method in their
    respective API reference page.
  prefs: []
  type: TYPE_NORMAL
- en: A configuration stores important parameters that specify how a particular PEFT
    method should be applied.
  prefs: []
  type: TYPE_NORMAL
- en: For example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json)
    for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json)
    for applying p-tuning (these configuration files are already JSON-serialized).
    Whenever you load a PEFT adapter, it is a good idea to check whether it has an
    associated adapter_config.json file which is required.
  prefs: []
  type: TYPE_NORMAL
- en: LoraConfigPromptEncoderConfig
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can create your own configuration for training by initializing a [LoraConfig](/docs/peft/v0.8.2/en/package_reference/lora#peft.LoraConfig).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: PEFT models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a PEFT configuration in hand, you can now apply it to any pretrained model
    to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel).
    Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers)
    library, a custom model, and even new and unsupported transformer architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)
    model to finetune.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Use the [get_peft_model()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.get_peft_model)
    function to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    from the base facebook/opt-350m model and the `lora_config` you created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now you can train the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    with your preferred training framework! After training, you can save your model
    locally with [save_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.save_pretrained)
    or upload it to the Hub with the [push_to_hub](https://huggingface.co/docs/transformers/v4.37.2/en/main_classes/model#transformers.PreTrainedModel.push_to_hub)
    method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    for inference, you’ll need to provide the [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig)
    used to create it and the base model it was trained from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: By default, the [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    is set for inference, but if you’d like to train the adapter some more you can
    set `is_trainable=True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The [PeftModel.from_pretrained()](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel.from_pretrained)
    method is the most flexible way to load a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    because it doesn’t matter what model framework was used (Transformers, timm, a
    generic PyTorch model). Other classes, like [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel),
    are just a convenient wrapper around the base [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel),
    and makes it easier to load PEFT models directly from the Hub or locally where
    the PEFT weights are stored.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at the [AutoPeftModel](package_reference/auto_class) API reference
    to learn more about the [AutoPeftModel](/docs/peft/v0.8.2/en/package_reference/auto_class#peft.AutoPeftModel)
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the appropriate [PeftConfig](/docs/peft/v0.8.2/en/package_reference/config#peft.PeftConfig),
    you can apply it to any pretrained model to create a [PeftModel](/docs/peft/v0.8.2/en/package_reference/peft_model#peft.PeftModel)
    and train large powerful models faster on freely available GPUs! To learn more
    about PEFT configurations and models, the following guide may be helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to configure a PEFT method for models that aren’t from Transformers
    in the [Working with custom models](../developer_guides/custom_models) guide.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
