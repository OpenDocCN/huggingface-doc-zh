["```py\n( vae: AutoencoderKL text_encoder: CLIPTextModel tokenizer: CLIPTokenizer unet: UNet2DConditionModel scheduler: KarrasDiffusionSchedulers safety_checker: StableDiffusionSafetyChecker feature_extractor: CLIPImageProcessor requires_safety_checker: bool = True )\n```", "```py\n( prompt: Union height: Optional = None width: Optional = None num_inference_steps: int = 50 guidance_scale: float = 7.5 negative_prompt: Union = None num_images_per_prompt: int = 1 eta: float = 0.0 generator: Union = None latents: Optional = None output_type: Optional = 'pil' return_dict: bool = True callback: Optional = None callback_steps: int = 1 editing_prompt: Union = None editing_prompt_embeddings: Optional = None reverse_editing_direction: Union = False edit_guidance_scale: Union = 5 edit_warmup_steps: Union = 10 edit_cooldown_steps: Union = None edit_threshold: Union = 0.9 edit_momentum_scale: Optional = 0.1 edit_mom_beta: Optional = 0.4 edit_weights: Optional = None sem_guidance: Optional = None ) \u2192 export const metadata = 'undefined';~pipelines.semantic_stable_diffusion.SemanticStableDiffusionPipelineOutput or tuple\n```", "```py\n>>> import torch\n>>> from diffusers import SemanticStableDiffusionPipeline\n\n>>> pipe = SemanticStableDiffusionPipeline.from_pretrained(\n...     \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n... )\n>>> pipe = pipe.to(\"cuda\")\n\n>>> out = pipe(\n...     prompt=\"a photo of the face of a woman\",\n...     num_images_per_prompt=1,\n...     guidance_scale=7,\n...     editing_prompt=[\n...         \"smiling, smile\",  # Concepts to apply\n...         \"glasses, wearing glasses\",\n...         \"curls, wavy hair, curly hair\",\n...         \"beard, full beard, mustache\",\n...     ],\n...     reverse_editing_direction=[\n...         False,\n...         False,\n...         False,\n...         False,\n...     ],  # Direction of guidance i.e. increase all concepts\n...     edit_warmup_steps=[10, 10, 10, 10],  # Warmup period for each concept\n...     edit_guidance_scale=[4, 5, 5, 5.4],  # Guidance scale for each concept\n...     edit_threshold=[\n...         0.99,\n...         0.975,\n...         0.925,\n...         0.96,\n...     ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions\n...     edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance\n...     edit_mom_beta=0.6,  # Momentum beta\n...     edit_weights=[1, 1, 1, 1, 1],  # Weights of the individual concepts against each other\n... )\n>>> image = out.images[0]\n```", "```py\n( images: Union nsfw_content_detected: Optional )\n```"]